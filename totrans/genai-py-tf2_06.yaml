- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Generation with GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative modeling is a powerful concept that provides us with immense potential
    to approximate or model underlying processes that generate data. In the previous
    chapters, we covered concepts associated with deep learning in general and more
    specifically related to **restricted Boltzmann machines** (**RBMs**) and **variational
    autoencoders** (**VAEs**). This chapter will introduce another family of generative
    models called **Generative Adversarial Networks** (**GANs**).
  prefs: []
  type: TYPE_NORMAL
- en: Heavily inspired by the concepts of game theory and picking up some of the best
    components from previously discussed techniques, GANs provide a powerful framework
    for working in the generative modeling space. Since their invention in 2014 by
    Goodfellow et al., GANs have benefitted from tremendous research and are now being
    used to explore creative domains such as art, fashion, and photography.
  prefs: []
  type: TYPE_NORMAL
- en: The following are two amazing high-quality samples from a variant of GANs called
    StyleGAN (*Figure 6.1*). The photograph of the kid is actually a fictional person
    who does not exist. The art sample is also generated by a similar network. StyleGANs
    are able to generate high-quality sharp images by using the concept of progressive
    growth (we will cover this in detail in later sections). These outputs were generated
    using the StyleGAN2 model trained on datasets such as the **Flickr-Faces-HQ**
    or **FFHQ** dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Imagined by a GAN (StyleGAN2) (Dec 2019) - Karras et al. and Nvidia²'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The taxonomy of generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of improved GANs, such as DCGAN, Conditional-GAN, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The progressive GAN setup and its various components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the challenges associated with GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The taxonomy of generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models are a class of models in the unsupervised machine learning
    space. They help us to model the underlying distributions responsible for generating
    the dataset under consideration. There are different methods/frameworks to work
    with generative models. The first set of methods correspond to models that represent
    data with an explicit density function. Here we define a probability density function,
    ![](img/B16176_06_001.png), explicitly and develop a model that increases the
    maximum likelihood of sampling from this distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two further types within explicit density methods, *tractable* and
    *approximate* density methods. PixelRNNs are an active area of research for tractable
    density methods. When we try to model complex real-world data distributions, for
    example, natural images or speech signals, defining a parametric function becomes
    challenging. To overcome this, you learned about RBMs and VAEs in *Chapter 4*,
    *Teaching Networks to Generate Digits*, and *Chapter 5*, *Painting Pictures with
    Neural Networks Using VAEs*, respectively. These techniques work by approximating
    the underlying probability density functions explicitly. VAEs work towards maximizing
    the likelihood estimates of the lower bound, while RBMs use Markov chains to make
    an estimate of the distribution. The overall landscape of generative models can
    be described using *Figure 6.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: The taxonomy of generative models³'
  prefs: []
  type: TYPE_NORMAL
- en: GANs fall under implicit density modeling methods. The implicit density functions
    give up the property of explicitly defining the underlying distribution but work
    by defining methods to draw samples from such distributions. The GAN framework
    is a class of methods that can sample directly from the underlying distributions.
    This alleviates some of the complexities associated with the methods we have covered
    so far, such as defining underlying probability distribution functions and the
    quality of outputs. Now that you have a high-level understanding of generative
    models, let's dive deeper into the details of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs have a pretty interesting origin story. It all began as a discussion/argument
    in a bar with Ian Goodfellow and friends discussing work related to generating
    data using neural networks. The argument ended with everyone downplaying each
    other's methods. Goodfellow went back home and coded the first version of what
    we now call a GAN. To his amazement, the code worked on the first try. A more
    verbose description of the chain of events was shared by Goodfellow himself in
    an interview with *Wired* magazine.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, GANs are implicit density functions that sample directly from
    the underlying distribution. They do this by defining a two-player game of adversaries.
    The adversaries compete against each other under well-defined reward functions
    and each player tries to maximize its rewards. Without going into the details
    of game theory, the framework can be explained as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model represents a differentiable function that tries to maximize a probability
    of 1 for samples drawn from the training distribution. This can be any classification
    model, but we usually prefer a deep neural network for this. This is the throw-away
    model (similar to the decoder part of autoencoders).
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator is also used to classify whether the output from the generator
    is real or fake. The main utility of this model is to help develop a robust generator.
    We denote the discriminator model as *D* and its output as *D*(*x*). When it is
    used to classify output from the generator model, the discriminator model is denoted
    as *D*(*G*(*z*)), where *G*(*z*) is the output from the generator model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Text, application, chat or text message  Description automatically generated](img/B16176_06_03_new.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: The discriminator model'
  prefs: []
  type: TYPE_NORMAL
- en: The generator model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the primary model of interest in the whole game. This model generates
    samples that are intended to resemble the samples from our training set. The model
    takes random unstructured noise as input (typically denoted as *z*) and tries
    to create a varied set of outputs. The generator model is usually a differentiable
    function; it is often represented by a deep neural network but is not restricted
    to that.
  prefs: []
  type: TYPE_NORMAL
- en: We denote the generator as *G* and its output as *G*(*z*). We typically use
    a lower-dimensional *z* as compared to the dimension of the original data, *x*,
    that is, ![](img/B16176_06_002.png). This is done as a way of compressing or encoding
    real-world information into lower-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B16176_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The generator model'
  prefs: []
  type: TYPE_NORMAL
- en: In simple words, the generator trains to generate samples good enough to fool
    the discriminator, while the discriminator trains to properly classify real (training
    samples) versus fake (output from the generator). Thus, this game of adversaries
    uses a generator model, *G*, which tries to make *D*(*G*(*z*)) as close to 1 as
    possible. The discriminator is incentivized to make *D*(*G*(*z*)) close to 0,
    where 1 denotes real and 0 denotes fake samples. The GAN model achieves equilibrium
    when the generator starts to easily fool the discriminator, that is, the discriminator
    reaches its saddle point. While, in theory, GANs have several advantages over
    other methods in the family tree described previously, they pose their own set
    of problems. We will discuss some of them in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Training GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training a GAN is like playing this game of two adversaries. The generator
    is learning to generate good enough fake samples, while the discriminator is working
    hard to discriminate between real and fake. More formally, this is termed as the
    minimax game, where the value function *V*(*G*, *D*) is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is also called the zero-sum game, which has an equilibrium that is the
    same as the Nash equilibrium. We can better understand the value function *V*(*G*,
    *D*) by separating out the objective function for each of the players. The following
    equations describe individual objective functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_004.png)![](img/B16176_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B16176_06_006.png)is the discriminator objective function in the
    classical sense, ![](img/B16176_06_007.png)is the generator objective equal to
    the negative of the discriminator, and ![](img/B16176_06_008.png) is the distribution
    of the training data. The rest of the terms have their usual meaning. This is
    one of the simplest ways of defining the game or corresponding objective functions.
    Over the years, different ways have been studied, some of which we will cover
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective functions help us to understand the aim of each of the players.
    If we assume both probability densities are non-zero everywhere, we can get the
    optimal value of *D*(*x*) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_009.png)'
  prefs: []
  type: TYPE_IMG
- en: We will revisit this equation in the latter part of the chapter. For now, the
    next step is to present a training algorithm wherein the discriminator and generator
    models train towards their respective objectives. The simplest yet widely used
    way of training a GAN (and by far the most successful one) is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the following steps *N* times. *N* is the number of total iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat steps *k* times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample a minibatch of size *m* from the generator: *{z*[1]*, z*[2]*, … z*[m]*}
    = p*[model]*(z)*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sample a minibatch of size m from the actual data: *{x*[1]*, x*[2]*, … x*[m]*}
    = p*[data]*(x)*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the discriminator loss, ![](img/B16176_06_010.png)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the discriminator as non-trainable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample a minibatch of size *m* from the generator: *{z*[1]*, z*[2]*, … z*[m]*}
    = p*[model]*(z)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the generator loss, ![](img/B16176_06_011.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In their original paper, Goodfellow et al. used *k* =1, that is, they trained
    discriminator and generator models alternately. There are some variants and hacks
    where it is observed that training the discriminator more often than the generator
    helps with better convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure (*Figure 6.5*) showcases the training phases of the generator
    and discriminator models. The smaller dotted line is the discriminator model,
    the solid line is the generator model, and the larger dotted line is the actual
    training data. The vertical lines at the bottom demonstrate the sampling of data
    points from the distribution of *z*, that is, *x* = *p*[model]*(z)*. The lines
    point to the fact that the generator contracts in the regions of high density
    and expands in the regions of low density. Part **(a)** shows the initial stages
    of the training phase where the discriminator *(D)* is a partially correct classifier.
    Parts **(b)** and **(c)** show how improvements in *D* guide changes in the generator,
    *G*. Finally, in part **(d)** you can see where *p*[model] *= p*[data] and the
    discriminator is no longer able to differentiate between fake and real samples,
    that is, ![](img/B16176_06_012.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a map  Description automatically generated](img/B16176_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: The training process for GAN⁴'
  prefs: []
  type: TYPE_NORMAL
- en: Non-saturating generator cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In practice, we do not train the generator to minimize *log*(*1* – *D*(*G*(*z*)))
    as this function does not provide sufficient gradients for learning. During the
    initial learning phases, where *G* is poor, the discriminator is able to classify
    the fake from the real with high confidence. This leads to the saturation of *log*(*1*
    – *D*(*G*(*z*))), which hinders improvements in the generator model. We thus tweak
    the generator to maximize *log*(*D*(*G*(*z*))) instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_0131.png)'
  prefs: []
  type: TYPE_IMG
- en: This provides stronger gradients for the generator to learn. This is shown in
    *Figure 6.6*. The *x*-axis denotes *D*(*G*(*z*)). The top line shows the objective,
    which is minimizing the likelihood of the discriminator being correct. The bottom
    line (updated objective) works by maximizing the likelihood of the discriminator
    being wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a map  Description automatically generated](img/B16176_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Generator objective functions⁵'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.6* illustrates how a slight change helps achieve better gradients
    during the initial phases of training.'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The minimax game can be transformed into a maximum likelihood game where the
    aim is to maximize the likelihood of the generator probability density. This is
    done to ensure that the generator probability density is similar to the real/training
    data probability density. In other words, the game can be transformed into minimizing
    the divergence between *p*[z] and *p*[data]. To do so, we make use of **Kullback-Leibler
    divergence** (**KL divergence**) to calculate the similarity between two distributions
    of interest. The overall value function can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost function for the generator transforms to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_015.png)'
  prefs: []
  type: TYPE_IMG
- en: One important point to note is that KL divergence is not a symmetric measure,
    that is, ![](img/B16176_06_016.png). The model typically uses ![](img/B16176_06_017.png)
    to achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three different cost functions discussed so far have slightly different
    trajectories and thus lead to different properties at different stages of training.
    These three functions can be visualized as shown in *Figure 6.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Generator cost functions⁶'
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered quite a bit of ground in understanding the basics of GANs. In
    this section, we will apply that understanding and build a GAN from scratch. This
    generative model will consist of a repeating block architecture, similar to the
    one presented in the original paper. We will try to replicate the task of generating
    MNIST digits using our network.
  prefs: []
  type: TYPE_NORMAL
- en: The overall GAN setup can be seen in *Figure 6.8*. The figure outlines a generator
    model with noise vector *z* as input and repeating blocks that transform and scale
    up the vector to the required dimensions. Each block consists of a dense layer
    followed by Leaky ReLU activation and a batch-normalization layer. We simply reshape
    the output from the final block to transform it into the required output image
    size.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator, on the other hand, is a simple feedforward network. This
    model takes an image as input (a real image or the fake output from the generator)
    and classifies it as real or fake. This simple setup of two competing models helps
    us to train the overall GAN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Vanilla GAN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be relying on TensorFlow 2 and using the high-level Keras API wherever
    possible. The first step is to define the discriminator model. In this implementation,
    we will use a very basic **multi-layer perceptron** (**MLP**) as the discriminator
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use the sequential API to prepare this simple model, with just four
    layers and the final output layer with sigmoid activation. Since we have a binary
    classification task, we have only one unit in the final layer. We will use binary
    cross-entropy loss to train the discriminator model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator model is also a multi-layer perceptron with multiple layers scaling
    up the noise vector *z* to the desired size. Since our task is to generate MNIST-like
    output samples, the final reshape layer will convert the flat vector into a 28x28
    output shape. Note that we will make use of batch normalization to stabilize model
    training. The following snippet shows a utility method for building the generator
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We simply use these utility methods to create generator and discriminator model
    objects. The following snippet uses these two model objects to create the GAN
    object as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The final piece of the puzzle is defining the training loop. As described in
    the previous section, we will train both (discriminator and generator) models
    alternatingly. Doing so is straightforward with high-level Keras APIs. The following
    code snippet first loads the MNIST dataset and scales the pixel values between
    -1 and +1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For each training iteration, we first sample real images from the MNIST dataset
    equal to our defined batch size. The next step involves sampling the same number
    of *z* vectors. We use these sampled *z* vectors to generate output from our generator
    model. Finally, we calculate the discriminator loss on both real and generated
    samples. These steps are explained in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the generator is straightforward. We prepare a stacked model object
    that resembles the GAN architecture we discussed previously. Simply using the
    `train_on_batch` helps us to calculate the generator loss and improve it, as shown
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We train our vanilla GAN for about 30,000 iterations. The following (*Figure
    6.9*) are model outputs at different stages of the training. You can clearly see
    how the sample quality improves as we move from one stage to another.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Vanilla GAN output at different stages of training'
  prefs: []
  type: TYPE_NORMAL
- en: The results from vanilla GAN are encouraging yet leave space for further improvements.
    In the next section, we will explore some improved architectures to enhance the
    generative capabilities of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Improved GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vanilla GAN proved the potential of adversarial networks. The ease of setting
    up the models and the quality of the output sparked much interest in this field.
    This led to a lot of research in improving the GAN paradigm. In this section,
    we will cover a few of the major improvements in developing GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Convolutional GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Published in 2016, this work by Radford et al. introduced several key contributions
    to improve GAN outputs apart from focusing on convolutional layers, which are
    discussed in the original GAN paper. The 2016 paper emphasized using deeper architectures
    instead. *Figure 6.10* shows the generator architecture for a **Deep Convolutional
    GAN** (**DCGAN**) (as proposed by the authors). The generator takes the noise
    vector as input and then passes it through a repeating setup of up-sampling layers,
    convolutional layers, and batch normalization layers to stabilize the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a map  Description automatically generated](img/B16176_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: DCGAN generator architecture⁷'
  prefs: []
  type: TYPE_NORMAL
- en: Until the introduction of DCGANs, the output image resolution was quite limited.
    A Laplacian pyramid or LAPGAN was proposed to generate high-quality images, but
    it also suffered from certain fuzziness in the output. The DCGAN paper also made
    use of another important invention, the batch normalization layer. Batch normalization
    was presented after the original GAN paper and proved useful in stabilizing the
    overall training by normalizing the input for each unit to have zero mean and
    unit variance. To get higher-resolution images, it made use of strides greater
    than 1 while moving the convolutional filters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by preparing the discriminator model. CNN-based binary classifiers
    are simple models. One modification we make here is to use strides longer than
    1 to down-sample the input between layers instead of using pooling layers. This
    helps in providing better stability for the training of the generator model. We
    also rely on batch normalization and Leaky ReLU for the same purposes (although
    these were not used in the original GAN paper). Another important aspect of this
    discriminator (as compared to the vanilla GAN discriminator) is the absence of
    fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator model is quite different to what you saw for vanilla GAN. Here
    we only need the input vector''s dimension to start with. We make use of reshaping
    and up-sampling layers to modify the vector into a two-dimensional image and increase
    its resolution, respectively. Similar to DCGAN''s discriminator, we do not have
    any fully connected layers apart from the input layer, which is reshaped into
    an image. The following code snippet shows how to build a generator model for
    DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: DCGAN output at different stages of training'
  prefs: []
  type: TYPE_NORMAL
- en: The results show how DCGAN is able to generate the required set of outputs in
    fewer training cycles. While it is difficult to make much out of the quality of
    generated images (given the nature of the MNIST dataset), in principle, DCGAN
    should be able to generate better quality output than vanilla GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Vector arithmetic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ability to manipulate the latent vectors by addition, subtraction, and
    so on to generate meaningful output transformations is a powerful tool. The authors
    of the DCGAN paper showed that indeed the *z* representative space of the generator
    obeys such a rich linear structure. Similar to vector arithmetic in the NLP domain,
    where word2vec generates a vector similar to "Queen" upon performing the manipulation
    "King" – "Man" + "Woman," DCGAN allows the same in the visual domain. The following
    is an example from the DCGAN paper (*Figure 6.12*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A group of people posing for the camera  Description automatically generated](img/B16176_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: DCGAN vector arithmetic⁸'
  prefs: []
  type: TYPE_NORMAL
- en: The example shows that we can generate examples of "woman with glasses" by performing
    the simple manipulation of "man with glasses" – "man without glasses" + "woman
    without glasses." This opens up the possibility of generating complex samples
    without the need for huge amounts of training data. Though unlike word2vec, where
    a single vector is sufficient, in this case, we average at least three samples
    to achieve stable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GANs are powerful systems that can generate realistic samples from the domain
    of training. In the previous sections, you saw vanilla GAN and DCGAN generate
    realistic samples from the MNIST dataset. These architectures have also been used
    to generate samples that resemble human faces and even real-world items (from
    training on CIFAR10 and so on). However, they miss out on the ability to control
    the samples we would like to generate.
  prefs: []
  type: TYPE_NORMAL
- en: In simple words, we can use a trained generator to generate any number of samples
    required, yet we cannot control it to generate a specific type of example. **Conditional
    GANs** (**CGANs**) are the class of GANs that provide us with precisely the control
    needed to generate a specific class of examples. Developed by Mirza et al. in
    2014⁹, they are some of the earliest enhancements to the original GAN architecture
    from Goodfellow et al.
  prefs: []
  type: TYPE_NORMAL
- en: CGANs work by training the generator model to generate fake samples conditioned
    on specific characteristics of the output required. The discriminator, on the
    other hand, needs to do some extra work. It needs to learn not only to differentiate
    between fake and real but also to mark out samples as fake if the generated sample
    and its conditioning characteristics do not match.
  prefs: []
  type: TYPE_NORMAL
- en: 'In their work *Conditional Adversarial Networks*, Mirza et al. pointed towards
    using class labels as additional conditioning input to both generator and discriminator
    models. We denote the conditioning input as *y* and transform the value function
    for the GAN minimax game as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_018.png)'
  prefs: []
  type: TYPE_IMG
- en: where *log log D* (*x*|*y*) is the discriminator output for a real sample, *x*,
    conditioned on *y* and similarly *log log* (1 - *D* (*G*(*z*|*y*))) is the discriminator
    output for a fake sample, *G*(*z*), conditioned on *y*. Note that the value function
    is only slightly changed from the original minimax equation for vanilla GAN. Thus,
    we can leverage the improved cost functions for the generator as well as the other
    enhancements we discussed in the previous sections. The conditioning information,
    *y* (the class label, for example), is provided as an additional input to both
    the models and the rest is taken care of by the GAN setup itself. *Figure 6.13*
    shows the architectural setup for a Conditional GAN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure: 6.13 CGAN generator architecture^(10)'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the implementation as close to the original implementation of CGAN as
    possible, we will now develop conditioned generator and discriminator models as
    MLPs. You are encouraged to experiment with DCGAN-like architectures conditioned
    on class labels. Since we would have multiple inputs to each of the constituent
    models, we will make use of the Keras functional API to define our models. We
    will be developing CGAN to generate MNIST digits.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once trained, CGAN can be asked to generate examples of a specific class. *Figure
    6.14* shows the output for different class labels across the training epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: CGAN output at different stages of training'
  prefs: []
  type: TYPE_NORMAL
- en: One major advantage apparent from *Figure 6.14* is the additional control that
    CGANs provide us with. As discussed, using additional inputs, we are able to easily
    control the generator to generate specific digits. This opens up a long list of
    use cases, some of which we will cover in later chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The improved GANs we have covered so far were mostly focused upon architectural
    enhancements to improve results. Two major issues with the GAN setup are the stability
    of the minimax game and the unintuitiveness of the generator loss. These issues
    arise due to the fact that we train the discriminator and generator networks alternatingly
    and at any given moment, the generator loss is indicative of the discriminator's
    performance so far.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GAN (or W-GAN) was an attempt by Arjovsky et al. to overcome some
    of the issues with the GAN setup. This is one of a few deep learning papers that
    are deeply rooted in theoretical foundations to explain the impact of their work
    (apart from empirical results). The main difference between typical GANs and W-GANs
    is the fact that W-GANs treat the discriminator as a critic (deriving from reinforcement
    learning; see *Chapter 11*, *Composing Music with Generative Models*). Hence,
    instead of simply classifying input images as real or fake, the W-GAN discriminator
    (or critic) generates a score to inform the generator about the realness or fakeness
    of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum likelihood game we discussed in the initial sections of the chapter
    explained the task as one where we try to minimize the divergence between *p*[z]
    and *p*[data] using KL divergence, that is, ![](img/B16176_06_019.png). Apart
    from being asymmetric, KL divergence also has issues when the distributions are
    too far off or completely disjointed. To overcome these issues, W-GANs make use
    of **Earth Mover''s** (**EM**) distance or Wasserstein distance. Simply put, EM
    distance is the minimum cost to move or transport mass from distribution *p* to
    *q*. For the GAN setup, we can imagine this as the minimum cost of moving from
    the generator distribution (*p*[z]) to the real distribution (*p*[data]). Mathematically,
    this can be stated as the infimum (or greatest lower bound, denoted as *inf*)
    for any transport plan (denoted as *W*(*source*, *destination*), that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since this is intractable, the authors used Kantorovich-Rubinstein duality
    to simplify the calculations. The simplified form is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_021.png)'
  prefs: []
  type: TYPE_IMG
- en: where *sup* is the supremum or least upper bound and *f* is a 1-Lipschitz function
    that imposes certain constraints. A great many details are required to fully understand
    the details and impact of using Wasserstein distance. You are encouraged to go
    through the paper for an in-depth understanding of the associated concepts or
    refer to [https://vincentherrmann.github.io/blog/wasserstein/](https://vincentherrmann.github.io/blog/wasserstein/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For brevity, we will focus on implementation-level changes that help in achieving
    a stable trainable architecture. A comparison of gradient updates of a GAN and
    W-GAN is shown in *Figure 6.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a map  Description automatically generated](img/B16176_06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: W-GAN versus GAN^(11)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure explains the vanishing gradients in the case of the GAN discriminator
    when the input is bimodal Gaussian, while the W-GAN critic has a smooth gradient
    throughout.
  prefs: []
  type: TYPE_NORMAL
- en: 'To transform this understanding into implementation-level details, the following
    are the key changes in W-GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator is termed as the critic, which generates and outputs a score
    of realness or fakeness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer in the critic/discriminator is a linear layer (instead of sigmoid).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -1 denotes real labels, while 1 denotes fake labels. These are expressed as
    positive and negative critics in the literature. We otherwise use 1 and 0 for
    real and fake labels, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We replace classification loss (binary cross-entropy) with Wasserstein loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critic model is allowed to train for a greater number of cycles compared
    to the generator model. This is done because in the case of W-GANs, a stable critic
    better guides the generator; the gradients are much smoother. The authors trained
    the critic model five times per generator cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of the critic layers are clipped within a range. This is required
    in order to maintain the 1-Lipschitz constraint. The authors used the range of
    -0.01 to 0.01.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp is the recommended optimizer to allow stable training. This is in contrast
    to the usage of Adam as an optimizer for the typical case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these changes, the authors noted a significant improvement in training
    stability and better feedback for the generator. *Figure 6.16* (from the paper)
    shows how the generator takes cues from a stable critic to train better. The results
    improve as the training epochs increase. The authors experimented with both MLP-based
    generators as well as convolutional generators and found similar results.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: W-GAN generator loss and output quality^(12)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we can use any generator and discriminator with minor modifications,
    let''s get to some of the implementation details. First and foremost is the Wasserstein
    loss. We calculate it by taking a mean of the critic score and the ground truth
    labels. This is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The major change for the discriminator is its final layer and its weight clipping.
    While the change of activation function for the last layer is straightforward,
    the weight clipping can be a bit challenging to implement at first. With the Keras
    API, this can be done in two ways: by sub-classing the `Constraint` class and
    using it as an additional argument for all layers or by iterating through the
    layers during the training loop. While the first approach is much cleaner, we''ll
    use the second approach as it is easier to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With these changes, we train our W-GAN to generate MNIST digits. The following
    (*Figure 6.17*) are the output samples during different stages of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: W-GAN output at different stages of training'
  prefs: []
  type: TYPE_NORMAL
- en: The promised stable training backed up by theoretical proofs is not free from
    its own set of issues. Mostly, the issues are due to the constraints of keeping
    the calculations tractable. Some of these concerns were addressed in a recent
    work titled *Improved Training of Wasserstein GAN*^(13) by Gulrajani et al. in
    2017\. This work presented a few tricks, with the most important one being gradient
    penalty (or, as the authors refer to it, W-GAN-GP). You are encouraged to go through
    this interesting work as well to better understand the contributions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered quite a few improvements, let's move towards a slightly
    more complex setup called Progressive GAN. In the next section, we will go through
    the details of this highly effective architecture to generate high-quality outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Progressive GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are powerful systems to generate high-quality samples, examples of which
    we have seen in the previous sections. Different works have utilized this adversarial
    setup to generate samples from different distributions like CIFAR10, celeb_a,
    LSUN-bedrooms, and so on (we covered examples using MNIST for explanation purposes).
    There have been some works that focused on generating higher-resolution output
    samples, like Lap-GANs, but they lacked perceived output quality and presented
    a larger set of challenges for training. Progressive GANs or Pro-GANs or PG-GANs
    were presented by Karras et al. in their work titled *GANs for Improved Quality,
    Stability, and Variation*^(14) at ICLR-2018, as a highly effective method for
    generating high-quality samples.
  prefs: []
  type: TYPE_NORMAL
- en: The method presented in this work not only mitigated many of the challenges
    present in earlier works but also brought about a very simple solution to crack
    this problem of generating high-quality output samples. The paper also presented
    a number of very impactful contributions, some of which we will cover in detail
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The overall method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The software engineering way of solving tough technical problems is often to
    break them down into simpler granular tasks. Pro-GANs also target the complex
    problem of generating high-resolution samples by breaking down the task into smaller
    and simpler problems to solve. The major issue with high-resolution images is
    the huge number of modes or details such images have. It makes it very easy to
    differentiate between generated samples and the real data (perceived quality issues).
    This inherently makes the task of building a generator, with enough capacity to
    train well on such datasets along with memory requirements, a very tough one.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle these issues, Karras et al. presented a method to grow both generator
    and discriminator models as the training progresses from lower to higher resolutions
    gradually. This is shown in *Figure 6.18*. They noted that this progressive growth
    of models has various advantages, such as the ability to generate high-quality
    samples, faster training, and lesser memory requirements (compared to directly
    training a GAN to generate high-resolution output).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Progressively increasing the resolution for discriminator and
    generator models^(15)'
  prefs: []
  type: TYPE_NORMAL
- en: Generating higher-resolution images step by step was not an entirely new idea.
    A lot of prior works used similar techniques, yet the authors pointed out that
    their work was most similar to the layer-wise training of autoencoders^(16).
  prefs: []
  type: TYPE_NORMAL
- en: The system learns by starting with lower-resolution samples and a generator-discriminator
    set up as mirror images of each other (architecture-wise). At a lower resolution
    (say 4x4), the training is much simpler and stable as there are fewer modes to
    learn. We then increase the resolution step by step by introducing additional
    layers for both models. This step-by-step increase in resolution limits the complexity
    of the task at hand rather than forcing the generator to learn all modes at once.
    This finally enables Pro-GANs to generate megapixel-size outputs, which all start
    from a very low-resolution initial point.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will cover the important contributions and
    implementation-level details. Note that the training time and compute requirements,
    despite improvements, for Pro-GANs are huge. The authors mentioned a training
    time of up to a week on multiple GPUs to generate said megapixel outputs. Keeping
    the requirements in check, we will cover component-level details but use TensorFlow
    Hub to present the trained model (instead of training one from scratch). This
    will enable us to focus on the important details and leverage pre-built blocks
    as required.
  prefs: []
  type: TYPE_NORMAL
- en: Progressive growth-smooth fade-in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pro-GANs were introduced as networks that increase the resolution step by step
    by adding additional layers to generator and discriminator models. But how does
    that actually work? The following is a step-by-step explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator and discriminator models start with a resolution of 4x4 each.
    Both networks perform their designated tasks of generating and discriminating
    the pre-scaled samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train these models for a number of epochs until the performance saturates.
    At this point, additional layers are added to both networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator gets an additional upscaling layer to generate 8x8 samples while
    the discriminator gets an additional downscaling layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The move from one step to the next (that is, from 4x4 to 8x8) is done gradually
    using an overlay factor, ![](img/B16176_06_022.png). *Figure 6.19* shows the transition
    pictorially.![A screenshot of a cell phone  Description automatically generated](img/B16176_06_19.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.19: Smooth fade-in^(17)'
  prefs: []
  type: TYPE_NORMAL
- en: The existing layers are upscaled and transitioned with a factor of 1-![](img/B16176_06_023.png),
    while the newly added layer is multiplied with a factor of ![](img/B16176_06_023.png).
    The value of ![](img/B16176_06_023.png) ranges between 0 and 1, which is gradually
    increased from 0 towards 1 to increase the contribution from the newly added layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same process is followed for the discriminator, where the transition moves
    it gradually from the existing setup to newly added layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that all layers are trained (existing upscaled and newly
    added ones) throughout the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors started from a 4x4 resolution and, step by step, increased it to
    finally take it to megapixel levels.
  prefs: []
  type: TYPE_NORMAL
- en: Minibatch standard deviation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Existing approaches rely on normalization techniques such as batch normalization,
    virtual normalization, and so on. These techniques use trainable parameters to
    compute minibatch-level statistics in order to maintain similarity across samples.
    Apart from adding additional parameters and compute load, these normalization
    methods do not completely alleviate issues.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of Pro-GAN introduced a simplified solution that does not require
    any trainable parameters. The proposed minibatch standard deviation method was
    introduced to improve the diversity of minibatches. From the last layer of the
    discriminator, the method computes the standard deviation of each spatial location
    (pixel location *x*, *y*). For a given batch of size *B* with images shaped *H*
    x *W* x *C* (height, width, and channels), a total of *B * H * W * C* standard
    deviations are calculated. The next step involves averaging these standard deviations
    and concatenating them to the layer's output. This is designed to be the same
    for each example in the minibatch.
  prefs: []
  type: TYPE_NORMAL
- en: Equalized learning rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors of Pro-GAN briefly mentioned that they focused on simpler weight
    initialization methods compared to the current trend of identifying custom initialization
    methods. They used an *N*(0,1) standard normal distribution for the initialization
    of weights and then explicitly scaled at runtime. The scaling was performed as
    ![](img/B16176_06_026.png), where *c* is the per-layer normalization constant
    from the *He's* initializer. They also pointed out issues with momentum-based
    optimizers such as Adam and RMSProp that get mitigated with this equalized learning
    rate method.
  prefs: []
  type: TYPE_NORMAL
- en: Pixelwise normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The enhancements mentioned so far either focus on the discriminator or the
    overall GAN training. This normalization technique is applied to the generator
    model. The authors pointed out that this method helps to prevent instability in
    the training process along with mode-collapse issues. As the name suggests, they
    proposed the application of the normalization per spatial location (or per pixel,
    denoted as (*x*, *y*)). The normalization equation is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_027.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B16176_06_028.png), *N* is the number of feature maps, and *a*
    and *b* are the original and normalized feature vectors, respectively. This strange-looking
    normalization equation helps in preventing huge random changes in magnitude effectively.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, despite the long list of effective contributions, Pro-GANs
    require huge amounts of compute to generate quality results. The official implementation
    on GitHub^(18) mentions a training time of two weeks on a single GPU for the CelebA-HQ
    dataset. This is beyond the time and effort available for most people. The following
    (*Figure 6.20*) is a snapshot of the generator and discriminator model architectures;
    each of them has about 23 million parameters!
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a piece of paper  Description automatically generated](img/B16176_06_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Generator and discriminator model summary^(19)'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we will focus on the pretrained Pro-GAN model available through TensorFlow
    Hub. TensorFlow Hub is a repository of a large number of deep learning models
    that can be easily downloaded and used for various downstream tasks using the
    TensorFlow ecosystem. The following is a miniature example to showcase how we
    can use the Pro-GAN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the required libraries. With TensorFlow Hub, the
    only additional `import` required is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We used TensorFlow Hub version 0.12.0 with TensorFlow version 2.4.1\. Make
    sure your versions are in sync otherwise there might be slight changes with respect
    to syntax. The next step is to load the model. We set a seed for our TensorFlow
    session to ensure the reproducibility of results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading a pretrained model using TensorFlow Hub is as simple as the preceding
    code. The next step is about randomly sampling a latent vector (*z*) from a normal
    distribution. The model requires the latent vector to be of size 512\. Once we
    have the latent vector, we pass it to our generator to get the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a sample face generated from the pretrained Pro-GAN model
    (*Figure 6.21*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person smiling for the camera  Description automatically generated](img/B16176_06_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: Sample face using pretrained Pro-GAN from TF Hub'
  prefs: []
  type: TYPE_NORMAL
- en: 'We wrote a simple sampling function, similar to one we have been using throughout
    the chapter to generate a few additional faces. This additional experiment helps
    us to understand the diversity of human faces this model has been able to capture
    and, of course, its triumph over issues such as mode collapse (more on this in
    the next section). The following image (*Figure 6.22*) is a sample of 25 such
    faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, food, different  Description automatically generated](img/B16176_06_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: 25 faces generated using Pro-GAN'
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious, TensorFlow Hub provides a training mechanism to train such
    models from scratch. Also, the Pro-GAN authors have open-sourced their implementation.
    You are encouraged to go through it.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a lot of ground to understand different architectures and their
    ability to generate images. In the next section, we will cover some of the challenges
    associated with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs provide an alternative method of developing generative models. Their design
    inherently helps in mitigating the issues we discussed with some of the other
    techniques. However, GANs are not free from their own set of issues. The choice
    to develop models using concepts of game theory is fascinating yet difficult to
    control. We have two agents/models trying to optimize opposing objectives, which
    can lead to all sorts of issues. Some of the most common challenges associated
    with GANs are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Training instability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GANs play a minimax game with opposing objectives. No wonder this leads to oscillating
    losses for generator and discriminator models across batches. A GAN setup that
    is training well will typically have a higher variation in losses initially but,
    eventually, it will stabilize and so will the loss of the two competing models.
    Yet it is very common for GANs (especially vanilla GANs) to spiral out of control.
    It is difficult to determine when to stop the training or to estimate an equilibrium
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Mode collapse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mode collapse refers to a failure state where the generator finds one or only
    a small number of samples that are enough to fool the discriminator. To understand
    this better, let''s take the example of a hypothetical dataset of temperatures
    from two cities, city **A** and city **B**. Let''s also assume city **A** is at
    a higher altitude and remains cold mostly while city **B** is near the equator
    and has high temperatures. Such a dataset might have a temperature distribution
    as shown in *Figure 6.23*. The distribution is bimodal, that is, it has two peaks:
    one for city **A** and one for city **B** (due to their different weather conditions).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a lamp  Description automatically generated](img/B16176_06_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Bimodal distribution of the temperatures of two cities'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our dataset, assume we are tasked with training a GAN that
    can mimic this distribution. In the perfect scenario, we will have the GAN generate
    samples of temperatures from city **A** and city **B** with roughly equal probability.
    However, a commonly occurring issue is that of mode collapse: the generator ends
    up generating samples only from a single mode (say, only city **B**). This happens
    when:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator learns to fool the discriminator by generating realistic-looking
    samples from city **B** only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminator tries to counter this by learning that all outputs for city
    **A** are real and tries to classify samples from city **B** as real or fake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator then flips to city **A**, abandoning the mode for city **B**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminator now assumes all samples for city **B** are real and tries
    to classify samples for city **A** instead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This cycle keeps on repeating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This cycle repeats as the generator is never incentivized enough to cover both
    modes. This limits the usefulness of the generator as it exhibits poor diversity
    of output samples. In a real-world setting, mode collapse varies from complete
    collapse (that is, all generated samples are identical) to partial collapse (that
    is, a few modes are captured).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have trained different GAN architectures in the chapter so far. The MNIST
    dataset is also multimodal in nature. A complete collapse for such a dataset would
    result in the GAN generating only a single digit as output, while a partial collapse
    would mean only a few digits were generated (out of 10). *Figure 6.24* shows the
    two scenarios for vanilla GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing crossword, drawing, clock  Description automatically
    generated](img/B16176_06_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Failure mode for GAN - mode collapse'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.24* shows how mode collapse can lead to limited diversity in the
    samples that a GAN can generate.'
  prefs: []
  type: TYPE_NORMAL
- en: Uninformative loss and evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks train using gradient descent and improve upon the loss values.
    Yet in the case of GANs (except W-GAN and related architectures), the loss values
    are mostly uninformative. We would assume that as training progresses, the generator
    loss would keep on decreasing while the discriminator would hit a saddle point,
    but this is not the case. The main reason is the alternate training cycles for
    generator and discriminator models. The generator loss at any given point is compared
    against the discriminator trained so far, thus making it difficult to compare
    the generator's performance across training epochs. You should note that in the
    case of W-GAN, critic loss in particular is the guiding signal for improving the
    generator model.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these issues, GANs also need a strict evaluation metric to understand
    the output quality of the samples. Inception score is one such way of calculating
    the output quality, yet there is scope for identifying better evaluation metrics
    in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to a new class of generative models called
    Generative Adversarial Networks. Inspired by concepts of game theory, GANs present
    an implicit method of modeling the data generation probability density. We started
    the chapter by first placing GANs in the overall taxonomy of generative models
    and comparing how these are different from some of the other methods we have covered
    in earlier chapters. Then we moved onto understanding the finer details of how
    GANs actually work by covering the value function for the minimax game, as well
    as a few variants like the non-saturating generator loss and the maximum likelihood
    game. We developed a multi-layer-perceptron-based vanilla GAN to generate MNIST
    digits using TensorFlow Keras APIs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we touched upon a few improved GANs in the form of Deep
    Convolutional GANs, Conditional GANs, and finally, Wasserstein GANs. We not only
    explored major contributions and enhancements, but also built some code bases
    to train these improved versions. The next section involved an advanced variant
    called Progressive GANs. We went through the nitty-gritty details of this advanced
    setup and used a pretrained model to generate fake faces. In the final section,
    we discussed a few common challenges associated with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was the foundation required before we jump into some even more
    advanced architectures in the upcoming chapters. We will cover additional topics
    in the computer vision space such as style transfer methods, face-swap/deep-fakes,
    and so on. We will also cover topics in domains such as text and audio. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Samples: [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)
    (left) and [https://thisartworkdoesnotexist.com/](https://thisartworkdoesnotexist.com/)
    (right)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adapted from *Ian Goodfellow, Tutorial on Generative Adversarial Networks, 2017*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adapted from lecture 13 CS231: [http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Radford, A., Metz, L., Chintala, S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv:1511.06434\. [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Radford, A., Metz, L., Chintala, S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv:1511.06434\. [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mirza, M., Osindero, S. (2014). *Conditional Generative Adversarial Nets*. arXiv:1411.1784\.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mirza, M., Osindero, S. (2014). *Conditional Generative Adversarial Nets*. arXiv:1411.1784\.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*. arXiv:1701.07875\.
    [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*. arXiv:1701.07875\.
    [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gulrajani, I., Ahmed, F., Arjovsky, M., Courville, A. (2017). *Improved Training
    of Wasserstein GANs*. arXiv:1704.00028\. [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). "*Progressive Growing
    of GANs for Improved Quality, Stability, and Variation*". arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bengio Y., Lamblin P., Popovici D., Larochelle H. (2006). *Greedy Layer-Wise
    Training of Deep Networks*. In Proceedings of the 19th International Conference
    on Neural Information Processing Systems (NIPS'06). MIT Press, Cambridge, MA,
    USA, 153–160\. [https://dl.acm.org/doi/10.5555/2976456.2976476](https://dl.acm.org/doi/10.5555/2976456.2976476)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Progressive GAN official implementation: [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
