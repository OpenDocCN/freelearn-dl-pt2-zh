- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Image Generation with GANs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN生成图像
- en: Generative modeling is a powerful concept that provides us with immense potential
    to approximate or model underlying processes that generate data. In the previous
    chapters, we covered concepts associated with deep learning in general and more
    specifically related to **restricted Boltzmann machines** (**RBMs**) and **variational
    autoencoders** (**VAEs**). This chapter will introduce another family of generative
    models called **Generative Adversarial Networks** (**GANs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模是一个强大的概念，它为我们提供了巨大的潜力来逼近或建模生成数据的基本过程。在前几章中，我们涵盖了与深度学习一般以及更具体地与**受限玻尔兹曼机**（**RBMs**）和**变分自编码器**（**VAEs**）相关的概念。本章将介绍另一类生成模型家族，称为**生成对抗网络**（**GANs**）。
- en: Heavily inspired by the concepts of game theory and picking up some of the best
    components from previously discussed techniques, GANs provide a powerful framework
    for working in the generative modeling space. Since their invention in 2014 by
    Goodfellow et al., GANs have benefitted from tremendous research and are now being
    used to explore creative domains such as art, fashion, and photography.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在受博弈理论概念的强烈启发并吸取了先前讨论的一些最佳组件的基础上，GANs为在生成建模空间中工作提供了一个强大的框架。自从它们于2014年由Goodfellow等人发明以来，GANs受益于巨大的研究，并且现在被用于探索艺术、时尚和摄影等创造性领域。
- en: The following are two amazing high-quality samples from a variant of GANs called
    StyleGAN (*Figure 6.1*). The photograph of the kid is actually a fictional person
    who does not exist. The art sample is also generated by a similar network. StyleGANs
    are able to generate high-quality sharp images by using the concept of progressive
    growth (we will cover this in detail in later sections). These outputs were generated
    using the StyleGAN2 model trained on datasets such as the **Flickr-Faces-HQ**
    or **FFHQ** dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是GANs的一个变体（StyleGAN）的两个惊人高质量样本（*图6.1*）。儿童的照片实际上是一个虚构的不存在的人物。艺术样本也是由类似的网络生成的。通过使用渐进增长的概念，StyleGANs能够生成高质量清晰的图像（我们将在后面的部分中详细介绍）。这些输出是使用在数据集上训练的StyleGAN2模型生成的，如**Flickr-Faces-HQ**或**FFHQ**数据集。
- en: '![](img/B16176_06_01.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_01.png)'
- en: 'Figure 6.1: Imagined by a GAN (StyleGAN2) (Dec 2019) - Karras et al. and Nvidia²'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：GAN（StyleGAN2）想象出的图像（2019年12月）- Karras等人和Nvidia²
- en: 'This chapter will cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖：
- en: The taxonomy of generative models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型的分类
- en: A number of improved GANs, such as DCGAN, Conditional-GAN, and so on
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些改进的GAN，如DCGAN、条件-GAN等。
- en: The progressive GAN setup and its various components
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渐进式GAN设定及其各个组件
- en: Some of the challenges associated with GANs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与GANs相关的一些挑战
- en: Hands-on examples
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例操作
- en: The taxonomy of generative models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型的分类
- en: Generative models are a class of models in the unsupervised machine learning
    space. They help us to model the underlying distributions responsible for generating
    the dataset under consideration. There are different methods/frameworks to work
    with generative models. The first set of methods correspond to models that represent
    data with an explicit density function. Here we define a probability density function,
    ![](img/B16176_06_001.png), explicitly and develop a model that increases the
    maximum likelihood of sampling from this distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是无监督机器学习领域的一类模型。它们帮助我们对生成数据集的潜在分布进行建模。有不同的方法/框架可以用来处理生成模型。第一组方法对应于用显式密度函数表示数据的模型。在这里，我们明确定义一个概率密度函数，![](img/B16176_06_001.png)，并开发一个增加从这个分布中采样的最大似然的模型。
- en: 'There are two further types within explicit density methods, *tractable* and
    *approximate* density methods. PixelRNNs are an active area of research for tractable
    density methods. When we try to model complex real-world data distributions, for
    example, natural images or speech signals, defining a parametric function becomes
    challenging. To overcome this, you learned about RBMs and VAEs in *Chapter 4*,
    *Teaching Networks to Generate Digits*, and *Chapter 5*, *Painting Pictures with
    Neural Networks Using VAEs*, respectively. These techniques work by approximating
    the underlying probability density functions explicitly. VAEs work towards maximizing
    the likelihood estimates of the lower bound, while RBMs use Markov chains to make
    an estimate of the distribution. The overall landscape of generative models can
    be described using *Figure 6.2*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在显式密度方法中，还有两种进一步的类型，即*可计算*和*近似*密度方法。 PixelRNNs 是可计算密度方法的一个活跃研究领域。当我们试图对复杂的现实世界数据分布建模时，例如自然图像或语音信号，定义参数函数变得具有挑战性。为了克服这一点，你在第四章“教网络生成数字”和第五章“使用VAEs绘制图像”中分别学习了关于
    RBMs 和 VAEs。这些技术通过明确地逼近概率密度函数来工作。VAEs 通过最大化下界的似然估计来工作，而 RBMs 则使用马尔可夫链来估计分布。生成模型的整体景观可以使用*图6.2*来描述：
- en: '![Diagram  Description automatically generated](img/B16176_06_02.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B16176_06_02.png)'
- en: 'Figure 6.2: The taxonomy of generative models³'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：生成模型的分类³
- en: GANs fall under implicit density modeling methods. The implicit density functions
    give up the property of explicitly defining the underlying distribution but work
    by defining methods to draw samples from such distributions. The GAN framework
    is a class of methods that can sample directly from the underlying distributions.
    This alleviates some of the complexities associated with the methods we have covered
    so far, such as defining underlying probability distribution functions and the
    quality of outputs. Now that you have a high-level understanding of generative
    models, let's dive deeper into the details of GANs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 属于隐式密度建模方法。隐式密度函数放弃了明确定义潜在分布的属性，但通过定义方法来从这些分布中抽样的方法来工作。 GAN 框架是一类可以直接从潜在分布中抽样的方法。这减轻了到目前为止我们所涵盖的方法的某些复杂性，例如定义底层概率分布函数和输出的质量。现在你已经对生成模型有了高层次的理解，让我们深入了解GAN的细节。
- en: Generative adversarial networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: GANs have a pretty interesting origin story. It all began as a discussion/argument
    in a bar with Ian Goodfellow and friends discussing work related to generating
    data using neural networks. The argument ended with everyone downplaying each
    other's methods. Goodfellow went back home and coded the first version of what
    we now call a GAN. To his amazement, the code worked on the first try. A more
    verbose description of the chain of events was shared by Goodfellow himself in
    an interview with *Wired* magazine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 有一个非常有趣的起源故事。一切始于在酒吧里讨论/争论与伊恩·古德费洛和朋友们讨论使用神经网络生成数据相关的工作。争论以每个人都贬低彼此的方法而告终。古德费洛回家编写了现在我们称之为
    GAN 的第一个版本的代码。令他惊讶的是，代码第一次尝试就成功了。古德费洛本人在接受《连线》杂志采访时分享了一份更详细的事件链描述。
- en: As mentioned, GANs are implicit density functions that sample directly from
    the underlying distribution. They do this by defining a two-player game of adversaries.
    The adversaries compete against each other under well-defined reward functions
    and each player tries to maximize its rewards. Without going into the details
    of game theory, the framework can be explained as follows.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GANs 是隐式密度函数，直接从底层分布中抽样。它们通过定义一个双方对抗的两人游戏来实现这一点。对抗者在定义良好的奖励函数下相互竞争，每个玩家都试图最大化其奖励。不涉及博弈论的细节，该框架可以解释如下。
- en: The discriminator model
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别器模型
- en: This model represents a differentiable function that tries to maximize a probability
    of 1 for samples drawn from the training distribution. This can be any classification
    model, but we usually prefer a deep neural network for this. This is the throw-away
    model (similar to the decoder part of autoencoders).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型代表了一个可微分函数，试图最大化从训练分布中抽样得到样本的概率为1。这可以是任何分类模型，但我们通常偏爱使用深度神经网络。这是一种一次性模型（类似于自动编码器的解码器部分）。
- en: The discriminator is also used to classify whether the output from the generator
    is real or fake. The main utility of this model is to help develop a robust generator.
    We denote the discriminator model as *D* and its output as *D*(*x*). When it is
    used to classify output from the generator model, the discriminator model is denoted
    as *D*(*G*(*z*)), where *G*(*z*) is the output from the generator model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器也用于分类生成器输出是真实的还是假的。这个模型的主要作用是帮助开发一个强大的生成器。我们将鉴别器模型表示为*D*，其输出为*D*(*x*)。当它用于分类生成器模型的输出时，鉴别器模型被表示为*D*(*G*(*z*))，其中*G*(*z*)是生成器模型的输出。
- en: '![Text, application, chat or text message  Description automatically generated](img/B16176_06_03_new.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![文本，应用程序，聊天或短信说明自动生成](img/B16176_06_03_new.png)'
- en: 'Figure 6.3: The discriminator model'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：鉴别器模型
- en: The generator model
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器模型
- en: This is the primary model of interest in the whole game. This model generates
    samples that are intended to resemble the samples from our training set. The model
    takes random unstructured noise as input (typically denoted as *z*) and tries
    to create a varied set of outputs. The generator model is usually a differentiable
    function; it is often represented by a deep neural network but is not restricted
    to that.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是整个游戏中主要关注的模型。这个模型生成样本，意图是与我们的训练集中的样本相似。该模型将随机的非结构化噪声作为输入（通常表示为*z*），并尝试创建各种输出。生成器模型通常是一个可微分函数；它通常由深度神经网络表示，但不局限于此。
- en: We denote the generator as *G* and its output as *G*(*z*). We typically use
    a lower-dimensional *z* as compared to the dimension of the original data, *x*,
    that is, ![](img/B16176_06_002.png). This is done as a way of compressing or encoding
    real-world information into lower-dimensional space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成器表示为*G*，其输出为*G*(*z*)。通常相对于原始数据*x*的维度，我们使用一个较低维度的*z*，即![](img/B16176_06_002.png)。这是对真实世界信息进行压缩或编码的一种方式。
- en: '![Text  Description automatically generated](img/B16176_06_04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![文本说明自动生成](img/B16176_06_04.png)'
- en: 'Figure 6.4: The generator model'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：生成器模型
- en: In simple words, the generator trains to generate samples good enough to fool
    the discriminator, while the discriminator trains to properly classify real (training
    samples) versus fake (output from the generator). Thus, this game of adversaries
    uses a generator model, *G*, which tries to make *D*(*G*(*z*)) as close to 1 as
    possible. The discriminator is incentivized to make *D*(*G*(*z*)) close to 0,
    where 1 denotes real and 0 denotes fake samples. The GAN model achieves equilibrium
    when the generator starts to easily fool the discriminator, that is, the discriminator
    reaches its saddle point. While, in theory, GANs have several advantages over
    other methods in the family tree described previously, they pose their own set
    of problems. We will discuss some of them in the upcoming sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，生成器的训练是为了生成足够好的样本以欺骗鉴别器，而鉴别器的训练是为了正确地区分真实（训练样本）与假的（输出自生成器）。因此，这种对抗游戏使用一个生成器模型*G*，试图使*D*(*G*(*z*))尽可能接近1。而鉴别器被激励让*D*(*G*(*z*))接近0，其中1表示真实样本，0表示假样本。当生成器开始轻松地欺骗鉴别器时，生成对抗网络模型达到均衡，即鉴别器达到鞍点。虽然从理论上讲，生成对抗网络相对于之前描述的其他方法有几个优点，但它们也存在自己的一系列问题。我们将在接下来的章节中讨论其中一些问题。
- en: Training GANs
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练生成对抗网络
- en: 'Training a GAN is like playing this game of two adversaries. The generator
    is learning to generate good enough fake samples, while the discriminator is working
    hard to discriminate between real and fake. More formally, this is termed as the
    minimax game, where the value function *V*(*G*, *D*) is described as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成对抗网络就像玩这个两个对手的游戏。生成器正在学习生成足够好的假样本，而鉴别器正在努力区分真实和假的。更正式地说，这被称为极小极大游戏，其中价值函数*V*(*G*,
    *D*)描述如下：
- en: '![](img/B16176_06_003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_003.png)'
- en: 'This is also called the zero-sum game, which has an equilibrium that is the
    same as the Nash equilibrium. We can better understand the value function *V*(*G*,
    *D*) by separating out the objective function for each of the players. The following
    equations describe individual objective functions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为零和游戏，其均衡点与纳什均衡相同。我们可以通过分离每个玩家的目标函数来更好地理解价值函数*V*(*G*, *D*)。以下方程描述了各自的目标函数：
- en: '![](img/B16176_06_004.png)![](img/B16176_06_005.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_004.png)![](img/B16176_06_005.png)'
- en: where ![](img/B16176_06_006.png)is the discriminator objective function in the
    classical sense, ![](img/B16176_06_007.png)is the generator objective equal to
    the negative of the discriminator, and ![](img/B16176_06_008.png) is the distribution
    of the training data. The rest of the terms have their usual meaning. This is
    one of the simplest ways of defining the game or corresponding objective functions.
    Over the years, different ways have been studied, some of which we will cover
    in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B16176_06_006.png)是传统意义上的鉴别器目标函数，![](img/B16176_06_007.png)是生成器目标等于鉴别器的负值，![](img/B16176_06_008.png)是训练数据的分布。其余项具有其通常的含义。这是定义博弈或相应目标函数的最简单方式之一。多年来，已经研究了不同的方式，其中一些我们将在本章中讨论。
- en: 'The objective functions help us to understand the aim of each of the players.
    If we assume both probability densities are non-zero everywhere, we can get the
    optimal value of *D*(*x*) as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数帮助我们理解每个参与者的目标。如果我们假设两个概率密度在任何地方都非零，我们可以得到*D*(*x*)的最优值为：
- en: '![](img/B16176_06_009.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_009.png)'
- en: We will revisit this equation in the latter part of the chapter. For now, the
    next step is to present a training algorithm wherein the discriminator and generator
    models train towards their respective objectives. The simplest yet widely used
    way of training a GAN (and by far the most successful one) is as follows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面重新讨论这个方程。现在，下一步是提出一个训练算法，其中鉴别器和生成器模型分别朝着各自的目标进行训练。训练 GAN 的最简单但也是最广泛使用的方法（迄今为止最成功的方法）如下。
- en: 'Repeat the following steps *N* times. *N* is the number of total iterations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重复以下步骤*N*次。*N*是总迭代次数：
- en: 'Repeat steps *k* times:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*k*次步骤：
- en: 'Sample a minibatch of size *m* from the generator: *{z*[1]*, z*[2]*, … z*[m]*}
    = p*[model]*(z)*'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从生成器中抽取大小为*m*的小批量：*{z*[1]*, z*[2]*, … z*[m]*} = p*[model]*(z)*
- en: 'Sample a minibatch of size m from the actual data: *{x*[1]*, x*[2]*, … x*[m]*}
    = p*[data]*(x)*'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实际数据中抽取大小为*m*的小批量：*{x*[1]*, x*[2]*, … x*[m]*} = p*[data]*(x)*
- en: Update the discriminator loss, ![](img/B16176_06_010.png)
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新鉴别器损失，![](img/B16176_06_010.png)
- en: Set the discriminator as non-trainable
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鉴别器设为不可训练
- en: 'Sample a minibatch of size *m* from the generator: *{z*[1]*, z*[2]*, … z*[m]*}
    = p*[model]*(z)*'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从生成器中抽取大小为*m*的小批量：*{z*[1]*, z*[2]*, … z*[m]*} = p*[model]*(z)*
- en: Update the generator loss, ![](img/B16176_06_011.png)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新生成器损失，![](img/B16176_06_011.png)
- en: In their original paper, Goodfellow et al. used *k* =1, that is, they trained
    discriminator and generator models alternately. There are some variants and hacks
    where it is observed that training the discriminator more often than the generator
    helps with better convergence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的原始论文中，Goodfellow 等人使用了*k*=1，也就是说，他们交替训练鉴别器和生成器模型。有一些变种和技巧，观察到更频繁地训练鉴别器比生成器有助于更好地收敛。
- en: 'The following figure (*Figure 6.5*) showcases the training phases of the generator
    and discriminator models. The smaller dotted line is the discriminator model,
    the solid line is the generator model, and the larger dotted line is the actual
    training data. The vertical lines at the bottom demonstrate the sampling of data
    points from the distribution of *z*, that is, *x* = *p*[model]*(z)*. The lines
    point to the fact that the generator contracts in the regions of high density
    and expands in the regions of low density. Part **(a)** shows the initial stages
    of the training phase where the discriminator *(D)* is a partially correct classifier.
    Parts **(b)** and **(c)** show how improvements in *D* guide changes in the generator,
    *G*. Finally, in part **(d)** you can see where *p*[model] *= p*[data] and the
    discriminator is no longer able to differentiate between fake and real samples,
    that is, ![](img/B16176_06_012.png):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图（*图6.5*）展示了生成器和鉴别器模型的训练阶段。较小的虚线是鉴别器模型，实线是生成器模型，较大的虚线是实际训练数据。底部的垂直线示意从*z*的分布中抽取数据点，即*x*=*p*[model]*(z)*。线指出了生成器在高密度区域收缩而在低密度区域扩张的事实。部分**(a)**展示了训练阶段的初始阶段，此时鉴别器*(D)*是部分正确的分类器。部分**(b)**和**(c)**展示了*D*的改进如何引导*G*的变化。最后，在部分**(d)**中，你可以看到*p*[model]*
    = p*[data]，鉴别器不再能区分假样本和真样本，即![](img/B16176_06_012.png)：
- en: '![A close up of a map  Description automatically generated](img/B16176_06_05.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of a map  Description automatically generated](img/B16176_06_05.png)'
- en: 'Figure 6.5: The training process for GAN⁴'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：GAN的训练过程⁴
- en: Non-saturating generator cost
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非饱和生成器成本
- en: 'In practice, we do not train the generator to minimize *log*(*1* – *D*(*G*(*z*)))
    as this function does not provide sufficient gradients for learning. During the
    initial learning phases, where *G* is poor, the discriminator is able to classify
    the fake from the real with high confidence. This leads to the saturation of *log*(*1*
    – *D*(*G*(*z*))), which hinders improvements in the generator model. We thus tweak
    the generator to maximize *log*(*D*(*G*(*z*))) instead:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们不训练生成器最小化*log*(*1* – *D*(*G*(*z*)))，因为该函数不能提供足够的梯度用于学习。在*G*表现较差的初始学习阶段，鉴别器能够以高置信度区分假的和真实的。这导致*log*(*1*
    – *D*(*G*(*z*)))饱和，阻碍了生成模型的改进。因此，我们调整生成器改为最大化*log*(*D*(*G*(*z*)))：
- en: '![](img/B16176_06_0131.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_0131.png)'
- en: This provides stronger gradients for the generator to learn. This is shown in
    *Figure 6.6*. The *x*-axis denotes *D*(*G*(*z*)). The top line shows the objective,
    which is minimizing the likelihood of the discriminator being correct. The bottom
    line (updated objective) works by maximizing the likelihood of the discriminator
    being wrong.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这为生成器提供了更强的梯度进行学习。这在*图6.6*中显示。*x*轴表示*D*(*G*(*z*))。顶部线显示目标，即最小化鉴别器正确的可能性。底部线（更新的目标）通过最大化鉴别器错误的可能性来工作。
- en: '![A close up of a map  Description automatically generated](img/B16176_06_06.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![地图特写 由计算机自动生成的描述](img/B16176_06_06.png)'
- en: 'Figure 6.6: Generator objective functions⁵'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：生成器目标函数⁵
- en: '*Figure 6.6* illustrates how a slight change helps achieve better gradients
    during the initial phases of training.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6*说明了在训练的初始阶段，轻微的变化如何帮助实现更好的梯度。'
- en: Maximum likelihood game
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大似然游戏
- en: 'The minimax game can be transformed into a maximum likelihood game where the
    aim is to maximize the likelihood of the generator probability density. This is
    done to ensure that the generator probability density is similar to the real/training
    data probability density. In other words, the game can be transformed into minimizing
    the divergence between *p*[z] and *p*[data]. To do so, we make use of **Kullback-Leibler
    divergence** (**KL divergence**) to calculate the similarity between two distributions
    of interest. The overall value function can be denoted as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 极小极大游戏可以转化为最大似然游戏，其目的是最大化生成器概率密度的可能性。这是为了确保生成器的概率密度类似于真实/训练数据的概率密度。换句话说，游戏可以转化为最小化*p*[z]和*p*[data]之间的离散度。为此，我们利用**Kullback-Leibler
    散度**（**KL 散度**）来计算两个感兴趣的分布之间的相似性。总的价值函数可以表示为：
- en: '![](img/B16176_06_014.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_014.png)'
- en: 'The cost function for the generator transforms to:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的成本函数转化为：
- en: '![](img/B16176_06_015.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_015.png)'
- en: One important point to note is that KL divergence is not a symmetric measure,
    that is, ![](img/B16176_06_016.png). The model typically uses ![](img/B16176_06_017.png)
    to achieve better results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点是KL散度不是对称度量，也就是说，![](img/B16176_06_016.png)。模型通常使用![](img/B16176_06_017.png)来获得更好的结果。
- en: 'The three different cost functions discussed so far have slightly different
    trajectories and thus lead to different properties at different stages of training.
    These three functions can be visualized as shown in *Figure 6.7*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止讨论过的三种不同成本函数具有略有不同的轨迹，因此在训练的不同阶段具有不同的特性。这三个函数可以如*图6.7*所示可视化：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_07.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图 由计算机自动生成的描述](img/B16176_06_07.png)'
- en: 'Figure 6.7: Generator cost functions⁶'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：生成器成本函数⁶
- en: Vanilla GAN
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本 GAN
- en: We have covered quite a bit of ground in understanding the basics of GANs. In
    this section, we will apply that understanding and build a GAN from scratch. This
    generative model will consist of a repeating block architecture, similar to the
    one presented in the original paper. We will try to replicate the task of generating
    MNIST digits using our network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在理解 GAN 的基础知识方面取得了相当大的进展。在本节中，我们将应用这些理解，从头开始构建一个 GAN。这个生成模型将由一个重复的块结构组成，类似于原始论文中提出的模型。我们将尝试使用我们的网络复制生成
    MNIST 数字的任务。
- en: The overall GAN setup can be seen in *Figure 6.8*. The figure outlines a generator
    model with noise vector *z* as input and repeating blocks that transform and scale
    up the vector to the required dimensions. Each block consists of a dense layer
    followed by Leaky ReLU activation and a batch-normalization layer. We simply reshape
    the output from the final block to transform it into the required output image
    size.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 整体的GAN设置可以在*图6.8*中看到。图中概述了一个以噪声向量*z*作为输入的生成器模型，并且利用重复块来转换和扩展向量以达到所需的尺寸。每个块由一个密集层后跟一个Leaky
    ReLU激活和一个批量归一化层组成。我们简单地将最终块的输出重新塑造以将其转换为所需的输出图像大小。
- en: The discriminator, on the other hand, is a simple feedforward network. This
    model takes an image as input (a real image or the fake output from the generator)
    and classifies it as real or fake. This simple setup of two competing models helps
    us to train the overall GAN.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，判别器是一个简单的前馈网络。该模型以图像作为输入（真实图像或来自生成器的伪造输出）并将其分类为真实或伪造。这两个竞争模型的简单设置有助于我们训练整体的GAN。
- en: '![](img/B16176_06_08.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_08.png)'
- en: 'Figure 6.8: Vanilla GAN architecture'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：Vanilla GAN架构
- en: 'We will be relying on TensorFlow 2 and using the high-level Keras API wherever
    possible. The first step is to define the discriminator model. In this implementation,
    we will use a very basic **multi-layer perceptron** (**MLP**) as the discriminator
    model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依赖于TensorFlow 2并尽可能使用高级Keras API。第一步是定义判别器模型。在此实现中，我们将使用一个非常基本的**多层感知器**（**MLP**）作为判别器模型：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use the sequential API to prepare this simple model, with just four
    layers and the final output layer with sigmoid activation. Since we have a binary
    classification task, we have only one unit in the final layer. We will use binary
    cross-entropy loss to train the discriminator model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用顺序API来准备这个简单的模型，仅含有四层和具有sigmoid激活的最终输出层。由于我们有一个二元分类任务，因此最终层中只有一个单元。我们将使用二元交叉熵损失来训练判别器模型。
- en: 'The generator model is also a multi-layer perceptron with multiple layers scaling
    up the noise vector *z* to the desired size. Since our task is to generate MNIST-like
    output samples, the final reshape layer will convert the flat vector into a 28x28
    output shape. Note that we will make use of batch normalization to stabilize model
    training. The following snippet shows a utility method for building the generator
    model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器模型也是一个多层感知器，其中含有多层将噪声向量*z*扩展到所需的尺寸。由于我们的任务是生成类似于MNIST的输出样本，最终的重塑层将把平面向量转换成28x28的输出形状。请注意，我们将利用批次归一化来稳定模型训练。以下代码片段显示了构建生成器模型的实用方法：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We simply use these utility methods to create generator and discriminator model
    objects. The following snippet uses these two model objects to create the GAN
    object as well:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地使用这些实用方法来创建生成器和判别器模型对象。以下代码片段还使用这两个模型对象来创建GAN对象：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The final piece of the puzzle is defining the training loop. As described in
    the previous section, we will train both (discriminator and generator) models
    alternatingly. Doing so is straightforward with high-level Keras APIs. The following
    code snippet first loads the MNIST dataset and scales the pixel values between
    -1 and +1:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是定义训练循环。正如前一节中所描述的，我们将交替训练（判别器和生成器）模型。通过高级Keras API，这样做非常简单。以下代码片段首先加载MNIST数据集并将像素值缩放到-1到+1之间：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For each training iteration, we first sample real images from the MNIST dataset
    equal to our defined batch size. The next step involves sampling the same number
    of *z* vectors. We use these sampled *z* vectors to generate output from our generator
    model. Finally, we calculate the discriminator loss on both real and generated
    samples. These steps are explained in the following snippet:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次训练迭代，我们首先从MNIST数据集中随机选择实际图像，数量等于我们定义的批量大小。下一步涉及对相同数量的*z*向量进行采样。我们使用这些采样的*z*向量来从我们的生成器模型中生成输出。最后，我们计算真实样本和生成样本的判别器损失。这些步骤在下面的代码片段中有详细说明：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Training the generator is straightforward. We prepare a stacked model object
    that resembles the GAN architecture we discussed previously. Simply using the
    `train_on_batch` helps us to calculate the generator loss and improve it, as shown
    in the following snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成器非常简单。我们准备了一个堆叠模型对象，类似于我们以前讨论过的GAN架构。简单地使用`train_on_batch`帮助我们计算生成器损失并改善它，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We train our vanilla GAN for about 30,000 iterations. The following (*Figure
    6.9*) are model outputs at different stages of the training. You can clearly see
    how the sample quality improves as we move from one stage to another.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练我们的普通 GAN 大约 30,000 次迭代。以下（*图 6.9*）是训练不同阶段的模型输出。您可以清楚地看到随着我们从一个阶段移到另一个阶段，样本质量是如何提高的。
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_09.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![一个手机的截图，描述自动生成](img/B16176_06_09.png)'
- en: 'Figure 6.9: Vanilla GAN output at different stages of training'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：普通 GAN 在训练的不同阶段的输出
- en: The results from vanilla GAN are encouraging yet leave space for further improvements.
    In the next section, we will explore some improved architectures to enhance the
    generative capabilities of GANs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 普通 GAN 的结果令人鼓舞，但也留下了进一步改进的空间。在下一节中，我们将探讨一些改进的架构，以增强 GAN 的生成能力。
- en: Improved GANs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进的 GAN
- en: Vanilla GAN proved the potential of adversarial networks. The ease of setting
    up the models and the quality of the output sparked much interest in this field.
    This led to a lot of research in improving the GAN paradigm. In this section,
    we will cover a few of the major improvements in developing GANs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 普通 GAN 证明了对抗网络的潜力。建立模型的简易性和输出的质量引发了该领域的很多兴趣。这导致了对改进 GAN 范式的大量研究。在本节中，我们将涵盖几个主要的改进，以发展
    GAN。
- en: Deep Convolutional GAN
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积 GAN
- en: Published in 2016, this work by Radford et al. introduced several key contributions
    to improve GAN outputs apart from focusing on convolutional layers, which are
    discussed in the original GAN paper. The 2016 paper emphasized using deeper architectures
    instead. *Figure 6.10* shows the generator architecture for a **Deep Convolutional
    GAN** (**DCGAN**) (as proposed by the authors). The generator takes the noise
    vector as input and then passes it through a repeating setup of up-sampling layers,
    convolutional layers, and batch normalization layers to stabilize the training.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 2016 年发表的这项由 Radford 等人完成的工作引入了几项关键改进，以改善 GAN 输出，除了关注卷积层之外，还讨论了原始 GAN 论文。2016
    年的论文强调使用更深的架构。*图 6.10* 展示了**深度卷积 GAN**（**DCGAN**）的生成器架构（如作者所提出的）。生成器将噪声向量作为输入，然后通过一系列重复的上采样层、卷积层和批量归一化层来稳定训练。
- en: '![A close up of a map  Description automatically generated](img/B16176_06_10.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![地图的特写，描述自动生成](img/B16176_06_10.png)'
- en: 'Figure 6.10: DCGAN generator architecture⁷'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：DCGAN 生成器架构⁷
- en: Until the introduction of DCGANs, the output image resolution was quite limited.
    A Laplacian pyramid or LAPGAN was proposed to generate high-quality images, but
    it also suffered from certain fuzziness in the output. The DCGAN paper also made
    use of another important invention, the batch normalization layer. Batch normalization
    was presented after the original GAN paper and proved useful in stabilizing the
    overall training by normalizing the input for each unit to have zero mean and
    unit variance. To get higher-resolution images, it made use of strides greater
    than 1 while moving the convolutional filters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 直到 DCGAN 的引入，输出图像的分辨率相当有限。提出了拉普拉斯金字塔或 LAPGAN 来生成高质量的图像，但它在输出中也存在一定程度的模糊。DCGAN
    论文还利用了另一个重要的发明，即批量归一化层。批量归一化是在原始 GAN 论文之后提出的，并且通过将每个单元的输入归一化为零均值和单位方差来稳定整体训练。为了获得更高分辨率的图像，它利用了大于
    1 的步长移动卷积滤波器。
- en: Let's start by preparing the discriminator model. CNN-based binary classifiers
    are simple models. One modification we make here is to use strides longer than
    1 to down-sample the input between layers instead of using pooling layers. This
    helps in providing better stability for the training of the generator model. We
    also rely on batch normalization and Leaky ReLU for the same purposes (although
    these were not used in the original GAN paper). Another important aspect of this
    discriminator (as compared to the vanilla GAN discriminator) is the absence of
    fully connected layers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先准备鉴别器模型。基于 CNN 的二元分类器是简单的模型。我们在这里做的一个修改是在层之间使用比 1 更长的步长来对输入进行下采样，而不是使用池化层。这有助于为生成器模型的训练提供更好的稳定性。我们还依赖批量归一化和泄漏整流线性单元来实现相同的目的（尽管这些在原始
    GAN 论文中未被使用）。与普通 GAN 鉴别器相比，这个鉴别器的另一个重要方面是没有全连接层。
- en: 'The generator model is quite different to what you saw for vanilla GAN. Here
    we only need the input vector''s dimension to start with. We make use of reshaping
    and up-sampling layers to modify the vector into a two-dimensional image and increase
    its resolution, respectively. Similar to DCGAN''s discriminator, we do not have
    any fully connected layers apart from the input layer, which is reshaped into
    an image. The following code snippet shows how to build a generator model for
    DCGAN:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器模型与普通GAN所见的截然不同。这里我们只需要输入向量的维度。我们利用重塑和上采样层将向量修改为二维图像，并增加其分辨率。类似于DCGAN的判别器，我们除了输入层被重塑为图像外，没有任何全连接层。以下代码片段展示了如何为DCGAN构建生成器模型：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_11.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一部手机的屏幕截图  自动生成的描述](img/B16176_06_11.png)'
- en: 'Figure 6.11: DCGAN output at different stages of training'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：DCGAN在不同训练阶段的输出
- en: The results show how DCGAN is able to generate the required set of outputs in
    fewer training cycles. While it is difficult to make much out of the quality of
    generated images (given the nature of the MNIST dataset), in principle, DCGAN
    should be able to generate better quality output than vanilla GAN.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，DCGAN能够在较少的训练周期内生成所需的输出。虽然很难从生成的图像质量中得出太多结论（考虑到MNIST数据集的性质），但原则上，DCGAN应该能够生成比普通GAN更高质量的输出。
- en: Vector arithmetic
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量算术
- en: 'The ability to manipulate the latent vectors by addition, subtraction, and
    so on to generate meaningful output transformations is a powerful tool. The authors
    of the DCGAN paper showed that indeed the *z* representative space of the generator
    obeys such a rich linear structure. Similar to vector arithmetic in the NLP domain,
    where word2vec generates a vector similar to "Queen" upon performing the manipulation
    "King" – "Man" + "Woman," DCGAN allows the same in the visual domain. The following
    is an example from the DCGAN paper (*Figure 6.12*):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加法、减法等操作操纵潜在向量以生成有意义的输出变换是一种强大的工具。DCGAN论文的作者们表明，生成器的*Z*表示空间确实具有如此丰富的线性结构。类似于NLP领域的向量算术，word2vec在执行"国王"
    - "男人" + "女人"的操作后生成类似于"女王"的向量，DCGAN在视觉领域也能实现相同的功能。以下是DCGAN论文的一个例子（*图6.12*）：
- en: '![A group of people posing for the camera  Description automatically generated](img/B16176_06_12.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![一群人为照相摆姿势  自动生成的描述](img/B16176_06_12.png)'
- en: 'Figure 6.12: DCGAN vector arithmetic⁸'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：DCGAN向量算术⁸
- en: The example shows that we can generate examples of "woman with glasses" by performing
    the simple manipulation of "man with glasses" – "man without glasses" + "woman
    without glasses." This opens up the possibility of generating complex samples
    without the need for huge amounts of training data. Though unlike word2vec, where
    a single vector is sufficient, in this case, we average at least three samples
    to achieve stable outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子显示，我们可以通过执行"带眼镜的男人" - "不带眼镜的男人" + "不带眼镜的女人"的简单操作来生成"带眼镜的女人"的示例。这打开了无需大量训练数据就能生成复杂样本的可能性。尽管不像word2vec那样只需一个向量就足够，但在这种情况下，我们需要平均至少三个样本才能实现稳定的输出。
- en: Conditional GAN
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件GAN
- en: GANs are powerful systems that can generate realistic samples from the domain
    of training. In the previous sections, you saw vanilla GAN and DCGAN generate
    realistic samples from the MNIST dataset. These architectures have also been used
    to generate samples that resemble human faces and even real-world items (from
    training on CIFAR10 and so on). However, they miss out on the ability to control
    the samples we would like to generate.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是可以从训练领域生成逼真样本的强大系统。在前面的章节中，您看到普通GAN和DCGAN可以从MNIST数据集生成逼真样本。这些架构也被用于生成类似人脸甚至真实世界物品的样本（从对CIFAR10的训练等）。但它们无法控制我们想要生成的样本。
- en: In simple words, we can use a trained generator to generate any number of samples
    required, yet we cannot control it to generate a specific type of example. **Conditional
    GANs** (**CGANs**) are the class of GANs that provide us with precisely the control
    needed to generate a specific class of examples. Developed by Mirza et al. in
    2014⁹, they are some of the earliest enhancements to the original GAN architecture
    from Goodfellow et al.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们可以使用训练过的生成器生成任意数量所需的样本，但我们不能控制它生成特定类型的示例。**条件GAN**（**CGANs**）是提供我们精确控制生成特定类别示例所需的GAN类别。由Mirza等人于2014年⁹开发，它们是对Goodfellow等人原始GAN架构的最早改进之一。
- en: CGANs work by training the generator model to generate fake samples conditioned
    on specific characteristics of the output required. The discriminator, on the
    other hand, needs to do some extra work. It needs to learn not only to differentiate
    between fake and real but also to mark out samples as fake if the generated sample
    and its conditioning characteristics do not match.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN 的工作方式是通过训练生成器模型生成伪造样本，同时考虑所需输出的特定特征。另一方面，鉴别器需要进行额外的工作。它不仅需要学会区分真实和伪造的样本，还需要在生成的样本和其条件特征不匹配时标记出伪造的样本。
- en: 'In their work *Conditional Adversarial Networks*, Mirza et al. pointed towards
    using class labels as additional conditioning input to both generator and discriminator
    models. We denote the conditioning input as *y* and transform the value function
    for the GAN minimax game as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作 *Conditional Adversarial Networks* 中，Mirza 等人指出了使用类标签作为生成器和鉴别器模型的附加条件输入。我们将条件输入标记为
    *y*，并将 GAN 极小极大游戏的价值函数转换如下：
- en: '![](img/B16176_06_018.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_018.png)'
- en: where *log log D* (*x*|*y*) is the discriminator output for a real sample, *x*,
    conditioned on *y* and similarly *log log* (1 - *D* (*G*(*z*|*y*))) is the discriminator
    output for a fake sample, *G*(*z*), conditioned on *y*. Note that the value function
    is only slightly changed from the original minimax equation for vanilla GAN. Thus,
    we can leverage the improved cost functions for the generator as well as the other
    enhancements we discussed in the previous sections. The conditioning information,
    *y* (the class label, for example), is provided as an additional input to both
    the models and the rest is taken care of by the GAN setup itself. *Figure 6.13*
    shows the architectural setup for a Conditional GAN.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*log log D* (*x*|*y*) 是对真实样本 *x* 在条件 *y* 下的鉴别器输出，而 *log log* (1 - *D* (*G*(*z*|*y*)))
    则是对伪造样本 *G*(*z*) 在条件 *y* 下的鉴别器输出。请注意，价值函数与普通 GAN 的原始极小极大方程仅略有变化。因此，我们可以利用改进的生成器成本函数以及我们在前几节讨论过的其他增强功能来加强生成器。条件信息
    *y*（例如，类标签）作为两个模型的额外输入，GAN 设置本身负责处理其余部分。*图 6.13* 展示了条件 GAN 的架构设定。'
- en: '![](img/B16176_06_13.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_13.png)'
- en: 'Figure: 6.13 CGAN generator architecture^(10)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图：6.13 CGAN 生成器架构^(10)
- en: Keeping the implementation as close to the original implementation of CGAN as
    possible, we will now develop conditioned generator and discriminator models as
    MLPs. You are encouraged to experiment with DCGAN-like architectures conditioned
    on class labels. Since we would have multiple inputs to each of the constituent
    models, we will make use of the Keras functional API to define our models. We
    will be developing CGAN to generate MNIST digits.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能保持实现与原始 CGAN 实现的接近，现在我们将开发条件生成器和鉴别器模型作为 MLPs。建议您尝试基于类标签的 DCGAN 类似的架构。由于每个成分模型都将有多个输入，我们将使用
    Keras 函数 API 来定义我们的模型。我们将开发 CGAN 以生成 MNIST 数字。
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once trained, CGAN can be asked to generate examples of a specific class. *Figure
    6.14* shows the output for different class labels across the training epochs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，可以要求 CGAN 生成特定类别的示例。*图 6.14* 展示了横跨训练周期的不同类别标签的输出。
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_14.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![一部手机的屏幕截图  自动生成的描述](img/B16176_06_14.png)'
- en: 'Figure 6.14: CGAN output at different stages of training'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.14: CGAN 在不同训练阶段的输出'
- en: One major advantage apparent from *Figure 6.14* is the additional control that
    CGANs provide us with. As discussed, using additional inputs, we are able to easily
    control the generator to generate specific digits. This opens up a long list of
    use cases, some of which we will cover in later chapters of the book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 6.14* 明显可见的一个主要优势是 CGAN 提供给我们的额外控制功能。如讨论所述，通过使用额外输入，我们能够轻松控制生成器生成特定的数字。这开启了长长的用例列表，其中一些将在本书的后续章节中介绍。
- en: Wasserstein GAN
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wasserstein GAN
- en: The improved GANs we have covered so far were mostly focused upon architectural
    enhancements to improve results. Two major issues with the GAN setup are the stability
    of the minimax game and the unintuitiveness of the generator loss. These issues
    arise due to the fact that we train the discriminator and generator networks alternatingly
    and at any given moment, the generator loss is indicative of the discriminator's
    performance so far.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所涵盖的改进 GAN 主要集中在增强架构以改进结果。GAN 设置的两个主要问题是极小极大游戏的稳定性和生成器损失的不直观性。这些问题是由于我们交替训练鉴别器和生成器网络，并在任何给定时刻，生成器损失表明鉴别器到目前为止的表现。
- en: Wasserstein GAN (or W-GAN) was an attempt by Arjovsky et al. to overcome some
    of the issues with the GAN setup. This is one of a few deep learning papers that
    are deeply rooted in theoretical foundations to explain the impact of their work
    (apart from empirical results). The main difference between typical GANs and W-GANs
    is the fact that W-GANs treat the discriminator as a critic (deriving from reinforcement
    learning; see *Chapter 11*, *Composing Music with Generative Models*). Hence,
    instead of simply classifying input images as real or fake, the W-GAN discriminator
    (or critic) generates a score to inform the generator about the realness or fakeness
    of the input image.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 沃瑟斯坦GAN（或W-GAN）是Arjovsky等人为克服GAN设置中的一些问题而提出的尝试。这是一些深度学习论文中深深植根于理论基础以解释其工作影响的论文之一（除了经验证据）。典型GAN和W-GAN之间的主要区别在于W-GAN将判别器视为评论家（来源于强化学习；参见第11章《用生成模型创作音乐》）。因此，W-GAN判别器（或评论家）不仅仅将输入图像分类为真实或伪造，还生成一个分数来告诉生成器输入图像的真实性或伪造性。
- en: 'The maximum likelihood game we discussed in the initial sections of the chapter
    explained the task as one where we try to minimize the divergence between *p*[z]
    and *p*[data] using KL divergence, that is, ![](img/B16176_06_019.png). Apart
    from being asymmetric, KL divergence also has issues when the distributions are
    too far off or completely disjointed. To overcome these issues, W-GANs make use
    of **Earth Mover''s** (**EM**) distance or Wasserstein distance. Simply put, EM
    distance is the minimum cost to move or transport mass from distribution *p* to
    *q*. For the GAN setup, we can imagine this as the minimum cost of moving from
    the generator distribution (*p*[z]) to the real distribution (*p*[data]). Mathematically,
    this can be stated as the infimum (or greatest lower bound, denoted as *inf*)
    for any transport plan (denoted as *W*(*source*, *destination*), that is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的初始部分讨论的最大似然游戏解释了这样一个任务，我们试图通过KL散度来最小化*p*[z]和*p*[data]之间的差异，即![](img/B16176_06_019.png)。除了是非对称的，KL散度在分布相距太远或完全不相交时也存在问题。为了克服这些问题，W-GAN使用**地球移动者**（**EM**）距离或Wasserstein距离。简单地说，EM距离是从分布*p*到*q*移动或转运质量的最小成本。对于GAN设置，我们可以将其想象为从生成器分布（*p*[z]）移动到实际分布（*p*[data]）的最小成本。在数学上，这可以被陈述为任何传输计划（表示为*W*（*source*，*destination*））的下确界（或最大下界，表示为*inf*）的方式：
- en: '![](img/B16176_06_020.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_020.png)'
- en: 'Since this is intractable, the authors used Kantorovich-Rubinstein duality
    to simplify the calculations. The simplified form is denoted as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是无法计算的，作者使用了康托洛维奇-鲁宾斯坦二元性来简化计算。简化形式表示为：
- en: '![](img/B16176_06_021.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_021.png)'
- en: where *sup* is the supremum or least upper bound and *f* is a 1-Lipschitz function
    that imposes certain constraints. A great many details are required to fully understand
    the details and impact of using Wasserstein distance. You are encouraged to go
    through the paper for an in-depth understanding of the associated concepts or
    refer to [https://vincentherrmann.github.io/blog/wasserstein/](https://vincentherrmann.github.io/blog/wasserstein/).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*sup*是最大值或最小上界，*f*是一个1-Lipschitz函数，施加了一定的约束条件。要完全理解使用Wasserstein距离的细节和影响，需要许多细节。建议您阅读论文以深入了解相关概念，或参考[https://vincentherrmann.github.io/blog/wasserstein/](https://vincentherrmann.github.io/blog/wasserstein/)。
- en: 'For brevity, we will focus on implementation-level changes that help in achieving
    a stable trainable architecture. A comparison of gradient updates of a GAN and
    W-GAN is shown in *Figure 6.15*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们将重点放在实现层级的改变上，以帮助实现稳定的可训练体系结构。*图6.15*展示了GAN和W-GAN之间的梯度更新对比：
- en: '![A close up of a map  Description automatically generated](img/B16176_06_15.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![一个地图的放大图  描述由自动生成](img/B16176_06_15.png)'
- en: 'Figure 6.15: W-GAN versus GAN^(11)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15：W-GAN对比GAN^(11)
- en: The figure explains the vanishing gradients in the case of the GAN discriminator
    when the input is bimodal Gaussian, while the W-GAN critic has a smooth gradient
    throughout.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 该图解释了当输入为双峰高斯分布时，GAN判别器中的梯度消失，而W-GAN评论家的梯度始终平滑。
- en: 'To transform this understanding into implementation-level details, the following
    are the key changes in W-GAN:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这种理解转化为实现层级的细节，W-GAN的关键变化如下：
- en: The discriminator is termed as the critic, which generates and outputs a score
    of realness or fakeness.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器被称为评论家，它生成并输出真实性或伪造性的分数。
- en: The final layer in the critic/discriminator is a linear layer (instead of sigmoid).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论家/判别器中的最后一层是一个线性层（而不是sigmoid）。
- en: -1 denotes real labels, while 1 denotes fake labels. These are expressed as
    positive and negative critics in the literature. We otherwise use 1 and 0 for
    real and fake labels, respectively.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -1表示真标签，而1表示假标签。这在文献中被表达为积极和消极的评论家。否则，我们分别使用1和0表示真和假标签。
- en: We replace classification loss (binary cross-entropy) with Wasserstein loss.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用Wasserstein损失替换了分类损失（二元交叉熵）。
- en: The critic model is allowed to train for a greater number of cycles compared
    to the generator model. This is done because in the case of W-GANs, a stable critic
    better guides the generator; the gradients are much smoother. The authors trained
    the critic model five times per generator cycle.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与生成器模型相比，评论家模型被允许进行更多次的训练周期。这是因为在W-GANs的情况下，稳定的评论家更好地指导生成器；梯度要平稳得多。作者每个生成器周期训练了评论家模型五次。
- en: The weights of the critic layers are clipped within a range. This is required
    in order to maintain the 1-Lipschitz constraint. The authors used the range of
    -0.01 to 0.01.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论家层的权重被剪切在一个范围内。这是为了保持1-李普希兹约束所必需的。作者使用了-0.01到0.01的范围。
- en: RMSProp is the recommended optimizer to allow stable training. This is in contrast
    to the usage of Adam as an optimizer for the typical case.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSProp是推荐的优化器，以保证稳定的训练。这与典型情况下使用Adam作为优化器的情况形成对比。
- en: With these changes, the authors noted a significant improvement in training
    stability and better feedback for the generator. *Figure 6.16* (from the paper)
    shows how the generator takes cues from a stable critic to train better. The results
    improve as the training epochs increase. The authors experimented with both MLP-based
    generators as well as convolutional generators and found similar results.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些改变，作者注意到训练稳定性有了显著的改善，并且生成器得到了更好的反馈。*图 6.16*（来自论文）展示了生成器如何从稳定的评论家中获得提示来进行更好的训练。随着训练轮数的增加，结果也会得到改善。作者们尝试了基于MLP的生成器和卷积生成器，发现了类似的结果。
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_16.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图 自动生成的描述](img/B16176_06_16.png)'
- en: 'Figure 6.16: W-GAN generator loss and output quality^(12)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16：W-GAN 生成器损失和输出质量^(12)
- en: 'Since we can use any generator and discriminator with minor modifications,
    let''s get to some of the implementation details. First and foremost is the Wasserstein
    loss. We calculate it by taking a mean of the critic score and the ground truth
    labels. This is shown in the following snippet:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以对任何生成器和鉴别器进行微小修改，让我们来看一些实现细节。首先，最重要的是Wasserstein损失。我们通过取评论家评分和真实标签的平均值来计算它。这在下面的片段中显示：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The major change for the discriminator is its final layer and its weight clipping.
    While the change of activation function for the last layer is straightforward,
    the weight clipping can be a bit challenging to implement at first. With the Keras
    API, this can be done in two ways: by sub-classing the `Constraint` class and
    using it as an additional argument for all layers or by iterating through the
    layers during the training loop. While the first approach is much cleaner, we''ll
    use the second approach as it is easier to understand.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对鉴别器的主要改变是它的最后一层和它的权重裁剪。虽然最后一层的激活函数的变化很直接，但权重裁剪在开头可能有些挑战。通过Keras API，这可以通过两种方式来完成：通过对`Constraint`类进行子类化，并将其作为所有层的附加参数使用，或者在训练循环期间遍历层。虽然第一种方法更清晰，但我们将使用第二种方法，因为更容易理解。
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With these changes, we train our W-GAN to generate MNIST digits. The following
    (*Figure 6.17*) are the output samples during different stages of training:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些改变，我们训练我们的 W-GAN 来生成 MNIST 数字。以下（*图 6.17*）是训练不同阶段的输出样本：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_06_17.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图 自动生成的描述](img/B16176_06_17.png)'
- en: 'Figure 6.17: W-GAN output at different stages of training'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：W-GAN 训练不同阶段的输出
- en: The promised stable training backed up by theoretical proofs is not free from
    its own set of issues. Mostly, the issues are due to the constraints of keeping
    the calculations tractable. Some of these concerns were addressed in a recent
    work titled *Improved Training of Wasserstein GAN*^(13) by Gulrajani et al. in
    2017\. This work presented a few tricks, with the most important one being gradient
    penalty (or, as the authors refer to it, W-GAN-GP). You are encouraged to go through
    this interesting work as well to better understand the contributions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺的稳定训练受到理论证明的支持，但也并非没有自身一套问题。大部分问题是由于保持计算的可处理性的限制。其中一些问题是在Gulrajani等人于2017年撰写的一篇名为*改良的Wasserstein
    GAN的训练*（13）的最近工作中得到解决。该工作提出了一些技巧，最重要的是梯度惩罚（或者，如作者称呼它的，W-GAN-GP）。你也被鼓励去阅读这个有趣的工作，以更好地理解其贡献。
- en: Now that we have covered quite a few improvements, let's move towards a slightly
    more complex setup called Progressive GAN. In the next section, we will go through
    the details of this highly effective architecture to generate high-quality outputs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了相当多的改进，让我们转向一个稍微更复杂的设置，称为Progressive GAN。在下一节中，我们将详细介绍这种高效的架构，以生成高质量的输出。
- en: Progressive GAN
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Progressive GAN
- en: GANs are powerful systems to generate high-quality samples, examples of which
    we have seen in the previous sections. Different works have utilized this adversarial
    setup to generate samples from different distributions like CIFAR10, celeb_a,
    LSUN-bedrooms, and so on (we covered examples using MNIST for explanation purposes).
    There have been some works that focused on generating higher-resolution output
    samples, like Lap-GANs, but they lacked perceived output quality and presented
    a larger set of challenges for training. Progressive GANs or Pro-GANs or PG-GANs
    were presented by Karras et al. in their work titled *GANs for Improved Quality,
    Stability, and Variation*^(14) at ICLR-2018, as a highly effective method for
    generating high-quality samples.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是生成高质量样本的强大系统，我们在前几节中已经看到了一些例子。不同的工作已经利用了这种对抗性设置来从不同的分布生成样本，比如CIFAR10，celeb_a，LSUN-bedrooms等（我们使用MNIST的例子来解释）。有一些工作集中于生成更高分辨率的输出样本，比如Lap-GANs，但它们缺乏感知输出质量，并且提出了更多的训练挑战。Progressive
    GANs或Pro-GANs或PG-GANs是由Karras等人在他们的名为*用于改善质量、稳定性和变化的GAN*（14）的ICLR-2018工作中提出的，是一种生成高质量样本的高效方法。
- en: The method presented in this work not only mitigated many of the challenges
    present in earlier works but also brought about a very simple solution to crack
    this problem of generating high-quality output samples. The paper also presented
    a number of very impactful contributions, some of which we will cover in detail
    in the following subsections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的方法不仅缓解了早期工作中存在的许多挑战，而且还提出了一个非常简单的解决方案来解决生成高质量输出样本的问题。该论文还提出了一些非常有影响力的贡献，其中一些我们将在接下来的子章节中详细介绍。
- en: The overall method
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整体方法
- en: The software engineering way of solving tough technical problems is often to
    break them down into simpler granular tasks. Pro-GANs also target the complex
    problem of generating high-resolution samples by breaking down the task into smaller
    and simpler problems to solve. The major issue with high-resolution images is
    the huge number of modes or details such images have. It makes it very easy to
    differentiate between generated samples and the real data (perceived quality issues).
    This inherently makes the task of building a generator, with enough capacity to
    train well on such datasets along with memory requirements, a very tough one.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 解决技术难题的软件工程方法通常是将其分解为更简单的颗粒任务。Pro-GANs也针对生成高分辨率样本的复杂问题，通过将任务拆分为更小更简单的问题来解决。高分辨率图像的主要问题是具有大量模式或细节。这使得很容易区分生成的样本和真实数据（感知质量问题）。这本质上使得构建一个具有足够容量在这样的数据集上训练良好并具有内存需求的生成器的任务非常艰巨。
- en: To tackle these issues, Karras et al. presented a method to grow both generator
    and discriminator models as the training progresses from lower to higher resolutions
    gradually. This is shown in *Figure 6.18*. They noted that this progressive growth
    of models has various advantages, such as the ability to generate high-quality
    samples, faster training, and lesser memory requirements (compared to directly
    training a GAN to generate high-resolution output).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_06_18.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Progressively increasing the resolution for discriminator and
    generator models^(15)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Generating higher-resolution images step by step was not an entirely new idea.
    A lot of prior works used similar techniques, yet the authors pointed out that
    their work was most similar to the layer-wise training of autoencoders^(16).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The system learns by starting with lower-resolution samples and a generator-discriminator
    set up as mirror images of each other (architecture-wise). At a lower resolution
    (say 4x4), the training is much simpler and stable as there are fewer modes to
    learn. We then increase the resolution step by step by introducing additional
    layers for both models. This step-by-step increase in resolution limits the complexity
    of the task at hand rather than forcing the generator to learn all modes at once.
    This finally enables Pro-GANs to generate megapixel-size outputs, which all start
    from a very low-resolution initial point.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will cover the important contributions and
    implementation-level details. Note that the training time and compute requirements,
    despite improvements, for Pro-GANs are huge. The authors mentioned a training
    time of up to a week on multiple GPUs to generate said megapixel outputs. Keeping
    the requirements in check, we will cover component-level details but use TensorFlow
    Hub to present the trained model (instead of training one from scratch). This
    will enable us to focus on the important details and leverage pre-built blocks
    as required.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Progressive growth-smooth fade-in
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pro-GANs were introduced as networks that increase the resolution step by step
    by adding additional layers to generator and discriminator models. But how does
    that actually work? The following is a step-by-step explanation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The generator and discriminator models start with a resolution of 4x4 each.
    Both networks perform their designated tasks of generating and discriminating
    the pre-scaled samples.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train these models for a number of epochs until the performance saturates.
    At this point, additional layers are added to both networks.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator gets an additional upscaling layer to generate 8x8 samples while
    the discriminator gets an additional downscaling layer.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The move from one step to the next (that is, from 4x4 to 8x8) is done gradually
    using an overlay factor, ![](img/B16176_06_022.png). *Figure 6.19* shows the transition
    pictorially.![A screenshot of a cell phone  Description automatically generated](img/B16176_06_19.png)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.19: Smooth fade-in^(17)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19：平滑淡入^(17)
- en: The existing layers are upscaled and transitioned with a factor of 1-![](img/B16176_06_023.png),
    while the newly added layer is multiplied with a factor of ![](img/B16176_06_023.png).
    The value of ![](img/B16176_06_023.png) ranges between 0 and 1, which is gradually
    increased from 0 towards 1 to increase the contribution from the newly added layers.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的层通过乘以 1-![](img/B16176_06_023.png) 进行扩大，并且逐渐过渡到新增层；而新增的层则乘以 ![](img/B16176_06_023.png)
    进行缩小。![](img/B16176_06_023.png) 的值在 0 和 1 之间，逐渐从 0 增加到 1，以增加新增层的贡献。
- en: The same process is followed for the discriminator, where the transition moves
    it gradually from the existing setup to newly added layers.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用同样的过程对判别器进行操作，其中过渡逐渐将其从现有设置过渡到新增的层。
- en: It is important to note that all layers are trained (existing upscaled and newly
    added ones) throughout the training process.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要注意的是，在整个训练过程中，所有层都会被训练（现有的增加和新增的层）。
- en: The authors started from a 4x4 resolution and, step by step, increased it to
    finally take it to megapixel levels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 作者从4x4分辨率开始，逐步增加到最终达到百万像素级别。
- en: Minibatch standard deviation
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量标准偏差
- en: Existing approaches rely on normalization techniques such as batch normalization,
    virtual normalization, and so on. These techniques use trainable parameters to
    compute minibatch-level statistics in order to maintain similarity across samples.
    Apart from adding additional parameters and compute load, these normalization
    methods do not completely alleviate issues.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的方法依赖于诸如批量归一化、虚拟归一化等归一化技术。这些技术使用可训练参数来计算小批量级别的统计数据，以保持样本之间的相似性。除了增加额外的参数和计算负载外，这些归一化方法并不能完全缓解问题。
- en: The authors of Pro-GAN introduced a simplified solution that does not require
    any trainable parameters. The proposed minibatch standard deviation method was
    introduced to improve the diversity of minibatches. From the last layer of the
    discriminator, the method computes the standard deviation of each spatial location
    (pixel location *x*, *y*). For a given batch of size *B* with images shaped *H*
    x *W* x *C* (height, width, and channels), a total of *B * H * W * C* standard
    deviations are calculated. The next step involves averaging these standard deviations
    and concatenating them to the layer's output. This is designed to be the same
    for each example in the minibatch.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Pro-GAN的作者提出了一种简化的解决方案，不需要任何可训练参数。提出的小批量标准偏差方法旨在提高小批量的多样性。从判别器的最后一层开始，该方法计算每个空间位置（像素位置
    *x*、*y*）的标准偏差。对于大小为 *B* 的批次，图像的形状为 *H* x *W* x *C*（高度、宽度和通道），计算了共 *B * H * W *
    C* 个标准偏差。下一步包括对这些标准偏差进行平均，并将它们连接到层的输出。这是为了保证每个示例在小批量中都相同。
- en: Equalized learning rate
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等化学习率
- en: The authors of Pro-GAN briefly mentioned that they focused on simpler weight
    initialization methods compared to the current trend of identifying custom initialization
    methods. They used an *N*(0,1) standard normal distribution for the initialization
    of weights and then explicitly scaled at runtime. The scaling was performed as
    ![](img/B16176_06_026.png), where *c* is the per-layer normalization constant
    from the *He's* initializer. They also pointed out issues with momentum-based
    optimizers such as Adam and RMSProp that get mitigated with this equalized learning
    rate method.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Pro-GAN的作者简要提到，他们专注于比当前流行的自定义初始化方法更简单的权重初始化方法。他们使用标准正态分布 *N*(0,1) 来初始化权重，然后在运行时明确进行了缩放。缩放是以![](img/B16176_06_026.png)的形式进行的，其中
    *c* 是来自 *He's* 初始化器的每层归一化常数。他们还指出了动量优化器（如Adam和RMSProp）存在的问题，这些问题通过这种等化的学习率方法得以缓解。
- en: Pixelwise normalization
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐像素归一化
- en: 'The enhancements mentioned so far either focus on the discriminator or the
    overall GAN training. This normalization technique is applied to the generator
    model. The authors pointed out that this method helps to prevent instability in
    the training process along with mode-collapse issues. As the name suggests, they
    proposed the application of the normalization per spatial location (or per pixel,
    denoted as (*x*, *y*)). The normalization equation is given as:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止提到的增强功能要么专注于判别器，要么是整个GAN训练。这种归一化技术是应用于生成器模型的。作者指出，这种方法有助于防止训练过程中的不稳定以及模式崩溃问题。正如名称所示，他们建议对每个空间位置（或每个像素，表示为
    (*x*, *y*)）应用归一化。归一化方程如下：
- en: '![](img/B16176_06_027.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_06_027.png)'
- en: where ![](img/B16176_06_028.png), *N* is the number of feature maps, and *a*
    and *b* are the original and normalized feature vectors, respectively. This strange-looking
    normalization equation helps in preventing huge random changes in magnitude effectively.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub implementation
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, despite the long list of effective contributions, Pro-GANs
    require huge amounts of compute to generate quality results. The official implementation
    on GitHub^(18) mentions a training time of two weeks on a single GPU for the CelebA-HQ
    dataset. This is beyond the time and effort available for most people. The following
    (*Figure 6.20*) is a snapshot of the generator and discriminator model architectures;
    each of them has about 23 million parameters!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a piece of paper  Description automatically generated](img/B16176_06_20.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Generator and discriminator model summary^(19)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we will focus on the pretrained Pro-GAN model available through TensorFlow
    Hub. TensorFlow Hub is a repository of a large number of deep learning models
    that can be easily downloaded and used for various downstream tasks using the
    TensorFlow ecosystem. The following is a miniature example to showcase how we
    can use the Pro-GAN model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the required libraries. With TensorFlow Hub, the
    only additional `import` required is:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We used TensorFlow Hub version 0.12.0 with TensorFlow version 2.4.1\. Make
    sure your versions are in sync otherwise there might be slight changes with respect
    to syntax. The next step is to load the model. We set a seed for our TensorFlow
    session to ensure the reproducibility of results:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Loading a pretrained model using TensorFlow Hub is as simple as the preceding
    code. The next step is about randomly sampling a latent vector (*z*) from a normal
    distribution. The model requires the latent vector to be of size 512\. Once we
    have the latent vector, we pass it to our generator to get the output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following is a sample face generated from the pretrained Pro-GAN model
    (*Figure 6.21*):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![A person smiling for the camera  Description automatically generated](img/B16176_06_21.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: Sample face using pretrained Pro-GAN from TF Hub'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'We wrote a simple sampling function, similar to one we have been using throughout
    the chapter to generate a few additional faces. This additional experiment helps
    us to understand the diversity of human faces this model has been able to capture
    and, of course, its triumph over issues such as mode collapse (more on this in
    the next section). The following image (*Figure 6.22*) is a sample of 25 such
    faces:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, food, different  Description automatically generated](img/B16176_06_22.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: 25 faces generated using Pro-GAN'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious, TensorFlow Hub provides a training mechanism to train such
    models from scratch. Also, the Pro-GAN authors have open-sourced their implementation.
    You are encouraged to go through it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a lot of ground to understand different architectures and their
    ability to generate images. In the next section, we will cover some of the challenges
    associated with GANs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了很多内容来了解不同的架构及其生成图像的能力。在下一节中，我们将涵盖与GANs相关的一些挑战。
- en: Challenges
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: GANs provide an alternative method of developing generative models. Their design
    inherently helps in mitigating the issues we discussed with some of the other
    techniques. However, GANs are not free from their own set of issues. The choice
    to develop models using concepts of game theory is fascinating yet difficult to
    control. We have two agents/models trying to optimize opposing objectives, which
    can lead to all sorts of issues. Some of the most common challenges associated
    with GANs are as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: GANs提供了一种开发生成模型的替代方法。它们的设计固有地有助于缓解我们使用其他技术时讨论的问题。然而，GANs并不是没有自己一套问题。使用博弈论概念开发模型的选择令人着迷，但难以控制。我们有两个试图优化对立目标的代理/模型，这可能导致各种问题。与GANs相关的一些最常见的挑战如下。
- en: Training instability
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练不稳定
- en: GANs play a minimax game with opposing objectives. No wonder this leads to oscillating
    losses for generator and discriminator models across batches. A GAN setup that
    is training well will typically have a higher variation in losses initially but,
    eventually, it will stabilize and so will the loss of the two competing models.
    Yet it is very common for GANs (especially vanilla GANs) to spiral out of control.
    It is difficult to determine when to stop the training or to estimate an equilibrium
    state.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: GANs通过对立的目标进行极小极大博弈。难怪这导致生成器和判别器模型在批次间产生振荡的损失。一个训练良好的GAN设置通常最初会有更高的损失变化，但最终会稳定下来，两个竞争模型的损失也会如此。然而，GANs（特别是原始的GANs）很常见地会失控。很难确定何时停止训练或估计一个平衡状态。
- en: Mode collapse
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式坍塌
- en: 'Mode collapse refers to a failure state where the generator finds one or only
    a small number of samples that are enough to fool the discriminator. To understand
    this better, let''s take the example of a hypothetical dataset of temperatures
    from two cities, city **A** and city **B**. Let''s also assume city **A** is at
    a higher altitude and remains cold mostly while city **B** is near the equator
    and has high temperatures. Such a dataset might have a temperature distribution
    as shown in *Figure 6.23*. The distribution is bimodal, that is, it has two peaks:
    one for city **A** and one for city **B** (due to their different weather conditions).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 模式坍塌是指生成器找到一个或仅有少数足以愚弄鉴别器的样本的失败状态。为了更好地理解这一点，让我们以两个城市的温度的假设数据集为例，城市**A**和城市**B**。我们还假设城市**A**位于较高的海拔处，大部分时间保持寒冷，而城市**B**位于赤道附近，气温较高。这样的数据集可能具有如*图6.23*所示的温度分布。该分布是双峰的，即有两个峰值：一个是城市**A**的，另一个是城市**B**的（由于它们不同的天气条件）。
- en: '![A close up of a lamp  Description automatically generated](img/B16176_06_23.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![一个灯的特写 描述自动生成](img/B16176_06_23.png)'
- en: 'Figure 6.23: Bimodal distribution of the temperatures of two cities'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23：两个城市温度的双峰分布
- en: 'Now that we have our dataset, assume we are tasked with training a GAN that
    can mimic this distribution. In the perfect scenario, we will have the GAN generate
    samples of temperatures from city **A** and city **B** with roughly equal probability.
    However, a commonly occurring issue is that of mode collapse: the generator ends
    up generating samples only from a single mode (say, only city **B**). This happens
    when:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的数据集，假设我们的任务是训练一个能够模仿这个分布的GAN。在完美的情况下，我们将有GAN生成来自城市**A**和城市**B**的温度样本，其概率大致相等。然而，一个常见的问题是模式坍塌：生成器最终只生成来自一个模式（比如，只有城市**B**）。当：
- en: The generator learns to fool the discriminator by generating realistic-looking
    samples from city **B** only
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器通过仅从城市**B**生成看起来逼真的样本来愚弄鉴别器
- en: The discriminator tries to counter this by learning that all outputs for city
    **A** are real and tries to classify samples from city **B** as real or fake
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器试图通过学习所有城市**A**的输出都是真实的，并试图将城市**B**的样本分类为真实或伪造来抵消这一点
- en: The generator then flips to city **A**, abandoning the mode for city **B**
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器然后转向城市**A**，放弃城市**B**的模式
- en: The discriminator now assumes all samples for city **B** are real and tries
    to classify samples for city **A** instead
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，鉴别器假定所有城市**B**的样本都是真实的，并试图代替城市**A**的样本进行分类
- en: This cycle keeps on repeating
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个周期不断重复
- en: This cycle repeats as the generator is never incentivized enough to cover both
    modes. This limits the usefulness of the generator as it exhibits poor diversity
    of output samples. In a real-world setting, mode collapse varies from complete
    collapse (that is, all generated samples are identical) to partial collapse (that
    is, a few modes are captured).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这种循环重复，因为生成器永远没有足够的动力来覆盖两种模式。这限制了生成器的实用性，因为它展示了样本输出的贫乏多样性。在实际应用中，模式崩溃会从完全崩溃（即，所有生成的样本都相同）到部分崩溃（即，捕捉到一些模式）不等。
- en: 'We have trained different GAN architectures in the chapter so far. The MNIST
    dataset is also multimodal in nature. A complete collapse for such a dataset would
    result in the GAN generating only a single digit as output, while a partial collapse
    would mean only a few digits were generated (out of 10). *Figure 6.24* shows the
    two scenarios for vanilla GAN:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中我们已经训练了不同的 GAN 架构。MNIST 数据集也是多模态的特性。对于这样的数据集，完全崩溃将导致 GAN 生成的只有一个数字，而部分崩溃意味着只生成了一些数字（共
    10 个）。*图 6.24* 展示了普通 GAN 的两种情况：
- en: '![A picture containing crossword, drawing, clock  Description automatically
    generated](img/B16176_06_24.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含填字游戏，绘画，时钟的图片](img/B16176_06_24.png)'
- en: 'Figure 6.24: Failure mode for GAN - mode collapse'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24：GAN 的失败模式 - 模式崩溃
- en: '*Figure 6.24* shows how mode collapse can lead to limited diversity in the
    samples that a GAN can generate.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.24* 展示了模式崩溃如何导致 GAN 能够生成的样本的多样性受限。'
- en: Uninformative loss and evaluation metrics
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无信息量的损失和评估指标
- en: Neural networks train using gradient descent and improve upon the loss values.
    Yet in the case of GANs (except W-GAN and related architectures), the loss values
    are mostly uninformative. We would assume that as training progresses, the generator
    loss would keep on decreasing while the discriminator would hit a saddle point,
    but this is not the case. The main reason is the alternate training cycles for
    generator and discriminator models. The generator loss at any given point is compared
    against the discriminator trained so far, thus making it difficult to compare
    the generator's performance across training epochs. You should note that in the
    case of W-GAN, critic loss in particular is the guiding signal for improving the
    generator model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用梯度下降进行训练，并改善损失值。然而在 GAN 的情况下（除了 W-GAN 和相关架构），损失值大多无信息量。我们会假设随着训练的进行，生成器损失会持续减少，而鉴别器会达到一个鞍点，但事实并非如此。主要原因是交替训练生成器和鉴别器模型。在任何给定点上，生成器的损失与到目前为止已经训练的鉴别器进行比较，因此很难在训练周期内比较生成器的性能。需要注意的是，在
    W-GAN 的情况下，批评者的损失尤其是用来指导改进生成器模型的信号。
- en: Apart from these issues, GANs also need a strict evaluation metric to understand
    the output quality of the samples. Inception score is one such way of calculating
    the output quality, yet there is scope for identifying better evaluation metrics
    in this space.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些问题，GAN 还需要一个严格的评估指标来了解样本输出的质量。Inception 分数就是计算输出质量的一种方式，然而在这一领域还有识别更好的评估指标的空间。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you were introduced to a new class of generative models called
    Generative Adversarial Networks. Inspired by concepts of game theory, GANs present
    an implicit method of modeling the data generation probability density. We started
    the chapter by first placing GANs in the overall taxonomy of generative models
    and comparing how these are different from some of the other methods we have covered
    in earlier chapters. Then we moved onto understanding the finer details of how
    GANs actually work by covering the value function for the minimax game, as well
    as a few variants like the non-saturating generator loss and the maximum likelihood
    game. We developed a multi-layer-perceptron-based vanilla GAN to generate MNIST
    digits using TensorFlow Keras APIs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种名为生成对抗网络（GAN）的新生成模型。受博弈论概念的启发，GAN 提出了一种隐式的建模数据生成概率密度的方法。我们首先将 GAN
    放在生成模型的总体分类中，并对比了这些与我们在早期章节介绍过的其他一些方法的不同之处。然后，我们继续通过涵盖极小极大博弈的价值函数以及一些变种，如非饱和生成器损失和最大似然博弈，来了解
    GAN 实际上是如何工作的。我们使用 TensorFlow Keras API 开发了基于多层感知器的普通 GAN 来生成 MNIST 数字。
- en: In the next section, we touched upon a few improved GANs in the form of Deep
    Convolutional GANs, Conditional GANs, and finally, Wasserstein GANs. We not only
    explored major contributions and enhancements, but also built some code bases
    to train these improved versions. The next section involved an advanced variant
    called Progressive GANs. We went through the nitty-gritty details of this advanced
    setup and used a pretrained model to generate fake faces. In the final section,
    we discussed a few common challenges associated with GANs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们触及了一些改进的 GAN，如深度卷积 GAN、条件 GAN 和最后的 Wasserstein GAN。我们不仅探讨了主要的贡献和增强，还建立了一些代码库来训练这些改进的版本。下一节涉及一个称为渐进式
    GAN 的高级变体。我们深入讨论了这个高级设置的细节，并使用预训练模型生成了假面孔。在最后一节中，我们讨论了与 GAN 相关的一些常见挑战。
- en: This chapter was the foundation required before we jump into some even more
    advanced architectures in the upcoming chapters. We will cover additional topics
    in the computer vision space such as style transfer methods, face-swap/deep-fakes,
    and so on. We will also cover topics in domains such as text and audio. Stay tuned!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章是我们在后续章节中跳入更高级架构之前所需的基础。我们将涵盖计算机视觉领域的其他主题，如风格转移方法、人脸交换/深度伪造等。我们还将涵盖文本和音频等领域的主题。请继续关注！
- en: References
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *生成对抗网络*. arXiv:1406.2661\. [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
- en: 'Samples: [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)
    (left) and [https://thisartworkdoesnotexist.com/](https://thisartworkdoesnotexist.com/)
    (right)'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本：[https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)（左）和
    [https://thisartworkdoesnotexist.com/](https://thisartworkdoesnotexist.com/)（右）
- en: Adapted from *Ian Goodfellow, Tutorial on Generative Adversarial Networks, 2017*
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改编自 *Ian Goodfellow, 2017 年生成对抗网络教程*
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *生成对抗网络*. arXiv:1406.2661\. [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
- en: 'Adapted from lecture 13 CS231: [http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '改编自 CS231 讲座 13: [http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)'
- en: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv:1406.2661\.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow, I J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *生成对抗网络*. arXiv:1406.2661\. [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
- en: Radford, A., Metz, L., Chintala, S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv:1511.06434\. [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Metz, L., Chintala, S. (2015). *深度卷积生成对抗网络的无监督表示学习*. arXiv:1511.06434\.
    [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
- en: Radford, A., Metz, L., Chintala, S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv:1511.06434\. [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Metz, L., Chintala, S. (2015). *深度卷积生成对抗网络的无监督表示学习*. arXiv:1511.06434\.
    [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)
- en: Mirza, M., Osindero, S. (2014). *Conditional Generative Adversarial Nets*. arXiv:1411.1784\.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mirza, M., Osindero, S. (2014). *条件生成对抗网络*. arXiv:1411.1784\. [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
- en: Mirza, M., Osindero, S. (2014). *Conditional Generative Adversarial Nets*. arXiv:1411.1784\.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mirza, M., Osindero, S. (2014). *条件生成对抗网络*. arXiv:1411.1784\. [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)
- en: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*. arXiv:1701.07875\.
    [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*. arXiv:1701.07875\.
    [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
- en: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*. arXiv:1701.07875\.
    [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Arjovsky, M., Chintala, S., Bottou, L. (2017). *Wasserstein GAN*。arXiv:1701.07875。[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)
- en: Gulrajani, I., Ahmed, F., Arjovsky, M., Courville, A. (2017). *Improved Training
    of Wasserstein GANs*. arXiv:1704.00028\. [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gulrajani, I., Ahmed, F., Arjovsky, M., Courville, A. (2017). *改善Wasserstein
    GANs的训练*。arXiv:1704.00028。[https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). "*Progressive Growing
    of GANs for Improved Quality, Stability, and Variation*". arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). "*渐进增长的GANs以提高质量、稳定性和变化*"。arXiv:1710.10196。[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *渐进增长的GANs以提高质量、稳定性和变化*。arXiv:1710.10196。[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
- en: Bengio Y., Lamblin P., Popovici D., Larochelle H. (2006). *Greedy Layer-Wise
    Training of Deep Networks*. In Proceedings of the 19th International Conference
    on Neural Information Processing Systems (NIPS'06). MIT Press, Cambridge, MA,
    USA, 153–160\. [https://dl.acm.org/doi/10.5555/2976456.2976476](https://dl.acm.org/doi/10.5555/2976456.2976476)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bengio Y., Lamblin P., Popovici D., Larochelle H. (2006). *深度网络的贪婪逐层训练*。在第19届国际神经信息处理系统会议论文集（NIPS'06）中。MIT出版社，剑桥，MA，美国，153-160。[https://dl.acm.org/doi/10.5555/2976456.2976476](https://dl.acm.org/doi/10.5555/2976456.2976476)
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *渐进增长的GANs以提高质量、稳定性和变化*。arXiv:1710.10196。[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
- en: 'Progressive GAN official implementation: [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渐进式GAN官方实现：[https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)
- en: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *Progressive Growing of
    GANs for Improved Quality, Stability, and Variation*. arXiv:1710.10196\. [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karras, T., Aila, T., Laine, S., Lehtinen, J. (2017). *渐进增长的GANs以提高质量、稳定性和变化*。arXiv:1710.10196。[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)
