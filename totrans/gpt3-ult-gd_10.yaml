- en: 'Chapter 5: GPT-3 as the Next Step in Corporate Innovation'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第五章：GPT-3作为企业创新的下一步
- en: 'When a new innovation or technical shift happens, big corporations are usually
    the last to adopt it. Their hierarchical structures are composed of various authoritarian
    levels, and standard processes of legal approvals and paperwork often limit freedom
    to experiment, making it difficult for enterprises to be early adopters. But this
    doesn’t seem to be the case with GPT-3\. As soon as the API was released, corporations
    started experimenting with it. However, they ran into a significant barrier: data
    privacy.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个新的创新或技术转变发生时，大公司通常是最后一个采纳的。它们的等级结构由各种各样的权威级别组成，而标准的法律批准和文书工作流程经常限制了实验的自由，使得企业难以成为早期采用者。但是，对于GPT-3似乎并非如此。一旦API发布，公司就开始尝试。然而，他们遇到了一个重大障碍：数据隐私。
- en: In its simplest form, all a language model does is to predict the next word
    given a series of previous words. As you learned in Chapter 2, OpenAI has devised
    several techniques to transform the functioning of language models like GPT-3
    from simple next-word prediction to more useful NLP tasks such as answering questions,
    summarizing documents, and generating context-specific text. Typically, the best
    results are achieved by ‘fine-tuning’ a language model or conditioning it to mimic
    a particular behavior by providing it with a few examples using domain-specific
    data. You can provide examples with the training prompt, but a more robust solution
    is to create a custom-trained model using the fine-tuning API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，一个语言模型所做的就是预测给定一系列前置词的下一个词。正如您在第2章中了解到的，OpenAI已经设计了几种技术，将语言模型（如GPT-3）的功能从简单的下一个词预测转变为更有用的自然语言处理任务，例如回答问题、summarizing文档和生成特定上下文的文本。通常，通过‘微调’一个语言模型或通过提供一些使用特定领域数据的示例来调节它以模仿特定行为，可以实现最佳结果。您可以提供训练提示的示例，但更强大的解决方案是使用微调API创建自定义训练模型。
- en: OpenAI offers GPT-3 in the form of an open-ended API, where users provide input
    data and the API returns output data. Properly securing, handling, and processing
    user data is a key concern for corporations looking to use GPT-3\. OpenAI’s Welinder
    notes that, while enterprise leaders have expressed a variety of concerns about
    GPT-3, “SOC2 compliance, geofencing, and the ability to run the API within a private
    network were the biggest of them.”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI以开放式API的形式提供GPT-3，用户提供输入数据，API返回输出数据。对于希望使用GPT-3的公司来说，正确保护、处理和处理用户数据是一个关键问题。OpenAI的Welinder指出，虽然企业领导人对GPT-3表达了各种担忧，但“SOC2合规性、地理围栏和在私人网络内运行API的能力是其中最重要的。”
- en: OpenAI’s measures for model safety and misuse are thus designed to cover a wide
    range of issues under the umbrella of data privacy and security. For example, 
    Adams, founder of Stenography, tells us about the privacy and security aspects
    of the OpenAI API. “As it stands, Stenography is a pass-through API—it's like
    a toll road. So that people will pass in their code, and then receive a signal
    that says that they have used the API, and then it passes the input without saving
    or logging it anywhere.” Outside of those guardrails, Stenography is a superset
    of [OpenAI's Terms of Use](https://openai.com/api/policies/terms/).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI针对模型安全和滥用的措施因此旨在涵盖数据隐私和安全的各种问题。例如，Stenography的创始人亚当斯告诉我们关于OpenAI API的隐私和安全方面的内容。"目前，Stenography是一个直通API——就像一个收费公路。所以人们会传递他们的代码，然后收到一个信号，表示他们已经使用了API，然后它会在任何地方都不保存或记录输入。"除了这些防护栏之外，Stenography是[OpenAI使用条款](https://openai.com/api/policies/terms/)的超集。
- en: 'We talked to representatives of several corporations about what’s stopping
    them from using OpenAI API in production. Most highlighted two common concerns:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与几家公司的代表谈到了什么阻止他们在生产中使用OpenAI API。大多数人都强调了两个常见的担忧：
- en: ●        The GPT-3 API endpoint exposed by OpenAI should not retain or save
    any part of the training data provided to it as part of the model fine-tuning/training
    process.[[13]](xhtml-0-12.xhtml#aid_86)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ●        由OpenAI公开的GPT-3 API端点不应保留或保存作为模型微调/训练过程的一部分提供给它的训练数据的任何部分。
- en: ●        Before sending their data to OpenAI API, companies want to make sure
    that there’s no way for a third party to extract or access the data by providing
    any input to the API.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ●        在将其数据发送到OpenAI API之前，公司希望确保没有第三方可以通过向API提供任何输入来提取或访问数据的方式。
- en: OpenAI responded to the above customer concerns and questions around data handling
    and privacy by offering security reviews, enterprise contracts, data processing
    agreements, third-party security certification efforts, and more. Some of the
    issues that customers and OpenAI discuss include whether the customer’s data can
    be used to improve OpenAI models, which may improve performance in the customer’s
    desired use cases but comes with concerns around data privacy and internal compliance
    obligations; limits around the storage and retention of customer data, and obligations
    around security handling and processing of the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 对以上客户关注的问题和对数据处理和隐私的疑问做出了回应，提供了安全审查、企业合同、数据处理协议、第三方安全认证工作等。一些客户和 OpenAI
    讨论的问题包括客户的数据是否可以用来改进 OpenAI 模型，这可能会提高客户所需用例的性能，但也带来了数据隐私和内部合规义务方面的担忧；关于客户数据存储和保留的限制，以及对数据的安全处理和处理的义务。
- en: The rest of this chapter delves into three case studies that show how global
    enterprises like GitHub, Microsoft, and Algolia are navigating these questions
    and using GPT-3 at scale. You’ll also learn how OpenAI has adapted to the demand
    for enterprise-grade products by collaborating with Microsoft Azure’s OpenAI service.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分深入探讨了三个案例研究，展示了像 GitHub、微软和 Algolia 这样的全球企业如何应对这些问题，并在规模上使用 GPT-3。您还将了解
    OpenAI 如何通过与微软 Azure 的 OpenAI 服务合作来适应企业级产品的需求。
- en: 'Case Study: GitHub Copilot'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究：GitHub Copilot
- en: Let's start this journey with GitHub Copilot, one of the hottest products of
    2021\. GitHub Copilot (Figure 5-1) is a first-of-its-kind AI pair programmer that
    helps users write code faster and with much less work. Oege De Moor, VP of GitHub
    Next, says the mission is “to reach all developers, with an ultimate goal to make
    programming accessible to everyone.” Automating mundane tasks, like writing redundant
    code and writing unit test cases, allows developers to “focus on the truly creative
    part of the job which involves deciding what the software should actually do”
    and to “think more about the product concept rather than being stuck in figuring
    out the code.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 GitHub Copilot 开始这段旅程，它是 2021 年最热门的产品之一。GitHub Copilot（图 5-1）是一款首创的 AI
    配对编程工具，可以帮助用户更快地编写代码，减少工作量。GitHub Next 的副总裁 Oege De Moor 表示，其使命是“触及所有开发者，最终目标是使编程对每个人都可访问。”
    自动化繁琐的任务，如编写冗余代码和编写单元测试用例，使开发者可以“专注于工作中真正有创造力的部分，即决定软件实际应该做什么”，并且可以“更多地思考产品概念，而不是陷在编码中无法自拔。”
- en: 'As Awan told us: “I''m excited to work on more side projects now, because I
    know I''ll have the help of GitHub Copilot. It''s almost like I have a co-founder
    now. Codex and copilot are writing 2 to 10% of my code, something like that. So
    it has already made me 2 to 10% more accelerated. And all of this is on an exponential
    scale. So what will GPT-3 be like next year? What will Codex be like next year?
    I may be 30% more accelerated.” Let’s dive into the inner workings of the Copilot.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Awan 告诉我们的：“我现在很兴奋地参与更多的副业项目，因为我知道我将有 GitHub Copilot 的帮助。这几乎就像我现在有了一个联合创始人。Codex
    和 copilot 正在写我的代码的 2 到 10%，大约是这样。所以它已经让我加速了 2 到 10%。而且所有这些都是按指数增长的。那么明年 GPT-3
    会是什么样子？明年 Codex 会是什么样子？我可能加速了 30%。” 让我们深入了解 Copilot 的内部工作原理。
- en: '![](img/image-0-26.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-26.jpg)'
- en: Figure 5-1\. GitHub Copilot
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-1\. GitHub Copilot
- en: How it works
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理
- en: GitHub Copilot draws context from the code you're working on, based on things
    like docstrings, comments, and function names.[[14]](xhtml-0-12.xhtml#aid_77)
    It then automatically suggests the next line, or even entire functions, right
    inside your editor to produce boilerplate code and suggest test cases that match
    the code implementation. works with a broad set of frameworks and programming
    languages by using a plugin to the user’s code editor, making it nearly language-agnostic
    as well as lightweight and easy to use.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 从您正在处理的代码中提取上下文，基于 docstrings、注释和函数名称等信息。[[14]](xhtml-0-12.xhtml#aid_77)
    然后，在您的编辑器中自动建议下一行，甚至整个函数，以生成样板代码并建议与代码实现相匹配的测试用例。通过使用插件到用户代码编辑器的方式，它可以与广泛的框架和编程语言一起使用，使其几乎是语言无关的，同时又轻量级且易于使用。
- en: 'OpenAI research scientist Harri Edwards notes that Copilot is also a useful
    tool for programmers working in a new language or framework: “Trying to code in
    an unfamiliar language by googling everything is like navigating a foreign country
    with just a phrasebook. Using GitHub Copilot is like hiring an interpreter.”[[15]](xhtml-0-12.xhtml#aid_30)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 研究科学家 Harri Edwards 指出，Copilot 也是对于使用新语言或框架编程的程序员的一个有用工具：“试图通过 Google
    搜索一切来编写不熟悉的语言的代码就像只带着一本小短语手册在一个陌生的国家中航行。使用 GitHub Copilot 就像雇佣了一位翻译。”
- en: GitHub Copilot is powered by OpenAI’s Codex, a descendant of the GPT-3 model
    that, as we noted in chapter 4, is designed specifically to interpret and write
    code. “GitHub is home to more than 73 million developers, which includes a massive
    amount of public data that embodies the collective knowledge of the community,”
    says De Moor. That translates to billions of lines of publicly available code
    for Codex to train on. It understands both programming and human languages.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 由 OpenAI 的 Codex 提供支持，Codex 是 GPT-3 模型的一个后裔，正如我们在第四章中所提到的，它专门设计用于解释和编写代码。“GitHub
    是超过 7300 万开发者的家园，其中包括了大量体现了社区集体知识的公开数据，”De Moor 说道。这意味着 Codex 有数十亿行公开可用的代码供其训练。它理解编程语言和人类语言。
- en: 'Codex draws on supporting comments or instructions in simple English to come
    up with relevant code as seen in Figure 5-2\. The Copilot editor extension intelligently
    chooses which context to send to the GitHub Copilot service, which in turn runs
    OpenAI’s Codex model to synthesize suggestions. Even though Copilot generates
    the code, users are still in charge: you can cycle through suggested options,
    choose which to accept or reject, and manually edit the suggested code. GitHub
    Copilot adapts to the edits you make and matches your coding style. De Moor explains,
    “It links natural language with source code so you can use it in both directions.
    You can use the source code to generate comments or you can use the comments to
    generate the source code, making it immensely powerful.”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Codex 根据简单英语中的支持性注释或说明提出相关的代码，如图 5-2 所示。Copilot 编辑器扩展智能地选择要发送到 GitHub Copilot
    服务的上下文，后者再运行 OpenAI 的 Codex 模型来综合建议。尽管 Copilot 生成了代码，但用户仍然处于控制之中：你可以循环浏览建议的选项，选择接受或拒绝，以及手动编辑建议的代码。GitHub
    Copilot 会适应你所做的编辑，并匹配你的编码风格。De Moor 解释道，“它将自然语言与源代码联系起来，因此你可以在两个方向上使用它。你可以使用源代码生成注释，也可以使用注释生成源代码，使其具有极大的强大性。”
- en: '![](img/image-0-27.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-27.jpg)'
- en: Figure 5-2\. Working of GitHub Copilot
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-2\. GitHub Copilot 的工作原理
- en: This functionality has also indirectly changed how developers write code. When
    they know that their code comments in human languages, like English, will be part
    of the model’s training, they write “better and more accurate comments in order
    to get better results from Copilot,” says De Moor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这项功能也间接改变了开发者编写代码的方式。当他们知道他们的代码注释在人类语言，比如英语中，将成为模型训练的一部分时，他们会写“更好更准确的注释，以获得
    Copilot 更好的结果，”De Moor 表示。
- en: Many critics worry that giving this tool in the hands of people who can’t judge
    the quality of code may result in introducing bugs or errors in the codebase.
    Contrary to that opinion De Moor tells us, “We have received a lot of feedback
    from developers that Copilot makes them write better and more efficient code.”
    In the current technical preview, Copilot can only help you write code if you
    understand how different pieces in software work, where you can precisely tell
    Copilot what it is that you want it to do. Copilot encourages healthy developer
    practices, like writing more accurate comments, and rewards developers with better
    code generation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多批评者担心将这个工具交给不能判断代码质量的人可能会导致代码库中引入 bug 或错误。与此相反，De Moor 告诉我们，“我们收到了很多开发者的反馈，说
    Copilot 让他们写出更好更高效的代码。”在当前的技术预览版中，只有当你理解软件中不同部分如何工作时，Copilot 才能帮助你编写代码，你可以准确地告诉
    Copilot 你希望它做什么。Copilot 鼓励健康的开发者实践，比如编写更准确的注释，并用更好的代码生成奖励开发者。
- en: Copilot is not just limited to the general rules of programming but it can also
    figure out the details of specific fields, such as writing programs to compose
    music. To do that you need to understand music theory to write such programs “Seeing
    how Copilot has somehow picked it up from its immensely large training data is
    just amazing,” De Moor adds.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 不仅仅局限于编程的一般规则，而且还可以找出特定领域的细节，比如编写作曲程序。为了做到这一点，你需要理解音乐理论来编写这样的程序，“从它庞大的训练数据中某种程度上学到这一点的
    Copilot 实在令人惊讶，” De Moor 补充道。
- en: Developing Copilot
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 的开发
- en: De Moor says one of the challenges of designing Copilot was creating the right
    user experience, one that “lets you use this model in a collaborative way without
    being intrusive.” The goal is for it to feel like working with a programming partner
    or coworker who “knows more about the mundane coding stuff so you can focus more
    on creating the important stuff.” Developers are constantly searching for existing
    solutions to problems and often refer to StackOverflow, search engines, and blogs
    to find implementation and code syntax details– which means lots of moving back
    and forth between editor and browser. As De Moor points out, “As a developer,
    you are more productive when you can stay in your environment and just think about
    the problem rather than switching context all the time.” This is why GitHub’s
    team designed Copilot to deliver suggestions inside the development environment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: De Moor 表示 Copilot 设计的一个挑战是创建正确的用户体验，一个“让您以一种不具侵入性的方式共同使用此模型”的体验。目标是让它感觉像是与一个编程伙伴或同事一起工作，他“更了解琐碎的编码内容，因此您可以更专注于创建重要的内容。”
    开发者不断地在寻找现有问题的解决方案，通常会参考 StackOverflow、搜索引擎和博客以查找实现和代码语法细节，这意味着编辑器和浏览器之间来回移动。正如
    De Moor 指出的那样，“作为开发者，当您可以留在自己的环境中，只需考虑问题而不是一直切换上下文时，您会更有生产力。” 这就是为什么 GitHub 团队设计
    Copilot 以在开发环境内提供建议的原因。
- en: What does low-code/no-code programming mean?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 低代码/无代码编程是什么意思？
- en: Right now, developing software-related products or services requires a technical
    or scientific background—for example, you have to learn at least one programming
    language. And that’s just a start. Even to develop a minimum viable product (MVP)
    with conventional techniques you have to understand the different elements of
    software engineering involved in developing both the frontend (how the user interacts
    with the software) and the backend (how the processing logic works). This creates
    a barrier to entry for those who don’t come from a technical or engineering background.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开发与软件相关的产品或服务需要技术或科学背景——例如，您必须至少学习一种编程语言。而这只是一个开始。甚至要用传统技术开发最小可行产品（MVP），您也必须了解开发前端（用户如何与软件交互）和后端（处理逻辑如何工作）所涉及的软件工程的不同要素。这给那些没有技术或工程背景的人造成了进入的障碍。
- en: De Moor sees Copilot as a step toward making technology more accessible and
    inclusive. If developers “have to worry less and less about the development details
    and just explain the design, explain the purpose of what [they] want to do,” and
    let Copilot handle the details, many more people will be able to use these tools
    to create new products and services.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: De Moor 将 Copilot 视为使技术更易接触和包容的一步。如果开发者“越来越不用担心开发细节，只需解释设计，解释[他们]想要做的事情的目的”，让
    Copilot 处理细节，那么更多的人将能够使用这些工具来创建新产品和服务。
- en: There are already several no-code programming platforms, but many users find
    their limits  constricting, in essence, “heavily simplifying the programming experience”
    by making it “more visual, more graphical, and easy to use,” according to de Moor.
    “These things are great to get started but unfortunately, it comes up with a limit
    on the things that are possible to build using those platforms.” De Moor argues
    that Copilot is equally easy to use but provides far more options by using fully
    operational programming tools rather than simplified versions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有几个无代码编程平台，但许多用户发现它们的限制限制很大，本质上是通过“大大简化编程体验”，使其“更可视化、更图形化、更易于使用”，根据 De Moor
    的说法。 “这些东西很适合入门，但不幸的是，它们在使用这些平台构建的事物上有一个限制。” De Moor 认为，Copilot 使用完全操作性的编程工具而不是简化版本，同样易于使用，但提供了更多选项。
- en: Scaling with the API
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 API 进行扩展
- en: Scaling in terms of language models has been undervalued for so long because
    of theoretical concepts like [Occam’s Razor](https://en.wikipedia.org/wiki/Occam%27s_razor)
    and vanishing results when you expand the neural network to a significant size.
    With conventional deep learning, it has always been a norm to keep the model size
    small with fewer parameters to avoid the problem of vanishing gradients and introducing
    complexity in the model training process. Occam’s Razor which means - “A simple
    model is the best model” has been sacred in the AI community since its inception.
    It has been a center of reference for training new models which restricted people
    from experimenting with scale.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就语言模型而言，长期以来都低估了扩展的潜力，因为像[奥卡姆剃刀](https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80)这样的理论概念以及当你将神经网络扩展到相当规模时结果消失的问题。使用传统的深度学习，一直以来都是保持模型规模小，参数较少，以避免梯度消失问题，并在模型训练过程中引入复杂性。奥卡姆剃刀的原则是
    - “一个简单的模型是最好的模型”，自其诞生以来一直是人工智能社区的圣经。它一直是训练新模型的参考中心，限制了人们尝试扩展的范围。
- en: In 2020, when OpenAI released its marquee language model GPT-3, the potential
    of scaling came into the limelight. This was the time when the common conception
    of the AI community started to shift and people started realizing that the "gift
    of scale" can give rise to a more generalized artificial intelligence where a
    single model like GPT-3 can perform an array of tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年，当 OpenAI 推出其标志性语言模型 GPT-3 时，扩展的潜力开始受到关注。这是人工智能社区的共识开始转变，人们开始意识到“规模的恩赐”可以催生出更广义的人工智能，其中像
    GPT-3 这样的单一模型可以执行一系列任务。
- en: Hosting and managing a model like GPT-3 requires sophistication on many different
    levels, including the optimization of model architecture, its deployment, and
    how the general public can access it. De Moor tells us, “When we launched Copilot,
    it was using the OpenAI API infrastructure in the initial phases and then we had
    this explosion of response after the launch with so many people signing up and
    wanting to use the product.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 托管和管理像 GPT-3 这样的模型需要在许多不同层面上进行复杂的处理，包括优化模型架构、部署以及公众如何访问它。De Moor 告诉我们，“当我们推出
    Copilot 时，在最初阶段使用的是 OpenAI API 基础设施，然后在推出后，我们迎来了大量用户的回应，有很多人注册并希望使用该产品。”
- en: Although the API was capable of handling large amounts of requests, the number
    of requests and their frequency still surprised the OpenAI team. De Moor and his
    team “realized the need of a more efficient and bigger infrastructure for deployment,
    and fortunately, it was about time that Microsoft Azure OpenAI came to light”
    allowing them to make the required switch to Azure deployment infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 API 能够处理大量的请求，但请求的数量和频率仍然让 OpenAI 团队感到惊讶。De Moor 和他的团队“意识到了部署需要更高效和更大规模基础设施的需求，幸运的是，这正是微软
    Azure OpenAI 被提出的时候”，使他们能够进行所需的切换到 Azure 部署基础设施。
- en: When we asked about the experience of building and scaling Copilot, De Moor
    shares, “Early on we had this misled belief that accuracy is the single most important
    thing that matters but sometime later into the product journey, we realized that
    it’s actually a trade-off between the powerful AI model and flawless user experience.”
    The Copilot team quickly realized that there is a trade-off between speed and
    the accuracy of suggestions as is the case with any deep learning model of sufficient
    scale.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们询问 De Moor 关于构建和扩展 Copilot 的经验时，他分享道，“早期我们错误地认为准确性是最重要的事情，但在产品发展的某个时候，我们意识到这实际上是强大的
    AI 模型和无缺陷用户体验之间的权衡。” Copilot 团队很快意识到，在任何足够规模的深度学习模型中，速度和建议的准确性之间存在权衡。
- en: 'Generally, the more layers a deep learning model has, the more accurate it
    will be. However, more layers also means it will be slower to run. The Copilot
    team had to somehow find a balance between the two, as de Moor explains: “Our
    use case required the model to deliver the response at lightning-fast speed with
    multiple alternative suggestions; if it’s not fast enough, users can easily outpace
    the model and write the code themselves. So, we found that a slightly less powerful
    model that gives the responses quickly while maintaining the quality of results”
    was the answer.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，深度学习模型的层数越多，准确性就越高。然而，更多的层数也意味着运行速度会变慢。Copilot 团队必须在两者之间找到平衡，正如 de Moor 所解释的：“我们的用例要求模型以极快的速度提供响应，并提供多个备选建议；如果速度不够快，用户很容易超过模型并自行编写代码。因此，我们发现，一个稍微不那么强大但能够快速提供响应并保持结果质量的模型”是答案。
- en: The rapid user adoption and interest in GitHub Copilot took everyone in the
    team by surprise but it just didn’t end there. Because of the usefulness of the
    product and the quality of code suggestions the team saw exponential growth in
    the amount of code generated using Copilot where “on an average, 35% of newly
    written code is being suggested by Copilot. This number will increase going forward
    as we get closer to finding the right balance between model capabilities and the
    speed of suggestions” says De Moor.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 的快速用户采用和兴趣让团队中的每个人都感到意外，但事情并没有就此结束。由于产品的用处和代码建议的质量，团队看到使用 Copilot
    生成的代码量呈指数增长，其中“平均而言，35% 的新编写的代码是由 Copilot 建议的。随着我们接近找到模型能力和建议速度之间的正确平衡，这个数字还将继续增长。”De
    Moor 说。
- en: When asked about the data security and privacy aspect of code submitted as part
    of the request to Copilot Moor tells us, “Copilot architecture is designed in
    a way that when a user types the code into the Copilot, there would not be any
    possibility of code leaking between one user to another. GitHub Copilot is a code
    synthesizer rather than a search engine, as it generates the majority of its suggestions
    based on unique algorithms. In rare cases, approximately 0.1% of suggestions may
    include snippets that are identical to those found in the training set.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当被问及作为请求的一部分提交的代码的数据安全性和隐私方面时，Moor 告诉我们，“Copilot 的架构设计使得当用户输入代码到 Copilot 时，不会有任何可能的代码泄霩到另一个用户之间。GitHub
    Copilot 是一个代码合成器而不是搜索引擎，它根据独特的算法生成大部分建议。在极少数情况下，大约 0.1% 的建议可能包含与训练集中发现的代码片段相同的片段。”
- en: What’s Next for Github Copilot?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 的未来展望是什么？
- en: De Moor sees a great potential for Copilot to assist in code review as well
    as writing. “Think of an automated code reviewer where it automatically looks
    at your changes and makes suggestions to make your code better and more efficient.
    The code review process at GitHub today consists of human reviewers, and we’re
    also exploring the idea of Copilot reviews.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ' De Moor 认为 Copilot 在代码审查和编写方面有很大的潜力。 “想象一下自动代码审查员，它会自动查看您的更改并提出建议，使您的代码更好、更高效。
    GitHub 的代码审查过程今天由人类审查员完成，我们也在探索 Copilot 审查的想法。”'
- en: Another feature under exploration is code explanation. De Moor explains that
    users can select a code snippet and “Copilot can explain it in simple English.”
    This has the potential as a useful learning tool. In addition, De Moor says, Copilot
    hopes to provide tools that assist in “translation of code from one programming
    language to another.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个正在探讨的功能是代码解释。 De Moor 解释说，用户可以选择代码片段，“Copilot 可以用简单的英语解释它。” 这具有潜力成为一个有用的学习工具。此外，De
    Moor 表示，Copilot 希望提供辅助“将代码从一种编程语言翻译为另一种”的工具。
- en: Copilot has opened the world of unlimited opportunities for not just the developers
    but for anyone who wants to get creative and build a piece of software to bring
    their ideas to reality. Prior to GitHub Copilot and OpenAI’s Codex, features like
    generating production-grade code, AI-assisted code review, and the translation
    of code from one language to another has been a far-fetched dream before the release.
    The advent of Large Language Models combined with no-code and low-code platforms
    will enable people to unleash their creativity and build interesting and unexpected
    applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 打开了无限可能的世界，不仅仅是为开发人员，还为任何想要创造并构建一个软件来实现他们的想法的人。在 GitHub Copilot 和 OpenAI
    的 Codex 之前，生成生产级代码、AI 辅助代码审查以及将代码从一种语言翻译为另一种语言的功能一直是一个遥不可及的梦想。大型语言模型的出现结合了无代码和低代码平台，将使人们释放创造力，并构建有趣和意想不到的应用程序。
- en: 'Case Study: Algolia Answers'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究：Algolia Answers
- en: Algolia is a renowned search solutions provider with clients spanning from Fortune
    500 companies to a new generation of startups. It offers a symbolic, keyword-based
    search API that can be integrated with any existing product or application. In
    2020, Algolia partnered with OpenAI to connect GPT-3 with its already existing
    search technology. The next-generation product offering resulted in Algolia Answers,
    which enables clients to build an intelligent, semantics-driven single-search
    endpoint for search queries. “We build the technology that other companies use,”
    says Dustin Coates, Product Manager at Algolia.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia是一家著名的搜索解决方案提供商，客户群从财富500强公司到新一代初创公司。它提供一个符号化的，基于关键词的搜索API，可与任何现有产品或应用程序集成。在2020年，Algolia与OpenAI合作，将GPT-3与其已经存在的搜索技术连接起来。下一代产品推出导致Algolia
    Answers的产生，该产品使客户能够构建一个智能的、以语义驱动的单一搜索终点，用于搜索查询。Algolia产品经理Dustin Coates表示：“我们开发其他公司使用的技术。”
- en: Coates says that what his team means by intelligent search is along the lines
    of “You search for something and you get back the response right away—not just
    you get back to the record, you get back to the article—but you get back to what's
    actually answering the question.” In short, it’s “ a search experience where people
    don’t have to type exactly what the words are.”
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Coates表示，他的团队所谓的智能搜索是指“您搜索某物并立刻收到回应 - 不仅是回到记录，不仅是回到文章 - 而是回到实际回答问题的内容。”简言之，这是“人们不必准确输入单词的搜索体验。”
- en: Evaluating NLP options
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 评估NLP选项
- en: Algolia set up a dedicated team to work in this area, of which Claire Helme-Guizon
    was an early member. When OpenAI reached out to them to see if Algolia might be
    interested in GPT-3, Coates’s team compared it to competing technologies. Algolia
    ML engineer Claire Helme-Guizon, a member of the original Algolia Answers team,
    explains, “We worked on BERT-like models, to optimize for speed, DistilBERT, and
    with more stable models like RoBERTa along with different variants of GPT-3 like
    DaVinci, Ada, etc.” They created a rating system to compare the quality of different
    models and understand their strengths and weaknesses. They found Coates says that
    “it performed really well in terms of the quality of the search results returned.”
    Speed and cost were weaknesses, but the API was ultimately a deciding factor since
    it allowed Algolia to use the model without having to maintain its infrastructure.
    Algolia asked existing clients whether they might be interested in such a search
    experience, and the response was very positive.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia成立了一个专门团队来在这个领域工作，Claire Helme-Guizon是最初的成员之一。当OpenAI与他们联系，询问Algolia是否对GPT-3感兴趣时，Coates的团队将其与竞争技术进行了比较。Algolia
    ML工程师Claire Helme-Guizon是最初Algolia Answers团队的成员之一，解释说：“我们研究了类似BERT的模型，以优化速度，DistilBERT，以及更稳定的像RoBERTa之类的模型，以及GPT-3的不同变体如DaVinci，Ada等。”
    他们创建了一个评分系统来比较不同模型的质量，并了解它们的优势和劣势。他们发现，Coates表示“它在检索结果质量方面表现得非常出色。” 速度和成本是弱点，但API最终是决定因素，因为它允许Algolia使用模型而无需维护基础设施。
    Algolia询问现有客户是否对这样的搜索体验感兴趣，反馈非常积极。
- en: 'Even with that quality of results, Algolia still had plenty of questions: How
    would it work for the customers? Would the architecture be scalable? Was it financially
    feasible? To answer them, Coates explains, “We sculpted specific use cases that
    had longer textual content,” such as publishing and help desks.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有了这样优质的结果，Algolia仍然有很多问题：它如何适用于客户？架构是否可扩展？财政上是否可行？为了回答这些问题，Coates解释说：“我们设计了具有更长文本内容的特定用例”，例如出版和帮助台。
- en: For some use cases, it’s good enough to rely solely on GPT-3 to get the search
    results, but for other complex use cases, you may need to integrate GPT-3 with
    other models. GPT-3, being trained on the data up to a certain point in time,
    struggles with use cases involving freshness, popularity, or personalized results.
    When it comes to the quality of results, the Algolia team was challenged by the
    fact that semantic similarity scores generated by GPT-3 were not the only metric
    that mattered to their customers. They need to somehow blend the similarity scores
    with other measures to ensure that the clients get satisfactory results. So, they
    introduced other open-source models to highlight the best results in combination
    with GPT-3.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些使用案例，仅依靠GPT-3来获取搜索结果就足够了，但对于其他复杂的使用案例，您可能需要将GPT-3与其他模型集成。由于GPT-3只是在某个时间点之前的数据上进行训练，因此在涉及新鲜度、流行度或个性化结果的使用案例中，它会遇到困难。在结果质量方面，Algolia团队面临的挑战是，GPT-3生成的语义相似性得分并不是他们的客户关心的唯一指标。他们需要以某种方式将相似性得分与其他指标结合起来，以确保客户获得满意的结果。因此，他们引入了其他开源模型，以与GPT-3结合以突出最佳结果。
- en: Data Privacy
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私
- en: The biggest challenges Algolia faced while introducing this novel technology,
    Coates says, were legal ones. “Getting through legal and security and procurements
    was maybe the hardest thing we did in this entire project because you're sending
    this customer data and it's feeding this ML model. How do we delete that data?
    How do we make sure it's GDPR compliant?[[16]](xhtml-0-12.xhtml#aid_70) How do
    we handle all of these things? How do we know that OpenAI isn't going to take
    this data and feed everyone else's model with it as well? So there were a lot
    of questions that needed to be answered and a lot of agreements that needed to
    be put into place.”
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia在引入这项新技术时面临的最大挑战，Coates表示，是法律方面的问题。“在整个项目中，通过法律、安全和采购可能是我们做的最困难的事情，因为你正在发送这些客户数据，它正在为这个ML模型提供数据。我们如何删除这些数据？我们如何确保它符合GDPR的要求？我们如何处理所有这些事情？我们怎么知道OpenAI不会拿这些数据并用它来喂其他人的模型呢？因此，有很多问题需要回答，有很多协议需要制定。”
- en: Cost
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: Most of the GPT-3 use cases that we’ve seen so far are business-to-consumer
    (B2C) products, but for a business-to-business (B2B) company like Algolia, the
    game is different. Not only do they need OpenAI’s pricing to work for them, but
    they also need to optimize their pricing for clients, so that  “we can be profitable
    and have customers still be interested in what we’re building.”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们见过的大多数GPT-3使用案例都是面向消费者的产品，但对于像Algolia这样的企业对企业（B2B）公司来说，情况就不同了。他们不仅需要OpenAI的定价适用于他们，而且还需要优化他们的定价以满足客户的需求，以便“我们可以盈利并且客户仍然对我们所构建的内容感兴趣。”
- en: In the search solutions business, success is measured on the basis of throughputs.
    So it naturally makes sense to think about the tradeoff between quality, cost,
    and speed. Coates says, “Even before we knew the costs, Ada was the right model
    for us because of the speed. But even if, let's say, Davinci was fast enough,
    we may have still gotten down to Ada just because of the cost measures.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索解决方案业务中，成功是以吞吐量为基础来衡量的。因此，自然而然地会考虑质量、成本和速度之间的权衡。Coates说：“即使在我们知道成本之前，Ada对我们来说也是正确的模型，因为速度很快。但是即使，比如说，Davinci足够快，我们也可能因为成本问题而最终选择Ada。”
- en: Helme-Guizon notes that the factors affecting the cost include, “the number
    of tokens, and the number of documents you are sending and their length.” Algolia’s
    approach was to build “the smallest possible context windows”--meaning the amount
    of data sent to the API at a time–that would still be “relevant enough in terms
    of quality.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Helme-Guizon指出，影响成本的因素包括“代币数量以及您发送的文档数量和长度。” Algolia的方法是构建“尽可能小的上下文窗口”——这意味着一次发送到API的数据量足够“质量上还是足够相关的”。
- en: So how did they solve this problem? “We started with OpenAI before they had
    announced pricing and we had gone far enough and saw that the quality was good
    enough from what we could see elsewhere, without knowing what the pricing was.
    So it was quite some sleepless nights, not knowing what the pricing was. And then
    once we knew the pricing, figuring out how to bring that cost down. Because when
    we first saw the pricing, we weren't sure if we were going to make it work.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么他们是如何解决这个问题的呢？“我们在OpenAI宣布定价之前就开始使用了，我们已经做了很多工作，并且看到了质量足够好，而我们又不知道定价是多少。所以那段时间我们睡不好觉，不知道定价是多少。然后一旦我们知道了定价，就要想办法降低成本。因为当我们第一次看到定价时，我们不确定我们是否能够承受。”
- en: They did put a lot of work into optimizing the price for their use case as according
    to Coates pricing will be “a universal challenge” for everyone trying to build
    their business on top of it. So, it is highly recommended to start thinking about
    price optimization in the very early stages of product development.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 他们确实在优化价格方面做了很多工作，因为根据Coates的说法，定价将对每个试图在其基础上构建业务的人来说都是一个“普遍的挑战”。因此，强烈建议在产品开发的早期阶段开始考虑价格优化。
- en: Speed and Latency
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 速度和延迟
- en: Speed is of particular importance to Algolia; the company promises its clients
    lightning-fast search capabilities with delays limited to just milliseconds. When
    the team evaluated Open AI’s proposal, they were happy with the quality of results,
    but GPT-3’s latency was just unacceptable. “In our traditional search, the results
    come back round trip in less than 50 milliseconds,” Coates says. “We're searching
    across hundreds of millions of documents and it has to be in real-time. When we
    worked with OpenAI early on, each of those queries took minutes.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 速度对Algolia尤为重要；该公司承诺为其客户提供闪电般快速的搜索能力，延迟仅限于毫秒级别。当团队评估Open AI的提案时，他们对结果的质量感到满意，但是GPT-3的延迟是无法接受的。Coates说：“在我们的传统搜索中，结果往返时间少于50毫秒。”“我们在数亿个文档中进行搜索，必须是实时的。当我们早期与OpenAI合作时，每个查询都需要几分钟。”
- en: Algolia did decide to give GPT-3 a shot and began an initial phase of experimentation
    and beta rollout for Algolia Answers. However, bringing down latency and monetary
    costs required a lot of effort. “We started out at around 300 milliseconds, sometimes
    400, total latency, which we have to bring down to somewhere in the range of 50
    to 100 milliseconds for it to be feasible for our clients to use.” Ultimately,
    Algolia came up with semantic highlighting, a technique that uses a trained Question-Answering
    model on top of GPT-3 to perform mini searches and figure out the correct answer.
    The combination of GPT-3 with other open-source models resulted in reduced overall
    latency. The quality of their results are better, Helme-Guizon adds, because “the
    models are trained to find the answers, not just the words that are related to
    one another.”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia确实决定尝试GPT-3，并开始了Algolia Answers的初始实验和测试阶段。然而，为了降低延迟和成本，需要付出很多努力。 “我们开始时的总延迟约为300毫秒，有时候到400毫秒，我们必须将其降低到50到100毫秒的范围内，以使我们的客户能够使用。”最终，Algolia提出了语义突出显示，这是一种使用在GPT-3之上的训练问答模型的技术，用于执行迷你搜索并找出正确答案。结合GPT-3和其他开源模型，可以减少总体延迟。Helme-Guizon补充说，他们的结果质量更好，因为“这些模型被训练来找到答案，而不仅仅是相关的单词。”
- en: A key aspect of Algolia Answers’ architecture, Helme-Guizon says, is reader
    retrieval architecture, in which an AI reader is “going through the subset of
    documents and reading them, understanding them with reference to the query using
    Ada, and giving us a confidence score for the semantic value.” While this was
    “a good first solution,” she adds, it has a lot of challenges– “especially latency,
    because you have that dependency where you cannot process the first batch and
    the second batch together” asynchronously.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia Answers体系结构的一个关键方面，Helme-Guizon说，是读者检索体系结构，在这个体系结构中，一个AI读者会“浏览子集文件，并阅读它们，借助Ada理解它们，并为语义值给出信心分数。”虽然这是一个“不错的解决方案”，但是她补充说，它有很多挑战-“特别是延迟问题，因为你有一个依赖关系，无法异步处理第一批和第二批”。
- en: 'GPT-3 was using the embedding from the predictions to compute cosine similarity,
    a mathematical metric used to determine how similar two documents are, irrespective
    of their size. Coates sums up these challenges: First, “you can''t send too many
    documents or else the response is going to be too slow or the cost is going to
    be too high monetarily.”  The second is casting “a net wide enough to fetch all
    the relevant documents while keeping time and costs under control.”'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3使用预测结果的嵌入来计算余弦相似度，这是一种用于确定两篇文档相似程度的数学度量，与其大小无关。Coates总结了这些挑战：首先，“你不能发送太多文件，否则响应速度将太慢，或者成本将过高”。
    第二个挑战是“在保持时间和成本可控的同时，将网梳得足够宽以获取所有相关文档”。
- en: Lessons Learned
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 启示与教训
- en: So, if Algolia Answers have to start from scratch today, what would they do
    differently? “Working with GPT-3 can be overwhelming at times,” Coates says. “We
    would have asked some of the first-principle questions in the early stages of
    your product development like, ‘Are we willing to take a hit in terms of semantic
    understanding because we take such an increase for everything else?’ I think we
    would have thought a lot more about the latency and the confluence of different
    ranking factors early on.” He adds that he could see the project “going back to
    a BERT-based model. We might say that the raw quality isn't the same as what we're
    going to get out of GPT-3\. There's no denying that. ​​But I think that as much
    as we fell in love with the technology, we uncovered customer problems that we
    weren't solving, and the technology has to follow the customer problems, not the
    other way around.”
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果 Algolia Answers 今天必须从零开始，他们会有什么不同的做法呢？“与 GPT-3 一起工作有时会让人不知所措，”Coates 说。“我们会在产品开发的早期阶段提出一些原则性问题，比如‘我们是否愿意在语义理解方面有所损失，因为我们在其他方面的提升大了许多？’我认为我们在初期应该更多地考虑延迟和不同排序因素的融合。”他补充说，他可以设想这个项目“回到基于
    BERT 模型。我们可能会说，原始质量与我们从 GPT-3 得到的不一样。这是不可否认的。但是我认为，尽管我们爱上了这项技术，但我们发现了一些我们没有解决的客户问题，技术必须跟随客户问题，而不是相反。”
- en: So what is Algolia’s take on the future of search? “We don't believe that anyone
    has truly solved blending textual relevance and semantic relevance. It's a very
    difficult problem because you can have situations where things are textually relevant,
    but don't really answer the question,” says Coates. He envisions “a marriage of
    the more traditional, textual base, the more understandable and explainable side
    of it, with these more advanced language models.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 Algolia 对搜索的未来有何看法？“我们认为没有人真正解决了文本相关性和语义相关性的融合问题。这是一个非常困难的问题，因为有时虽然事物在文本上相关，却并不能真正回答问题，”Coates
    说道。他设想“将更传统的、文本基础、更可理解和可解释的一面与这些更高级的语言模型结合起来。”
- en: 'Case Study: Microsoft’s Azure OpenAI Service'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究：微软的 Azure OpenAI 服务
- en: Algolia has matured on the OpenAI API, but soon they wanted to expand their
    business in Europe– which meant they needed GDPR compliance. They began working
    with Microsoft, which was launching its Azure OpenAI Service. In the next case
    study, we’ll take a look at that service.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia 在 OpenAI API 上已经成熟，但很快他们想要扩展他们的业务到欧洲 - 这意味着他们需要遵守 GDPR合规性。他们开始与微软合作，微软当时正在推出
    Azure OpenAI 服务。在下一个案例研究中，我们将了解该服务。
- en: A Partnership That Was Meant to Be
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一场注定的合作伙伴关系
- en: Microsoft and OpenAI announced a partnership in 2019, with the goal of giving
    Microsoft Azure customers access to GPT-3’s capabilities. The partnership is based
    on the shared vision of wanting to ensure that AI and AGI are deployed safely
    and securely. Microsoft invested a billion dollars in OpenAI, funding the launch
    of the API, which runs on Azure. The partnership culminates in shipping the API
    to provide more people access to large language models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 微软和 OpenAI 在2019年宣布合作伙伴关系，目标是让微软 Azure 的客户能够使用 GPT-3 的能力。这个合作伙伴关系基于双方对确保人工智能和通用人工智能的安全部署的共同愿景。微软对
    OpenAI 进行了10亿美元的投资，为 API 的推出提供了资金支持，该 API 运行在 Azure 上。这个合作伙伴关系最终导致了 API 的发布，以使更多人能够访问大型语言模型。
- en: Dominic Divakaruni, Principal Group Product Manager and Head of Azure OpenAI
    Service, says he’s always thought of this collaboration as a partnership that
    feels like it was meant to be, noting that Microsoft CEO Satya Nadella and OpenAI
    CEO Sam Altman have both spoken often about ensuring that the benefits of AI are
    accessible and widely distributed. Both companies are also concerned with safety
    in AI innovation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Dominic Divakaruni，Azure OpenAI 服务总产品经理兼负责人表示，他一直认为这种合作伙伴关系就像是注定的合作伙伴关系，他指出微软
    CEO Satya Nadella 和 OpenAI CEO Sam Altman 都经常谈到确保 AI 的好处是可访问和广泛分发的。两家公司都关注 AI
    创新的安全性。
- en: The goal, Divakaruni says, “was to leverage each other's strengths, ”in particular
    Open AI’s user experience and modeling progress and Microsoft's existing relationships
    with companies, large salesforce, and cloud infrastructure.  Given its customer
    base, Microsoft Azure understands enterprise cloud customers’ fundamental requirements
    in terms of compliance, certifications, network security, and related issues.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni表示，目标是“利用彼此的优势”，特别是OpenAI的用户体验和建模进展以及Microsoft与公司、大型销售人员和云基础设施之间的现有关系。鉴于其客户基础，Microsoft
    Azure在合规性、认证、网络安全和相关问题方面了解企业云客户的基本要求。
- en: For Microsoft, the interest in GPT-3 begins largely with it breaking new ground
    and being available before any other model from the LLM category. Another crucial
    factor in Microsoft's investment is that it gained the ability to use OpenAI’s
    Intellectual Property assets exclusively. Although GPT-3 alternatives are available,
    Divarakuni says that the centralization of the OpenAI API is unique. He notes
    that models for services like text analytics or translation require “quite a bit
    of work” on a cloud provider’s part to adapt into an API service. OpenAI, however,
    offers “the same API used for various tasks” rather than “bespoke API that are
    created for particular tasks.”
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Microsoft来说，对GPT-3的兴趣主要在于它在LLM类别中独占鳌头，并在其他任何模型之前提供。Microsoft投资的另一个关键因素是其获得了独家使用OpenAI的知识产权资产的能力。虽然存在GPT-3的替代方案，但Divarakuni表示，OpenAI
    API的中心化是独一无二的。他指出，像文本分析或翻译这样的服务模型需要云提供商做“相当多的工作”来适应API服务。然而，OpenAI提供的是“用于各种任务的相同API”，而不是为特定任务创建的“特别定制的API”。
- en: An Azure-Native OpenAI API
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Azure本机OpenAI API
- en: 'OpenAI knew that the cloud fundamentals would be essential for them to scale.
    From the inception of the OpenAI API, the idea has always been to have an instantiation
    of the API within Azure as well, in order to reach more customers. Divakaruni
    mentions that there are more similarities than differences between OpenAI API
    and Azure OpenAI Service platforms. From a technology perspective, the objective
    is very similar: to provide people with the same API and access to the same models.
    The shape of the Azure OpenAI Service is going to be more Azure native, but they
    want to match the developer experience of OpenAI customers, especially as some
    of them graduate from the OpenAI API into the Azure OpenAI Service.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI知道云基础架构对于他们的扩展非常重要。从OpenAI API的初始阶段开始，其想法一直是在Azure中拥有API的实例，以便更多地接触到客户。Divakaruni提到，OpenAI
    API和Azure OpenAI Service平台之间存在更多的相似之处而不是差异。从技术角度来看，目标非常相似：为人们提供相同的API和对相同模型的访问。Azure
    OpenAI Service的形态将更加本机Azure，但他们希望匹配OpenAI客户的开发者体验，特别是当其中一些客户从OpenAI API过渡到Azure
    OpenAI Service时。
- en: At the moment of writing this book, we have captured the Azure OpenAI Service
    team still kicking off the platform, with lots to be fixed before they broadly
    release it to the customers.  OpenAI Service is now adding more and more models
    to their service, and they want to eventually reach parity or to be only a few
    months behind OpenAI API in terms of the models available.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书的时候，我们捕捉到Azure OpenAI服务团队仍在全面启动平台，还有很多问题需要解决，然后才能向广大客户发布。OpenAI Service现在正在向其服务中添加越来越多的模型，并希望在可用模型方面最终与OpenAI
    API保持平衡或只落后几个月。
- en: Resource Management
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 资源管理
- en: One difference between the two services is in how they handle resource management.
    A resource is a manageable item that is available through the service (whether
    it is OpenAI API or Microsoft Azure). In the context of OpenAI, examples of resources
    would be an API account or a pool of credits associated with an account. Azure
    offers a more complex set of resources, such as virtual machines, storage accounts,
    databases, virtual networks, subscriptions, and management groups.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个服务之间的一个区别在于它们如何处理资源管理。资源是通过服务（无论是OpenAI API还是Microsoft Azure）提供的可管理的项目。在OpenAI的情况下，资源的示例可以是API帐户或与帐户相关联的积分池。Azure提供了一组更复杂的资源，例如虚拟机、存储帐户、数据库、虚拟网络、订阅和管理组。
- en: While OpenAI offers a single API account per organization, within Azure companies
    can create multiple different resources, which they can track, monitor, and allocate
    to different cost centers. “It's just another Azure resource in general.” says
    Christopher Hoder, Senior Program Manager at Microsoft Azure OpenAI Service, which
    makes it easy to use out of the box.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 OpenAI 为每个组织提供单个 API 帐户，但在 Azure 中，公司可以创建多个不同的资源，可以对其进行跟踪、监视和分配给不同的成本中心。“它基本上只是另一个通用的
    Azure 资源。” 微软 Azure OpenAI 服务的高级项目经理 Christopher Hoder 表示，这使得它易于开箱即用。
- en: Resource management within Azure is a deployment and management functionality
    that enables customers to create, update, and delete resources in Azure accounts.
    It comes with features like access control, locks, and tags to secure and organize
    customer resources after deployment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 中的资源管理是一种部署和管理功能，使客户能够在 Azure 帐户中创建、更新和删除资源。它提供访问控制、锁定和标记等功能，以在部署后安全组织和管理客户资源。
- en: Azure has several layers of resource management that allow companies and organizations
    to better manage pricing and resources, Hoder says. At a high level, there is
    an organizational Azure account,  and within that account, there are multiple
    Azure subscriptions. Within that, there are resource groups, and then the resources
    themselves. “All of those can be monitored and segmented and access controlled,”
    Hoder adds, which becomes especially important for deployments at scale.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 具有多个层次的资源管理，使公司和组织能够更好地管理定价和资源，Hoder 表示。在较高层次上，有一个组织级别的 Azure 帐户，然后在该帐户中，有多个
    Azure 订阅。在其中，有资源组，然后是资源本身。“所有这些都可以进行监控、分段和访问控制，” Hoder 补充道，这在规模化部署中变得尤为重要。
- en: Security and Data Privacy
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和数据隐私
- en: 'While Microsoft hasn’t said much publicly about its security so far, Divakaruni
    told us that the company is focused on three main points: content filters, monitoring
    of abuse, and safety-first approach. The team is working on more safety-enforcing
    elements and plans to use customer feedback to understand which of these elements
    will be the most meaningful for the users before they officially launch it.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微软到目前为止对其安全性没有公开过多的信息，但 Divakaruni 告诉我们，公司专注于三个主要方面：内容过滤器、滥用监控和以安全为首要考虑的方法。团队正在努力开发更多的安全强制元素，并计划利用客户反馈来了解哪些元素对用户最有意义，然后在正式推出之前加以实施。
- en: They are also working on documentation that lays out the architecture of the
    way the privacy policy is implemented that they will share with the customers
    to provide assurances that they are protecting customer data while ensuring that
    their obligations for responsibly using artificial intelligence are maintained.
    “Lots of customers that come to us have concerns about the way it is currently
    implemented on OpenAI, because it is more open, and we are addressing [those concerns],”
    says Divakaruni.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还正在编写文档，概述隐私政策实施的架构，他们将与客户共享这些文档，以提供保证，即他们正在保护客户数据，同时确保他们履行了对人工智能的负责任使用的义务。“许多来找我们的客户对目前在
    OpenAI 上的实现方式表示担忧，因为它更加开放，而我们正在解决[这些担忧]，” Divakaruni 表示。
- en: Content filters are introduced in the form of PII (Personally Identifiable Information)
    filters, filters blocking sexual and other types of content, the scope of which
    they are still establishing. “The philosophy there is providing the customers
    the right knobs to adjust and iterate the content for their particular domain,”
    Divakaruni says.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 内容过滤器以 PII（个人身份信息）过滤器、阻止性和其他类型内容的过滤器的形式引入，他们目前仍在确定其范围。“这里的理念是为客户提供合适的旋钮，以调整和迭代其特定领域的内容，”
    Divakaruni 表示。
- en: Microsoft’s enterprise customers are demanding with regard to security. The
    Azure OpenAI API Service team is leveraging the work it’s done for other products,
    such as Bing and Office. Microsoft has a legacy of model development and pushing
    the envelope.  “Office has provided language products for a while. So there is
    a pretty extensive content moderation capability . . .  and we have a science
    team dedicated to building out filters that are appropriate for these models in
    this space,“ says Divakaruni.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的企业客户对安全性要求严格。Azure OpenAI API 服务团队正在利用其为其他产品（如必应和 Office）所做的工作。微软在模型开发和突破方面有着丰富的经验。“Office
    已经提供了一段时间的语言产品。因此，在这个空间中我们有相当广泛的内容审查能力……我们还有一个专门负责为这些模型构建合适的过滤器的科学团队，” Divakaruni
    说道。
- en: OpenAI API users often request geofencing, a technology that sets a virtual
    boundary around a real-world geographical area. If data is moved outside the specified
    radius, it can trigger an action in a geo-enabled phone or other portable electronic
    devices. For example, it can alert administrators when a person enters or exits
    the geofence and generates an alert to the user’s mobile device in the form of
    a push notification or email. Geofencing enables businesses to accurately track,
    market to, and effectively alert administrators when Geofencing creates silos
    to keep the data in a particular location. Azure’s geofencing feature is still
    a work in progress, but Divakaruni says that it’s been implemented on an experimental
    basis for a few select customers, such as GitHub Co-pilot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 的用户经常请求地理围栏技术，这是一种在真实地理区域周围设置虚拟边界的技术。如果数据移动到指定半径之外，它可以触发地理启用手机或其他便携式电子设备中的操作。例如，它可以在人员进入或退出地理围栏时向管理员发出警报，并以推送通知或电子邮件的形式向用户的移动设备生成警报。地理围栏使企业能够准确跟踪、营销和有效地向管理员发出警报，当地理围栏创建存储数据的特定位置时，
    Azure 的地理围栏功能仍在积极研发中，但 Divakaruni 表示，已经根据一些特选客户的实验性实施，例如 GitHub Copilot。
- en: Model-as-a-Service at the Enterprise Level
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业级别的模型即服务
- en: While Azure OpenAI Service has been engaged with a lot of big enterprise customers
    on the platform, the company isn’t ready to discuss them publicly, citing privacy
    concerns and the sensitivity of public opinion. What they can mention now are
    examples of their internal services. GitHub Copilot started off on OpenAI API
    but now, mostly for scale reasons, has transitioned to Azure OpenAI Service. Other
    examples of internal services running on Azure are Dynamics 365 Customer Service,
    Power Apps, ML to code, and Power BI services.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Azure OpenAI 服务已经与许多大型企业客户在平台上合作，但公司尚未准备公开讨论它们，理由是隐私问题和公众舆论的敏感性。他们现在可以提及的是一些内部服务的例子。GitHub
    Copilot 最初是基于 OpenAI API，但现在，主要是出于规模考虑，已经过渡到 Azure OpenAI 服务。在 Azure 上运行的其他内部服务的例子包括
    Dynamics 365 客户服务、Power Apps、ML to code 以及 Power BI 服务。
- en: Divakaruni says they’re seeing a lot of interest from financial services industries
    and traditional enterprises looking to enhance their customer experience. “There
    is a lot of text information to process and there's a lot of need for summarization
    and helping analysts, for example, quickly zero in on the text that is relevant
    and meaningful for them. The customer service industry, I think, is a big untapped
    domain as well. There's a vast amount of information that is locked in audio,
    which can be transcribed, in call center information that could be meaningful
    insights for a company that is trying to improve their customer experience.” Another
    set of use cases they are seeing is companies accelerating their developer productivity
    with training the GPT-3 for their internal APIs and software development kits
    to make these tools more accessible to their employees.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni 表示，他们看到了金融服务行业和传统企业对提升客户体验的浓厚兴趣。“需要处理大量文本信息，并且需要摘要和帮助分析师，例如，快速锁定对他们有意义和相关的文本。客户服务行业，我认为，也是一个巨大的未开发领域。有大量信息被锁定在音频中，可以被转录，呼叫中心信息可能是对试图改善客户体验的公司有意义的见解。”
    他们正在看到的另一组用例是公司通过为其内部 API 和软件开发工具培训 GPT-3 来加速开发人员的生产力，以使这些工具更易于员工使用。
- en: Divakaruni notes that many businesses whose core strength is not in AI or ML
    want to apply AI in ways that add meaningful value to their business processes
    or enhance their customer experience. They leverage Microsoft's field strength
    to help them build solutions. The Azure OpenAI Service team fully expects its
    sophisticated model-as-a-service approach to become mainstream, Hoder says. He
    notes that Microsoft provides its ready-to-use experience by embedding it into
    consumer applications such as Office and Dynamics. Customers that need more unique
    or tailored support go down a layer to services like the Power platform, which
    is aimed at business users and developers, providing no-code or low-code ways
    to tailor machine learning and AI. “If you go a little bit lower, a little bit
    more customized, a little bit more developer-focused, you end up at Cognitive
    Services. This has really been our model to provide AI capabilities through REST
    API-based services. And now we're introducing a more granular layer with OpenAI
    Service. . .  And then at the bottom layer, we have the data science-focused tooling
    with Azure Machine Learning” Hoder explains.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni指出，许多业务的核心实力不在于人工智能或机器学习的企业希望以增加业务流程的有意义的方式应用人工智能，或者增强他们的客户体验。他们利用微软的领域实力帮助他们构建解决方案。Azure
    OpenAI Service团队完全期待其复杂的模型即服务方法成为主流，Hoder说。他指出，微软通过将其嵌入到Office和Dynamics等消费应用程序中提供了其即用即用的体验。需要更独特或定制支持的客户会下降到像Power平台这样的服务层，该平台旨在面向企业用户和开发人员，提供无代码或低代码方式来定制机器学习和人工智能。“如果你进一步降低一点，更加定制化，更加专注于开发人员，你最终会到达认知服务。通过REST
    API为服务提供AI能力一直是我们的模式。现在我们正在引入一个更加细化的层次，即OpenAI Service。...然后在底层，我们有面向数据科学的工具，即Azure
    Machine Learning。” Hoder解释。
- en: Microsoft sees a big customer demand for Azure OpenAI Service, but also can
    vouch for its so far success with other services, like speech service and the
    form recognizers. “We see a lot of demand for the ability to take an image, extract
    information in a structured way and extract tables and other information from
    PDFs to do automated data ingestion, and then combine analytics and search capabilities.”
    Hoder says. (See, for example, this [case study](https://news.microsoft.com/transform/progressive-gives-voice-to-flos-chatbot-and-its-as-no-nonsense-and-reassuring-as-she-is/)
    of how customers are using their REST API-based AI/ML services.)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 微软认为Azure OpenAI Service有很大的客户需求，但也可以为其迄今在其他服务方面的成功作保证，比如语音服务和表单识别器。“我们看到很多客户需要的能力是能够获取图像，以结构化方式提取信息并从PDF中提取表格和其他信息以进行自动化数据摄入，然后结合分析和搜索能力。”
    Hoder说。（例如，查看此[案例研究](https://news.microsoft.com/transform/progressive-gives-voice-to-flos-chatbot-and-its-as-no-nonsense-and-reassuring-as-she-is/)，了解客户如何使用其基于REST
    API的人工智能/机器学习服务。）
- en: Other Microsoft AI and ML Services
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其他微软人工智能和机器学习服务
- en: 'Will Azure OpenAI Service affect other AI/ML services from Microsoft product
    line such as Azure ML Studio? Divakaruni tells us that there is a place for both
    on the market: “It''s definitely not a winner take all. There is a need for multiple
    solutions in the market that provide for specific customer requirements.” he tells
    us. Customers’ requirements may differ substantially. They might need to generate
    and then label data specific to their particular use case. They can build a model
    from scratch using platforms like Azure Machine Learning, or SageMaker, and then
    train a distilled, smaller model for that purpose.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI Service会影响微软产品线中的其他人工智能/机器学习服务吗，比如Azure ML Studio？Divakaruni告诉我们市场上对两者都有需求：“绝对不是一家独大。市场上需要提供满足特定客户需求的多种解决方案。”他告诉我们。客户的需求可能大相径庭。他们可能需要生成然后标记特定于其特定用例的数据。他们可以使用Azure
    Machine Learning、SageMaker等平台从头开始构建模型，然后为此目的训练一个精简的、更小的模型。
- en: 'Of course, that’s a niche that’s not accessible to most people. Hoder notes
    that bringing data science capabilities to customers “broadens access, it democratizes
    it.” Divakaruni agrees: “You''ll increasingly see a trend towards the larger,
    most sophisticated models being exposed through services, as opposed to people
    going to build their own.” Why?  “The fundamental truth is that it takes a tremendous
    amount of compute and lots of data to train these models. The companies that have
    the means to develop these models are unfortunately fewer. But it''s our responsibility
    as we do to make them available for the world.”'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是大多数人无法接触的一个利基市场。霍德尔指出，将数据科学能力带给客户“扩大了接触面，使其民主化。”迪瓦卡鲁尼同意：“你会越来越多地看到一个趋势，即最大、最复杂的模型通过服务公开，而不是人们去构建自己的模型。”为什么呢？“根本的事实是，训练这些模型需要大量的计算和大量的数据。能够开发这些模型的公司很少。但作为我们所做的，我们有责任使它们对世界可用。”
- en: Generally, data science teams from companies that can afford costly resources
    strongly prefer to build their own IP for their specific use cases, using lower-level
    ML platforms like Azure ML Studio. That demand, Divakaruni argues, is unlikely
    to disappear.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，能够负担昂贵资源的公司的数据科学团队强烈倾向于为其特定用例构建自己的IP，使用像Azure ML Studio这样的低级ML平台。迪瓦卡鲁尼认为，这种需求不太可能消失。
- en: Advice for Enterprises
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 企业建议
- en: 'Enterprises investigating the Azure OpenAI Service, Divakaruni says, can approach
    it much as they would investigating any other cloud service: you start with what
    makes the most sense for you and then look to see if the various technologies
    meet your needs. “While the technology is cool and that certainly has a wow factor,
    you still have to start with, ‘where can this be most applicable for me as a business,
    for my group?’ And then look to solve that with a set of technologies.”'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 研究Azure OpenAI Service的企业可以像研究其他任何云服务一样，迪瓦卡鲁尼说：你从最合适你的地方开始，然后看看各种技术是否满足你的需求。“虽然技术很酷，这确实有一种哇的因素，但你仍然必须从‘这对我作为企业，对我的团队最适用的地方是哪里开始’。然后用一系列技术来解决这个问题。”
- en: The next step is to examine how to get from experimentation into production: 
    “What are the other things that you need to build?”  Divakaruni refers to this
    step as an “application glue that someone needs to inject around, making sure
    these models actually behave and can be used in a live application scenario”.
    That’s a nontrivial task, but enterprises need to think about this to understand
    what kind of investment a GPT-3-based application will require. Divakaruni advises
    asking, “Is this model actually producing things that are relevant when you have
    automation around? The use of the capability, when it's actually built into an
    application—is it doing what it's supposed to be doing?”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是审查如何从实验转入生产：“你需要构建什么其他东西？”迪瓦卡鲁尼将这一步称为“需要有人注入的应用粘合剂，确保这些模型实际上能够发挥作用并在实际应用场景中使用”。这是一项非常重要的任务，但企业需要考虑这一点，以了解基于GPT-3的应用程序将需要什么样的投资。迪瓦卡鲁尼建议问：“当您有自动化环绕时，该模型实际上是否产生了相关的东西？当它实际内置到一个应用程序中时，它是否在做它应该做的事情？”
- en: 'OpenAI or Azure OpenAI Service: Which should you use?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还是Azure OpenAI Service：你应该使用哪个？
- en: 'The question for companies interested in exploring GPT-3, then, is: OpenAI
    API or Azure OpenAI Service? Divakaruni maintains that the OpenAI API version
    is more suitable for companies that are exploring their options but don’t have
    any specific project implementation in mind. In terms of access, OpenAI is definitely
    further along, with its Playground making it easier for individual users and companies
    to experiment there. The OpenAI API also allows access to the latest experimental
    models and API endpoints that expand the API’s capabilities.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于有兴趣探索GPT-3的公司来说，问题是：OpenAI API还是Azure OpenAI Service？迪瓦卡鲁尼认为，OpenAI API版本更适合正在探索其选择但没有任何具体项目实施计划的公司。在访问方面，OpenAI显然走在前面，其Playground使个人用户和公司更容易在那里进行实验。OpenAI
    API还允许访问最新的实验模型和扩展API功能的API端点。
- en: Azure OpenAI Service, on the other hand, is targeting a cohort of users with
    production use cases who “graduate” from OpenAI API, or need to meet different
    compliance and privacy regulations  The two organizations encourage customers
    to experiment, and validate their use cases and firm them up with OpenAI API.
    If that platform meets their needs, Microsoft is encouraging the customers to
    stay on OpenAI API, but when their production needs become more mature and they
    start to need more compliance, they should consider transitioning to Azure.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Azure OpenAI 服务正针对一批具有生产用例的用户群，这些用户“毕业”于 OpenAI API，或需要满足不同的合规性和隐私法规。这两家组织鼓励客户进行实验，并验证其用例，并使用
    OpenAI API 加以巩固。如果该平台满足其需求，微软鼓励客户继续使用 OpenAI API，但当他们的生产需求变得更加成熟，并且开始需要更多的合规性时，他们应考虑过渡到
    Azure。
- en: Conclusion
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you saw how corporations are using GPT-3 based products at
    scale and how the new Microsoft Azure OpenAI service is paving the way for enterprises
    interested in becoming part of the GPT-3 ecosystem. We have dived into the nuances
    of scaling a GPT-3 powered product and shared some tips from the journey of large-scale
    enterprise-grade products. In Chapter 6, we will look at some of the controversies
    and challenges surrounding the OpenAI API and LLMs more generally.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了企业如何大规模使用基于 GPT-3 的产品，以及新的 Microsoft Azure OpenAI 服务如何为有意加入 GPT-3 生态系统的企业铺平道路。我们深入探讨了扩展
    GPT-3 驱动产品的细微差别，并分享了一些来自大规模企业级产品旅程的技巧。在第 6 章中，我们将探讨围绕 OpenAI API 和 LLMs 的一些争议和挑战。
