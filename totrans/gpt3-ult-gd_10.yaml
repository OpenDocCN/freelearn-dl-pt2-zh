- en: 'Chapter 5: GPT-3 as the Next Step in Corporate Innovation'
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new innovation or technical shift happens, big corporations are usually
    the last to adopt it. Their hierarchical structures are composed of various authoritarian
    levels, and standard processes of legal approvals and paperwork often limit freedom
    to experiment, making it difficult for enterprises to be early adopters. But this
    doesn’t seem to be the case with GPT-3\. As soon as the API was released, corporations
    started experimenting with it. However, they ran into a significant barrier: data
    privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest form, all a language model does is to predict the next word
    given a series of previous words. As you learned in Chapter 2, OpenAI has devised
    several techniques to transform the functioning of language models like GPT-3
    from simple next-word prediction to more useful NLP tasks such as answering questions,
    summarizing documents, and generating context-specific text. Typically, the best
    results are achieved by ‘fine-tuning’ a language model or conditioning it to mimic
    a particular behavior by providing it with a few examples using domain-specific
    data. You can provide examples with the training prompt, but a more robust solution
    is to create a custom-trained model using the fine-tuning API.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI offers GPT-3 in the form of an open-ended API, where users provide input
    data and the API returns output data. Properly securing, handling, and processing
    user data is a key concern for corporations looking to use GPT-3\. OpenAI’s Welinder
    notes that, while enterprise leaders have expressed a variety of concerns about
    GPT-3, “SOC2 compliance, geofencing, and the ability to run the API within a private
    network were the biggest of them.”
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s measures for model safety and misuse are thus designed to cover a wide
    range of issues under the umbrella of data privacy and security. For example, 
    Adams, founder of Stenography, tells us about the privacy and security aspects
    of the OpenAI API. “As it stands, Stenography is a pass-through API—it's like
    a toll road. So that people will pass in their code, and then receive a signal
    that says that they have used the API, and then it passes the input without saving
    or logging it anywhere.” Outside of those guardrails, Stenography is a superset
    of [OpenAI's Terms of Use](https://openai.com/api/policies/terms/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We talked to representatives of several corporations about what’s stopping
    them from using OpenAI API in production. Most highlighted two common concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        The GPT-3 API endpoint exposed by OpenAI should not retain or save
    any part of the training data provided to it as part of the model fine-tuning/training
    process.[[13]](xhtml-0-12.xhtml#aid_86)
  prefs: []
  type: TYPE_NORMAL
- en: ●        Before sending their data to OpenAI API, companies want to make sure
    that there’s no way for a third party to extract or access the data by providing
    any input to the API.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI responded to the above customer concerns and questions around data handling
    and privacy by offering security reviews, enterprise contracts, data processing
    agreements, third-party security certification efforts, and more. Some of the
    issues that customers and OpenAI discuss include whether the customer’s data can
    be used to improve OpenAI models, which may improve performance in the customer’s
    desired use cases but comes with concerns around data privacy and internal compliance
    obligations; limits around the storage and retention of customer data, and obligations
    around security handling and processing of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this chapter delves into three case studies that show how global
    enterprises like GitHub, Microsoft, and Algolia are navigating these questions
    and using GPT-3 at scale. You’ll also learn how OpenAI has adapted to the demand
    for enterprise-grade products by collaborating with Microsoft Azure’s OpenAI service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: GitHub Copilot'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start this journey with GitHub Copilot, one of the hottest products of
    2021\. GitHub Copilot (Figure 5-1) is a first-of-its-kind AI pair programmer that
    helps users write code faster and with much less work. Oege De Moor, VP of GitHub
    Next, says the mission is “to reach all developers, with an ultimate goal to make
    programming accessible to everyone.” Automating mundane tasks, like writing redundant
    code and writing unit test cases, allows developers to “focus on the truly creative
    part of the job which involves deciding what the software should actually do”
    and to “think more about the product concept rather than being stuck in figuring
    out the code.”
  prefs: []
  type: TYPE_NORMAL
- en: 'As Awan told us: “I''m excited to work on more side projects now, because I
    know I''ll have the help of GitHub Copilot. It''s almost like I have a co-founder
    now. Codex and copilot are writing 2 to 10% of my code, something like that. So
    it has already made me 2 to 10% more accelerated. And all of this is on an exponential
    scale. So what will GPT-3 be like next year? What will Codex be like next year?
    I may be 30% more accelerated.” Let’s dive into the inner workings of the Copilot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. GitHub Copilot
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot draws context from the code you're working on, based on things
    like docstrings, comments, and function names.[[14]](xhtml-0-12.xhtml#aid_77)
    It then automatically suggests the next line, or even entire functions, right
    inside your editor to produce boilerplate code and suggest test cases that match
    the code implementation. works with a broad set of frameworks and programming
    languages by using a plugin to the user’s code editor, making it nearly language-agnostic
    as well as lightweight and easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI research scientist Harri Edwards notes that Copilot is also a useful
    tool for programmers working in a new language or framework: “Trying to code in
    an unfamiliar language by googling everything is like navigating a foreign country
    with just a phrasebook. Using GitHub Copilot is like hiring an interpreter.”[[15]](xhtml-0-12.xhtml#aid_30)'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot is powered by OpenAI’s Codex, a descendant of the GPT-3 model
    that, as we noted in chapter 4, is designed specifically to interpret and write
    code. “GitHub is home to more than 73 million developers, which includes a massive
    amount of public data that embodies the collective knowledge of the community,”
    says De Moor. That translates to billions of lines of publicly available code
    for Codex to train on. It understands both programming and human languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Codex draws on supporting comments or instructions in simple English to come
    up with relevant code as seen in Figure 5-2\. The Copilot editor extension intelligently
    chooses which context to send to the GitHub Copilot service, which in turn runs
    OpenAI’s Codex model to synthesize suggestions. Even though Copilot generates
    the code, users are still in charge: you can cycle through suggested options,
    choose which to accept or reject, and manually edit the suggested code. GitHub
    Copilot adapts to the edits you make and matches your coding style. De Moor explains,
    “It links natural language with source code so you can use it in both directions.
    You can use the source code to generate comments or you can use the comments to
    generate the source code, making it immensely powerful.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Working of GitHub Copilot
  prefs: []
  type: TYPE_NORMAL
- en: This functionality has also indirectly changed how developers write code. When
    they know that their code comments in human languages, like English, will be part
    of the model’s training, they write “better and more accurate comments in order
    to get better results from Copilot,” says De Moor.
  prefs: []
  type: TYPE_NORMAL
- en: Many critics worry that giving this tool in the hands of people who can’t judge
    the quality of code may result in introducing bugs or errors in the codebase.
    Contrary to that opinion De Moor tells us, “We have received a lot of feedback
    from developers that Copilot makes them write better and more efficient code.”
    In the current technical preview, Copilot can only help you write code if you
    understand how different pieces in software work, where you can precisely tell
    Copilot what it is that you want it to do. Copilot encourages healthy developer
    practices, like writing more accurate comments, and rewards developers with better
    code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot is not just limited to the general rules of programming but it can also
    figure out the details of specific fields, such as writing programs to compose
    music. To do that you need to understand music theory to write such programs “Seeing
    how Copilot has somehow picked it up from its immensely large training data is
    just amazing,” De Moor adds.
  prefs: []
  type: TYPE_NORMAL
- en: Developing Copilot
  prefs: []
  type: TYPE_NORMAL
- en: De Moor says one of the challenges of designing Copilot was creating the right
    user experience, one that “lets you use this model in a collaborative way without
    being intrusive.” The goal is for it to feel like working with a programming partner
    or coworker who “knows more about the mundane coding stuff so you can focus more
    on creating the important stuff.” Developers are constantly searching for existing
    solutions to problems and often refer to StackOverflow, search engines, and blogs
    to find implementation and code syntax details– which means lots of moving back
    and forth between editor and browser. As De Moor points out, “As a developer,
    you are more productive when you can stay in your environment and just think about
    the problem rather than switching context all the time.” This is why GitHub’s
    team designed Copilot to deliver suggestions inside the development environment.
  prefs: []
  type: TYPE_NORMAL
- en: What does low-code/no-code programming mean?
  prefs: []
  type: TYPE_NORMAL
- en: Right now, developing software-related products or services requires a technical
    or scientific background—for example, you have to learn at least one programming
    language. And that’s just a start. Even to develop a minimum viable product (MVP)
    with conventional techniques you have to understand the different elements of
    software engineering involved in developing both the frontend (how the user interacts
    with the software) and the backend (how the processing logic works). This creates
    a barrier to entry for those who don’t come from a technical or engineering background.
  prefs: []
  type: TYPE_NORMAL
- en: De Moor sees Copilot as a step toward making technology more accessible and
    inclusive. If developers “have to worry less and less about the development details
    and just explain the design, explain the purpose of what [they] want to do,” and
    let Copilot handle the details, many more people will be able to use these tools
    to create new products and services.
  prefs: []
  type: TYPE_NORMAL
- en: There are already several no-code programming platforms, but many users find
    their limits  constricting, in essence, “heavily simplifying the programming experience”
    by making it “more visual, more graphical, and easy to use,” according to de Moor.
    “These things are great to get started but unfortunately, it comes up with a limit
    on the things that are possible to build using those platforms.” De Moor argues
    that Copilot is equally easy to use but provides far more options by using fully
    operational programming tools rather than simplified versions.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with the API
  prefs: []
  type: TYPE_NORMAL
- en: Scaling in terms of language models has been undervalued for so long because
    of theoretical concepts like [Occam’s Razor](https://en.wikipedia.org/wiki/Occam%27s_razor)
    and vanishing results when you expand the neural network to a significant size.
    With conventional deep learning, it has always been a norm to keep the model size
    small with fewer parameters to avoid the problem of vanishing gradients and introducing
    complexity in the model training process. Occam’s Razor which means - “A simple
    model is the best model” has been sacred in the AI community since its inception.
    It has been a center of reference for training new models which restricted people
    from experimenting with scale.
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, when OpenAI released its marquee language model GPT-3, the potential
    of scaling came into the limelight. This was the time when the common conception
    of the AI community started to shift and people started realizing that the "gift
    of scale" can give rise to a more generalized artificial intelligence where a
    single model like GPT-3 can perform an array of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting and managing a model like GPT-3 requires sophistication on many different
    levels, including the optimization of model architecture, its deployment, and
    how the general public can access it. De Moor tells us, “When we launched Copilot,
    it was using the OpenAI API infrastructure in the initial phases and then we had
    this explosion of response after the launch with so many people signing up and
    wanting to use the product.”
  prefs: []
  type: TYPE_NORMAL
- en: Although the API was capable of handling large amounts of requests, the number
    of requests and their frequency still surprised the OpenAI team. De Moor and his
    team “realized the need of a more efficient and bigger infrastructure for deployment,
    and fortunately, it was about time that Microsoft Azure OpenAI came to light”
    allowing them to make the required switch to Azure deployment infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: When we asked about the experience of building and scaling Copilot, De Moor
    shares, “Early on we had this misled belief that accuracy is the single most important
    thing that matters but sometime later into the product journey, we realized that
    it’s actually a trade-off between the powerful AI model and flawless user experience.”
    The Copilot team quickly realized that there is a trade-off between speed and
    the accuracy of suggestions as is the case with any deep learning model of sufficient
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the more layers a deep learning model has, the more accurate it
    will be. However, more layers also means it will be slower to run. The Copilot
    team had to somehow find a balance between the two, as de Moor explains: “Our
    use case required the model to deliver the response at lightning-fast speed with
    multiple alternative suggestions; if it’s not fast enough, users can easily outpace
    the model and write the code themselves. So, we found that a slightly less powerful
    model that gives the responses quickly while maintaining the quality of results”
    was the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: The rapid user adoption and interest in GitHub Copilot took everyone in the
    team by surprise but it just didn’t end there. Because of the usefulness of the
    product and the quality of code suggestions the team saw exponential growth in
    the amount of code generated using Copilot where “on an average, 35% of newly
    written code is being suggested by Copilot. This number will increase going forward
    as we get closer to finding the right balance between model capabilities and the
    speed of suggestions” says De Moor.
  prefs: []
  type: TYPE_NORMAL
- en: When asked about the data security and privacy aspect of code submitted as part
    of the request to Copilot Moor tells us, “Copilot architecture is designed in
    a way that when a user types the code into the Copilot, there would not be any
    possibility of code leaking between one user to another. GitHub Copilot is a code
    synthesizer rather than a search engine, as it generates the majority of its suggestions
    based on unique algorithms. In rare cases, approximately 0.1% of suggestions may
    include snippets that are identical to those found in the training set.”
  prefs: []
  type: TYPE_NORMAL
- en: What’s Next for Github Copilot?
  prefs: []
  type: TYPE_NORMAL
- en: De Moor sees a great potential for Copilot to assist in code review as well
    as writing. “Think of an automated code reviewer where it automatically looks
    at your changes and makes suggestions to make your code better and more efficient.
    The code review process at GitHub today consists of human reviewers, and we’re
    also exploring the idea of Copilot reviews.”
  prefs: []
  type: TYPE_NORMAL
- en: Another feature under exploration is code explanation. De Moor explains that
    users can select a code snippet and “Copilot can explain it in simple English.”
    This has the potential as a useful learning tool. In addition, De Moor says, Copilot
    hopes to provide tools that assist in “translation of code from one programming
    language to another.”
  prefs: []
  type: TYPE_NORMAL
- en: Copilot has opened the world of unlimited opportunities for not just the developers
    but for anyone who wants to get creative and build a piece of software to bring
    their ideas to reality. Prior to GitHub Copilot and OpenAI’s Codex, features like
    generating production-grade code, AI-assisted code review, and the translation
    of code from one language to another has been a far-fetched dream before the release.
    The advent of Large Language Models combined with no-code and low-code platforms
    will enable people to unleash their creativity and build interesting and unexpected
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Algolia Answers'
  prefs: []
  type: TYPE_NORMAL
- en: Algolia is a renowned search solutions provider with clients spanning from Fortune
    500 companies to a new generation of startups. It offers a symbolic, keyword-based
    search API that can be integrated with any existing product or application. In
    2020, Algolia partnered with OpenAI to connect GPT-3 with its already existing
    search technology. The next-generation product offering resulted in Algolia Answers,
    which enables clients to build an intelligent, semantics-driven single-search
    endpoint for search queries. “We build the technology that other companies use,”
    says Dustin Coates, Product Manager at Algolia.
  prefs: []
  type: TYPE_NORMAL
- en: Coates says that what his team means by intelligent search is along the lines
    of “You search for something and you get back the response right away—not just
    you get back to the record, you get back to the article—but you get back to what's
    actually answering the question.” In short, it’s “ a search experience where people
    don’t have to type exactly what the words are.”
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating NLP options
  prefs: []
  type: TYPE_NORMAL
- en: Algolia set up a dedicated team to work in this area, of which Claire Helme-Guizon
    was an early member. When OpenAI reached out to them to see if Algolia might be
    interested in GPT-3, Coates’s team compared it to competing technologies. Algolia
    ML engineer Claire Helme-Guizon, a member of the original Algolia Answers team,
    explains, “We worked on BERT-like models, to optimize for speed, DistilBERT, and
    with more stable models like RoBERTa along with different variants of GPT-3 like
    DaVinci, Ada, etc.” They created a rating system to compare the quality of different
    models and understand their strengths and weaknesses. They found Coates says that
    “it performed really well in terms of the quality of the search results returned.”
    Speed and cost were weaknesses, but the API was ultimately a deciding factor since
    it allowed Algolia to use the model without having to maintain its infrastructure.
    Algolia asked existing clients whether they might be interested in such a search
    experience, and the response was very positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with that quality of results, Algolia still had plenty of questions: How
    would it work for the customers? Would the architecture be scalable? Was it financially
    feasible? To answer them, Coates explains, “We sculpted specific use cases that
    had longer textual content,” such as publishing and help desks.'
  prefs: []
  type: TYPE_NORMAL
- en: For some use cases, it’s good enough to rely solely on GPT-3 to get the search
    results, but for other complex use cases, you may need to integrate GPT-3 with
    other models. GPT-3, being trained on the data up to a certain point in time,
    struggles with use cases involving freshness, popularity, or personalized results.
    When it comes to the quality of results, the Algolia team was challenged by the
    fact that semantic similarity scores generated by GPT-3 were not the only metric
    that mattered to their customers. They need to somehow blend the similarity scores
    with other measures to ensure that the clients get satisfactory results. So, they
    introduced other open-source models to highlight the best results in combination
    with GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Data Privacy
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenges Algolia faced while introducing this novel technology,
    Coates says, were legal ones. “Getting through legal and security and procurements
    was maybe the hardest thing we did in this entire project because you're sending
    this customer data and it's feeding this ML model. How do we delete that data?
    How do we make sure it's GDPR compliant?[[16]](xhtml-0-12.xhtml#aid_70) How do
    we handle all of these things? How do we know that OpenAI isn't going to take
    this data and feed everyone else's model with it as well? So there were a lot
    of questions that needed to be answered and a lot of agreements that needed to
    be put into place.”
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: Most of the GPT-3 use cases that we’ve seen so far are business-to-consumer
    (B2C) products, but for a business-to-business (B2B) company like Algolia, the
    game is different. Not only do they need OpenAI’s pricing to work for them, but
    they also need to optimize their pricing for clients, so that  “we can be profitable
    and have customers still be interested in what we’re building.”
  prefs: []
  type: TYPE_NORMAL
- en: In the search solutions business, success is measured on the basis of throughputs.
    So it naturally makes sense to think about the tradeoff between quality, cost,
    and speed. Coates says, “Even before we knew the costs, Ada was the right model
    for us because of the speed. But even if, let's say, Davinci was fast enough,
    we may have still gotten down to Ada just because of the cost measures.”
  prefs: []
  type: TYPE_NORMAL
- en: Helme-Guizon notes that the factors affecting the cost include, “the number
    of tokens, and the number of documents you are sending and their length.” Algolia’s
    approach was to build “the smallest possible context windows”--meaning the amount
    of data sent to the API at a time–that would still be “relevant enough in terms
    of quality.”
  prefs: []
  type: TYPE_NORMAL
- en: So how did they solve this problem? “We started with OpenAI before they had
    announced pricing and we had gone far enough and saw that the quality was good
    enough from what we could see elsewhere, without knowing what the pricing was.
    So it was quite some sleepless nights, not knowing what the pricing was. And then
    once we knew the pricing, figuring out how to bring that cost down. Because when
    we first saw the pricing, we weren't sure if we were going to make it work.”
  prefs: []
  type: TYPE_NORMAL
- en: They did put a lot of work into optimizing the price for their use case as according
    to Coates pricing will be “a universal challenge” for everyone trying to build
    their business on top of it. So, it is highly recommended to start thinking about
    price optimization in the very early stages of product development.
  prefs: []
  type: TYPE_NORMAL
- en: Speed and Latency
  prefs: []
  type: TYPE_NORMAL
- en: Speed is of particular importance to Algolia; the company promises its clients
    lightning-fast search capabilities with delays limited to just milliseconds. When
    the team evaluated Open AI’s proposal, they were happy with the quality of results,
    but GPT-3’s latency was just unacceptable. “In our traditional search, the results
    come back round trip in less than 50 milliseconds,” Coates says. “We're searching
    across hundreds of millions of documents and it has to be in real-time. When we
    worked with OpenAI early on, each of those queries took minutes.”
  prefs: []
  type: TYPE_NORMAL
- en: Algolia did decide to give GPT-3 a shot and began an initial phase of experimentation
    and beta rollout for Algolia Answers. However, bringing down latency and monetary
    costs required a lot of effort. “We started out at around 300 milliseconds, sometimes
    400, total latency, which we have to bring down to somewhere in the range of 50
    to 100 milliseconds for it to be feasible for our clients to use.” Ultimately,
    Algolia came up with semantic highlighting, a technique that uses a trained Question-Answering
    model on top of GPT-3 to perform mini searches and figure out the correct answer.
    The combination of GPT-3 with other open-source models resulted in reduced overall
    latency. The quality of their results are better, Helme-Guizon adds, because “the
    models are trained to find the answers, not just the words that are related to
    one another.”
  prefs: []
  type: TYPE_NORMAL
- en: A key aspect of Algolia Answers’ architecture, Helme-Guizon says, is reader
    retrieval architecture, in which an AI reader is “going through the subset of
    documents and reading them, understanding them with reference to the query using
    Ada, and giving us a confidence score for the semantic value.” While this was
    “a good first solution,” she adds, it has a lot of challenges– “especially latency,
    because you have that dependency where you cannot process the first batch and
    the second batch together” asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 was using the embedding from the predictions to compute cosine similarity,
    a mathematical metric used to determine how similar two documents are, irrespective
    of their size. Coates sums up these challenges: First, “you can''t send too many
    documents or else the response is going to be too slow or the cost is going to
    be too high monetarily.”  The second is casting “a net wide enough to fetch all
    the relevant documents while keeping time and costs under control.”'
  prefs: []
  type: TYPE_NORMAL
- en: Lessons Learned
  prefs: []
  type: TYPE_NORMAL
- en: So, if Algolia Answers have to start from scratch today, what would they do
    differently? “Working with GPT-3 can be overwhelming at times,” Coates says. “We
    would have asked some of the first-principle questions in the early stages of
    your product development like, ‘Are we willing to take a hit in terms of semantic
    understanding because we take such an increase for everything else?’ I think we
    would have thought a lot more about the latency and the confluence of different
    ranking factors early on.” He adds that he could see the project “going back to
    a BERT-based model. We might say that the raw quality isn't the same as what we're
    going to get out of GPT-3\. There's no denying that. ​​But I think that as much
    as we fell in love with the technology, we uncovered customer problems that we
    weren't solving, and the technology has to follow the customer problems, not the
    other way around.”
  prefs: []
  type: TYPE_NORMAL
- en: So what is Algolia’s take on the future of search? “We don't believe that anyone
    has truly solved blending textual relevance and semantic relevance. It's a very
    difficult problem because you can have situations where things are textually relevant,
    but don't really answer the question,” says Coates. He envisions “a marriage of
    the more traditional, textual base, the more understandable and explainable side
    of it, with these more advanced language models.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Microsoft’s Azure OpenAI Service'
  prefs: []
  type: TYPE_NORMAL
- en: Algolia has matured on the OpenAI API, but soon they wanted to expand their
    business in Europe– which meant they needed GDPR compliance. They began working
    with Microsoft, which was launching its Azure OpenAI Service. In the next case
    study, we’ll take a look at that service.
  prefs: []
  type: TYPE_NORMAL
- en: A Partnership That Was Meant to Be
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft and OpenAI announced a partnership in 2019, with the goal of giving
    Microsoft Azure customers access to GPT-3’s capabilities. The partnership is based
    on the shared vision of wanting to ensure that AI and AGI are deployed safely
    and securely. Microsoft invested a billion dollars in OpenAI, funding the launch
    of the API, which runs on Azure. The partnership culminates in shipping the API
    to provide more people access to large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Dominic Divakaruni, Principal Group Product Manager and Head of Azure OpenAI
    Service, says he’s always thought of this collaboration as a partnership that
    feels like it was meant to be, noting that Microsoft CEO Satya Nadella and OpenAI
    CEO Sam Altman have both spoken often about ensuring that the benefits of AI are
    accessible and widely distributed. Both companies are also concerned with safety
    in AI innovation.
  prefs: []
  type: TYPE_NORMAL
- en: The goal, Divakaruni says, “was to leverage each other's strengths, ”in particular
    Open AI’s user experience and modeling progress and Microsoft's existing relationships
    with companies, large salesforce, and cloud infrastructure.  Given its customer
    base, Microsoft Azure understands enterprise cloud customers’ fundamental requirements
    in terms of compliance, certifications, network security, and related issues.
  prefs: []
  type: TYPE_NORMAL
- en: For Microsoft, the interest in GPT-3 begins largely with it breaking new ground
    and being available before any other model from the LLM category. Another crucial
    factor in Microsoft's investment is that it gained the ability to use OpenAI’s
    Intellectual Property assets exclusively. Although GPT-3 alternatives are available,
    Divarakuni says that the centralization of the OpenAI API is unique. He notes
    that models for services like text analytics or translation require “quite a bit
    of work” on a cloud provider’s part to adapt into an API service. OpenAI, however,
    offers “the same API used for various tasks” rather than “bespoke API that are
    created for particular tasks.”
  prefs: []
  type: TYPE_NORMAL
- en: An Azure-Native OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI knew that the cloud fundamentals would be essential for them to scale.
    From the inception of the OpenAI API, the idea has always been to have an instantiation
    of the API within Azure as well, in order to reach more customers. Divakaruni
    mentions that there are more similarities than differences between OpenAI API
    and Azure OpenAI Service platforms. From a technology perspective, the objective
    is very similar: to provide people with the same API and access to the same models.
    The shape of the Azure OpenAI Service is going to be more Azure native, but they
    want to match the developer experience of OpenAI customers, especially as some
    of them graduate from the OpenAI API into the Azure OpenAI Service.'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment of writing this book, we have captured the Azure OpenAI Service
    team still kicking off the platform, with lots to be fixed before they broadly
    release it to the customers.  OpenAI Service is now adding more and more models
    to their service, and they want to eventually reach parity or to be only a few
    months behind OpenAI API in terms of the models available.
  prefs: []
  type: TYPE_NORMAL
- en: Resource Management
  prefs: []
  type: TYPE_NORMAL
- en: One difference between the two services is in how they handle resource management.
    A resource is a manageable item that is available through the service (whether
    it is OpenAI API or Microsoft Azure). In the context of OpenAI, examples of resources
    would be an API account or a pool of credits associated with an account. Azure
    offers a more complex set of resources, such as virtual machines, storage accounts,
    databases, virtual networks, subscriptions, and management groups.
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI offers a single API account per organization, within Azure companies
    can create multiple different resources, which they can track, monitor, and allocate
    to different cost centers. “It's just another Azure resource in general.” says
    Christopher Hoder, Senior Program Manager at Microsoft Azure OpenAI Service, which
    makes it easy to use out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Resource management within Azure is a deployment and management functionality
    that enables customers to create, update, and delete resources in Azure accounts.
    It comes with features like access control, locks, and tags to secure and organize
    customer resources after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Azure has several layers of resource management that allow companies and organizations
    to better manage pricing and resources, Hoder says. At a high level, there is
    an organizational Azure account,  and within that account, there are multiple
    Azure subscriptions. Within that, there are resource groups, and then the resources
    themselves. “All of those can be monitored and segmented and access controlled,”
    Hoder adds, which becomes especially important for deployments at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Security and Data Privacy
  prefs: []
  type: TYPE_NORMAL
- en: 'While Microsoft hasn’t said much publicly about its security so far, Divakaruni
    told us that the company is focused on three main points: content filters, monitoring
    of abuse, and safety-first approach. The team is working on more safety-enforcing
    elements and plans to use customer feedback to understand which of these elements
    will be the most meaningful for the users before they officially launch it.'
  prefs: []
  type: TYPE_NORMAL
- en: They are also working on documentation that lays out the architecture of the
    way the privacy policy is implemented that they will share with the customers
    to provide assurances that they are protecting customer data while ensuring that
    their obligations for responsibly using artificial intelligence are maintained.
    “Lots of customers that come to us have concerns about the way it is currently
    implemented on OpenAI, because it is more open, and we are addressing [those concerns],”
    says Divakaruni.
  prefs: []
  type: TYPE_NORMAL
- en: Content filters are introduced in the form of PII (Personally Identifiable Information)
    filters, filters blocking sexual and other types of content, the scope of which
    they are still establishing. “The philosophy there is providing the customers
    the right knobs to adjust and iterate the content for their particular domain,”
    Divakaruni says.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s enterprise customers are demanding with regard to security. The
    Azure OpenAI API Service team is leveraging the work it’s done for other products,
    such as Bing and Office. Microsoft has a legacy of model development and pushing
    the envelope.  “Office has provided language products for a while. So there is
    a pretty extensive content moderation capability . . .  and we have a science
    team dedicated to building out filters that are appropriate for these models in
    this space,“ says Divakaruni.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API users often request geofencing, a technology that sets a virtual
    boundary around a real-world geographical area. If data is moved outside the specified
    radius, it can trigger an action in a geo-enabled phone or other portable electronic
    devices. For example, it can alert administrators when a person enters or exits
    the geofence and generates an alert to the user’s mobile device in the form of
    a push notification or email. Geofencing enables businesses to accurately track,
    market to, and effectively alert administrators when Geofencing creates silos
    to keep the data in a particular location. Azure’s geofencing feature is still
    a work in progress, but Divakaruni says that it’s been implemented on an experimental
    basis for a few select customers, such as GitHub Co-pilot.
  prefs: []
  type: TYPE_NORMAL
- en: Model-as-a-Service at the Enterprise Level
  prefs: []
  type: TYPE_NORMAL
- en: While Azure OpenAI Service has been engaged with a lot of big enterprise customers
    on the platform, the company isn’t ready to discuss them publicly, citing privacy
    concerns and the sensitivity of public opinion. What they can mention now are
    examples of their internal services. GitHub Copilot started off on OpenAI API
    but now, mostly for scale reasons, has transitioned to Azure OpenAI Service. Other
    examples of internal services running on Azure are Dynamics 365 Customer Service,
    Power Apps, ML to code, and Power BI services.
  prefs: []
  type: TYPE_NORMAL
- en: Divakaruni says they’re seeing a lot of interest from financial services industries
    and traditional enterprises looking to enhance their customer experience. “There
    is a lot of text information to process and there's a lot of need for summarization
    and helping analysts, for example, quickly zero in on the text that is relevant
    and meaningful for them. The customer service industry, I think, is a big untapped
    domain as well. There's a vast amount of information that is locked in audio,
    which can be transcribed, in call center information that could be meaningful
    insights for a company that is trying to improve their customer experience.” Another
    set of use cases they are seeing is companies accelerating their developer productivity
    with training the GPT-3 for their internal APIs and software development kits
    to make these tools more accessible to their employees.
  prefs: []
  type: TYPE_NORMAL
- en: Divakaruni notes that many businesses whose core strength is not in AI or ML
    want to apply AI in ways that add meaningful value to their business processes
    or enhance their customer experience. They leverage Microsoft's field strength
    to help them build solutions. The Azure OpenAI Service team fully expects its
    sophisticated model-as-a-service approach to become mainstream, Hoder says. He
    notes that Microsoft provides its ready-to-use experience by embedding it into
    consumer applications such as Office and Dynamics. Customers that need more unique
    or tailored support go down a layer to services like the Power platform, which
    is aimed at business users and developers, providing no-code or low-code ways
    to tailor machine learning and AI. “If you go a little bit lower, a little bit
    more customized, a little bit more developer-focused, you end up at Cognitive
    Services. This has really been our model to provide AI capabilities through REST
    API-based services. And now we're introducing a more granular layer with OpenAI
    Service. . .  And then at the bottom layer, we have the data science-focused tooling
    with Azure Machine Learning” Hoder explains.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft sees a big customer demand for Azure OpenAI Service, but also can
    vouch for its so far success with other services, like speech service and the
    form recognizers. “We see a lot of demand for the ability to take an image, extract
    information in a structured way and extract tables and other information from
    PDFs to do automated data ingestion, and then combine analytics and search capabilities.”
    Hoder says. (See, for example, this [case study](https://news.microsoft.com/transform/progressive-gives-voice-to-flos-chatbot-and-its-as-no-nonsense-and-reassuring-as-she-is/)
    of how customers are using their REST API-based AI/ML services.)
  prefs: []
  type: TYPE_NORMAL
- en: Other Microsoft AI and ML Services
  prefs: []
  type: TYPE_NORMAL
- en: 'Will Azure OpenAI Service affect other AI/ML services from Microsoft product
    line such as Azure ML Studio? Divakaruni tells us that there is a place for both
    on the market: “It''s definitely not a winner take all. There is a need for multiple
    solutions in the market that provide for specific customer requirements.” he tells
    us. Customers’ requirements may differ substantially. They might need to generate
    and then label data specific to their particular use case. They can build a model
    from scratch using platforms like Azure Machine Learning, or SageMaker, and then
    train a distilled, smaller model for that purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, that’s a niche that’s not accessible to most people. Hoder notes
    that bringing data science capabilities to customers “broadens access, it democratizes
    it.” Divakaruni agrees: “You''ll increasingly see a trend towards the larger,
    most sophisticated models being exposed through services, as opposed to people
    going to build their own.” Why?  “The fundamental truth is that it takes a tremendous
    amount of compute and lots of data to train these models. The companies that have
    the means to develop these models are unfortunately fewer. But it''s our responsibility
    as we do to make them available for the world.”'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, data science teams from companies that can afford costly resources
    strongly prefer to build their own IP for their specific use cases, using lower-level
    ML platforms like Azure ML Studio. That demand, Divakaruni argues, is unlikely
    to disappear.
  prefs: []
  type: TYPE_NORMAL
- en: Advice for Enterprises
  prefs: []
  type: TYPE_NORMAL
- en: 'Enterprises investigating the Azure OpenAI Service, Divakaruni says, can approach
    it much as they would investigating any other cloud service: you start with what
    makes the most sense for you and then look to see if the various technologies
    meet your needs. “While the technology is cool and that certainly has a wow factor,
    you still have to start with, ‘where can this be most applicable for me as a business,
    for my group?’ And then look to solve that with a set of technologies.”'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to examine how to get from experimentation into production: 
    “What are the other things that you need to build?”  Divakaruni refers to this
    step as an “application glue that someone needs to inject around, making sure
    these models actually behave and can be used in a live application scenario”.
    That’s a nontrivial task, but enterprises need to think about this to understand
    what kind of investment a GPT-3-based application will require. Divakaruni advises
    asking, “Is this model actually producing things that are relevant when you have
    automation around? The use of the capability, when it's actually built into an
    application—is it doing what it's supposed to be doing?”
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI or Azure OpenAI Service: Which should you use?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question for companies interested in exploring GPT-3, then, is: OpenAI
    API or Azure OpenAI Service? Divakaruni maintains that the OpenAI API version
    is more suitable for companies that are exploring their options but don’t have
    any specific project implementation in mind. In terms of access, OpenAI is definitely
    further along, with its Playground making it easier for individual users and companies
    to experiment there. The OpenAI API also allows access to the latest experimental
    models and API endpoints that expand the API’s capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI Service, on the other hand, is targeting a cohort of users with
    production use cases who “graduate” from OpenAI API, or need to meet different
    compliance and privacy regulations  The two organizations encourage customers
    to experiment, and validate their use cases and firm them up with OpenAI API.
    If that platform meets their needs, Microsoft is encouraging the customers to
    stay on OpenAI API, but when their production needs become more mature and they
    start to need more compliance, they should consider transitioning to Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you saw how corporations are using GPT-3 based products at
    scale and how the new Microsoft Azure OpenAI service is paving the way for enterprises
    interested in becoming part of the GPT-3 ecosystem. We have dived into the nuances
    of scaling a GPT-3 powered product and shared some tips from the journey of large-scale
    enterprise-grade products. In Chapter 6, we will look at some of the controversies
    and challenges surrounding the OpenAI API and LLMs more generally.
  prefs: []
  type: TYPE_NORMAL
