["```py\n[B-PER|George] [I-PER|Washington] [O|is] [O|one] [O|the] [O|presidents] [O|of] [B-LOC|United] [I-LOC|States] [I-LOC|of] [I-LOC|America] [O|.]\n```", "```py\n    import datasets\n    conll2003 = datasets.load_dataset(\"conll2003\")\n    ```", "```py\n    >>> conll2003[\"train\"][0]\n    ```", "```py\n    >>> conll2003[\"train\"].features[\"ner_tags\"]\n    ```", "```py\n    >>> Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)\n    ```", "```py\n    from transformers import BertTokenizerFast\n    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n    ```", "```py\n    >>> tokenizer([\"Oh\",\"this\",\"sentence\",\"is\",\"tokenized\",\"and\", \"splitted\",\"by\",\"spaces\"], is_split_into_words=True)\n    ```", "```py\n    def tokenize_and_align_labels(examples):\n        tokenized_inputs = tokenizer(examples[\"tokens\"], \n               truncation=True, is_split_into_words=True)\n        labels = []\n        for i, label in enumerate(examples[\"ner_tags\"]):\n            word_ids = \\   \n             tokenized_inputs.word_ids(batch_index=i)\n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:\n                if word_idx is None:\n                    label_ids.append(-100)\n                elif word_idx != previous_word_idx:\n                     label_ids.append(label[word_idx])\n                else:\n                     label_ids.append(label[word_idx] if label_all_tokens else -100)\n                previous_word_idx = word_idx\n            labels.append(label_ids)\n        tokenized_inputs[\"labels\"] = labels\n        return tokenized_inputs\n    ```", "```py\n    q = tokenize_and_align_labels(conll2003['train'][4:5])\n    print(q)\n    ```", "```py\n    >>> {'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, -100, 0, 0, 0, 3, 4, 0, -100, 0, 0, 1, 2, -100, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n    ```", "```py\n    for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n        print(f\"{token:_<40} {label}\")\n    ```", "```py\n    >>> tokenized_datasets = \\ conll2003.map(tokenize_and_align_labels, batched=True)\n    ```", "```py\n    from transformers import\\ AutoModelForTokenClassification\n    model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    args = TrainingArguments(\n    \"test-ner\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    )\n    ```", "```py\n    from transformers import \\ DataCollatorForTokenClassification\n    data_collator = \\ DataCollatorForTokenClassification(tokenizer)\n    ```", "```py\n    pip install seqeval\n    ```", "```py\n    >>> metric = datasets.load_metric(\"seqeval\")\n    ```", "```py\n    example = conll2003['train'][0] \n    label_list = \\ conll2003[\"train\"].features[\"ner_tags\"].feature.names\n    labels = [label_list[i] for i in example[\"ner_tags\"]]\n    metric.compute(predictions=[labels], references=[labels])\n    ```", "```py\n    import numpy as np def compute_metrics(p):\n        predictions, labels = p\n        predictions = np.argmax(predictions, axis=2)\n        true_predictions = [\n            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)    ]\n        true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n           for prediction, label in zip(predictions, labels)\n       ]\n       results = \\ \n           metric.compute(predictions=true_predictions,  \n           references=true_labels)\n       return {\n       \"precision\": results[\"overall_precision\"],\n       \"recall\": results[\"overall_recall\"],\n       \"f1\": results[\"overall_f1\"],\n      \"accuracy\": results[\"overall_accuracy\"],\n      }\n    ```", "```py\n    trainer = Trainer(\n        model,\n        args,\n       train_dataset=tokenized_datasets[\"train\"],\n       eval_dataset=tokenized_datasets[\"validation\"],\n       data_collator=data_collator,\n       tokenizer=tokenizer,\n       compute_metrics=compute_metrics\n    )\n    trainer.train()\n    ```", "```py\n    model.save_pretrained(\"ner_model\")\n    tokenizer.save_pretrained(\"tokenizer\")\n    ```", "```py\n    id2label = {\n    str(i): label for i,label in enumerate(label_list)\n    }\n    label2id = {\n    label: str(i) for i,label in enumerate(label_list)\n    }\n    import json\n    config = json.load(open(\"ner_model/config.json\"))\n    config[\"id2label\"] = id2label\n    config[\"label2id\"] = label2id\n    json.dump(config, open(\"ner_model/config.json\",\"w\"))\n    ```", "```py\n    from transformers import pipeline\n    model = \\ AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n    nlp = \\\n    pipeline(\"ner\", model=mmodel, tokenizer=tokenizer)\n    example = \"I live in Istanbul\"\n    ner_results = nlp(example)\n    print(ner_results)\n    ```", "```py\n    [{'entity': 'B-LOC', 'score': 0.9983942, 'index': 4, 'word': 'istanbul', 'start': 10, 'end': 18}] \n    ```", "```py\n    from pprint import pprint\n    from datasets import load_dataset\n    squad = load_dataset(\"squad\")\n    for item in squad[\"train\"][1].items():\n        print(item[0])\n        pprint(item[1])\n        print(\"=\"*20)\n    ```", "```py\n    answers\n    {'answer_start': [188], 'text': ['a copper statue of Christ']}\n    ====================\n    Context\n    ('Architecturally, the school has a Catholic character. Atop the Main ' \"Building's gold dome is a golden statue of the Virgin Mary. Immediately in \" 'front of the Main Building and facing it, is a copper statue of Christ with ' 'arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main ' 'Building is the Basilica of the Sacred Heart. Immediately behind the ' 'basilica is the Grotto, a Marian place of prayer and reflection. It is a ' 'replica of the grotto at Lourdes, France where the Virgin Mary reputedly ' 'appeared to Saint Bernadette Soubirous in 1858\\. At the end of the main drive ' '(and in a direct line that connects through 3 statues and the Gold Dome), is ' 'a simple, modern stone statue of Mary.')\n    ====================\n    Id\n    '5733be284776f4190066117f'\n    ====================\n    Question\n    'What is in front of the Notre Dame Main Building?'\n    ====================\n    Title\n    'University_of_Notre_Dame'\n    ====================\n    ```", "```py\n    from datasets import load_dataset\n    squad = load_dataset(\"squad_v2\")\n    ```", "```py\n    >>> squad\n    ```", "```py\n    from transformers import AutoTokenizer\n    model = \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    ```", "```py\n    max_length = 384\n    doc_stride = 128\n    example = squad[\"train\"][173]\n    tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=max_length,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=doc_stride\n    )\n    ```", "```py\n    >>> len(tokenized_example['input_ids'])\n    >>> 2\n    ```", "```py\n    for input_ids in tokenized_example[\"input_ids\"][:2]:\n        print(tokenizer.decode(input_ids))\n        print(\"-\"*50)\n    ```", "```py\n    [CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008\\. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n    --------------------------------------------------\n    [CLS] beyonce got married in 2008 to whom? [SEP] single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift's award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119\\. 5 million. [SEP]\n    --------------------------------------------------\n    ```", "```py\n    def prepare_train_features(examples):\n        # tokenize examples\n        tokenized_examples = tokenizer(\n            examples[\"question\" if pad_on_right else \"context\"],\n            examples[\"context\" if pad_on_right else \"question\"],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_length,\n            stride=doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n        # map from a feature to its example\n        sample_mapping = \\ tokenized_examples.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = \\ tokenized_examples.pop(\"offset_mapping\")\n        tokenized_examples[\"start_positions\"] = []\n        tokenized_examples[\"end_positions\"] = []\n        # label impossible answers with CLS\n        # start and end token are the answers for each one\n        for i, offsets in enumerate(offset_mapping):\n            input_ids = tokenized_examples[\"input_ids\"][i]\n            cls_index = \\ input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = \\ tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[\"answers\"][sample_index]\n            if len(answers[\"answer_start\"]) == 0:\n                tokenized_examples[\"start_positions\"].\\ append(cls_index)\n                tokenized_examples[\"end_positions\"].\\ append(cls_index)\n            else:\n                start_char = answers[\"answer_start\"][0]\n                end_char = \\                        \n                   start_char + len(answers[\"text\"][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != / (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples[\"start_positions\"].append(cls_index)\n                    tokenized_examples[\"end_positions\"].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n        return tokenized_examples\n    ```", "```py\n    >>> tokenized_datasets = squad.map(prepare_train_features, batched=True, remove_columns=squad[\"train\"].column_names)\n    ```", "```py\n    from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n    model = AutoModelForQuestionAnswering.from_pretrained(model)\n    ```", "```py\n    args = TrainingArguments(\n    \"test-squad\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    )\n    ```", "```py\n    from transformers import default_data_collator\n    data_collator = default_data_collator\n    ```", "```py\n    trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    )\n    ```", "```py\n    trainer.train()\n    ```", "```py\n    >>> trainer.save_model(\"distillBERT_SQUAD\")\n    ```", "```py\n    from transformers import pipeline\n    qa_model = pipeline('question-answering', model='distilbert-base-cased-distilled-squad', tokenizer='distilbert-base-cased')\n    ```", "```py\n    >>> question = squad[\"validation\"][0][\"question\"]\n    >>> context = squad[\"validation\"][0][\"context\"]\n    The question and the context can be seen by using following code:\n    >>> print(\"Question:\")\n    >>> print(question)\n    >>> print(\"Context:\")\n    >>> print(context)\n    Question:\n    In what country is Normandy located?\n    Context:\n    ('The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the ' 'people who in the 10th and 11th centuries gave their name to Normandy, a ' 'region in France. They were descended from Norse (\"Norman\" comes from ' '\"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under ' 'their leader Rollo, agreed to swear fealty to King Charles III of West ' 'Francia. Through generations of assimilation and mixing with the native ' 'Frankish and Roman-Gaulish populations, their descendants would gradually ' 'merge with the Carolingian-based cultures of West Francia. The distinct ' 'cultural and ethnic identity of the Normans emerged initially in the first ' 'half of the 10th century, and it continued to evolve over the succeeding ' 'centuries.')\n    ```", "```py\n    >>> qa_model(question=question, context=context)\n    ```", "```py\n    {'answer': 'France', 'score': 0.9889379143714905, 'start': 159, 'end': 165,}\n    ```"]