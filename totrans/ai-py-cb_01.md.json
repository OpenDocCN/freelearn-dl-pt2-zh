["```py\nbash Anaconda3-2019.10-Linux-x86_64.sh\n```", "```py\n> which Python\n```", "```py\n/home/ben/anaconda3/bin/Python\n```", "```py\nsource ~/.bashrc\n```", "```py\n> jupyter notebook\n```", "```py\npip install <LIBRARY_NAME>\n```", "```py\nconda install <LIBRARY_NAME>\n```", "```py\npip install scikit-learn pandas numpy tensorflow-gpu torch\n\n```", "```py\nconda install scikit-learn pandas numpy tensorflow-gpu pytorch\n```", "```py\nconda install -c pytorch pytorch\n```", "```py\n> jupyter notebook --generate-config\n```", "```py\nimport random, string\nfrom notebook.auth import passwd\n\nc = get_config()\nc.NotebookApp.ip = '*'\n\npassword = ''.join(random.SystemRandom().choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(8))\nprint('The password is {}'.format(password))\nc.NotebookApp.password = passwd(password)\nc.NotebookApp.open_browser = False\nc.NotebookApp.port = 8888\n```", "```py\npip install swifter tqdm ray joblib jax jaxlib seaborn numba cython\n```", "```py\nwith open('command_history.py', 'w') as file:\n    for cell_input in _ih[:-1]:\n        file.write(cell_input + '\\n')\n```", "```py\n!cat command_history.py\nprint('hello, world!')\n```", "```py\n%load_ext autoreload \n```", "```py\n%autoreload 2\n```", "```py\ndef normalize(x, norm=10.0):\n  return x / norm\n\nnormalize(5, 1)\n```", "```py\n%debug\nnormalize(5, 0)\n```", "```py\n> <iPython-input-11-a940a356f993>(2)normalize() \n     1 def normalize(x, norm=10): ----> \n     2   return x / norm \n     3 \n     4 normalize(5, 1) \nipdb> a \nx = 5 \nnorm = 0 \nipdb> q\n--------------------------------------------------------------------------- ZeroDivisionError                         Traceback (most recent call last)\n<iPython-input-13-8ade44ebcb0c> in <module>()\n     1 get_iPython().magic('debug') ---->\n     2 normalize(5, 0)\n\n<iPython-input-11-a940a356f993> in normalize(a, norm)\n     1 def normalize(x, norm=10): ----> \n     2   return x / norm \n     3 \n     4 normalize(5, 1) \nZeroDivisionError: division by zero\n```", "```py\n%%timeit -n 10 -r 1\nimport time\ntime.sleep(1)\n```", "```py\n1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n```", "```py\npip install iPython-autotime\n```", "```py\n!pip install iPython-autotime\n```", "```py\n%load_ext autotime\n```", "```py\nsum([i for i in range(10)])\n```", "```py\nfrom tqdm.notebook import trange\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n```", "```py\nglobal_sum = 0.0\nfor i in trange(1000000):\n   global_sum += 1.0\n```", "```py\nfor _ in tqdm(range(10)):\n   print()\n```", "```py\n%load_ext Cython\n```", "```py\n%%cython\ndef multiply(float x, float y):\n    return x * y\n```", "```py\nmultiply(10, 5)  # 50\n```", "```py\nfrom numba import jit\n@jit\ndef add_numbers(N):\n    a = 0\n    for i in range(N):\n        a += i\n\nadd_numbers(10)\n```", "```py\ntime: 2.19 s\n```", "```py\nadd_numbers(10)\n```", "```py\ntime: 867 µs\n```", "```py\nimport jax.numpy as np\nfrom jax import jit\ndef slow_f(x):\n    return x * x + x * 2.0\n\nx = np.ones((5000, 5000)) \nfast_f = jit(slow_f) \nfast_f(x)\n```", "```py\nimport pandas as pd\nimport swifter\n\ndf = pd.read_csv('some_big_dataset.csv')\ndf['datacol'] = df['datacol'].swifter.apply(some_long_running_function)\n```", "```py\nsquarer = lambda t: t ** 2\nvfunc = np.vectorize(squarer)\ndf['squared'] = vfunc(df[col].values)\n```", "```py\n# run on multiple cores\nimport multiprocessing\n\ndataset = [\n    {\n        'data': 'large arrays and pandas DataFrames',\n        'filename': 'path/to/files/image_1.png'\n    }, # ... 100,000 datapoints\n]\n\ndef get_filename(datapoint):\n    return datapoint['filename'].split('/')[-1]\n\npool = multiprocessing.Pool(64)\nresult = pool.map(get_filename, dataset)\n```", "```py\n# run on multiple machines and their cores\nimport ray\nray.init(ignore_reinit_error=True)\n\n@ray.remote\ndef get_filename(datapoint):\n    return datapoint['filename'].split('/')[-1]\n\nresult = []\nfor datapoint in dataset:\n    result.append(get_filename.remote(datapoint))\n\n```", "```py\n\nfrom joblib import Parallel, delayed\n\ndef complex_function(x):\n    '''this is an example for a function that potentially coult take very long.\n    '''\n    return sqrt(x)\n\nParallel(n_jobs=2)(delayed(complex_function)(i ** 2) for i in range(10))\n```", "```py\n!pip install seaborn scikit-plot\n```", "```py\nimport seaborn as sns\niris = sns.load_dataset('iris')\n```", "```py\n%matplotlib inline\n# this^ is not necessary on Colab\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True)\n\ng = sns.pairplot(iris, hue='species')\n```", "```py\niris.head()\n```", "```py\nclasses = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\ny = iris['species'].apply(lambda x: classes[x]).values\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=0\n)\n```", "```py\nparams = dict(\n    max_depth=20,\n    random_state=0,\n    n_estimators=100,\n)\nclf = RandomForestClassifier(**params)\n```", "```py\nclf.fit(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(\n    clf, X_test, y_test,\n    display_labels=['setosa', 'versicolor', 'virginica'],\n    normalize='true'\n)\n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport tensorflow as tf\n\ndef create_iris_model():\n    '''\n    Create the iris classification model\n    '''\n    iris_model = Sequential()\n    iris_model.add(Dense(10, activation='selu', input_dim=4))\n    iris_model.add(Dense(3, activation='softmax'))\n    iris_model.compile(\n        optimizer='rmsprop',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    iris_model.summary()\n    return iris_model\n\niris_model = create_iris_model()\n```", "```py\ndot = tf.keras.utils.model_to_dot(\n    iris_model,\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=True,\n    dpi=96,\n    subgraph=False,\n)\ndot.write_png('iris_model_keras.png')\n```", "```py\ny_categorical = tf.keras.utils.to_categorical(y, 3)\n```", "```py\nX = (X - X.mean(axis=0)) / X.std(axis=0)\nX.mean(axis=0)\n```", "```py\narray([-4.73695157e-16, -7.81597009e-16, -4.26325641e-16, -4.73695157e-16])\n```", "```py\nX.std(axis=0)\n```", "```py\narray([1., 1., 1., 1.])\n```", "```py\n%load_ext tensorboard\nimport os\n\nlogs_base_dir = \"./logs\"\nos.makedirs(logs_base_dir, exist_ok=True)\n%tensorboard --logdir {logs_base_dir}\n```", "```py\nimport datetime\n\nlogdir = os.path.join(\n  logs_base_dir,\n  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n)\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n  logdir, histogram_freq=1\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_categorical, test_size=0.33, random_state=0\n)\niris_model.fit(\n  x=X_train,\n  y=y_train,\n  epochs=150,\n  callbacks=[tensorboard_callback]\n)\n```", "```py\nimport scikitplot as skplt\n\ny_pred = iris_model.predict(X_test).argmax(axis=1)\nskplt.metrics.plot_confusion_matrix(\n    y_test.argmax(axis=1),\n    y_pred,\n    normalize=True\n)\n```", "```py\nimport torch\nfrom torch import nn\n\niris_model = nn.Sequential(\n  torch.nn.Linear(4, 10), # equivalent to Dense in keras\n  torch.nn.SELU(),\n  torch.nn.Linear(10, 3),\n  torch.nn.Softmax(dim=1)\n)\nprint(iris_model)\n```", "```py\nfrom torch.autograd import Variable\n\nX_train = Variable(\n    torch.Tensor(X_train).float()\n)\ny_train = Variable(torch.Tensor(\n   y_train.argmax(axis=1)).long()\n)\nX_test = Variable(\n    torch.Tensor(X_test).float()\n)\ny_test = Variable(\n    torch.Tensor(y_test.argmax(axis=1)).long()\n)\n```", "```py\ncriterion = torch.nn.CrossEntropyLoss()  # cross entropy loss\noptimizer = torch.optim.RMSprop(\n    iris_model.parameters(), lr=0.01\n)\nfor epoch in range(1000):\n  optimizer.zero_grad()\n  out = iris_model(X_train)\n  loss = criterion(out, y_train)\n  loss.backward()\n  optimizer.step()\n  if epoch % 10 == 0:\n    print('number of epoch', epoch, 'loss', loss)\n```", "```py\nimport scikitplot as skplt\n\ny_pred = iris_model(X_test).detach().numpy()\nskplt.metrics.plot_confusion_matrix(\n    y_test,\n    y_pred.argmax(axis=1),\n    normalize=True\n)\nlabels = ['setosa', 'versicolor', 'virginica']\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\n```", "```py\nimport jax.numpy as np\nfrom jax import grad, jit\nimport numpy.random as npr\n\ndef predict(params, inputs):\n    for W, b in params:\n        outputs = np.dot(inputs, W) + b\n        inputs = np.tanh(outputs)\n    return outputs\n\ndef construct_network(layer_sizes=[10, 5, 1]):\n    '''Please make sure your final layer corresponds to\n    the target dimensionality.\n    '''\n    def init_layer(n_in, n_out):\n        W = npr.randn(n_in, n_out)\n        b = npr.randn(n_out,)\n        return W, b\n    return list(\n        map(init_layer, layer_sizes[:-1], layer_sizes[1:])\n    )\n\nparams = construct_network()\n```", "```py\ndef mse(preds, targets):\n    return np.sum((preds - targets)**2)\n\ndef propagate_and_error(loss_fun):\n    def error(params, inputs, targets):\n        preds = predict(params, inputs)\n        return loss_fun(preds, targets)\n    return error\n\nerror_grads = jit(grad(propagate_and_error(mse)))\n```", "```py\n!pip install category_encoders minepy eli5 seaborn\n```", "```py\n!wget http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n!wget http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n```", "```py\nimport pandas as pd\ncols = [\n    'age', 'workclass', 'fnlwgt',\n    'education', 'education-num',\n    'marital-status', 'occupation',\n    'relationship', 'race', 'sex',\n    'capital-gain', 'capital-loss',\n    'hours-per-week', 'native-country', '50k'\n]\ntrain = pd.read_csv(\n    'adult.data',\n    names=cols\n)\ntest = pd.read_csv(\n    'adult.test',\n    names=cols\n)\n```", "```py\ntrain.head()\n```", "```py\ntest.head()\n```", "```py\ntest.drop(0, axis=0, inplace=True)\n```", "```py\nimport category_encoders as ce\n\nX = train.drop('50k', axis=1)\nencoder = ce.OrdinalEncoder(cols=list(\n    X.select_dtypes(include='object').columns)[:]\n)\nencoder.fit(X, train['50k'])\nX_cleaned = encoder.transform(X)\n\nX_cleaned.head()\n```", "```py\nfrom scipy import stats\nimport seaborn as sns\nsns.set(color_codes=True)\nsns.set_context(\n 'notebook', font_scale=1.5,\n rc={\"lines.linewidth\": 2.0}\n)\nsns.distplot(train['age'], bins=20, kde=False, fit=stats.gamma)\n```", "```py\nimport numpy as np\n\nnum_cols = list(\n  set(\n    train.select_dtypes(\n      include='number'\n    ).columns\n  ) - set(['education-num'])\n) + ['50k']]\ng = sns.pairplot(\n train[num_cols],\n hue='50k',\n height=2.5,\n aspect=1,\n)\nfor i, j in zip(*np.triu_indices_from(g.axes, 1)):\n  g.axes[i, j].set_visible(False)\n```", "```py\nsns.catplot(x='50k', y='age', kind='box', data=train)\n```", "```py\nimport numpy as np\nimport os\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\nfrom minepy import MINE\nimport multiprocessing\n\ndef calc_mic(args):\n  (a, b, i1, i2) = args\n  mine = MINE(alpha=0.6, c=15, est='mic_approx')\n  mine.compute_score(a, b)\n  return (mine.mic(), i1, i2)\n\npool = multiprocessing.Pool(os.cpu_count())\n\ncorrs = np.zeros((len(X_cleaned.columns), len(X_cleaned.columns)))\nqueue = []\nfor i1, col1 in enumerate(X_cleaned.columns):\n  if i1 == 1:\n    continue\n  for i2, col2 in enumerate(X_cleaned.columns):\n    if i1 < i2:\n      continue\n    queue.append((X_cleaned[col1], X_cleaned[col2], i1, i2))\n\nresults = pool.map(calc_mic, queue)\n\nfor (mic, i1, i2) in results:\n  corrs[i1, i2] = mic\n\ncorrs = pd.DataFrame(\n    corrs,\n    columns=list(X_cleaned.columns),\n    index=list(X_cleaned.columns)\n)\n```", "```py\nmask = np.zeros_like(corrs, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(\n    h_neg=220, h_pos=10, n=50, as_cmap=True\n)\nsns.set_context(\n    'notebook', font_scale=1.1,\n    rc={'lines.linewidth': 2.0}\n)\nsns.heatmap(\n    corrs,\n    square=True,\n    mask=mask,\n    cmap=cmap, vmax=1.0, center=0.5,\n    linewidths=.5,\n    cbar_kws={\"shrink\": .5}\n)\n```", "```py\ncorrs.loc['education-num', 'education']\n```", "```py\ntrain.groupby(by='education')['education-num'].std()\n```", "```py\ntrain.isnull().any()\n```", "```py\nencoder = ce.OneHotEncoder(\n    cols=list(X.select_dtypes(include='object').columns)[:]\n)\nencoder.fit(X, train['50k'])\nX_cleaned = encoder.transform(X)\nx_cleaned_cols = X_cleaned.columns\nx_cleaned_cols\n```", "```py\ny = np.zeros((len(X_cleaned), 2))\ny[:, 0] = train['50k'].apply(lambda x: x == ' <=50K')\ny[:, 1] = train['50k'].apply(lambda x: x == ' >50K')\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\nX_cleaned = standard_scaler.fit_transform(X_cleaned)\nX_test = standard_scaler.transform(encoder.transform(test[cols[:-1]]))\n```", "```py\nimport joblib\njoblib.dump(\n    [encoder, standard_scaler, X_cleaned, X_test],\n    'adult_encoder.joblib'\n)\n```", "```py\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(20, activation='selu', input_dim=108))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_hinge',\n    metrics=['accuracy']\n)\nmodel.summary()\n```", "```py\ndef adult_feed(X_cleaned, y, batch_size=10, shuffle=True):\n  def init_batches():\n    return (\n        np.zeros((batch_size, X_cleaned.shape[1])),\n        np.zeros((batch_size, y.shape[1]))\n        )\n  batch_x, batch_y = init_batches()\n  batch_counter = 0\n  while True: # this is for every epoch\n    indexes = np.arange(X_cleaned.shape[0])\n    if shuffle == True:\n      np.random.shuffle(indexes)\n    for index in indexes:\n      batch_x[batch_counter, :] = X_cleaned[index, :]\n      batch_y[batch_counter, :] = y[index, :]\n      batch_counter += 1\n      if batch_counter >= batch_size:\n        yield (batch_x, batch_y)\n        batch_counter = 0\n        batch_x, batch_y = init_batches()\n```", "```py\nhistory = model.fit_generator(\n    adult_feed(X_cleaned, y, 10),\n    steps_per_epoch=len(X_cleaned) // 10,\n    epochs=50\n)\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['loss'])\nplt.title('Model Training')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy', 'Loss'], loc='center left')\n```", "```py\nfrom sklearn.metrics import roc_auc_score\n\npredictions = model.predict(X_test)\n# Please note that the targets have slightly different names in the test set than in the training dataset. We'll need to take care of this here:\ntarget_lookup = {' <=50K.': 0, ' >50K.': 1 }\ny_test = test['50k'].apply(\n    lambda x: target_lookup[x]\n).values\nroc_auc_score(y_test, predictions.argmax(axis=1))\n```", "```py\nfrom eli5.permutation_importance import get_score_importances\n\ndef score(data, y=None, weight=None):\n  return model.predict(data).argmax(axis=1)\n\nbase_score, score_decreases = get_score_importances(score, X_test, y_test)\nfeature_importances = np.mean(score_decreases, axis=0).mean(axis=1)\n```", "```py\nimport operator\n\nfeature_importances_annotated = {col: imp for col, imp in zip(x_cleaned_cols, feature_importances)}\nsorted_feature_importances_annotated = sorted(feature_importances_annotated.items(), key=operator.itemgetter(1), reverse=True)\n\nfor i, (k, v) in enumerate(sorted_feature_importances_annotated):\n  print('{i}: {k}: {v}'.format(i=i, k=k, v=v))\n  if i > 9:\n        break\n```"]