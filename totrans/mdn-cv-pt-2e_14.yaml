- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders and Image Manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we learned about classifying images, detecting objects
    in an image, and segmenting the pixels corresponding to objects in images. In
    this chapter, we will learn about representing an image in a lower dimension using
    **autoencoders** and then leveraging the lower-dimensional representation of an
    image to generate new images by using **variational autoencoders**. Learning how
    to represent images in a lower number of dimensions helps us manipulate (modify)
    the images to a considerable degree. We will also learn about generating novel
    images that are based on the content and style of two different images. We will
    then explore how to modify images in such a way that the image is visually unaltered;
    however, the class corresponding to the image is changed from one to another when
    the image is passed through an image classification model. Finally, we will learn
    about generating deepfakes: given a source image of person A, we generate a target
    image of person B with a similar facial expression as that of person A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we will go through the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and implementing autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolutional autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding variational autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing an adversarial attack on images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing neural style transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating deepfakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code snippets within this chapter are available in the `Chapter11` folder
    of the Github repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in previous chapters, we have learned about classifying images by training
    a model based on the input image and its corresponding label. Now let’s imagine
    a scenario where we need to cluster images based on their similarity and with
    the constraint of not having their corresponding labels. Autoencoders come in
    handy for identifying and grouping similar images.
  prefs: []
  type: TYPE_NORMAL
- en: How autoencoders work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An autoencoder takes an image as input, stores it in a lower dimension, and
    tries to reproduce the same image as output, hence the term **auto** (which, in
    short, means being able to reproduce the input). However, if we just reproduce
    the input in the output, we would not need a network, but a simple multiplication
    of the input by 1 would do. The differentiating aspect of an autoencoder from
    the typical neural network architectures we have learned about so far is that
    it encodes the information present in an image in a lower dimension and then reproduces
    the image, hence the term **encoder**. This way, images that are similar will
    have similar encoding. Further, the **decoder** works toward reconstructing the
    original image from the encoded vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to further understand autoencoders, let’s take a look at the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Typical autoencoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say the input image is a flattened version of the MNIST handwritten digits
    and the output image is the same as what is provided as input. The middlemost
    layer is the layer of encoding called the **bottleneck** layer. The operations
    happening between the input and the bottleneck layer represent the **encoder**
    and the operations between the bottleneck layer and output represent the **decoder**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the bottleneck layer, we can represent an image in a much lower dimension.
    Furthermore, with the bottleneck layer, we can reconstruct the original image.
    We leverage the bottleneck layer to solve the problems of identifying similar
    images as well as generating new images, which we will learn how to do in subsequent
    sections. The bottleneck layer helps in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Images that have similar bottleneck layer values (encoded representations) are
    likely to be similar to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By changing the node values of the bottleneck layer, we can change the output
    image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the preceding understanding, let’s do the following in subsequent sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement autoencoders from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the similarity of images based on bottleneck-layer values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn about how autoencoders are built and the
    impact of different units in the bottleneck layer on the decoder’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing vanilla autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how to build an autoencoder, let’s implement one on the MNIST
    dataset, which contains images of handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find the code in the `simple_auto_encoder_with_different_latent_size.ipynb`
    file in the `Chapter11` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and define the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the transformation that we want our images to pass through:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we see that we are converting an image into a tensor,
    normalizing it, and then passing it to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the training and validation datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the dataloaders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the network architecture. We define the `AutoEncoder` class constituting
    the encoder and decoder in the `__init__` method, along with the dimension of
    the bottleneck layer, `latent_dim`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `forward` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the preceding model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: UNet architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding output, we can see that the `Linear: 2-5 layer` is the bottleneck
    layer, where each image is represented as a three-dimensional vector. Furthermore,
    the decoder layer reconstructs the original image using the three values in the
    bottleneck layer (`Linear: 2-5 layer`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function named `train_batch` to train the model on a batch of data,
    just like we did in previous chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `validate_batch` function to validate the model on a batch of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the model, loss criterion, and optimizer. Ensure we use `MSELoss` as
    we are reconstructing the pixel values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the training and validation loss over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B18457_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Training and validation loss over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate the model on the `val_ds` dataset by looking at a few predictions/
    outputs, which were not provided during training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Autoencoder generated predictions/outputs'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the network can reproduce input with a very high level of accuracy
    even though the bottleneck layer is only three dimensions in size. However, the
    images are not as clear as expected. This is primarily because of the small number
    of nodes in the bottleneck layer. In the following image, we will visualize the
    reconstructed images after training networks with different bottleneck layer sizes:
    2, 3, 5, 10, and 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Autoencoder generated predictions/outputs'
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that as the number of vectors in the bottleneck layer increased,
    the clarity of the reconstructed image improved.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about generating clearer images using a **convolutional
    neural network** (**CNN**) and we will learn about grouping similar images.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing convolutional autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned about autoencoders and implemented them
    in PyTorch. While we have implemented them, one convenience that we had through
    the dataset was that each image had only one channel (each image was represented
    as a black and white image) and the images were relatively small (28 x 28 pixels).
    Hence, the network flattened the input and was able to train on 784 (28*28) input
    values to predict 784 output values. However, in reality, we will encounter images
    that have three channels and are much bigger than a 28 x 28 image.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about implementing a convolutional autoencoder
    that is able to work on multi-dimensional input images. However, for the purpose
    of comparison with vanilla autoencoders, we will work on the same MNIST dataset
    that we worked on in the previous section, but modify the network in such a way
    that we now build a convolutional autoencoder and not a vanilla autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'A convolutional autoencoder is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Convolutional autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we can see that the input image is represented as
    a block in the bottleneck layer that is used to reconstruct the image. The image
    goes through multiple convolutions to fetch the bottleneck representation (which
    is the **Bottleneck** layer that is obtained by passing through the **Encoder**)
    and the bottleneck representation is up-scaled to fetch the original image (the
    original image is reconstructed by passing through the **Decoder**). Note that,
    in a convolutional autoencoder, the number of channels in the bottleneck layer
    can be very high when compared to the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how a convolutional autoencoder is represented, let’s implement
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available as `conv_auto_encoder.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository at `https://bit.ly/mcvp-2e`.
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps* *1* to *4*, which are exactly the same as in the *Implementing vanilla
    autoencoders section*, are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the class of neural network, `ConvAutoEncoder`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the class and the `__init__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `encoder` architecture:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, we started with the initial number of channels,
    which is `1`, and increased it to `32`, and then further increased it to `64`
    while reducing the size of the output values by performing `nn.MaxPool2d` and
    `nn.Conv2d` operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `decoder` architecture:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `forward` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the summary of the model using the `summary` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding summary, we can see that the `MaxPool2d-6` layer with a shape
    of batch size x 64 x 2 x 2 acts as the bottleneck layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we train the model, just like we did in the previous section (in *steps
    6, 7, 8, and 9*), the variation of training and validation loss over increasing
    epochs and the predictions on input images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Variation of loss over epochs and sample predictions'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we can see that a convolutional autoencoder is able
    to make much clearer predictions of the image than the vanilla autoencoder. As
    an exercise, we suggest you vary the number of channels in the encoder and decoder
    and then analyze the variation in results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will address the question of grouping similar images
    based on bottleneck-layer values when the labels of images are not present.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping similar images using t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we represented each image in a much lower dimension
    with the assumption that similar images will have similar embeddings, and images
    that are not similar will have dissimilar embeddings. However, we have not yet
    looked at the image similarity measure or examined embedding representations in
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will plot embedding (bottleneck) vectors in a two-dimensional
    space. We can reduce the 64-dimensional vector of a convolutional autoencoder
    to a two-dimensional space by using a technique called **t-SNE**, which helps
    in compressing information in such a way that similar data points are grouped
    together while dissimilar ones are grouped far away from each other. (More about
    t-SNE is available here: [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html).)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This way, our understanding that similar images will have similar embeddings
    can be proved, as similar images should be clustered together in the two-dimensional
    plane. We will represent embeddings of all the test images in a two-dimensional
    plane:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available as `conv_auto_encoder.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize lists so that we store the latent vectors (`latent_vectors`) and
    the corresponding `classes` of images (note that we store the class of each image
    only to verify if images of the same class, which are expected to have a very
    high similarity with each other, are indeed close to each other in terms of representation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through the images in the validation dataloader (`val_dl`) and store the
    output of the encoder layer `(model.encoder(im).view(len(im),-1)` and the class
    (`clss`) corresponding to each image (`im`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the NumPy array of `latent_vectors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import t-SNE (`TSNE`) and specify that each vector is to be converted into
    a two-dimensional vector (`TSNE(2)`) so that we can plot it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit t-SNE by running the `fit_transform` method on image embeddings (`latent_vectors`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the data points after fitting t-SNE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code provides the following output (you can refer to the digital
    version of the book for the colored image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Data points (images) grouped using t-SNE'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that images belonging to the same class are clustered together, which
    reinforces our understanding that the bottleneck layer has values in such a way
    that images that look similar will have similar values.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about using autoencoders to group similar images together.
    In the next section, we will learn about using autoencoders to generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen a scenario where we can group similar images into clusters.
    Furthermore, we have learned that when we take embeddings of images that fall
    in a given cluster, we can re-construct (decode) them. However, what if an embedding
    (a latent vector) falls in between two clusters? There is no guarantee that we
    would generate realistic images. **Variational autoencoders** (**VAEs**) come
    in handy in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The need for VAEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into understanding and building a VAE, let’s explore the limitations
    of generating images from embeddings that do not fall into a cluster (or in the
    middle of different clusters). First, we generate images by sampling vectors by
    following these steps (available in the `conv_auto_encoder.ipynb` file):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the latent vectors (embeddings) of the validation images used in
    the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate random vectors with a column-level mean (`mu`) and a standard deviation
    (`sigma`) and add slight noise to the standard deviation (`torch.randn(1,100)`)
    before creating a vector from the mean and standard deviation. Finally, save them
    in a list (`rand_vectors`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the images reconstructed from the vectors obtained in *step 2* and the
    convolutional autoencoder model trained in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: Images generated from the mean and standard deviation of latent
    vectors'
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the preceding output that when we plot images that were generated
    from the mean and the noise-added standard deviation of columns of known vectors,
    we get images that are less clear than before. This is a realistic scenario, as
    we would not know beforehand about the range of embedding vectors that would generate
    realistic pictures.
  prefs: []
  type: TYPE_NORMAL
- en: '**VAEs** help us resolve this problem by generating vectors that have a mean
    of 0 and a standard deviation of 1, thereby ensuring that we generate images that
    have a mean of 0 and a standard deviation of 1.'
  prefs: []
  type: TYPE_NORMAL
- en: In essence, in a VAE, we are specifying that the bottleneck layer should follow
    a certain distribution. In the next sections, we will learn about the strategy
    we adopt with VAEs, and we will also learn about **Kullback-Leibler** (**KL**)
    divergence loss, which helps us fetch bottleneck features that follow a certain
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: How VAEs work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a VAE, we are building the network in such a way that a random vector that
    is generated from a pre-defined distribution can generate a realistic image. This
    was not possible with a simple autoencoder, as we did not specify the distribution
    of data that generates an image in the network. We enable that with a VAE by adopting
    the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the encoder is two vectors for each image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One vector represents the mean.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The other represents the standard deviation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From these two vectors, we fetch a modified vector that is the sum of the mean
    and standard deviation (which is multiplied by a random small number). The modified
    vector will be of the same number of dimensions as each vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The modified vector obtained in the previous step is passed as input to the
    decoder to fetch the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss value that we optimize for is a combination of the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'KL divergence loss: Measures the deviation of the distribution of the mean
    vector and the standard deviation vector from 0 and 1, respectively'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mean squared loss: Is the optimization we use to re-construct (decode) an image'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By specifying that the mean vector should have a distribution centered around
    0 and the standard deviation vector should be centered around 1, we are training
    the network in such a way that when we generate random noise with a mean of 0
    and standard deviation of 1, the decoder will be able to generate a realistic
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Further, note that had we only minimized KL divergence, the encoder would have
    predicted a value of 0 for the mean vector and a standard deviation of 1 for every
    input. Thus, it is important to minimize KL divergence loss and mean squared loss
    together.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s learn about KL divergence so that we can incorporate
    it into the model’s loss value calculation.
  prefs: []
  type: TYPE_NORMAL
- en: KL divergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KL divergence helps explain the difference between two distributions of data.
    In our specific case, we want our bottleneck feature values to follow a normal
    distribution with a mean of 0 and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we use KL divergence loss to understand how different our bottleneck feature
    values are with respect to the expected distribution of values having a mean of
    0 and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at how KL divergence loss helps by going through how it is
    calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_001.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, σ and μ stand for the mean and standard deviation
    values, respectively, of each input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the mean vector is distributed around 0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimizing the mean squared error (![](img/B18457_11_002.png)) in the preceding
    equation ensures that ![](img/B18457_11_003.png) is as close to 0 as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the standard deviation vector is distributed around 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The terms in the rest of the equation (except ![](img/B18457_11_004.png)) ensure
    that sigma (the standard deviation vector) is distributed around 1.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding loss function is minimized when the mean (µ) is 0 and the standard
    deviation is 1\. Further, by specifying that we are considering the logarithm
    of standard deviation, we are ensuring that sigma values cannot be negative.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the high-level strategy of building a VAE and the loss
    function to minimize in order to obtain a pre-defined distribution of encoder
    output, let’s implement a VAE in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a VAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will code up a VAE to generate new images of handwritten
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `VAE.ipynb` in the `Chapter11` folder of
    this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have the same data, all the steps in the *Implementing vanilla autoencoders*
    section remain the same except *steps 5 and 6*, where we define the network architecture
    and train the model respectively. Instead, we define these differently in the
    following code (available in the `VAE.ipynb` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1* to *step 4*, which are exactly the same as in the *Implementing vanilla
    autoencoders* section, are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network class, `VAE`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the layers in the `__init__` method that will be used in the other methods:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the `d1` and `d2` layers will correspond to the encoder section, and
    `d5` and `d6` will correspond to the decoder section. The `d31` and `d32` layers
    are the layers that correspond to the mean and standard deviation vectors, respectively.
    However, for convenience, one assumption we will make is that we will use the
    `d32` layer as a representation of the log of the variance vectors.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `encoder` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the encoder returns two vectors: one vector for the mean `(self.d31(h))`
    and the other for the log of variance values `(self.d32(h))`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the method to sample (`sampling`) from the encoder’s outputs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the exponential of *0.5*log_var* (`torch.exp(0.5*log_var)`) represents
    the standard deviation (`std`). Also, we are returning the addition of the mean
    and the standard deviation multiplied by noise generated by a random normal distribution.
    By multiplying by `eps`, we ensure that even with a slight change in the encoder
    vector, we can generate an image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `decoder` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `forward` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding method, we are ensuring that the encoder returns the mean and
    log of the variance values. Next, we are sampling with the addition of mean with
    epsilon multiplied by the log of the variance and returning the values after passing
    through the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define functions to train the model on a batch and validate it on a different
    batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are fetching the MSE loss (`RECON`) between the original
    image (`x`) and the reconstructed image (`recon_x`). Next, we are calculating
    the KL divergence loss (`KLD`) based on the formula we defined in the previous
    section. Note that the exponential of the log of the variance is the variance
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model object (`vae`) and the `optimizer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While the majority of the preceding code is familiar, let’s go over the grid
    image generation process. We are first generating a random vector (`z`) and passing
    it through the decoder (`vae.decoder`) to fetch a sample of images. The `make_grid`
    function plots images (and denormalizes them automatically, if required, before
    plotting).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of loss value variations and a sample of images generated is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: (Left): Variation in loss over increasing epochs. (Right): Images
    generated using VAE'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we are able to generate realistic new images that were not present
    in the original image.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about generating new images using VAEs. However, what
    if we want to modify images in such a way that a model cannot identify the right
    class? We will learn about the technique leveraged to achieve this in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing an adversarial attack on images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned about generating an image from random noise
    using a VAE. However, it was an unsupervised exercise. What if we want to modify
    an image in such a way that the change is so minimal that it is indistinguishable
    from the original image for a human, but still the neural network model perceives
    the object as belonging to a different class? Adversarial attacks on images come
    in handy in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial attacks refer to the changes that we make to input image values
    (pixels) so that we meet a certain objective. This is especially helpful in making
    our models robust so that they are not fooled by minor modifications. In this
    section, we will learn about modifying an image slightly in such a way that the
    pre-trained models now predict them as a different class (specified by the user)
    and not the original class. The strategy we will adopt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an image of an elephant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the target class corresponding to the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import a pre-trained model where the parameters of the model are set so that
    they are not updated (`gradients = False`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify that we calculate gradients on input image pixel values and not on the
    weight values of the network. This is because while training to fool a network,
    we do not have control over the model, but have control only over the image we
    send to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss corresponding to the model predictions and the target class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform backpropagation on the model. This step helps us understand the gradient
    associated with each input pixel value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the input image pixel values based on the gradient corresponding to each
    input pixel value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 5, 6,* and *7* over multiple epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s do this with code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `adversarial_attack.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from. We strongly recommend you execute
    the notebook in GitHub to reproduce the results and understand the steps to perform
    and the explanation of various code components in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages, the image that we work on for this use case,
    and the pre-trained ResNet50 model. Also, specify that we want to freeze parameters
    as we are not updating the model (but updating the image so that the image fools
    the model):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `image_net_classes` and assign IDs to each class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify a function to normalize (`image2tensor`) and denormalize (`tensor2image`)
    the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to predict the class of a given image (`predict_on_image`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are converting an input image into a tensor (which
    is a function to normalize using the `image2tensor` method defined earlier) and
    passing through a `model` to fetch the class (`clss`) of the object in the image
    and the probability (`prob`) of prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `attack` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `attack` function takes `image`, `model`, and `target` as input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the image into a tensor and specify that the input requires gradients
    to be calculated:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the prediction by passing the normalized input (`input`) through
    the model, and then calculate the loss value corresponding to the specified target
    class:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform backpropagation to reduce the loss:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the image very slightly based on the gradient direction:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are updating input values by a very small amount (multiplying
    by `epsilon`). Also, we are not updating the image by the magnitude of the gradient,
    but the direction of the gradient only (`input.grad.sign()`) after multiplying
    it by a very small value (`epsilon`).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Return the output after converting the tensor into an image (`tensor2image`),
    which denormalizes the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify the image to belong to a different class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the targets (`desired_targets`) that we want to convert the image to:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through the targets and specify the target class in each iteration:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify the image to attack over increasing epochs and collect them in a list:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code results in modified images and the corresponding classes:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: Modified image and its corresponding class'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that as we modify the image very slightly, the prediction class is
    completely different but with very high confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to modify images so that they are classed as we wish,
    in the next section, we will learn about modifying an image (a content image)
    in the style of our choice.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural style transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a scenario where you want to draw an image in the style of Van Gogh.
    Neural style transfer comes in handy in such a scenario. In neural style transfer,
    we use a content image and a style image, and we combine these two images in such
    a way that the combined image preserves the content of the content image while
    maintaining the style of the style image.
  prefs: []
  type: TYPE_NORMAL
- en: How neural style transfer works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An example style image and content image are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13: (Left) Style image. (Right) Content image'
  prefs: []
  type: TYPE_NORMAL
- en: We want to retain the content in the picture on the right (the content image),
    but overlay it with the color and texture in the picture on the left (the style
    image).
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of performing neural style transfer is as follows (we’ll go through
    the technique outlined in this paper: [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)).
    We try to modify the original image in a way that the loss value is split into
    **content loss** and **style loss**. Content loss refers to how **different**
    the generated image is from the content image. Style loss refers to how **correlated**
    the style image is to the generated image.'
  prefs: []
  type: TYPE_NORMAL
- en: While we mentioned that the loss is calculated based on the difference in images,
    in practice, we modify it slightly by ensuring that the loss is calculated using
    the feature layer activations of images and not the original images. For example,
    the content loss at layer 2 will be the squared difference between the *activations
    of the content image and the generated image* when passed through the second layer.
    This is because the feature layers capture certain attributes of the original
    image (for example, the outline of the foreground corresponding to the original
    image in the higher layers and the details of fine-grained objects in the lower
    layers).
  prefs: []
  type: TYPE_NORMAL
- en: 'While calculating the content loss seems straightforward, let’s try to understand
    how to calculate the similarity between the generated image and the style image.
    A technique called **gram matrix** comes in handy. The gram matrix calculates
    the similarity between a generated image and a style image, and is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_005.png)'
  prefs: []
  type: TYPE_IMG
- en: '*GM**l* is the gram matrix value at layer *l* for the style image, *S*, and
    the generated image, *G*. *N*[l]stands for the number of feature maps, where *M*[l]
    is the height times the width of the feature map.'
  prefs: []
  type: TYPE_NORMAL
- en: A gram matrix results from multiplying a matrix by the transpose of itself.
    Let’s discuss how this operation is used. Imagine that you are working on a layer
    that has a feature output of 32 x 32 x 256\. The gram matrix is calculated as
    the correlation of each of the 32 x 32 values in a channel with respect to the
    values across all channels. Thus, the gram matrix calculation results in a matrix
    that is 256 x 256 in shape. We now compare the 256 x 256 values of the style image
    and the generated image to calculate the style loss.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand why the gram matrix is important for style transfer. In a successful
    scenario, say we transferred Picasso’s style to the Mona Lisa. Let’s call the
    Picasso style *St* (for style), the original Mona Lisa *So* (for source), and
    the final image *Ta* (for target). Note that in an ideal scenario, the local features
    in image *Ta* are the same as the local features in *St*. Even though the content
    might not be the same, getting similar colors, shapes, and textures as the style
    image into the target image is what is important in style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: By extension, if we were to send *So* and extract its features from an intermediate
    layer of VGG19, they would vary from the features obtained by sending *Ta*. However,
    within each feature set, the corresponding vectors will vary relative to each
    other in a similar fashion. Say, for example, the ratio of the mean of the first
    channel to the mean of the second channel if both the feature sets will be similar.
    This is why we are trying to compute using the gram loss.
  prefs: []
  type: TYPE_NORMAL
- en: Content loss is calculated by comparing the difference in feature activations
    of the content image with respect to the generated image. Style loss is calculated
    by first calculating the gram matrix in the pre-defined layers and then comparing
    the gram matrices of the generated image and the style image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are able to calculate the style loss and the content loss, the final
    modified input image is the image that minimizes the overall loss, that is, a
    weighted average of the style and content loss.
  prefs: []
  type: TYPE_NORMAL
- en: Performing neural style transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The high-level strategy we adopt to implement neural style transfer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the input image through a pre-trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the layer values at pre-defined layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the generated image through the model and extract its values at the same
    pre-defined layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the content loss at each layer corresponding to the content image
    and generated image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the style image through multiple layers of the model and calculate the
    gram matrix values of the style image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the generated image through the same layers that the style image is passed
    through and calculate its corresponding gram matrix values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the squared difference of the gram matrix values of the two images.
    This will be the style loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The overall loss will be the weighted average of the style loss and content
    loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generated image that minimizes the overall loss will be the final image
    of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s now code up the preceding strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `neural_style_transfer.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the functions to preprocess and postprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `GramMatrix` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are computing all the possible inner products of the
    features with themselves, which is basically asking how all the vectors relate
    to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the gram matrix’s corresponding MSE loss, `GramMSELoss`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we have the gram vectors for both feature sets, it is important that they
    match as closely as possible, hence the `mse_loss`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model class, `vgg19_modified`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the class:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the features:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `forward` method, which takes the list of layers and returns the
    features corresponding to each layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model object:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the content and style images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that the images are resized to be of the same shape, 512 x 512 x
    3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify that the content image is to be modified with `requires_grad = True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the layers that define content loss and style loss, that is, which
    intermediate VGG layers we are using, to compare gram matrices for style and raw
    feature vectors for content (note, the chosen layers below are purely experimental):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function for content and style loss values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the weightage associated with content and style loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to manipulate our image such that the style of the target image resembles
    `style_image` as much as possible. Hence, we compute the `style_targets` values
    of `style_image` by computing `GramMatrix` of features obtained from a few chosen
    layers of VGG. Since the overall content should be preserved, we choose the `content_layer`
    variable with which we compute the raw features from VGG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `optimizer` and the number of iterations (`max_iters`). Even though
    we could have used Adam or any other optimizer, LBFGS is an optimizer that has
    been observed to work best in deterministic scenarios. Additionally, since we
    are dealing with exactly one image, there is nothing random. Many experiments
    have revealed that LBFGS converges faster and with lower losses in neural transfer
    settings, so we will use this optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform the optimization. In deterministic scenarios where we are iterating
    on the same tensor again and again, we can wrap the optimizer step as a function
    with zero arguments and repeatedly call it, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the variation in the loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.14: Loss variation over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the image with the combination of content and style images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.15: Style transferred image'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding picture, we can see that the image is a combination of the
    content and style images.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have seen two ways of manipulating an image: an adversarial attack
    to modify the class of an image and a style transfer to combine the style of one
    image with the content of another image. In the next section, we will learn about
    generating deepfakes, which transfer an expression from one face to another.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deepfakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have learned about two different image-to-image tasks so far: semantic segmentation
    with UNet and image reconstruction with autoencoders. Deepfakery is an image-to-image
    task that has a very similar underlying theory.'
  prefs: []
  type: TYPE_NORMAL
- en: How deepfakes work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a scenario where you want to create an application that takes an image
    of a face and changes the expression in a way that you want. Deepfakes come in
    handy in this scenario. While we made a conscious choice to not discuss the very
    latest in deepfakes in this book, techniques such as few-shot adversarial learning
    are developed to generate realistic images with the facial expression of interest.
    Knowledge of how deepfakes work and GANs (which you will learn about in the next
    chapters) will help you identify videos that are fake.
  prefs: []
  type: TYPE_NORMAL
- en: In the task of deepfakery, we have a few hundred pictures of person A and a
    few hundred pictures of person B (or, possibly a video of people A and B). The
    objective is to reconstruct person B’s face with the facial expression of person
    A and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagrams explain how the deepfake image generation process works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.16: AutoEncoder workflow, where there is a single encoder and separate
    decoders for the two classes/set of faces'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding picture, we are passing images of person A and person B through
    an encoder (**Encoder**). Once we get the latent vectors corresponding to person
    A (**Latent Face A**) and person B (**Latent Face B**), we pass the latent vectors
    through their corresponding decoders (**Decoder A** and **Decoder B**) to fetch
    the corresponding original images (**Reconstructed Face A** and **Reconstructed
    Face B**). So far, the concepts of the encoder and decoder are very similar to
    what we saw in the *Understanding autoencoders* section. However, in this scenario,
    *we have only one encoder, but two decoders* (each decoder corresponding to a
    different person). The expectation is that the latent vectors obtained from the
    encoder represent the information about the facial expression present within the
    image, while the decoder fetches the image corresponding to the person. Once the
    encoder and the two decoders are trained, while performing deepfake image generation,
    we switch the connection within our architecture as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.17: Image reconstruction from latent representation once the decoders
    are swapped'
  prefs: []
  type: TYPE_NORMAL
- en: When the latent vector of person A is passed through decoder B, the reconstructed
    face of person B will have the characteristics of person A (a smiling face) and
    vice versa for person B when passed through decoder A (a sad face).
  prefs: []
  type: TYPE_NORMAL
- en: One additional trick that helps in generating a realistic image is warping face
    images and feeding them to the network in such a way that the warped face is the
    input and the original image is expected as the output.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how it works, let’s implement the generation of fake
    images of one person with the expression of another person using autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a deepfake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now take a practical look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Generating_Deep_Fakes.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s download the data (which we have synthetically created) and the source
    code as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch face crops from the images as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the face cascade, which draws a bounding box around the face in an image.
    There’s more on cascades in the *OpenCV Utilities for Image Analysis* PDF in the
    GitHub repository. However, for now, it suffices to say that the face cascade
    draws a tight bounding box around the face present in the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function (`crop_face`) for cropping faces from an image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding function, we are passing the grayscale image (`gray`) through
    the face cascade and cropping the rectangle that contains the face. Next, we are
    returning a re-sized image (`img2`). Further, to account for a scenario where
    there is no face detected in the image, we are passing a flag to show whether
    a face is detected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Crop the images of `personA` and `personB` and place them in separate folders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dataloader and inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The dataloader is returning four tensors, `imsA`, `imsB`, `targetA`, and `targetB`.
    The first tensor (`imsA`) is a distorted (warped) version of the third tensor
    (`targetA`) and the second (`imsB`) is a distorted (warped) version of the fourth
    tensor (`targetB`).
  prefs: []
  type: TYPE_NORMAL
- en: Also, as you can see in the line `a =ImageDataset(Glob('cropped_faces_personA'),
    Glob('cropped_faces_personB'))`, we have two folders of images, one for each person.
    There is no relation between any of the faces, and in the `__iteritems__` dataset,
    we are randomly fetching two faces every time.
  prefs: []
  type: TYPE_NORMAL
- en: The key function in this step is `get_training_data`, present in `collate_fn`.
    This is an augmentation function for warping (distorting) faces. We are giving
    distorted faces as input to the autoencoder and trying to predict regular faces.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of warping is that not only does it increase our training data
    size but it also acts as a regularizer to the network, which is forced to understand
    key facial features despite being given a distorted face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect a few images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.18: Input and output combination of a batch of images'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input images are warped, while the output images are not, and
    the input-to-output images now have a one-to-one correspondence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the convolution (`_ConvLayer`) and upscaling (`_UpScale`) functions
    as well as the `Reshape` class that will be leveraged while building the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Autoencoder` model class, which has a single `encoder` and two
    decoders (`decoder_A` and `decoder_B`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_19.png)Figure 11.19: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `train_batch` logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What we are interested in is running `model(imgA, 'B')` (which would return
    an image of class B using an input image from class A), but we do not have a ground
    truth to compare it against. So instead, what we are doing is predicting `_imgA`
    from `imgA` (where `imgA` is a distorted version of `targetA`) and comparing `_imgA`
    with `targetA` using `nn.L1Loss`.
  prefs: []
  type: TYPE_NORMAL
- en: We do not need `validate_batch` as there is no validation dataset. We will predict
    new images during training and qualitatively see the progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create all the required components to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in reconstructed images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.20: Original and reconstructed images'
  prefs: []
  type: TYPE_NORMAL
- en: 'The variation in loss values is as follows (you can refer to the digital version
    of the book for the colored image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_11_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.21: Variation in loss across increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we can swap expressions from one face to another by tweaking
    an autoencoder to have two decoders instead of one. Furthermore, with more epochs,
    the reconstructed image gets more realistic.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have learned about the different variants of autoencoders:
    vanilla, convolutional, and variational. We also learned about how the number
    of units in the bottleneck layer influences the reconstructed image. Next, we
    learned about identifying images that are similar to a given image using the t-SNE
    technique. We learned that when we sample vectors, we cannot get realistic images,
    and by using VAEs, we learned about generating new images by using a combination
    of reconstruction loss and KL divergence loss. Next, we learned how to perform
    an adversarial attack on images to modify the class of an image while not changing
    the perceptive content of the image. We then learned about leveraging the combination
    of content loss and gram matrix-based style loss to optimize for content and style
    loss of images to come up with an image that is a combination of two input images.
    Finally, we learned about tweaking an autoencoder to swap two faces without any
    supervision.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about generating novel images from a given set of images,
    in the next chapter, we will build upon this topic to generate completely new
    images using variants of a network called the generative adversarial network.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the “encoder” in an autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What loss function does an autoencoder optimize for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do autoencoders help in grouping similar images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When is a convolutional autoencoder useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we get non-intuitive images if we randomly sample from the vector space
    of embeddings obtained from a vanilla/convolutional autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the loss functions that VAEs optimize for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do VAEs overcome the limitation of vanilla/convolutional autoencoders to
    generate new images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During an adversarial attack, why do we modify the input image pixels and not
    the weight values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a neural style transfer, what are the losses that we optimize for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we consider the activation of different layers and not the original image
    when calculating style and content loss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we consider the gram matrix loss and not the difference between images
    when calculating the style loss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we warp images while building a model to generate deepfakes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
