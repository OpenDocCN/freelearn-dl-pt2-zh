- en: Appendix II — Hardware Constraints for Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer models could not exist without optimized hardware. Memory and disk
    management design remain critical components. However, computing power remains
    a prerequisite. It would be nearly impossible to train the original Transformer
    described in *Chapter 2*, *Getting Started with the Architecture of the Transformer
    Model*, without GPUs. GPUs are at the center of the battle for efficient transformer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This appendix to *Chapter 3*, *Fine-Tuning BERT Models*, will take you through
    the importance of GPUs in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture and scale of transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPUs versus GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing GPUs in PyTorch as an example of how any other optimized language
    optimizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Architecture and Scale of Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A hint about hardware-driven design appears in the *The architecture of multi-head
    attention* section of *Chapter 2*, *Getting Started with the Architecture of the
    Transformer Model*:'
  prefs: []
  type: TYPE_NORMAL
- en: “However, we would only get one point of view at a time by analyzing the sequence
    with one *d*[model] block. Furthermore, it would take quite some calculation time
    to find other perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: A better way is to divide the *d*[model] = 512 dimensions of each word *x*[n]
    of *x* (all the words of a sequence) into 8 *d*[k] = 64 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then can run the 8 “heads” in parallel to speed up the training and obtain
    8 different representation subspaces of how each word relates to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant table  Description générée automatiquement](img/B17948_Appendix_II_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.1: Multi-head representations'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that there are now 8 heads running in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily see the motivation for forcing the attention heads to learn 8
    different perspectives. However, digging deeper into the motivations of the original
    8 attention heads performing different calculations in parallel led us directly
    to hardware optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '*Brown* et al. (2020), in *Language Models* *are Few-Shot Learners*, [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165),
    describe how they designed GPT models. They confirm that transformer architectures
    are hardware-driven.'
  prefs: []
  type: TYPE_NORMAL
- en: We partition the model across GPUs along with both the depth and width dimension
    to minimize data-transfer between nodes. The precise architectural parameters
    for each model are chosen based on computational efficiency and load-balancing
    in the layout of models across GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers differ in their construction (encoders and decoders) and size.
    But they all have hardware constraints that require parallel processing. We need
    to take this a step further and see why GPUs are so special.
  prefs: []
  type: TYPE_NORMAL
- en: Why GPUs are so special
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A clue to GPU-driven design emerges in the *The architecture of multi-head attention*
    section of *Chapter 2*, *Getting Started with the Architecture of the Transformer
    Model*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention is defined as “Scaled Dot-Product Attention,” which is represented
    in the following equation into which we plug *Q*, *K*, and *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Appendix_II_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now conclude the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Attention heads are designed for parallel computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention heads are based on *matmul*, matrix multiplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs are designed for parallel computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **CPU** (**central processing unit**) is optimized for *serial processing*.
    But if we run the attention heads through serial processing, it would take far
    longer to train an efficient transformer model. Very small educational transformers
    can run on CPUs. However, they do not qualify as state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: A **GPU** (**graphics processing unit**) is designed for *parallel processing*.
    Transformer models were designed for *parallel processing (GPUs)*, not *serial
    processing (CPUs)*.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are also designed for matrix multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NVIDIA GPUs, for example, contain tensor cores that accelerate matrix operations.
    A significant proportion of artificial intelligence algorithms use matrix operations,
    including transformer models. NVIDIA GPUs contain a goldmine of hardware optimization
    for matrix operations. The following links provide more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nvidia.com/en-us/data-center/tesla-p100/](https://www.nvidia.com/en-us/data-center/tesla-p100/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google’s **Tensor Processing Unit** (**TPU**) is the equivalent of NVIDIA’s
    GPUs. TensorFlow will optimize the use of tensors when using TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: For more on TPUs, see [https://cloud.google.com/tpu/docs/tpus](https://cloud.google.com/tpu/docs/tpus).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on tensors in TensorFlow, see [https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT[BASE] (110M parameters) was initially trained with 16 TPU chips. BERT[LARGE]
    (340M parameters) was trained with 64 TPU chips. For more on training BERT, see
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).
  prefs: []
  type: TYPE_NORMAL
- en: We have established that the architecture of the transformer perfectly fits
    the constraints of parallel hardware. We still need to address the issue of implementing
    source code that runs on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GPUs in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`PyTorch`, among other languages and frameworks, manages GPUs. PyTorch contains
    tensors just as TensorFlow does. A tensor may look like NumPy `np.arrays()`. However,
    NumPy is not fit for parallel processing. Tensors use the parallel processing
    features of GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensors open the doors to distributed data over GPUs in PyTorch, among other
    frameworks: [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the `Chapter03` notebook, `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb`,
    we used **CUDA** (**Compute Unified Device Architecture**) to communicate with
    NVIDIA GPUs. CUDA is an NVIDIA platform for general computing on GPUs. Specific
    instructions can be added to our source code. For more, see [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Chapter03` notebook, we used CUDA instructions to transfer our model
    and data to NVIDIA GPUs. `PyTorch` has an instruction to specify the device we
    wish to use: `torch.device`.'
  prefs: []
  type: TYPE_NORMAL
- en: For more, see [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explain `device` to illustrate the implementation of GPUs in PyTorch
    and programs in general. Let’s focus on selecting a device, data parallelism,
    loading a model to a device, and adding batch data to the device. Each bullet
    point contains the way device is used and the cell number in `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select device (Cell 3)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The program checks to see if CUDA is available on an NVIDIA GPU. If not, the
    device will be CPU:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Data parallelism (Cell 16)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model can be distributed for parallel computing over several GPUs if more
    than one GPU is available:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Loading the model to the device (cell 16)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model is sent to the device:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Add batch to device (cell 20) for training and validation data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batches of data are added to the GPUs available (`1` to `n`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the following section, I describe tests I made to illustrate the use of GPUs
    for transformer models by running a notebook of the chapter with three runtime
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Testing GPUs with Google Colab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, I describe informal tests I ran to illustrate the potential
    of GPUs. We’ll use the same `Chapter03` notebook: `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I ran the notebook on three scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab Free with a CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Colab Free with a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Colab Pro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Colab Free with a CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is nearly impossible to fine-tune or train a transformer model with millions
    or billions of parameters on a CPU. CPUs are mostly sequential. Transformer models
    are designed for parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Runtime** menu and **Change Runtime Type** submenu, you can select
    a hardware accelerator: **None (CPU)**, **GPU**, or **TPU**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This test was run with **None (CPU)**, as shown in *Figure II.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_Appendix_II_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.2: Selecting a hardware accelerator'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the notebook reaches the training loop, it slows down right from the start:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_Appendix_II_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.3: Training loop'
  prefs: []
  type: TYPE_NORMAL
- en: After 15 minutes, nothing has really happened.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs are not designed for parallel processing. Transformer models are designed
    for parallel processing, so part from toy models, they require GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab Free with a GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s go back to the notebook settings to select a **GPU**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_Appendix_II_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure II.4 Selecting a GPU
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, I tested Google Colab, and an NVIDIA K80 was attributed
    to the VM with CUDA 11.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17948_Appendix_II_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.5: NVIDIA K80 GPU activated'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop advanced normally and lasted about 20 minutes. However, Google
    Colab VMs, at the time of these tests (November 2021), do not provide more than
    one GPU. GPUs are expensive. In any case, *Figure II.6*, shows that the training
    loop was performed in a reasonable time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_Appendix_II_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.6: Training loop with a K80 GPU'
  prefs: []
  type: TYPE_NORMAL
- en: I found it interesting to see whether Google Colab Pro provides faster GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab Pro with a GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VM activated with Google Colab provided an NVIDIA P100 GPU, as shown in
    *Figure II.7*. That was interesting because the original Transformer was trained
    with 8 NVIDIA P100s as stated in *Vaswani* et al.(2017), *Attention is All you
    Need*. It took 12 hours to train the base models with 10⁶×65 parameters and with
    8 GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with medium confidence](img/B17948_Appendix_II_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.7: The Google Colab Pro VM was provided with a P100 GPU'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop time was considerably reduced and lasted less than 10 minutes,
    as shown in *Figure II.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_Appendix_II_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure II.8: Training loop with a P100 GPU'
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
