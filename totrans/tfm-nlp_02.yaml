- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Getting Started with the Architecture of the Transformer Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 起步使用Transformer模型的架构
- en: 'Language is the essence of human communication. Civilizations would never have
    been born without the word sequences that form language. We now mostly live in
    a world of digital representations of language. Our daily lives rely on NLP digitalized
    language functions: web search engines, emails, social networks, posts, tweets,
    smartphone texting, translations, web pages, speech-to-text on streaming sites
    for transcripts, text-to-speech on hotline services, and many more everyday functions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是人类交流的本质。如果没有形成语言的词序列，文明将永远不会诞生。现在，我们大多生活在语言的数字表示世界中。我们的日常生活依赖于NLP数字化语言功能：网络搜索引擎、电子邮件、社交网络、帖子、推文、智能手机短信、翻译、网页、流媒体网站上的语音转文字、热线服务上的文字转语音以及许多其他日常功能。
- en: '*Chapter 1*, *What are Transformers?*, explained the limits of RNNs and the
    birth of cloud AI transformers taking over a fair share of design and development.
    The role of the Industry 4.0 developer is to understand the architecture of the
    original Transformer and the multiple transformer ecosystems that followed.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1章*，*什么是Transformer？*，解释了RNN的局限性以及云AI转换器的诞生，占据了设计和开发的一部分份额。工业4.0开发者的角色是理解原始Transformer的架构以及随之而来的多个transformer生态系统。'
- en: In December 2017, Google Brain and Google Research published the seminal *Vaswani*
    et al., *Attention is All You Need* paper. The Transformer was born. The Transformer
    outperformed the existing state-of-the-art NLP models. The Transformer trained
    faster than previous architectures and obtained higher evaluation results. As
    a result, transformers have become a key component of NLP.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年12月，Google Brain和Google Research发表了具有开创性意义的*瓦斯瓦尼*等人的*Attention is All You
    Need*论文。Transformer诞生了。Transformer超越了现有的最先进的NLP模型。Transformer比以前的架构训练速度更快，并获得了更高的评估结果。因此，transformer已成为NLP的关键组成部分。
- en: The idea of the attention head of the Transformer is to do away with recurrent
    neural network features. In this chapter, we will open the hood of the Transformer
    model described by *Vaswani* et al. (2017) and examine the main components of
    its architecture. We will explore the fascinating world of attention and illustrate
    the key components of the Transformer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的注意头的思想是消除循环神经网络特征。在本章中，我们将揭开*瓦斯瓦尼*等人（2017）描述的Transformer模型的面纱，检查其架构的主要组件。我们将探索引人入胜的关注世界，并说明Transformer的关键组件。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: The architecture of the Transformer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer的架构
- en: The Transformer’s self-attention model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer的自注意力模型
- en: The encoding and decoding stacks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和解码堆栈
- en: Input and output embedding
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出嵌入
- en: Positional embedding
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: Self-attention
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Multi-head attention
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Masked multi-attention
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩模多头注意力
- en: Residual connections
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Normalization
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化
- en: Feedforward network
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Output probabilities
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出概率
- en: Let’s dive directly into the structure of the original Transformer’s architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接深入原始Transformer架构的结构。
- en: 'The rise of the Transformer: Attention is All You Need'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer的崛起：Attention is All You Need
- en: In December 2017, *Vaswani* et al. (2017) published their seminal paper, *Attention
    is All You Need.* They performed their work at Google Research and Google Brain.
    I will refer to the model described in *Attention is All You Need* as the “original
    Transformer model” throughout this chapter and book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年12月，*瓦斯瓦尼*等人（2017）发表了他们具有开创性意义的论文*Attention is All You Need*。他们在Google
    Research和Google Brain进行了这项工作。在本章和本书中，我将称*Attention is All You Need*中描述的模型为“原始Transformer模型”。
- en: '*Appendix I*, *Terminology of Transformer Models*, can help the transition
    from the classical usage of deep learning words to transformer vocabulary. *Appendix
    I* summarizes some of the changes to the classical AI definition of neural network
    models.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录I*，*Transformer模型术语*，可以帮助从深度学习词汇的经典用法过渡到Transformer词汇。*附录I*总结了经典人工智能神经网络模型的一些变化。'
- en: In this section, we will look at the structure of the Transformer model they
    built. In the following sections, we will explore what is inside each component
    of the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一下他们构建的Transformer模型的结构。在接下来的章节中，我们将探讨模型的每个组件内部的内容。
- en: 'The original Transformer model is a stack of 6 layers. The output of layer
    *l* is the input of layer *l*+1 until the final prediction is reached. There is
    a 6-layer encoder stack on the left and a 6-layer decoder stack on the right:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Transformer模型是一个包含6个层叠的堆栈。第*l*层的输出是第*l*+1层的输入，直到达到最终预测。左侧有一个6层编码器堆栈，右侧有一个6层解码器堆栈：
- en: '![](img/B17948_02_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_01.png)'
- en: 'Figure 2.1: The architecture of the Transformer'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：Transformer的架构
- en: On the left, the inputs enter the encoder side of the Transformer through an
    attention sublayer and a feedforward sublayer. On the right, the target outputs
    go into the decoder side of the Transformer through two attention sublayers and
    a feedforward network sublayer. We immediately notice that there is no RNN, LSTM,
    or CNN. Recurrence has been abandoned in this architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，输入通过一个注意力子层和一个前馈子层进入Transformer的编码器侧。在右侧，目标输出通过两个注意力子层和一个前馈网络子层进入Transformer的解码器侧。我们立即注意到这个架构中没有RNN、LSTM或CNN。在这种架构中已经放弃了循环。
- en: 'Attention has replaced recurrence functions requiring increasing parameters
    as the distance between two words increases. The attention mechanism is a “word
    to word” operation. It is actually a token-to-token operation, but we will keep
    it to the word level to keep the explanation simple. The attention mechanism will
    find how each word is related to all other words in a sequence, including the
    word being analyzed itself. Let’s examine the following sequence:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力已经取代了需要随着两个单词之间距离增加而增加的循环函数。注意力机制是一种“单词对单词”的操作。实际上它是一个标记到标记的操作，但为了保持解释简单，我们将其保持在单词级别上。注意力机制将找到每个单词与序列中所有其他单词的关系，包括正在分析的单词本身。让我们来看一下以下序列：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Attention will run dot products between word vectors and determine the strongest
    relationships of a word with all the other words, including itself (“cat” and
    “cat”):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力将在单词向量之间运行点积，并确定一个单词与所有其他单词（包括自己）的最强关系（“猫”和“猫”）：
- en: '![](img/B17948_02_02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_02.png)'
- en: 'Figure 2.2: Attending to all the words'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：关注所有单词
- en: The attention mechanism will provide a deeper relationship between words and
    produce better results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制将提供单词之间更深的关系并产生更好的结果。
- en: 'For each attention sublayer, the original Transformer model runs not one but
    eight attention mechanisms in parallel to speed up the calculations. We will explore
    this architecture in the following section, *The encoder stack*. This process
    is named “multi-head attention,” providing:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个注意子层，原始的Transformer模型不是运行一个而是运行八个并行的注意机制以加快计算速度。我们将在下一节“编码器堆栈”中探讨这种架构。这个过程被称为“多头注意力”，提供：
- en: A broader in-depth analysis of sequences
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对序列的更广泛的深入分析
- en: The preclusion of recurrence reducing calculation operations
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除了减少计算操作的循环先验
- en: Implementation of parallelization, which reduces training time
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化的实现，可以减少训练时间
- en: Each attention mechanism learns different perspectives of the same input sequence
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个注意机制学习相同输入序列的不同视角
- en: '*Attention replaced recurrence*. However, there are several other creative
    aspects of the Transformer, which are as critical as the attention mechanism,
    as you will see when we look inside the architecture.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意力取代了循环*。然而，Transformer还有其他几个创造性方面，它们和注意力机制一样关键，当我们深入研究架构时，你将会看到。'
- en: We just looked at the Transformer structure from the outside. Let’s now go into
    each component of the Transformer. We will start with the encoder.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚从外部看了Transformer的结构。现在让我们深入了解Transformer的每个组件。我们将从编码器开始。
- en: The encoder stack
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器堆栈
- en: 'The layers of the encoder and decoder of the original Transformer model are
    *stacks of layers.* Each layer of the encoder stack has the following structure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Transformer模型的编码器和解码器的层都是*层叠的层*。编码器堆栈的每一层都有以下结构：
- en: '![](img/B17948_02_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_03.png)'
- en: 'Figure 2.3: A layer of the encoder stack of the Transformer'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：Transformer编码器堆栈的一层
- en: 'The original encoder layer structure remains the same for all *N*=6 layers
    of the Transformer model. Each layer contains two main sublayers: a multi-headed
    attention mechanism and a fully connected position-wise feedforward network.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原始编码器层结构对于Transformer模型的所有*N*=6层保持不变。每一层包含两个主要的子层：一个多头注意力机制和一个完全连接的位置逐层前馈网络。
- en: 'Notice that a residual connection surrounds each main sublayer, *sublayer*(*x*),
    in the Transformer model. These connections transport the unprocessed input *x*
    of a sublayer to a layer normalization function. This way, we are certain that
    key information such as positional encoding is not lost on the way. The normalized
    output of each layer is thus:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Transformer模型中，每个主要子层*子层*(*x*)周围都有一个残差连接。这些连接将子层的未处理输入*x*传输到层标准化函数。这样，我们可以确定诸如位置编码之类的关键信息在传输过程中不会丢失。因此，每层的标准化输出如下：
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*层标准化* (*x* + *子层*(*x*))'
- en: Though the structure of each of the *N*=6 layers of the encoder is identical,
    the content of each layer is not strictly identical to the previous layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编码器的*N*=6层的结构是相同的，但每层的内容并不严格与上一层相同。
- en: For example, the embedding sublayer is only present at the bottom level of the
    stack. The other five layers do not contain an embedding layer, and this guarantees
    that the encoded input is stable through all the layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，嵌入子层仅出现在堆栈的底层。其他五层不包含嵌入层，这保证了经过所有层的编码输入是稳定的。
- en: Also, the multi-head attention mechanisms perform the same functions from layer
    1 to 6\. However, they do not perform the same tasks. Each layer learns from the
    previous layer and explores different ways of associating the tokens in the sequence.
    It looks for various associations of words, just like we look for different associations
    of letters and words when we solve a crossword puzzle.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多头注意力机制从第1层到第6层执行相同的功能。但它们并不执行相同的任务。每一层都从前一层学习，并探索关联序列中标记的不同方式。它寻找单词的不同关联，就像我们在解决填字游戏时寻找字母和单词的不同关联一样。
- en: The designers of the Transformer introduced a very efficient constraint. The
    output of every sublayer of the model has a constant dimension, including the
    embedding layer and the residual connections. This dimension is *d*[model] and
    can be set to another value depending on your goals. In the original Transformer
    architecture, *d*[model] = 512.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的设计者引入了一个非常高效的约束。模型的每个子层的输出都具有恒定的维度，包括嵌入层和残差连接。该维度为*d*[model]，可以根据您的目标设置为另一个值。在原始的Transformer架构中，*d*[model]
    = 512。
- en: '*d*[model] has a powerful consequence. Practically all the key operations are
    dot products. As a result, the dimensions remain stable, which reduces the number
    of operations to calculate, reduces machine consumption, and makes it easier to
    trace the information as it flows through the model.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*d*[model]具有强大的影响。几乎所有关键操作都是点积。因此，维度保持稳定，这减少了计算操作的数量，减少了机器的消耗，并使跟踪信息在模型中流动变得更容易。'
- en: This global view of the encoder shows the highly optimized architecture of the
    Transformer. In the following sections, we will zoom into each of the sublayers
    and mechanisms.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的这个全局视图展示了Transformer的高度优化的架构。在接下来的几节中，我们将深入研究每个子层和机制。
- en: We will begin with the embedding sublayer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从嵌入子层开始。
- en: Input embedding
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: 'The input embedding sublayer converts the input tokens to vectors of dimension
    *d*[model] = 512 using learned embeddings in the original Transformer model. The
    structure of the input embedding is classical:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入子层使用原始Transformer模型中学习的嵌入将输入标记转换为维度为*d*[model] = 512的向量。输入嵌入的结构是经典的：
- en: '![](img/B17948_02_04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_04.png)'
- en: 'Figure 2.4: The input embedding sublayer of the Transformer'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：Transformer的输入嵌入子层
- en: The embedding sublayer works like other standard transduction models. A tokenizer
    will transform a sentence into tokens. Each tokenizer has its methods, such as
    BPE, word piece, and sentence piece methods. The Transformer initially used BPE,
    but other models use other methods.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入子层的工作方式类似于其他标准的转换模型。标记器将一个句子转换为标记。每个标记器都有其方法，如BPE、单词片段和句子片段方法。Transformer最初使用了BPE，但其他模型使用其他方法。
- en: 'The goals are similar, and the choice depends on the strategy chosen. For example,
    a tokenizer applied to the sequence `the Transformer is an innovative NLP model!`
    will produce the following tokens in one type of model:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是相似的，选择取决于选择的策略。例如，应用于序列 `Transformer is an innovative NLP model!`的标记器将在一种模型中产生以下标记：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You will notice that this tokenizer normalized the string to lowercase and
    truncated it into subparts. A tokenizer will generally provide an integer representation
    that will be used for the embedding process. For example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到这个分词器将字符串标准化为小写字母并将其截断为子部分。分词器通常会提供一个用于嵌入过程的整数表示。例如：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There is not enough information in the tokenized text at this point to go further.
    The tokenized text must be embedded.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，标记化文本中没有足够的信息进行更深入的分析。标记化文本必须被嵌入。
- en: The Transformer contains a learned embedding sublayer. Many embedding methods
    can be applied to the tokenized input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer包含了一个学习到的嵌入子层。许多嵌入方法可以应用到标记化的输入中。
- en: I chose the skip-gram architecture of the `word2vec` embedding approach Google
    made available in 2013 to illustrate the embedding sublayer of the Transformer.
    A skip-gram will focus on a center word in a window of words and predicts *context*
    words. For example, if word(i) is the center word in a two-step window, a skip-gram
    model will analyze word(i-2), word(i-1), word(i+1), and word(i+2). Then the window
    will *slide* and repeat the process. A skip-gram model generally contains an input
    layer, weights, a hidden layer, and an output containing the word embeddings of
    the tokenized input words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了`word2vec`嵌入方法中谷歌在2013年发布的skip-gram架构来说明Transformer的嵌入子层。skip-gram会关注窗口中的中心词，并预测*上下文*词。例如，如果word(i)是一个两步窗口中的中心词，skip-gram模型会分析word(i-2)，word(i-1)，word(i+1)，和word(i+2)。然后窗口会*滑动*并重复这个过程。skip-gram模型通常包含输入层，权重，隐藏层，以及包含标记化输入词的词嵌入的输出。
- en: 'Suppose we need to perform embedding for the following sentence:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要为以下句子进行嵌入：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will focus on two words, `black` and `brown`. The word embedding vectors
    of these two words should be similar.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于两个词，`black`和`brown`。这两个词的词嵌入向量应该是相似的。
- en: 'Since we must produce a vector of size *d*[model] = 512 for each word, we will
    obtain a size `512` vector embedding for each word:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们必须为每个词生成一个大小为*d*[model] = 512的向量，我们将为每个词获得大小为`512`的向量嵌入：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The word `black` is now represented by `512` dimensions. Other embedding methods
    could be used and *d*[model] could have a higher number of dimensions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 单词`black`现在用`512`维度表示。其他嵌入方法可以被使用，*d*[model]可以有更多维度。
- en: 'The word embedding of `brown` is also represented by `512` dimensions:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`brown`的词嵌入也用`512`维度表示：'
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To verify the word embedding produced for these two words, we can use cosine
    similarity to see if the word embeddings of the words `black` and `brown` are
    similar.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这两个词产生的词嵌入，我们可以使用余弦相似度来查看单词`black`和`brown`的词嵌入是否相似。
- en: 'Cosine similarity uses Euclidean (L2) norm to create vectors in a unit sphere.
    The dot product of the vectors we are comparing is the cosine between the points
    of those two vectors. For more on the theory of cosine similarity, you can consult
    scikit-learn’s documentation, among many other sources: [https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度使用欧几里得（L2）范数在一个单位球中创建向量。我们比较的向量的点积是这两个向量之间的余弦。更多关于余弦相似度理论的内容，您可以查阅scikit-learn的文档，以及其他很多来源：[https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity)。
- en: 'The cosine similarity between the black vector of size *d*[model] = 512 and
    the brown vector of size *d*[model] = 512 in the embedding of the example is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例的嵌入中，大小为*d*[model] = 512的黑色向量与大小为*d*[model] = 512的棕色向量之间的余弦相似度是：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The skip-gram produced two vectors that are close to each other. It detected
    that black and brown form a color subset of the dictionary of words.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram产生了两个彼此接近的向量。它检测到black和brown形成了一个颜色子集的词典。
- en: The Transformer’s subsequent layers do not start empty-handed. They have learned
    word embeddings that already provide information on how the words can be associated.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的后续层并不是空手起步。它们已经学会了词嵌入，这些词嵌入已经提供了有关如何关联这些词的信息。
- en: However, a big chunk of information is missing because no additional vector
    or information indicates a word’s position in a sequence.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于没有额外的向量或信息指示序列中单词的位置，很多信息都缺失了。
- en: 'The designers of the Transformer came up with yet another innovative feature:
    positional encoding.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的设计者提出了另一个创新特性：位置编码。
- en: Let’s see how positional encoding works.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看位置编码是如何工作的。
- en: Positional encoding
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'We enter this positional encoding function of the Transformer with no idea
    of the position of a word in a sequence:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入 Transformer 的这个位置编码函数时并不知道词在序列中的位置：
- en: '![](img/B17948_02_05.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_05.png)'
- en: 'Figure 2.5: Positional encoding'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：位置编码
- en: We cannot create independent positional vectors that would have a high cost
    on the training speed of the Transformer and make attention sublayers overly complex
    to work with. The idea is to add a positional encoding value to the input embedding
    instead of having additional vectors to describe the position of a token in a
    sequence.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能创建独立的位置向量，这将对 Transformer 的训练速度产生较高的成本，并使注意力子层过于复杂。这个想法是向输入嵌入中添加一个位置编码值，而不是添加额外的向量来描述序列中一个标记的位置。
- en: Industry 4.0 is pragmatic and model-agnostic. The original Transformer model
    has only one vector that contains word embedding and position encoding. We will
    explore disentangled attention with a separate matrix for positional encoding
    in *Chapter 15, From NLP to Task-Agnostic Transformer Models*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 是实用的，且不受模型限制。原始 Transformer 模型只有一个包含词嵌入和位置编码的向量。我们将在 *第 15 章，从自然语言处理到任务不可知的
    Transformer 模型* 中探索使用一个独立的矩阵来进行位置编码的分离注意力。
- en: The Transformer expects a fixed size *d*[model] = 512 (or other constant value
    for the model) for each vector of the output of the positional encoding function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 期望输出的每个向量都具有固定大小 *d*[model] = 512（或模型的其他常量值）。
- en: 'If we go back to the sentence we used in the word embedding sublayer, we can
    see that black and brown may be semantically similar, but they are far apart in
    the sentence:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到我们在词嵌入子层中使用的句子，我们可以看到黑色和棕色可能在语义上相似，但在句子中相距甚远：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The word `black` is in position 2, `pos=2`, and the word `brown` is in position
    10, `pos=10`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 `black` 处于位置 2，`pos=2`，而单词 `brown` 处于位置 10，`pos=10`。
- en: Our problem is to find a way to add a value to the word embedding of each word
    so that it has that information. However, we need to add a value to the *d*[model]
    = 512 dimensions! For each word embedding vector, we need to find a way to provide
    information to `i` in the `range(0,512)` dimensions of the word embedding vector
    of `black` and `brown`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问题是找到一种方法，向每个单词的词嵌入中添加一个值，以便它具有该信息。但是，我们需要向 *d*[model] = 512 维度添加一个值！对于每个单词嵌入向量，我们需要找到一种方法，为
    `black` 和 `brown` 的词嵌入向量的 `range(0,512)` 维度中的 `i` 提供信息。
- en: '*There are many ways to achieve positional encoding*. This section will focus
    on the designers’ clever way to use a unit sphere to represent positional encoding
    with sine and cosine values that will thus remain small but useful.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*有许多方法可以实现位置编码*。本节将重点介绍设计者巧妙地使用单位球来表示位置编码，使用正弦和余弦值，因此保持小但有用。'
- en: '*Vaswani* et al. (2017) provide sine and cosine functions so that we can generate
    different frequencies for the positional encoding (**PE**) for each position and
    each dimension *i* of the *d*[model] = 512 of the word embedding vector:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Vaswani* 等人（2017年）提供了正弦和余弦函数，以便我们可以为每个位置和 *d*[model] = 512 的词嵌入向量的每个维度 *i*
    生成不同的频率来生成位置编码 (**PE**)：'
- en: '![](img/B17948_02_016.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_016.png)'
- en: '![](img/B17948_02_017.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_017.png)'
- en: If we start at the beginning of the word embedding vector, we will begin with
    a constant (`512`), `i=0,` and end with `i=511`. This means that the sine function
    will be applied to the even numbers and the cosine function to the odd numbers.
    Some implementations do it differently. In that case, the domain of the sine function
    can be ![](img/B17948_02_001.png) and the domain of the cosine function can be
    ![](img/B17948_02_002.png). This will produce similar results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从词嵌入向量的开头开始，我们将从一个常数开始 (`512`)，`i=0`，并以 `i=511` 结束。这意味着正弦函数将应用于偶数，余弦函数将应用于奇数。一些实现方式可能不同。在这种情况下，正弦函数的定义域可能是
    ![](img/B17948_02_001.png)，余弦函数的定义域可能是 ![](img/B17948_02_002.png)。这将产生类似的结果。
- en: 'In this section, we will use the functions the way they were described by *Vaswani*
    et al. (2017). A literal translation into Python pseudo code produces the following
    code for a positional vector `pe[0][i]` for a position `pos`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 *Vaswani* 等人（2017年）描述的函数方式。将其直译为 Python 伪代码产生了以下代码，用于表示位置向量 `pe[0][i]`
    的位置 `pos`：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Google Brain Trax and Hugging Face, among others, provide ready-to-use libraries
    for the word embedding section and the present positional encoding section. Thus,
    you don’t need to run the code I share in this section. However, if you wish to
    explore the code, you will find it in the Google Colaboratory `positional_encoding.ipynb`
    notebook and the `text.txt` file in this chapter’s GitHub repository.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain Trax 和 Hugging Face 等公司提供了用于单词嵌入部分和现在位置编码部分的即用型库。因此，你不需要运行我在本节中分享的代码。但是，如果你想探索代码，你可以在
    Google Colaboratory 的 `positional_encoding.ipynb` 笔记本和本章的 GitHub 仓库中的 `text.txt`
    文件中找到它。
- en: Before going further, you might want to see the plot of the sine function, for
    example, for `pos=2`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你可能想看一下正弦函数的图表，例如 `pos=2`。
- en: 'You can Google the following plot, for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以谷歌以下图表：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Just enter the plot request:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 只需输入绘图请求：
- en: '![](img/B17948_02_06.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_06.png)'
- en: 'Figure 2.6: Plotting with Google'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：使用Google绘图
- en: 'You will obtain the following graph:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获得以下图表：
- en: '![](img/B17948_02_07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_07.png)'
- en: 'Figure 2.7: The graph'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：图
- en: 'If we go back to the sentence we are parsing in this section, we can see that
    `black` is in position `pos=2` and `brown` is in position `pos=10`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到这一部分正在解析的句子，我们可以看到 `black` 位于位置 `pos=2`，而 `brown` 位于位置 `pos=10`：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we apply the sine and cosine functions literally for `pos=2`, we obtain
    a size=`512` positional encoding vector:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将正弦和余弦函数直接应用于 `pos=2`，我们将获得大小为`512`的位置编码向量：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We also obtain a size=`512` positional encoding vector for position 10, *pos=10*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为位置10获得了大小为`512`的位置编码向量，*pos=10*：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When we look at the results we obtained with an intuitive literal translation
    of the *Vaswani* et al. (2017) functions into Python, we would like to check whether
    the results are meaningful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们直观地将 *Vaswani* 等人（2017年）的函数翻译成Python并查看结果时，我们希望检查结果是否有意义。
- en: 'The cosine similarity function used for word embedding comes in handy for having
    a better visualization of the proximity of the positions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 用于单词嵌入的余弦相似度函数对于更好地可视化位置的接近度非常方便：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The similarity between the position of the words `black` and `brown` and the
    lexical field (groups of words that go together) similarity is different:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 `black` 和 `brown` 的位置之间的相似度以及词汇领域（一起使用的单词组）的相似度是不同的：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The encoding of the position shows a lower similarity value than the word embedding
    similarity.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 位置的编码显示出比单词嵌入相似度更低的相似度值。
- en: The positional encoding has taken these words apart. Bear in mind that word
    embeddings will vary with the corpus used to train them. The problem is now how
    to add the positional encoding to the word embedding vectors.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码已经将这些词分开。请记住，单词嵌入会随用于训练它们的语料库而变化。现在的问题是如何将位置编码添加到单词嵌入向量中。
- en: Adding positional encoding to the embedding vector
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将位置编码添加到嵌入向量中
- en: 'The authors of the Transformer found a simple way by merely adding the positional
    encoding vector to the word embedding vector:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的作者们发现了一种简单的方法，只需将位置编码向量简单地添加到单词嵌入向量中：
- en: '![](img/B17948_02_08.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_08.png)'
- en: 'Figure 2.8: Positional encoding'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：位置编码
- en: 'If we go back and take the word embedding of `black`, for example, and name
    it y[1] = *black*, we are ready to add it to the positional vector *pe*(*2*)we
    obtained with positional encoding functions. We will obtain the positional encoding
    *pc*(*black*) of the input word `black`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回过头来，例如，提取 `black` 的单词嵌入，然后将其命名为 y[1] = *black*，我们就可以将其添加到通过位置编码函数获得的位置向量
    *pe*(*2*)中。我们将获得输入单词 `black` 的位置编码 *pc*(*black*)：
- en: '*pc*(*black*) = y[1] + *pe*(2)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*pc*(*black*) = y[1] + *pe*(2)'
- en: The solution is straightforward. However, if we apply it as shown, we might
    lose the information of the word embedding, which will be minimized by the positional
    encoding vector.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案很简单。然而，如果我们按照所示应用它，我们可能会丢失单词嵌入的信息，这将被位置编码向量最小化。
- en: There are many possibilities to increase the value of y[1] to make sure that
    the information of the word embedding layer can be used efficiently in the subsequent
    layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可能性来增加 y[1] 的价值，以确保单词嵌入层的信息可以在后续层中有效使用。
- en: 'One of the many possibilities is to add an arbitrary value to y[1], the word
    embedding of `black`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 许多可能性之一是向 y[1]，即 `black` 的单词嵌入添加一个任意值：
- en: y[1] * *math.sqrt*(*d_model*)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: y[1] * *math.sqrt*(*d_model*)
- en: 'We can now add the positional vector to the embedding vector of the word `black`,
    which are both of the same size (`512`):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将单词`black`的位置向量加到其嵌入向量中，它们都是相同的大小（`512`）：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The result obtained is the final positional encoding vector of dimension *d*[model]
    = 512:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的结果是维度为*d*[model] = 512*的最终位置编码向量：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The same operation is applied to the word `brown` and all of the other words
    in a sequence.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对单词`brown`和序列中的所有其他单词应用相同的操作。
- en: 'We can apply the cosine similarity function to the positional encoding vectors
    of `black` and `brown`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将余弦相似性函数应用于`black`和`brown`的位置编码向量：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now have a clear view of the positional encoding process through the three
    cosine similarity functions we applied to the three states representing the words
    `black` and `brown`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过我们应用的三个表示单词`black`和`brown`的余弦相似性函数，我们对位置编码过程有了清晰的认识：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We saw that the initial word similarity of their embeddings was high, with a
    value of `0.99`. Then we saw the positional encoding vector of positions 2 and
    10 drew these two words apart with a lower similarity value of `0.86`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到初始单词嵌入的相似性较高，值为`0.99`。然后我们看到位置编码向量的位置2和10使这两个单词的相似性值降低为`0.86`。
- en: Finally, we added the word embedding vector of each word to its respective positional
    encoding vector. We saw that this brought the cosine similarity of the two words
    to `0.96`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将每个单词的单词嵌入向量添加到其相应的位置编码向量中。我们发现，这使得两个单词的余弦相似度为`0.96`。
- en: The positional encoding of each word now contains the initial word embedding
    information and the positional encoding values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的位置编码现在包含初始单词嵌入信息和位置编码值。
- en: The output of positional encoding leads to the multi-head attention sublayer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码的输出导致了多头注意力子层。
- en: 'Sublayer 1: Multi-head attention'
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子层1：多头注意力
- en: 'The multi-head attention sublayer contains eight heads and is followed by post-layer
    normalization, which will add residual connections to the output of the sublayer
    and normalize it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力子层包含八个头，并带有后层规范化，将在子层输出中添加残差连接并对其进行规范化：
- en: '![](img/B17948_02_09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_09.png)'
- en: 'Figure 2.9: Multi-head attention sublayer'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：多头注意力子层
- en: This section begins with the architecture of an attention layer. Then, an example
    of multi-attention is implemented in a small module in Python. Finally, post-layer
    normalization is described.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本节开始讲解注意力层的架构。接着，以 Python 中的一个小模块实现了多头注意力的示例。最后，描述了后层规范化。
- en: Let’s start with the architecture of multi-head attention.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从多头注意力的架构开始。
- en: The architecture of multi-head attention
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头注意力的架构
- en: The input of the multi-attention sublayer of the first layer of the encoder
    stack is a vector that contains the embedding and the positional encoding of each
    word. The next layers of the stack do not start these operations over.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器堆叠的第一层的多头注意力子层的输入是包含每个单词的嵌入和位置编码的向量。堆叠的下一层不会重新开始这些操作。
- en: 'The dimension of the vector of each word *x*[n] of an input sequence is *d*[model]
    = 512:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列每个单词*x*[n]的向量维度是 *d*[model] = 512：
- en: '*pe*(*x*[n])=[*d*[1]=9.09297407e^-01, *d*[2]=-4.16146845e^-01, .., *d*[512]=1.00000000e+00]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*pe*(*x*[n])=[*d*[1]=9.09297407e^-01, *d*[2]=-4.16146845e^-01, .., *d*[512]=1.00000000e+00]'
- en: The representation of each word *x*[n] has become a vector of *d*[model] = 512
    dimensions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词*x*[n]的表示现在已经变成了*512*维的向量 *d*[model] = 512。
- en: Each word is mapped to all the other words to determine how it fits in a sequence.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词都映射到所有其他单词，以确定它在序列中的位置。
- en: 'In the following sentence, we can see that it could be related to `cat` and
    `rug` in the sequence:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的句子中，我们可以看到它可能与序列中的`cat`和`rug`相关：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The model will train to find out if `it` is related to `cat` or `rug`. We could
    run a huge calculation by training the model using the *d*[model] = 512 dimensions
    as they are now.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将训练以确定`it`是与`cat`还是`rug`相关联。我们可以通过使用当前的*512*维度训练该模型进行大量的计算。
- en: However, we would only get one point of view at a time by analyzing the sequence
    with one *d*[model] block. Furthermore, it would take quite some calculation time
    to find other perspectives.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，通过分析一个*d*[model]块的序列，我们只能得到一个观点。此外，使用现在的*512*维度将需要相当长的计算时间来找到其他观点。
- en: A better way is to divide the *d*[model] = 512 dimensions of each word *x*[n]
    of *x* (all the words of a sequence) into 8 *d*[k]=64 dimensions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法是将每个单词*x*[n]的*512*维度划分为*8*个*64*维度。
- en: 'We then can run the 8 “heads” in parallel to speed up the training and obtain
    8 different representation subspaces of how each word relates to another:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以并行运行8个“头”来加速训练，并获得每个单词如何与另一个相关的8个不同表示子空间：
- en: '![](img/B17948_02_10.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_10.png)'
- en: 'Figure 2.10: Multi-head representations'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：多头表示
- en: You can see that there are now `8` heads running in parallel. One head might
    decide that `it` fits well with `cat` and another that `it` fits well with `rug`
    and another that `rug` fits well with `dry-cleaned`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以看到有`8`个并行运行的头。其中一个头可能认为`it`和`cat`很合适，另一个认为`it`和`rug`很合适，另一个认为`rug`和`dry-cleaned`很合适。
- en: 'The output of each head is a matrix *Z*[i] with a shape of *x* * *d*[k]. The
    output of a multi-attention head is *Z* defined as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个头的输出是形状为*x* * *d*[k]的矩阵*Z*[i]。多头注意力输出*Z*定义为：
- en: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
- en: However, *Z* must be concatenated so that the output of the multi-head sublayer
    is not a sequence of dimensions but one line of an *xm* * *d*[model] matrix.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*Z*必须被连接，这样多头子层的输出不是尺寸的序列，而是*xm* * *d*[model]矩阵的一行。
- en: 'Before exiting the multi-head attention sublayer, the elements of *Z* are concatenated:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在退出多头注意力子层之前，*Z*的元素被连接：
- en: '*MultiHead*(*output*) = *Concat*(*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) = *x*, *d*[model]'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*MultiHead*(*output*) = *Concat*(*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) = *x*，*d*[model]'
- en: Notice that each head is concatenated into *z* that has a dimension of *d*[model]
    = 512\. The output of the multi-headed layer respects the constraint of the original
    Transformer model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个头都被连接成一个具有维度*d*[model] = 512的*z*。多头层的输出遵循原始Transformer模型的约束。
- en: 'Inside each head *h*[n] of the attention mechanism, the "word" matrices have
    three representations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意机制的每个头*h*[n]内，“单词”矩阵有三种表示：
- en: A query matrix (*Q*) that has a dimension of *d*[q] = 64, which seeks all the
    key-value pairs of the "word" matrices.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个查询矩阵(*Q*)的维度为*d*[q] = 64，它寻求所有“单词”矩阵的键-值对。
- en: A key matrix (*K*) that has a dimension of *d*[k] = 64, which will be trained
    to provide an attention value.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个键矩阵(*K*)的维度为*d*[k] = 64，它将被训练以提供一个注意力值。
- en: A value matrix (*V*) that has a dimension of *d*[v] = 64, which will be trained
    to provide another attention value.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个值矩阵(*V*)的维度为*d*[v] = 64，它将被训练以提供另一个注意力值。
- en: 'Attention is defined as “Scaled Dot-Product Attention,” which is represented
    in the following equation in which we plug *Q*, *K*, and *V*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力被定义为“缩放点积注意力”，它在下面的方程中表示，我们将*Q*、*K*和*V*代入其中：
- en: '![](img/B17948_02_003.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_003.png)'
- en: The matrices all have the same dimension, making it relatively simple to use
    a scaled dot product to obtain the attention values for each head and then concatenate
    the output *Z* of the 8 heads.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所有矩阵都具有相同的维度，这样可以相对简单地使用缩放点积来获得每个头的注意力值，然后连接8个头的输出*Z*。
- en: To obtain *Q*, *K*, and *V*, we must train the model with their weight matrices
    *Q*[w], *K*[w], and *V*[w], which have *d*[k] =64 columns and *d*[model] = 512
    rows. For example, *Q* is obtained by a dot-product between *x* and *Q*[w]. *Q*
    will have a dimension of *d*[k] =64.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得*Q*、*K*和*V*，我们必须使用它们的权重矩阵*Q*[w]、*K*[w]和*V*[w]训练模型，它们具有*d*[k] = 64列和*d*[model]
    = 512行。例如，*Q*是通过*x*和*Q*[w]的点积获得的。*Q*将具有*d*[k] = 64的维度。
- en: You can modify all the parameters, such as the number of layers, heads, *d*[model],
    *d*[k], and other variables of the Transformer to fit your model. This chapter
    describes the original Transformer parameters by *Vaswani* et al. (2017). It is
    essential to understand the original architecture before modifying it or exploring
    variants of the original model designed by others.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以修改所有参数，例如层数、头部、*d*[model]、*d*[k]和Transformer的其他变量，以适应您的模型。本章描述了由*Vaswani*等人（2017年）提出的原始Transformer参数。在修改或探索其他人设计的原始模型变体之前，了解原始架构是至关重要的。
- en: Google Brain Trax, OpenAI, and Hugging Face, among others, provide ready-to-use
    libraries that we will be using throughout this book.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain Trax、OpenAI和Hugging Face等提供了可供我们在本书中使用的即用型库。
- en: However, let’s open the hood of the Transformer model and get our hands dirty
    in Python to illustrate the architecture we just explored to visualize the model
    in code and show it with intermediate images.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们打开Transformer模型的机制，并在Python中动手实现来说明我们刚刚探索的架构，以便可视化该模型的代码并用中间图形表示出来。
- en: We will use basic Python code with only `numpy` and a `softmax` function in
    10 steps to run the key aspects of the attention mechanism.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that an Industry 4.0 developer will face the challenge of multiple
    architectures for the same algorithm.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now start building *Step 1* of our model to represent the input.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Represent the input'
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Save `Multi_Head_Attention_Sub_Layer.ipynb` to your Google Drive (make sure
    you have a Gmail account) and then open it in Google Colaboratory. The notebook
    is in the GitHub repository for this chapter.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by only using minimal Python functions to understand the Transformer
    at a low level with the inner workings of an attention head. We will explore the
    inner workings of the multi-head attention sublayer using basic code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The input of the attention mechanism we are building is scaled down to *d*[model]
    ==4 instead of *d*[model] = 512\. This brings the dimensions of the vector of
    an input *x* down to *d*[model] =4, which is easier to visualize.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*x* contains `3` inputs with `4` dimensions each instead of `512`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output shows that we have 3 vectors of *d*[model] =4:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The first step of our model is ready:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_11.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Input of a multi-head attention sublayer'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: We will now add the weight matrices to our model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Initializing the weight matrices'
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each input has 3 weight matrices:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '*Q*[w] to train the queries'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K*[w] to train the keys'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V*[w] to train the values'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These 3 weight matrices will be applied to all the inputs in this model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The weight matrices described by *Vaswani* et al. (2017) are *d*[K] ==64 dimensions.
    However, let’s scale the matrices down to *d*[K] ==3\. The dimensions are scaled
    down to `3*4` weight matrices to be able to visualize the intermediate results
    more easily and perform dot products with the input *x*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The size and shape of the matrices in this educational notebook are arbitrary.
    The goal is to go through the overall process of an attention mechanism.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The three weight matrices are initialized starting with the query weight matrix:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is the `w_query` weight matrix:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will now initialize the key weight matrix:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is the key weight matrix:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we initialize the value weight matrix:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is the value weight matrix:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The second step of our model is ready:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_12.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Weight matrices added to the model'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We will now multiply the weights by the input vectors to obtain *Q*, *K*, and
    *V*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Matrix multiplication to obtain Q, K, and V'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will now multiply the input vectors by the weight matrices to obtain a query,
    key, and value vector for each input.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: In this model, we will assume that there is one `w_query`, `w_key`, and `w_value`
    weight matrix for all inputs. Other approaches are possible.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first multiply the input vectors by the `w_query` weight matrix:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is a vector for *Q*[1] ==64= [1, 0, 2], *Q*[2]= [2,2, 2], and *Q*[3]=
    [2,1, 3]:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now multiply the input vectors by the `w_key` weight matrix:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将输入向量乘以`w_key`权重矩阵：
- en: '[PRE31]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We obtain a vector for *K*[1]= [0, 1, 1], *K*[2]= [4, 4, 0], and *K*[3]= [2
    ,3, 1]:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到*K*[1]= [0, 1, 1]，*K*[2]= [4, 4, 0]，以及*K*[3]= [2 ,3, 1]的向量：
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we multiply the input vectors by the `w_value` weight matrix:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输入向量乘以`w_value`权重矩阵：
- en: '[PRE33]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We obtain a vector for *V*[1]= [1, 2, 3], *V*[2]= [2, 8, 0], and *V*[3]= [2
    ,6, 3]:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到*V*[1]= [1, 2, 3]，*V*[2]= [2, 8, 0]，以及*V*[3]= [2 ,6, 3]的向量：
- en: '[PRE34]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The third step of our model is ready:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的第三步准备好了：
- en: '![](img/B17948_02_13.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_13.png)'
- en: 'Figure 2.13: Q, K, and V are generated'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：生成了*Q*、*K*和*V*
- en: We have the *Q*, *K*, and *V* values we need to calculate the attention scores.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了需要计算注意力分数的*Q*、*K*和*V*值。
- en: 'Step 4: Scaled attention scores'
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：缩放注意力分数
- en: 'The attention head now implements the original Transformer equation:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力头现在实现了原始的 Transformer 方程：
- en: '![](img/B17948_02_003.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_003.png)'
- en: '*Step 4* focuses on *Q* and *K*:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4* 关注*Q*和*K*：'
- en: '![](img/B17948_02_005.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_005.png)'
- en: 'For this model, we will round ![](img/B17948_02_006.png) = ![](img/B17948_02_007.png)
    = 1.75 to 1 and plug the values into the *Q* and *K* part of the equation:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我们将四舍五入 ![](img/B17948_02_006.png) = ![](img/B17948_02_007.png) = 1.75
    为 1，并将值代入方程的*Q*和*K*部分：
- en: '[PRE35]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The intermediate result is displayed:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 中间结果显示为：
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*Step 4* is now complete. For example, the score for *x*[1] is [2,4,4] across
    the *K* vectors across the head as displayed:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4* 现在已完成。例如，*x*[1]的分数为[2,4,4]跨越了*K*向量的头部显示为：'
- en: '![](img/B17948_02_14.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_14.png)'
- en: 'Figure 2.14: Scaled attention scores for input #1'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14：输入＃1的缩放注意力分数
- en: The attention equation will now apply softmax to the intermediate scores for
    each vector.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意力方程将为每个向量的中间分数应用 softmax。
- en: 'Step 5: Scaled softmax attention scores for each vector'
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 5：每个向量的缩放 softmax 注意力分数
- en: 'We now apply a softmax function to each intermediate attention score. Instead
    of doing a matrix multiplication, let’s zoom down to each individual vector:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对每个中间注意力分数应用 softmax 函数。与进行矩阵乘法不同，让我们放大到每个单独的向量：
- en: '[PRE37]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We obtain scaled softmax attention scores for each vector:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个向量得到了缩放的 softmax 注意力分数：
- en: '[PRE38]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*Step 5* is now complete. For example, the softmax of the score of *x*[1] for
    all the keys is:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 5* 现在已完成。例如，所有键的*x*[1]的分数的 softmax 是：'
- en: '![](img/B17948_02_15.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_15.png)'
- en: 'Figure 2.15: The softmax score of input #1 for all of the keys'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15：所有键的输入＃1的 softmax 分数
- en: We can now calculate the final attention values with the complete equation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用完整的方程计算最终的注意力值。
- en: 'Step 6: The final attention representations'
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 6：最终的注意力表示
- en: 'We now can finalize the attention equation by plugging *V* in:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将*V*代入来完成注意力方程：
- en: '![](img/B17948_02_003.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_003.png)'
- en: We will first calculate the attention score of input *x*[1] for *Steps 6* and
    *7*. We calculate one attention value for one word vector. When we reach *Step
    8*, we will generalize the attention calculation to the other two input vectors.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算输入*x*[1]对*步骤 6* 和 *7* 的注意力分数。我们为一个词向量计算一个注意力值。当我们到达*步骤 8* 时，我们将将注意力计算推广到另外两个输入向量。
- en: 'To obtain *Attention*(*Q,K,V*) for *x*[1] we multiply the intermediate attention
    score by the 3 value vectors one by one to zoom down into the inner workings of
    the equation:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得*x*[1]的 *Attention*(*Q,K,V*)，我们将中间注意力分数逐个与 3 个值向量相乘，以放大方程的内部工作：
- en: '[PRE39]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*Step 6* is complete. For example, the 3 attention values for *x*[1] for each
    input have been calculated:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 6* 完成。例如，已计算了每个输入的*x*[1]的 3 个注意力值：'
- en: '![](img/B17948_02_16.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_16.png)'
- en: 'Figure 2.16: Attention representations'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16：注意力表示
- en: The attention values now need to be summed up.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要将注意力值相加。
- en: 'Step 7: Summing up the results'
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 7：将结果求和
- en: 'The 3 attention values of input #1 obtained will now be summed to obtain the
    first line of the output matrix:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 输入＃1的 3 个注意力值现在将被求和以获得输出矩阵的第一行：
- en: '[PRE40]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is the first line of the output matrix for input #1:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是输入＃1的输出矩阵的第一行：
- en: '[PRE41]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The second line will be for the output of the next input, input #2, for example.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行将是下一个输入，例如输入＃2的输出。
- en: 'We can see the summed attention value for *x*[1] in *Figure 2.17*:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图2.17*中看到*x*[1]的求和注意力值：
- en: '![](img/B17948_02_17.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_17.png)'
- en: 'Figure 2.17: Summed results for one input'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17：一个输入的求和结果
- en: 'We have completed the steps for input #1\. We now need to add the results of
    all the inputs to the model.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已完成了输入＃1的步骤。现在我们需要将所有输入的结果添加到模型中。
- en: 'Step 8: Steps 1 to 7 for all the inputs'
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤8：所有输入的步骤1至步骤7
- en: 'The Transformer can now produce the attention values of input #2 and input
    #3 using the same method described from *Step 1* to *Step 7* for one attention
    head.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，变压器可以使用与步骤1到步骤7描述的相同方法产生输入＃2和输入＃3的注意力值，用于一个注意力头。
- en: From this step onwards, we will assume we have 3 attention values with learned
    weights with *d*[model] = 64\. We now want to see what the original dimensions
    look like when they reach the sublayer’s output.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一步开始，我们假设有3个学习权重的注意力值，*d*[model] = 64。现在我们想看到这些原始维度在达到子层输出时是什么样子的。
- en: 'We have seen the attention representation process in detail with a small model.
    Let’s go directly to the result and assume we have generated the 3 attention representations
    with a dimension of *d*[model] = 64:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详细了解了小模型中的注意力表示过程。让我们直接看结果，假设我们已经生成了3个*d*[model]=64维的注意力表示：
- en: '[PRE42]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following output displays the simulation of *z*[0], which represents the
    3 output vectors of *d*[model] = 64 dimensions for head 1:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了*z*[0]的模拟，它代表了头1的*d*[model]=64维的3个输出向量：
- en: '[PRE43]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The results will vary when you run the notebook because of the stochastic nature
    of the generation of the vectors.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 运行笔记本时，由于向量生成的随机性质，结果将有所不同。
- en: The Transformer now has the output vectors for the inputs of one head. The next
    step is to generate the output of the 8 heads to create the final output of the
    attention sublayer.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，变压器有一个头的输入的输出向量。下一步是生成8个头的输出，以创建注意力子层的最终输出。
- en: 'Step 9: The output of the heads of the attention sublayer'
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤9：注意力子层的头部输出
- en: 'We assume that we have trained the 8 heads of the attention sublayer. The Transformer
    now has 3 output vectors (of the 3 input vectors that are words or word pieces)
    of *d*[model] = 64 dimensions each:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们已经训练了注意力子层的8个头。现在，变压器有3个输出向量（3个输入向量，即单词或词片段）每个*d*[model] = 64维：
- en: '[PRE44]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output shows the shape of one of the heads:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了一个头的形状：
- en: '[PRE45]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The 8 heads have now produced *Z*:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这8个头现在产生了*Z*：
- en: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
- en: The Transformer will now concatenate the 8 elements of *Z* for the final output
    of the multi-head attention sublayer.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器将现在连接*Z*的8个元素以产生多头注意力子层的最终输出。
- en: 'Step 10: Concatenation of the output of the heads'
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤10：连接头部的输出
- en: 'The Transformer concatenates the 8 elements of *Z*:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器连接了*Z*的8个元素：
- en: '*MultiHead*(*Output*) = *Concat* (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) *W*⁰ = *x*, *d*[model]'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*MultiHead*(*Output*) = *Concat* (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) *W*⁰ = *x*，*d*[model]'
- en: Note that *Z* is multiplied by *W*⁰, which is a weight matrix that is trained
    as well. In this model, we will assume *W*⁰ is trained and integrated into the
    concatenation function.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*Z*被乘以*W*⁰，它是一个被训练的权重矩阵。在这个模型中，我们将假设*W*⁰是被训练并集成到连接函数中的。
- en: '*Z*[0] to *Z*[7] are concantenated:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z*[0]到*Z*[7]被连接在一起：'
- en: '[PRE46]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is the concatenation of *Z*:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是*Z*的连接：
- en: '[PRE47]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The concatenation can be visualized as stacking the elements of *Z* side by
    side:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 连接可以可视化为将*Z*的元素并排堆叠：
- en: '![](img/B17948_02_18.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_18.png)'
- en: 'Figure 2.18: Attention sublayer output'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18：注意力子层输出
- en: 'The concatenation produced a standard *d*[model] = 512 dimensional output:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 连接产生了一个标准的*d*[model]=512维的输出：
- en: '![](img/B17948_02_19.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_19.png)'
- en: 'Figure 2.19: Concatenation of the output of the 8 heads'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19：连接8个头的输出
- en: Layer normalization will now process the attention sublayer.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，层归一化将处理注意力子层。
- en: Post-layer normalization
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后层归一化
- en: 'Each attention sublayer and each Feedforward sublayer of the Transformer is
    followed by **post-layer normalization** (**Post-LN**):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变压器的注意力子层和每个前馈子层之后都跟随**后层归一化**（**Post-LN**）：
- en: '![](img/B17948_02_20.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_20.png)'
- en: 'Figure 2.20: Post-layer normalization'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20：后层归一化
- en: 'The Post-LN contains an add function and a layer normalization process. The
    add function processes the residual connections that come from the input of the
    sublayer. The goal of the residual connections is to make sure critical information
    is not lost. The Post-LN or layer normalization can thus be described as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 后层归一化包含加法函数和层归一化过程。加法函数处理来自子层输入的残余连接。残余连接的目标是确保关键信息不会丢失。后层归一化或层归一化可以描述如下：
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
- en: '*Sublayer*(*x*) is the sublayer itself. *x* is the information available at
    the input step of *Sublayer*(*x*).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sublayer*(*x*) 是子层本身。*x* 是 *Sublayer*(*x*) 输入步骤的可用信息。'
- en: The input of the *LayerNormalization* is a vector *v* resulting from *x* + *Sublayer*(*x*).
    *d*[model] = 512 for every input and output of the Transformer, which standardizes
    all the processes.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*LayerNormalization* 的输入是由 *x* + *Sublayer*(*x*) 产生的向量 *v*。Transformer 的每个输入和输出都为
    *d*[model] = 512，这标准化了所有过程。'
- en: 'Many layer normalization methods exist, and variations exist from one model
    to another. The basic concept for *v* = *x* + *Sublayer*(*x*) can be defined by
    *LayerNormalization* (*v*):'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多层归一化方法，且从一个模型到另一个模型存在变化。*v* = *x* + *Sublayer*(*x*) 的基本概念可以由 *LayerNormalization*
    (*v*) 定义：
- en: '![](img/B17948_02_009.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_009.png)'
- en: 'The variables are:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是：
- en: '![](img/B17948_02_010.png) is the mean of *v* of dimension *d*. As such:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_02_010.png) 是维度 *d* 的 *v* 的均值。因此：'
- en: '![](img/B17948_02_011.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_011.png)'
- en: '![](img/B17948_02_012.png) is the standard deviation *v* of dimension *d*.
    As such:'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_02_012.png) 是维度 *d* 的标准差 *v*。因此：'
- en: '![](img/B17948_02_013.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_013.png)'
- en: '![](img/B17948_02_014.png) is a scaling parameter.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_02_014.png) 是一个缩放参数。'
- en: '![](img/B17948_02_015.png) is a bias vector.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_02_015.png) 是一个偏置向量。'
- en: This version of *LayerNormalization* (*v*) shows the general idea of the many
    possible post-LN methods. The next sublayer can now process the output of the
    post-LN or *LayerNormalization* (*v*). In this case, the sublayer is a feedforward
    network.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的 *LayerNormalization* (*v*) 展示了许多可能的后归一化方法的基本思想。下一个子层现在可以处理后归一化或 *LayerNormalization*
    (*v*) 的输出。在这种情况下，子层是一个前馈网络。
- en: 'Sublayer 2: Feedforward network'
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子层 2：前馈网络
- en: 'The input of the **feedforward network** (**FFN**) is the *d*[model] = 512
    output of the post-LN of the previous sublayer:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈网络** (**FFN**) 的输入是前一个子层的后归一化输出 *d*[model] = 512：'
- en: '![](img/B17948_02_21.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_21.png)'
- en: 'Figure 2.21: Feedforward sublayer'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：前馈子层
- en: 'The FFN sublayer can be described as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: FFN 子层可以描述如下：
- en: The FFNs in the encoder and decoder are fully connected.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器中的 FFN 是全连接的。
- en: The FFN is a position-wise network. Each position is processed separately and
    in an identical way.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN 是一种逐位置的网络。每个位置都是分别且相同方式处理的。
- en: The FFN contains two layers and applies a ReLU activation function.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN 包含两个层，并应用 ReLU 激活函数。
- en: The input and output of the FFN layers is *d*[model] = 512, but the inner layer
    is larger with *d*[ff] =2048.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN 层的输入和输出为 *d*[model] = 512，但内部层更大，为 *d*[ff] =2048。
- en: The FFN can be viewed as performing two convolutions with size 1 kernels.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFN 可以被视为使用大小为 1 的卷积执行两次。
- en: 'Taking this description into account, we can describe the optimized and standardized
    FFN as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个描述，我们可以描述优化和标准化的 FFN 如下：
- en: '*FFN*(*x*) = max (0, *xW*[1] + *b*[1]) *W*[2]+*b*[2]'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '*FFN*(*x*) = max (0, *xW*[1] + *b*[1]) *W*[2]+*b*[2]'
- en: The output of the FFN goes to post-LN, as described in the previous section.
    Then the output is sent to the next layer of the encoder stack and the multi-head
    attention layer of the decoder stack.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: FFN 的输出经过了后归一化，如前一节所述。然后输出被发送到编码器堆栈的下一层和解码器堆栈的多头注意力层。
- en: Let’s now explore the decoder stack.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探索解码器堆栈。
- en: The decoder stack
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器堆栈
- en: 'The layers of the decoder of the Transformer model are *stacks of layers* like
    the encoder layers. Each layer of the decoder stack has the following structure:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的解码器层是与编码器层类似的层堆栈。每个解码器堆栈层具有以下结构：
- en: '![](img/B17948_02_22.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_02_22.png)'
- en: 'Figure 2.22: A layer of the decoder stack of the Transformer'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：Transformer 解码器堆栈的一层
- en: 'The structure of the decoder layer remains the same as the encoder for all
    the *N*=6 layers of the Transformer model. Each layer contains three sublayers:
    a multi-headed masked attention mechanism, a multi-headed attention mechanism,
    and a fully connected position-wise feedforward network.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Transformer 模型的所有 *N*=6 层，解码器层的结构与编码器相同。每一层包含三个子层：多头掩码注意力机制，多头注意力机制和全连接的逐位置前馈网络。
- en: The decoder has a third main sublayer, which is the masked multi-head attention
    mechanism. In this sublayer output, at a given position, the following words are
    masked so that the Transformer bases its assumptions on its inferences without
    seeing the rest of the sequence. That way, in this model, it cannot see future
    parts of the sequence.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器有一个第三个主要子层，即遮罩式多头注意力机制。在此子层的输出中，在给定位置，随后的单词被掩盖，以便 Transformer 在不看到序列的其余部分的情况下基于其推断。因此，在这个模型中，它无法看到序列的未来部分。
- en: 'A residual connection, *Sublayer*(*x*), surrounds each of the three main sublayers
    in the Transformer model like in the encoder stack:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Transformer 模型中的三个主要子层都被一个残差连接，*Sublayer*(*x*)，像编码器堆栈中一样：
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '*层归一化* (*x* + *Sublayer*(*x*))'
- en: The embedding layer sublayer is only present at the bottom level of the stack,
    like for the encoder stack. The output of every sublayer of the decoder stack
    has a constant dimension, *d*[model] like in the encoder stack, including the
    embedding layer and the output of the residual connections.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层子层只存在于堆栈的底层，就像编码器堆栈一样。解码器堆栈的每个子层的输出都具有恒定的维度，*d*[model]，就像编码器堆栈一样，包括嵌入层和残差连接的输出。
- en: We can see that the designers worked hard to create symmetrical encoder and
    decoder stacks.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，设计者们努力创建了对称的编码器和解码器堆栈。
- en: The structure of each sublayer and function of the decoder is similar to the
    encoder. In this section, we can refer to the encoder for the same functionality
    when we need to. We will only focus on the differences between the decoder and
    the encoder.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子层的结构和解码器功能与编码器类似。在本节中，当我们需要时，我们可以参考编码器来获得相同的功能。我们只关注解码器和编码器之间的差异。
- en: Output embedding and position encoding
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出嵌入和位置编码
- en: The structure of the sublayers of the decoder is mostly the same as the sublayers
    of the encoder. The output embedding layer and position encoding function are
    the same as in the encoder stack.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器子层的结构与编码器子层的大部分相同。输出嵌入层和位置编码函数与编码器堆栈中的相同。
- en: 'In the Transformer usage we are exploring through the model presented by *Vaswani*
    et al. (2017), the output is a translation we need to learn. I chose to use a
    French translation:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过 *Vaswani* 等人 (2017) 提出的模型中探索 Transformer 的用法时，输出是我们需要学习的翻译。我选择使用法语翻译：
- en: '[PRE48]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This output is the French translation of the English input sentence:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出是英文输入句子的法语翻译：
- en: '[PRE49]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The output words go through the word embedding layer and then the positional
    encoding function like in the first layer of the encoder stack.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的词经过词嵌入层，然后是位置编码函数，就像编码器堆栈的第一层一样。
- en: Let’s see the specific properties of the multi-head attention layers of the
    decoder stack.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下解码器堆栈的多头注意力层的具体属性。
- en: The attention layers
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力层
- en: The Transformer is an auto-regressive model. It uses the previous output sequences
    as an additional input. The multi-head attention layers of the decoder use the
    same process as the encoder.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是一个自回归模型。它使用先前的输出序列作为额外的输入。解码器的多头注意力层使用与编码器相同的过程。
- en: However, the masked multi-head attention sublayer 1 only lets attention apply
    to the positions up to and including the current position. The future words are
    hidden from the Transformer, and this forces it to learn how to predict.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，遮罩式多头注意力子层 1 只允许注意力应用到当前位置及其之前的位置。未来的词对 Transformer 是隐藏的，这迫使它学会预测。
- en: A post-layer normalization process follows the masked multi-head attention sublayer
    1 as in the encoder.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在遮罩式多头注意力子层 1 之后，像在编码器中一样，遵循一个后层归一化过程。
- en: The multi-head attention sublayer 2 also only attends to the positions up to
    the current position the Transformer is predicting to avoid seeing the sequence
    it must predict.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力子层 2 也只关注到 Transformer 正在预测的当前位置之前的位置，以避免看到它必须预测的序列。
- en: 'The multi-head attention sublayer 2 draws information from the encoder by taking
    encoder (*K*, *V*) into account during the dot-product attention operations. This
    sublayer also draws information from the masked multi-head attention sublayer
    1 (masked attention) by also taking sublayer 1(*Q*) into account during the dot-product
    attention operations. The decoder thus uses the trained information of the encoder.
    We can define the input of the self-attention multi-head sublayer of a decoder
    as:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力子层 2 从编码器中汲取信息，在点积注意力操作期间考虑编码器 (*K*, *V*)。该子层还从掩码的多头注意力子层 1（掩码关注）中汲取信息，在点积注意力操作期间还考虑子层
    1(*Q*)。因此，解码器使用编码器的训练信息。我们可以将解码器的自注意力多头子层的输入定义为：
- en: '*Input_Attention* = (*Output_decoder_sub_layer* – 1(*Q*), *Output_encoder_layer*(*K*,
    *V*))'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入_注意力* = (*输出_解码器_子层* – 1(*Q*), *输出_编码器_层*(*K*, *V*))'
- en: A post-layer normalization process follows the masked multi-head attention sublayer
    1 as in the encoder.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器后，会跟随一个后层标准化过程。
- en: The Transformer then goes to the FFN sublayer, followed by a post-LN and the
    linear layer.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 然后Transformer会进入FFN子层，然后是一个后层标准化和线性层。
- en: The FFN sublayer, the post-LN, and the linear layer
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FFN子层，后层标准化和线性层
- en: The FFN sublayer has the same structure as the FFN of the encoder stack. The
    post-layer normalization of the FFN works as the layer normalization of the encoder
    stack.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: FFN子层的结构与编码器堆栈的FFN相同。FFN的后层标准化像编码器堆栈的层标准化一样工作。
- en: 'The Transformer produces an output sequence of only one element at a time:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer仅每次产生一个元素的输出序列：
- en: '*Output sequence* = (*y*[1], *y*[2], … *y*[n])'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出序列* = (*y*[1], *y*[2], … *y*[n])'
- en: 'The linear layer produces an output sequence with a linear function that varies
    per model but relies on the standard method:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层产生一个输出序列，其线性函数因模型而异，但依赖于标准方法：
- en: '*y* =*w* * *x* + *b*'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* =*w* * *x* + *b*'
- en: '*w* and *b* are learned parameters.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*和*b*是学习参数。'
- en: The linear layer will thus produce the next probable elements of a sequence
    that a softmax function will convert into a probable element.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层因此会产生序列的下一个可能元素，softmax函数将其转换为概率元素。
- en: The decoder layer, like the encoder layer, will then go from layer *l* to layer
    *l+1*, up to the top layer of the *N*=6-layer transformer stack.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层，如编码器层，将从层*l*到*l+1*，一直到*6*层Transformer堆叠的顶层。
- en: Let’s now see how the Transformer was trained and the performance it obtained.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下Transformer是如何训练的以及其性能如何。
- en: Training and performance
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和性能
- en: The original Transformer was trained on a 4.5 million sentence pair English-German
    dataset and a 36 million sentence pair English-French dataset.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的Transformer是在一个450万句对的英德数据集和一个3600万句对的英法数据集上进行训练的。
- en: 'The datasets come from **Workshops on Machine Translation** (**WMT**), which
    can be found at the following link if you wish to explore the WMT datasets: [http://www.statmt.org/wmt14/](http://www.statmt.org/wmt14/)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集来自**机器翻译研讨会** (**WMT**)，如果你希望探索WMT数据集，可以在以下链接找到：[http://www.statmt.org/wmt14/](http://www.statmt.org/wmt14/)
- en: The training of the original Transformer base models took 12 hours to train
    for 100,000 steps on a machine with 8 NVIDIA P100 GPUs. The big models took 3.5
    days for 300,000 steps.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的Transformer基础模型在装有8个NVIDIA P100 GPU的计算机上进行了100,000步的训练，用时12个小时。大型模型则用了3.5天进行了300,000步的训练。
- en: The original Transformer outperformed all the previous machine translation models
    with a BLEU score of 41.8\. The result was obtained on the WMT English-to-French
    dataset.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的Transformer在WMT英法数据集上取得了41.8的BLEU分数，优于所有以前的机器翻译模型。
- en: BLEU stands for Bilingual Evaluation Understudy. It is an algorithm that evaluates
    the quality of the results of machine translations.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU代表双语评估助手。它是一个评估机器翻译结果质量的算法。
- en: The Google Research and Google Brain team applied optimization strategies to
    improve the performance of the Transformer. For example, the Adam optimizer was
    used, but the learning rate varied by first going through warmup states with a
    linear rate and decreasing the rate afterward.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: Google研究和Google Brain团队应用了优化策略来提高Transformer的性能。例如，使用了Adam优化器，但学习率通过首先经历线性增长的预热阶段，然后在之后降低。
- en: Different types of regularization techniques, such as residual dropout and dropouts,
    were applied to the sums of embeddings. Also, the Transformer applies label smoothing
    that avoids overfitting with overconfident one-hot outputs. It introduces less
    accurate evaluations and forces the model to train more and better.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Several other Transformer model variations have led to other models and usages
    that we will explore in the subsequent chapters.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Before end the chapter, let’s get a feel of the simplicity of ready-to-use transformer
    models in Hugging Face, for example.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Tranformer models in Hugging Face
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything you saw in this chapter can be condensed in to a ready-to-use Hugging
    Face transformer model.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: With Hugging Face, you can implement machine translation in three lines of code!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Open `Multi_Head_Attention_Sub_Layer.ipynb` in Google Colaboratory. Save the
    notebook in your Google Drive (make sure you have a Gmail account). Go to the
    two last cells.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'We first ensure that Hugging Face transformers are installed:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The first cell imports the Hugging Face pipeline that contains several transformer
    usages:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We then implement the Hugging Face pipeline, which contains ready-to-use functions.
    In our case, to illustrate the Transformer model of this chapter, we activate
    the translator model and enter a sentence to translate from English to French:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And *voilà*! The translation is displayed:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Hugging Face shows how transformer architectures can be used in ready-to-use
    models.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first got started by examining the mind-blowing long-distance
    dependencies transformer architectures can uncover. Transformers can perform transductions
    from written and oral sequences to meaningful representations as never before
    in the history of **Natural Language Understanding** (**NLU**).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: These two dimensions, the expansion of transduction and the simplification of
    implementation, are taking artificial intelligence to a level never seen before.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: We explored the bold approach of removing RNNs, LSTMs, and CNNs from transduction
    problems and sequence modeling to build the Transformer architecture. The symmetrical
    design of the standardized dimensions of the encoder and decoder makes the flow
    from one sublayer to another nearly seamless.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: We saw that beyond removing recurrent network models, transformers introduce
    parallelized layers that reduce training time. We discovered other innovations,
    such as positional encoding and masked multi-headed attention.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: The flexible, original Transformer architecture provides the basis for many
    other innovative variations that open the way for yet more powerful transduction
    problems and language modeling.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: We will zoom more in-depth into some aspects of the Transformer’s architecture
    in the following chapters when describing the many variants of the original model.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: The arrival of the Transformer marks the beginning of a new generation of ready-to-use
    artificial intelligence models. For example, Hugging Face and Google Brain make
    artificial intelligence easy to implement with a few lines of code.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Fine-Tuning BERT Models*, we will explore the powerful
    evolutions of the original Transformer model.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP transduction can encode and decode text representations. (True/False)
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Natural Language Understanding** (**NLU**) is a subset of **Natural Language
    Processing** (**NLP**). (True/False)'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Language modeling algorithms generate probable sequences of words based on input
    sequences. (True/False)
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer is a customized LSTM with a CNN layer. (True/False)
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer does not contain LSTM or CNN layers. (True/False)
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention examines all the tokens in a sequence, not just the last one. (True/False)
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer uses a positional vector, not positional encoding. (True/False)
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer contains a feedforward network. (True/False)
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The masked multi-headed attention component of the decoder of a transformer
    prevents the algorithm parsing a given position from seeing the rest of a sequence
    that is being processed. (True/False)
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can analyze long-distance dependencies better than LSTMs. (True/False)
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, *2017*, *Attention
    Is All You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformer Usage: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tensor2Tensor (T2T) Introduction: [https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?hl=en](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?hl=en)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manuel Romero Notebook with link to explanations by *Raimi Karim*: [https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google language research: [https://research.google/teams/language/](https://research.google/teams/language/)'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face research: [https://huggingface.co/transformers/index.html](https://huggingface.co/transformers/index.html)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Annotated Transformer*: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jay Alammar*, *The Illustrated Transformer*: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-441
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
