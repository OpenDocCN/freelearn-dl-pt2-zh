- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started with the Architecture of the Transformer Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Language is the essence of human communication. Civilizations would never have
    been born without the word sequences that form language. We now mostly live in
    a world of digital representations of language. Our daily lives rely on NLP digitalized
    language functions: web search engines, emails, social networks, posts, tweets,
    smartphone texting, translations, web pages, speech-to-text on streaming sites
    for transcripts, text-to-speech on hotline services, and many more everyday functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 1*, *What are Transformers?*, explained the limits of RNNs and the
    birth of cloud AI transformers taking over a fair share of design and development.
    The role of the Industry 4.0 developer is to understand the architecture of the
    original Transformer and the multiple transformer ecosystems that followed.'
  prefs: []
  type: TYPE_NORMAL
- en: In December 2017, Google Brain and Google Research published the seminal *Vaswani*
    et al., *Attention is All You Need* paper. The Transformer was born. The Transformer
    outperformed the existing state-of-the-art NLP models. The Transformer trained
    faster than previous architectures and obtained higher evaluation results. As
    a result, transformers have become a key component of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the attention head of the Transformer is to do away with recurrent
    neural network features. In this chapter, we will open the hood of the Transformer
    model described by *Vaswani* et al. (2017) and examine the main components of
    its architecture. We will explore the fascinating world of attention and illustrate
    the key components of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer’s self-attention model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoding and decoding stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input and output embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-head attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masked multi-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive directly into the structure of the original Transformer’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rise of the Transformer: Attention is All You Need'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In December 2017, *Vaswani* et al. (2017) published their seminal paper, *Attention
    is All You Need.* They performed their work at Google Research and Google Brain.
    I will refer to the model described in *Attention is All You Need* as the “original
    Transformer model” throughout this chapter and book.
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix I*, *Terminology of Transformer Models*, can help the transition
    from the classical usage of deep learning words to transformer vocabulary. *Appendix
    I* summarizes some of the changes to the classical AI definition of neural network
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at the structure of the Transformer model they
    built. In the following sections, we will explore what is inside each component
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Transformer model is a stack of 6 layers. The output of layer
    *l* is the input of layer *l*+1 until the final prediction is reached. There is
    a 6-layer encoder stack on the left and a 6-layer decoder stack on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The architecture of the Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: On the left, the inputs enter the encoder side of the Transformer through an
    attention sublayer and a feedforward sublayer. On the right, the target outputs
    go into the decoder side of the Transformer through two attention sublayers and
    a feedforward network sublayer. We immediately notice that there is no RNN, LSTM,
    or CNN. Recurrence has been abandoned in this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention has replaced recurrence functions requiring increasing parameters
    as the distance between two words increases. The attention mechanism is a “word
    to word” operation. It is actually a token-to-token operation, but we will keep
    it to the word level to keep the explanation simple. The attention mechanism will
    find how each word is related to all other words in a sequence, including the
    word being analyzed itself. Let’s examine the following sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Attention will run dot products between word vectors and determine the strongest
    relationships of a word with all the other words, including itself (“cat” and
    “cat”):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Attending to all the words'
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism will provide a deeper relationship between words and
    produce better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each attention sublayer, the original Transformer model runs not one but
    eight attention mechanisms in parallel to speed up the calculations. We will explore
    this architecture in the following section, *The encoder stack*. This process
    is named “multi-head attention,” providing:'
  prefs: []
  type: TYPE_NORMAL
- en: A broader in-depth analysis of sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preclusion of recurrence reducing calculation operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of parallelization, which reduces training time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each attention mechanism learns different perspectives of the same input sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention replaced recurrence*. However, there are several other creative
    aspects of the Transformer, which are as critical as the attention mechanism,
    as you will see when we look inside the architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: We just looked at the Transformer structure from the outside. Let’s now go into
    each component of the Transformer. We will start with the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The layers of the encoder and decoder of the original Transformer model are
    *stacks of layers.* Each layer of the encoder stack has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: A layer of the encoder stack of the Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original encoder layer structure remains the same for all *N*=6 layers
    of the Transformer model. Each layer contains two main sublayers: a multi-headed
    attention mechanism and a fully connected position-wise feedforward network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that a residual connection surrounds each main sublayer, *sublayer*(*x*),
    in the Transformer model. These connections transport the unprocessed input *x*
    of a sublayer to a layer normalization function. This way, we are certain that
    key information such as positional encoding is not lost on the way. The normalized
    output of each layer is thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  prefs: []
  type: TYPE_NORMAL
- en: Though the structure of each of the *N*=6 layers of the encoder is identical,
    the content of each layer is not strictly identical to the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the embedding sublayer is only present at the bottom level of the
    stack. The other five layers do not contain an embedding layer, and this guarantees
    that the encoded input is stable through all the layers.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the multi-head attention mechanisms perform the same functions from layer
    1 to 6\. However, they do not perform the same tasks. Each layer learns from the
    previous layer and explores different ways of associating the tokens in the sequence.
    It looks for various associations of words, just like we look for different associations
    of letters and words when we solve a crossword puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: The designers of the Transformer introduced a very efficient constraint. The
    output of every sublayer of the model has a constant dimension, including the
    embedding layer and the residual connections. This dimension is *d*[model] and
    can be set to another value depending on your goals. In the original Transformer
    architecture, *d*[model] = 512.
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[model] has a powerful consequence. Practically all the key operations are
    dot products. As a result, the dimensions remain stable, which reduces the number
    of operations to calculate, reduces machine consumption, and makes it easier to
    trace the information as it flows through the model.'
  prefs: []
  type: TYPE_NORMAL
- en: This global view of the encoder shows the highly optimized architecture of the
    Transformer. In the following sections, we will zoom into each of the sublayers
    and mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with the embedding sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: Input embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input embedding sublayer converts the input tokens to vectors of dimension
    *d*[model] = 512 using learned embeddings in the original Transformer model. The
    structure of the input embedding is classical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The input embedding sublayer of the Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding sublayer works like other standard transduction models. A tokenizer
    will transform a sentence into tokens. Each tokenizer has its methods, such as
    BPE, word piece, and sentence piece methods. The Transformer initially used BPE,
    but other models use other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goals are similar, and the choice depends on the strategy chosen. For example,
    a tokenizer applied to the sequence `the Transformer is an innovative NLP model!`
    will produce the following tokens in one type of model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that this tokenizer normalized the string to lowercase and
    truncated it into subparts. A tokenizer will generally provide an integer representation
    that will be used for the embedding process. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There is not enough information in the tokenized text at this point to go further.
    The tokenized text must be embedded.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer contains a learned embedding sublayer. Many embedding methods
    can be applied to the tokenized input.
  prefs: []
  type: TYPE_NORMAL
- en: I chose the skip-gram architecture of the `word2vec` embedding approach Google
    made available in 2013 to illustrate the embedding sublayer of the Transformer.
    A skip-gram will focus on a center word in a window of words and predicts *context*
    words. For example, if word(i) is the center word in a two-step window, a skip-gram
    model will analyze word(i-2), word(i-1), word(i+1), and word(i+2). Then the window
    will *slide* and repeat the process. A skip-gram model generally contains an input
    layer, weights, a hidden layer, and an output containing the word embeddings of
    the tokenized input words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we need to perform embedding for the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will focus on two words, `black` and `brown`. The word embedding vectors
    of these two words should be similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we must produce a vector of size *d*[model] = 512 for each word, we will
    obtain a size `512` vector embedding for each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The word `black` is now represented by `512` dimensions. Other embedding methods
    could be used and *d*[model] could have a higher number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word embedding of `brown` is also represented by `512` dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To verify the word embedding produced for these two words, we can use cosine
    similarity to see if the word embeddings of the words `black` and `brown` are
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cosine similarity uses Euclidean (L2) norm to create vectors in a unit sphere.
    The dot product of the vectors we are comparing is the cosine between the points
    of those two vectors. For more on the theory of cosine similarity, you can consult
    scikit-learn’s documentation, among many other sources: [https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cosine similarity between the black vector of size *d*[model] = 512 and
    the brown vector of size *d*[model] = 512 in the embedding of the example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The skip-gram produced two vectors that are close to each other. It detected
    that black and brown form a color subset of the dictionary of words.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer’s subsequent layers do not start empty-handed. They have learned
    word embeddings that already provide information on how the words can be associated.
  prefs: []
  type: TYPE_NORMAL
- en: However, a big chunk of information is missing because no additional vector
    or information indicates a word’s position in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The designers of the Transformer came up with yet another innovative feature:
    positional encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how positional encoding works.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We enter this positional encoding function of the Transformer with no idea
    of the position of a word in a sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Positional encoding'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot create independent positional vectors that would have a high cost
    on the training speed of the Transformer and make attention sublayers overly complex
    to work with. The idea is to add a positional encoding value to the input embedding
    instead of having additional vectors to describe the position of a token in a
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Industry 4.0 is pragmatic and model-agnostic. The original Transformer model
    has only one vector that contains word embedding and position encoding. We will
    explore disentangled attention with a separate matrix for positional encoding
    in *Chapter 15, From NLP to Task-Agnostic Transformer Models*.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer expects a fixed size *d*[model] = 512 (or other constant value
    for the model) for each vector of the output of the positional encoding function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go back to the sentence we used in the word embedding sublayer, we can
    see that black and brown may be semantically similar, but they are far apart in
    the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The word `black` is in position 2, `pos=2`, and the word `brown` is in position
    10, `pos=10`.
  prefs: []
  type: TYPE_NORMAL
- en: Our problem is to find a way to add a value to the word embedding of each word
    so that it has that information. However, we need to add a value to the *d*[model]
    = 512 dimensions! For each word embedding vector, we need to find a way to provide
    information to `i` in the `range(0,512)` dimensions of the word embedding vector
    of `black` and `brown`.
  prefs: []
  type: TYPE_NORMAL
- en: '*There are many ways to achieve positional encoding*. This section will focus
    on the designers’ clever way to use a unit sphere to represent positional encoding
    with sine and cosine values that will thus remain small but useful.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) provide sine and cosine functions so that we can generate
    different frequencies for the positional encoding (**PE**) for each position and
    each dimension *i* of the *d*[model] = 512 of the word embedding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_016.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17948_02_017.png)'
  prefs: []
  type: TYPE_IMG
- en: If we start at the beginning of the word embedding vector, we will begin with
    a constant (`512`), `i=0,` and end with `i=511`. This means that the sine function
    will be applied to the even numbers and the cosine function to the odd numbers.
    Some implementations do it differently. In that case, the domain of the sine function
    can be ![](img/B17948_02_001.png) and the domain of the cosine function can be
    ![](img/B17948_02_002.png). This will produce similar results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the functions the way they were described by *Vaswani*
    et al. (2017). A literal translation into Python pseudo code produces the following
    code for a positional vector `pe[0][i]` for a position `pos`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Google Brain Trax and Hugging Face, among others, provide ready-to-use libraries
    for the word embedding section and the present positional encoding section. Thus,
    you don’t need to run the code I share in this section. However, if you wish to
    explore the code, you will find it in the Google Colaboratory `positional_encoding.ipynb`
    notebook and the `text.txt` file in this chapter’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Before going further, you might want to see the plot of the sine function, for
    example, for `pos=2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can Google the following plot, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Just enter the plot request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Plotting with Google'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will obtain the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: The graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go back to the sentence we are parsing in this section, we can see that
    `black` is in position `pos=2` and `brown` is in position `pos=10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we apply the sine and cosine functions literally for `pos=2`, we obtain
    a size=`512` positional encoding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We also obtain a size=`512` positional encoding vector for position 10, *pos=10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we look at the results we obtained with an intuitive literal translation
    of the *Vaswani* et al. (2017) functions into Python, we would like to check whether
    the results are meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cosine similarity function used for word embedding comes in handy for having
    a better visualization of the proximity of the positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The similarity between the position of the words `black` and `brown` and the
    lexical field (groups of words that go together) similarity is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The encoding of the position shows a lower similarity value than the word embedding
    similarity.
  prefs: []
  type: TYPE_NORMAL
- en: The positional encoding has taken these words apart. Bear in mind that word
    embeddings will vary with the corpus used to train them. The problem is now how
    to add the positional encoding to the word embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Adding positional encoding to the embedding vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The authors of the Transformer found a simple way by merely adding the positional
    encoding vector to the word embedding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Positional encoding'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go back and take the word embedding of `black`, for example, and name
    it y[1] = *black*, we are ready to add it to the positional vector *pe*(*2*)we
    obtained with positional encoding functions. We will obtain the positional encoding
    *pc*(*black*) of the input word `black`:'
  prefs: []
  type: TYPE_NORMAL
- en: '*pc*(*black*) = y[1] + *pe*(2)'
  prefs: []
  type: TYPE_NORMAL
- en: The solution is straightforward. However, if we apply it as shown, we might
    lose the information of the word embedding, which will be minimized by the positional
    encoding vector.
  prefs: []
  type: TYPE_NORMAL
- en: There are many possibilities to increase the value of y[1] to make sure that
    the information of the word embedding layer can be used efficiently in the subsequent
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the many possibilities is to add an arbitrary value to y[1], the word
    embedding of `black`:'
  prefs: []
  type: TYPE_NORMAL
- en: y[1] * *math.sqrt*(*d_model*)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add the positional vector to the embedding vector of the word `black`,
    which are both of the same size (`512`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The result obtained is the final positional encoding vector of dimension *d*[model]
    = 512:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The same operation is applied to the word `brown` and all of the other words
    in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply the cosine similarity function to the positional encoding vectors
    of `black` and `brown`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a clear view of the positional encoding process through the three
    cosine similarity functions we applied to the three states representing the words
    `black` and `brown`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We saw that the initial word similarity of their embeddings was high, with a
    value of `0.99`. Then we saw the positional encoding vector of positions 2 and
    10 drew these two words apart with a lower similarity value of `0.86`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we added the word embedding vector of each word to its respective positional
    encoding vector. We saw that this brought the cosine similarity of the two words
    to `0.96`.
  prefs: []
  type: TYPE_NORMAL
- en: The positional encoding of each word now contains the initial word embedding
    information and the positional encoding values.
  prefs: []
  type: TYPE_NORMAL
- en: The output of positional encoding leads to the multi-head attention sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sublayer 1: Multi-head attention'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The multi-head attention sublayer contains eight heads and is followed by post-layer
    normalization, which will add residual connections to the output of the sublayer
    and normalize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Multi-head attention sublayer'
  prefs: []
  type: TYPE_NORMAL
- en: This section begins with the architecture of an attention layer. Then, an example
    of multi-attention is implemented in a small module in Python. Finally, post-layer
    normalization is described.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the architecture of multi-head attention.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of multi-head attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input of the multi-attention sublayer of the first layer of the encoder
    stack is a vector that contains the embedding and the positional encoding of each
    word. The next layers of the stack do not start these operations over.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dimension of the vector of each word *x*[n] of an input sequence is *d*[model]
    = 512:'
  prefs: []
  type: TYPE_NORMAL
- en: '*pe*(*x*[n])=[*d*[1]=9.09297407e^-01, *d*[2]=-4.16146845e^-01, .., *d*[512]=1.00000000e+00]'
  prefs: []
  type: TYPE_NORMAL
- en: The representation of each word *x*[n] has become a vector of *d*[model] = 512
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Each word is mapped to all the other words to determine how it fits in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sentence, we can see that it could be related to `cat` and
    `rug` in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The model will train to find out if `it` is related to `cat` or `rug`. We could
    run a huge calculation by training the model using the *d*[model] = 512 dimensions
    as they are now.
  prefs: []
  type: TYPE_NORMAL
- en: However, we would only get one point of view at a time by analyzing the sequence
    with one *d*[model] block. Furthermore, it would take quite some calculation time
    to find other perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: A better way is to divide the *d*[model] = 512 dimensions of each word *x*[n]
    of *x* (all the words of a sequence) into 8 *d*[k]=64 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then can run the 8 “heads” in parallel to speed up the training and obtain
    8 different representation subspaces of how each word relates to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Multi-head representations'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that there are now `8` heads running in parallel. One head might
    decide that `it` fits well with `cat` and another that `it` fits well with `rug`
    and another that `rug` fits well with `dry-cleaned`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of each head is a matrix *Z*[i] with a shape of *x* * *d*[k]. The
    output of a multi-attention head is *Z* defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
  prefs: []
  type: TYPE_NORMAL
- en: However, *Z* must be concatenated so that the output of the multi-head sublayer
    is not a sequence of dimensions but one line of an *xm* * *d*[model] matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before exiting the multi-head attention sublayer, the elements of *Z* are concatenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MultiHead*(*output*) = *Concat*(*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) = *x*, *d*[model]'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that each head is concatenated into *z* that has a dimension of *d*[model]
    = 512\. The output of the multi-headed layer respects the constraint of the original
    Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside each head *h*[n] of the attention mechanism, the "word" matrices have
    three representations:'
  prefs: []
  type: TYPE_NORMAL
- en: A query matrix (*Q*) that has a dimension of *d*[q] = 64, which seeks all the
    key-value pairs of the "word" matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key matrix (*K*) that has a dimension of *d*[k] = 64, which will be trained
    to provide an attention value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value matrix (*V*) that has a dimension of *d*[v] = 64, which will be trained
    to provide another attention value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention is defined as “Scaled Dot-Product Attention,” which is represented
    in the following equation in which we plug *Q*, *K*, and *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrices all have the same dimension, making it relatively simple to use
    a scaled dot product to obtain the attention values for each head and then concatenate
    the output *Z* of the 8 heads.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain *Q*, *K*, and *V*, we must train the model with their weight matrices
    *Q*[w], *K*[w], and *V*[w], which have *d*[k] =64 columns and *d*[model] = 512
    rows. For example, *Q* is obtained by a dot-product between *x* and *Q*[w]. *Q*
    will have a dimension of *d*[k] =64.
  prefs: []
  type: TYPE_NORMAL
- en: You can modify all the parameters, such as the number of layers, heads, *d*[model],
    *d*[k], and other variables of the Transformer to fit your model. This chapter
    describes the original Transformer parameters by *Vaswani* et al. (2017). It is
    essential to understand the original architecture before modifying it or exploring
    variants of the original model designed by others.
  prefs: []
  type: TYPE_NORMAL
- en: Google Brain Trax, OpenAI, and Hugging Face, among others, provide ready-to-use
    libraries that we will be using throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: However, let’s open the hood of the Transformer model and get our hands dirty
    in Python to illustrate the architecture we just explored to visualize the model
    in code and show it with intermediate images.
  prefs: []
  type: TYPE_NORMAL
- en: We will use basic Python code with only `numpy` and a `softmax` function in
    10 steps to run the key aspects of the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that an Industry 4.0 developer will face the challenge of multiple
    architectures for the same algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now start building *Step 1* of our model to represent the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Represent the input'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Save `Multi_Head_Attention_Sub_Layer.ipynb` to your Google Drive (make sure
    you have a Gmail account) and then open it in Google Colaboratory. The notebook
    is in the GitHub repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by only using minimal Python functions to understand the Transformer
    at a low level with the inner workings of an attention head. We will explore the
    inner workings of the multi-head attention sublayer using basic code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The input of the attention mechanism we are building is scaled down to *d*[model]
    ==4 instead of *d*[model] = 512\. This brings the dimensions of the vector of
    an input *x* down to *d*[model] =4, which is easier to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: '*x* contains `3` inputs with `4` dimensions each instead of `512`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that we have 3 vectors of *d*[model] =4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step of our model is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Input of a multi-head attention sublayer'
  prefs: []
  type: TYPE_NORMAL
- en: We will now add the weight matrices to our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Initializing the weight matrices'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each input has 3 weight matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q*[w] to train the queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K*[w] to train the keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V*[w] to train the values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These 3 weight matrices will be applied to all the inputs in this model.
  prefs: []
  type: TYPE_NORMAL
- en: The weight matrices described by *Vaswani* et al. (2017) are *d*[K] ==64 dimensions.
    However, let’s scale the matrices down to *d*[K] ==3\. The dimensions are scaled
    down to `3*4` weight matrices to be able to visualize the intermediate results
    more easily and perform dot products with the input *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The size and shape of the matrices in this educational notebook are arbitrary.
    The goal is to go through the overall process of an attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three weight matrices are initialized starting with the query weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the `w_query` weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now initialize the key weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the key weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we initialize the value weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the value weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step of our model is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Weight matrices added to the model'
  prefs: []
  type: TYPE_NORMAL
- en: We will now multiply the weights by the input vectors to obtain *Q*, *K*, and
    *V*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Matrix multiplication to obtain Q, K, and V'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will now multiply the input vectors by the weight matrices to obtain a query,
    key, and value vector for each input.
  prefs: []
  type: TYPE_NORMAL
- en: In this model, we will assume that there is one `w_query`, `w_key`, and `w_value`
    weight matrix for all inputs. Other approaches are possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first multiply the input vectors by the `w_query` weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a vector for *Q*[1] ==64= [1, 0, 2], *Q*[2]= [2,2, 2], and *Q*[3]=
    [2,1, 3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We now multiply the input vectors by the `w_key` weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain a vector for *K*[1]= [0, 1, 1], *K*[2]= [4, 4, 0], and *K*[3]= [2
    ,3, 1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we multiply the input vectors by the `w_value` weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain a vector for *V*[1]= [1, 2, 3], *V*[2]= [2, 8, 0], and *V*[3]= [2
    ,6, 3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The third step of our model is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.13: Q, K, and V are generated'
  prefs: []
  type: TYPE_NORMAL
- en: We have the *Q*, *K*, and *V* values we need to calculate the attention scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Scaled attention scores'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The attention head now implements the original Transformer equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 4* focuses on *Q* and *K*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For this model, we will round ![](img/B17948_02_006.png) = ![](img/B17948_02_007.png)
    = 1.75 to 1 and plug the values into the *Q* and *K* part of the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The intermediate result is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 4* is now complete. For example, the score for *x*[1] is [2,4,4] across
    the *K* vectors across the head as displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Scaled attention scores for input #1'
  prefs: []
  type: TYPE_NORMAL
- en: The attention equation will now apply softmax to the intermediate scores for
    each vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Scaled softmax attention scores for each vector'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now apply a softmax function to each intermediate attention score. Instead
    of doing a matrix multiplication, let’s zoom down to each individual vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain scaled softmax attention scores for each vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 5* is now complete. For example, the softmax of the score of *x*[1] for
    all the keys is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: The softmax score of input #1 for all of the keys'
  prefs: []
  type: TYPE_NORMAL
- en: We can now calculate the final attention values with the complete equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: The final attention representations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now can finalize the attention equation by plugging *V* in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: We will first calculate the attention score of input *x*[1] for *Steps 6* and
    *7*. We calculate one attention value for one word vector. When we reach *Step
    8*, we will generalize the attention calculation to the other two input vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain *Attention*(*Q,K,V*) for *x*[1] we multiply the intermediate attention
    score by the 3 value vectors one by one to zoom down into the inner workings of
    the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 6* is complete. For example, the 3 attention values for *x*[1] for each
    input have been calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Attention representations'
  prefs: []
  type: TYPE_NORMAL
- en: The attention values now need to be summed up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Summing up the results'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 3 attention values of input #1 obtained will now be summed to obtain the
    first line of the output matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the first line of the output matrix for input #1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The second line will be for the output of the next input, input #2, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the summed attention value for *x*[1] in *Figure 2.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Summed results for one input'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have completed the steps for input #1\. We now need to add the results of
    all the inputs to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 8: Steps 1 to 7 for all the inputs'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Transformer can now produce the attention values of input #2 and input
    #3 using the same method described from *Step 1* to *Step 7* for one attention
    head.'
  prefs: []
  type: TYPE_NORMAL
- en: From this step onwards, we will assume we have 3 attention values with learned
    weights with *d*[model] = 64\. We now want to see what the original dimensions
    look like when they reach the sublayer’s output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the attention representation process in detail with a small model.
    Let’s go directly to the result and assume we have generated the 3 attention representations
    with a dimension of *d*[model] = 64:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output displays the simulation of *z*[0], which represents the
    3 output vectors of *d*[model] = 64 dimensions for head 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The results will vary when you run the notebook because of the stochastic nature
    of the generation of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer now has the output vectors for the inputs of one head. The next
    step is to generate the output of the 8 heads to create the final output of the
    attention sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 9: The output of the heads of the attention sublayer'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We assume that we have trained the 8 heads of the attention sublayer. The Transformer
    now has 3 output vectors (of the 3 input vectors that are words or word pieces)
    of *d*[model] = 64 dimensions each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the shape of one of the heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The 8 heads have now produced *Z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z* = (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5], *Z*[6], *Z*[7])'
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer will now concatenate the 8 elements of *Z* for the final output
    of the multi-head attention sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 10: Concatenation of the output of the heads'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Transformer concatenates the 8 elements of *Z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MultiHead*(*Output*) = *Concat* (*Z*[0], *Z*[1], *Z*[2], *Z*[3], *Z*[4], *Z*[5],
    *Z*[6], *Z*[7]) *W*⁰ = *x*, *d*[model]'
  prefs: []
  type: TYPE_NORMAL
- en: Note that *Z* is multiplied by *W*⁰, which is a weight matrix that is trained
    as well. In this model, we will assume *W*⁰ is trained and integrated into the
    concatenation function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Z*[0] to *Z*[7] are concantenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the concatenation of *Z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The concatenation can be visualized as stacking the elements of *Z* side by
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Attention sublayer output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The concatenation produced a standard *d*[model] = 512 dimensional output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Concatenation of the output of the 8 heads'
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization will now process the attention sublayer.
  prefs: []
  type: TYPE_NORMAL
- en: Post-layer normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each attention sublayer and each Feedforward sublayer of the Transformer is
    followed by **post-layer normalization** (**Post-LN**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: Post-layer normalization'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Post-LN contains an add function and a layer normalization process. The
    add function processes the residual connections that come from the input of the
    sublayer. The goal of the residual connections is to make sure critical information
    is not lost. The Post-LN or layer normalization can thus be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sublayer*(*x*) is the sublayer itself. *x* is the information available at
    the input step of *Sublayer*(*x*).'
  prefs: []
  type: TYPE_NORMAL
- en: The input of the *LayerNormalization* is a vector *v* resulting from *x* + *Sublayer*(*x*).
    *d*[model] = 512 for every input and output of the Transformer, which standardizes
    all the processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many layer normalization methods exist, and variations exist from one model
    to another. The basic concept for *v* = *x* + *Sublayer*(*x*) can be defined by
    *LayerNormalization* (*v*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variables are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_010.png) is the mean of *v* of dimension *d*. As such:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_02_011.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17948_02_012.png) is the standard deviation *v* of dimension *d*.
    As such:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_02_013.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17948_02_014.png) is a scaling parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_02_015.png) is a bias vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This version of *LayerNormalization* (*v*) shows the general idea of the many
    possible post-LN methods. The next sublayer can now process the output of the
    post-LN or *LayerNormalization* (*v*). In this case, the sublayer is a feedforward
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sublayer 2: Feedforward network'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input of the **feedforward network** (**FFN**) is the *d*[model] = 512
    output of the post-LN of the previous sublayer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: Feedforward sublayer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The FFN sublayer can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The FFNs in the encoder and decoder are fully connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FFN is a position-wise network. Each position is processed separately and
    in an identical way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FFN contains two layers and applies a ReLU activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input and output of the FFN layers is *d*[model] = 512, but the inner layer
    is larger with *d*[ff] =2048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FFN can be viewed as performing two convolutions with size 1 kernels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taking this description into account, we can describe the optimized and standardized
    FFN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*FFN*(*x*) = max (0, *xW*[1] + *b*[1]) *W*[2]+*b*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the FFN goes to post-LN, as described in the previous section.
    Then the output is sent to the next layer of the encoder stack and the multi-head
    attention layer of the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The layers of the decoder of the Transformer model are *stacks of layers* like
    the encoder layers. Each layer of the decoder stack has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_02_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: A layer of the decoder stack of the Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the decoder layer remains the same as the encoder for all
    the *N*=6 layers of the Transformer model. Each layer contains three sublayers:
    a multi-headed masked attention mechanism, a multi-headed attention mechanism,
    and a fully connected position-wise feedforward network.'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder has a third main sublayer, which is the masked multi-head attention
    mechanism. In this sublayer output, at a given position, the following words are
    masked so that the Transformer bases its assumptions on its inferences without
    seeing the rest of the sequence. That way, in this model, it cannot see future
    parts of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'A residual connection, *Sublayer*(*x*), surrounds each of the three main sublayers
    in the Transformer model like in the encoder stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LayerNormalization* (*x* + *Sublayer*(*x*))'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer sublayer is only present at the bottom level of the stack,
    like for the encoder stack. The output of every sublayer of the decoder stack
    has a constant dimension, *d*[model] like in the encoder stack, including the
    embedding layer and the output of the residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the designers worked hard to create symmetrical encoder and
    decoder stacks.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of each sublayer and function of the decoder is similar to the
    encoder. In this section, we can refer to the encoder for the same functionality
    when we need to. We will only focus on the differences between the decoder and
    the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Output embedding and position encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The structure of the sublayers of the decoder is mostly the same as the sublayers
    of the encoder. The output embedding layer and position encoding function are
    the same as in the encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Transformer usage we are exploring through the model presented by *Vaswani*
    et al. (2017), the output is a translation we need to learn. I chose to use a
    French translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This output is the French translation of the English input sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The output words go through the word embedding layer and then the positional
    encoding function like in the first layer of the encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the specific properties of the multi-head attention layers of the
    decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: The attention layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Transformer is an auto-regressive model. It uses the previous output sequences
    as an additional input. The multi-head attention layers of the decoder use the
    same process as the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: However, the masked multi-head attention sublayer 1 only lets attention apply
    to the positions up to and including the current position. The future words are
    hidden from the Transformer, and this forces it to learn how to predict.
  prefs: []
  type: TYPE_NORMAL
- en: A post-layer normalization process follows the masked multi-head attention sublayer
    1 as in the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-head attention sublayer 2 also only attends to the positions up to
    the current position the Transformer is predicting to avoid seeing the sequence
    it must predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multi-head attention sublayer 2 draws information from the encoder by taking
    encoder (*K*, *V*) into account during the dot-product attention operations. This
    sublayer also draws information from the masked multi-head attention sublayer
    1 (masked attention) by also taking sublayer 1(*Q*) into account during the dot-product
    attention operations. The decoder thus uses the trained information of the encoder.
    We can define the input of the self-attention multi-head sublayer of a decoder
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input_Attention* = (*Output_decoder_sub_layer* – 1(*Q*), *Output_encoder_layer*(*K*,
    *V*))'
  prefs: []
  type: TYPE_NORMAL
- en: A post-layer normalization process follows the masked multi-head attention sublayer
    1 as in the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer then goes to the FFN sublayer, followed by a post-LN and the
    linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: The FFN sublayer, the post-LN, and the linear layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The FFN sublayer has the same structure as the FFN of the encoder stack. The
    post-layer normalization of the FFN works as the layer normalization of the encoder
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer produces an output sequence of only one element at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output sequence* = (*y*[1], *y*[2], … *y*[n])'
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear layer produces an output sequence with a linear function that varies
    per model but relies on the standard method:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* =*w* * *x* + *b*'
  prefs: []
  type: TYPE_NORMAL
- en: '*w* and *b* are learned parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The linear layer will thus produce the next probable elements of a sequence
    that a softmax function will convert into a probable element.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder layer, like the encoder layer, will then go from layer *l* to layer
    *l+1*, up to the top layer of the *N*=6-layer transformer stack.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the Transformer was trained and the performance it obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Training and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original Transformer was trained on a 4.5 million sentence pair English-German
    dataset and a 36 million sentence pair English-French dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets come from **Workshops on Machine Translation** (**WMT**), which
    can be found at the following link if you wish to explore the WMT datasets: [http://www.statmt.org/wmt14/](http://www.statmt.org/wmt14/)'
  prefs: []
  type: TYPE_NORMAL
- en: The training of the original Transformer base models took 12 hours to train
    for 100,000 steps on a machine with 8 NVIDIA P100 GPUs. The big models took 3.5
    days for 300,000 steps.
  prefs: []
  type: TYPE_NORMAL
- en: The original Transformer outperformed all the previous machine translation models
    with a BLEU score of 41.8\. The result was obtained on the WMT English-to-French
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU stands for Bilingual Evaluation Understudy. It is an algorithm that evaluates
    the quality of the results of machine translations.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Research and Google Brain team applied optimization strategies to
    improve the performance of the Transformer. For example, the Adam optimizer was
    used, but the learning rate varied by first going through warmup states with a
    linear rate and decreasing the rate afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of regularization techniques, such as residual dropout and dropouts,
    were applied to the sums of embeddings. Also, the Transformer applies label smoothing
    that avoids overfitting with overconfident one-hot outputs. It introduces less
    accurate evaluations and forces the model to train more and better.
  prefs: []
  type: TYPE_NORMAL
- en: Several other Transformer model variations have led to other models and usages
    that we will explore in the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Before end the chapter, let’s get a feel of the simplicity of ready-to-use transformer
    models in Hugging Face, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Tranformer models in Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything you saw in this chapter can be condensed in to a ready-to-use Hugging
    Face transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: With Hugging Face, you can implement machine translation in three lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: Open `Multi_Head_Attention_Sub_Layer.ipynb` in Google Colaboratory. Save the
    notebook in your Google Drive (make sure you have a Gmail account). Go to the
    two last cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first ensure that Hugging Face transformers are installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The first cell imports the Hugging Face pipeline that contains several transformer
    usages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We then implement the Hugging Face pipeline, which contains ready-to-use functions.
    In our case, to illustrate the Transformer model of this chapter, we activate
    the translator model and enter a sentence to translate from English to French:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'And *voilà*! The translation is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face shows how transformer architectures can be used in ready-to-use
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first got started by examining the mind-blowing long-distance
    dependencies transformer architectures can uncover. Transformers can perform transductions
    from written and oral sequences to meaningful representations as never before
    in the history of **Natural Language Understanding** (**NLU**).
  prefs: []
  type: TYPE_NORMAL
- en: These two dimensions, the expansion of transduction and the simplification of
    implementation, are taking artificial intelligence to a level never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: We explored the bold approach of removing RNNs, LSTMs, and CNNs from transduction
    problems and sequence modeling to build the Transformer architecture. The symmetrical
    design of the standardized dimensions of the encoder and decoder makes the flow
    from one sublayer to another nearly seamless.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that beyond removing recurrent network models, transformers introduce
    parallelized layers that reduce training time. We discovered other innovations,
    such as positional encoding and masked multi-headed attention.
  prefs: []
  type: TYPE_NORMAL
- en: The flexible, original Transformer architecture provides the basis for many
    other innovative variations that open the way for yet more powerful transduction
    problems and language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: We will zoom more in-depth into some aspects of the Transformer’s architecture
    in the following chapters when describing the many variants of the original model.
  prefs: []
  type: TYPE_NORMAL
- en: The arrival of the Transformer marks the beginning of a new generation of ready-to-use
    artificial intelligence models. For example, Hugging Face and Google Brain make
    artificial intelligence easy to implement with a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Fine-Tuning BERT Models*, we will explore the powerful
    evolutions of the original Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP transduction can encode and decode text representations. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Natural Language Understanding** (**NLU**) is a subset of **Natural Language
    Processing** (**NLP**). (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Language modeling algorithms generate probable sequences of words based on input
    sequences. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer is a customized LSTM with a CNN layer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer does not contain LSTM or CNN layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention examines all the tokens in a sequence, not just the last one. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer uses a positional vector, not positional encoding. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer contains a feedforward network. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The masked multi-headed attention component of the decoder of a transformer
    prevents the algorithm parsing a given position from seeing the rest of a sequence
    that is being processed. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can analyze long-distance dependencies better than LSTMs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, *2017*, *Attention
    Is All You Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformer Usage: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tensor2Tensor (T2T) Introduction: [https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?hl=en](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?hl=en)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manuel Romero Notebook with link to explanations by *Raimi Karim*: [https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google language research: [https://research.google/teams/language/](https://research.google/teams/language/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face research: [https://huggingface.co/transformers/index.html](https://huggingface.co/transformers/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Annotated Transformer*: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jay Alammar*, *The Illustrated Transformer*: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
