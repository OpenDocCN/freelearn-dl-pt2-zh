- en: Building Blocks of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding the basic building blocks of a neural network, such as tensors,
    tensor operations, and gradient descents, is important for building complex neural
    networks. In this chapter, we will build our first `Hello world` program in neural
    networks by covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing our first neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the neural network into functional blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking through each fundamental block covering tensors, variables, autograds,
    gradients, and optimizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is available as a Python package and you can either use `pip`, or `conda`,
    to build it or you can build it from source. The recommended approach for this
    book is to use the Anaconda Python 3 distribution. To install Anaconda, please
    refer to the Anaconda official documentation at [https://conda.io/docs/user-guide/install/index.html](https://conda.io/docs/user-guide/install/index.html).
    All the examples will be available as Jupyter Notebooks in the book's GitHub repository.
    I would strongly recommend you use Jupyter Notebook, since it allows you to experiment
    interactively. If you already have Anaconda Python installed, then you can proceed
    with the following steps for PyTorch installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPU-based installation with Cuda 8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For GPU-based installation with Cuda 7.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For non-GPU-based installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: At the time of writing, PyTorch does not work on a Windows machine, so you can
    try a **virtual machine** (**VM**) or Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Our first neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We present our first neural network, which learns how to map training examples
    (input array) to targets (output array). Let''s assume that we work for one of
    the largest online companies, **Wondermovies,** which serves videos on demand.
    Our training dataset contains a feature that represents the average hours spent
    by users watching movies on the platform and we would like to predict how much
    time each user would spend on the platform in the coming week. It''s just an imaginary
    use case, don''t think too much about it. Some of the high-level activities for
    building such a solution are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: The `get_data` function prepares the tensors (arrays)
    containing input and output data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating learnable** **parameters**: The `get_weights` function provides
    us with tensors containing random values that we will optimize to solve our problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network model**: The `simple_network` function produces the output for the
    input data, applying a linear rule, multiplying weights with input data, and adding
    the bias term (*y = Wx+b*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss**: The `loss_fn` function provides information about how good the model
    is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: The `optimize` function helps us in adjusting random weights
    created initially to help the model calculate target values more accurately'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are new to machine learning, do not worry, as we will understand exactly
    what each function does by the end of the chapter. The following functions abstract
    away PyTorch code to make it easier for us to understand. We will dive deep into
    each of these functionalities in detail. The aforementioned high level activities
    are common for most machine learning and deep learning problems. Later chapters
    in the book discuss techniques that can be used to improve each function to build
    useful applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets consider following linear regression equation for our neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c152c855-4352-491c-8602-80be5cd3c4d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s write our first neural network in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By the end of this chapter, you will have an idea of what is happening inside
    each function.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides two kinds of data abstractions called `tensors` and `variables`.
    Tensors are similar to `numpy` arrays and they can also be used on GPUs, which
    provide increased performance. They provide easy methods of switching between
    GPUs and CPUs. For certain operations, we can notice a boost in performance and
    machine learning algorithms can understand different forms of data, only when
    represented as tensors of numbers. Tensors are like Python arrays and can change
    in size. For example, images can be represented as three-dimensional arrays (height,
    weight, channel (RGB)). It is common in deep learning to use tensors of sizes
    up to five dimensions. Some of the commonly used tensors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar (0-D tensors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector (1-D tensors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix (2-D tensors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3-D tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slicing tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4-D tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5-D tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalar (0-D tensors)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A tensor containing only one element is called a **scalar**. It will generally
    be of type `FloatTensor` or `LongTensor`. At the time of writing, PyTorch does
    not have a special tensor with zero dimensions. So, we use a one-dimension tensor
    with one element, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Vectors (1-D tensors)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A `vector` is simply an array of elements. For example, we can use a vector
    to store the average temperature for the last week:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Matrix (2-D tensors)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the structured data is represented in the form of tables or matrices.
    We will use a dataset called `Boston House Prices`, which is readily available
    in the Python scikit-learn machine learning library. The dataset is a `numpy`
    array consisting of `506` samples or rows and `13` features representing each
    sample. Torch provides a utility function called `from_numpy()`, which converts
    a `numpy` array into a `torch` tensor. The shape of the resulting tensor is `506`
    rows x `13` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 3-D tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we add multiple matrices together, we get a *3-D tensor*. 3-D tensors
    are used to represent data-like images. Images can be represented as numbers in
    a matrix, which are stacked together. An example of an image shape is `224`, `224`,
    `3`, where the first index represents height, the second represents width, and
    the third represents a channel (RGB). Let''s see how a computer sees a panda,
    using the next code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since displaying the tensor of size `224`, `224`, `3` would occupy a couple
    of pages in the book, we will display the image and learn to slice the image into
    smaller tensors to visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5de7e6a-4c8a-4aed-91da-5d4180b3f9f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Displaying the image
  prefs: []
  type: TYPE_NORMAL
- en: Slicing tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common thing to do with a tensor is to slice a portion of it. A simple example
    could be choosing the first five elements of a one-dimensional tensor; let''s
    call the tensor `sales`. We use a simple notation, `sales[:slice_index]` where
    `slice_index` represents the index where you want to slice the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's do more interesting things with our panda image, such as see what the
    panda image looks like when only one channel is chosen and see how to select the
    face of the panda.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we select only one channel from the panda image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21adcb3c-80af-424f-9cc0-16258a64e071.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, lets crop the image. Say we want to build a face detector for pandas and
    we need just the face of a panda for that. We crop the tensor image such that
    it contains only the panda''s face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47dea173-616d-4538-89fd-c376e409ff73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another common example would be where you need to pick a specific element of
    a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We will revisit image data in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep
    Learning for Computer Vision,* when we discuss using CNNs to build image classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the PyTorch tensor operations are very similar to `NumPy` operations.
  prefs: []
  type: TYPE_NORMAL
- en: 4-D tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common example for four-dimensional tensor types is a batch of images. Modern
    CPUs and GPUs are optimized to perform the same operations on multiple examples
    faster. So, they take a similar time to process one image or a batch of images.
    So, it is common to use a batch of examples rather than use a single image at
    a time. Choosing the batch size is not straightforward; it depends on several
    factors. One major restriction for using a bigger batch or the complete dataset
    is GPU memory limitations—*16*, *32*, and *64* are commonly used batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example where we load a batch of cat images of size `64`
    x `224` x `224` x `3` where *64* represents the batch size or the number of images,
    *244* represents height and width, and *3* represents channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 5-D tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common example where you may have to use a five-dimensional tensor is video
    data. Videos can be split into frames, for example, a 30-second video containing
    a panda playing with a ball may contain 30 frames, which could be represented
    as a tensor of shape (1 x 30 x 224 x 224 x 3). A batch of such videos can be represented
    as tensors of shape (32 x 30 x 224 x 224 x 3)—*30* in the example represents,
    number of frames in that single video clip, where *32* represents the number of
    such video clips.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors on GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned how to represent different forms of data in tensor representation.
    Some of the common operations we perform once we have data in the form of tensors
    are addition, subtraction, multiplication, dot product, and matrix multiplication.
    All of these operations can be either performed on the CPU or the GPU. PyTorch
    provides a simple function called `cuda()` to copy a tensor on the CPU to the
    GPU. We will take a look at some of the operations and compare the performance
    between matrix multiplication operations on the CPU and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor addition can be obtained by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For tensor matrix multiplication, lets compare the code performance on CPU and
    GPU. Any tensor can be moved to the GPU by calling the `.cuda()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication on the GPU runs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These fundamental operations of addition, subtraction, and matrix multiplication
    can be used to build complex operations, such as a **Convolution Neural Network**
    (**CNN**) and a **recurrent neural network** (**RNN**), which we will learn about
    in the later chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning algorithms are often represented as computation graphs. Here
    is a simple example of the variable computation graph that we built in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bffe8e5-599a-424e-8217-abbdee0cc8b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Variable computation graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Each circle in the preceding computation graph represents a variable. A variable
    forms a thin wrapper around a tensor object, its gradients, and a reference to
    the function that created it. The following figure shows `Variable` class components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3cefc1f-2e5c-4d58-a488-f970ea340fa4.png)'
  prefs: []
  type: TYPE_IMG
- en: Variable class
  prefs: []
  type: TYPE_NORMAL
- en: The gradients refer to the rate of the change of the `loss` function with respect
    to various parameters (**W**, **b**). For example, if the gradient of **a** is
    2, then any change in the value of **a** would modify the value of **Y** by two
    times. If that is not clear, do not worry—most of the deep learning frameworks
    take care of calculating gradients for us. In this chapter, we learn how to use
    these gradients to improve the performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from gradients, a variable also has a reference to the function that created
    it, which in turn refers to how each variable was created. For example, the variable
    `a` has information that it is generated as a result of the product between `X`
    and `W`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example where we create variables and check the gradients
    and the function reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we called a `backward` operation on the variable to
    compute the gradients. By default, the gradients of the variables are none.
  prefs: []
  type: TYPE_NORMAL
- en: The `grad_fn` of the variable points to the function it created. If the variable
    is created by a user, like the variable `x` in our case, then the function reference
    is `None`. In the case of variable `y,` it refers to its function reference, `MeanBackward`.
  prefs: []
  type: TYPE_NORMAL
- en: The Data attribute accesses the tensor associated with the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Creating data for our neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_data` function in our first neural network code creates two variables, `x`
    and `y`, of sizes (`17`, `1`) and (`17`). We will take a look at what happens
    inside the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating learnable parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our neural network example, we have two learnable parameters, `w` and `b`,
    and two fixed parameters, `x` and `y`. We have created variables `x` and `y` in
    our `get_data` function. Learnable parameters are created using random initialization
    and have the `require_grad` parameter set to `True`, unlike `x` and `y`, where
    it is set to `False`. There are different practices for initializing learnable
    parameters, which we will explore in the coming chapters. Let''s take a look at
    our `get_weights` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Most of the preceding code is self-explanatory; `torch.randn` creates a random
    value of any given shape.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have defined the inputs and outputs of the model using PyTorch variables,
    we have to build a model which learns how to map the outputs from the inputs.
    In traditional programming, we build a function by hand coding different logic
    to map the inputs to the outputs. However, in deep learning and machine learning,
    we learn the function by showing it the inputs and the associated outputs. In
    our example, we implement a simple neural network which tries to map the inputs
    to outputs, assuming a linear relationship. The linear relationship can be represented
    as *y = wx + b*, where *w* and *b* are learnable parameters. Our network has to
    learn the values of *w* and *b*, so that *wx + b* will be closer to the actual
    *y*. Let''s visualize our training dataset and the model that our neural network
    has to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4780c540-f052-46b7-94eb-6580d4ae5814.png)'
  prefs: []
  type: TYPE_IMG
- en: Input data points
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents a linear model fitted on input data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81c03405-d31a-45b7-9fbd-4de358111092.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear model fitted on input data points
  prefs: []
  type: TYPE_NORMAL
- en: The dark-gray (blue) line in the image represents the model that our network
    learns.
  prefs: []
  type: TYPE_NORMAL
- en: Network implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have all the parameters (`x`, `w`, `b`, and `y`) required to implement
    the network, we perform a matrix multiplication between `w` and `x`. Then, sum
    the result with `b`. That will give our predicted `y`. The function is implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch also provides a higher-level abstraction in `torch.nn` called **layers**,
    which will take care of most of these underlying initialization and operations
    associated with most of the common techniques available in the neural network.
    We are using the lower-level operations to understand what happens inside these
    functions. In later chapters, that is [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    Deep Learning for Computer Vision and [Chapter 6](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml),
    Deep Learning with Sequence Data and Text, we will be relying on the PyTorch abstractions
    to build complex neural networks or functions. The previous model can be represented
    as a `torch.nn` layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have calculated the `y` values, we need to know how good our model
    is, which is done in the `loss` function.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we start with random values, our learnable parameters, `w` and `b`, will
    result in `y_pred`, which will not be anywhere close to the actual `y`. So, we
    need to define a function which tells the model how close its predictions are
    to the actual values. Since this is a regression problem, we use a loss function
    called **sum of squared error** (**SSE**). We take the difference between the
    predicted `y` and the actual `y` and square it. SSE helps the model to understand
    how close the predicted values are to the actual values. The `torch.nn` library
    has different loss functions, such as MSELoss and cross-entropy loss. However,
    for this chapter, let''s implement the `loss` function ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Apart from calculating the loss, we also call the `backward` operation, which
    calculates the gradients of our learnable parameters, `w` and `b`. As we will
    use the `loss` function more than once, we remove any previously calculated gradients
    by calling the `grad.data.zero_()` operation. The first time we call the `backward`
    function, the gradients are empty, so we zero the gradients only when they are
    not `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started with random weights to predict our targets and calculate loss for
    our algorithm. We calculate the gradients by calling the `backward` function on
    the final `loss` variable. This entire process repeats for one epoch, that is,
    for the entire set of examples. In most of the real-world examples, we will do
    the optimization step per iteration, which is a small subset of the total set.
    Once the loss is calculated, we optimize the values with the calculated gradients
    so that the loss reduces, which is implemented in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The learning rate is a hyper-parameter, which allows us to adjust the values
    in the variables by a small amount of the gradients, where the gradients denote
    the direction in which each variable (`w` and `b`) needs to be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: Different optimizers, such as Adam, RmsProp, and SGD are already implemented
    for use in the `torch.optim` package. We will be making use of these optimizers
    in later chapters to reduce the loss or improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing data for deep learning algorithms could be a complex pipeline by itself.
    PyTorch provides many utility classes that abstract a lot of complexity such as
    data-parallelization through multi-threading, data-augmenting, and batching. In
    this chapter, we will take a look at two of the important utility classes, namely
    the `Dataset` class and the `DataLoader` class. To understand how to use these
    classes, let's take the `Dogs vs. Cats` dataset from Kaggle ([https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data))
    and create a data pipeline that generates a batch of images in the form of PyTorch
    tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any custom dataset class, say for example, our `Dogs` dataset class, has to
    inherit from the PyTorch dataset class. The custom class has to implement two
    main functions, namely `__len__(self)` and `__getitem__(self, idx)`. Any custom
    class acting as a `Dataset` class should look like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We do any initialization, if required, inside the `init` method—for example,
    reading the index of the table and reading the filenames of the images, in our
    case. The `__len__(self)` operation is responsible for returning the maximum number
    of elements in our dataset. The `__getitem__(self, idx)` operation returns an
    element based on the `idx` every time it is called. The following code implements
    our `DogsAndCatsDataset` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the `DogsAndCatsDataset` class is created, we can create an object and
    iterate over it, which is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Applying a deep learning algorithm on a single instance of data is not optimal.
    We need a batch of data, as modern GPUs are optimized for better performance when
    executed on a batch of data. The `DataLoader` class helps to create batches by
    abstracting a lot of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `DataLoader` class present in PyTorch''s `utils` class combines a dataset
    object along with different samplers, such as `SequentialSampler` and `RandomSampler`,
    and provides us with a batch of images, either using a single or multi-process
    iterators. Samplers are different strategies for providing data to algorithms.
    The following is an example of a `DataLoader` for our `Dogs vs. Cats` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`imgs` will contain a tensor of shape (32, 224, 224, 3), where *32* represents
    the batch size.'
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch team also maintains two useful libraries, called `torchvision` and
    `torchtext`, which are built on top of the `Dataset` and `DataLoader` classes.
    We will use them in the relevant chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various data structures and operations provided
    by PyTorch. We implemented several components, using the fundamental blocks of
    PyTorch. For our data preparation, we created the tensors used by our algorithm.
    Our network architecture was a model for learning to predict average hours spent
    by users on our Wondermovies platform. We used the loss function to check the
    standard of our model and used the `optimize` function to adjust the learnable
    parameters of our model to make it perform better.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how PyTorch makes it easier to create data pipelines by abstracting
    away several complexities that would require us to parallelize and augment data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deep into how neural networks and deep learning
    algorithms work. We will explore various PyTorch built-in modules for building
    network architectures, loss functions, and optimizations. We will also show how
    to use them on real-world datasets.
  prefs: []
  type: TYPE_NORMAL
