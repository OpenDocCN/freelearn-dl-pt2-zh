["```py\n>>> import torch\n>>> sentence = torch.tensor(\n>>>     [0, # can\n>>>      7, # you     \n>>>      1, # help\n>>>      2, # me\n>>>      5, # to\n>>>      6, # translate\n>>>      4, # this\n>>>      3] # sentence\n>>> )\n>>> sentence\ntensor([0, 7, 1, 2, 5, 6, 4, 3]) \n```", "```py\n>>> torch.manual_seed(123)\n>>> embed = torch.nn.Embedding(10, 16)\n>>> embedded_sentence = embed(sentence).detach()\n>>> embedded_sentence.shape\ntorch.Size([8, 16]) \n```", "```py\n>>> omega = torch.empty(8, 8)\n>>> for i, x_i in enumerate(embedded_sentence):\n>>>     for j, x_j in enumerate(embedded_sentence):\n>>>         omega[i, j] = torch.dot(x_i, x_j) \n```", "```py\n>>> omega_mat = embedded_sentence.matmul(embedded_sentence.T) \n```", "```py\n>>> torch.allclose(omega_mat, omega)\nTrue \n```", "```py\n>>> import torch.nn.functional as F\n>>> attention_weights = F.softmax(omega, dim=1)\n>>> attention_weights.shape\ntorch.Size([8, 8]) \n```", "```py\n>>> attention_weights.sum(dim=1)\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) \n```", "```py\n>>> x_2 = embedded_sentence[1, :]\n>>> context_vec_2 = torch.zeros(x_2.shape)\n>>> for j in range(8):\n...     x_j = embedded_sentence[j, :]\n...     context_vec_2 += attention_weights[1, j] * x_j \n>>> context_vec_2\ntensor([-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01, -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n-7.1288e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n-2.1601e+00]) \n```", "```py\n>>> context_vectors = torch.matmul(\n...     attention_weights, embedded_sentence) \n```", "```py\n>>> torch.allclose(context_vec_2, context_vectors[1])\nTrue \n```", "```py\n>>> torch.manual_seed(123)\n>>> d = embedded_sentence.shape[1]\n>>> U_query = torch.rand(d, d)\n>>> U_key = torch.rand(d, d)\n>>> U_value = torch.rand(d, d) \n```", "```py\n>>> x_2 = embedded_sentence[1]\n>>> query_2 = U_query.matmul(x_2) \n```", "```py\n>>> key_2 = U_key.matmul(x_2)\n>>> value_2 = U_value.matmul(x_2) \n```", "```py\n>>> keys = U_key.matmul(embedded_sentence.T).T\n>>> values = U_value.matmul(embedded_sentence.T).T \n```", "```py\n>>> keys = U_key.matmul(embedded_sentence.T).T\n>>> torch.allclose(key_2, keys[1])\n>>> values = U_value.matmul(embedded_sentence.T).T\n>>> torch.allclose(value_2, values[1]) \n```", "```py\n>>> omega_23 = query_2.dot(keys[2])\n>>> omega_23\ntensor(14.3667) \n```", "```py\n>>> omega_2 = query_2.matmul(keys.T)\n>>> omega_2\ntensor([-25.1623,   9.3602,  14.3667,  32.1482,  53.8976,  46.6626,  -1.2131, -32.9391]) \n```", "```py\n>>> attention_weights_2 = F.softmax(omega_2 / d**0.5, dim=0)\n>>> attention_weights_2\ntensor([2.2317e-09, 1.2499e-05, 4.3696e-05, 3.7242e-03, 8.5596e-01, 1.4025e-01, 8.8896e-07, 3.1936e-10]) \n```", "```py\n>>> context_vector_2 = attention_weights_2.matmul(values)\n>>> context_vector_2\ntensor([-1.2226, -3.4387, -4.3928, -5.2125, -1.1249, -3.3041, \n-1.4316, -3.2765, -2.5114, -2.6105, -1.5793, -2.8433, -2.4142, \n-0.3998, -1.9917, -3.3499]) \n```", "```py\n>>> torch.manual_seed(123)\n>>> d = embedded_sentence.shape[1]\n>>> one_U_query = torch.rand(d, d) \n```", "```py\n>>> h = 8\n>>> multihead_U_query = torch.rand(h, d, d)\n>>> multihead_U_key = torch.rand(h, d, d)\n>>> multihead_U_value = torch.rand(h, d, d) \n```", "```py\n>>> multihead_query_2 = multihead_U_query.matmul(x_2)\n>>> multihead_query_2.shape\ntorch.Size([8, 16]) \n```", "```py\n>>> multihead_key_2 = multihead_U_key.matmul(x_2)\n>>> multihead_value_2 = multihead_U_value.matmul(x_2)\n>>> multihead_key_2[2]\ntensor([-1.9619, -0.7701, -0.7280, -1.6840, -1.0801, -1.6778,  0.6763,  0.6547,\n         1.4445, -2.7016, -1.1364, -1.1204, -2.4430, -0.5982, -0.8292, -1.4401]) \n```", "```py\n>>> stacked_inputs = embedded_sentence.T.repeat(8, 1, 1)\n>>> stacked_inputs.shape\ntorch.Size([8, 16, 8]) \n```", "```py\n>>> multihead_keys = torch.bmm(multihead_U_key, stacked_inputs)\n>>> multihead_keys.shape\ntorch.Size([8, 16, 8]) \n```", "```py\n>>> multihead_keys = multihead_keys.permute(0, 2, 1)\n>>> multihead_keys.shape\ntorch.Size([8, 8, 16]) \n```", "```py\n>>> multihead_keys[2, 1] \ntensor([-1.9619, -0.7701, -0.7280, -1.6840, -1.0801, -1.6778,  0.6763,  0.6547,\n         1.4445, -2.7016, -1.1364, -1.1204, -2.4430, -0.5982, -0.8292, -1.4401]) \n```", "```py\n>>> multihead_values = torch.matmul(\n        multihead_U_value, stacked_inputs)\n>>> multihead_values = multihead_values.permute(0, 2, 1) \n```", "```py\n>>> multihead_z_2 = torch.rand(8, 16) \n```", "```py\n>>> linear = torch.nn.Linear(8*16, 16)\n>>> context_vector_2 = linear(multihead_z_2.flatten())\n>>> context_vector_2.shape\ntorch.Size([16]) \n```", "```py\npip install transformers==4.9.1 \n```", "```py\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2') \n```", "```py\n>>> set_seed(123)\n>>> generator(\"Hey readers, today is\",\n...           max_length=20,\n...           num_return_sequences=3)\n[{'generated_text': \"Hey readers, today is not the last time we'll be seeing one of our favorite indie rock bands\"},\n {'generated_text': 'Hey readers, today is Christmas. This is not Christmas, because Christmas is so long and I hope'},\n {'generated_text': \"Hey readers, today is CTA Day!\\n\\nWe're proud to be hosting a special event\"}] \n```", "```py\n>>> from transformers import GPT2Tokenizer\n>>> tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n>>> text = \"Let us encode this sentence\"\n>>> encoded_input = tokenizer(text, return_tensors='pt')\n>>> encoded_input\n{'input_ids': tensor([[ 5756,   514, 37773,   428,  6827]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])} \n```", "```py\n>>> from transformers import GPT2Model\n>>> model = GPT2Model.from_pretrained('gpt2')\n>>> output = model(**encoded_input) \n```", "```py\n>>> output['last_hidden_state'].shape\ntorch.Size([1, 5, 768]) \n```", "```py\n>>> import gzip\n>>> import shutil\n>>> import time\n>>> import pandas as pd\n>>> import requests\n>>> import torch\n>>> import torch.nn.functional as F\n>>> import torchtext\n>>> import transformers\n>>> from transformers import DistilBertTokenizerFast\n>>> from transformers import DistilBertForSequenceClassification \n```", "```py\n>>> torch.backends.cudnn.deterministic = True\n>>> RANDOM_SEED = 123\n>>> torch.manual_seed(RANDOM_SEED)\n>>> DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n>>> NUM_EPOCHS = 3 \n```", "```py\n>>> url = (\"https://github.com/rasbt/\"\n...        \"machine-learning-book/raw/\"\n...        \"main/ch08/movie_data.csv.gz\")\n>>> filename = url.split(\"/\")[-1]\n>>> with open(filename, \"wb\") as f:\n...     r = requests.get(url)\n...     f.write(r.content)\n>>> with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n...     with open('movie_data.csv', 'wb') as f_out:\n...         shutil.copyfileobj(f_in, f_out) \n```", "```py\n>>> df = pd.read_csv('movie_data.csv')\n>>> df.head(3) \n```", "```py\n>>> train_texts = df.iloc[:35000]['review'].values\n>>> train_labels = df.iloc[:35000]['sentiment'].values\n>>> valid_texts = df.iloc[35000:40000]['review'].values\n>>> valid_labels = df.iloc[35000:40000]['sentiment'].values\n>>> test_texts = df.iloc[40000:]['review'].values\n>>> test_labels = df.iloc[40000:]['sentiment'].values \n```", "```py\n>>> tokenizer = DistilBertTokenizerFast.from_pretrained(\n...     'distilbert-base-uncased'\n... )\n>>> train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n>>> valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n>>> test_encodings = tokenizer(list(test_texts), truncation=True, padding=True) \n```", "```py\n>>> class IMDbDataset(torch.utils.data.Dataset):\n...     def __init__(self, encodings, labels):\n...         self.encodings = encodings\n...         self.labels = labels\n>>>     def __getitem__(self, idx):\n...         item = {key: torch.tensor(val[idx]) \n...                 for key, val in self.encodings.items()}\n...         item['labels'] = torch.tensor(self.labels[idx])\n...         return item\n>>>     def __len__(self):\n...         return len(self.labels)\n>>> train_dataset = IMDbDataset(train_encodings, train_labels)\n>>> valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n>>> test_dataset = IMDbDataset(test_encodings, test_labels)\n>>> train_loader = torch.utils.data.DataLoader(\n...     train_dataset, batch_size=16, shuffle=True) \n>>> valid_loader = torch.utils.data.DataLoader(\n...     valid_dataset, batch_size=16, shuffle=False) \n>>> test_loader = torch.utils.data.DataLoader(\n...     test_dataset, batch_size=16, shuffle=False) \n```", "```py\n>>> model = DistilBertForSequenceClassification.from_pretrained(\n...     'distilbert-base-uncased')\n>>> model.to(DEVICE)\n>>> model.train()\n>>> optim = torch.optim.Adam(model.parameters(), lr=5e-5) \n```", "```py\n>>> def compute_accuracy(model, data_loader, device):\n...         with torch.no_grad():\n...             correct_pred, num_examples = 0, 0\n...             for batch_idx, batch in enumerate(data_loader):\n...                 ### Prepare data\n...                 input_ids = batch['input_ids'].to(device)\n...                 attention_mask = \\\n...                     batch['attention_mask'].to(device)\n...                 labels = batch['labels'].to(device)\n\n...                 outputs = model(input_ids,\n...                    attention_mask=attention_mask)\n...                 logits = outputs['logits']\n...                 predicted_labels = torch.argmax(logits, 1)\n...                 num_examples += labels.size(0)\n...                 correct_pred += \\\n...                     (predicted_labels == labels).sum()\n...         return correct_pred.float()/num_examples * 100 \n```", "```py\n>>> start_time = time.time()\n>>> for epoch in range(NUM_EPOCHS):\n\n...     model.train()\n\n...     for batch_idx, batch in enumerate(train_loader):\n\n...         ### Prepare data\n...         input_ids = batch['input_ids'].to(DEVICE)\n...         attention_mask = batch['attention_mask'].to(DEVICE)\n...         labels = batch['labels'].to(DEVICE)\n...         ### Forward pass\n...         outputs = model(input_ids, \n...                         attention_mask=attention_mask,\n...                         labels=labels)\n...         loss, logits = outputs['loss'], outputs['logits']\n\n...         ### Backward pass\n...         optim.zero_grad()\n...         loss.backward()\n...         optim.step()\n\n...         ### Logging\n...         if not batch_idx % 250:\n...             print(f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d}' \n...                     f' | Batch' \n...                     f'{batch_idx:04d}/'\n...                     f'{len(train_loader):04d} | '\n...                     f'Loss: {loss:.4f}')\n\n...     model.eval()\n...     with torch.set_grad_enabled(False):\n...         print(f'Training accuracy: '\n...              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n...              f'\\nValid accuracy: '\n...              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n\n...     print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n\n... print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n... print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%') \n```", "```py\nEpoch: 0001/0003 | Batch 0000/2188 | Loss: 0.6771\nEpoch: 0001/0003 | Batch 0250/2188 | Loss: 0.3006\nEpoch: 0001/0003 | Batch 0500/2188 | Loss: 0.3678\nEpoch: 0001/0003 | Batch 0750/2188 | Loss: 0.1487\nEpoch: 0001/0003 | Batch 1000/2188 | Loss: 0.6674\nEpoch: 0001/0003 | Batch 1250/2188 | Loss: 0.3264\nEpoch: 0001/0003 | Batch 1500/2188 | Loss: 0.4358\nEpoch: 0001/0003 | Batch 1750/2188 | Loss: 0.2579\nEpoch: 0001/0003 | Batch 2000/2188 | Loss: 0.2474\nTraining accuracy: 96.32%\nValid accuracy: 92.34%\nTime elapsed: 20.67 min\nEpoch: 0002/0003 | Batch 0000/2188 | Loss: 0.0850\nEpoch: 0002/0003 | Batch 0250/2188 | Loss: 0.3433\nEpoch: 0002/0003 | Batch 0500/2188 | Loss: 0.0793\nEpoch: 0002/0003 | Batch 0750/2188 | Loss: 0.0061\nEpoch: 0002/0003 | Batch 1000/2188 | Loss: 0.1536\nEpoch: 0002/0003 | Batch 1250/2188 | Loss: 0.0816\nEpoch: 0002/0003 | Batch 1500/2188 | Loss: 0.0786\nEpoch: 0002/0003 | Batch 1750/2188 | Loss: 0.1395\nEpoch: 0002/0003 | Batch 2000/2188 | Loss: 0.0344\nTraining accuracy: 98.35%\nValid accuracy: 92.46%\nTime elapsed: 41.41 min\nEpoch: 0003/0003 | Batch 0000/2188 | Loss: 0.0403\nEpoch: 0003/0003 | Batch 0250/2188 | Loss: 0.0036\nEpoch: 0003/0003 | Batch 0500/2188 | Loss: 0.0156\nEpoch: 0003/0003 | Batch 0750/2188 | Loss: 0.0114\nEpoch: 0003/0003 | Batch 1000/2188 | Loss: 0.1227\nEpoch: 0003/0003 | Batch 1250/2188 | Loss: 0.0125\nEpoch: 0003/0003 | Batch 1500/2188 | Loss: 0.0074\nEpoch: 0003/0003 | Batch 1750/2188 | Loss: 0.0202\nEpoch: 0003/0003 | Batch 2000/2188 | Loss: 0.0746\nTraining accuracy: 99.08%\nValid accuracy: 91.84%\nTime elapsed: 62.15 min\nTotal Training Time: 62.15 min\nTest accuracy: 92.50% \n```", "```py\n>>> model = DistilBertForSequenceClassification.from_pretrained(\n...     'distilbert-base-uncased')\n>>> model.to(DEVICE)\n>>> model.train(); \n```", "```py\n>>> optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n>>> from transformers import Trainer, TrainingArguments\n>>> training_args = TrainingArguments(\n...     output_dir='./results', \n...     num_train_epochs=3,     \n...     per_device_train_batch_size=16, \n...     per_device_eval_batch_size=16,   \n...     logging_dir='./logs',\n...     logging_steps=10,\n... )\n>>> trainer = Trainer(\n...    model=model,\n...    args=training_args,\n...    train_dataset=train_dataset,\n...    optimizers=(optim, None) # optim and learning rate scheduler\n... ) \n```", "```py\n>>> from datasets import load_metric\n>>> import numpy as np\n>>> metric = load_metric(\"accuracy\")\n>>> def compute_metrics(eval_pred):\n...       logits, labels = eval_pred\n...       # note: logits are a numpy array, not a pytorch tensor\n...       predictions = np.argmax(logits, axis=-1)\n...       return metric.compute(\n...           predictions=predictions, references=labels) \n```", "```py\n>>> trainer=Trainer(\n...     model=model,        \n...     args=training_args,\n...     train_dataset=train_dataset,\n...     eval_dataset=test_dataset,\n...     compute_metrics=compute_metrics,\n...     optimizers=(optim, None) # optim and learning rate scheduler\n... ) \n```", "```py\n>>> start_time = time.time()\n>>> trainer.train()\n***** Running training *****\n  Num examples = 35000\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 6564\nStep  Training Loss\n10    0.705800\n20    0.684100\n30    0.681500\n40    0.591600\n50    0.328600\n60    0.478300\n...\n>>> print(f'Total Training Time: ' \n...       f'{(time.time() - start_time)/60:.2f} min')\nTotal Training Time: 45.36 min \n```", "```py\n>>> print(trainer.evaluate())\n***** Running Evaluation *****\nNum examples = 10000\nBatch size = 16\n100%|█████████████████████████████████████████| 625/625 [10:59<00:00,  1.06s/it]\n{'eval_loss': 0.30534815788269043,\n 'eval_accuracy': 0.9327,\n 'eval_runtime': 87.1161,\n 'eval_samples_per_second': 114.789,\n 'eval_steps_per_second': 7.174,\n 'epoch': 3.0} \n```", "```py\n>>> model.eval()\n>>> model.to(DEVICE)\n>>> print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\nTest accuracy: 93.27% \n```", "```py\n>>> from transformers import TrainingArguments\n>>> training_args = TrainingArguments(\"test_trainer\", \n...     evaluation_strategy=\"epoch\", ...) \n```", "```py\n>>> trainer=Trainer(\n...     model=model,        \n...     args=training_args,\n...     train_dataset=train_dataset,\n...     eval_dataset=valid_dataset,\n...     compute_metrics=compute_metrics,\n... ) \n```"]