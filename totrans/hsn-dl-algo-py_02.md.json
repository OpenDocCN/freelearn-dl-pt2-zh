["```py\ndef sigmoid(x):\n\n    return 1/ (1+np.exp(-x))\n```", "```py\ndef tanh(x):\n    numerator = 1-np.exp(-2*x)\n    denominator = 1+np.exp(-2*x)\n\n    return numerator/denominator\n```", "```py\ndef ReLU(x):\n    if x<0:\n        return 0\n    else:\n        return x\n```", "```py\ndef leakyReLU(x,alpha=0.01):\n    if x<0:\n        return (alpha*x)\n    else:\n        return x\n```", "```py\ndef ELU(x,alpha=0.01):\n    if x<0:\n        return ((alpha*(np.exp(x)-1))\n    else:\n        return x\n```", "```py\ndef swish(x,beta):\n    return 2*x*sigmoid(beta*x)\n```", "```py\ndef softmax(x):\n    return np.exp(x) / np.exp(x).sum(axis=0)\n```", "```py\ndef forward_prop(X):\n    z1 = np.dot(X,Wxh) + bh\n    a1 = sigmoid(z1)\n    z2 = np.dot(a1,Why) + by\n    y_hat = sigmoid(z2)\n\n    return y_hat\n```", "```py\ndelta2 = np.multiply(-(y-yHat),sigmoidPrime(z2))\n```", "```py\ndJ_dWhy = np.dot(a1.T,delta2)\n```", "```py\ndelta1 = np.dot(delta2,Why.T)*sigmoidPrime(z1)\n\ndJ_dWxh = np.dot(X.T,delta1)\n```", "```py\nWxh = Wxh - alpha * dJ_dWhy\nWhy = Why - alpha * dJ_dWxh \n```", "```py\ndef backword_prop(y_hat, z1, a1, z2):\n    delta2 = np.multiply(-(y-y_hat),sigmoid_derivative(z2))\n    dJ_dWhy = np.dot(a1.T, delta2)\n\n    delta1 = np.dot(delta2,Why.T)*sigmoid_derivative(z1)\n    dJ_dWxh = np.dot(X.T, delta1) \n\n    Wxh = Wxh - alpha * dJ_dWhy\n    Why = Why - alpha * dJ_dWxh\n\n    return Wxh,Why\n```", "```py\ndef f(x):\n    return x**2\n```", "```py\nepsilon = 1e-2\nx=3\n```", "```py\nanalytical_gradient = 2*x\n\nprint analytical_gradient\n\n6\n```", "```py\nnumerical_gradient = (f(x+epsilon) - f(x-epsilon)) / (2*epsilon)\n\nprint numerical_gradient\n\n6.000000000012662\n```", "```py\nweights_plus = weights + epsilon \nweights_minus = weights - epsilon \n```", "```py\nJ_plus = forward_prop(x, weights_plus) \nJ_minus = forward_prop(x, weights_minus) \n```", "```py\nnumerical_grad = (J_plus - J_minus) / (2 * epsilon) \n```", "```py\nanalytical_grad = backword_prop(x, weights)\n```", "```py\nnumerator = np.linalg.norm(analytical_grad - numerical_grad) \ndenominator = np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad) \nrelative_error = numerator / denominator \n```", "```py\nif relative_error < 1e-7:\n       print (\"The gradient is correct!\")\nelse:\n       print (\"The gradient is wrong!\")\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\nX = np.array([ [0, 1], [1, 0], [1, 1],[0, 0] ])\ny = p.array([ [1], [1], [0], [0]])\n```", "```py\nnum_input = 2\nnum_hidden = 5\nnum_output = 1\n```", "```py\nWxh = np.random.randn(num_input,num_hidden)\nbh = np.zeros((1,num_hidden))\n```", "```py\nWhy = np.random.randn (num_hidden,num_output)\nby = np.zeros((1,num_output))\n```", "```py\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n```", "```py\ndef sigmoid_derivative(z):\n     return np.exp(-z)/((1+np.exp(-z))**2)\n```", "```py\ndef forward_prop(X,Wxh,Why):\n    z1 = np.dot(X,Wxh) + bh\n    a1 = sigmoid(z1)\n    z2 = np.dot(a1,Why) + by\n    y_hat = sigmoid(z2)\n\n    return z1,a1,z2,y_hat\n```", "```py\ndef backword_prop(y_hat, z1, a1, z2):\n    delta2 = np.multiply(-(y-y_hat),sigmoid_derivative(z2))\n    dJ_dWhy = np.dot(a1.T, delta2)\n    delta1 = np.dot(delta2,Why.T)*sigmoid_derivative(z1)\n    dJ_dWxh = np.dot(X.T, delta1) \n\n    return dJ_dWxh, dJ_dWhy\n```", "```py\ndef cost_function(y, y_hat):\n    J = 0.5*sum((y-y_hat)**2)\n\n    return J\n```", "```py\nalpha = 0.01\nnum_iterations = 5000\n```", "```py\ncost =[]\n\nfor i in range(num_iterations):\n    z1,a1,z2,y_hat = forward_prop(X,Wxh,Why)    \n    dJ_dWxh, dJ_dWhy = backword_prop(y_hat, z1, a1, z2)\n\n    #update weights\n    Wxh = Wxh -alpha * dJ_dWxh\n    Why = Why -alpha * dJ_dWhy\n\n    #compute cost\n    c = cost_function(y, y_hat)\n\n    cost.append(c)\n```", "```py\nplt.grid()\nplt.plot(range(num_iteratins),cost)\n\nplt.title('Cost Function')\nplt.xlabel('Training Iterations')\nplt.ylabel('Cost')\n```"]