["```py\npython\n```", "```py\nconda install pytorch torchvision -c pytorch\n```", "```py\n>>> import torch\n>>> x = torch.empty(3, 4)\n>>> print(x)\ntensor([[ 0.0000e+00,  2.0000e+00, -1.2750e+16, -2.0005e+00],\n [ 9.8742e-37,  1.4013e-45, 9.9222e-37,  1.4013e-45],\n [ 9.9220e-37,  1.4013e-45, 9.9225e-37,  2.7551e-40]])\n```", "```py\nconda install pip\n```", "```py\npip install gym\n```", "```py\nconda install pip\n```", "```py\ngit clone https://github.com/openai/gym\n```", "```py\ncd gym\npip install -e .\n```", "```py\n>>> from gym import envs\n>>> print(envs.registry.all())\ndict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0)\n...\n...\n```", "```py\npip install gym[atari]\n```", "```py\npip install -e '.[atari]'\n```", "```py\n>>> import gym\n```", "```py\n>>> env = gym.make('SpaceInvaders-v0')\n```", "```py\n>>> env.reset()\n array([[[ 0,  0, 0],\n         [ 0, 0,  0],\n         [ 0, 0,  0],\n         ...,\n         ...,\n         [80, 89, 22],\n         [80, 89, 22],\n         [80, 89, 22]]], dtype=uint8)\n```", "```py\n>>> env.render()\nTrue\n```", "```py\n>>> action = env.action_space.sample()\n>>> new_state, reward, is_done, info = env.step(action)\n```", "```py\n>>> print(is_done)\nFalse\n>>> print(info)\n{'ale.lives': 3}\n```", "```py\n>>> env.render()\n True\n```", "```py\n>>> is_done = False\n>>> while not is_done:\n...     action = env.action_space.sample()\n...     new_state, reward, is_done, info = env.step(action)\n...     print(info)\n...     env.render()\n{'ale.lives': 3}\nTrue\n{'ale.lives': 3}\nTrue\n……\n……\n{'ale.lives': 2}\nTrue\n{'ale.lives': 2}\nTrue\n……\n……\n{'ale.lives': 1}\nTrue\n{'ale.lives': 1}\nTrue\n```", "```py\n>>> print(info)\n{'ale.lives': 0}\n```", "```py\n>>> env.action_space.sample()\n0\n>>> env.action_space.sample()\n3\n>>> env.action_space.sample()\n0\n>>> env.action_space.sample()\n4\n>>> env.action_space.sample()\n2\n>>> env.action_space.sample()\n1\n>>> env.action_space.sample()\n4\n>>> env.action_space.sample()\n5\n>>> env.action_space.sample()\n1\n>>> env.action_space.sample()\n0\n```", "```py\n>>> env.action_space\nDiscrete(6)\n```", "```py\n>>> print(new_state.shape)\n(210, 160, 3)\n```", "```py\npip install gym[box2d]\npip install -e '.[box2d]'\n```", "```py\n>>> env = gym.make('LunarLander-v2')\n>>> env.reset()\narray([-5.0468446e-04,  1.4135642e+00, -5.1140346e-02,  1.1751971e-01,\n 5.9164839e-04,  1.1584054e-02, 0.0000000e+00,  0.0000000e+00],\n dtype=float32)\n>>> env.render()\n```", "```py\n >>> import gym >>> env = gym.make('CartPole-v0')\n```", "```py\n >>> env.reset() array([-0.00153354,  0.01961605, -0.03912845, -0.01850426])\n```", "```py\n >>> env.render() True\n```", "```py\n >>> is_done = False >>> while not is_done:\n ...     action = env.action_space.sample()\n ...     new_state, reward, is_done, info = env.step(action)\n ...     print(new_state)\n ...     env.render()\n ...\n [-0.00114122 -0.17492355 -0.03949854  0.26158095]\n True\n [-0.00463969 -0.36946006 -0.03426692  0.54154857]\n True\n ……\n ……\n [-0.11973207 -0.41075106  0.19355244 1.11780626]\n True\n [-0.12794709 -0.21862176  0.21590856 0.89154351]\n True\n```", "```py\nbrew install ffmpeg\n```", "```py\nsudo apt-get install ffmpeg\n```", "```py\n>>> video_dir = './cartpole_video/' >>> env = gym.wrappers.Monitor(env, video_dir)\n```", "```py\n >>> n_episode = 10000\n```", "```py\n >>> total_rewards = [] >>> for episode in range(n_episode):\n ...     state = env.reset()\n ...     total_reward = 0\n ...     is_done = False\n ...     while not is_done:\n ...         action = env.action_space.sample()\n ...         state, reward, is_done, _ = env.step(action)\n ...         total_reward += reward\n ...     total_rewards.append(total_reward)\n```", "```py\n >>> print('Average total reward over {} episodes: {}'.format( n_episode, sum(total_rewards) / n_episode))\n Average total reward over 10000 episodes: 22.2473\n```", "```py\n >>> import torch >>> x = torch.rand(3, 4)\n >>> print(x)\n tensor([[0.8052, 0.3370, 0.7676, 0.2442],\n        [0.7073, 0.4468, 0.1277, 0.6842],\n        [0.6688, 0.2107, 0.0527, 0.4391]])\n```", "```py\n >>> x = torch.rand(3, 4, dtype=torch.double) >>> print(x)\n tensor([[0.6848, 0.3155, 0.8413, 0.5387],\n        [0.9517, 0.1657, 0.6056, 0.5794],\n        [0.0351, 0.3801, 0.7837, 0.4883]], dtype=torch.float64)\n```", "```py\n >>> x = torch.zeros(3, 4) >>> print(x)\n    tensor([[0., 0., 0., 0.],\n           [0., 0., 0., 0.],\n           [0., 0., 0., 0.]])\n    >>> x = torch.ones(3, 4)\n    >>> print(x)\n    tensor([[1., 1., 1., 1.],\n           [1., 1., 1., 1.],\n           [1., 1., 1., 1.]])\n```", "```py\n >>> print(x.size()) torch.Size([3, 4])\n```", "```py\n >>> x_reshaped = x.view(2, 6) >>> print(x_reshaped)\n tensor([[1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1.]])\n```", "```py\n >>> x1 = torch.tensor(3) >>> print(x1)\n tensor(3)\n >>> x2 = torch.tensor([14.2, 3, 4])\n >>> print(x2)\n tensor([14.2000,  3.0000, 4.0000])\n >>> x3 = torch.tensor([[3, 4, 6], [2, 1.0, 5]])\n >>> print(x3)\n tensor([[3., 4., 6.],\n         [2., 1., 5.]])\n```", "```py\n >>> print(x2[1]) tensor(3.)\n >>> print(x3[1, 0])\n tensor(2.)\n >>> print(x3[:, 1])\n tensor([4., 1.])\n >>> print(x3[:, 1:])\n tensor([[4., 6.],\n         [1., 5.]])\n```", "```py\n >>> print(x1.item()) 3\n```", "```py\n >>> x3.numpy() array([[3., 4., 6.],\n        [2., 1., 5.]], dtype=float32)\n```", "```py\n>>> import numpy as np >>> x_np = np.ones(3)\n>>> x_torch = torch.from_numpy(x_np)\n>>> print(x_torch)\ntensor([1., 1., 1.], dtype=torch.float64)\n```", "```py\n >>> print(x_torch.float())\n tensor([1., 1., 1.])\n```", "```py\n>>> x4 = torch.tensor([[1, 0, 0], [0, 1.0, 0]]) >>> print(x3 + x4)\ntensor([[4., 4., 6.],\n         [2., 2., 5.]])\n```", "```py\n >>> print(torch.add(x3, x4)) tensor([[4., 4., 6.],\n         [2., 2., 5.]])\n```", "```py\n >>> x3.add_(x4) tensor([[4., 4., 6.],\n         [2., 2., 5.]])\n```", "```py\n >>> print(x3) tensor([[4., 4., 6.],\n         [2., 2., 5.]])\n```", "```py\n>>> import gym >>> import torch\n>>> env = gym.make('CartPole-v0')\n```", "```py\n>>> n_state = env.observation_space.shape[0] >>> n_state\n 4\n>>> n_action = env.action_space.n\n>>> n_action\n 2\n```", "```py\n >>> def run_episode(env, weight): ...     state = env.reset()\n ...     total_reward = 0\n ...     is_done = False\n ...     while not is_done:\n ...         state = torch.from_numpy(state).float()\n ...         action = torch.argmax(torch.matmul(state, weight))\n ...         state, reward, is_done, _ = env.step(action.item())\n ...         total_reward += reward\n ...     return total_reward\n```", "```py\n>>> n_episode = 1000\n```", "```py\n>>> best_total_reward = 0 >>> best_weight = None\n```", "```py\n>>> total_rewards = []\n```", "```py\n >>> for episode in range(n_episode): ...     weight = torch.rand(n_state, n_action)\n ...     total_reward = run_episode(env, weight)\n ...     print('Episode {}: {}'.format(episode+1, total_reward))\n ...     if total_reward > best_total_reward:\n ...         best_weight = weight\n ...         best_total_reward =  total_reward\n ...     total_rewards.append(total_reward)\n ...\n Episode 1: 10.0\n Episode 2: 73.0\n Episode 3: 86.0\n Episode 4: 10.0\n Episode 5: 11.0\n ……\n ……\n Episode 996: 200.0\n Episode 997: 11.0\n Episode 998: 200.0\n Episode 999: 200.0\n Episode 1000: 9.0\n```", "```py\n >>> print('Average total reward over {} episode: {}'.format( n_episode, sum(total_rewards) / n_episode))\n Average total reward over 1000 episode: 47.197\n```", "```py\n >>> n_episode_eval = 100 >>> total_rewards_eval = []\n >>> for episode in range(n_episode_eval):\n ...     total_reward = run_episode(env, best_weight)\n ...     print('Episode {}: {}'.format(episode+1, total_reward))\n ...     total_rewards_eval.append(total_reward)\n ...\n Episode 1: 200.0\n Episode 2: 200.0\n Episode 3: 200.0\n Episode 4: 200.0\n Episode 5: 200.0\n ……\n ……\n Episode 96: 200.0\n Episode 97: 188.0\n Episode 98: 200.0\n Episode 99: 200.0\n Episode 100: 200.0\n >>> print('Average total reward over {} episode: {}'.format(\n           n_episode, sum(total_rewards_eval) / n_episode_eval))\n Average total reward over 1000 episode: 196.72\n```", "```py\n>>> import matplotlib.pyplot as plt >>> plt.plot(total_rewards)\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Reward')\n>>> plt.show()\n```", "```py\nconda install matplotlib\n```", "```py\n >>> n_episode = 1000 >>> best_total_reward = 0\n >>> best_weight = None\n >>> total_rewards = []\n >>> for episode in range(n_episode):\n ...     weight = torch.rand(n_state, n_action)\n ...     total_reward = run_episode(env, weight)\n ...     print('Episode {}: {}'.format(episode+1, total_reward))\n ...     if total_reward > best_total_reward:\n ...         best_weight = weight\n ...         best_total_reward = total_reward\n ...     total_rewards.append(total_reward)\n ...     if best_total_reward == 200:\n ...         break\n Episode 1: 9.0\n Episode 2: 8.0\n Episode 3: 10.0\n Episode 4: 10.0\n Episode 5: 10.0\n Episode 6: 9.0\n Episode 7: 17.0\n Episode 8: 10.0\n Episode 9: 43.0\n Episode 10: 10.0\n Episode 11: 10.0\n Episode 12: 106.0\n Episode 13: 8.0\n Episode 14: 32.0\n Episode 15: 98.0\n Episode 16: 10.0\n Episode 17: 200.0\n```", "```py\n >>> n_training = 1000 >>> n_episode_training = []\n >>> for _ in range(n_training):\n ...     for episode in range(n_episode):\n ...         weight = torch.rand(n_state, n_action)\n ...         total_reward = run_episode(env, weight)\n ...         if total_reward == 200:\n ...             n_episode_training.append(episode+1)\n ...             break\n >>> print('Expectation of training episodes needed: ',\n            sum(n_episode_training) / n_training)\n Expectation of training episodes needed:  13.442\n```", "```py\n>>> import gym >>> import torch\n>>> env = gym.make('CartPole-v0')\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n```", "```py\n>>> n_episode = 1000\n```", "```py\n>>> best_total_reward = 0 >>> best_weight = torch.rand(n_state, n_action)\n```", "```py\n>>> total_rewards = []\n```", "```py\n>>> noise_scale = 0.01\n```", "```py\n >>> for episode in range(n_episode): ...     weight = best_weight +\n                     noise_scale * torch.rand(n_state, n_action)\n ...     total_reward = run_episode(env, weight)\n ...     if total_reward >= best_total_reward:\n ...         best_total_reward = total_reward\n ...         best_weight = weight\n ...     total_rewards.append(total_reward)\n ...     print('Episode {}: {}'.format(episode + 1, total_reward))\n ...\n Episode 1: 56.0\n Episode 2: 52.0\n Episode 3: 85.0\n Episode 4: 106.0\n Episode 5: 41.0\n ……\n ……\n Episode 996: 39.0\n Episode 997: 51.0\n Episode 998: 49.0\n Episode 999: 54.0\n Episode 1000: 41.0\n```", "```py\n >>> print('Average total reward over {} episode: {}'.format( n_episode, sum(total_rewards) / n_episode))\n Average total reward over 1000 episode: 50.024\n```", "```py\nAverage total reward over 1000 episode: 9.261   \nAverage total reward over 1000 episode: 88.565\nAverage total reward over 1000 episode: 51.796\nAverage total reward over 1000 episode: 9.41\nAverage total reward over 1000 episode: 109.758\nAverage total reward over 1000 episode: 55.787\nAverage total reward over 1000 episode: 189.251\nAverage total reward over 1000 episode: 177.624\nAverage total reward over 1000 episode: 9.146\nAverage total reward over 1000 episode: 102.311\n```", "```py\n >>> noise_scale = 0.01 >>> best_total_reward = 0\n >>> total_rewards = []\n >>> for episode in range(n_episode):\n ...     weight = best_weight +\n                       noise_scale * torch.rand(n_state, n_action)\n ...     total_reward = run_episode(env, weight)\n ...     if total_reward >= best_total_reward:\n ...         best_total_reward = total_reward\n ...         best_weight = weight\n ...         noise_scale = max(noise_scale / 2, 1e-4)\n ...     else:\n ...         noise_scale = min(noise_scale * 2, 2)\n ...     print('Episode {}: {}'.format(episode + 1, total_reward))\n ...     total_rewards.append(total_reward)\n ...\n Episode 1: 9.0\n Episode 2: 9.0\n Episode 3: 9.0\n Episode 4: 10.0\n Episode 5: 10.0\n ……\n ……\n Episode 996: 200.0\n Episode 997: 200.0\n Episode 998: 200.0\n Episode 999: 200.0\n Episode 1000: 200.0\n```", "```py\n >>> print('Average total reward over {} episode: {}'.format( n_episode, sum(total_rewards) / n_episode))\n Average total reward over 1000 episode: 186.11\n```", "```py\n>>> import matplotlib.pyplot as plt >>> plt.plot(total_rewards)\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Reward')\n>>> plt.show()\n```", "```py\n >>> n_episode_eval = 100 >>> total_rewards_eval = []\n >>> for episode in range(n_episode_eval):\n ...     total_reward = run_episode(env, best_weight)\n ...     print('Episode {}: {}'.format(episode+1, total_reward))\n ...     total_rewards_eval.append(total_reward)\n ...\n Episode 1: 200.0\n Episode 2: 200.0\n Episode 3: 200.0\n Episode 4: 200.0\n Episode 5: 200.0\n ……\n ……\n Episode 96: 200.0\n Episode 97: 200.0\n Episode 98: 200.0\n Episode 99: 200.0\n Episode 100: 200.0 \n```", "```py\n>>> print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode)) Average total reward over 1000 episode: 199.94\n```", "```py\n >>> noise_scale = 0.01 >>> best_total_reward = 0\n >>> total_rewards = []\n >>> for episode in range(n_episode):\n ...     weight = best_weight + noise_scale * torch.rand(n_state, n_action)\n ...     total_reward = run_episode(env, weight)\n ...     if total_reward >= best_total_reward:\n ...         best_total_reward = total_reward\n ...         best_weight = weight\n ...         noise_scale = max(noise_scale / 2, 1e-4)\n ...     else:\n ...         noise_scale = min(noise_scale * 2, 2)\n ...     print('Episode {}: {}'.format(episode + 1, total_reward))\n ...     total_rewards.append(total_reward)\n ...     if episode >= 99 and sum(total_rewards[-100:]) >= 19500:\n ...         break\n ...\n Episode 1: 9.0\n Episode 2: 9.0\n Episode 3: 10.0\n Episode 4: 10.0\n Episode 5: 9.0\n ……\n ……\n Episode 133: 200.0\n Episode 134: 200.0\n Episode 135: 200.0\n Episode 136: 200.0\n Episode 137: 200.0\n```", "```py\n>>> import gym >>> import torch\n>>> env = gym.make('CartPole-v0')\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n```", "```py\n >>> def run_episode(env, weight): ...     state = env.reset()\n ...     grads = []\n ...     total_reward = 0\n ...     is_done = False\n ...     while not is_done:\n ...         state = torch.from_numpy(state).float()\n ...         z = torch.matmul(state, weight)\n ...         probs = torch.nn.Softmax()(z)\n ...         action = int(torch.bernoulli(probs[1]).item())\n ...         d_softmax = torch.diag(probs) -\n                             probs.view(-1, 1) * probs\n ...         d_log = d_softmax[action] / probs[action]\n ...         grad = state.view(-1, 1) * d_log\n ...         grads.append(grad)\n ...         state, reward, is_done, _ = env.step(action)\n ...         total_reward += reward\n ...         if is_done:\n ...             break\n ...     return total_reward, grads\n```", "```py\n>>> n_episode = 1000\n```", "```py\n>>> weight = torch.rand(n_state, n_action)\n```", "```py\n>>> total_rewards = []\n```", "```py\n>>> learning_rate = 0.001\n```", "```py\n >>> for episode in range(n_episode): ...     total_reward, gradients = run_episode(env, weight)\n ...     print('Episode {}: {}'.format(episode + 1, total_reward))\n ...     for i, gradient in enumerate(gradients):\n ...         weight += learning_rate * gradient * (total_reward - i)\n ...     total_rewards.append(total_reward)\n ……\n ……\n Episode 101: 200.0\n Episode 102: 200.0\n Episode 103: 200.0\n Episode 104: 190.0\n Episode 105: 133.0\n ……\n ……\n Episode 996: 200.0\n Episode 997: 200.0\n Episode 998: 200.0\n Episode 999: 200.0\n Episode 1000: 200.0\n```", "```py\n >>> print('Average total reward over {} episode: {}'.format( n_episode, sum(total_rewards) / n_episode))\n Average total reward over 1000 episode: 179.728\n```", "```py\n >>> import matplotlib.pyplot as plt >>> plt.plot(total_rewards)\n >>> plt.xlabel('Episode')\n >>> plt.ylabel('Reward')\n >>> plt.show()\n```", "```py\n >>> n_episode_eval = 100 >>> total_rewards_eval = []\n >>> for episode in range(n_episode_eval):\n ...     total_reward, _ = run_episode(env, weight)\n ...     print('Episode {}: {}'.format(episode+1, total_reward))\n ...     total_rewards_eval.append(total_reward)\n ...\n Episode 1: 200.0\n Episode 2: 200.0\n Episode 3: 200.0\n Episode 4: 200.0\n Episode 5: 200.0\n ……\n ……\n Episode 96: 200.0\n Episode 97: 200.0\n Episode 98: 200.0\n Episode 99: 200.0\n Episode 100: 200.0\n```", "```py\n>>> print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode)) Average total reward over 1000 episode: 199.78\n```", "```py\n >>> if episode >= 99 and sum(total_rewards[-100:]) >= 19500: ...     break\n```", "```py\nEpisode 1: 10.0 Episode 2: 27.0\nEpisode 3: 28.0\nEpisode 4: 15.0\nEpisode 5: 12.0\n……\n……\nEpisode 549: 200.0\nEpisode 550: 200.0\nEpisode 551: 200.0\nEpisode 552: 200.0\nEpisode 553: 200.0\n```"]