- en: Chapter 7. Deep Learning for Board Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。棋盘游戏的深度学习
- en: You may have read sci-fi novels from the 50's and 60's; they are full of visions
    of what life in the 21st century would look like. They imagined a world of people
    with personal jet packs, underwater cities, intergalactic travel, flying cars,
    and truly intelligent robots capable of independent thought. The 21st century
    has arrived now; sadly, we are not going to get those flying cars, but thanks
    to deep learning, we may get that robot.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你读过五六十年代的科幻小说；它们充满了对21世纪生活会是什么样子的设想。他们想象了一个人们拥有个人喷气背包、水下城市、星际旅行、飞行汽车和真正有独立思考能力的机器人的世界。21世纪现在已经到来了；可悲的是，我们不会得到那些飞行汽车，但由于深度学习，我们可能会得到那些机器人。
- en: What does this have to do with deep learning for board games? In the next two
    chapters, including the current one, we will look at how to build **Artificial
    Intelligence** (**AI**) that can learn game environments. Reality has a vast space
    of possibilities. Doing even simple human tasks, such as getting a robot arm to
    pick up objects, requires analyzing huge amounts of sensory data and controlling
    many continuous response variables for the movement of the arms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这与棋盘游戏的深度学习有什么关系？在接下来的两个章节，包括当前章节，我们将看看如何构建**人工智能**（**AI**），以学习游戏环境。现实具有广阔的可能性空间。即使是进行简单的人类任务，如让机器人手臂抓取物体，也需要分析大量的感官数据并控制许多用于移动手臂的连续响应变量。
- en: Games act as a great playing field for testing general purpose learning algorithms.
    They give you an environment of large, but manageable possibilities. Also, when
    it comes to computer games, we know that humans can learn to play a game just
    from the pixels visible on the screen and the most minor of instructions. If we
    input the same pixels plus an objective into a computer agent, we know we have
    a solvable problem, given the right algorithm. In fact, for the computer, the
    problem is easier because a human being identifies that the things they seeing
    in their field of vision are actually game pixels, as opposed to the area around
    the screen. This is why so many researchers are looking at games as a great place
    to start developing true AI's—self-learning machines that can operate independently
    from us. Also, if you like games, it's lots of fun.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏作为测试通用学习算法的绝佳场所。它们给你一个庞大但可控制的可能性环境。此外，说到电脑游戏，我们知道人类可以仅通过屏幕上可见的像素和最微小的指示就学会玩游戏。如果我们将相同的像素以及一个目标输入到计算机代理中，我们知道我们有一个可解决的问题，只要使用正确的算法。实际上，对于电脑来说，问题更容易，因为人类可以识别出他们在视野中看到的东西实际上是游戏像素，而不是屏幕周围的区域。这就是为什么如此多的研究人员将游戏视为开发真正的人工智能的绝佳起点——能够独立于我们运行的自学习机器。此外，如果你喜欢游戏，它会非常有趣。
- en: In this chapter, we will cover the different tools used for solving board games,
    such as checkers and chess. Eventually, we'll build up enough knowledge to be
    able to understand and implement the kind of deep learning solution that was used
    to build AlphaGo, the AI that defeated the greatest human Go player. We'll use
    a variety of deep learning techniques to accomplish this. The next chapter will
    build on this knowledge and cover how deep learning can be used to learn how to
    play computer games, such as Pong and Breakout.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍解决棋盘游戏（如跳棋和国际象棋）的不同工具。最终，我们将积累足够的知识，以便理解并实现构建AlphaGo的深度学习解决方案，该解决方案击败了最伟大的人类围棋选手。我们将使用各种深度学习技术来实现这一点。接下来的章节将在此基础知识之上构建，并介绍如何使用深度学习来学习如何玩计算机游戏，如乒乓球和打砖块。
- en: 'The full list of concepts that we will cover across both the chapters is as
    follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个章节中涵盖的概念列表如下：
- en: The min-max algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极小极大算法
- en: Monte-Carlo Tree Search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索
- en: Reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Policy gradients
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度
- en: Q-learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: Actor-Critic
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论家
- en: Model-based approaches
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的方法
- en: We will use a few different terms to describe tasks and their solutions. The
    following are some of the definitions. They all use the example of a basic maze
    game as it is a good, simple example of a reinforcement learning environment.
    In a maze game, there are a set of locations with paths between them. There is
    an agent in this maze that can use the paths to move between the different locations.
    Some locations have a reward associated with them. The agent's objective is to
    navigate their way through the maze to get the best possible reward.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Learning for Board Games](img/00257.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent** is the entity for which we are trying to learn actions. In the game,
    this is the player that will try to find its way through the maze.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment** is a world/level/game in which the agent operates, that is,
    the maze itself.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward** is the feedback that the agent gets within the environment. In the
    case of this example maze game, it might be the exit square or the carrots in
    the image that the agent is trying to collect. Some mazes may also have traps
    that give a negative reward, which the agent should try to avoid.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State** refers to all of the information available to the agent about its
    current environment. In a maze, the state is simply the agent''s position.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action** is a possible response, or set of responses, that an agent can make.
    In a maze, this is a potential path that an agent can take from one state to another.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control policy** determines what actions the agent will take. In the context
    of deep learning, this is the neural network that we will train. Other policies
    might be selecting actions at random or selecting actions based on the code that
    the programmer has written.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of this chapter is code-heavy, so as an alternative to copying all the
    samples from the book, you can find the full code in a GitHub repository at [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples).
    All the examples in the chapters are presented using TensorFlow, but the concepts
    could be translated into other deep learning frameworks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Early game playing AI
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building AI''s to play games started in the 50''s with researchers building
    programs that played checkers and chess. These two games have a few properties
    in common:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: They are zero-sum games. Any reward that one player receives is a corresponding
    loss to the other player and vice versa. When one player wins, the other loses.
    There is no possibility of cooperation. For example, consider a game such as the
    prisoner's dilemma; here, the two players can agree to cooperate and both receive
    a smaller reward.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are both games of perfect information. The entire state of the game is
    always known to both the players unlike a game such as poker, where the exact
    cards that your opponents are holding is unknown. This fact reduces the complexity
    that the AI must handle. It also means that a decision about what the best move
    can be made is based on just the current state. In poker, the hypothetical optimal
    decision about how to play would require information that is not just on your
    current hand and how much money is available to each player, but also about the
    playing styles of the opponents and what they had bid in the previous positions
    they were in.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both games are deterministic. If a given move is made by either player, then
    that will result in an exact next state. In some games, the play may be based
    on a dice roll or random drawing of a card from a deck; in these cases, there
    would be many possible next states to consider.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of perfect information and determinism in chess and checkers
    means that given the current state, we can exactly know what state we will be
    in if the current player takes an action. This property also chains if we have
    a state, then takes an action leading to a new state. We can again take an action
    in this new state to keep playing as far into the future as we want.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: To experiment with some of the approaches of mastering board games, we will
    give examples using a Python implementation of the game called *Tic-Tac-Toe*.
    Also known as *noughts and crosses,* this is a simple game where players take
    turns making marks on a 3 by 3 grid. The first player to get three marks in a
    row wins. *Tic-Tac-Toe* is another deterministic, zero sum, perfect information
    game and is chosen here because a Python implementation of it is a lot simpler
    than chess. In fact, the whole game can be done in less than a page of code, which
    will be shown later in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Using the min-max algorithm to value game states
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Say we want to work out the best move in a zero sum, deterministic, perfect
    information game. How can we do this? Well, first off, given that we have perfect
    information, we know exactly what moves are available to us. Given that the game
    is deterministic, we know exactly what state the game will change to due to each
    of those moves. The same is then true for the opponent's move as well; we know
    exactly what possible moves they have and how the state would look as a result
    of each of those moves.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach for finding the best move would be to construct a full tree of
    every possible move for each player at each state until we reach a state where
    the game is over. This end state of the game is also known as the terminal state.
    We can assign a value to this terminal state; a win could carry the value 1, a
    draw 0, and a loss -1\. These values reflect the states'' desirability to us.
    We would prefer a win than a draw and a draw to a loss. *Figure 2* shows an example
    of this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the min-max algorithm to value game states](img/00258.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Tree of all the states of tic-tac-toe'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In a terminal state, we can go back to the state where the player chose the
    move that led to the terminal state. That player, whose objective is to find the
    best possible move, can determine exactly what value they will get from the actions
    they would take, which is the terminal state that they eventually led the game
    to. They will obviously want to select the move that would lead to the best possible
    value for themselves. If they have a choice of actions that would either lead
    to winning the terminal state or losing it, they will select the one that leads
    to a winning state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The value of the state where terminal states are selected can then be marked
    with the value of the best possible action that the player could make. This gives
    us the value to that player of being in this state. But we are playing a two-player
    game here, so if we go back a state, we would be in a state where the other player
    is due to make a move. We now on our graph have the value that this opponent will
    get from their actions in this state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: This being a zero sum game, we want our opponent to do as badly as possible,
    so we will select the move that leads to the lowest value state for them. If we
    keep going back through the graph of states, marking all the states with the value
    of the best state that any action could lead to, we can determine exactly what
    is the best action in the current state.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the min-max algorithm to value game states](img/00259.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Min-max algorithm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: In this way, a complete tree of the game can be constructed, showing us the
    best move that we can make in the current state. This approach is called the min-max
    algorithm and is what the early researchers used for their chess and checkers
    games.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Though this approach tells us the exact best move for any zero sum, deterministic,
    perfect information game, it unfortunately has a major problem. Chess has on average
    about 30 possible moves per turn and games last on average 40 turns. So to build
    a graph from the first state in chess to all the terminal states would require
    approximately 30^(40) states. Many orders of magnitude larger than this is possible
    on the world's best hardware. When talking about games, the number of moves a
    player can take per turn is referred to as the **breadth** and the number of moves
    the game takes per turn as the **depth**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: To make chess tractable with the Min-Max algorithm, we need to massively reduce
    the depth of our search. Rather than calculate the whole tree through to the end
    of the game, we can construct our tree down to a fixed depth, say six moves on
    from the current state. At each leaf that is not an actual terminal state, we
    can use an evaluation function to estimate how likely the player is to win in
    that state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: For chess, a good evaluation function is to do a weighted count of the number
    of pieces available with each player. So, one point for a pawn, three for a bishop
    or knight, five for a rook, and eight for a queen. If I have three pawns and a
    knight, I have six points; similarly, if you have two pawns and a rook, you have
    seven points. Therefore, you are one point ahead. A player with more pieces left
    generally tends to win in chess. However, as any keen chess player who has played
    against a good exchange sacrifice will know, this evaluation function has its
    limits.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于国际象棋，一个良好的评估函数是对每个玩家可用的棋子数量进行加权计数。因此，兵的得分为1分，主教或骑士的为3分，车的为5分，后的为8分。如果我有三个兵和一个骑士，我得到六分；同样地，如果你有两个兵和一个车，你有七分。因此，你领先一分。通常情况下，在国际象棋中，剩下的棋子更多的玩家往往会取胜。然而，任何曾经与好的交换牺牲对手交战的国际象棋玩家都会知道，这个评估函数是有局限性的。
- en: Implementing a Python Tic-Tac-Toe game
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Python版的Tic-Tac-Toe游戏
- en: Let's build a basic implementation of *Tic-Tac-Toe* so we can see what an implementation
    of the min-max algorithm looks like. If you do not feel like copying all of this,
    you can find the full code in the GitHub repository [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)
    in the `tic_tac_toe.py` file.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们构建一个基本的*Tic-Tac-Toe*实现，这样我们就可以看到min-max算法的实现是什么样子的。如果你不想复制所有这些，你可以在GitHub仓库[https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)的`tic_tac_toe.py`文件中找到完整的代码。 '
- en: 'In the game board, we will be represented by a 3 x 3 tuple of integers. Tuples
    are used instead of lists so that later on, we can get equality between matching
    board states. In this case, **0** represents a square that has not been played
    in. The two players will be marked **1** and **-1**. If player one makes a move
    in a square, that square will be marked with their number. So here we go:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏棋盘中，我们将用一个3 x 3的整数元组表示。使用元组而不是列表，以便以后能够在匹配的棋盘状态之间得到相等。在这种情况下，**0**表示一个未被玩过的方格。两名玩家将分别用**1**和**-1**表示。如果玩家一在一个方格上下了一步，那么该方格将被标记为他们的数字。所以让我们开始：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `new_board` method will be called before the play for a fresh board, ready
    for the players to make their moves on:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在玩家进行下一步前，将会调用`new_board`方法，准备好一个新的棋盘：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `apply_move` method takes one of the 3 x 3 tuples for `board_state` and
    returns a new `board_state` with the move by the given side applied. A move will
    be a tuple of length 2, containing the coordinate of the space that we want to
    move to as two integers. Side will an integer representing the player who is playing
    the move, either 1 or -1:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_move`方法接受`board_state`的3 x 3元组之一，并返回应用了给定方向移动的新的`board_state`。移动将是一个包含两个整数坐标的长度为2的元组。方向将是代表玩家的整数，要么是1，要么是-1：'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This method gives us the list of legal moves for a given 3 x 3 `board_state`,
    which is simply all the non-zero squares. Now we just need a method to determine
    whether a player has the three winning marks in a row:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法为给定的3 x 3 `board_state`列出了合法的移动列表，它就是所有非零方格。现在我们只需要一个方法来确定玩家是否已经连成了三个获胜的标记：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `has_3_in_a_line` takes a sequence of three squares from the board. If
    all are either 1 or -1, it means one of the players has gotten three in a row
    and has won. We then need to run this method against each possible line on the
    Tic-Tac-Toe board to determine whether a player has won:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`has_3_in_a_line`将获取棋盘上的三个方格的序列。如果所有的方格都是1或-1，这意味着其中一名玩家连成了三个获胜的标记，赢得了胜利。然后，我们需要对Tic-Tac-Toe棋盘上的每条可能的线运行这个方法，以确定玩家是否已经获胜：'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With just these few functions, you can now play a game of *Tic-Tac-Toe*. Simply
    start by getting a new board, then have the players successively choose moves
    and apply those moves to `board_state`. If we find that there are no available
    moves left, the game is a draw. Otherwise, if `has_winner` returns either `1`
    or `-1`, it means one of the players has won. Let''s write a simple function for
    running a Tic-Tac-Toe game with the moves decided by methods that we pass in,
    which will be the control policies of the different AI players that we will try
    out:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 只需这几个功能，你就可以玩一局*Tic-Tac-Toe*游戏。简单地开始，获取一个新的棋盘，然后让玩家依次选择移动并将这些移动应用到`board_state`上。如果我们发现没有剩余可用的移动，游戏就是平局。否则，如果`has_winner`返回`1`或`-1`，这意味着其中一名玩家获胜。接下来，让我们编写一个简单的函数来运行一个Tic-Tac-Toe游戏，其中的移动由我们传递的方法来决定，这些方法将会是我们将尝试的不同AI玩家的控制策略：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We declare the method and take it to the function that will choose the action
    for each player. Each `player_func` will take two arguments: the first being the
    current `board_state` and the second being the side that the player is playing,
    1 or -1\. The `player_turn` variable will keep track of this for us:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们宣告这个方法，并将其带到将为每个玩家选择动作的函数中。每个`player_func`将会有两个参数：第一个是当前的`board_state`，第二个是玩家所执的一方，1或-1。`player_turn`变量将为我们跟踪这一切：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is the main loop of the game. First we have to check whether there are
    any available moves left on `board_state`; if there are, the game is not over
    and it is a draw:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是游戏的主要循环。首先，我们要检查`board_state`上是否还有可用的走法；如果有，游戏还没结束，就是平局：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the function associated with whichever player''s turn it is to decide a
    move:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行与轮到哪个玩家的函数相关联的方法来决定一步棋：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If either player makes an illegal move, that is an automatic loss. Agents should
    know better:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任一玩家走出违规步骤，那就是自动认输。代理应该更明白：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Apply the move to `board_state` and check whether we have a winner. If we do,
    end the game; if we don't, switch `player_turn` to the other player and loop back
    around.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将走法应用到`board_state`上，并检查我们是否有获胜者。如果有，结束游戏；如果没有，切换`player_turn`到另一个玩家，并重新循环。
- en: 'Here is how we could write a method for a control policy that would choose
    actions completely at random out of the available legal moves:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何编写一种控制策略的方法，该方法将完全随机选择可用的合法走法：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s run two random players against each other and check whether the output
    might look something like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行两个随机玩家相互对战，然后检查输出是否可能看起来像这样：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we have a good way of trying out different control policies on a board game,
    so let's go about writing something a bit better. We can start with a min-max
    function that should play at a much higher standard than our current random players.
    The full code for the min-max function is also available in the GitHub repo in
    the `min_max.py` file.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种很好的方法来尝试在棋盘游戏上尝试不同的控制策略，所以让我们写些稍微好一点的东西。我们可以从一个min-max函数开始，该函数的水平应该比我们当前的随机玩家高得多。Min-max函数的完整代码也可以在GitHub库的`min_max.py`文件中找到。
- en: 'Tic-tac-toe is a game with a small space of possibilities, so we could simply
    run a min-max for the whole game from the board''s starting position until we
    have gone through every possible move for every player. But it is good practice
    to still use an evaluation function, as for most other games we might play, this
    will not be the case. The evaluation function here will give us one point for
    getting two in a line if the third space is empty; it''ll be the opposite if our
    opponent achieves this. First, we will need a method for scoring each individual
    line that we might make. The `score_line` will take sequences of length 3 and
    score them:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 井字棋是一个可能性空间较小的游戏，所以我们可以简单地从棋盘的起始位置运行整个游戏的min-max，直到我们遍历了每个玩家的每个可能走法。但是使用一个评估函数是个好习惯，因为对我们玩的大多数其他游戏来说，情况并非如此。这里的评估函数将为我们在后面得到两条线中的一个空位置时给我们一个分数；如果我们的对手实现了这一点，那么他将是相反的。首先，我们将需要一个为我们可能做出的每条线得分的方法。`score_line`将使用长度为3的序列并对它们进行评分：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then the `evaluate` method simply runs through each possible line on the tic-tac-toe
    board and sums them up:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`evaluate`方法简单地遍历井字棋棋盘上的每条可能的线，并将它们加总起来：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we come to the actual `min_max` algorithm method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们来到实际的`min_max`算法方法：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first two arguments to the method, which we are already familiar with,
    are `board_state` and `side`; however, `max_depth` is new. Min-max is a recursive
    algorithm, and `max_depth` will be the maximum number of recursive calls we will
    make before we stop going down the tree and just evaluate it to get the result.
    Each time we call `min_max` recursively, we will reduce `max_depth` by 1, stopping
    to evaluate when we hit 0:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的前两个参数，我们已经熟悉了，是`board_state`和`side`；不过，`max_depth`是新的。Min-max是一种递归算法，`max_depth`将是我们在停止沿树向下移动并仅评估其以获取结果之前所使用的最大递归调用次数。每次我们递归调用`min_max`时，我们将`max_depth`减少1，当我们达到0时停止评估：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If there are no moves to make, then there is no need to evaluate anything;
    it is a draw, so let''s return with a score of 0:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有可走的步骤，那么就没有必要评估任何东西；这是平局，所以让我们返回一个分数为0：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will run through each legal move and create a `new_board_state` with
    that move applied:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将详细介绍每个合法走法，并创建一个应用了该走法的`new_board_state`：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Check whether the game is already won in this `new_board_state`. There is no
    need to do any more recursive calling if the game is already won. Here, we are
    multiplying the winner''s score by 1,000; this is just an arbitrary large number
    so that an actual win or loss is always considered better/worse than the most
    extreme result we might get from a call to `evaluate`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个`new_board_state`是否已经获胜。如果游戏已经获胜，则不需要再进行递归调用。在这里，我们将获胜者的分数乘以1,000；这只是一个任意的大数字，以便实际的胜利或失败总是被认为比我们可能从对`evaluate`的调用中获得的最极端结果更好/更差：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you don''t already have a winning position, then the real meat of the algorithm
    starts. If you have reached `max_depth`, then now is the time to evaluate the
    current `board_state` to get our heuristic for how favorable the current position
    is to the first player. If you haven''t reached `max_depth`, then recursively
    call `min_max` with a lower `max_depth` until you hit the bottom:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有获胜位置，那么算法的真正精华就开始了。如果达到`max_depth`，那么现在就是评估当前`board_state`以获得我们的启发式的时候，这能告诉我们当前位置对第一个玩家有多有利。如果还没有达到`max_depth`，则递归调用`min_max`，直到达到底部：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we have our evaluation for the score in `new_board_state`, we want
    either the best or worst scoring position depending on which side we are. We keep
    track of which move leads to this in the `best_score_move` variable, which we
    return to with the score at the end of the method.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对`new_board_state`中的评分有了，我们想要获得最佳或最差的得分位置，取决于我们是哪一方。我们通过`best_score_move`变量跟踪导致这一点的移动，最终在方法结束时将其与分数一起返回。
- en: 'A `min_max_player` method can now be created to go to our earlier `play_game`
    method:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以创建一个`min_max_player`方法，以便回到我们之前的`play_game`方法：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now if we run a series of games with `random_player` against a `min_max` player,
    we will find that the min_max player wins almost every time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们让`random_player`和`min_max`玩家进行一系列游戏，我们会发现`min_max`玩家几乎每次都会赢。
- en: 'The min max algorithm, though important to understand, is never used in practice
    because there is a better version of it: min max with alpha beta pruning. This
    takes advantage of the fact that certain branches of the tree can be ignored or
    pruned, without needing to be fully evaluated. Alpha beta pruning will produce
    the same result as min max but with, on average, half as much search time.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管重要理解min-max算法，但实际上从未被使用，因为有一个更好的版本：带有alpha-beta剪枝的min-max。这利用了树的某些分支可以被忽略或剪枝的事实，而无需完全评估它们。alpha-beta剪枝将产生与min-max相同的结果，但平均搜索时间减少了一半。
- en: To explain the idea behind alpha beta pruning, let's consider that while building
    our min-max tree, half of the nodes are trying to make decisions to maximize the
    score and the other half to minimize it. As we start evaluating some of the leaves,
    we get results that are good for both min and max decisions. If taking a certain
    path through the tree scores, say -6, the min branch knows it can get this score
    by following the branch. The thing that stops it from using this score is that
    max decisions has to make the decisions, and it cannot choose a leaf favorable
    to the min node.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释alpha-beta剪枝背后的思想，让我们考虑在构建我们的min-max树时，一半的节点试图做出决策以最大化分数，另一半则试图最小化它。当我们开始评估一些叶子时，我们会得到对min和max决策都有利的结果。如果通过树的某条路径得分为-6，min分支知道它可以通过跟随该分支获得这个分数。阻止它使用这个分数的是max决策必须做出决策，而且它不能选择对min节点有利的叶子。
- en: But as more leaves are evaluated, another might be good for the max node, with
    a score of +5\. The max node will never choose a worse outcome than this. But
    now that we have a score for both min and max, we know if we start going down
    a branch where the best score for min is worse than -6 and the best score for
    max is worse than +5, then neither min nor max will choose this branch, and we
    can save on the evaluation of that whole branch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着更多叶子的评估，另一个可能对max节点有利的叶子出现，得分为+5。max节点永远不会选择比这更差的结果。但是现在我们对min和max都有了得分，我们知道如果开始沿着一个最佳min得分比-6更糟糕，而最佳max得分比+5更糟糕的分支，那么无论min还是max都不会选择这个分支，我们就可以节省对整个分支的评估。
- en: The alpha in alpha beta pruning stores the best result that the max decisions
    can achieve. The beta stores the best result (lowest score) that the min decisions
    can achieve. If alpha is ever greater than or equal to beta, we know we can skip
    further evaluation of the current branch we are on. This is because both the decisions
    already have better options.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* gives an example of this. Here see that from the very first leaf
    itself, we can set an alpha value of 0\. This is because once the max player has
    found a score of 0 in a branch, they need never choose a lower score. Next, in
    the third leaf across, the score is 0 again, so the min player can set their beta
    score to 0\. The branch that reads *branch ignored* no longer needs to be evaluated
    because both alpha and beta are 0\.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this, consider all the possible results that we could get from
    evaluating the branch. If it were to result in a score of +1, then the min player
    would simply choose an already existing branch where it had scored 0\. In this
    case, the branch to the ignored branches left. If the score results in -1, then
    the max player would simply choose the left most branch in the image where they
    can get 0\. Finally, if it results in a score of 0, it means no one has improved,
    so the evaluation of our position remains unchanged. You will never get a result
    where evaluating a branch would change the overall evaluation of the position.
    Here is an example of the min max method modified to use alpha beta pruning:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Implementing a Python Tic-Tac-Toe game](img/00260.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Min max method with alpha beta pruning'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now pass in both `alpha` and `beta` as parameters; we stop searching through
    the branches that are either less than alpha or more than beta:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now when we recursively call `min_max_alpha_beta`, we pass in our new alpha
    and beta values that may have been updated as part of the search:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `side > 0` expression means that we are looking to maximize our score,
    so we will store the score in the alpha variable if it''s better than our current
    alpha:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If `side` is < 0 we are minimizing, so store the lowest scores in the beta
    variable:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If alpha is greater than beta, then this branch cannot improve the current
    score, so we stop searching it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In 1997, IBM created a chess program called *Deep Blue*. It was the first to
    beat the reigning world chess champion Garry Kasparov. While an amazing achievement,
    it would be hard to call *Deep Blue* intelligent. Though, it has huge computational
    power, and its underlying algorithm is just the same min-max algorithm from the
    50's. The only major difference is that *Deep Blue* took advantage of the opening
    theory in chess.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The opening theory comprises of a sequences of moves that are from the starting
    position and are known to lead to favorable or unfavorable positions. For example,
    if white starts with the move pawn e4 (the pawn in front of the king moved forward
    by two spaces), then black responds with pawn c5; this is known as the Sicilian
    defense, and there are many books written on the sequences of play that could
    follow from this position. Deep Blue was programmed to simply follow the best
    moves recommended from these opening books and only start calculating the best
    min-max move once the opening line of play reaches its end. In this way, it saves
    on computational time, but it also takes advantage of the vast human research
    that has gone into the working out of the best positions in the opening stages
    of chess.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 开局理论由一系列从起始位置开始的走法组成，这些走法被认为会导致有利或不利的局面。例如，如果白方棋手以e4（王前的兵向前移动两格）开局，那么黑方应该回应c5，这就是西西里防御，对于这个局面，有许多书籍介绍接下来可能出现的走法。深蓝计算机只是简单地遵循这些开局书籍推荐的最佳走法，并且只在开局走法结束时开始计算最佳的极小极大走法。这样，它既省去了计算时间，也利用了人类在国际象棋开局阶段找到最佳局面所进行的大量研究。
- en: Learning a value function
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习一个价值函数
- en: Let's get a bit more details on exactly how much computation the min max algorithm
    has to do. If we have a game of breadth *b* and depth *d*, then evaluating a complete
    game with min-max would require the construction of a tree with eventual *d* ^(*b*)
    leaves. If we use a max depth of *n* with an evaluation function, it would reduce
    our tree size to *n* ^(*b*). But this is an exponential equation, and even though
    *n* is as small as 4 and *b* as 20, you still have 1,099,511,627,776 possibilities
    to evaluate. The tradeoff here is that as *n* gets lower, our evaluation function
    is called at a shallower level, where it may be a lot less good than the estimated
    quality of the position. Again, think of chess where our evaluation function is
    simply counting the number of pieces left on the board. Stopping at a shallow
    point may miss the fact that the last move put the queen in a position where it
    could be taken in the following move. Greater depth always equals greater accuracy
    of evaluation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对极小极大算法需要计算的具体数量进行更详细的了解。如果我们的游戏广度为*b*，深度为*d*，那么使用极小极大评估一个完整游戏需要构建一棵树，最终有*d*^(*b*)个叶子。如果我们使用最大深度*n*和一个评估函数，它将把我们的树大小减小到*n*^(*b*)。但这是一个指数方程，即使*n*只有4，*b*为20，你仍然有1,099,511,627,776种可能性需要评估。这里的权衡是，随着*n*的降低，我们的评估函数在较浅的层次上被调用，这可能比局面的预估质量要差得多。再一次，以国际象棋为例，我们的评估函数只是简单地统计棋盘上剩下的棋子数量。在较浅的位置停止可能会忽略最后一步将皇后放在可以在下一步中被吃掉的位置的事实。更大的深度总是意味着更准确的评估。
- en: Training AI to master Go
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练AI掌握围棋
- en: The number of possibilities in chess, though vast, is not so vast that with
    a powerful computer, you can't defeat the world's greatest human player. Go, an
    ancient Chinese game whose origin goes back to more than 5,500 years, is far more
    complex. In Go, a piece can be placed anywhere on the 19 x 19 board. To begin
    with, there are 361 possible moves. So to search forward *k* moves, you must consider
    361k possibilities. To make things even more difficult, in chess, you can evaluate
    how good a position is fairly accurately by counting the number of pieces on each
    side, but in Go, no such simple evaluation function has been found. To know the
    value of a position, you must calculate through to the end of the game, some 200+
    moves later. This makes the game impossible to play to a good standard using min-max.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 国际象棋中的可能性虽然很多，但并不是如此之多，以至于用一台强大的计算机无法击败世界上最伟大的人类棋手。围棋是一种源远流长的中国古老游戏，其起源可以追溯到5500多年前，远比国际象棋复杂得多。在围棋中，一子可以放在19
    x 19的棋盘上的任何地方。首先有361个可能的走法。因此，要往前搜索*k*步，你必须考虑361k种可能性。使情况更加困难的是，在国际象棋中，你可以通过计算每一方的棋子数量相对精确地评估一个局势的好坏，但在围棋中，没有找到这样简单的评估函数。
    要知道一个局势的价值，你必须计算到游戏结束，再往后走200多步。这使得游戏通过极小极大来达到一个良好水平几乎是不可能的。
- en: '![Training AI to master Go](img/00261.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![训练AI掌握围棋](img/00261.jpeg)'
- en: Figure 5
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: To get a good feel of the complexity of Go, it is worth thinking about how humans
    learn to play Go versus Chess. When beginners starts learning Chess, they make
    a series of moves in the direction of their opponent's side of the board. At some
    point, they make a move that leaves one of their pieces open for capture. So the
    opponent obliges and takes the piece. It is then that the beginner player immediately
    understands that their last move was bad, and if they want to improve, they cannot
    make the same mistake again. It is very easy for the player to identify what they
    did wrong, though correcting yourselves consistently may require a lot of practice.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, when a beginner learns Go, it looks like a series of almost random
    moves across the board. At a certain point, both players run out of their moves
    and the position is counted up to see who won. The beginner finds out he has lost
    and stares at the mass of pieces in different positions and scratches his head
    wondering exactly what happened. For humans, Go is incredibly difficult and takes
    a high degree of experience and skill to be able to understand where players are
    going wrong.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Also, Go doesn't have anything like the opening theory books that Chess has.
    Go's opening theory rather than being sequences of moves that a computer could
    follow is lots of general principles instead, such as good shapes to aim for or
    ways to take corners of the board. There is something called *Joseki* in Go, which
    are studied sequences of moves known to lead to different advantages. But all
    of these must be applied to that context when a player recognizes a particular
    arrangement is possible; they are not actions that can be blindly followed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach for games such as Go, where evaluation is so difficult, is **Monte
    Carlo Tree Search** (**MCTS**). If you have studied Bayesian probability, you
    will have heard of Monte Carlo sampling. This involves sampling from a probability
    distribution to obtain an approximation for an intractable value. MCTS is similar.
    A single sample involves randomly selecting actions for each player until you
    reach a terminal state. We maintain statistics for each sample so that after we
    are done, we can select the action from the current state with the highest mean
    success rate. Here is an example of MCTS for the tic tac toe game we spoke about.
    The complete code can also be found in the GitHub repo in the `monte_carlo.py`
    file:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `monte_carlo_sample` method here generates a single sample from a given
    position. Again, we have a method that has `board_state` and `side` as arguments.
    This method will be called recursively until we reach a terminal state, so either
    a draw because no new move can be played or a win for one player or another:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A move will be selected randomly from the legal moves in the position, and
    we will recursively call the sample method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Take monte carlo samples from this board state and update our results based
    on them:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Get the move with the best average result:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 获得同样结果最佳走法：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is the method that brings it all together. We will call the `monte_carlo_smaple`
    method `number_of_samples` times, keeping track of the result of each call. We
    then return the move with the best average performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是将所有内容整合在一起的方法。我们将调用`monte_carlo_sample`方法`number_of_samples`次，跟踪每次调用的结果。然后我们返回平均表现最佳的走法。
- en: It is good to think about how different the results obtained from MCTS will
    be of those that involve min-max. If we go back to chess as an example, in the
    position illustrated, white has a winning move, putting the rook on the back rank,
    c8, to give mate. Using min-max, this position would be evaluated as a winning
    position for white. But using MCTS, given that all other moves here lead to a
    probable victory for black, this position will be rated as favorable to black.
    This is why MCTS is very poor at chess and should give you a feel of why MCTS
    should only be used when Min-Max is not viable. In Go, which falls into the other
    category, the best AI performance was traditionally found using MCTS.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下MCTS得到的结果与涉及最小最大的结果有多大不同是很有意义的。如果我们以国际象棋为例，以这个局面来说，白方有一个获胜的着法，将车移到后排c8，将黑方将军。使用最小最大算法，这个局面会被评价为白方获胜的局面。但是使用MCTS，考虑到这里的所有其他走法都会导致黑方潜在的胜利，这个局面将被评价为对黑方有利。这就是为什么MCTS在国际象棋中表现很差，并且应该让你感受到为什么只有在最小最大算法不可行时才应该使用MCTS。在围棋这类游戏中，传统上使用MCTS找到了最佳的人工智能表现。
- en: '![Training AI to master Go](img/00262.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![训练AI精通围棋](img/00262.jpeg)'
- en: Figure 6:A Chess position that is badly evaluated by Monte Carlo sampling. If
    white is to move, they have a winning move; however, if the samples randomly move,
    black has an opportunity to win
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个被蒙特卡洛采样严重低估的国际象棋局面。如果轮到白走，他们有一个获胜的着法；但是，如果随机走子，黑方有获胜的机会。
- en: Upper confidence bounds applied to trees
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将置信上界应用到树结构
- en: To recap, Min-Max gives us the actual best move in a position, given perfect
    information; however, MCTS only gives an average value; though it allows us to
    work with much larger state spaces that cannot be evaluated with Min-Max. Is there
    a way that we could improve MCTS so it could converge to the Min-Max algorithm
    if enough evaluations are given? Yes, Monte Carlo Tree Search with Confidence
    bounds applied to Trees (UCT) does exactly this. The idea behind it is to treat
    MCTS like a multiarmed bandit problem. The multiarmed bandit problem is that we
    have a group of slot machines—one armed bandits—each of which has an undetermined
    payout and average amount of money received per play. The payout for each machine
    is random, but the mean payout may vary significantly. How should we determine
    which slot machines to play?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，最小最大算法可以给出具体的最佳着法，假设有完美的信息；但是MCTS只给出一个平均值；尽管它允许我们处理无法用最小最大算法评估的更大状态空间。有没有办法改进MCTS，使其在给出足够的评价时能收敛到最小最大算法？是的，置信上界应用到树结构的蒙特卡洛树搜索（UCT）确实可以做到这一点。其背后的想法是把MCTS看作是一个多臂老虎机问题。多臂老虎机问题是我们有一组老虎机——单臂老虎机——每台机器都有一个未确定的赔付和每次游戏的平均赔付金额。每台机器的赔付是随机的，但平均赔付金额可能差异很大。我们该如何确定要玩哪些老虎机？
- en: There are two factors that need to be considered when choosing a slot machine.
    The first is the obvious one, an exploitative value, which is the expected return
    that the given slot machine will output. To maximize the payout, we would need
    to always play the machine with the highest expected payout. The second is the
    explorative value, where we want our playing machine to increase the information
    we have about the payoffs of different machines.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择老虎机时需要考虑两个因素。第一点是显而易见的，即利用价值，也就是给定老虎机预期的回报。为了最大化赔付，我们需要始终玩出预期赔付最高的机器。第二点是探索价值，我们希望我们玩的机器增加我们对不同机器赔付的信息。
- en: If we play machine *A* thrice, you get a payoff of 13, 10, and 7 for an average
    payoff of 10\. We also have machine *B*; we have played it once and have gotten
    a payoff of 9\. In this case, it might be preferable to play machine *B* because
    though the average payoff is lower, 9 versus 10\. The fact that we have only played
    it once means the lower payout may have just been bad luck. If we play it again
    and get a payout of 13, our average for machine B would be 11\. Therefore, we
    should switch to playing that machine for the best payout.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The multiarmed bandit problem has been widely studied within mathematics. If
    we can reframe our MCTS evaluation to look like a multiarmed bandit problem, we
    can take advantage of these well-developed theories. One way of thinking about
    it is rather than seeing the problem as one with maximizing reward, think of it
    as a problem with minimizing regret. Regret here is defined as the difference
    between the reward we get for the machine we play and the maximum possible reward
    we would get if we knew the best machine from the beginning. If we follow a policy,
    *p(a)* chooses an action that gives a reward at each time step. The regret for
    *t* number of plays, given *r** as the reward of the best possible action, is
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00263.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: If we were to choose a policy of always picking the machine with the highest
    reward, it may not be the true best machine. Therefore, our regret will increase
    linearly with each play. Similarly, if we take a policy of always trying to explore
    for finding the best machine, our regret will also increase linearly. What we
    want is a policy for *p(a)* that increases in sublinear time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The best theoretical solution is to perform the search based on confidence intervals.
    A confidence interval is the range within which we expect the true mean, with
    some probability. We want to be optimistic in the face of uncertainty. If we don't
    know something, we want to find it out. The confidence interval represents our
    uncertainty about the true mean of a given random variable. Select something based
    on your sample mean plus the confidence interval; it will encourage you to explore
    the space of possibilities while also exploiting it at the same time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'For an i.i.d random variable *x*, in the range of 0 to 1, over n samples, the
    probability that the true mean is greater than the sample mean—![Upper confidence
    bounds applied to trees](img/00264.jpeg) plus constant *u*—is given by Hoeffding''s
    inequality: Hoeffding, Wassily (1963). *Probability inequalities for sums of bounded
    random variables*. Journal of the American Statistical Association:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00265.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'We want to use this equation to find the upper bound confidence for each machine.
    *E{x},x*, and *n* are all part of statistics we have already. We need to solve
    it to use it for the purpose of finding a value for *u*. In order to do this,
    reduce the left side of the equation to p and find where it equals the right side:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00266.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'We can rearrange it to make *u* defined in terms of *n* and *p*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00267.jpeg)![Upper confidence
    bounds applied to trees](img/00268.jpeg)![Upper confidence bounds applied to trees](img/00269.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Now we want to choose a value for *p* so that our precision increases over
    time. If we set ![Upper confidence bounds applied to trees](img/00270.jpeg), then
    as n approaches infinity, our regret will tend toward 0\. Substitute that in and
    we can simplify it down to:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00271.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'The mean plus u is our upper confidence bounds, so we can use it to give us
    the **UCB1** (**Upper Confidence Bounds**) algorithm. We can substitute our values
    with the values in the multiarmed bandit problem we saw earlier, where *r* [*i*]
    is the sum of the reward received from the machine *i*, *n* [*i*] is the number
    of plays of machine *i*, and *n* is the sum of plays across all machines:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Upper confidence bounds applied to trees](img/00272.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: We will always want to choose the machine that will give us the highest score
    for this equation. If we do so, our regret will scale logarithmically with the
    number of plays, which is the theoretical best we can do. Using this equation
    for our action choice has the behavior that we will try a range of machines early
    on, but the more we try a single machine, the more it will encourage us to eventually
    try a different machine.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: It's also good to remember that an assumption at the beginning of this series
    of equations was that the range, for x in early equations, and *r* for when we
    apply it to the multiarmed bandit problem was that values were in the range of
    0 to 1\. So if we are not working in this range, we need to scale our input. We
    have not made any assumptions about the nature of the distribution though; it
    could be Gaussian, binomial, and so on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Now we have an optimal solution to the problem of sampling from a set of unknown
    distributions; how do you apply it to MCTS? The simplest way to do this is to
    only treat the first moves from the current board state as bandits or slot machines.
    Though this would improve the estimation at the top level a little, every move
    beneath that would be completely random, meaning the *r* [*i*] estimation would
    be very inaccurate.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could treat every move at every branch of the tree as a multiarmed
    bandit problem. The issue with this is that if our tree is very deep, as our evaluation
    goes deeper, we will reach positions we have never encountered before so we would
    have no samples for the range of moves we need to choose between. We would be
    keeping a huge number of statistics for a huge range of positions, most of which
    will never be used.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The compromise solution, known as Upper Confidence for Trees, is to do what
    we discuss next. We will do successive rollouts from the current board state.
    At each branch of the tree, where we have a range of actions to choose from, if
    we have previous sample statistics for each potential move, we will use the UCB1
    algorithm to choose which action to choose for the rollout. If we do not have
    sample statistics for every move, we will choose the move randomly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: How do we decide which sample statistics to keep? For each rollout, we keep
    new statistics for the first position we encounter that we do not have previous
    statistics for. After the rollout is complete, we update the statistics for every
    position we are keeping track of. This way, we ignore all the positions deeper
    down the rollout. After x evaluations, we should have exactly *x* nodes of our
    tree, growing by one with each rollout. What's more, the nodes we keep track of
    are likely to be around the paths we are using the most, allowing us to increase
    our top-level evaluation accuracy by increasing the accuracy of the moves we evaluate
    further down the tree.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a rollout from the current board state. When you select a move, do the
    following:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have statistics for every move from the current position, use the UCB1
    algorithm to choose the move.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, choose the move randomly. If this is the first randomly chosen position,
    add it to the list of positions we are keeping statistics for.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the rollout until you hit a terminal state, which will give you the result
    of this rollout.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the statistics for every position you are keeping statistics for, indicating
    what you went through in the rollout.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until you get to the maximum number of rollouts. Upper confidence bounds
    applied to Trees, the statistics for each position, are shown in the square boxes:![Upper
    confidence bounds applied to trees](img/00273.jpeg)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding diagram illustrates how this happens. In position A, there is
    statistics collected for all four possible moves. Because of this, the UCB1 algorithm
    can be used to select the best move, balancing exploitative for exploitative value.
    In the preceding diagram, the leftmost move is chosen. This leads us to **position
    B**; here only two out of the three possible moves have statistics collected on
    them. Because of this, the move you need to make for this rollout is selected
    randomly. By chance, the rightmost move is selected; the remaining moves are selected
    randomly until you reach the final **position C**, where the noughts were to win.
    This information is then applied to a graph, as shown in the following diagram:![Upper
    confidence bounds applied to trees](img/00274.jpeg)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add statistics for any position that we passed through that already has statistics,
    so 1/2 in the first diagram now becomes 2/3\. We also add statistics for the first
    position we encounter with no stats. Here, it is the rightmost position in the
    second row; it now has a score of 1/1 because the nought player won. If this branch
    is selected again and you get to position D, use the UCB1 algorithm to select
    the move, not just make a random selection as before.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is what this looks like in Python for our *Tic-Tac-Toe* game:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'First, we need a method that calculates UCB1; this is the UCB1 equation in
    Python. The one difference is here we are using `log_total_samples` as input because
    it allows us to do a small optimization later on:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Declare the method and the two dictionaries, namely `state_results` and `state_samples`.
    They will keep track of our statistics for the different board states we will
    encounter during the rollouts:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The main loop is what we go through for each rollout. At the beginning of the
    rollout, we need to initialize the variables that will track our progress within
    the rollout. `first_unvisited_node` will keep track of whether we have created
    a new statistics tracking node for this rollout. On encountering the first state
    for which we have no statistics, we create the new statistics node, adding it
    to `state_results` and `state_samples` dictionaries and then setting the variable
    to `False`. `rollout_path` will keep track of each node we visit in this rollout
    that we are keeping statistics nodes for. Once we obtain the result at the end
    of the rollout, we will update the statistics of all the states along the path:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `while result == 0` puts us into the loop for a rollout; this will run
    until one side or the other wins. In each loop of the rollout, we first construct
    a dictionary, `move_states`, mapping each available move to the state that move
    will put us into. If there are no moves to make, we are in a terminal state, it
    is a draw. So you need to record that as `result` and break out of the rollout
    loop:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we need to choose which move we are going to take at this step of the rollout.
    As specified by the MCTS-UCT algorithm, if we have statistics for every possible
    move, we choose the move with the best `upper_confidence_bounds` score; otherwise,
    we make the selection randomly:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that we have selected our move, we can update `current_board_state` to
    the state that the move puts us in:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we need to check whether we have hit the end of our MCTS-UCT tree. We will
    add every node we visit up to the first previously unvisited node to `rollout_path`.
    We will update the statistics of all these nodes once we get our result from this
    rollout:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We are at the end of our rollout loop, so switch the sides for the next iteration
    and check whether anyone has won in the current state. If so, it will cause us
    to break out of the rollout loop when we pop back to the `while result == 0` statement:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we have completed a single rollout and thus left the rollout loop. We now
    need to update our statistics with the result. `rollout_path` contains `path_board_state`
    and `path_side` for each node we want to update, so we need to go through every
    entry in there. The last two points to make are that the results from our game
    are between -1 and 1\. But the UCB1 algorithm expects its payouts between 0 and
    1; the line result `*path_side/2.+.5` does this. Second, we also need to switch
    the results to represent the side they are for. A good move for my opponent is
    the opposite of a good move for me:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Finally, once we have done the required number of rollouts, we can choose the
    best move from the current state based on the best expected payout. There is no
    longer any need to use UCB1 to choose the best move. It's because this being the
    final decision, there is no value in doing any extra exploration; the best move
    is simply the best mean payout.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: This is the MCTS-UCT algorithm. There are many different variants to it with
    different advantages in specific situations, but they all have this as core logic.
    MCTS-UCT gives us a general way to judge moves for games, such as Go, with vast
    search spaces. Also, it isn't limited to games of perfect information; it can
    often perform well in games with partially observed states, such as poker. Or,
    even more generally, any problem we might encounter that we can reconfigure to
    fit it, for example, it was used as a basis for an automated theorem proving machine.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning in Monte Carlo Tree Search
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even with MCTS-UCT, computers could still not even come close to beating the
    best Go players; however, in 2016, a team from *Google Deep Mind* developed an
    AI they called AlphaGo. It defeated Lee Sedol, the world's top Go player, over
    a five game series, winning 4-1\. The way they did this was using three improvements
    over the standard MCTS UCT approach.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: If we were to think about why MCTS is so inaccurate, an intuitive answer that
    might arise is that the moves used in the evaluation are selected randomly when
    we know that some moves are much more likelier than others. In Go, when there
    is a battle for control of a corner, the moves around that area are much better
    candidates for selection, as opposed to moves on the opposite side of the board.
    If we had a good way of selecting which moves are likely to be played, we would
    have massively reduced the breadthof our search, and by extension, increased the
    accuracy of our MCTS evaluations. If we go back to the preceding chess position,
    although every legal move can potentially be played, if you are playing against
    someone who without any chess skill will only play the winning move, evaluation
    of the other moves is simply wasted CPU cycles.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: This is where deep learning can help us. We can use the pattern recognition
    qualities of a neural network to give us a rough estimate of the probability of
    a move being played in the game, given a position. For AlphaGo, a 13-layer convolutional
    network with relu activation functions was used. The input to the network was
    the 19 x 19 board state and its output, another 19 x 19 softmax layer representing
    the probability of a move being played in each square of the board. It was then
    trained on a large database of expert-level human Go games. The network would
    be given a single position as input and the move that was played from that position
    as a target. The loss function is the mean squared error between network activation
    and the human move made. Given plenty of training, the network learned to predict
    human moves with 57 percent accuracy against a test set. The use of a test set
    here is particularly important because overfitting is a big worry. Unless the
    network can generalize its understanding of a position to a previously unseen
    position, it is useless.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to implement something similar in our preceding Tic-tac-toe example,
    we would simply replace the `move = random.choice(moves)` line with the `monte_carlo_sample`
    method or the UCT version with a move chosen by a trained neural network. This
    technique will work for any discrete game if you have a large training set of
    example games.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have a database of example games, there is another approach you
    can use. If you have an agent that plays with a tiny degree of skill, you can
    even use that agent to generate the initial collection of example games. A good
    approach, for instance, is to generate example positions and moves using the min-max
    or MCTS UCT algorithms. A network can then be trained to play moves from that
    collection. This is a good way to get a network to learn how to play a game at
    a good enough standard so that it can at least explore the space of the game with
    the plausible moves, as opposed to completely random moves.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: If we implement such a neural network, use it to select which moves to use in
    a Monte-Carlo rollout, with this, our evaluation will be much more accurate, but
    we will still suffer from the problem that our MCTS will be evaluating averages
    when we still care about the best outcome for us from the moves we make. This
    is where reinforcement learning can be introduced to improve our agent.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Quick recap on reinforcement learning
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first encountered reinforcement learning in [Chapter 1](part0016_split_000.html#F8901-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 1. Machine Learning – An Introduction"), *Machine Learning – An Introduction*,
    when we looked at the three different types of learning processes: supervised,
    unsupervised, and reinforcement. In reinforcement learning, an agent receives
    rewards within an environment. For example, the agent might be a mouse in a maze
    and the reward might be some food somewhere in that maze. Reinforcement learning
    can sometimes feel a bit like a supervised recurrent network problem. A network
    is given a series of data and must learn a response.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The key distinction that makes a task a reinforcement learning problem is that
    the responses the agent gives changes the data it receives in future time steps.
    If the mouse turns left instead of right at a *T* section of the maze, it changes
    what its next state would be. In contrast, supervised recurrent networks simply
    predict a series. The predictions they make do not influence the future values
    in the series.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The AlphaGo network has already been through supervised training, but now the
    problem can be reformatted as a reinforcement learning task to improve the agent
    further. For AlphaGo, a new network was created that shares the structure and
    weights with the supervised network. Its training is then continued using reinforcement
    learning and by specifically using an approach called policy gradients.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients for learning policy functions
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem policy gradients aims to solve is a more general version of the
    problem of reinforcement learning, which is how you can use backpropagation on
    a task that has no gradient, from the reward to the output of our parameters.
    To give a more concrete example, we have a neural network that produces the probability
    of taking an action *a*, given a state *s* and some parameters ?, which are the
    weights of our neural network:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00275.jpeg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: We also have our reward signal *R*. The actions affect the reward signal we
    take, but there is no gradient between them and the parameters. There is no equation
    in which we can plug *R*; it is just a value we obtain from our environment in
    response to *a*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: However, given that we know there is a link between the *a* we choose and *R*,
    there are a few things we could try. We could create a range of values for our
    ? from a Gaussian distribution and run them in the environment. We could then
    select a percentage of the most successful group and get their mean and variance.
    We then create a new population of ? using the new mean and variance in our Gaussian
    distribution. We can keep doing this iteratively until we stop seeing improvements
    in *R* and then use our final mean as the best choice for our parameters. This
    method is known as the **Cross Entropy Method**.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Though it can be quite successful, it is a hill-climbing method, which does
    not do a good job of exploring the space of possibilities. It is very likely to
    get stuck in local optima, which is very common in reinforcement learning. Also,
    it still does not take advantage of gradient information.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'To use gradients, we can take advantage of the fact that although there is
    no mathematical relationship between *a* and *R*, there is a probabilistic one.
    Certain *a* taken in certain *s* will tend to receive more *R* than others. We
    can write the problem of getting the gradients of ? with respect to *R* as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00276.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Here, *r* [*t*] is the reward at time step *t*. This can be rearranged into:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00277.jpeg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'If we multiply and divide it by ![Policy gradients for learning policy functions](img/00278.jpeg),
    we have the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00279.jpeg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Use the fact ![Policy gradients for learning policy functions](img/00280.jpeg)
    and simplify it to the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00281.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'What this amounts to is if we nudge our parameters along the log of the direction
    of the gradient of the reward at each time step, we tend to move towards the gradient
    of the reward across all time steps. To implement this in Python, we will need
    to take the following steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Create a neural network whose output is the probability of taking different
    actions, given an input state. In terms of the preceding equations, it will represent
    ![Policy gradients for learning policy functions](img/00278.jpeg).
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a batch of episodes with our agent running in its environment. Select its
    actions randomly according to the probability distribution output of the network.
    At every time step, record the input state, reward received, and the action you
    actually took.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of each episode of training, assign rewards to each step using the
    sum of rewards in the episode from that point on. In the case of a game such as
    Go, this will just be 1, 0, or -1 representing the final result applied to each
    step. This will represent *r*[*t*] in the equations. For more dynamic games, discounted
    rewards can be used; discounted rewards will be explained in detail in the next
    chapter.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have stored a set number of states running our episodes, we train them
    by updating our network parameters based on the log of our network output times
    the actual move that was taken, times the reward. This is used as a loss function
    of our neural network. We do this for each time step as a single batch update.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is then repeated from step 2 until we hit a stopping point, either at some
    number of iterations or some score within the environment.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The effect of this loop is that if an action is associated with positive rewards,
    we increase the parameters that lead to this action in that state. If the reward
    is negative, we decrease the parameters leading to the action. Note that for this
    to work, it requires us to have some negative valued rewards; otherwise, over
    time, all actions are simply pulled up. The best option if this does not occur
    naturally is to normalize our rewards in each batch.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient approach has been shown to be successful at learning a range
    of complex tasks, although it can take a very long time to train well and is very
    sensitive to the learning rate. Too high the learning rate and the behavior will
    oscillate wildly, never becoming stable enough to learn anything of note. Too
    low and it will never converge. This is why in the following example, we use RMSProp
    as the optimizer. Standard gradient descent with a fixed learning rate is often
    a lot less successful. Also, although the example shown here is for board games,
    policy gradients also work very well for learning more dynamic games, such as
    Pong.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create `player_func` for our tic-tac-toe''s `play_game` method;
    it uses policy gradients to learn the optimal play. We will set up the neural
    network that will take the nine squares of the board as input. The number 1 will
    be a mark for the player, -1 for the opponent, and 0 an unmarked square. Here
    the network will be set up with three hidden layers, each with 100 hidden nodes
    and relu activation functions. The output layer will also contain nine nodes,
    one for each board square. Because we want our final output to be the *probability*
    of a move being the best one, we want the output of all the nodes in the final
    layer to sum to 1\. This means using a softmax activation function is a natural
    choice. The softmax activation function looks as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00282.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Here, *x* and *y* are vectors with equal dimensions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for creating the network in TensorFlow. The full code can
    also be found in the GitHub repo in `policy_gradients.py`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'First, we import NumPy and TensorFlow, which will be used for the network,
    and create a few constant variables, which will be used later. The 3 * 3 input
    nodes is the size of the board:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `input_placeholder` variable is the placeholder that holds the input to
    the neural network. In TensorFlow, placeholder objects are used for all values
    provided to the network. When running the network, it will be set to `board_state`
    of the game. Also, the first dimension of `input_placeholder` is `None`. This
    is because, as mentioned a few times in this book, training mini-batching is much
    faster. The `None` will adjust to become the size of our mini-batch of samples
    come training time:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here we create the weights we will need for the three layers of our network.
    They will all be created with a random Xavier initialization; more on this in
    chapter:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create the first hidden layer, our `hidden_weights_1` 2d tensor, and matrix
    multiply it by `input_placeholder`. Then add the bias variable, `tf.Variable(tf.constant(0.01,
    shape=(HIDDEN_NODES[0],)))`, which gives the network a bit more flexibility in
    learning patterns. The output is then put through a relu activation function:
    `tf.nn.relu`. This is how we write the basic equation for a layer of a neural
    network in TensorFlow. The other thing to note is 0.01\. When using the `relu`
    function, it is good practice to add a small amount of positive bias. This is
    because the relu function is the maximum value and is 0\. This means that values
    below 0 will have no gradient and so will not be adjusted during learning. If
    node activation is always below zero, because of bad luck with weight initialization,
    then it is considered a dead node and will never have an impact on the network
    and will simply take up GPU/CPU cycles. A small amount of positive bias greatly
    reduces the chance of having completely dead nodes in the network:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The next few layers are created in the same way:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'For the `loss` function, we need two additional placeholders. One of them is
    for the reward we receive from the environment, in this case, the result of our
    game of tic-tac-toe. The other is meant for the actual action we will take at
    each time step. Remember, we will choose our moves according to a stochastic policy
    based on the output of our network. When we adjust our parameters, we need to
    know the actual move we took so we can move the parameters towards it if we have
    a positive reward and away from it if the reward is negative:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `actual_move_placeholder` when activated will be a one hot vector, for example,
    `[0, 0, 0, 0, 1, 0, 0, 0, 0],` with 1 being the square in which the actual move
    was played. This will act as a mask across `output_layer`, so that only the gradients
    of that move are adjusted. Success or failure in moving to the first square tells
    us nothing about the success or failure of moving to the second square. Multiplying
    it by `reward_placeholder` tells us whether we want to increase the weights leading
    to this move or reduce them. We then put `policy_gradient` into our optimizer;
    we want to maximize our reward, which means minimizing the inverse of it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: One final point is that here we are using `RMSPropOptimizer`. As mentioned before,
    policy gradients are very sensitive to the learning rate and type used. `RMSProp`
    has been shown to work well.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Within TensorFlow, variables also need to be initialized within a session;
    this session will then be used to run our calculations:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now we need a method for running our network to choose the actions that will
    be passed in to the `play_game` method that we created previously:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the `make_move` method, we do a few different things. First, we flatten
    `board_state`, which starts as the second array in a one-dimensional array that
    we need to use as input for the network. We then append that state to our `board_states`
    list so we can later use it for training, once we have the reward for the episode.
    We then run the network using our TensorFlow session: `probability_of_actions`.
    There will now be an array with nine numbers that will sum up to one; these are
    the numbers that the network will learn to have the probability where it can set
    each move as the current most favorable:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We now use `probability_of_actions` as the input to a multinomial distribution.
    The `np.random.multinomial` returns a series of values from the distribution you
    pass. Because we gave 1 for the first argument, only a single value will be generated;
    this is the move we will make. The `try…catch` around the multinomial call exists
    because owing to the small rounding errors, `probability_of_actions` sometimes
    sums up to be greater than 1\. This only happens roughly once every 10,000 calls,
    so we will be *pythonic*; if it fails, simply adjust it by some small epsilon
    and try again:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The last bit of the `make_move` method is that we need to store the move we
    actually used later in training. Then return the move to the format that our Tic-Tac-Toe
    game expects it in, which is as a tuple of two integers: one for the *x* position
    and one for the *y* position.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step before training is that once we have a complete batch to train
    on, we need to normalize the rewards from the batch. There are a few advantages
    to this. First, during early training, when it is losing or winning almost all
    games, we want to encourage the network to move towards better examples. Normalizing
    will allow us to have that extra weight applied to the rare examples that are
    more significant. Also, batch normalization tends to speed up training because
    it reduces the variance in targets:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We define a constant for how big our `BATCH_SIZE` is. This defines how many
    examples go into our mini-batches for training. Many different values of this
    work well; 100 is one of these. `episode_number` will keep track of how many game
    loops we have done. This will track when we need to kick off a mini-batch training:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`while True` puts us into the main loop. The first step we need to make here
    is to run a game using our old friend, the `play_game` method from earlier in
    the chapter. For simplicity''s sake, we will always have the policy gradient player,
    using the `make_move` method as the first player and `random_player` as the second
    player. It would not be too difficult to change it to alternate the order of moves:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Get the length of the game we just played and append the reward we received
    for it to the `rewards` array so that each board state gets the same final reward
    we received. In reality, some moves may have had more or less impact on the final
    reward than others, but we cannot know that here. We will hope that through training,
    with similar good states showing up with positive rewards more often, the network
    will learn this over time. We also scale the reward by `last_game_length`, so
    winning quickly is better than winning slowly and losing slowly is better than
    losing quickly. Another point to note is if we were running a game with a more
    unevenly distributed reward—such as Pong, where most frames would have 0 reward
    with the occasional one—this is where we might apply future discounting across
    the time steps of the episode:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Increment `episode_number`, and if we have a `BATCH_SIZE` set of samples, jump
    into the training code. We start this by doing batch normalization on our rewards.
    This is not always necessary, but it is almost always advisable because it has
    many benefits. It tends to improve training time by reducing variance across training.
    If we have issues with all rewards being positive/negative, this solves them without
    you having to give it a second thought. Finally, kick off the training by running
    the `train_step` operation through the TensorFlow session object:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, clear the current mini-batch to make way for the next one. Now let''s
    see how policy gradients perform:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00283.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: As you can see, it eventually achieves a respectable 85 percent winning rate.
    With more time and tuning of hyper-parameters, it could do even better. Also,
    note the reason that indicates why a random player who only chooses valid moves
    has a greater than 50 percent winning rate. This is because here, the observed
    player always goes first.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients in AlphaGo
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For AlphaGo using policy gradients, the network was set up to play games against
    itself. It did so with a reward of 0 for every time step until the final one where
    the game is either won or lost, giving a reward of 1 or -1\. This final reward
    is then applied to every time step in the network, and the network is trained
    using policy gradients in the same way as our Tic-tac-toe example. To prevent
    overfitting, games were played against a randomly selected previous version of
    the network. If the network constantly plays against itself, the risk is it could
    end up with some very niche strategies, which would not work against varied opponents,
    a local minima of sorts.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Building the initial supervised learning network that predicted the most likely
    moves by human players allowed AlphaGo to massively reduce the breadth of the
    search it needs to perform in MCTS. This allowed them to get much more accurate
    evaluation per rollout. The problem is that running a large many-layered neural
    network is very slow, compared to just selecting a random action. In our Monte-Carlo
    rollout, we need to select 100 moves on average and we want to do this in the
    order of hundreds of thousands of rollouts to evaluate a position. This makes
    using the network this way impractical. We need to find a way to reduce our computation
    time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: If we use the best moves selected by our network instead of manually selecting
    a move with the probability of our output, then our network is deterministic.
    Given a position on the board, the result achieved by the board will also be deterministic.
    When evaluated using the best moves from the network, the position is either a
    winning one for white or black or a draw. This result is the value of the position
    under the network's optimal policy. Because the result is deterministic, we can
    train a new deep neural network to learn the value of this position. If it performs
    well, a position can be evaluated accurately using just one pass of the neural
    network, rather than one for each move.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'A final supervised network is created using the same structure as the previous
    networks, except this time the final output, rather than being a probability of
    actions across the board, is just a single node representing the expected result
    of the game: win for white, win for black, or draw.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for this network is the mean squared error between its output
    and the result achieved by the reinforcement learning network. It was found after
    training that the value network could achieve a mean squared error of just 0.226
    and 0.234 on the training and test sets, respectively. This indicated that it
    could learn the result with good accuracy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, at this point, Alpha Go has three differently trained deep neural
    networks:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**SL**: This is a network trained using supervised learning to predict the
    probability of a human move from a board position.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL**: This is a network trained that initially used the weights from the
    SL network, but was then further trained using reinforcement learning to choose
    the best move from a given position.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: This is a network again trained with supervised learning to learn the
    expected result of the position when played using the RL network. It provides
    the value of the state.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a real game against Lee Sedol, Alpha Go used a variant on the MCTS-UCT
    that we introduced earlier. When the rollout was simulated from the MCTS leaves,
    rather than selecting moves randomly, they were selected using another, much smaller,
    single layer network. This network called the fast rollout policy and used a softmax
    classifier across all possible moves, where the input was the 3 x 3 color pattern
    around the action and a collection of handcrafted features, such as the liberty
    count. This is, in our example, the following line:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This could be replaced with something like this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This small network was used to run the Monte-Carlo rollout. The SL network would
    almost certainly have been better, but would have been prohibitively slow.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating the success value of a rollout from a leaf, the score was determined
    using a combination of the result from the fast rollout policy and the score as
    given by the V-network. A mixing parameter ? was used to determine the relative
    weights of these:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients in AlphaGo](img/00284.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Here, *s* is the state of the leaf and *f* is the result of the rollout using
    the fast rollout policy. After experimenting with a wide range of the values for
    ?, it was found that 0.5 yielded the best results, suggesting that both methods
    of evaluation are complementary.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The five-game series between Lee Sedol and Alpha Go started on March 9, 2016,
    in front of a large audience with a $1,000,000 prize for the winner. Lee Sedol
    was very confident in the buildup, declaring, "I have heard that Google DeepMind's
    AI is surprisingly strong and getting stronger, but I am confident that I can
    win at least this time." Sadly, for him, Alpha Go proceeded to win the first three
    games, each forcing a resignation. At this point, with the competition decided,
    he came back to win the fourth, but lost the fifth, leaving the series at 4-1.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: This was very significant progress on the part of AI, marking the first time
    that an AI had come even close to beating a top human player at such a complex
    game. It raises all kinds of questions such as in which other domains, it might
    be possible to develop AI that can outperform the best humans. The match's full
    significance on humanity remains to be seen.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot in this chapter and looked at a lot of Python code. We
    talked a bit about the theory of discrete state and zero sum games. We showed
    how min-max can be used to evaluate the best moves in positions. We also showed
    that evaluation functions can be used to allow min-max to operate on games where
    the state space of possible moves and positions are too vast.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: For games where no good evaluation function exists, we showed how Monte-Carlo
    Tree Search can be used to evaluate the positions and then how Monte-Carlo Tree
    Search with Upper Confidence bounds for Trees can allow the performance of MCTS
    to coverage toward what you would get from Min-max. This took us to the UCB1 algorithm.
    Apart from allowing us to compute MCTS-UCT, it is also a great general purpose
    method for choosing between collections of unknown outcomes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how reinforcement learning can be integrated into these approaches.
    We also saw how the policy gradient can be used to train deep networks to learn
    complex patterns and find advantages in games with difficult-to-evaluate states.
    Finally, we looked at how these techniques were applied in AlphaGo to beat the
    reigning human world champion.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in getting more involved in deep learning for board games,
    the Alpha Toe project ([https://github.com/DanielSlater/AlphaToe](https://github.com/DanielSlater/AlphaToe))
    has examples of running deep learning on a wider range of games, including Connect
    Four and Tic-Tac-Toe on a 5 x 5 board.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Though these techniques have been introduced for board games, their application
    runs a lot wider. Many problems that we encounter can be formulated, such as like
    discrete state games, for example, optimizing routes for delivery companies, investing
    in financial markets, and planning strategies for businesses. We've only just
    started exploring all the possibilities.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at using deep learning for learning computer
    games. This will build on our knowledge of policy gradients from this chapter
    and introduce new techniques for dealing with the dynamic environments of computer
    games.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
