- en: Chapter 7. Deep Learning for Board Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。棋盘游戏的深度学习
- en: You may have read sci-fi novels from the 50's and 60's; they are full of visions
    of what life in the 21st century would look like. They imagined a world of people
    with personal jet packs, underwater cities, intergalactic travel, flying cars,
    and truly intelligent robots capable of independent thought. The 21st century
    has arrived now; sadly, we are not going to get those flying cars, but thanks
    to deep learning, we may get that robot.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你读过五六十年代的科幻小说；它们充满了对21世纪生活会是什么样子的设想。他们想象了一个人们拥有个人喷气背包、水下城市、星际旅行、飞行汽车和真正有独立思考能力的机器人的世界。21世纪现在已经到来了；可悲的是，我们不会得到那些飞行汽车，但由于深度学习，我们可能会得到那些机器人。
- en: What does this have to do with deep learning for board games? In the next two
    chapters, including the current one, we will look at how to build **Artificial
    Intelligence** (**AI**) that can learn game environments. Reality has a vast space
    of possibilities. Doing even simple human tasks, such as getting a robot arm to
    pick up objects, requires analyzing huge amounts of sensory data and controlling
    many continuous response variables for the movement of the arms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这与棋盘游戏的深度学习有什么关系？在接下来的两个章节，包括当前章节，我们将看看如何构建**人工智能**（**AI**），以学习游戏环境。现实具有广阔的可能性空间。即使是进行简单的人类任务，如让机器人手臂抓取物体，也需要分析大量的感官数据并控制许多用于移动手臂的连续响应变量。
- en: Games act as a great playing field for testing general purpose learning algorithms.
    They give you an environment of large, but manageable possibilities. Also, when
    it comes to computer games, we know that humans can learn to play a game just
    from the pixels visible on the screen and the most minor of instructions. If we
    input the same pixels plus an objective into a computer agent, we know we have
    a solvable problem, given the right algorithm. In fact, for the computer, the
    problem is easier because a human being identifies that the things they seeing
    in their field of vision are actually game pixels, as opposed to the area around
    the screen. This is why so many researchers are looking at games as a great place
    to start developing true AI's—self-learning machines that can operate independently
    from us. Also, if you like games, it's lots of fun.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏作为测试通用学习算法的绝佳场所。它们给你一个庞大但可控制的可能性环境。此外，说到电脑游戏，我们知道人类可以仅通过屏幕上可见的像素和最微小的指示就学会玩游戏。如果我们将相同的像素以及一个目标输入到计算机代理中，我们知道我们有一个可解决的问题，只要使用正确的算法。实际上，对于电脑来说，问题更容易，因为人类可以识别出他们在视野中看到的东西实际上是游戏像素，而不是屏幕周围的区域。这就是为什么如此多的研究人员将游戏视为开发真正的人工智能的绝佳起点——能够独立于我们运行的自学习机器。此外，如果你喜欢游戏，它会非常有趣。
- en: In this chapter, we will cover the different tools used for solving board games,
    such as checkers and chess. Eventually, we'll build up enough knowledge to be
    able to understand and implement the kind of deep learning solution that was used
    to build AlphaGo, the AI that defeated the greatest human Go player. We'll use
    a variety of deep learning techniques to accomplish this. The next chapter will
    build on this knowledge and cover how deep learning can be used to learn how to
    play computer games, such as Pong and Breakout.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍解决棋盘游戏（如跳棋和国际象棋）的不同工具。最终，我们将积累足够的知识，以便理解并实现构建AlphaGo的深度学习解决方案，该解决方案击败了最伟大的人类围棋选手。我们将使用各种深度学习技术来实现这一点。接下来的章节将在此基础知识之上构建，并介绍如何使用深度学习来学习如何玩计算机游戏，如乒乓球和打砖块。
- en: 'The full list of concepts that we will cover across both the chapters is as
    follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个章节中涵盖的概念列表如下：
- en: The min-max algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极小极大算法
- en: Monte-Carlo Tree Search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索
- en: Reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Policy gradients
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度
- en: Q-learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: Actor-Critic
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论家
- en: Model-based approaches
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的方法
- en: We will use a few different terms to describe tasks and their solutions. The
    following are some of the definitions. They all use the example of a basic maze
    game as it is a good, simple example of a reinforcement learning environment.
    In a maze game, there are a set of locations with paths between them. There is
    an agent in this maze that can use the paths to move between the different locations.
    Some locations have a reward associated with them. The agent's objective is to
    navigate their way through the maze to get the best possible reward.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些不同的术语来描述任务及其解决方案。以下是一些定义。它们都使用基本迷宫游戏的示例，因为它是一个很好的、简单的强化学习环境的例子。在迷宫游戏中，有一组位置，它们之间有路径。在这个迷宫中有一个代理，它可以利用路径在不同的位置之间移动。一些位置与奖励相关联。代理的目标是通过迷宫找到最好的奖励。
- en: '![Deep Learning for Board Games](img/00257.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![棋盘游戏的深度学习](img/00257.jpeg)'
- en: Figure 1
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: '**Agent** is the entity for which we are trying to learn actions. In the game,
    this is the player that will try to find its way through the maze.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Agent** 是我们试图学习行动的实体。在游戏中，这是玩家，他将尝试找到迷宫的出口。'
- en: '**Environment** is a world/level/game in which the agent operates, that is,
    the maze itself.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境** 是代理操作的世界/关卡/游戏，也就是迷宫本身。'
- en: '**Reward** is the feedback that the agent gets within the environment. In the
    case of this example maze game, it might be the exit square or the carrots in
    the image that the agent is trying to collect. Some mazes may also have traps
    that give a negative reward, which the agent should try to avoid.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励** 是代理在环境中获得的反馈。在这个示例迷宫游戏中，它可能是出口方块或图像中的胡萝卜，代理正在尝试收集的物品。一些迷宫还可能有陷阱，会给予负面奖励，代理应该尽量避免。'
- en: '**State** refers to all of the information available to the agent about its
    current environment. In a maze, the state is simply the agent''s position.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态** 指的是代理关于其当前环境的所有可用信息。在迷宫中，状态就是代理的位置。'
- en: '**Action** is a possible response, or set of responses, that an agent can make.
    In a maze, this is a potential path that an agent can take from one state to another.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动** 是代理可以采取的可能响应或一组响应。在迷宫中，这是代理可以从一个状态到另一个状态的潜在路径。'
- en: '**Control policy** determines what actions the agent will take. In the context
    of deep learning, this is the neural network that we will train. Other policies
    might be selecting actions at random or selecting actions based on the code that
    the programmer has written.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制策略** 确定了代理将采取的行动。在深度学习的背景下，这是我们将要训练的神经网络。其他策略可能是随机选择行动或根据程序员编写的代码选择行动。'
- en: A lot of this chapter is code-heavy, so as an alternative to copying all the
    samples from the book, you can find the full code in a GitHub repository at [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples).
    All the examples in the chapters are presented using TensorFlow, but the concepts
    could be translated into other deep learning frameworks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章大部分内容都是代码密集型的，因此，作为从书中复制所有示例的替代方法，你可以在 GitHub 仓库 [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)
    中找到完整的代码。章节中的所有示例都是使用 TensorFlow 呈现的，但这些概念可以转化为其他深度学习框架。
- en: Early game playing AI
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早期游戏AI
- en: 'Building AI''s to play games started in the 50''s with researchers building
    programs that played checkers and chess. These two games have a few properties
    in common:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 50年代开始，研究人员构建了玩跳棋和国际象棋的程序，从而开始了构建AI来玩游戏的工作。这两个游戏有一些共同点：
- en: They are zero-sum games. Any reward that one player receives is a corresponding
    loss to the other player and vice versa. When one player wins, the other loses.
    There is no possibility of cooperation. For example, consider a game such as the
    prisoner's dilemma; here, the two players can agree to cooperate and both receive
    a smaller reward.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是零和游戏。一个玩家获得的任何奖励都对应着另一个玩家的损失，反之亦然。当一个玩家赢了，另一个玩家输了。不存在合作的可能性。例如，考虑一个游戏，比如囚徒困境；在这里，两个玩家可以同意合作，并且都获得较小的奖励。
- en: They are both games of perfect information. The entire state of the game is
    always known to both the players unlike a game such as poker, where the exact
    cards that your opponents are holding is unknown. This fact reduces the complexity
    that the AI must handle. It also means that a decision about what the best move
    can be made is based on just the current state. In poker, the hypothetical optimal
    decision about how to play would require information that is not just on your
    current hand and how much money is available to each player, but also about the
    playing styles of the opponents and what they had bid in the previous positions
    they were in.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都是完全信息游戏。游戏的整个状态对于两个玩家始终是已知的，不像扑克牌这样的游戏，你的对手手中确切的牌是未知的。这个事实减少了人工智能必须处理的复杂性。它还意味着关于什么是最佳移动的决定可以基于当前状态。在扑克中，关于如何打牌的假设最佳决策需要的信息不仅仅是你目前的手牌和每个玩家可用的金额，还有关于对手的打法以及他们在之前位置中的出价。
- en: Both games are deterministic. If a given move is made by either player, then
    that will result in an exact next state. In some games, the play may be based
    on a dice roll or random drawing of a card from a deck; in these cases, there
    would be many possible next states to consider.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个游戏都是确定性的。如果任一玩家采取了某个移动，那么下一个状态将是确切的。在某些游戏中，游戏可能基于掷骰子或从牌堆中随机抽取卡片；在这些情况下，将会有许多可能的下一个状态需要考虑。
- en: The combination of perfect information and determinism in chess and checkers
    means that given the current state, we can exactly know what state we will be
    in if the current player takes an action. This property also chains if we have
    a state, then takes an action leading to a new state. We can again take an action
    in this new state to keep playing as far into the future as we want.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际象棋和跳棋中完美信息和确定性的组合意味着鉴于当前状态，我们可以确切地知道如果当前玩家采取行动我们将处于什么状态。这个属性也适用于如果我们有一个状态，然后采取行动导致一个新的状态。我们可以再次在这个新状态中采取行动，以保持玩得尽可能长的时间。
- en: To experiment with some of the approaches of mastering board games, we will
    give examples using a Python implementation of the game called *Tic-Tac-Toe*.
    Also known as *noughts and crosses,* this is a simple game where players take
    turns making marks on a 3 by 3 grid. The first player to get three marks in a
    row wins. *Tic-Tac-Toe* is another deterministic, zero sum, perfect information
    game and is chosen here because a Python implementation of it is a lot simpler
    than chess. In fact, the whole game can be done in less than a page of code, which
    will be shown later in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试一些掌握棋盘游戏的方法，我们将使用名为*Tic-Tac-Toe*的游戏的Python实现来举例。也被称为*井字游戏*，这是一个简单的游戏，玩家轮流在一个3乘3的网格上做标记。第一个在一行中得到三个标记的玩家获胜。*Tic-Tac-Toe*是另一种确定性、零和、完全信息游戏，在这里选择它是因为它的Python实现比国际象棋简单得多。事实上，整个游戏可以用不到一页的代码来完成，这将在本章后面展示。
- en: Using the min-max algorithm to value game states
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用极大极小算法来评估游戏状态
- en: Say we want to work out the best move in a zero sum, deterministic, perfect
    information game. How can we do this? Well, first off, given that we have perfect
    information, we know exactly what moves are available to us. Given that the game
    is deterministic, we know exactly what state the game will change to due to each
    of those moves. The same is then true for the opponent's move as well; we know
    exactly what possible moves they have and how the state would look as a result
    of each of those moves.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算在一个零和、确定性、完全信息游戏中的最佳移动。我们怎么做呢？首先，考虑到我们有完全信息，我们知道确切地哪些移动是可用的。鉴于游戏是确定性的，我们知道每一个移动会导致游戏状态的确切变化。对于对手的移动也是如此；我们也知道他们有哪些可能的移动以及每个移动导致的状态会是怎样。
- en: 'One approach for finding the best move would be to construct a full tree of
    every possible move for each player at each state until we reach a state where
    the game is over. This end state of the game is also known as the terminal state.
    We can assign a value to this terminal state; a win could carry the value 1, a
    draw 0, and a loss -1\. These values reflect the states'' desirability to us.
    We would prefer a win than a draw and a draw to a loss. *Figure 2* shows an example
    of this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳移动的一种方法是为每个玩家在每个状态下构造每个可能移动的完整树，直到我们到达游戏结束的状态。游戏的最终状态也称为终端状态。我们可以为这个终端状态赋予一个值；赢得的可以是值1，平局是0，输掉是-1。这些值反映了对我们来说状态的可取之处。我们宁愿赢也不愿平局，而宁愿平局也不愿输。*图2*显示了一个例子：
- en: '![Using the min-max algorithm to value game states](img/00258.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![使用极小极大算法估值游戏状态](img/00258.jpeg)'
- en: 'Figure 2: Tree of all the states of tic-tac-toe'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：井字棋所有状态的树
- en: In a terminal state, we can go back to the state where the player chose the
    move that led to the terminal state. That player, whose objective is to find the
    best possible move, can determine exactly what value they will get from the actions
    they would take, which is the terminal state that they eventually led the game
    to. They will obviously want to select the move that would lead to the best possible
    value for themselves. If they have a choice of actions that would either lead
    to winning the terminal state or losing it, they will select the one that leads
    to a winning state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个终止状态中，我们可以回到玩家选择导致终止状态的移动的状态。那位玩家，其目标是找到最佳的可能移动，可以确定他们将从他们将采取的行动中获得的确切值，即他们最终将游戏带入的终止状态。他们显然会选择将导致对自己可能获得的最佳值的移动。如果他们有选择要么导致赢得终止状态要么输掉终止状态的行动，他们将选择导致赢得状态的那个行动。
- en: The value of the state where terminal states are selected can then be marked
    with the value of the best possible action that the player could make. This gives
    us the value to that player of being in this state. But we are playing a two-player
    game here, so if we go back a state, we would be in a state where the other player
    is due to make a move. We now on our graph have the value that this opponent will
    get from their actions in this state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 终止状态被选择的状态的值可以标记为玩家可能采取的最佳行动的值。这给了我们在这种状态下的玩家的价值。但是在这里我们正在玩一个双人游戏，所以如果我们回到一个状态，我们将处于另一位玩家要做出移动的状态。现在在我们的图中，我们有了对手在该状态下将从他们在该状态下的行动中获得的价值。
- en: This being a zero sum game, we want our opponent to do as badly as possible,
    so we will select the move that leads to the lowest value state for them. If we
    keep going back through the graph of states, marking all the states with the value
    of the best state that any action could lead to, we can determine exactly what
    is the best action in the current state.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个零和游戏，我们希望我们的对手表现得尽可能糟糕，因此我们将选择导致对方状态值最低的移动。如果我们不断回溯状态图，标记所有状态的值为任何动作可能导致的最佳状态值，我们就可以确定当前状态中的最佳动作。
- en: '![Using the min-max algorithm to value game states](img/00259.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![使用极小极大算法估值游戏状态](img/00259.jpeg)'
- en: Figure 3\. Min-max algorithm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 极小极大算法
- en: In this way, a complete tree of the game can be constructed, showing us the
    best move that we can make in the current state. This approach is called the min-max
    algorithm and is what the early researchers used for their chess and checkers
    games.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，可以构建游戏的完整树，显示我们可以在当前状态下进行的最佳移动。这种方法称为极小极大算法，是早期研究者用于国际象棋和跳棋游戏的方法。
- en: Though this approach tells us the exact best move for any zero sum, deterministic,
    perfect information game, it unfortunately has a major problem. Chess has on average
    about 30 possible moves per turn and games last on average 40 turns. So to build
    a graph from the first state in chess to all the terminal states would require
    approximately 30^(40) states. Many orders of magnitude larger than this is possible
    on the world's best hardware. When talking about games, the number of moves a
    player can take per turn is referred to as the **breadth** and the number of moves
    the game takes per turn as the **depth**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法告诉我们任何零和、确定性、完美信息游戏的确切最佳移动，但不幸的是它有一个主要问题。国际象棋平均每回合大约有 30 个可能的移动，并且游戏平均持续
    40 回合。因此，要从国际象棋的第一个状态构建到所有终止状态将需要大约 30^(40) 个状态。这比世界上最好的硬件可能的数量要大得多。在谈论游戏时，玩家每回合可以采取的移动数量称为**广度**，游戏每回合采取的移动数量称为**深度**。
- en: To make chess tractable with the Min-Max algorithm, we need to massively reduce
    the depth of our search. Rather than calculate the whole tree through to the end
    of the game, we can construct our tree down to a fixed depth, say six moves on
    from the current state. At each leaf that is not an actual terminal state, we
    can use an evaluation function to estimate how likely the player is to win in
    that state.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要使极小极大算法在棋类游戏中可行，我们需要大幅减少搜索的深度。与其计算整个树直到游戏结束，我们可以构建我们的树到一个固定的深度，比如从当前状态开始后的六步。在每个不是实际终止状态的叶子节点上，我们可以使用一个评估函数来估计在该状态下玩家获胜的可能性。
- en: For chess, a good evaluation function is to do a weighted count of the number
    of pieces available with each player. So, one point for a pawn, three for a bishop
    or knight, five for a rook, and eight for a queen. If I have three pawns and a
    knight, I have six points; similarly, if you have two pawns and a rook, you have
    seven points. Therefore, you are one point ahead. A player with more pieces left
    generally tends to win in chess. However, as any keen chess player who has played
    against a good exchange sacrifice will know, this evaluation function has its
    limits.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于国际象棋，一个良好的评估函数是对每个玩家可用的棋子数量进行加权计数。因此，兵的得分为1分，主教或骑士的为3分，车的为5分，后的为8分。如果我有三个兵和一个骑士，我得到六分；同样地，如果你有两个兵和一个车，你有七分。因此，你领先一分。通常情况下，在国际象棋中，剩下的棋子更多的玩家往往会取胜。然而，任何曾经与好的交换牺牲对手交战的国际象棋玩家都会知道，这个评估函数是有局限性的。
- en: Implementing a Python Tic-Tac-Toe game
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Python版的Tic-Tac-Toe游戏
- en: Let's build a basic implementation of *Tic-Tac-Toe* so we can see what an implementation
    of the min-max algorithm looks like. If you do not feel like copying all of this,
    you can find the full code in the GitHub repository [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)
    in the `tic_tac_toe.py` file.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们构建一个基本的*Tic-Tac-Toe*实现，这样我们就可以看到min-max算法的实现是什么样子的。如果你不想复制所有这些，你可以在GitHub仓库[https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)的`tic_tac_toe.py`文件中找到完整的代码。 '
- en: 'In the game board, we will be represented by a 3 x 3 tuple of integers. Tuples
    are used instead of lists so that later on, we can get equality between matching
    board states. In this case, **0** represents a square that has not been played
    in. The two players will be marked **1** and **-1**. If player one makes a move
    in a square, that square will be marked with their number. So here we go:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏棋盘中，我们将用一个3 x 3的整数元组表示。使用元组而不是列表，以便以后能够在匹配的棋盘状态之间得到相等。在这种情况下，**0**表示一个未被玩过的方格。两名玩家将分别用**1**和**-1**表示。如果玩家一在一个方格上下了一步，那么该方格将被标记为他们的数字。所以让我们开始：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `new_board` method will be called before the play for a fresh board, ready
    for the players to make their moves on:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在玩家进行下一步前，将会调用`new_board`方法，准备好一个新的棋盘：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `apply_move` method takes one of the 3 x 3 tuples for `board_state` and
    returns a new `board_state` with the move by the given side applied. A move will
    be a tuple of length 2, containing the coordinate of the space that we want to
    move to as two integers. Side will an integer representing the player who is playing
    the move, either 1 or -1:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_move`方法接受`board_state`的3 x 3元组之一，并返回应用了给定方向移动的新的`board_state`。移动将是一个包含两个整数坐标的长度为2的元组。方向将是代表玩家的整数，要么是1，要么是-1：'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This method gives us the list of legal moves for a given 3 x 3 `board_state`,
    which is simply all the non-zero squares. Now we just need a method to determine
    whether a player has the three winning marks in a row:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法为给定的3 x 3 `board_state`列出了合法的移动列表，它就是所有非零方格。现在我们只需要一个方法来确定玩家是否已经连成了三个获胜的标记：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `has_3_in_a_line` takes a sequence of three squares from the board. If
    all are either 1 or -1, it means one of the players has gotten three in a row
    and has won. We then need to run this method against each possible line on the
    Tic-Tac-Toe board to determine whether a player has won:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`has_3_in_a_line`将获取棋盘上的三个方格的序列。如果所有的方格都是1或-1，这意味着其中一名玩家连成了三个获胜的标记，赢得了胜利。然后，我们需要对Tic-Tac-Toe棋盘上的每条可能的线运行这个方法，以确定玩家是否已经获胜：'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With just these few functions, you can now play a game of *Tic-Tac-Toe*. Simply
    start by getting a new board, then have the players successively choose moves
    and apply those moves to `board_state`. If we find that there are no available
    moves left, the game is a draw. Otherwise, if `has_winner` returns either `1`
    or `-1`, it means one of the players has won. Let''s write a simple function for
    running a Tic-Tac-Toe game with the moves decided by methods that we pass in,
    which will be the control policies of the different AI players that we will try
    out:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 只需这几个功能，你就可以玩一局*Tic-Tac-Toe*游戏。简单地开始，获取一个新的棋盘，然后让玩家依次选择移动并将这些移动应用到`board_state`上。如果我们发现没有剩余可用的移动，游戏就是平局。否则，如果`has_winner`返回`1`或`-1`，这意味着其中一名玩家获胜。接下来，让我们编写一个简单的函数来运行一个Tic-Tac-Toe游戏，其中的移动由我们传递的方法来决定，这些方法将会是我们将尝试的不同AI玩家的控制策略：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We declare the method and take it to the function that will choose the action
    for each player. Each `player_func` will take two arguments: the first being the
    current `board_state` and the second being the side that the player is playing,
    1 or -1\. The `player_turn` variable will keep track of this for us:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们宣告这个方法，并将其带到将为每个玩家选择动作的函数中。每个`player_func`将会有两个参数：第一个是当前的`board_state`，第二个是玩家所执的一方，1或-1。`player_turn`变量将为我们跟踪这一切：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is the main loop of the game. First we have to check whether there are
    any available moves left on `board_state`; if there are, the game is not over
    and it is a draw:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是游戏的主要循环。首先，我们要检查`board_state`上是否还有可用的走法；如果有，游戏还没结束，就是平局：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the function associated with whichever player''s turn it is to decide a
    move:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行与轮到哪个玩家的函数相关联的方法来决定一步棋：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If either player makes an illegal move, that is an automatic loss. Agents should
    know better:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任一玩家走出违规步骤，那就是自动认输。代理应该更明白：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Apply the move to `board_state` and check whether we have a winner. If we do,
    end the game; if we don't, switch `player_turn` to the other player and loop back
    around.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将走法应用到`board_state`上，并检查我们是否有获胜者。如果有，结束游戏；如果没有，切换`player_turn`到另一个玩家，并重新循环。
- en: 'Here is how we could write a method for a control policy that would choose
    actions completely at random out of the available legal moves:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何编写一种控制策略的方法，该方法将完全随机选择可用的合法走法：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s run two random players against each other and check whether the output
    might look something like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行两个随机玩家相互对战，然后检查输出是否可能看起来像这样：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we have a good way of trying out different control policies on a board game,
    so let's go about writing something a bit better. We can start with a min-max
    function that should play at a much higher standard than our current random players.
    The full code for the min-max function is also available in the GitHub repo in
    the `min_max.py` file.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种很好的方法来尝试在棋盘游戏上尝试不同的控制策略，所以让我们写些稍微好一点的东西。我们可以从一个min-max函数开始，该函数的水平应该比我们当前的随机玩家高得多。Min-max函数的完整代码也可以在GitHub库的`min_max.py`文件中找到。
- en: 'Tic-tac-toe is a game with a small space of possibilities, so we could simply
    run a min-max for the whole game from the board''s starting position until we
    have gone through every possible move for every player. But it is good practice
    to still use an evaluation function, as for most other games we might play, this
    will not be the case. The evaluation function here will give us one point for
    getting two in a line if the third space is empty; it''ll be the opposite if our
    opponent achieves this. First, we will need a method for scoring each individual
    line that we might make. The `score_line` will take sequences of length 3 and
    score them:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 井字棋是一个可能性空间较小的游戏，所以我们可以简单地从棋盘的起始位置运行整个游戏的min-max，直到我们遍历了每个玩家的每个可能走法。但是使用一个评估函数是个好习惯，因为对我们玩的大多数其他游戏来说，情况并非如此。这里的评估函数将为我们在后面得到两条线中的一个空位置时给我们一个分数；如果我们的对手实现了这一点，那么他将是相反的。首先，我们将需要一个为我们可能做出的每条线得分的方法。`score_line`将使用长度为3的序列并对它们进行评分：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then the `evaluate` method simply runs through each possible line on the tic-tac-toe
    board and sums them up:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`evaluate`方法简单地遍历井字棋棋盘上的每条可能的线，并将它们加总起来：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we come to the actual `min_max` algorithm method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们来到实际的`min_max`算法方法：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first two arguments to the method, which we are already familiar with,
    are `board_state` and `side`; however, `max_depth` is new. Min-max is a recursive
    algorithm, and `max_depth` will be the maximum number of recursive calls we will
    make before we stop going down the tree and just evaluate it to get the result.
    Each time we call `min_max` recursively, we will reduce `max_depth` by 1, stopping
    to evaluate when we hit 0:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的前两个参数，我们已经熟悉了，是`board_state`和`side`；不过，`max_depth`是新的。Min-max是一种递归算法，`max_depth`将是我们在停止沿树向下移动并仅评估其以获取结果之前所使用的最大递归调用次数。每次我们递归调用`min_max`时，我们将`max_depth`减少1，当我们达到0时停止评估：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If there are no moves to make, then there is no need to evaluate anything;
    it is a draw, so let''s return with a score of 0:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有可走的步骤，那么就没有必要评估任何东西；这是平局，所以让我们返回一个分数为0：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will run through each legal move and create a `new_board_state` with
    that move applied:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将详细介绍每个合法走法，并创建一个应用了该走法的`new_board_state`：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Check whether the game is already won in this `new_board_state`. There is no
    need to do any more recursive calling if the game is already won. Here, we are
    multiplying the winner''s score by 1,000; this is just an arbitrary large number
    so that an actual win or loss is always considered better/worse than the most
    extreme result we might get from a call to `evaluate`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个`new_board_state`是否已经获胜。如果游戏已经获胜，则不需要再进行递归调用。在这里，我们将获胜者的分数乘以1,000；这只是一个任意的大数字，以便实际的胜利或失败总是被认为比我们可能从对`evaluate`的调用中获得的最极端结果更好/更差：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you don''t already have a winning position, then the real meat of the algorithm
    starts. If you have reached `max_depth`, then now is the time to evaluate the
    current `board_state` to get our heuristic for how favorable the current position
    is to the first player. If you haven''t reached `max_depth`, then recursively
    call `min_max` with a lower `max_depth` until you hit the bottom:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有获胜位置，那么算法的真正精华就开始了。如果达到`max_depth`，那么现在就是评估当前`board_state`以获得我们的启发式的时候，这能告诉我们当前位置对第一个玩家有多有利。如果还没有达到`max_depth`，则递归调用`min_max`，直到达到底部：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we have our evaluation for the score in `new_board_state`, we want
    either the best or worst scoring position depending on which side we are. We keep
    track of which move leads to this in the `best_score_move` variable, which we
    return to with the score at the end of the method.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对`new_board_state`中的评分有了，我们想要获得最佳或最差的得分位置，取决于我们是哪一方。我们通过`best_score_move`变量跟踪导致这一点的移动，最终在方法结束时将其与分数一起返回。
- en: 'A `min_max_player` method can now be created to go to our earlier `play_game`
    method:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以创建一个`min_max_player`方法，以便回到我们之前的`play_game`方法：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now if we run a series of games with `random_player` against a `min_max` player,
    we will find that the min_max player wins almost every time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们让`random_player`和`min_max`玩家进行一系列游戏，我们会发现`min_max`玩家几乎每次都会赢。
- en: 'The min max algorithm, though important to understand, is never used in practice
    because there is a better version of it: min max with alpha beta pruning. This
    takes advantage of the fact that certain branches of the tree can be ignored or
    pruned, without needing to be fully evaluated. Alpha beta pruning will produce
    the same result as min max but with, on average, half as much search time.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管重要理解min-max算法，但实际上从未被使用，因为有一个更好的版本：带有alpha-beta剪枝的min-max。这利用了树的某些分支可以被忽略或剪枝的事实，而无需完全评估它们。alpha-beta剪枝将产生与min-max相同的结果，但平均搜索时间减少了一半。
- en: To explain the idea behind alpha beta pruning, let's consider that while building
    our min-max tree, half of the nodes are trying to make decisions to maximize the
    score and the other half to minimize it. As we start evaluating some of the leaves,
    we get results that are good for both min and max decisions. If taking a certain
    path through the tree scores, say -6, the min branch knows it can get this score
    by following the branch. The thing that stops it from using this score is that
    max decisions has to make the decisions, and it cannot choose a leaf favorable
    to the min node.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释alpha-beta剪枝背后的思想，让我们考虑在构建我们的min-max树时，一半的节点试图做出决策以最大化分数，另一半则试图最小化它。当我们开始评估一些叶子时，我们会得到对min和max决策都有利的结果。如果通过树的某条路径得分为-6，min分支知道它可以通过跟随该分支获得这个分数。阻止它使用这个分数的是max决策必须做出决策，而且它不能选择对min节点有利的叶子。
- en: But as more leaves are evaluated, another might be good for the max node, with
    a score of +5\. The max node will never choose a worse outcome than this. But
    now that we have a score for both min and max, we know if we start going down
    a branch where the best score for min is worse than -6 and the best score for
    max is worse than +5, then neither min nor max will choose this branch, and we
    can save on the evaluation of that whole branch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着更多叶子的评估，另一个可能对max节点有利的叶子出现，得分为+5。max节点永远不会选择比这更差的结果。但是现在我们对min和max都有了得分，我们知道如果开始沿着一个最佳min得分比-6更糟糕，而最佳max得分比+5更糟糕的分支，那么无论min还是max都不会选择这个分支，我们就可以节省对整个分支的评估。
- en: The alpha in alpha beta pruning stores the best result that the max decisions
    can achieve. The beta stores the best result (lowest score) that the min decisions
    can achieve. If alpha is ever greater than or equal to beta, we know we can skip
    further evaluation of the current branch we are on. This is because both the decisions
    already have better options.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Alpha beta 剪枝中的 alpha 存储了最大决策可以实现的最佳结果。Beta 存储了最小决策可以实现的最佳结果（最低分数）。如果 alpha
    大于或等于 beta，我们知道可以跳过对当前分支的进一步评估。这是因为这两个决策已经有更好的选择。
- en: '*Figure 4* gives an example of this. Here see that from the very first leaf
    itself, we can set an alpha value of 0\. This is because once the max player has
    found a score of 0 in a branch, they need never choose a lower score. Next, in
    the third leaf across, the score is 0 again, so the min player can set their beta
    score to 0\. The branch that reads *branch ignored* no longer needs to be evaluated
    because both alpha and beta are 0\.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4*给出了这一点的一个示例。在这里我们看到，从第一个叶子开始，我们可以将 alpha 值设为 0。这是因为一旦最大玩家在一个分支中找到分数为 0，他们就不需要选择一个更低的分数。接下来，第三个叶子的位置上，分数再次为
    0，所以最小玩家可以将他们的 beta 分数设为 0。读取*branch ignored*的分支不再需要进行评估，因为 alpha 和 beta 都是 0。'
- en: 'To understand this, consider all the possible results that we could get from
    evaluating the branch. If it were to result in a score of +1, then the min player
    would simply choose an already existing branch where it had scored 0\. In this
    case, the branch to the ignored branches left. If the score results in -1, then
    the max player would simply choose the left most branch in the image where they
    can get 0\. Finally, if it results in a score of 0, it means no one has improved,
    so the evaluation of our position remains unchanged. You will never get a result
    where evaluating a branch would change the overall evaluation of the position.
    Here is an example of the min max method modified to use alpha beta pruning:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，考虑一下从评估分支中可能获得的所有可能结果。如果结果为 +1，则最小玩家只需选择已经获得分数为 0 的现有分支。在这种情况下，分支被忽略的分支向左（left）走。如果分数结果为
    -1，那么最大玩家只需选择图像中得分为 0 的最左边分支。最后，如果分数为 0，这意味着没有人发生改进，因此我们的位置的评估保持不变。你永远不会得到一个结果，评估一个分支会改变位置的整体评估。以下是修改后使用
    alpha beta 剪枝的 min-max 方法的示例：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Implementing a Python Tic-Tac-Toe game](img/00260.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![实现 Python 井字棋游戏](img/00260.jpeg)'
- en: 'Figure 4: Min max method with alpha beta pruning'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用 alpha beta 剪枝的 min-max 方法
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now pass in both `alpha` and `beta` as parameters; we stop searching through
    the branches that are either less than alpha or more than beta:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们传入 `alpha` 和 `beta` 作为参数；我们停止搜索那些小于 alpha 或大于 beta 的分支：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now when we recursively call `min_max_alpha_beta`, we pass in our new alpha
    and beta values that may have been updated as part of the search:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们递归调用 `min_max_alpha_beta` 时，我们传入可能已经更新的新 alpha 和 beta 值作为搜索的一部分：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `side > 0` expression means that we are looking to maximize our score,
    so we will store the score in the alpha variable if it''s better than our current
    alpha:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`side > 0` 表达式意味着我们希望最大化我们的分数，所以如果新的分数比我们当前的 alpha 更好，我们会将分数存储在 alpha 变量中：'
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If `side` is < 0 we are minimizing, so store the lowest scores in the beta
    variable:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `side` 是 < 0，我们在进行最小化，所以把最低分数存储在 beta 变量中：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If alpha is greater than beta, then this branch cannot improve the current
    score, so we stop searching it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 alpha 大于 beta，那么这个分支不能改善当前的分数，所以我们停止搜索：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In 1997, IBM created a chess program called *Deep Blue*. It was the first to
    beat the reigning world chess champion Garry Kasparov. While an amazing achievement,
    it would be hard to call *Deep Blue* intelligent. Though, it has huge computational
    power, and its underlying algorithm is just the same min-max algorithm from the
    50's. The only major difference is that *Deep Blue* took advantage of the opening
    theory in chess.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 1997 年，IBM 创建了一个名为*深蓝*的国际象棋程序。它是第一个击败现任世界象棋冠军加里·卡斯帕罗夫的程序。虽然这是一个了不起的成就，但很难称*深蓝*具有智能。尽管它具有巨大的计算能力，但其基础算法只是上世纪
    50 年代的 min-max 算法。唯一的主要区别是*深蓝*利用了国际象棋的开局理论。
- en: The opening theory comprises of a sequences of moves that are from the starting
    position and are known to lead to favorable or unfavorable positions. For example,
    if white starts with the move pawn e4 (the pawn in front of the king moved forward
    by two spaces), then black responds with pawn c5; this is known as the Sicilian
    defense, and there are many books written on the sequences of play that could
    follow from this position. Deep Blue was programmed to simply follow the best
    moves recommended from these opening books and only start calculating the best
    min-max move once the opening line of play reaches its end. In this way, it saves
    on computational time, but it also takes advantage of the vast human research
    that has gone into the working out of the best positions in the opening stages
    of chess.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 开局理论由一系列从起始位置开始的走法组成，这些走法被认为会导致有利或不利的局面。例如，如果白方棋手以e4（王前的兵向前移动两格）开局，那么黑方应该回应c5，这就是西西里防御，对于这个局面，有许多书籍介绍接下来可能出现的走法。深蓝计算机只是简单地遵循这些开局书籍推荐的最佳走法，并且只在开局走法结束时开始计算最佳的极小极大走法。这样，它既省去了计算时间，也利用了人类在国际象棋开局阶段找到最佳局面所进行的大量研究。
- en: Learning a value function
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习一个价值函数
- en: Let's get a bit more details on exactly how much computation the min max algorithm
    has to do. If we have a game of breadth *b* and depth *d*, then evaluating a complete
    game with min-max would require the construction of a tree with eventual *d* ^(*b*)
    leaves. If we use a max depth of *n* with an evaluation function, it would reduce
    our tree size to *n* ^(*b*). But this is an exponential equation, and even though
    *n* is as small as 4 and *b* as 20, you still have 1,099,511,627,776 possibilities
    to evaluate. The tradeoff here is that as *n* gets lower, our evaluation function
    is called at a shallower level, where it may be a lot less good than the estimated
    quality of the position. Again, think of chess where our evaluation function is
    simply counting the number of pieces left on the board. Stopping at a shallow
    point may miss the fact that the last move put the queen in a position where it
    could be taken in the following move. Greater depth always equals greater accuracy
    of evaluation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对极小极大算法需要计算的具体数量进行更详细的了解。如果我们的游戏广度为*b*，深度为*d*，那么使用极小极大评估一个完整游戏需要构建一棵树，最终有*d*^(*b*)个叶子。如果我们使用最大深度*n*和一个评估函数，它将把我们的树大小减小到*n*^(*b*)。但这是一个指数方程，即使*n*只有4，*b*为20，你仍然有1,099,511,627,776种可能性需要评估。这里的权衡是，随着*n*的降低，我们的评估函数在较浅的层次上被调用，这可能比局面的预估质量要差得多。再一次，以国际象棋为例，我们的评估函数只是简单地统计棋盘上剩下的棋子数量。在较浅的位置停止可能会忽略最后一步将皇后放在可以在下一步中被吃掉的位置的事实。更大的深度总是意味着更准确的评估。
- en: Training AI to master Go
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练AI掌握围棋
- en: The number of possibilities in chess, though vast, is not so vast that with
    a powerful computer, you can't defeat the world's greatest human player. Go, an
    ancient Chinese game whose origin goes back to more than 5,500 years, is far more
    complex. In Go, a piece can be placed anywhere on the 19 x 19 board. To begin
    with, there are 361 possible moves. So to search forward *k* moves, you must consider
    361k possibilities. To make things even more difficult, in chess, you can evaluate
    how good a position is fairly accurately by counting the number of pieces on each
    side, but in Go, no such simple evaluation function has been found. To know the
    value of a position, you must calculate through to the end of the game, some 200+
    moves later. This makes the game impossible to play to a good standard using min-max.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 国际象棋中的可能性虽然很多，但并不是如此之多，以至于用一台强大的计算机无法击败世界上最伟大的人类棋手。围棋是一种源远流长的中国古老游戏，其起源可以追溯到5500多年前，远比国际象棋复杂得多。在围棋中，一子可以放在19
    x 19的棋盘上的任何地方。首先有361个可能的走法。因此，要往前搜索*k*步，你必须考虑361k种可能性。使情况更加困难的是，在国际象棋中，你可以通过计算每一方的棋子数量相对精确地评估一个局势的好坏，但在围棋中，没有找到这样简单的评估函数。
    要知道一个局势的价值，你必须计算到游戏结束，再往后走200多步。这使得游戏通过极小极大来达到一个良好水平几乎是不可能的。
- en: '![Training AI to master Go](img/00261.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![训练AI掌握围棋](img/00261.jpeg)'
- en: Figure 5
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: To get a good feel of the complexity of Go, it is worth thinking about how humans
    learn to play Go versus Chess. When beginners starts learning Chess, they make
    a series of moves in the direction of their opponent's side of the board. At some
    point, they make a move that leaves one of their pieces open for capture. So the
    opponent obliges and takes the piece. It is then that the beginner player immediately
    understands that their last move was bad, and if they want to improve, they cannot
    make the same mistake again. It is very easy for the player to identify what they
    did wrong, though correcting yourselves consistently may require a lot of practice.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解围棋的复杂性，值得思考人类学习围棋与国际象棋的方式。当初学国际象棋时，新手们会在棋盘向对手方向的一系列移动中前进。在某个时刻，他们会做出一步让自己的棋子暴露给对方吃掉的移动。于是对手就会乐意接受并吃掉这个棋子。这时新手玩家立刻就会意识到他们上一步走得不好，如果想要提高，就不能再犯同样的错误。对于玩家来说很容易找出他们做错了什么，尽管要一直自我纠正可能需要大量的实践。
- en: Alternatively, when a beginner learns Go, it looks like a series of almost random
    moves across the board. At a certain point, both players run out of their moves
    and the position is counted up to see who won. The beginner finds out he has lost
    and stares at the mass of pieces in different positions and scratches his head
    wondering exactly what happened. For humans, Go is incredibly difficult and takes
    a high degree of experience and skill to be able to understand where players are
    going wrong.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当初学围棋时，它看起来就像是棋盘上一系列几乎是随机的移动。在某个时刻，双方玩家都用完了他们的棋子，然后计算局面以确定谁赢了。初学者发现自己输了，盯着摆在不同位置的一堆棋子，想弄清楚到底发生了什么。对于人类来说，围棋是极其困难的，需要高度的经验和技巧才能理解玩家出错的地方。
- en: Also, Go doesn't have anything like the opening theory books that Chess has.
    Go's opening theory rather than being sequences of moves that a computer could
    follow is lots of general principles instead, such as good shapes to aim for or
    ways to take corners of the board. There is something called *Joseki* in Go, which
    are studied sequences of moves known to lead to different advantages. But all
    of these must be applied to that context when a player recognizes a particular
    arrangement is possible; they are not actions that can be blindly followed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，围棋没有像国际象棋那样的开局理论书籍。围棋的开局理论不是一系列计算机可以遵循的移动序列，而是许多通用原则，例如要追求的良好形状或者控制棋盘角落的方法。围棋中有一种叫做*定式*的东西，它是研究出的一系列走法，已知会导致不同的优势。但所有这些都必须在玩家意识到可能存在某种特定局面时应用；它们不是可以盲目遵循的动作。
- en: 'One approach for games such as Go, where evaluation is so difficult, is **Monte
    Carlo Tree Search** (**MCTS**). If you have studied Bayesian probability, you
    will have heard of Monte Carlo sampling. This involves sampling from a probability
    distribution to obtain an approximation for an intractable value. MCTS is similar.
    A single sample involves randomly selecting actions for each player until you
    reach a terminal state. We maintain statistics for each sample so that after we
    are done, we can select the action from the current state with the highest mean
    success rate. Here is an example of MCTS for the tic tac toe game we spoke about.
    The complete code can also be found in the GitHub repo in the `monte_carlo.py`
    file:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于围棋等游戏，评估如此困难的一个方法是**蒙特卡罗树搜索**（**MCTS**）。如果你学过贝叶斯概率，你会听说过蒙特卡罗采样。这涉及从概率分布中采样以获得无法计算的值的近似值。MCTS类似。一个样本包括随机选择每个玩家的动作，直到达到终局状态。我们维护每个样本的统计数据，这样在完成后，我们就可以从当前状态中选择具有最高平均成功率的动作。这是我们之前提到的井字棋游戏的MCTS示例。完整的代码也可以在GitHub存储库的
    `monte_carlo.py` 文件中找到：
- en: '[PRE28]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `monte_carlo_sample` method here generates a single sample from a given
    position. Again, we have a method that has `board_state` and `side` as arguments.
    This method will be called recursively until we reach a terminal state, so either
    a draw because no new move can be played or a win for one player or another:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `monte_carlo_sample` 方法从给定位置生成一个样本。同样，我们有一个方法，它的参数是 `board_state` 和 `side`。这个方法将被递归调用，直到我们达到一个终局状态，所以要么是平局因为不能再下新的棋了，要么是某一方玩家赢了：
- en: '[PRE29]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A move will be selected randomly from the legal moves in the position, and
    we will recursively call the sample method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将从局面中的合法移动中随机选择一个移动，并递归调用样本方法：
- en: '[PRE30]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Take monte carlo samples from this board state and update our results based
    on them:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个棋盘状态中取出蒙特卡罗样本，并根据它们更新我们的结果：
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Get the move with the best average result:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 获得同样结果最佳走法：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is the method that brings it all together. We will call the `monte_carlo_smaple`
    method `number_of_samples` times, keeping track of the result of each call. We
    then return the move with the best average performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是将所有内容整合在一起的方法。我们将调用`monte_carlo_sample`方法`number_of_samples`次，跟踪每次调用的结果。然后我们返回平均表现最佳的走法。
- en: It is good to think about how different the results obtained from MCTS will
    be of those that involve min-max. If we go back to chess as an example, in the
    position illustrated, white has a winning move, putting the rook on the back rank,
    c8, to give mate. Using min-max, this position would be evaluated as a winning
    position for white. But using MCTS, given that all other moves here lead to a
    probable victory for black, this position will be rated as favorable to black.
    This is why MCTS is very poor at chess and should give you a feel of why MCTS
    should only be used when Min-Max is not viable. In Go, which falls into the other
    category, the best AI performance was traditionally found using MCTS.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下MCTS得到的结果与涉及最小最大的结果有多大不同是很有意义的。如果我们以国际象棋为例，以这个局面来说，白方有一个获胜的着法，将车移到后排c8，将黑方将军。使用最小最大算法，这个局面会被评价为白方获胜的局面。但是使用MCTS，考虑到这里的所有其他走法都会导致黑方潜在的胜利，这个局面将被评价为对黑方有利。这就是为什么MCTS在国际象棋中表现很差，并且应该让你感受到为什么只有在最小最大算法不可行时才应该使用MCTS。在围棋这类游戏中，传统上使用MCTS找到了最佳的人工智能表现。
- en: '![Training AI to master Go](img/00262.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![训练AI精通围棋](img/00262.jpeg)'
- en: Figure 6:A Chess position that is badly evaluated by Monte Carlo sampling. If
    white is to move, they have a winning move; however, if the samples randomly move,
    black has an opportunity to win
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个被蒙特卡洛采样严重低估的国际象棋局面。如果轮到白走，他们有一个获胜的着法；但是，如果随机走子，黑方有获胜的机会。
- en: Upper confidence bounds applied to trees
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将置信上界应用到树结构
- en: To recap, Min-Max gives us the actual best move in a position, given perfect
    information; however, MCTS only gives an average value; though it allows us to
    work with much larger state spaces that cannot be evaluated with Min-Max. Is there
    a way that we could improve MCTS so it could converge to the Min-Max algorithm
    if enough evaluations are given? Yes, Monte Carlo Tree Search with Confidence
    bounds applied to Trees (UCT) does exactly this. The idea behind it is to treat
    MCTS like a multiarmed bandit problem. The multiarmed bandit problem is that we
    have a group of slot machines—one armed bandits—each of which has an undetermined
    payout and average amount of money received per play. The payout for each machine
    is random, but the mean payout may vary significantly. How should we determine
    which slot machines to play?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，最小最大算法可以给出具体的最佳着法，假设有完美的信息；但是MCTS只给出一个平均值；尽管它允许我们处理无法用最小最大算法评估的更大状态空间。有没有办法改进MCTS，使其在给出足够的评价时能收敛到最小最大算法？是的，置信上界应用到树结构的蒙特卡洛树搜索（UCT）确实可以做到这一点。其背后的想法是把MCTS看作是一个多臂老虎机问题。多臂老虎机问题是我们有一组老虎机——单臂老虎机——每台机器都有一个未确定的赔付和每次游戏的平均赔付金额。每台机器的赔付是随机的，但平均赔付金额可能差异很大。我们该如何确定要玩哪些老虎机？
- en: There are two factors that need to be considered when choosing a slot machine.
    The first is the obvious one, an exploitative value, which is the expected return
    that the given slot machine will output. To maximize the payout, we would need
    to always play the machine with the highest expected payout. The second is the
    explorative value, where we want our playing machine to increase the information
    we have about the payoffs of different machines.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择老虎机时需要考虑两个因素。第一点是显而易见的，即利用价值，也就是给定老虎机预期的回报。为了最大化赔付，我们需要始终玩出预期赔付最高的机器。第二点是探索价值，我们希望我们玩的机器增加我们对不同机器赔付的信息。
- en: If we play machine *A* thrice, you get a payoff of 13, 10, and 7 for an average
    payoff of 10\. We also have machine *B*; we have played it once and have gotten
    a payoff of 9\. In this case, it might be preferable to play machine *B* because
    though the average payoff is lower, 9 versus 10\. The fact that we have only played
    it once means the lower payout may have just been bad luck. If we play it again
    and get a payout of 13, our average for machine B would be 11\. Therefore, we
    should switch to playing that machine for the best payout.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们玩机器*A*三次，你将得到13、10和7的回报，平均回报为10。我们也有机器*B*；我们已经玩了它一次，得到了9的回报。在这种情况下，可能更倾向于玩机器*B*，因为尽管平均回报较低，为9对10。我们只玩了一次的事实意味着较低的支付可能只是运气不佳。如果我们再次玩它并得到13的回报，机器B的平均为11。因此，我们应该切换到玩那台机器以获得最佳回报。
- en: 'The multiarmed bandit problem has been widely studied within mathematics. If
    we can reframe our MCTS evaluation to look like a multiarmed bandit problem, we
    can take advantage of these well-developed theories. One way of thinking about
    it is rather than seeing the problem as one with maximizing reward, think of it
    as a problem with minimizing regret. Regret here is defined as the difference
    between the reward we get for the machine we play and the maximum possible reward
    we would get if we knew the best machine from the beginning. If we follow a policy,
    *p(a)* chooses an action that gives a reward at each time step. The regret for
    *t* number of plays, given *r** as the reward of the best possible action, is
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题在数学领域得到了广泛研究。如果我们重构我们的MCTS评估，使其看起来像一个多臂老虎机问题，我们就可以利用这些成熟的理论。一种思考方式是，与其将问题视为最大化奖励，不如将其视为最小化遗憾的问题。这里的遗憾定义为我们为我们玩的机器获得的奖励与如果我们从一开始就知道最佳机器会得到的最大可能奖励之间的差异。如果我们遵循一项政策，*p(a)*每次选择一个能给予奖励的动作。给定*r*为最佳可能行动的奖励的*t*次玩后的遗憾如下：
- en: '![Upper confidence bounds applied to trees](img/00263.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![应用于树的上置信界限](img/00263.jpeg)'
- en: If we were to choose a policy of always picking the machine with the highest
    reward, it may not be the true best machine. Therefore, our regret will increase
    linearly with each play. Similarly, if we take a policy of always trying to explore
    for finding the best machine, our regret will also increase linearly. What we
    want is a policy for *p(a)* that increases in sublinear time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个始终选取奖励最高的机器的政策，它可能并不是真正的最佳机器。因此，我们的遗憾会随着每次玩而线性增加。同样，如果我们采取一个始终试图探索以找到最佳机器的政策，我们的遗憾也会线性增加。我们希望的是一项*p(a)*的政策，其增长呈次线性时间。
- en: The best theoretical solution is to perform the search based on confidence intervals.
    A confidence interval is the range within which we expect the true mean, with
    some probability. We want to be optimistic in the face of uncertainty. If we don't
    know something, we want to find it out. The confidence interval represents our
    uncertainty about the true mean of a given random variable. Select something based
    on your sample mean plus the confidence interval; it will encourage you to explore
    the space of possibilities while also exploiting it at the same time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的理论解决方案是根据置信区间执行搜索。置信区间是我们期望真实均值落在其中的范围，具有一定的概率。在面对不确定性时，我们想要保持乐观。如果我们不知道某件事，我们想要找出答案。置信区间代表了我们对给定随机变量的真实均值的不确定性。根据你的样本均值加上置信区间选择某样本，这将鼓励你探索可能性的空间，并同时加以利用。
- en: 'For an i.i.d random variable *x*, in the range of 0 to 1, over n samples, the
    probability that the true mean is greater than the sample mean—![Upper confidence
    bounds applied to trees](img/00264.jpeg) plus constant *u*—is given by Hoeffding''s
    inequality: Hoeffding, Wassily (1963). *Probability inequalities for sums of bounded
    random variables*. Journal of the American Statistical Association:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于i.i.d在0到1范围内的随机变量*x*，在n个样本上，真实均值大于样本均值的概率，即![应用于树的上置信界限](img/00264.jpeg)加上常数*u*，由Hoeffding不等式给出：Hoeffding,
    Wassily (1963). *有界随机变量之和的概率不等式*，美国统计协会杂志：
- en: '![Upper confidence bounds applied to trees](img/00265.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![应用于树的上置信界限](img/00265.jpeg)'
- en: 'We want to use this equation to find the upper bound confidence for each machine.
    *E{x},x*, and *n* are all part of statistics we have already. We need to solve
    it to use it for the purpose of finding a value for *u*. In order to do this,
    reduce the left side of the equation to p and find where it equals the right side:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用这个方程来找到每台机器的上界置信度。* E {x}, x *, 和 * n *都是我们已经有的统计学的一部分。我们需要解这个方程来计算一个值
    * u *。为了做到这一点，把方程的左边化简为 p，并找到它与右边相等的点：
- en: '![Upper confidence bounds applied to trees](img/00266.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![上置信界应用于树中](img/00266.jpeg)'
- en: 'We can rearrange it to make *u* defined in terms of *n* and *p*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重排它，让*u*用*n*和*p*表示：
- en: '![Upper confidence bounds applied to trees](img/00267.jpeg)![Upper confidence
    bounds applied to trees](img/00268.jpeg)![Upper confidence bounds applied to trees](img/00269.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![上置信界应用于树中](img/00267.jpeg)![上置信界应用于树中](img/00268.jpeg)![上置信界应用于树中](img/00269.jpeg)'
- en: 'Now we want to choose a value for *p* so that our precision increases over
    time. If we set ![Upper confidence bounds applied to trees](img/00270.jpeg), then
    as n approaches infinity, our regret will tend toward 0\. Substitute that in and
    we can simplify it down to:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们希望选择一个* p *的值，这样我们的精度随时间增加而提高。如果我们设![上置信界注入到树中](img/00270.jpeg)，那么当n趋近无穷大时，我们的损失将趋向于0。代入这个值，我们可以简化为：
- en: '![Upper confidence bounds applied to trees](img/00271.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![上置信界应用于树中](img/00271.jpeg)'
- en: 'The mean plus u is our upper confidence bounds, so we can use it to give us
    the **UCB1** (**Upper Confidence Bounds**) algorithm. We can substitute our values
    with the values in the multiarmed bandit problem we saw earlier, where *r* [*i*]
    is the sum of the reward received from the machine *i*, *n* [*i*] is the number
    of plays of machine *i*, and *n* is the sum of plays across all machines:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 均值加上u是我们的上界置信界，所以我们可以用它来得到**UCB1**（**上置信界**）算法。我们可以用这些值代替我们之前在多臂老虎机问题中看到的值，其中*
    r * [* i *] 是从机器*i*得到的奖励的总和，* n * [* i *]是机器*i*的玩的次数，* n *是所有机器的总玩的次数：
- en: '![Upper confidence bounds applied to trees](img/00272.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![上置信界应用于树中](img/00272.jpeg)'
- en: We will always want to choose the machine that will give us the highest score
    for this equation. If we do so, our regret will scale logarithmically with the
    number of plays, which is the theoretical best we can do. Using this equation
    for our action choice has the behavior that we will try a range of machines early
    on, but the more we try a single machine, the more it will encourage us to eventually
    try a different machine.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是希望选择能为我们带来最高分数的机器。如果我们这样做，我们的损失将以对数的方式随着玩的次数增加，这是我们可以实现的理论最佳情况。使用这个方程做出我们的行动选择会导致这样的行为，我们在早期会尝试各种各样的机器，但我们越多地尝试单一机器，它就会更鼓励我们最终尝试不同的机器。
- en: It's also good to remember that an assumption at the beginning of this series
    of equations was that the range, for x in early equations, and *r* for when we
    apply it to the multiarmed bandit problem was that values were in the range of
    0 to 1\. So if we are not working in this range, we need to scale our input. We
    have not made any assumptions about the nature of the distribution though; it
    could be Gaussian, binomial, and so on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 还要记住，这一系列方程的假设是在早期方程中的x的范围，以及当我们将其应用到多臂老虎机问题时的* r *，它们的值都在0到1的范围内。所以，如果我们的工作不在这个范围内，我们需要缩放我们的输入。不过，我们并没有做出任何关于分布性质的假设；它可以是高斯的、二项式的等等。
- en: Now we have an optimal solution to the problem of sampling from a set of unknown
    distributions; how do you apply it to MCTS? The simplest way to do this is to
    only treat the first moves from the current board state as bandits or slot machines.
    Though this would improve the estimation at the top level a little, every move
    beneath that would be completely random, meaning the *r* [*i*] estimation would
    be very inaccurate.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了从一组未知分布中采样的最优解决方案；我们如何将其应用于MCTS呢？最简单的方法是将当前棋盘状态的第一个移动视为老虎机或投币机。尽管这样会稍微提高顶层的估计，但下面的每一步都会完全随机，这意味着*
    r * [* i *] 的估计将会非常不准确。
- en: Alternatively, we could treat every move at every branch of the tree as a multiarmed
    bandit problem. The issue with this is that if our tree is very deep, as our evaluation
    goes deeper, we will reach positions we have never encountered before so we would
    have no samples for the range of moves we need to choose between. We would be
    keeping a huge number of statistics for a huge range of positions, most of which
    will never be used.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将树的每个分支上的每个移动都视为一个多臂赌博机问题。问题在于，如果我们的树非常深，随着我们的评估越深入，我们将到达我们以前从未遇到过的位置，因此我们将没有样本用于我们需要在其中选择的移动范围。我们将为大范围的位置保留大量统计数据，其中大多数将永远不会被使用。
- en: The compromise solution, known as Upper Confidence for Trees, is to do what
    we discuss next. We will do successive rollouts from the current board state.
    At each branch of the tree, where we have a range of actions to choose from, if
    we have previous sample statistics for each potential move, we will use the UCB1
    algorithm to choose which action to choose for the rollout. If we do not have
    sample statistics for every move, we will choose the move randomly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 妥协解决方案称为树的上限置信，是我们接下来要讨论的内容。我们将从当前棋盘状态开始进行连续的模拟。在树的每个分支处，我们有一系列可供选择的操作，如果我们对每个潜在移动都有先前的样本统计数据，我们将使用
    UCB1 算法来选择用于模拟的动作。如果我们没有每个移动的样本统计数据，我们将随机选择移动。
- en: How do we decide which sample statistics to keep? For each rollout, we keep
    new statistics for the first position we encounter that we do not have previous
    statistics for. After the rollout is complete, we update the statistics for every
    position we are keeping track of. This way, we ignore all the positions deeper
    down the rollout. After x evaluations, we should have exactly *x* nodes of our
    tree, growing by one with each rollout. What's more, the nodes we keep track of
    are likely to be around the paths we are using the most, allowing us to increase
    our top-level evaluation accuracy by increasing the accuracy of the moves we evaluate
    further down the tree.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何决定保留哪些样本统计数据？对于每次模拟，我们为我们以前没有统计数据的第一个遇到的位置保留新的统计数据。完成模拟后，我们更新我们跟踪的每个位置的统计数据。这样，我们忽略了模拟深处的所有位置。经过
    x 次评估，我们应该恰好有 *x* 个树节点，每次模拟增加一个。更重要的是，我们跟踪的节点可能位于我们最常使用的路径周围，使我们能够通过增加我们在树深处评估的移动的准确性来增加我们的顶层评估准确性。
- en: 'The steps are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Start a rollout from the current board state. When you select a move, do the
    following:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从当前棋盘状态开始进行一次模拟。当你选择一个移动时，请执行以下操作：
- en: If you have statistics for every move from the current position, use the UCB1
    algorithm to choose the move.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你对当前位置的每一步都有统计数据，就使用 UCB1 算法来选择移动。
- en: Otherwise, choose the move randomly. If this is the first randomly chosen position,
    add it to the list of positions we are keeping statistics for.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，随机选择移动。如果这是第一个随机选择的位置，则将其添加到我们正在保留统计数据的位置列表中。
- en: Run the rollout until you hit a terminal state, which will give you the result
    of this rollout.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模拟，直到达到终止状态，这将给出这次模拟的结果。
- en: Update the statistics for every position you are keeping statistics for, indicating
    what you went through in the rollout.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新你正在保留统计数据的每个位置的统计数据，指示你在模拟中经历了什么。
- en: Repeat until you get to the maximum number of rollouts. Upper confidence bounds
    applied to Trees, the statistics for each position, are shown in the square boxes:![Upper
    confidence bounds applied to trees](img/00273.jpeg)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复，直到达到最大模拟次数。应用于树的上限置信边界，每个位置的统计数据显示在方框中：![应用于树的上限置信边界](img/00273.jpeg)
- en: The preceding diagram illustrates how this happens. In position A, there is
    statistics collected for all four possible moves. Because of this, the UCB1 algorithm
    can be used to select the best move, balancing exploitative for exploitative value.
    In the preceding diagram, the leftmost move is chosen. This leads us to **position
    B**; here only two out of the three possible moves have statistics collected on
    them. Because of this, the move you need to make for this rollout is selected
    randomly. By chance, the rightmost move is selected; the remaining moves are selected
    randomly until you reach the final **position C**, where the noughts were to win.
    This information is then applied to a graph, as shown in the following diagram:![Upper
    confidence bounds applied to trees](img/00274.jpeg)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上图说明了这是如何发生的。在位置 A，四个可能的移动都收集了统计数据。因此，我们可以使用 UCB1 算法来选择最佳移动，平衡开发性和探索性的价值。在上图中，选择了最左边的移动。这将我们带到**位置
    B**；在这里，只有三个可能的移动中的两个收集了统计数据。因此，你需要为这次模拟随机选择一个移动。由于巧合，选择了最右边的移动；剩下的移动是随机选择的，直到到达最终的**位置
    C**，在那里圈圈玩家获胜。然后，将这些信息应用于一个图表，如下图所示：![应用于树的置信上界](img/00274.jpeg)
- en: We add statistics for any position that we passed through that already has statistics,
    so 1/2 in the first diagram now becomes 2/3\. We also add statistics for the first
    position we encounter with no stats. Here, it is the rightmost position in the
    second row; it now has a score of 1/1 because the nought player won. If this branch
    is selected again and you get to position D, use the UCB1 algorithm to select
    the move, not just make a random selection as before.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们会为我们经过且已经有统计数据的任何一个位置添加统计信息，因此第一个图表中的 1/2 现在变成了 2/3。我们还会为我们遇到的第一个没有统计信息的位置添加统计信息。在这里，它是第二行中最右边的位置；它现在的分数是
    1/1，因为圈圈玩家赢了。如果再次选择这条分支并且你到达位置 D，使用 UCB1 算法来选择移动，而不是随机选择。
- en: 'Here is what this looks like in Python for our *Tic-Tac-Toe* game:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们的*井字棋*游戏在 Python 中的实现：
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'First, we need a method that calculates UCB1; this is the UCB1 equation in
    Python. The one difference is here we are using `log_total_samples` as input because
    it allows us to do a small optimization later on:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个计算 UCB1 值的方法；这是在 Python 中的 UCB1 公式。唯一的区别是这里我们使用`log_total_samples`作为输入，因为它允许我们稍后进行小优化：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Declare the method and the two dictionaries, namely `state_results` and `state_samples`.
    They will keep track of our statistics for the different board states we will
    encounter during the rollouts:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 声明该方法和两个字典，即`state_results`和`state_samples`。它们将跟踪我们在模拟期间遇到的不同棋盘状态的统计信息：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The main loop is what we go through for each rollout. At the beginning of the
    rollout, we need to initialize the variables that will track our progress within
    the rollout. `first_unvisited_node` will keep track of whether we have created
    a new statistics tracking node for this rollout. On encountering the first state
    for which we have no statistics, we create the new statistics node, adding it
    to `state_results` and `state_samples` dictionaries and then setting the variable
    to `False`. `rollout_path` will keep track of each node we visit in this rollout
    that we are keeping statistics nodes for. Once we obtain the result at the end
    of the rollout, we will update the statistics of all the states along the path:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 主循环是我们每次模拟都会经历的过程。在模拟开始时，我们需要初始化将跟踪模拟进展的变量。`first_unvisited_node`将跟踪我们是否为此次模拟创建了一个新的统计跟踪节点。当遇到第一个没有统计信息的状态时，我们创建新的统计节点，将其添加到`state_results`和`state_samples`字典中，并将变量设置为`False`。`rollout_path`将跟踪我们在此次模拟中访问的每个节点，这些节点是我们保留了统计节点的节点。当我们在模拟结束时获得结果时，我们将更新沿路径的所有状态的统计信息：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `while result == 0` puts us into the loop for a rollout; this will run
    until one side or the other wins. In each loop of the rollout, we first construct
    a dictionary, `move_states`, mapping each available move to the state that move
    will put us into. If there are no moves to make, we are in a terminal state, it
    is a draw. So you need to record that as `result` and break out of the rollout
    loop:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当`result == 0`时，我们进入模拟的循环；这将一直运行，直到一方胜利。在模拟的每个循环中，我们首先构造一个字典`move_states`，将每个可用的移动映射到该移动将带我们进入的状态。如果没有可以进行的移动，那么我们处于终止状态，这是一局平局。所以你需要将其记录为结果，并跳出模拟循环：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we need to choose which move we are going to take at this step of the rollout.
    As specified by the MCTS-UCT algorithm, if we have statistics for every possible
    move, we choose the move with the best `upper_confidence_bounds` score; otherwise,
    we make the selection randomly:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要选择在这次投掷中要采取的棋步。根据MCTS-UCT算法的规定，如果我们对每个可能的移动都有统计数据，我们选择具有最佳`upper_confidence_bounds`得分的移动；否则，我们随机选择。
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that we have selected our move, we can update `current_board_state` to
    the state that the move puts us in:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了我们的走步，我们可以将`current_board_state`更新为移动将我们置于的状态：
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we need to check whether we have hit the end of our MCTS-UCT tree. We will
    add every node we visit up to the first previously unvisited node to `rollout_path`.
    We will update the statistics of all these nodes once we get our result from this
    rollout:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要检查我们是否已经到达了MCTS-UCT树的末端。我们将向`rollout_path`中添加我们访问的每个节点，直到第一个之前未访问的节点。一旦我们从这次投掷中得到结果，我们将更新所有这些节点的统计数据。
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We are at the end of our rollout loop, so switch the sides for the next iteration
    and check whether anyone has won in the current state. If so, it will cause us
    to break out of the rollout loop when we pop back to the `while result == 0` statement:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处于我们的投掷循环的最后，所以下一次迭代时要改变双方的位置，并检查当前状态是否有人获胜。如果是的话，当我们回到`while result == 0`语句时，它将导致我们退出投掷循环：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we have completed a single rollout and thus left the rollout loop. We now
    need to update our statistics with the result. `rollout_path` contains `path_board_state`
    and `path_side` for each node we want to update, so we need to go through every
    entry in there. The last two points to make are that the results from our game
    are between -1 and 1\. But the UCB1 algorithm expects its payouts between 0 and
    1; the line result `*path_side/2.+.5` does this. Second, we also need to switch
    the results to represent the side they are for. A good move for my opponent is
    the opposite of a good move for me:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了一次投掷，离开了投掷循环。我们现在需要用结果更新我们的统计数据。`rollout_path`中包含要更新的每个节点的`path_board_state`和`path_side`，因此我们需要遍历其中的每个条目。最后需要指出的两点是，我们的游戏结果介于-1和1之间。但是UCB1算法期望其支付在0和1之间；行`result*path_side/2.+.5`就做到了这一点。其次，我们还需要转换结果以代表它们所代表的一方。对我来说一个好棋步是对手的坏棋步的对立面：
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Finally, once we have done the required number of rollouts, we can choose the
    best move from the current state based on the best expected payout. There is no
    longer any need to use UCB1 to choose the best move. It's because this being the
    final decision, there is no value in doing any extra exploration; the best move
    is simply the best mean payout.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦完成了所需数量的投掷，我们可以基于最佳预期报酬从当前状态选择最佳的走步。不再需要使用UCB1来选择最佳走步。因为这是最终决定，不需要进行额外的探索，最佳走步就是最佳平均报酬。
- en: This is the MCTS-UCT algorithm. There are many different variants to it with
    different advantages in specific situations, but they all have this as core logic.
    MCTS-UCT gives us a general way to judge moves for games, such as Go, with vast
    search spaces. Also, it isn't limited to games of perfect information; it can
    often perform well in games with partially observed states, such as poker. Or,
    even more generally, any problem we might encounter that we can reconfigure to
    fit it, for example, it was used as a basis for an automated theorem proving machine.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是MCTS-UCT算法。它有许多不同的变体，针对特定情况具有不同的优势，但它们都有这个作为核心逻辑。MCTS-UCT给了我们一种一般的方式来评判类似围棋这样具有庞大搜索空间的游戏的走步。而且，它并不仅限于完全信息的游戏；在具有部分观察状态的游戏中，如扑克牌游戏中，它通常也表现良好。甚至更一般地说，任何我们遇到的可以重新配置适应它的问题，例如，它被用作自动定理证明机器的基础。
- en: Deep learning in Monte Carlo Tree Search
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索中的深度学习
- en: Even with MCTS-UCT, computers could still not even come close to beating the
    best Go players; however, in 2016, a team from *Google Deep Mind* developed an
    AI they called AlphaGo. It defeated Lee Sedol, the world's top Go player, over
    a five game series, winning 4-1\. The way they did this was using three improvements
    over the standard MCTS UCT approach.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用了MCTS-UCT算法，计算机仍然无法与最优秀的围棋选手相提并论；然而，在2016年， *Google Deep Mind* 团队开发了一个名为AlphaGo的人工智能。它在五局比赛中击败了世界顶尖的围棋选手李世石，以4-1的比分获胜。他们这样做的方式是在标准MCTS
    UCT方法的基础上进行了三项改进。
- en: If we were to think about why MCTS is so inaccurate, an intuitive answer that
    might arise is that the moves used in the evaluation are selected randomly when
    we know that some moves are much more likelier than others. In Go, when there
    is a battle for control of a corner, the moves around that area are much better
    candidates for selection, as opposed to moves on the opposite side of the board.
    If we had a good way of selecting which moves are likely to be played, we would
    have massively reduced the breadthof our search, and by extension, increased the
    accuracy of our MCTS evaluations. If we go back to the preceding chess position,
    although every legal move can potentially be played, if you are playing against
    someone who without any chess skill will only play the winning move, evaluation
    of the other moves is simply wasted CPU cycles.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们思考为什么MCTS如此不准确，一个直观的答案可能是，评估中使用的动作是随机选择的，而我们知道某些动作比其他动作更有可能。在围棋中，当争夺角落的控制权时，该区域周围的动作是更好的选择，而不是棋盘另一侧的动作。如果我们有一种良好的方法来选择哪些动作可能被下，我们将大大减少搜索的广度，从而增加我们MCTS评估的准确性。如果我们回到前面的国际象棋局面，尽管每个合法的动作理论上都可以被下，但是如果你对手没有任何国际象棋技巧，只会下赢棋的动作，评估其他动作就是在浪费CPU周期。
- en: This is where deep learning can help us. We can use the pattern recognition
    qualities of a neural network to give us a rough estimate of the probability of
    a move being played in the game, given a position. For AlphaGo, a 13-layer convolutional
    network with relu activation functions was used. The input to the network was
    the 19 x 19 board state and its output, another 19 x 19 softmax layer representing
    the probability of a move being played in each square of the board. It was then
    trained on a large database of expert-level human Go games. The network would
    be given a single position as input and the move that was played from that position
    as a target. The loss function is the mean squared error between network activation
    and the human move made. Given plenty of training, the network learned to predict
    human moves with 57 percent accuracy against a test set. The use of a test set
    here is particularly important because overfitting is a big worry. Unless the
    network can generalize its understanding of a position to a previously unseen
    position, it is useless.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以帮助我们解决这个问题。我们可以利用神经网络的模式识别特性来粗略估计在游戏中给定位置的棋子被下的概率。对于AlphaGo，使用了一个具有13层卷积网络和relu激活函数的网络。网络的输入是19
    x 19的棋盘状态，输出是另一个19 x 19的softmax层，表示每个棋盘方格中下棋的概率。然后，它在大量专家级人类围棋对局的数据库上进行训练。网络将接收一个单一的位置作为输入，以及从该位置下的棋子作为目标。损失函数是网络激活和人类下的棋子之间的均方误差。在充分的训练下，该网络学会了以57%的准确率预测人类下棋的动作。在这里使用测试集特别重要，因为过度拟合是一个大问题。除非网络能够将对一个位置的理解推广到以前未见过的位置，否则它是无用的。
- en: If we wanted to implement something similar in our preceding Tic-tac-toe example,
    we would simply replace the `move = random.choice(moves)` line with the `monte_carlo_sample`
    method or the UCT version with a move chosen by a trained neural network. This
    technique will work for any discrete game if you have a large training set of
    example games.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在前面的井字棋示例中实现类似的东西，我们只需将`move = random.choice(moves)`这行替换为使用由训练过的神经网络选择的动作的`monte_carlo_sample`方法或UCT版本。如果你有一个大型的训练集合例子游戏，这种技术将适用于任何离散游戏。
- en: If you do not have a database of example games, there is another approach you
    can use. If you have an agent that plays with a tiny degree of skill, you can
    even use that agent to generate the initial collection of example games. A good
    approach, for instance, is to generate example positions and moves using the min-max
    or MCTS UCT algorithms. A network can then be trained to play moves from that
    collection. This is a good way to get a network to learn how to play a game at
    a good enough standard so that it can at least explore the space of the game with
    the plausible moves, as opposed to completely random moves.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有例子游戏的数据库，你可以使用另一种方法。如果你有一个稍微有技巧的代理程序，你甚至可以使用该代理程序来生成初始的例子游戏集合。例如，一个好的方法是使用极小-极大或MCTS
    UCT算法来生成例子位置和动作。然后，可以训练一个网络来从该集合中下棋。这是一个很好的方法，可以让网络学会如何以足够高的标准玩游戏，以至于它至少可以探索游戏空间的可能动作，而不是完全随机的动作。
- en: If we implement such a neural network, use it to select which moves to use in
    a Monte-Carlo rollout, with this, our evaluation will be much more accurate, but
    we will still suffer from the problem that our MCTS will be evaluating averages
    when we still care about the best outcome for us from the moves we make. This
    is where reinforcement learning can be introduced to improve our agent.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Quick recap on reinforcement learning
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first encountered reinforcement learning in [Chapter 1](part0016_split_000.html#F8901-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 1. Machine Learning – An Introduction"), *Machine Learning – An Introduction*,
    when we looked at the three different types of learning processes: supervised,
    unsupervised, and reinforcement. In reinforcement learning, an agent receives
    rewards within an environment. For example, the agent might be a mouse in a maze
    and the reward might be some food somewhere in that maze. Reinforcement learning
    can sometimes feel a bit like a supervised recurrent network problem. A network
    is given a series of data and must learn a response.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The key distinction that makes a task a reinforcement learning problem is that
    the responses the agent gives changes the data it receives in future time steps.
    If the mouse turns left instead of right at a *T* section of the maze, it changes
    what its next state would be. In contrast, supervised recurrent networks simply
    predict a series. The predictions they make do not influence the future values
    in the series.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The AlphaGo network has already been through supervised training, but now the
    problem can be reformatted as a reinforcement learning task to improve the agent
    further. For AlphaGo, a new network was created that shares the structure and
    weights with the supervised network. Its training is then continued using reinforcement
    learning and by specifically using an approach called policy gradients.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients for learning policy functions
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem policy gradients aims to solve is a more general version of the
    problem of reinforcement learning, which is how you can use backpropagation on
    a task that has no gradient, from the reward to the output of our parameters.
    To give a more concrete example, we have a neural network that produces the probability
    of taking an action *a*, given a state *s* and some parameters ?, which are the
    weights of our neural network:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients for learning policy functions](img/00275.jpeg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: We also have our reward signal *R*. The actions affect the reward signal we
    take, but there is no gradient between them and the parameters. There is no equation
    in which we can plug *R*; it is just a value we obtain from our environment in
    response to *a*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: However, given that we know there is a link between the *a* we choose and *R*,
    there are a few things we could try. We could create a range of values for our
    ? from a Gaussian distribution and run them in the environment. We could then
    select a percentage of the most successful group and get their mean and variance.
    We then create a new population of ? using the new mean and variance in our Gaussian
    distribution. We can keep doing this iteratively until we stop seeing improvements
    in *R* and then use our final mean as the best choice for our parameters. This
    method is known as the **Cross Entropy Method**.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，鉴于我们知道我们选择的*a*和*R*之间存在链接，有几件事情我们可以尝试。我们可以从高斯分布中创建一个?的值范围并在环境中运行它们。然后我们可以选择最成功的一部分，并获取它们的平均值和方差。然后，我们使用新的均值和方差在我们的高斯分布中创建一个新的?种群。我们可以反复执行此过程，直到在*R*中不再看到改进，然后将我们的最终均值作为参数的最佳选择。这种方法被称为**交叉熵方法**。
- en: Though it can be quite successful, it is a hill-climbing method, which does
    not do a good job of exploring the space of possibilities. It is very likely to
    get stuck in local optima, which is very common in reinforcement learning. Also,
    it still does not take advantage of gradient information.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它可能非常成功，但它是一种爬山法，不能很好地探索可能性空间。它很容易陷入局部最优解，这在强化学习中非常常见。此外，它仍然没有利用梯度信息。
- en: 'To use gradients, we can take advantage of the fact that although there is
    no mathematical relationship between *a* and *R*, there is a probabilistic one.
    Certain *a* taken in certain *s* will tend to receive more *R* than others. We
    can write the problem of getting the gradients of ? with respect to *R* as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度，我们可以利用* a *和* R *之间虽然没有数学关系，但存在概率关系的事实。在某个* s *中采取特定的* a *往往会比其他* R *获得更多的*
    R *。我们可以将获得* R *的?对* R *的梯度的问题写成如下形式：
- en: '![Policy gradients for learning policy functions](img/00276.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![学习策略函数的策略梯度](img/00276.jpeg)'
- en: 'Here, *r* [*t*] is the reward at time step *t*. This can be rearranged into:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*r* [*t*] 是时间步骤 *t* 的奖励。这可以重新排列成：
- en: '![Policy gradients for learning policy functions](img/00277.jpeg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![学习策略函数的策略梯度](img/00277.jpeg)'
- en: 'If we multiply and divide it by ![Policy gradients for learning policy functions](img/00278.jpeg),
    we have the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们乘以并除以![学习策略函数的策略梯度](img/00278.jpeg)，我们有以下结果：
- en: '![Policy gradients for learning policy functions](img/00279.jpeg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![学习策略函数的策略梯度](img/00279.jpeg)'
- en: 'Use the fact ![Policy gradients for learning policy functions](img/00280.jpeg)
    and simplify it to the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用事实![学习策略函数的策略梯度](img/00280.jpeg)并简化为以下形式：
- en: '![Policy gradients for learning policy functions](img/00281.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![学习策略函数的策略梯度](img/00281.jpeg)'
- en: 'What this amounts to is if we nudge our parameters along the log of the direction
    of the gradient of the reward at each time step, we tend to move towards the gradient
    of the reward across all time steps. To implement this in Python, we will need
    to take the following steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是如果我们使参数沿着每个时间步骤的奖励梯度的对数方向推动，我们倾向于向所有时间步骤的奖励梯度移动。要在Python中实现这一点，我们需要执行以下步骤：
- en: Create a neural network whose output is the probability of taking different
    actions, given an input state. In terms of the preceding equations, it will represent
    ![Policy gradients for learning policy functions](img/00278.jpeg).
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个输出是在给定输入状态下采取不同动作的概率的神经网络。根据先前的方程，它将表示![学习策略函数的策略梯度](img/00278.jpeg)。
- en: Run a batch of episodes with our agent running in its environment. Select its
    actions randomly according to the probability distribution output of the network.
    At every time step, record the input state, reward received, and the action you
    actually took.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的代理在其环境中运行的批次中运行若干个训练。根据网络输出的概率分布随机选择其动作。在每个时间步骤，记录输入状态、收到的奖励和实际采取的动作。
- en: At the end of each episode of training, assign rewards to each step using the
    sum of rewards in the episode from that point on. In the case of a game such as
    Go, this will just be 1, 0, or -1 representing the final result applied to each
    step. This will represent *r*[*t*] in the equations. For more dynamic games, discounted
    rewards can be used; discounted rewards will be explained in detail in the next
    chapter.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个训练的最后，使用从该点开始的该训练中的奖励总和为每一步分配奖励。在围棋等游戏中，这将只是一个表示最终结果的1、0或-1应用于每一步的值。这将代表方程中的*r*
    [*t*]。对于更动态的游戏，可以使用折扣奖励；折扣奖励将在下一章中详细解释。
- en: Once we have stored a set number of states running our episodes, we train them
    by updating our network parameters based on the log of our network output times
    the actual move that was taken, times the reward. This is used as a loss function
    of our neural network. We do this for each time step as a single batch update.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦在我们的情节中存储了一组数量的状态，我们就会通过更新我们的网络参数来训练它们，更新依据是网络输出的对数乘以实际的移动，乘以奖励。这被用作我们神经网络的损失函数。我们对每个时间步执行这一操作，作为单批次更新。
- en: This is then repeated from step 2 until we hit a stopping point, either at some
    number of iterations or some score within the environment.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从步骤2开始重复执行，直到达到停止点，要么在一定的迭代次数内，要么在环境内得到一定的分数。
- en: The effect of this loop is that if an action is associated with positive rewards,
    we increase the parameters that lead to this action in that state. If the reward
    is negative, we decrease the parameters leading to the action. Note that for this
    to work, it requires us to have some negative valued rewards; otherwise, over
    time, all actions are simply pulled up. The best option if this does not occur
    naturally is to normalize our rewards in each batch.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此循环的效果是，如果一个动作与正向奖励相关联，我们会增加导致该状态下此动作的参数。如果奖励是负向的，我们会减少导致该动作的参数。需要注意的是，为了使其工作，我们需要具有一些负值的奖励；否则，随着时间的推移，所有动作都会被简单地提升。如果这种情况没有自然发生，最好的选择是在每个批次中对我们的奖励进行归一化。
- en: The policy gradient approach has been shown to be successful at learning a range
    of complex tasks, although it can take a very long time to train well and is very
    sensitive to the learning rate. Too high the learning rate and the behavior will
    oscillate wildly, never becoming stable enough to learn anything of note. Too
    low and it will never converge. This is why in the following example, we use RMSProp
    as the optimizer. Standard gradient descent with a fixed learning rate is often
    a lot less successful. Also, although the example shown here is for board games,
    policy gradients also work very well for learning more dynamic games, such as
    Pong.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明政策梯度方法在学习一系列复杂任务时取得了成功，尽管训练速度可能会非常缓慢，并且对学习速率非常敏感。学习速率过高时，行为将会发生剧烈震荡，永远无法保持稳定，以至于无法学到有意义的东西。学习速率过低时，它永远无法收敛。这就是为什么在下面的示例中，我们使用RMSProp作为优化器。标准的梯度下降法带有固定学习率通常会不成功。另外，尽管这里展示的例子是针对棋盘游戏的，但政策梯度在学习更具动态性的游戏，如乒乓球，也表现得非常出色。
- en: 'Now let''s create `player_func` for our tic-tac-toe''s `play_game` method;
    it uses policy gradients to learn the optimal play. We will set up the neural
    network that will take the nine squares of the board as input. The number 1 will
    be a mark for the player, -1 for the opponent, and 0 an unmarked square. Here
    the network will be set up with three hidden layers, each with 100 hidden nodes
    and relu activation functions. The output layer will also contain nine nodes,
    one for each board square. Because we want our final output to be the *probability*
    of a move being the best one, we want the output of all the nodes in the final
    layer to sum to 1\. This means using a softmax activation function is a natural
    choice. The softmax activation function looks as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为井字游戏的`play_game`方法创建`player_func`；它使用政策梯度来学习最佳策略。我们将建立一个神经网络，以棋盘的九个方格作为输入。数字1代表玩家的标记，-1代表对手的标记，0代表未标记的方格。在这里，网络将设置为三个隐藏层，每个隐藏层有100个隐藏节点和relu激活函数。输出层还将包含九个节点，每个代表一个方格。因为我们希望最终的输出是*移动是最佳移动的概率*，我们希望最后一层的所有节点的输出总和为1。这意味着使用softmax激活函数是一个自然的选择。softmax激活函数如下：
- en: '![Policy gradients for learning policy functions](img/00282.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![学习策略函数的政策梯度](img/00282.jpeg)'
- en: Here, *x* and *y* are vectors with equal dimensions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 和 *y* 是具有相同维数的向量。
- en: 'Here is the code for creating the network in TensorFlow. The full code can
    also be found in the GitHub repo in `policy_gradients.py`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在TensorFlow中创建网络的代码。完整的代码也可以在GitHub仓库中的`policy_gradients.py`中找到。
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'First, we import NumPy and TensorFlow, which will be used for the network,
    and create a few constant variables, which will be used later. The 3 * 3 input
    nodes is the size of the board:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入NumPy和TensorFlow，它将用于网络，并创建一些常量变量，稍后将使用它们。3 * 3输入节点是棋盘的大小：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `input_placeholder` variable is the placeholder that holds the input to
    the neural network. In TensorFlow, placeholder objects are used for all values
    provided to the network. When running the network, it will be set to `board_state`
    of the game. Also, the first dimension of `input_placeholder` is `None`. This
    is because, as mentioned a few times in this book, training mini-batching is much
    faster. The `None` will adjust to become the size of our mini-batch of samples
    come training time:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_placeholder`变量是神经网络的输入占位符。在TensorFlow中，占位符对象用于向网络提供所有值。在运行网络时，它将设置为游戏的`board_state`。此外，`input_placeholder`的第一个维度是`None`。这是因为在训练时，使用小批量训练会更快。`None`将在训练时调整为我们的样本小批量的大小：'
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here we create the weights we will need for the three layers of our network.
    They will all be created with a random Xavier initialization; more on this in
    chapter:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建我们网络三层所需的权重。它们都将使用随机的Xavier初始化创建；在本章中会更详细讲解：
- en: '[PRE46]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create the first hidden layer, our `hidden_weights_1` 2d tensor, and matrix
    multiply it by `input_placeholder`. Then add the bias variable, `tf.Variable(tf.constant(0.01,
    shape=(HIDDEN_NODES[0],)))`, which gives the network a bit more flexibility in
    learning patterns. The output is then put through a relu activation function:
    `tf.nn.relu`. This is how we write the basic equation for a layer of a neural
    network in TensorFlow. The other thing to note is 0.01\. When using the `relu`
    function, it is good practice to add a small amount of positive bias. This is
    because the relu function is the maximum value and is 0\. This means that values
    below 0 will have no gradient and so will not be adjusted during learning. If
    node activation is always below zero, because of bad luck with weight initialization,
    then it is considered a dead node and will never have an impact on the network
    and will simply take up GPU/CPU cycles. A small amount of positive bias greatly
    reduces the chance of having completely dead nodes in the network:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 创建第一个隐藏层，我们的`hidden_weights_1` 2维张量，并将其与`input_placeholder`进行矩阵相乘。然后添加偏差变量`tf.Variable(tf.constant(0.01,
    shape=(HIDDEN_NODES[0],)))`，这可以使网络在学习模式中具有更大的灵活性。然后通过relu激活函数处理输出：`tf.nn.relu`。这就是我们在TensorFlow中写神经网络层的基本方程。另一点需要注意的是0.01。使用`relu`函数时，添加一小部分正偏差是一个好的实践。这是因为relu函数是最大值并且为0。这意味着值小于0将没有梯度，所以在学习过程中不会被调整。如果节点激活始终小于零，那么因为权重初始化不佳的不幸，这将被视为一个死节点，并且永远不会对网络产生影响，并且只会浪费GPU/CPU周期。一小部分正偏差极大地减少了网络中完全死节点的机会：
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The next few layers are created in the same way:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几层以相同的方式创建：
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'For the `loss` function, we need two additional placeholders. One of them is
    for the reward we receive from the environment, in this case, the result of our
    game of tic-tac-toe. The other is meant for the actual action we will take at
    each time step. Remember, we will choose our moves according to a stochastic policy
    based on the output of our network. When we adjust our parameters, we need to
    know the actual move we took so we can move the parameters towards it if we have
    a positive reward and away from it if the reward is negative:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`loss`函数，我们需要两个额外的占位符。其中一个用于表示我们从环境中获得的奖励，即井字棋游戏的结果。另一个用于表示每个时间步我们将要采取的实际动作。请记住，我们将根据网络输出的随机策略选择我们的动作。当我们调整参数时，我们需要知道我们实际采取的动作，这样我们就可以根据奖励的正负移动参数的方向：
- en: '[PRE49]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `actual_move_placeholder` when activated will be a one hot vector, for example,
    `[0, 0, 0, 0, 1, 0, 0, 0, 0],` with 1 being the square in which the actual move
    was played. This will act as a mask across `output_layer`, so that only the gradients
    of that move are adjusted. Success or failure in moving to the first square tells
    us nothing about the success or failure of moving to the second square. Multiplying
    it by `reward_placeholder` tells us whether we want to increase the weights leading
    to this move or reduce them. We then put `policy_gradient` into our optimizer;
    we want to maximize our reward, which means minimizing the inverse of it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当激活`actual_move_placeholder`时，它将是一个独热向量，例如，`[0, 0, 0, 0, 1, 0, 0, 0, 0]`，其中1表示实际移动所在的方格。这将作为一个掩码应用到`output_layer`上，以便只调整该移动的梯度。到达第一个方格的成功与失败对第二个方格的成功与失败没有影响。将它与`reward_placeholder`相乘，可以告诉我们是否要增加导致这个动作的权重还是减少权重。然后将`policy_gradient`输入我们的优化器；我们想要最大化我们的奖励，这意味着最小化其的倒数。
- en: One final point is that here we are using `RMSPropOptimizer`. As mentioned before,
    policy gradients are very sensitive to the learning rate and type used. `RMSProp`
    has been shown to work well.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点是我们在这里使用了 `RMSPropOptimizer`。如前所述，策略梯度对使用的学习率和类型非常敏感。已经证明 `RMSProp` 效果很好。
- en: 'Within TensorFlow, variables also need to be initialized within a session;
    this session will then be used to run our calculations:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，变量也需要在会话中初始化；然后会话将用于运行我们的计算：
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now we need a method for running our network to choose the actions that will
    be passed in to the `play_game` method that we created previously:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个方法来运行我们的网络以选择要传递给之前创建的 `play_game` 方法的动作：
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the `make_move` method, we do a few different things. First, we flatten
    `board_state`, which starts as the second array in a one-dimensional array that
    we need to use as input for the network. We then append that state to our `board_states`
    list so we can later use it for training, once we have the reward for the episode.
    We then run the network using our TensorFlow session: `probability_of_actions`.
    There will now be an array with nine numbers that will sum up to one; these are
    the numbers that the network will learn to have the probability where it can set
    each move as the current most favorable:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `make_move` 方法中，我们做了一些不同的事情。首先，我们将 `board_state` 展开，它起初是一个我们需要用作网络输入的一维数组中的第二个数组。然后，我们将该状态附加到我们的
    `board_states` 列表中，以便稍后在我们拿到该场景奖励后用于训练。然后，我们使用 TensorFlow 会话运行网络： `probability_of_actions`。现在会有一个包含九个数字的数组，它们将加起来等于一；这些数字是网络将学习将每个动作设置为当前最有利的概率的数字：
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We now use `probability_of_actions` as the input to a multinomial distribution.
    The `np.random.multinomial` returns a series of values from the distribution you
    pass. Because we gave 1 for the first argument, only a single value will be generated;
    this is the move we will make. The `try…catch` around the multinomial call exists
    because owing to the small rounding errors, `probability_of_actions` sometimes
    sums up to be greater than 1\. This only happens roughly once every 10,000 calls,
    so we will be *pythonic*; if it fails, simply adjust it by some small epsilon
    and try again:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 `probability_of_actions` 作为多项式分布的输入。`np.random.multinomial` 返回你传递给它的分布的一系列值。因为我们为第一个参数给了
    1，所以只会生成一个值；这就是我们将要做出的移动。围绕 multinomial 调用的 `try…catch` 的存在是因为由于小的舍入误差，`probability_of_actions`
    有时会加起来大于 1。这大约每 10,000 次调用会发生一次，因此我们将*pythonic*；如果失败了，只需通过一些小的 epsilon 调整它，然后再试一次：
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The last bit of the `make_move` method is that we need to store the move we
    actually used later in training. Then return the move to the format that our Tic-Tac-Toe
    game expects it in, which is as a tuple of two integers: one for the *x* position
    and one for the *y* position.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_move` 方法的最后一部分是我们需要在训练后存储我们实际使用的移动。然后将移动返回到我们的井字游戏期望的格式中，即作为两个整数的元组：一个是
    *x* 位置，一个是 *y* 位置。'
- en: 'The final step before training is that once we have a complete batch to train
    on, we need to normalize the rewards from the batch. There are a few advantages
    to this. First, during early training, when it is losing or winning almost all
    games, we want to encourage the network to move towards better examples. Normalizing
    will allow us to have that extra weight applied to the rare examples that are
    more significant. Also, batch normalization tends to speed up training because
    it reduces the variance in targets:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前的最后一步是，一旦我们有了一个完整的批次进行训练，我们需要对批次中的奖励进行归一化。这样做有几个优点。首先，在早期训练时，当几乎所有游戏都输了或赢了，我们希望鼓励网络朝着更好的示例迈进。归一化将使我们能够对罕见、更重要的示例施加额外的权重。此外，批量归一化倾向于加速训练，因为它减少了目标的方差：
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We define a constant for how big our `BATCH_SIZE` is. This defines how many
    examples go into our mini-batches for training. Many different values of this
    work well; 100 is one of these. `episode_number` will keep track of how many game
    loops we have done. This will track when we need to kick off a mini-batch training:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们的 `BATCH_SIZE` 定义了一个大小常量。这定义了我们用于训练的小批量中有多少示例。许多不同的值都可以很好地工作；100 就是其中之一。`episode_number`
    将跟踪我们已经完成了多少个游戏循环。这将追踪我们何时需要启动小批量训练：
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`while True` puts us into the main loop. The first step we need to make here
    is to run a game using our old friend, the `play_game` method from earlier in
    the chapter. For simplicity''s sake, we will always have the policy gradient player,
    using the `make_move` method as the first player and `random_player` as the second
    player. It would not be too difficult to change it to alternate the order of moves:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`while True`将我们置于主循环中。我们需要在这里迈出的第一步是运行一场比赛，使用我们在本章前面使用过的老朋友`play_game`方法。为了简单起见，我们将始终让策略梯度玩家以`make_move`方法作为第一玩家，以`random_player`作为第二玩家。更改顺序也不难：'
- en: '[PRE56]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Get the length of the game we just played and append the reward we received
    for it to the `rewards` array so that each board state gets the same final reward
    we received. In reality, some moves may have had more or less impact on the final
    reward than others, but we cannot know that here. We will hope that through training,
    with similar good states showing up with positive rewards more often, the network
    will learn this over time. We also scale the reward by `last_game_length`, so
    winning quickly is better than winning slowly and losing slowly is better than
    losing quickly. Another point to note is if we were running a game with a more
    unevenly distributed reward—such as Pong, where most frames would have 0 reward
    with the occasional one—this is where we might apply future discounting across
    the time steps of the episode:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 获取我们刚刚玩的游戏的长度，并将我们收到的奖励附加到`rewards`数组中，这样每个棋盘状态都可以得到我们收到的相同的最终奖励。实际上，有些移动可能对最终奖励产生了更大或更小的影响，但我们在这里无法知道。我们希望通过训练，随着类似的好状态更频繁地出现，并且有正向奖励，网络会随着时间学会这一点。我们还通过`last_game_length`来缩放奖励，所以快速获胜比慢速获胜更好，慢速失败比快速失败更好。另一个需要注意的是，如果我们运行的游戏奖励分布更不均匀——比如Pong，大部分帧都没有奖励，只有偶尔才有——这就是我们可能会在剧集的时间步上应用未来的折现的地方：
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Increment `episode_number`, and if we have a `BATCH_SIZE` set of samples, jump
    into the training code. We start this by doing batch normalization on our rewards.
    This is not always necessary, but it is almost always advisable because it has
    many benefits. It tends to improve training time by reducing variance across training.
    If we have issues with all rewards being positive/negative, this solves them without
    you having to give it a second thought. Finally, kick off the training by running
    the `train_step` operation through the TensorFlow session object:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 增加`episode_number`，如果我们有一个`BATCH_SIZE`数量的样本，就进入训练代码。我们首先对我们的奖励进行批量归一化。这并不总是必需的，但几乎总是值得推荐的，因为它有很多好处。它倾向于通过减少训练中的差异来改善训练时间。如果我们的所有奖励都是正数/负数，这将解决问题，而无需您再考虑。最后，通过在TensorFlow会话对象上运行`train_step`操作来启动训练：
- en: '[PRE58]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, clear the current mini-batch to make way for the next one. Now let''s
    see how policy gradients perform:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，清空当前的小批量以为下一个小批量让路。现在让我们看看策略梯度的表现如何：
- en: '![Policy gradients for learning policy functions](img/00283.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![用于学习策略函数的策略梯度](img/00283.jpeg)'
- en: As you can see, it eventually achieves a respectable 85 percent winning rate.
    With more time and tuning of hyper-parameters, it could do even better. Also,
    note the reason that indicates why a random player who only chooses valid moves
    has a greater than 50 percent winning rate. This is because here, the observed
    player always goes first.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，最终它实现了尊重的85%的获胜率。随着更多的时间和超参数的调整，它可能会做得更好。另外，请注意这一点，这说明了只选择有效移动的随机玩家的获胜率超过50%的原因。这是因为在这里，被观察到的玩家总是先行动的。
- en: Policy gradients in AlphaGo
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo中的策略梯度
- en: For AlphaGo using policy gradients, the network was set up to play games against
    itself. It did so with a reward of 0 for every time step until the final one where
    the game is either won or lost, giving a reward of 1 or -1\. This final reward
    is then applied to every time step in the network, and the network is trained
    using policy gradients in the same way as our Tic-tac-toe example. To prevent
    overfitting, games were played against a randomly selected previous version of
    the network. If the network constantly plays against itself, the risk is it could
    end up with some very niche strategies, which would not work against varied opponents,
    a local minima of sorts.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Building the initial supervised learning network that predicted the most likely
    moves by human players allowed AlphaGo to massively reduce the breadth of the
    search it needs to perform in MCTS. This allowed them to get much more accurate
    evaluation per rollout. The problem is that running a large many-layered neural
    network is very slow, compared to just selecting a random action. In our Monte-Carlo
    rollout, we need to select 100 moves on average and we want to do this in the
    order of hundreds of thousands of rollouts to evaluate a position. This makes
    using the network this way impractical. We need to find a way to reduce our computation
    time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: If we use the best moves selected by our network instead of manually selecting
    a move with the probability of our output, then our network is deterministic.
    Given a position on the board, the result achieved by the board will also be deterministic.
    When evaluated using the best moves from the network, the position is either a
    winning one for white or black or a draw. This result is the value of the position
    under the network's optimal policy. Because the result is deterministic, we can
    train a new deep neural network to learn the value of this position. If it performs
    well, a position can be evaluated accurately using just one pass of the neural
    network, rather than one for each move.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'A final supervised network is created using the same structure as the previous
    networks, except this time the final output, rather than being a probability of
    actions across the board, is just a single node representing the expected result
    of the game: win for white, win for black, or draw.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for this network is the mean squared error between its output
    and the result achieved by the reinforcement learning network. It was found after
    training that the value network could achieve a mean squared error of just 0.226
    and 0.234 on the training and test sets, respectively. This indicated that it
    could learn the result with good accuracy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, at this point, Alpha Go has three differently trained deep neural
    networks:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**SL**: This is a network trained using supervised learning to predict the
    probability of a human move from a board position.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL**: This is a network trained that initially used the weights from the
    SL network, but was then further trained using reinforcement learning to choose
    the best move from a given position.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: This is a network again trained with supervised learning to learn the
    expected result of the position when played using the RL network. It provides
    the value of the state.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a real game against Lee Sedol, Alpha Go used a variant on the MCTS-UCT
    that we introduced earlier. When the rollout was simulated from the MCTS leaves,
    rather than selecting moves randomly, they were selected using another, much smaller,
    single layer network. This network called the fast rollout policy and used a softmax
    classifier across all possible moves, where the input was the 3 x 3 color pattern
    around the action and a collection of handcrafted features, such as the liberty
    count. This is, in our example, the following line:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This could be replaced with something like this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This small network was used to run the Monte-Carlo rollout. The SL network would
    almost certainly have been better, but would have been prohibitively slow.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating the success value of a rollout from a leaf, the score was determined
    using a combination of the result from the fast rollout policy and the score as
    given by the V-network. A mixing parameter ? was used to determine the relative
    weights of these:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients in AlphaGo](img/00284.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Here, *s* is the state of the leaf and *f* is the result of the rollout using
    the fast rollout policy. After experimenting with a wide range of the values for
    ?, it was found that 0.5 yielded the best results, suggesting that both methods
    of evaluation are complementary.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The five-game series between Lee Sedol and Alpha Go started on March 9, 2016,
    in front of a large audience with a $1,000,000 prize for the winner. Lee Sedol
    was very confident in the buildup, declaring, "I have heard that Google DeepMind's
    AI is surprisingly strong and getting stronger, but I am confident that I can
    win at least this time." Sadly, for him, Alpha Go proceeded to win the first three
    games, each forcing a resignation. At this point, with the competition decided,
    he came back to win the fourth, but lost the fifth, leaving the series at 4-1.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: This was very significant progress on the part of AI, marking the first time
    that an AI had come even close to beating a top human player at such a complex
    game. It raises all kinds of questions such as in which other domains, it might
    be possible to develop AI that can outperform the best humans. The match's full
    significance on humanity remains to be seen.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot in this chapter and looked at a lot of Python code. We
    talked a bit about the theory of discrete state and zero sum games. We showed
    how min-max can be used to evaluate the best moves in positions. We also showed
    that evaluation functions can be used to allow min-max to operate on games where
    the state space of possible moves and positions are too vast.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了很多内容，并查看了很多Python代码。我们简单讨论了离散状态和零和博弈的理论。我们展示了如何使用Min-max来评估位置的最佳移动。我们还展示了评估函数如何允许Min-max在可能的移动和位置状态空间过大的游戏中运行。
- en: For games where no good evaluation function exists, we showed how Monte-Carlo
    Tree Search can be used to evaluate the positions and then how Monte-Carlo Tree
    Search with Upper Confidence bounds for Trees can allow the performance of MCTS
    to coverage toward what you would get from Min-max. This took us to the UCB1 algorithm.
    Apart from allowing us to compute MCTS-UCT, it is also a great general purpose
    method for choosing between collections of unknown outcomes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有好的评估函数的游戏，我们展示了如何使用蒙特卡洛树搜索来评估位置，以及如何使用带有置信上界的蒙特卡洛树搜索来让MCTS的性能接近Min-max。这使我们了解了UCB1算法。除了允许我们计算MCTS-UCT外，它还是一种在不同结果中选择的通用方法。
- en: We then looked at how reinforcement learning can be integrated into these approaches.
    We also saw how the policy gradient can be used to train deep networks to learn
    complex patterns and find advantages in games with difficult-to-evaluate states.
    Finally, we looked at how these techniques were applied in AlphaGo to beat the
    reigning human world champion.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了如何将强化学习与这些方法相结合。我们还看到了如何使用策略梯度来训练深度网络以学习复杂的模式，并在难以评估的状态下找到优势。最后，我们看到了这些技术如何应用在AlphaGo中击败了当时的世界冠军。
- en: If you are interested in getting more involved in deep learning for board games,
    the Alpha Toe project ([https://github.com/DanielSlater/AlphaToe](https://github.com/DanielSlater/AlphaToe))
    has examples of running deep learning on a wider range of games, including Connect
    Four and Tic-Tac-Toe on a 5 x 5 board.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣更深入地参与深度学习的棋盘游戏，Alpha Toe项目（[https://github.com/DanielSlater/AlphaToe](https://github.com/DanielSlater/AlphaToe)）提供了在更广泛的游戏上运行深度学习的示例，包括在5
    x 5棋盘上的连连看和井字棋。
- en: Though these techniques have been introduced for board games, their application
    runs a lot wider. Many problems that we encounter can be formulated, such as like
    discrete state games, for example, optimizing routes for delivery companies, investing
    in financial markets, and planning strategies for businesses. We've only just
    started exploring all the possibilities.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些技术是为棋盘游戏引入的，但它们的应用范围更广。我们遇到的许多问题都可以被形式化，例如为送货公司优化路线、在金融市场上投资以及制定企业战略。我们只是刚刚开始探索所有的可能性。
- en: In the next chapter, we will look at using deep learning for learning computer
    games. This will build on our knowledge of policy gradients from this chapter
    and introduce new techniques for dealing with the dynamic environments of computer
    games.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究如何使用深度学习来学习电脑游戏。这将在本章的策略梯度知识基础上进行，并介绍处理动态环境的新技术。
