["```py\nclass ResNetBasicBlock(nn.Module):\n\n    def __init__(self,in_channels,out_channels,stride):\n\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.stride = stride\n\n    def forward(self,x):\n\n        residual = x\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out),inplace=True)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        return F.relu(out)       \n```", "```py\nfrom torchvision.models import resnet18\n\nresnet = resnet18(pretrained=False)\n```", "```py\ndata_transform = transforms.Compose([\n        transforms.Resize((299,299)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n# For Dogs & Cats dataset\ntrain_dset = ImageFolder('../../chapter5/dogsandcats/train/',transform=data_transform)\nval_dset = ImageFolder('../../chapter5/dogsandcats/valid/',transform=data_transform)\nclasses=2\n```", "```py\ntrain_loader = DataLoader(train_dset,batch_size=32,shuffle=False,num_workers=3)\nval_loader = DataLoader(val_dset,batch_size=32,shuffle=False,num_workers=3)\n\n```", "```py\n#Create ResNet model\nmy_resnet = resnet34(pretrained=True)\n\nif is_cuda:\n    my_resnet = my_resnet.cuda()\n\nmy_resnet = nn.Sequential(*list(my_resnet.children())[:-1])\n\nfor p in my_resnet.parameters():\n    p.requires_grad = False\n```", "```py\nfor p in my_resnet.parameters():\n    p.requires_grad = False\n```", "```py\n#For training data\n\n# Stores the labels of the train data\ntrn_labels = [] \n\n# Stores the pre convoluted features of the train data\ntrn_features = [] \n\n#Iterate through the train data and store the calculated features and the labels\nfor d,la in train_loader:\n    o = m(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    trn_labels.extend(la)\n    trn_features.extend(o.cpu().data)\n\n#For validation data\n\n#Iterate through the validation data and store the calculated features and the labels\nval_labels = []\nval_features = []\nfor d,la in val_loader:\n    o = m(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    val_labels.extend(la)\n    val_features.extend(o.cpu().data)\n```", "```py\nclass FeaturesDataset(Dataset):\n\n    def __init__(self,featlst,labellst):\n        self.featlst = featlst\n        self.labellst = labellst\n\n    def __getitem__(self,index):\n        return (self.featlst[index],self.labellst[index])\n\n    def __len__(self):\n        return len(self.labellst)\n```", "```py\n#Creating dataset for train and validation\ntrn_feat_dset = FeaturesDataset(trn_features,trn_labels)\nval_feat_dset = FeaturesDataset(val_features,val_labels)\n\n#Creating data loader for train and validation\ntrn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)\nval_feat_loader = DataLoader(val_feat_dset,batch_size=64)\n```", "```py\nclass FullyConnectedModel(nn.Module):\n\n    def __init__(self,in_size,out_size):\n        super().__init__()\n        self.fc = nn.Linear(in_size,out_size)\n\n    def forward(self,inp):\n        out = self.fc(inp)\n        return out\n\nfc_in_size = 8192\n\nfc = FullyConnectedModel(fc_in_size,classes)\nif is_cuda:\n    fc = fc.cuda()\n```", "```py\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n```", "```py\n#Results\ntraining loss is 0.082 and training accuracy is 22473/23000     97.71\nvalidation loss is   0.1 and validation accuracy is 1934/2000      96.7\ntraining loss is  0.08 and training accuracy is 22456/23000     97.63\nvalidation loss is  0.12 and validation accuracy is 1917/2000     95.85\ntraining loss is 0.077 and training accuracy is 22507/23000     97.86\nvalidation loss is   0.1 and validation accuracy is 1930/2000      96.5\ntraining loss is 0.075 and training accuracy is 22518/23000      97.9\nvalidation loss is 0.096 and validation accuracy is 1938/2000      96.9\ntraining loss is 0.073 and training accuracy is 22539/23000      98.0\nvalidation loss is   0.1 and validation accuracy is 1936/2000      96.8\ntraining loss is 0.073 and training accuracy is 22542/23000     98.01\nvalidation loss is 0.089 and validation accuracy is 1942/2000      97.1\ntraining loss is 0.071 and training accuracy is 22545/23000     98.02\nvalidation loss is  0.09 and validation accuracy is 1941/2000     97.05\ntraining loss is 0.068 and training accuracy is 22591/23000     98.22\nvalidation loss is 0.092 and validation accuracy is 1934/2000      96.7\ntraining loss is 0.067 and training accuracy is 22573/23000     98.14\nvalidation loss is 0.085 and validation accuracy is 1942/2000      97.1\n```", "```py\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\nclass InceptionBasicBlock(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super().__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n```", "```py\nbranch1x1 = self.branch1x1(x)\n```", "```py\nbranch5x5 = self.branch5x5_1(x)\nbranch5x5 = self.branch5x5_2(branch5x5)\n```", "```py\nbranch3x3dbl = self.branch3x3dbl_1(x)\nbranch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n```", "```py\nbranch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\nbranch_pool = self.branch_pool(branch_pool)\n```", "```py\nmy_inception = inception_v3(pretrained=True)\nmy_inception.aux_logits = False\nif is_cuda:\n    my_inception = my_inception.cuda()\n```", "```py\nclass LayerActivations():\n    features=[]\n\n    def __init__(self,model):\n        self.features = []\n        self.hook = model.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self,module,input,output):\n\n        self.features.extend(output.view(output.size(0),-1).cpu().data)\n\n    def remove(self):\n\n        self.hook.remove()\n```", "```py\n# Create LayerActivations object to store the output of inception model at a particular layer.\ntrn_features = LayerActivations(my_inception.Mixed_7c)\ntrn_labels = []\n\n# Passing all the data through the model , as a side effect the outputs will get stored \n# in the features list of the LayerActivations object. \nfor da,la in train_loader:\n    _ = my_inception(Variable(da.cuda()))\n    trn_labels.extend(la)\ntrn_features.remove()\n\n# Repeat the same process for validation dataset .\n\nval_features = LayerActivations(my_inception.Mixed_7c)\nval_labels = []\nfor da,la in val_loader:\n    _ = my_inception(Variable(da.cuda()))\n    val_labels.extend(la)\nval_features.remove()\n```", "```py\n#Dataset for pre computed features for train and validation data sets\n\ntrn_feat_dset = FeaturesDataset(trn_features.features,trn_labels)\nval_feat_dset = FeaturesDataset(val_features.features,val_labels)\n\n#Data loaders for pre computed features for train and validation data sets\n\ntrn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)\nval_feat_loader = DataLoader(val_feat_dset,batch_size=64)\n```", "```py\nclass FullyConnectedModel(nn.Module):\n\n    def __init__(self,in_size,out_size,training=True):\n        super().__init__()\n        self.fc = nn.Linear(in_size,out_size)\n\n    def forward(self,inp):\n        out = F.dropout(inp, training=self.training)\n        out = self.fc(out)\n        return out\n\n# The size of the output from the selected convolution feature \nfc_in_size = 131072\n\nfc = FullyConnectedModel(fc_in_size,classes)\nif is_cuda:\n    fc = fc.cuda()\n```", "```py\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n\n#Results\n\ntraining loss is 0.78 and training accuracy is 22825/23000 99.24\nvalidation loss is 5.3 and validation accuracy is 1947/2000 97.35\ntraining loss is 0.84 and training accuracy is 22829/23000 99.26\nvalidation loss is 5.1 and validation accuracy is 1952/2000 97.6\ntraining loss is 0.69 and training accuracy is 22843/23000 99.32\nvalidation loss is 5.1 and validation accuracy is 1951/2000 97.55\ntraining loss is 0.58 and training accuracy is 22852/23000 99.36\nvalidation loss is 4.9 and validation accuracy is 1953/2000 97.65\ntraining loss is 0.67 and training accuracy is 22862/23000 99.4\nvalidation loss is 4.9 and validation accuracy is 1955/2000 97.75\ntraining loss is 0.54 and training accuracy is 22870/23000 99.43\nvalidation loss is 4.8 and validation accuracy is 1953/2000 97.65\ntraining loss is 0.56 and training accuracy is 22856/23000 99.37\nvalidation loss is 4.8 and validation accuracy is 1955/2000 97.75\ntraining loss is 0.7 and training accuracy is 22841/23000 99.31\nvalidation loss is 4.8 and validation accuracy is 1956/2000 97.8\ntraining loss is 0.47 and training accuracy is 22880/23000 99.48\nvalidation loss is 4.7 and validation accuracy is 1956/2000 97.8\n\n```", "```py\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module('denselayer%d' % (i + 1), layer)\n```", "```py\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm.1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu.1', nn.ReLU(inplace=True)),\n        self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu.2', nn.ReLU(inplace=True)),\n        self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n```", "```py\ndef forward(self, input):\n    for module in self._modules.values():\n        input = module(input)\n    return input\n```", "```py\nmy_densenet = densenet121(pretrained=True).features\nif is_cuda:\n    my_densenet = my_densenet.cuda()\n\nfor p in my_densenet.parameters():\n    p.requires_grad = False\n```", "```py\n#For training data\ntrn_labels = []\ntrn_features = []\n\n#code to store densenet features for train dataset.\nfor d,la in train_loader:\n    o = my_densenet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    trn_labels.extend(la)\n    trn_features.extend(o.cpu().data)\n\n#For validation data\nval_labels = []\nval_features = []\n\n#Code to store densenet features for validation dataset. \nfor d,la in val_loader:\n    o = my_densenet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    val_labels.extend(la)\n    val_features.extend(o.cpu().data)\n```", "```py\n# Create dataset for train and validation convolution features\ntrn_feat_dset = FeaturesDataset(trn_features,trn_labels)\nval_feat_dset = FeaturesDataset(val_features,val_labels)\n\n# Create data loaders for batching the train and validation datasets\ntrn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True,drop_last=True)\nval_feat_loader = DataLoader(val_feat_dset,batch_size=64)\n```", "```py\nclass FullyConnectedModel(nn.Module):\n\n    def __init__(self,in_size,out_size):\n        super().__init__()\n        self.fc = nn.Linear(in_size,out_size)\n\n    def forward(self,inp):\n        out = self.fc(inp)\n        return out\n\nfc = FullyConnectedModel(fc_in_size,classes)\nif is_cuda:\n    fc = fc.cuda()\n```", "```py\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n\n```", "```py\n# Results\n\ntraining loss is 0.057 and training accuracy is 22506/23000 97.85\nvalidation loss is 0.034 and validation accuracy is 1978/2000 98.9\ntraining loss is 0.0059 and training accuracy is 22953/23000 99.8\nvalidation loss is 0.028 and validation accuracy is 1981/2000 99.05\ntraining loss is 0.0016 and training accuracy is 22974/23000 99.89\nvalidation loss is 0.022 and validation accuracy is 1983/2000 99.15\ntraining loss is 0.00064 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.023 and validation accuracy is 1983/2000 99.15\ntraining loss is 0.00043 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.024 and validation accuracy is 1983/2000 99.15\ntraining loss is 0.00033 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.024 and validation accuracy is 1984/2000 99.2\ntraining loss is 0.00025 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.024 and validation accuracy is 1984/2000 99.2\ntraining loss is 0.0002 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.025 and validation accuracy is 1985/2000 99.25\ntraining loss is 0.00016 and training accuracy is 22976/23000 99.9\nvalidation loss is 0.024 and validation accuracy is 1986/2000 99.3\n```", "```py\n#Create ResNet model\nmy_resnet = resnet34(pretrained=True)\n\nif is_cuda:\n    my_resnet = my_resnet.cuda()\n\nmy_resnet = nn.Sequential(*list(my_resnet.children())[:-1])\n\nfor p in my_resnet.parameters():\n    p.requires_grad = False\n\n#Create inception model\n\nmy_inception = inception_v3(pretrained=True)\nmy_inception.aux_logits = False\nif is_cuda:\n    my_inception = my_inception.cuda()\nfor p in my_inception.parameters():\n    p.requires_grad = False\n\n#Create densenet model\n\nmy_densenet = densenet121(pretrained=True).features\nif is_cuda:\n    my_densenet = my_densenet.cuda()\n\nfor p in my_densenet.parameters():\n    p.requires_grad = False\n```", "```py\n### For ResNet\n\ntrn_labels = []\ntrn_resnet_features = []\nfor d,la in train_loader:\n    o = my_resnet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    trn_labels.extend(la)\n    trn_resnet_features.extend(o.cpu().data)\nval_labels = []\nval_resnet_features = []\nfor d,la in val_loader:\n    o = my_resnet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    val_labels.extend(la)\n    val_resnet_features.extend(o.cpu().data)\n\n### For Inception\n\ntrn_inception_features = LayerActivations(my_inception.Mixed_7c)\nfor da,la in train_loader:\n    _ = my_inception(Variable(da.cuda()))\n\ntrn_inception_features.remove()\n\nval_inception_features = LayerActivations(my_inception.Mixed_7c)\nfor da,la in val_loader:\n    _ = my_inception(Variable(da.cuda()))\n\nval_inception_features.remove()\n\n### For DenseNet\n\ntrn_densenet_features = []\nfor d,la in train_loader:\n    o = my_densenet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n\n    trn_densenet_features.extend(o.cpu().data)\n\nval_densenet_features = []\nfor d,la in val_loader:\n    o = my_densenet(Variable(d.cuda()))\n    o = o.view(o.size(0),-1)\n    val_densenet_features.extend(o.cpu().data)\n```", "```py\nclass FeaturesDataset(Dataset):\n\n    def __init__(self,featlst1,featlst2,featlst3,labellst):\n        self.featlst1 = featlst1\n        self.featlst2 = featlst2\n        self.featlst3 = featlst3\n        self.labellst = labellst\n\n    def __getitem__(self,index):\n        return (self.featlst1[index],self.featlst2[index],self.featlst3[index],self.labellst[index])\n\n    def __len__(self):\n        return len(self.labellst)\n\ntrn_feat_dset = FeaturesDataset(trn_resnet_features,trn_inception_features.features,trn_densenet_features,trn_labels)\nval_feat_dset = FeaturesDataset(val_resnet_features,val_inception_features.features,val_densenet_features,val_labels)\n```", "```py\ntrn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)\nval_feat_loader = DataLoader(val_feat_dset,batch_size=64)\n```", "```py\nclass EnsembleModel(nn.Module):\n\n    def __init__(self,out_size,training=True):\n        super().__init__()\n        self.fc1 = nn.Linear(8192,512)\n        self.fc2 = nn.Linear(131072,512)\n        self.fc3 = nn.Linear(82944,512)\n        self.fc4 = nn.Linear(512,out_size)\n\n    def forward(self,inp1,inp2,inp3):\n        out1 = self.fc1(F.dropout(inp1,training=self.training))\n        out2 = self.fc2(F.dropout(inp2,training=self.training))\n        out3 = self.fc3(F.dropout(inp3,training=self.training))\n        out = out1 + out2 + out3\n        out = self.fc4(F.dropout(out,training=self.training))\n        return out\n\nem = EnsembleModel(2)\nif is_cuda:\n    em = em.cuda()\n```", "```py\ndef fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , (data1,data2,data3,target) in enumerate(data_loader):\n        if is_cuda:\n            data1,data2,data3,target = data1.cuda(),data2.cuda(),data3.cuda(),target.cuda()\n        data1,data2,data3,target = Variable(data1,volatile),Variable(data2,volatile),Variable(data3,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data1,data2,data3)\n        loss = F.cross_entropy(output,target)\n\n        running_loss += F.cross_entropy(output,target,size_average=False).data[0]\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n\n    loss = running_loss/len(data_loader.dataset)\n    accuracy = 100\\. * running_correct/len(data_loader.dataset)\n\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy\n```", "```py\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,em,trn_feat_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,em,val_feat_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n\n```", "```py\n#Results \n\ntraining loss is 7.2e+01 and training accuracy is 21359/23000 92.87\nvalidation loss is 6.5e+01 and validation accuracy is 1968/2000 98.4\ntraining loss is 9.4e+01 and training accuracy is 22539/23000 98.0\nvalidation loss is 1.1e+02 and validation accuracy is 1980/2000 99.0\ntraining loss is 1e+02 and training accuracy is 22714/23000 98.76\nvalidation loss is 1.4e+02 and validation accuracy is 1976/2000 98.8\ntraining loss is 7.3e+01 and training accuracy is 22825/23000 99.24\nvalidation loss is 1.6e+02 and validation accuracy is 1979/2000 98.95\ntraining loss is 7.2e+01 and training accuracy is 22845/23000 99.33\nvalidation loss is 2e+02 and validation accuracy is 1984/2000 99.2\ntraining loss is 1.1e+02 and training accuracy is 22862/23000 99.4\nvalidation loss is 4.1e+02 and validation accuracy is 1975/2000 98.75\ntraining loss is 1.3e+02 and training accuracy is 22851/23000 99.35\nvalidation loss is 4.2e+02 and validation accuracy is 1981/2000 99.05\ntraining loss is 2e+02 and training accuracy is 22845/23000 99.33\nvalidation loss is 6.1e+02 and validation accuracy is 1982/2000 99.1\ntraining loss is 1e+02 and training accuracy is 22917/23000 99.64\nvalidation loss is 5.3e+02 and validation accuracy is 1986/2000 99.3\n```"]