["```py\n    !pip install diffusers transformers accelerate \n    ```", "```py\n    import PIL\n    import requests\n    import torch\n    from io import BytesIO\n    from diffusers import StableDiffusionInpaintPipeline \n    ```", "```py\n    pipeline = StableDiffusionInpaintPipeline.from_pretrained(\n                                      \"runwayml/stable-diffusion-inpainting\",\n                                       torch_dtype=torch.float16)\n    pipeline = pipeline.to(\"cuda\") \n    ```", "```py\n    def download_image(url):\n        response = requests.get(url)\n        return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n    img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n    mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n    init_image = download_image(img_url).resize((512, 512))\n    mask_image = download_image(mask_url).resize((512, 512)) \n    ```", "```py\n    prompt = \"Face of a white cat, high resolution, sitting on a park bench\"\n    image = pipeline(prompt=prompt, image=init_image, \n                                    mask_image=mask_image).images[0] \n    ```", "```py\n    %pip install -Uqq torch-snippets diffusers accelerate\n    from torch_snippets import *\n    import cv2\n    import numpy as np\n    from PIL import Image \n    ```", "```py\n    image0 = read('/content/Screenshot 2023-11-06 at 11.14.37 AM.png')\n    low_threshold = 10\n    high_threshold = 250\n    image = cv2.Canny(image0, low_threshold, high_threshold)\n    image = image[:, :, None]\n    canny_image = np.concatenate([image, image, image], axis=2)\n    canny_image = Image.fromarray(canny_image)\n    show(canny_image, sz=3)\n    canny_image.save('canny.png') \n    ```", "```py\n    subplots([image0, canny_image]) \n    ```", "```py\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n    from diffusers.utils import load_image\n    import torch \n    ```", "```py\n    generator = torch.Generator(device='cuda').manual_seed(12345)\n    controlnet = ControlNetModel.from_pretrained(\n                                            \"lllyasviel/sd-controlnet-canny\") \n    ```", "```py\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n                                            \"runwayml/stable-diffusion-v1-5\",\n                                             controlnet=controlnet\n                                             ).to('cuda')\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.\\\n                                                                       config) \n    ```", "```py\n    image = pipe(\n        \"a man with beard, realistic, high quality\",\n        negative_prompt=\"cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, blurry, bad anatomy, bad proportions, nsfw\",\n        num_inference_steps=100,\n        generator=generator,\n        image=canny_image,\n        controlnet_conditioning_scale=0.5\n    ).images[0]\n    image.save('output.png') \n    ```", "```py\n    %pip -q install diffusers accelerate torch-snippets torchinfo lovely_tensors \n    ```", "```py\n    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n    import torch \n    ```", "```py\n    pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-\n                           turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n    pipeline = pipeline.to(\"cuda\") \n    ```", "```py\n    %%time\n    prompt = \"baby in superman dress, photorealistic, cinematic\"\n    n_prompt = 'bad, ugly, blur, deformed'\n    image = pipeline(prompt, num_inference_steps = 1, \n                 guidance_scale=0.0, negative_prompt = n_prompt, \n                 seed = 42).images[0]\n    image \n    ```", "```py\n    %pip -q install diffusers accelerate torch-snippets torchinfo lovely_tensors\n    !wget https://png.pngtree.com/thumb_back/fw800/background/20230811/pngtree-two-glasses-with-a-lime-wedge-in-them-next-to-a-image_13034833.jpg -O lime_juice.png \n    ```", "```py\n    import torch\n    import requests\n    from PIL import Image\n    from torch_snippets import *\n    from diffusers import StableDiffusionDepth2ImgPipeline \n    ```", "```py\n    pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n                                      \"stabilityai/stable-diffusion-2-depth\",\n                                       torch_dtype=torch.float16,) \n    ```", "```py\n    init_image = Image.open('/content/test.jpg')\n    prompt = \"glasses of lime juice with skyline view in background in a cool afternoon\"\n    n_propmt = \"bad, deformed, ugly, bad anotomy\"\n    image = pipe(prompt=prompt, image=init_image,  \n                 negative_prompt=n_propmt, strength=0.7, \n                 num_inference_steps = 100).images[0] \n    ```", "```py\n    %pip -q install -U diffusers transformers accelerate torch-snippets lovely_tensors torchinfo pysnooper\n    import torch\n    from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n    from diffusers.utils import export_to_video\n    from IPython.display import HTML\n    from base64 import b64encode \n    ```", "```py\n    pipe = DiffusionPipeline.from_pretrained(\\\n                   \"damo-vilab/text-to-video-ms-1.7b\",  \n                   torch_dtype=torch.float16, variant=\"fp16\")\n    pipe.enable_model_cpu_offload() \n    ```", "```py\n    prompt = 'bear running on the ocean'\n    negative_prompt = 'low quality'\n    video_duration_seconds = 3\n    num_frames = video_duration_seconds * 25 + 1 \n    ```", "```py\n    video_frames = pipe(prompt, negative_prompt=\"low quality\", num_inference_steps=25, num_frames=num_frames).frames \n    ```", "```py\n    def display_video(video):\n        fig = plt.figure(figsize=(4.2,4.2))  #Display size specification\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n        mov = []\n        for i in range(len(video)):  #Append videos one by one to mov\n            img = plt.imshow(video[i], animated=True)\n            plt.axis('off')\n            mov.append([img])\n        #Animation creation\n        anime = animation.ArtistAnimation(fig, mov, \n                              interval=100, repeat_delay=1000)\n        plt.close()\n        return anime\n    video_path = export_to_video(video_frames)\n    video = imageio.mimread(video_path)  #Loading video\n    HTML(display_video(video).to_html5_video())  #Inline video display in HTML5 \n    ```"]