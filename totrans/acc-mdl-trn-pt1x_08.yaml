- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying the Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you heard about parsimony? **Parsimony**, in the context of model estimation,
    concerns keeping a model as simple as possible. Such a principle comes from the
    assumption that complex models (models with a higher number of parameters) overfit
    the training data, thus reducing the capacity to generalize and make good predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, simplifying neural networks has two main benefits: reducing the
    model training time and making the model feasible to run in resource-constrained
    environments. One of the approaches to simplifying a model relies on reducing
    the number of parameters of the neural network by employing pruning and compression
    techniques.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we show how to simplify a model by reducing the number of parameters
    of the neural network without sacrificing its quality.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The key benefits of simplifying a model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept and techniques of model pruning and compression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the Microsoft NNI toolkit to simplify a model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the complete code of examples mentioned in this chapter in the
    book’s GitHub repository at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: You can access your favorite environments to execute this notebook, such as
    Google Colab or Kaggle.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the model simplifying process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simpler words, simplifying a model concerns removing connections, neurons,
    or entire layers of the neural network to get a lighter model, i.e., a model with
    a reduced number of parameters. Naturally, the efficiency of the simplified version
    must be very close to the one achieved by the original model. Otherwise, simplifying
    the model does not make any sense.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this topic, we must answer the following questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Why simplify a model? (reason)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we simplify a model? (process)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do we simplify a model? (moment)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will go through each of these questions in the following sections to get
    an overall understanding of model simplification.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on in this chapter, it is essential to say that model simplification
    is still an open research area. Consequently, some concepts and terms cited in
    this book may differ a little bit from other materials or how they are employed
    on frameworks and toolkits.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Why simplify a model? (reason)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get an insight into the reasons why a model should be simplified let’s make
    use of a simple yet nice analogy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the hypothetical situation where we must build a bridge to connect
    two sides of a river. For safety purposes, we decided to put a column in every
    two meters of the bridge, as shown in *Figure 6**.1*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Bridge analogy](img/B20959_06_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Bridge analogy
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The bridge seems pretty safe, being sustained by its 16 columns. However, someone
    could look at the project and say we do not need all 16 columns to maintain the
    bridge. As the bridge’s designers, we could argue that safety comes first; thus,
    there is no problem with having additional columns to undoubtedly guarantee the
    bridge’s integrity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Even so, what if we could spare the columns a little bit without affecting
    the bridge’s structure? To put it differently, perhaps using 16 columns to support
    the bridge is too much in terms of safety. As we can see in *Figure 6**.2*, maybe
    nine columns could work fine for this scenario:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – The bridge remains up after removing some columns](img/B20959_06_2.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – The bridge remains up after removing some columns
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: If we could put fewer columns in the bridge and keep it as safe as before, we
    would reduce the budget and building time, as well as simplify the maintenance
    process in the future. There will not be a reasonable argument to refute this
    approach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: This naïve analogy is helpful to heat our discussion about the reasons to simplify
    a model. As for the columns in the bridge, does a neural network model need all
    of its parameters to achieve good accuracy? The answer is not straightforward
    and depends on issues such as the problem type and model itself. However, considering
    that a simplified version of the model performs precisely as the original, why
    not try the former?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'After all, it is undeniable that simplifying a model has clear benefits:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Accelerating the training process**: A neural network composed of a lower
    number of parameters is usually faster to train. As discussed in [*Chapter 1*](B20959_01.xhtml#_idTextAnchor016)*,
    Deconstructing the Training Process,* the number of parameters directly impacts
    the computational burden of neural networks.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running inference on resource-constrained environments**: Some models are
    too large to store and so complex to execute that they do not fit in environments
    comprised of reduced memory and computing capacity. In this case, the only way
    to run the model in these environments relies on simplifying it as much as possible.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that the benefits of simplifying the model are crystal clear let’s jump
    to the next section to learn how to execute this process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: How to simplify a model? (process)
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We simplify a model by applying a workflow comprised of two steps: **pruning**
    and **compression**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Simplifying the workflow](img/B20959_06_3.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Simplifying the workflow
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: As pictorially described in *Figure 6**.3*, the simplifying workflow receives
    a dense neural network as input, where all neurons are fully connected to themselves,
    and this outputs a simplified version of the original model. In other words, the
    workflow transforms a dense neural network into a sparse neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The terms **dense** and **sparse** come from math and are used to describe matrices.
    A dense matrix is filled with useful numbers, whereas a sparse matrix possesses
    a relevant quantity of null values (zeros). As the parameters of neural networks
    can be expressed in *n*-dimensional matrices, a non-fully connected neural network
    is also known as a sparse neural network because of the high number of null connections
    between neurons.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Let’s zoom in on the workflow to understand the role played by each step, starting
    with the pruning phase.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The pruning phase
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **pruning phase** is responsible for receiving the original model and cutting
    off the parameters present in the connections (weights), neurons (bias), and filters
    (kernel values), resulting in a pruned model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Pruning phase](img/B20959_06_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Pruning phase
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 6**.4*, many connections from the original model have been
    disabled (represented as opaque lines in the pruned model). The pruning phase
    decides which parameters should be removed depending on the **technique** applied
    during the process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'A pruning technique has three dimensions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**Criterion**: Defines which parameters to cut off'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scope**: Determines whether to drop an entire structure (neuron, layer, or
    filter) or isolated parameters'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**: Defines whether to prune the network at once or iteratively prune
    the model until it reaches some stop criteria'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Model pruning is a brave new world. Therefore, you can easily find many scientific
    papers proposing new methods and solutions to this area. One exciting paper is
    entitled *A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis,
    and Recommendations*, which summarizes the recent advances in this field and briefly
    presents other simplifying techniques such as quantization and knowledge distillation.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the pruned model occupies the same amount of memory and requires
    the same computational capacity as the original model. This happens because the
    null parameters, although not having a practical effect on the forward and backward
    computations and results, were *not definitively removed from* *the network*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose two fully connected layers comprised of three neurons
    each. The weights of the connections can be represented as a matrix, as shown
    in *Figure 6**.5*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Weights represented as a matrix](img/B20959_06_5.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Weights represented as a matrix
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying the pruning process, the neural network had three connections
    disabled, as we can see in *Figure 6**.6*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Pruned weights changed to null](img/B20959_06_6.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Pruned weights changed to null
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Remark that the weights were changed to null (0.00), leaving the connections
    (represented by those weights) out of the calculations carried out by the network.
    Therefore, these connections merely do not exist in terms of the meaning of neural
    network results.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: However, the data structure is exactly the same as the original model. We still
    have nine float numbers, and all of them are still being multiplied (although
    with no practical effect) by the output of their respective neurons. From the
    point of view of memory consumption and computational burden, nothing has changed
    so far.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, if the purpose of simplifying a model is to reduce the number of parameters,
    why do we continue having the same structure as before? Keep calm, and let’s execute
    the second phase of the simplifying workflow: the compression phase.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Compression phase
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As illustrated in *Figure 6**.7*, the **compression phase** receives a pruned
    model as input and generates a new brain model comprised only of the non-pruned
    parameters, i.e., the parameters that the pruning process left untouched:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Compression phase](img/B20959_06_7.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Compression phase
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The new network can have a shape utterly different from the original model,
    with a distinct disposal of neurons and layers. Beyond everything, the compression
    process is free to generate a new model since it respects the parameters preserved
    by the pruning step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the compression phase effectively removes the parameters of the
    pruned model, resulting in a truly simplified model. Let’s take the example presented
    in *Figure 6**.8* to understand what happens after model compression:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Model compression applied to a pruned network](img/B20959_06_8.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Model compression applied to a pruned network
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The set of disabled parameters—connections, in this example—were removed from
    the model, reducing the weights’ matrix by one-third. Consequently, the weights’
    matrix now occupies less memory and requires fewer operations to complete the
    forward and backward phases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: We can think about the relationship between pruning and compression phases as
    the process of deleting a file from a disk. When we ask the operating system to
    delete a file, it just marks the block as free where the file is allocated. In
    other words, the file content is still there and will be erased only when overwritten
    by a new file.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: When do we simplify a model? (moment)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can simplify a model before or after training the neural network. In other
    words, the model simplification process can be applied to non-trained, pre-trained,
    or trained models, as explained in the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-trained model**: Our goal here is to speed up the training process. As
    the model has not been trained yet, the neural network is filled with random parameters,
    preventing most pruning techniques from doing a useful job. To work around this
    issue, we usually run a **warmup phase**, which consists of training the network
    over a single epoch before simplifying the model.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-trained model**: We use pre-trained networks, which have a general efficiency
    on a general domain, to tackle a particular problem of that domain. In this case,
    we do not need to execute the warmup phase because the model has already been
    trained.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trained model**: Simplifying a trained model is usually done to deploy the
    trained network in a resource-constrained environment.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have the answers to the questions about model simplification, shall
    we use PyTorch, alongside a toolkit, to simplify a model? Follow me to the next
    section to learn how to do it!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Using Microsoft NNI to simplify a model
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural Network Intelligence** (**NNI**) is an open-source project created
    by Microsoft to help deep learning practitioners automate tasks such as hyperparameter
    automatization and neural architecture searches.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: NNI also has a set of tools to deal with model simplification in a simpler and
    straightforward manner. So, we can easily simplify a model by adding a couple
    of lines to our original code. NNI supports PyTorch and other well-known deep
    learning frameworks such as TensorFlow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch has its own API to prune models, namely `torch.prune`. Unfortunately,
    at the time of writing this book, this API does not provide a mechanism to compress
    a model. Therefore, we have decided to introduce NNI as the solution to accomplish
    this task. More information about NNI can be found at [https://github.com/microsoft/nni](https://github.com/microsoft/nni).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by getting an overview of NNI in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Overview of NNI
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because NNI is not a native component of PyTorch, we need to install it via
    pip by executing the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'NNI has many modules, but for the purpose of model simplification, we are going
    to use only two of them, namely `pruning` and `speedup`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Pruning module
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `pruning` module provides a set of pruning techniques, also known as **pruners**.
    Each pruner applies a specific method to prune the model and requires a particular
    set of parameters. Among the parameters needed by the pruner, two of them are
    mandatory: the model and a **configuration list**.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The configuration list is a dictionary-based structure used to control pruner
    behavior. From the configuration list, we can indicate which structures (layers,
    operations, or filters) the pruner must work on and which ones it should leave
    untouched.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following configuration list tells the pruner to work on all
    layers implementing the `Linear` operator (layers created with the class `torch.nn.Linear`),
    except the one named `layer4`. In addition, the pruner will try to nullify 30%
    of the parameters, as indicated on the `sparse_ratio` key:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete list of key-value pairs accepted by the config list
    at [https://nni.readthedocs.io/en/stable/compression/config_list.html](https://nni.readthedocs.io/en/stable/compression/config_list.html).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the configuration list, we have everything to instantiate the
    pruner, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The most crucial method provided by the pruner is called `compress`. Despite
    what the name suggests, it executes the pruning process by applying the corresponding
    pruning algorithm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The `compress` method returns a data structure called **masks**, which denotes
    what parameters were dropped by the pruning algorithm. This information is used
    further to remove the pruned parameters from the network effectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: As stated before, the simplifying process is still ongoing. Therefore, we can
    face some tricks, such as the incoherent usage of terms. This is the reason why
    NNI calls the pruning phase `compress`, though the compressing step is accomplished
    by another method called `speedup`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note that, until this point, nothing has indeed changed to the original model;
    not yet. To remove the pruned parameters effectively, we must rely on the `speedup`
    module.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The speedup module
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `speedup` module provides a class named `ModelSpeedup`, which is used to
    create a **speeder**. A speeder executes the compression phase on the pruned model,
    i.e., it effectively removes the parameters dropped by the pruner.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the lines of pruners, we also must instantiate an object from the `ModelSpeedup`
    class. This class requires three obligatory parameters: the pruned model, an input
    sample, and the masks generated by the pruner:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After that, we just need to call the `speedup_model` method so the speeder
    can compress the model and return a simplified version of the original model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that you have an overview of the fundamental steps to simplify a model through
    NNI let’s jump to the next section to learn how to use this toolkit in a practical
    example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: NNI in action!
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see NNI working in practice let’s simplify our well-known CNN model. In this
    example, we are going to simplify this model by training it over the CIFAR-10
    dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by counting the original number of parameters of the CNN model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The CNN model has `2,122,186` parameters distributed among the biases, weights,
    and filters of the neural network. We trained this model using the CIFAR-10 dataset
    only during 10 epochs since we are interested in comparing the training time and
    corresponding accuracy between distinct pruning configurations. So, the original
    model took 122 seconds to train in a CPU-based machine, reaching an accuracy of
    47.10%.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so let’s remove some pillars of the bridge to see whether it still stands
    up. We are going to simplify the CNN model by considering the following strategy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Operation types: Conv2d'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparsity per layer: 0.50'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pruner algorithm: L1 Norm'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This strategy tells the simplification process to look at only the convolutional
    layers of the neural network, and, for each layer, the pruning algorithm must
    throw away 50% of the parameters. As we are simplifying a fresh model, we need
    to execute a warmup phase to populate the network with some valuable parameters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: For this experiment, we have chosen the L1 Norm pruner, which removes parameters
    according to the magnitude measured by L1 normalization. In simpler words, the
    pruner will drop off the parameters, which will have a minor influence on neural
    network results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information about the L1 Norm pruner at [https://nni.readthedocs.io/en/stable/reference/compression/pruner.html#l1-norm-pruner](https://nni.readthedocs.io/en/stable/reference/compression/pruner.html#l1-norm-pruner).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The following excerpt of code shows a couple of lines needed to simplify the
    CNN model by applying the aforementioned strategy:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'During the simplification process, NNI will output a bunch of lines, such as
    these:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After the process has been carried out, we can verify that the number of parameters
    of the original neural network has decreased by around 50%, as expected:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Well, the model is smaller and simpler. But how about the training time and
    efficiency? Let’s find out!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: We trained the simplified model against CIFAR-10 by using the same hyperparameters
    through the same number of epochs. The training process of the simplified model
    took 89 seconds to complete, representing a performance improvement of 37%! Although
    the model’s efficiency decreased a little bit (from 47.10% to 42.71%), it remains
    very close to the original version.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'It is interesting to note the trade-off between training time, accuracy, and
    sparsity ratio. As shown in *Table 6.1*, the model’s efficiency falls to 38.87%
    when 80% of parameters are removed from the network. On the other hand, the training
    process took only 76 seconds to finish, which is 61% faster than training the
    original network:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sparsity** **per layer** | **Training time** | **Accuracy** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| 10% | 118 | 47.26% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 20% | 113 | 45.84% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| 30% | 107 | 44.66% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| 40% | 100 | 45.18% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 50% | 89 | 42.71% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| 60% | 84 | 41.90% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| 70% | 81 | 40.84% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| 80% | 76 | 38.87% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Relation between model accuracy, training time, and sparsity level
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: As the saying goes, there is no free lunch So, accuracy is expected to deteriorate
    slightly when simplifying the model. The goal here is to find a balance between
    a tolerable decrease in the model’s quality in the face of a reasonable performance
    improvement.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to use NNI to simplify our model. By changing
    a couple of lines on our original code, we can simplify the model by cutting off
    a certain number of connections, thus contributing to reducing the training time,
    yet retaining the model’s quality.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The next section brings a couple of questions to help you retain what you have
    learned in this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Quiz time!
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s review what we have learned in this chapter by answering a few questions.
    At first, try to answer these questions without consulting the material.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The answers to all these questions are available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Before starting the quiz, remember that it is not a test at all! This section
    aims to complement your learning process by revising and consolidating the content
    covered in this chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Choose the correct option for the following questions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: What are the two steps to take when simplifying a workflow?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduction and compression.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning and reduction.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning and compression.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduction and zipping.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A pruning technique usually has the following dimensions:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Criterion, scope, and method.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithm, scope, and magnitude.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Criterion, constraints, and targets.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithm, constraints, and targets.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Concerning the compression phase, we can assert which of the following?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It receives a compressed model as input and verifies the model’s integrity.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It receives a compressed model as input and generates a model partially comprised
    only of the non-pruned parameters.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It receives a pruned model as input and generates a new brain model comprised
    only of the non-pruned parameters.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It receives a pruned model as input and evaluates the pruning degree applied
    to that model.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We can execute the model simplifying process on which of the following?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-trained models only.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-trained and non-trained models only.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-trained models only.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-trained, pre-trained, and trained models.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is one of the main goals of simplifying a trained model?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accelerate the training process.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy it on resource-constrained environments.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Improve the model’s accuracy.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no reason to simplify a trained model.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider the following configuration list passed to the pruner:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Which of the following actions would the pruner take?
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The pruner will try to nullify 75% of all network parameters.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The pruner will try to nullify 25% of the parameters of all fully connected
    layers.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The pruner will try to nullify 25% of the parameters of convolutional layers,
    except the one labeled as “layer2”.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The pruner will try to nullify 75% of the parameters of the convolutional layers,
    except the one labeled as “layer2.”
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is more likely to happen to the model’s accuracy after executing the simplified
    workflow?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model’s accuracy tends to increase.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The model’s accuracy surely increases.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The model’s accuracy tends to reduce.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The model’s accuracy stays the same.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It is necessary to execute a warmup phase before simplifying which of the following?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-trained models.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Trained models.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-trained models.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None of the above.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned that simplifying a model by reducing the number
    of parameters can accelerate the network training process, besides making the
    model feasible to run on resource-constrained platforms.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解到通过减少参数数量来简化模型可以加速网络训练过程，使模型能够在资源受限的平台上运行。
- en: 'Then, we saw that the simplification process consists of two phases: pruning
    and compression. The former is responsible for determining which parameters must
    be dropped off from the network, whereas the latter effectively removes the parameters
    from the model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们看到简化过程包括两个阶段：剪枝和压缩。前者负责确定网络中必须删除的参数，而后者则有效地从模型中移除这些参数。
- en: Although PyTorch provides an API to prune the model, it is not fully useful
    to simplify a model. Thus, you were introduced to Microsoft NNI, a powerful toolkit
    to automate tasks related to deep learning modes. Among the features provided
    by NNI, this tool offers a complete workflow to simplify a model. All of this
    is achieved with a couple of new lines added to the original code.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PyTorch提供了一个API来剪枝模型，但它并不完全有助于简化模型。因此，介绍了Microsoft NNI，一个强大的工具包，用于自动化与深度学习模型相关的任务。在NNI提供的功能中，这个工具提供了一个完整的工作流程来简化模型。所有这些都是通过向原始代码添加几行新代码来实现的。
- en: In the next chapter, you will learn how to reduce the numeric precision adopted
    by the neural network to accelerate the training process and decrease the amount
    of memory needed to store the model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将学习如何减少神经网络采用的数值精度，以加快训练过程并减少存储模型所需的内存量。
