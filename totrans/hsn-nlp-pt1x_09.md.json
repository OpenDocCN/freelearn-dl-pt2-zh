["```py\n    from torchtext import data\n    from torchtext import datasets\n    ```", "```py\n    questions = data.Field(tokenize = ‘spacy’, batch_first = True)\n    labels = data.LabelField(dtype = torch.float)\n    ```", "```py\n    pip3 install spacy\n    python3 -m spacy download en\n    ```", "```py\n    train_data, _ = datasets.TREC.splits(questions, labels)\n    train_data, valid_data = train_data.split()\n    ```", "```py\n    train_data\n    ```", "```py\ntrain_data.examples[0].text\n```", "```py\ntrain_data.examples[0].label\n```", "```py\nprint(len(train_data))\nprint(len(valid_data))\n```", "```py\nquestions.build_vocab(train_data,\n                 vectors = “glove.6B.200d”, \n                 unk_init = torch.Tensor.normal_)\nlabels.build_vocab(train_data)\n```", "```py\nquestions.vocab.vectors\n```", "```py\ndevice = torch.device(‘cuda’ if torch.cuda.is_available() else                       ‘cpu’)\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, valid_data), \n    batch_size = 64, \n    device = device)\n```", "```py\n    class CNN(nn.Module):\n        def __init__(self, vocab_size, embedding_dim,     n_filters, filter_sizes, output_dim, dropout,     pad_idx):\n\n            super().__init__()\n    ```", "```py\n    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n    ```", "```py\n    self.convs = nn.ModuleList([\n    nn.Conv2d(in_channels = 1, \n         out_channels = n_filters, \n         kernel_size = (fs, embedding_dim)) \n         \t\tfor fs in filter_sizes\n               ])\n    ```", "```py\n    self.conv_2 = nn.Conv2d(in_channels = 1, \n         out_channels = n_filters, \n         kernel_size = (2, embedding_dim)) \n    self.conv_3 = nn.Conv2d(in_channels = 1, \n         out_channels = n_filters, \n         kernel_size = (3, embedding_dim)) \n    ```", "```py\nself.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\nself.dropout = nn.Dropout(dropout)\n```", "```py\ndef forward(self, text):\nemb = self.embedding(text).unsqueeze(1)\nconved = [F.relu(c(emb)).squeeze(3) for c in self.convs]\npooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) \n          for c in conved]\nconcat = self.dropout(torch.cat(pooled, dim = 1))\nreturn self.fc(concat)\n```", "```py\ninput_dimensions = len(questions.vocab)\noutput_dimensions = 6\nembedding_dimensions = 200\npad_index = questions.vocab.stoi[questions.pad_token]\nnumber_of_filters = 100\nfilter_sizes = [2,3,4]\ndropout_pc = 0.5\nmodel = CNN(input_dimensions, embedding_dimensions, number_of_filters, filter_sizes, output_dimensions, dropout_pc, pad_index)\n```", "```py\nglove_embeddings = questions.vocab.vectors\nmodel.embedding.weight.data.copy_(glove_embeddings)\n```", "```py\nunknown_index = questions.vocab.stoi[questions.unk_token]\nmodel.embedding.weight.data[unknown_index] = torch.zeros(embedding_dimensions)\nmodel.embedding.weight.data[pad_index] = torch.zeros(embedding_dimensions)\n```", "```py\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss().to(device)\nmodel = model.to(device)\n```", "```py\ndef multi_accuracy(preds, y):\n    pred = torch.max(preds,1).indices\n    correct = (pred == y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n```", "```py\ndef train(model, iterator, optimizer, criterion):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n```", "```py\nfor batch in iterator:\n\noptimizer.zero_grad()\n\npreds = model(batch.text).squeeze(1)\nloss = criterion(preds, batch.label.long())\n\nacc = multi_accuracy(preds, batch.label)\n\nloss.backward()\n\noptimizer.step()\n```", "```py\nepoch_loss += loss.item()\nepoch_acc += acc.item()\n\ntotal_epoch_loss = epoch_loss / len(iterator)\ntotal_epoch_accuracy = epoch_acc / len(iterator)\n\nreturn total_epoch_loss, total_epoch_accuracy\n```", "```py\nmodel.eval()\n\nwith torch.no_grad():\n```", "```py\nepochs = 10\nlowest_validation_loss = float(‘inf’)\n```", "```py\nfor epoch in range(epochs):\n    start_time = time.time()\n\n    train_loss, train_acc = train(model, train_iterator,                            optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator,                            criterion)\n\n    end_time = time.time()\n```", "```py\nif valid_loss < lowest_validation_loss:\n    lowest_validation_loss = valid_loss\n    torch.save(model.state_dict(), ‘cnn_model.pt’)\n```", "```py\nprint(f’Epoch: {epoch+1:02} | Epoch Time: {int(end_time -       start_time)}s’)\nprint(f’\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_      acc*100:.2f}%’)\nprint(f’\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_      acc*100:.2f}%’)\n```", "```py\nmodel.load_state_dict(torch.load(‘cnn_model.pt’))\n```", "```py\ndef predict_class(model, sentence, min_len = 5):\n\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n    if len(tokenized) < min_len:\n        tokenized += [‘<pad>’] * (min_len - len(tokenized))\n    indexed = [questions.vocab.stoi[t] for t in tokenized]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(0)\n```", "```py\nmodel.eval()\nprediction = torch.max(model(tensor),1).indices.item()\npred_index = labels.vocab.itos[prediction]\n    return pred_index\n```", "```py\npred_class = predict_class(model, “How many roads must a man                            walk down?”)\nprint(‘Predicted class is: ‘ + str(pred_class))\n```"]