["```py\npip install -q openml openml_speed_dating_pipeline_steps==0.5.5 imbalanced_learn category_encoders shap\n```", "```py\nimport openml\ndataset = openml.datasets.get_dataset(40536)\nX, y, categorical_indicator, _ = dataset.get_data(\n  dataset_format='DataFrame',\n  target=dataset.default_target_attribute\n)\ncategorical_features = list(X.columns[categorical_indicator]) numeric_features = list(\n  X.columns[[not(i) for i in categorical_indicator]]\n)\n```", "```py\n[[0-0.33], [0.33-1], [-1-0]]\n```", "```py\ndef encode_ranges(range_str):\n  splits = range_str[1:-1].split('-')\n  range_max = splits[-1]\n  range_min = '-'.join(splits[:-1])\n  return range_min, range_max\n\nexamples = X['d_interests_correlate'].unique()\n[encode_ranges(r) for r in examples]\n```", "```py\n[('0', '0.33'), ('0.33', '1'), ('-1', '0')]\n```", "```py\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport category_encoders.utils as util\n\nclass RangeTransformer(BaseEstimator, TransformerMixin):\n  def __init__(self, range_features=None, suffix='_range/mean', n_jobs=-1):\n    assert isinstance(range_features, list) or range_features is None\n    self.range_features = range_features\n    self.suffix = suffix\n    self.n_jobs = n_jobs\n\n  def fit(self, X, y=None):\n    return self\n\n  def transform(self, X, y=None):\n    X = util.convert_input(X)\n    if self.range_features is None:\n      self.range_features = list(X.columns)\n\n    range_data = pd.DataFrame(index=X.index)\n    for col in self.range_features:\n      range_data[str(col) + self.suffix] = pd.to_numeric(\n        self._vectorize(X[col])\n      )\n    self.feature_names = list(range_data.columns)\n    return range_data\n\n    def _vectorize(self, s):\n        return Parallel(n_jobs=self.n_jobs)(\n            delayed(self._encode_range)(x) for x in s\n        )\n\n    @staticmethod\n    @lru_cache(maxsize=32)\n    def _encode_range(range_str):\n        splits = range_str[1:-1].split('-')\n        range_max = float(splits[-1])\n        range_min = float('-'.join(splits[:-1]))\n        return sum([range_min, range_max]) / 2.0\n\n    def get_feature_names(self):\n        return self.feature_names\n```", "```py\nimport operator\n\nclass NumericDifferenceTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, features=None,\n                 suffix='_numdist', op=operator.sub, n_jobs=-1\n                 ):\n        assert isinstance(\n            features, list\n        ) or features is None\n        self.features = features\n        self.suffix = suffix\n        self.op = op\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        X = util.convert_input(X)\n        if self.features is None:\n            self.features = list(\n                X.select_dtypes(include='number').columns\n            )\n        return self\n\n    def _col_name(self, col1, col2):\n        return str(col1) + '_' + str(col2) + self.suffix\n\n    def _feature_pairs(self):\n        feature_pairs = []\n        for i, col1 in enumerate(self.features[:-1]):\n            for col2 in self.features[i+1:]:\n                feature_pairs.append((col1, col2))\n        return feature_pairs\n\n    def transform(self, X, y=None):\n        X = util.convert_input(X)\n\n        feature_pairs = self._feature_pairs()\n        columns = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._col_name)(col1, col2)\n            for col1, col2 in feature_pairs\n        )\n        data_cols = Parallel(n_jobs=self.n_jobs)(\n            delayed(self.op)(X[col1], X[col2])\n            for col1, col2 in feature_pairs\n        )\n        data = pd.concat(data_cols, axis=1)\n        data.rename(\n            columns={i: col for i, col in enumerate(columns)},\n            inplace=True, copy=False\n        )\n        data.index = X.index\n        return data\n\n    def get_feature_names(self):\n        return self.feature_names\n```", "```py\nimport operator\noperator.sub(1, 2) == 1 - 2\n# True\n```", "```py\nrange_cols = [\n    col for col in X.select_dtypes(include='category')\n    if X[col].apply(lambda x: x.startswith('[')\n    if isinstance(x, str) else False).any()\n]\ncat_columns = list(\n  set(X.select_dtypes(include='category').columns) - set(range_cols)\n)\nnum_columns = list(\n    X.select_dtypes(include='number').columns\n)\n```", "```py\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nimport category_encoders as ce\nimport openml_speed_dating_pipeline_steps as pipeline_steps\n\npreprocessor = ColumnTransformer(\n transformers=[\n ('ranges', Pipeline(steps=[\n ('impute', pipeline_steps.SimpleImputerWithFeatureNames(strategy='constant', fill_value=-1)),\n ('encode', pipeline_steps.RangeTransformer())\n ]), range_cols),\n ('cat', Pipeline(steps=[\n ('impute', pipeline_steps.SimpleImputerWithFeatureNames(strategy='constant', fill_value='-1')),\n ('encode', ce.OneHotEncoder(\n cols=None, # all features that it given by ColumnTransformer\n handle_unknown='ignore',\n use_cat_names=True\n )\n )\n ]), cat_columns),\n ('num', pipeline_steps.SimpleImputerWithFeatureNames(strategy='median'), num_columns),\n ],\n remainder='drop', n_jobs=-1\n)\n```", "```py\ndef create_model(n_estimators=100):\n    return Pipeline(\n        steps=[('preprocessor', preprocessor),\n               ('numeric_differences', pipeline_steps.NumericDifferenceTransformer()),\n               ('feature_selection', SelectKBest(f_classif, k=20)),\n               ('rf', BalancedRandomForestClassifier(\n                  n_estimators=n_estimators,\n                  )\n               )]\n       )\n```", "```py\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.33,\n  random_state=42,\n  stratify=y\n)\nclf = create_model(50)\nclf.fit(X_train, y_train)\ny_predicted = clf.predict(X_test)\nauc = roc_auc_score(y_test, y_predicted)\nprint('auc: {:.3f}'.format(auc))\n```", "```py\nauc: 0.779\n```", "```py\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfeature_preprocessing = make_column_transformer(\n  (StandardScaler(), ['column1', 'column2']),\n  (OneHotEncoder(), ['column3', 'column4', 'column5']) \n)\n```", "```py\nprocessed_features = feature_preprocessing.fit_transform(X)\n\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nmake_pipeline(\n    feature_preprocessing,\n    LogisticRegression()\n)\n```", "```py\nfrom sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=42165, as_frame=True)\n\n```", "```py\nimport pandas as pd\ndata_ames = pd.DataFrame(data.data, columns=data.feature_names)\ndata_ames['SalePrice'] = data.target\ndata_ames.info()\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\nId               1460 non-null float64\nMSSubClass       1460 non-null float64\nMSZoning         1460 non-null object\nLotFrontage      1201 non-null float64\nLotArea          1460 non-null float64\nStreet           1460 non-null object\nAlley            91 non-null object\nLotShape         1460 non-null object\nLandContour      1460 non-null object\nUtilities        1460 non-null object\nLotConfig        1460 non-null object\nLandSlope        1460 non-null object\nNeighborhood     1460 non-null object\nCondition1       1460 non-null object\nCondition2       1460 non-null object\nBldgType         1460 non-null object\nHouseStyle       1460 non-null object\nOverallQual      1460 non-null float64\nOverallCond      1460 non-null float64\nYearBuilt        1460 non-null float64\nYearRemodAdd     1460 non-null float64\nRoofStyle        1460 non-null object\nRoofMatl         1460 non-null object\nExterior1st      1460 non-null object\nExterior2nd      1460 non-null object\nMasVnrType       1452 non-null object\nMasVnrArea       1452 non-null float64\nExterQual        1460 non-null object\nExterCond        1460 non-null object\nFoundation       1460 non-null object\nBsmtQual         1423 non-null object\nBsmtCond         1423 non-null object\nBsmtExposure     1422 non-null object\nBsmtFinType1     1423 non-null object\nBsmtFinSF1       1460 non-null float64\nBsmtFinType2     1422 non-null object\nBsmtFinSF2       1460 non-null float64\nBsmtUnfSF        1460 non-null float64\nTotalBsmtSF      1460 non-null float64\nHeating          1460 non-null object\nHeatingQC        1460 non-null object\nCentralAir       1460 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1460 non-null float64\n2ndFlrSF         1460 non-null float64\nLowQualFinSF     1460 non-null float64\nGrLivArea        1460 non-null float64\nBsmtFullBath     1460 non-null float64\nBsmtHalfBath     1460 non-null float64\nFullBath         1460 non-null float64\nHalfBath         1460 non-null float64\nBedroomAbvGr     1460 non-null float64\nKitchenAbvGr     1460 non-null float64\nKitchenQual      1460 non-null object\nTotRmsAbvGrd     1460 non-null float64\nFunctional       1460 non-null object\nFireplaces       1460 non-null float64\nFireplaceQu      770 non-null object\nGarageType       1379 non-null object\nGarageYrBlt      1379 non-null float64\nGarageFinish     1379 non-null object\nGarageCars       1460 non-null float64\nGarageArea       1460 non-null float64\nGarageQual       1379 non-null object\nGarageCond       1379 non-null object\nPavedDrive       1460 non-null object\nWoodDeckSF       1460 non-null float64\nOpenPorchSF      1460 non-null float64\nEnclosedPorch    1460 non-null float64\n3SsnPorch        1460 non-null float64\nScreenPorch      1460 non-null float64\nPoolArea         1460 non-null float64\nPoolQC           7 non-null object\nFence            281 non-null object\nMiscFeature      54 non-null object\nMiscVal          1460 non-null float64\nMoSold           1460 non-null float64\nYrSold           1460 non-null float64\nSaleType         1460 non-null object\nSaleCondition    1460 non-null object\nSalePrice        1460 non-null float64\ndtypes: float64(38), object(43)\nmemory usage: 924.0+ KB\n```", "```py\n!pip install captum\n```", "```py\nimport numpy as np\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = list(data_ames.select_dtypes(include='float'))\ncat_cols = list(data_ames.select_dtypes(include='object'))\n\nordinal_encoder = OrdinalEncoder().fit(\n    data_ames[cat_cols]\n)\nstandard_scaler = StandardScaler().fit(\n    data_ames[num_cols]\n)\n\nX = pd.DataFrame(\n    data=np.column_stack([\n        ordinal_encoder.transform(data_ames[cat_cols]),\n        standard_scaler.transform(data_ames[num_cols])\n    ]),\n    columns=cat_cols + num_cols\n)\n```", "```py\nnp.random.seed(12)  \nfrom sklearn.model_selection import train_test_split\n\nbins = 5\nsale_price_bins = pd.qcut(\n    X['SalePrice'], q=bins, labels=list(range(bins))\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X.drop(columns='SalePrice'),\n    X['SalePrice'],\n    random_state=12,\n    stratify=sale_price_bins\n)\n```", "```py\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True\n```", "```py\nfrom torch.autograd import Variable \n\nnum_features = list(\n    set(num_cols) - set(['SalePrice', 'Id'])\n)\nX_train_num_pt = Variable(\n    torch.cuda.FloatTensor(\n        X_train[num_features].values\n    )\n)\nX_train_cat_pt = Variable(\n    torch.cuda.LongTensor(\n        X_train[cat_cols].values\n    )\n)\ny_train_pt = Variable(\n    torch.cuda.FloatTensor(y_train.values)\n).view(-1, 1)\nX_test_num_pt = Variable(\n    torch.cuda.FloatTensor(\n        X_test[num_features].values\n    )\n)\nX_test_cat_pt = Variable(\n   torch.cuda.LongTensor(\n        X_test[cat_cols].values\n    ).long()\n)\ny_test_pt = Variable(\n    torch.cuda.FloatTensor(y_test.values)\n).view(-1, 1)\n```", "```py\nclass RegressionModel(torch.nn.Module): \n\n    def __init__(self, X, num_cols, cat_cols, device=torch.device('cuda'), embed_dim=2, hidden_layer_dim=2, p=0.5): \n        super(RegressionModel, self).__init__() \n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.embed_dim = embed_dim\n        self.hidden_layer_dim = hidden_layer_dim\n\n        self.embeddings = [\n            torch.nn.Embedding(\n                num_embeddings=len(X[col].unique()),\n                embedding_dim=embed_dim\n            ).to(device)\n            for col in cat_cols\n        ]\n        hidden_dim = len(num_cols) + len(cat_cols) * embed_dim,\n\n        # hidden layer\n        self.hidden = torch.nn.Linear(torch.IntTensor(hidden_dim), hidden_layer_dim).to(device)\n        self.dropout_layer = torch.nn.Dropout(p=p).to(device)\n        self.hidden_act = torch.nn.ReLU().to(device)\n\n        # output layer\n        self.output = torch.nn.Linear(hidden_layer_dim, 1).to(device)\n\n    def forward(self, num_inputs, cat_inputs):\n        '''Forward method with two input variables -\n        numeric and categorical.\n        '''\n        cat_x = [\n            torch.squeeze(embed(cat_inputs[:, i] - 1))\n            for i, embed in enumerate(self.embeddings)\n        ]\n        x = torch.cat(cat_x + [num_inputs], dim=1)\n        x = self.hidden(x)\n        x = self.dropout_layer(x)\n        x = self.hidden_act(x)\n        y_pred = self.output(x)\n        return y_pred\n\nhouse_model = RegressionModel(\n    data_ames, num_features, cat_cols\n)\n```", "```py\ncriterion = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.SGD(house_model.parameters(), lr=0.001)\n```", "```py\ndata_batch = torch.utils.data.TensorDataset(\n    X_train_num_pt, X_train_cat_pt, y_train_pt\n)\ndataloader = torch.utils.data.DataLoader(\n    data_batch, batch_size=10, shuffle=True\n)\n```", "```py\nfrom tqdm.notebook import trange\n\ntrain_losses, test_losses = [], []\nn_epochs = 30\nfor epoch in trange(n_epochs):\n    train_loss, test_loss = 0, 0\n    # training code will go here:\n    # <...>\n\n    # print the errors in training and test:\n    if epoch % 10 == 0 :\n        print(\n            'Epoch: {}/{}\\t'.format(epoch, 1000),\n            'Training Loss: {:.3f}\\t'.format(\n                train_loss / len(dataloader)\n            ),\n            'Test Loss: {:.3f}'.format(\n                test_loss / len(dataloader)\n            )\n        )\n```", "```py\n    for (x_train_num_batch,\n         x_train_cat_batch,\n         y_train_batch) in dataloader:\n        # predict y by passing x to the model \n        (x_train_num_batch,\n         x_train_cat_batch, y_train_batch) = (\n                x_train_num_batch.to(device),\n                x_train_cat_batch.to(device),\n                y_train_batch.to(device)\n        )\n        pred_ytrain = house_model.forward(\n            x_train_num_batch, x_train_cat_batch\n        )\n        # calculate and print loss:\n        loss = torch.sqrt(\n            criterion(pred_ytrain, y_train_batch)\n        ) \n\n        # zero gradients, perform a backward pass, \n        # and update the weights. \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()\n        train_loss += loss.item()\n        with torch.no_grad():\n            house_model.eval()\n            pred_ytest = house_model.forward(\n                X_test_num_pt, X_test_cat_pt\n            )\n            test_loss += torch.sqrt(\n                criterion(pred_ytest, y_test_pt)\n            )\n\n        train_losses.append(train_loss / len(dataloader))\n        test_losses.append(test_loss / len(dataloader))\n```", "```py\n((input-target)**2).mean()\n```", "```py\nplt.plot(\n    np.array(train_losses).reshape((n_epochs, -1)).mean(axis=1),\n    label='Training loss'\n)\nplt.plot(\n    np.array(test_losses).reshape((n_epochs, -1)).mean(axis=1),\n    label='Validation loss'\n)\nplt.legend(frameon=False)\nplt.xlabel('epochs')\nplt.ylabel('MSE')\n```", "```py\nopt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)\nopt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.6)\nopt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.1)\nopt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.8, 0.98))\n```", "```py\nfrom captum.attr import (\n    IntegratedGradients,\n    LayerConductance,\n    NeuronConductance\n)\nhouse_model.cpu()\nfor embedding in house_model.embeddings:\n    embedding.cpu()\n\nhouse_model.cpu()\ning_house = IntegratedGradients(forward_func=house_model.forward, )\n#X_test_cat_pt.requires_grad_()\nX_test_num_pt.requires_grad_()\nattr, delta = ing_house.attribute(\n X_test_num_pt.cpu(),\n target=None,\n return_convergence_delta=True,\n additional_forward_args=X_test_cat_pt.cpu()\n)\nattr = attr.detach().numpy()\n```", "```py\ncond_layer1 = LayerConductance(house_model, house_model.act1)\ncond_vals = cond_layer1.attribute(X_test, target=None)\ncond_vals = cond_vals.detach().numpy()\ndf_neuron = pd.DataFrame(data = np.mean(cond_vals, axis=0), columns=['Neuron Importance'])\ndf_neuron['Neuron'] = range(10)\n```", "```py\ndf_feat = pd.DataFrame(np.mean(attr, axis=0), columns=['feature importance'] )\ndf_feat['features'] = num_features\ndf_feat.sort_values(\n    by='feature importance', ascending=False\n).head(10)\n```", "```py\n!pip install -q openml\n\nimport openml \ndataset = openml.datasets.get_dataset(1461)\nX, y, categorical_indicator, _ = dataset.get_data(\n  dataset_format='DataFrame',\n  target=dataset.default_target_attribute\n)\ncategorical_features = X.columns[categorical_indicator]\nnumeric_features = X.columns[\n  [not(i) for i in categorical_indicator]\n]\n```", "```py\n!pip install scikit-multiflow category_encoders\n```", "```py\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nimport category_encoders as ce\n\nordinal_encoder = ce.OrdinalEncoder(\n  cols=None,  # all features that it encounters\n  handle_missing='return_nan',\n  handle_unknown='ignore'\n).fit(X)\n\npreprocessor = ColumnTransformer(\n  transformers=[\n    ('cat', ordinal_encoder, categorical_features),\n    ('num', FunctionTransformer(validate=False), numeric_features)\n])\npreprocessor = preprocessor.fit(X)\n```", "```py\nimport numpy as np\nfrom skmultiflow.trees.hoeffding_tree import HoeffdingTreeClassifier \nfrom sklearn.metrics import roc_auc_score\nimport random\n\nclass ActivePipeline:\n  def __init__(self, model, preprocessor, class_weights):\n    self.model = model\n    self.preprocessor = preprocessor\n    self.class_weights = class_weights\n\n  @staticmethod\n  def values(X):\n    if isinstance(X, (np.ndarray, np.int64)):\n      return X\n    else:\n      return X.values\n\n  def preprocess(self, X):\n    X_ = pd.DataFrame(\n      data=self.values(X),\n      columns=[\n        'V1', 'V2', 'V3', 'V4',\n        'V5', 'V6', 'V7', 'V8',\n        'V9', 'V10', 'V11', 'V12',\n        'V13', 'V14', 'V15', 'V16'\n      ])\n    return self.preprocessor.transform(X_)\n\n  def fit(self, X, ys):\n    weights = [self.class_weights[y] for y in ys]\n    self.model.fit(self.preprocess(X), self.values(ys))\n\n  def update(self, X, ys):\n    if isinstance(ys, (int, float)):\n      weight = self.class_weights[y]\n    else:\n      weight = [self.class_weights[y] for y in ys]\n\n    self.model.partial_fit(\n      self.preprocess(X),\n      self.values(ys),\n      weight\n    )\n\n  def predict(self, X):\n    return self.model.predict(\n      self.preprocess(X)\n    )\n\n  def predict_proba(self, X):\n    return self.model.predict_proba(\n      self.preprocess(X)\n    )\n\n  @staticmethod\n  def entropy(preds):\n    return -np.sum(\n      np.log((preds + 1e-15) * preds)\n      / np.log(np.prod(preds.size))\n    )\n\n  def max_margin_uncertainty(self, X, method: str='entropy',\n      exploitation: float=0.9, favor_class: int=1, k: int=1\n  ):\n    '''similar to modAL.uncertainty.margin_uncertainty\n    ''' \n    probs = self.predict_proba(X)\n    if method=='margin':\n      uncertainties = np.abs(probs[:,2] - probs[:, 1]) / 2.0\n    elif method=='entropy':\n      uncertainties = np.apply_along_axis(self.entropy, 1, probs[:, (1,2)])\n    else: raise(ValueError('method not implemented!'))\n\n    if favor_class is None:\n      weights = uncertainties \n    else: weights = (1.0 - exploitation) * uncertainties + exploitation * probs[:, favor_class]\n\n    if self.sampling:\n      ind = random.choices(\n        range(len(uncertainties)), weights, k=k\n      )\n    else:\n      ind = np.argsort(weights, axis=0)[::-1][:k]\n    return ind, np.mean(uncertainties[ind])\n\n  def score(self, X, y, scale=True):\n    probs = self.predict_proba(X, probability=2)\n    if scale:\n      probs = np.clip(probs - np.mean(probs) + 0.5, 0, 1)\n    return roc_auc_score(y, probs)\n```", "```py\nactive_pipeline = ActivePipeline(\n  HoeffdingTreeClassifier(),\n  preprocessor,\n  class_weights.to_dict()\n)\nactive_pipeline.model.classes = [0, 1, 2]\n```", "```py\nclass_weights = len(X) / (y.astype(int).value_counts() * 2)\n```", "```py\n!wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\nimport pandas as pd\ndate_cols = [\n    'compas_screening_date', 'c_offense_date',\n    'c_arrest_date', 'r_offense_date', \n    'vr_offense_date', 'screening_date',\n    'v_screening_date', 'c_jail_in',\n    'c_jail_out', 'dob', 'in_custody', \n    'out_custody'\n]\ndata = pd.read_csv(\n    'compas-scores-two-years.csv',\n    parse_dates=date_cols\n)\n```", "```py\nimport datetime\nindexes = data.compas_screening_date <= pd.Timestamp(datetime.date(2014, 4, 1))\nassert indexes.sum() == 6216\ndata = data[indexes]\n```", "```py\n!pip install category-encoders\n```", "```py\ndef confusion_metrics(actual, scores, threshold):\n    y_predicted = scores.apply(\n        lambda x: x >= threshold\n    ).values\n    y_true = actual.values\n    TP = (\n        (y_true==y_predicted) & \n        (y_predicted==1)\n    ).astype(int)\n    FP = (\n        (y_true!=y_predicted) &\n        (y_predicted==1)\n    ).astype(int)\n    TN = (\n        (y_true==y_predicted) &\n        (y_predicted==0)\n    ).astype(int)\n    FN = (\n        (y_true!=y_predicted) &\n        (y_predicted==0)\n    ).astype(int)\n    return TP, FP, TN, FN\n\n```", "```py\ndef calculate_impacts(data, sensitive_column='race', recid_col='is_recid', score_col='decile_score.1', threshold=5.0):\n    if sensitive_column == 'race':\n      norm_group = 'Caucasian'\n    elif sensitive_column == 'sex':\n      norm_group = 'Male'\n    else:\n      raise ValueError('sensitive column not implemented')\n    TP, FP, TN, FN = confusion_metrics(\n        actual=data[recid_col],\n        scores=data[score_col],\n        threshold=threshold\n    )\n    impact = pd.DataFrame(\n        data=np.column_stack([\n              FP, TN, FN, TN,\n              data[sensitive_column].values, \n              data[recid_col].values,\n              data[score_col].values / 10.0\n             ]),\n        columns=['FP', 'TP', 'FN', 'TN', 'sensitive', 'reoffend', 'score']\n    ).groupby(by='sensitive').agg({\n        'reoffend': 'sum', 'score': 'sum',\n        'sensitive': 'count', \n        'FP': 'sum', 'TP': 'sum', 'FN': 'sum', 'TN': 'sum'\n    }).rename(\n        columns={'sensitive': 'N'}\n    )\n\n    impact['FPR'] = impact['FP'] / (impact['FP'] + impact['TN'])\n    impact['FNR'] = impact['FN'] / (impact['FN'] + impact['TP'])\n    impact['reoffend'] = impact['reoffend'] / impact['N']\n    impact['score'] = impact['score'] / impact['N']\n    impact['DFP'] = impact['FPR'] / impact.loc[norm_group, 'FPR']\n    impact['DFN'] = impact['FNR'] / impact.loc[norm_group, 'FNR']\n    return impact.drop(columns=['FP', 'TP', 'FN', 'TN'])\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom category_encoders.one_hot import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ncharge_desc = data['c_charge_desc'].apply(lambda x: x if isinstance(x, str) else '')\ncount_vectorizer = CountVectorizer(\n    max_df=0.85, stop_words='english',\n    max_features=100, decode_error='ignore'\n)\ncharge_desc_features = count_vectorizer.fit_transform(charge_desc)\n\none_hot_encoder = OneHotEncoder()\ncharge_degree_features = one_hot_encoder.fit_transform(\n    data['c_charge_degree']\n)\n\ndata['race_black'] = data['race'].apply(lambda x: x == 'African-American').astype(int)\nstratification = data['race_black'] + (data['is_recid']).astype(int) * 2\n\n```", "```py\ny = data['is_recid']\nX = pd.DataFrame(\n    data=np.column_stack(\n        [data[['juv_fel_count', 'juv_misd_count',\n 'juv_other_count', 'priors_count', 'days_b_screening_arrest']], \n          charge_degree_features, \n          charge_desc_features.todense()\n        ]\n    ),\n    columns=['juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'days_b_screening_arrest'] \\\n    + one_hot_encoder.get_feature_names() \\\n    + count_vectorizer.get_feature_names(),\n    index=data.index\n)\nX['jailed_days'] = (data['c_jail_out'] - data['c_jail_in']).apply(lambda x: abs(x.days))\nX['waiting_jail_days'] = (data['c_jail_in'] - data['c_offense_date']).apply(lambda x: abs(x.days))\nX['waiting_arrest_days'] = (data['c_arrest_date'] - data['c_offense_date']).apply(lambda x: abs(x.days))\nX.fillna(0, inplace=True)\n\ncolumns = list(X.columns)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33,\n    random_state=42,\n    stratify=stratification\n)  # we stratify by black and the target\n```", "```py\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, ops, lax\nimport numpy.random as npr\nimport numpy as onp\nimport random\nfrom tqdm import trange\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass JAXLearner(ClassifierMixin):\n  def __init__(self, layer_sizes=[10, 5, 1], epochs=20, batch_size=500, lr=1e-2):\n    self.params = self.construct_network(layer_sizes)\n    self.perex_grads = jit(grad(self.error))\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.lr = lr\n\n  @staticmethod\n  def construct_network(layer_sizes=[10, 5, 1]):\n    '''Please make sure your final layer corresponds to targets in dimensions.\n    '''\n    def init_layer(n_in, n_out):\n      W = npr.randn(n_in, n_out)\n      b = npr.randn(n_out,)\n      return W, b\n\n    return list(map(init_layer, layer_sizes[:-1], layer_sizes[1:]))\n\n  @staticmethod\n  def sigmoid(X):  # or tanh\n    return 1/(1+jnp.exp(-X))\n\n  def _predict(self, inputs):\n    for W, b in self.params:\n      outputs = jnp.dot(inputs, W) + b\n      inputs = self.sigmoid(outputs)\n    return outputs\n\n  def predict(self, inputs):\n    inputs = self.standard_scaler.transform(inputs)\n    return onp.asarray(self._predict(inputs))\n\n  @staticmethod\n  def mse(preds, targets, other=None):\n    return jnp.sqrt(jnp.sum((preds - targets)**2))\n\n  @staticmethod\n  def penalized_mse(preds, targets, sensitive):\n    err = jnp.sum((preds - targets)**2)\n    err_s = jnp.sum((preds * sensitive - targets * sensitive)**2)\n    penalty = jnp.clip(err_s / err, 1.0, 2.0)\n    return err * penalty\n\n  def error(self, params, inputs, targets, sensitive):\n      preds = self._predict(inputs)\n      return self.penalized_mse(preds, targets, sensitive)\n\n  def fit(self, X, y, sensitive):\n    self.standard_scaler = StandardScaler()\n    X = self.standard_scaler.fit_transform(X)\n    N = X.shape[0]\n    indexes = list(range(N))\n    steps_per_epoch = N // self.batch_size\n\n    for epoch in trange(self.epochs, desc='training'):\n        random.shuffle(indexes)\n        index_offset = 0\n        for step in trange(steps_per_epoch, desc='iteration'):\n            grads = self.perex_grads(\n                self.params, \n                X[indexes[index_offset:index_offset+self.batch_size], :], \n                y[indexes[index_offset:index_offset+self.batch_size]],\n                sensitive[indexes[index_offset:index_offset+self.batch_size]]\n            )\n            # print(grads)\n            self.params = [(W - self.lr * dW, b - self.lr * db)\n                      for (W, b), (dW, db) in zip(self.params, grads)]\n            index_offset += self.batch_size\n```", "```py\nsensitive_train = X_train.join(\n    data, rsuffix='_right'\n)['race_black']\njax_learner = JAXLearner([X.values.shape[1], 100, 1])\njax_learner.fit(\n    X_train.values,\n    y_train.values,\n    sensitive_train.values\n)\n\n```", "```py\nX_predicted = pd.DataFrame(\n    data=jax_learner.predict(\n        X_test.values\n    ) * 10,\n    columns=['score'], \n    index=X_test.index\n).join(\n    data[['sex', 'race', 'is_recid']], \n    rsuffix='_right'\n)\ncalculate_impacts(X_predicted, score_col='score')\n\n```", "```py\n  @staticmethod\n  def penalized_mse(preds, targets, sensitive):\n    err = jnp.sum((preds - targets)**2)\n    err_s = jnp.sum((preds * sensitive - targets * sensitive)**2)\n    penalty = jnp.clip(err_s / err, 1.0, 2.0)\n    return err * penalty\n```", "```py\n%%cython\n\nfrom sklearn.tree._criterion cimport ClassificationCriterion\nfrom sklearn.tree._criterion cimport SIZE_t\n\nimport numpy as np\ncdef double INFINITY = np.inf\n\nfrom libc.math cimport sqrt, pow\nfrom libc.math cimport abs\n\ncdef class PenalizedHellingerDistanceCriterion(ClassificationCriterion):    \n\n    cdef double proxy_impurity_improvement(self) nogil:\n        cdef double impurity_left\n        cdef double impurity_right\n\n        self.children_impurity(&impurity_left, &impurity_right)\n\n        return impurity_right + impurity_left\n\n    cdef double impurity_improvement(self, double impurity) nogil:\n        cdef double impurity_left\n        cdef double impurity_right\n\n        self.children_impurity(&impurity_left, &impurity_right)\n        return impurity_right + impurity_left\n\n    cdef double node_impurity(self) nogil:\n        cdef SIZE_t* n_classes = self.n_classes\n        cdef double* sum_total = self.sum_total\n        cdef double hellinger = 0.0\n        cdef double sq_count\n        cdef double count_k\n        cdef SIZE_t k\n        cdef SIZE_t c\n\n        for k in range(self.n_outputs):\n            for c in range(n_classes[k]):\n                hellinger += 1.0\n\n        return hellinger / self.n_outputs\n\n    cdef void children_impurity(self, double* impurity_left,\n                                double* impurity_right) nogil:\n        cdef SIZE_t* n_classes = self.n_classes\n        cdef double* sum_left = self.sum_left\n        cdef double* sum_right = self.sum_right\n        cdef double hellinger_left = 0.0\n        cdef double hellinger_right = 0.0\n        cdef double count_k1 = 0.0\n        cdef double count_k2 = 0.0\n\n        cdef SIZE_t k\n        cdef SIZE_t c\n\n        # stop splitting in case reached pure node with 0 samples of second class\n        if sum_left[1] + sum_right[1] == 0:\n            impurity_left[0] = -INFINITY\n            impurity_right[0] = -INFINITY\n            return\n\n        for k in range(self.n_outputs):\n            if(sum_left[0] + sum_right[0] > 0):\n                count_k1 = sqrt(sum_left[0] / (sum_left[0] + sum_right[0]))\n            if(sum_left[1] + sum_right[1] > 0):\n                count_k2 = sqrt(sum_left[1] / (sum_left[1] + sum_right[1]))\n\n            hellinger_left += pow((count_k1  - count_k2), 2)\n\n            if(sum_left[0] + sum_right[0] > 0):    \n                count_k1 = sqrt(sum_right[0] / (sum_left[0] + sum_right[0]))\n            if(sum_left[1] + sum_right[1] > 0):\n                count_k2 = sqrt(sum_right[1] / (sum_left[1] + sum_right[1]))\n\n            if k==0:\n              hellinger_right += pow((count_k1  - count_k2), 2)\n            else:\n              hellinger_right -= pow((count_k1  - count_k2), 2)\n\n        impurity_left[0]  = hellinger_left  / self.n_outputs\n        impurity_right[0] = hellinger_right / self.n_outputs\n```", "```py\nensemble = [\n    DecisionTreeClassifier(\n      criterion=PenalizedHellingerDistanceCriterion(\n        2, np.array([2, 2], dtype='int64')\n      ),\n      max_depth=100\n    ) for i in range(100)\n]\nfor model in ensemble:\n    model.fit(\n        X_train,\n        X_train.join(\n            data,\n            rsuffix='_right'\n        )[['is_recid', 'race_black']]\n    )\nY_pred = np.array(\n    [model.predict(X_test) for model in\n     ensemble]\n)\npredictions2 = Y_pred.mean(axis=0)\n```", "```py\npip install statsmodels fbprophet\n```", "```py\nX,y = load_mauna_loa_atmospheric_co2()\n```", "```py\ndf_CO2 = pd.DataFrame(data = X, columns = ['Year'])\ndf_CO2['CO2 in ppm'] = y\nlm = sns.lmplot(x='Year', y='CO2 in ppm', data=df_CO2, height=4, aspect=4)\nfig = lm.fig \nfig.suptitle('CO2 conc. mauna_loa 1958-2001', fontsize=12)\n```", "```py\nimport statsmodels.api as stmd\nd = stm.datasets.co2.load_pandas()\nco2 = d.data\nco2.head()\ny = co2['co2']\ny = y.fillna(\n    y.interpolate()\n)  # Fill missing values by interpolation\n```", "```py\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 11, 9\nresult = stm.tsa.seasonal_decompose(y, model='additive')\npd.plotting.register_matplotlib_converters()\nresult.plot()\nplt.show()\n```", "```py\n# taking a 90/10 split between training and testing:\nfuture = int(len(y) * 0.9)\nprint('number of train samples: %d test samples %d' (future, len(y)-future)\n)\ntrain, test = y[:future], y[future:]\n\n```", "```py\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef get_arima_model(history, order=(5, 1, 0)):\n    return ARIMA(history, order=order)\n\ndef get_sarima_model(\n    history,\n    order=(5, 1, 1),\n    seasonal_order=(0, 1, 1, 4)\n    ):\n    return SARIMAX(\n        history,\n        order=order,\n        enforce_stationarity=True,\n        enforce_invertibility=False,\n        seasonal_order=seasonal_order\n    )\n\n```", "```py\nfrom sklearn.metrics import mean_squared_error\n\ndef apply_model(train, test, model_fun=get_arima_model):\n  '''we just roll with the model and apply it to successive\n  time steps\n  '''\n  history = list(train)\n  predictions = []\n  for t in test:\n    model = model_fun(history).fit(disp=0)\n    output = model.forecast()\n    predictions.append(output[0])\n    history.append(t)\n  error = mean_squared_error(test, predictions)\n  print('Test MSE: %.3f' % error)\n  #print(model.summary().tables[1])\n  return predictions, error\n\npredictions_arima, error_arima = apply_model(train, test)\npredictions_sarima, error_sarima = apply_model(\n    train, test, get_sarima_model\n)\n```", "```py\nfrom fbprophet import Prophet\n\ntrain_df = df_CO2_fb['1958':'1997']\ntest_df = df_CO2_fb['1998':'2001']\ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()Co2_model= Prophet(interval_width=0.95)\n\nCo2_model.fit(train_df)\ntrain_forecast = Co2_model.predict(train_df)\ntest_forecast = Co2_model.predict(test_df)\nfut = Co2_model.make_future_DataFrame(periods=12, freq='M')\nforecast_df = Co2_model.predict(fut)\nCo2_model.plot(forecast_df)\n```"]