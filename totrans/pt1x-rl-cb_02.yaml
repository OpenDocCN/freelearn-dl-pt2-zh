- en: Markov Decision Processes and Dynamic Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å’ŒåŠ¨æ€è§„åˆ’
- en: 'In this chapter, we will continue our practical reinforcement learning journey
    with PyTorch by looking at **Markov decision processes** (**MDPs**) and dynamic
    programming. This chapter will start with the creation of a Markov chain and an
    MDP, which is the core of most reinforcement learning algorithms. You will also
    become more familiar with Bellman equations by practicing policy evaluation. We
    will then move on and apply two approaches to solving an MDP: value iteration
    and policy iteration. We will use the FrozenLake environment as an example. At
    the end of the chapter, we will demonstrate how to solve the interesting coin-flipping
    gamble problem with dynamic programming step by step.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡è§‚å¯Ÿ**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**ï¼ˆMDPsï¼‰å’ŒåŠ¨æ€è§„åˆ’æ¥ç»§ç»­æˆ‘ä»¬çš„å®è·µå¼ºåŒ–å­¦ä¹ æ—…ç¨‹ã€‚æœ¬ç« å°†ä»åˆ›å»ºé©¬å°”å¯å¤«é“¾å’ŒMDPå¼€å§‹ï¼Œè¿™æ˜¯å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ¸å¿ƒã€‚æ‚¨è¿˜å°†é€šè¿‡å®è·µç­–ç•¥è¯„ä¼°æ›´åŠ ç†Ÿæ‚‰è´å°”æ›¼æ–¹ç¨‹ã€‚ç„¶åæˆ‘ä»¬å°†ç»§ç»­å¹¶åº”ç”¨ä¸¤ç§æ–¹æ³•è§£å†³MDPé—®é¢˜ï¼šå€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£ã€‚æˆ‘ä»¬å°†ä»¥FrozenLakeç¯å¢ƒä½œä¸ºç¤ºä¾‹ã€‚åœ¨æœ¬ç« çš„æœ€åï¼Œæˆ‘ä»¬å°†é€æ­¥å±•ç¤ºå¦‚ä½•ä½¿ç”¨åŠ¨æ€è§„åˆ’è§£å†³æœ‰è¶£çš„ç¡¬å¸æŠ›æ·èµŒåšé—®é¢˜ã€‚
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ç¤ºä¾‹ï¼š
- en: Creating a Markov chain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºé©¬å°”å¯å¤«é“¾
- en: Creating an MDP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª MDP
- en: Performing policy evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œç­–ç•¥è¯„ä¼°
- en: Simulating the FrozenLake environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿ FrozenLake ç¯å¢ƒ
- en: Solving an MDP with a value iteration algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP
- en: Solving an MDP with a policy iteration algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³ MDP
- en: Solving the coin-flipping gamble problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: 'You will need the following programs installed on your system to successfully
    execute the recipes in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æˆåŠŸæ‰§è¡Œæœ¬ç« ä¸­çš„ç¤ºä¾‹ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿä¸­å®‰è£…äº†ä»¥ä¸‹ç¨‹åºï¼š
- en: Python 3.6, 3.7, or above
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.6, 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬
- en: Anaconda
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: PyTorch 1.0 or above
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 1.0 æˆ–æ›´é«˜ç‰ˆæœ¬
- en: OpenAI Gym
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: Creating a Markov chain
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºé©¬å°”å¯å¤«é“¾
- en: Let's get started by creating a Markov chain, on which the MDP is developed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åˆ›å»ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾å¼€å§‹ï¼Œä»¥ä¾¿äºå¼€å‘ MDPã€‚
- en: 'A Markov chain describes a sequence of events that comply with the **Markov
    property**. It is defined by a set of possible states, *S = {s0, s1, ... , sm}*,
    and a transition matrix, *T(s, s'')*, consisting of the probabilities of state
    *s* transitioning to state s''. With the Markov property, the future state of
    the process, given the present state, is conditionally independent of past states.
    In other words, the state of the process at *t+1* is dependent only on the state
    at *t*. Here, we use a process of study and sleep as an example and create a Markov
    chain based on two states, *s0* (study) and *s1* (sleep). Let''s say we have the
    following transition matrix:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«é“¾æè¿°äº†éµå®ˆ**é©¬å°”å¯å¤«æ€§è´¨**çš„äº‹ä»¶åºåˆ—ã€‚å®ƒç”±ä¸€ç»„å¯èƒ½çš„çŠ¶æ€ *S = {s0, s1, ... , sm}* å’Œè½¬ç§»çŸ©é˜µ *T(s, s')*
    å®šä¹‰ï¼Œå…¶ä¸­åŒ…å«çŠ¶æ€ *s* è½¬ç§»åˆ°çŠ¶æ€ *s'* çš„æ¦‚ç‡ã€‚æ ¹æ®é©¬å°”å¯å¤«æ€§è´¨ï¼Œè¿‡ç¨‹çš„æœªæ¥çŠ¶æ€ï¼Œåœ¨ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œä¸è¿‡å»çŠ¶æ€æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚æ¢å¥è¯è¯´ï¼Œè¿‡ç¨‹åœ¨
    *t+1* æ—¶åˆ»çš„çŠ¶æ€ä»…ä¾èµ–äº *t* æ—¶åˆ»çš„çŠ¶æ€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥å­¦ä¹ å’Œç¡çœ è¿‡ç¨‹ä¸ºä¾‹ï¼ŒåŸºäºä¸¤ä¸ªçŠ¶æ€ *s0*ï¼ˆå­¦ä¹ ï¼‰å’Œ *s1*ï¼ˆç¡çœ ï¼‰ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹è½¬ç§»çŸ©é˜µï¼š
- en: '![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)'
- en: In the next section, we will compute the transition matrix after k steps, and
    the probabilities of being in each state given an initial distribution of states,
    such as *[0.7, 0.3]*, meaning there is a 70% chance that the process starts with
    study and a 30% chance that it starts with sleep.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—ç»è¿‡ k æ­¥åçš„è½¬ç§»çŸ©é˜µï¼Œä»¥åŠåœ¨åˆå§‹çŠ¶æ€åˆ†å¸ƒï¼ˆå¦‚ *[0.7, 0.3]*ï¼Œè¡¨ç¤ºæœ‰ 70% çš„æ¦‚ç‡ä»å­¦ä¹ å¼€å§‹ï¼Œ30% çš„æ¦‚ç‡ä»ç¡çœ å¼€å§‹ï¼‰ä¸‹å„ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åš...
- en: 'To create a Markov chain for the study - and - sleep process and conduct some
    analysis on it, perform the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸ºå­¦ä¹  - ç¡çœ è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå¹¶å¯¹å…¶è¿›è¡Œä¸€äº›åˆ†æï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: 'Import the library and define the transition matrix:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥åº“å¹¶å®šä¹‰è½¬ç§»çŸ©é˜µï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Calculate the transition probability after k steps. Here, we use k = `2`, `5`,
    `10`, `15`, and `20` as examples:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—ç»è¿‡ k æ­¥åçš„è½¬ç§»æ¦‚ç‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ k = `2`, `5`, `10`, `15`, å’Œ `20` ä¸ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the initial distribution of two states:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸¤ä¸ªçŠ¶æ€çš„åˆå§‹åˆ†å¸ƒï¼š
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Calculate the state distribution after k = `1`, `2`, `5`, `10`, `15`, and `20`
    steps:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨*æ­¥éª¤ 2*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»è¿‡ k = `1`, `2`, `5`, `10`, `15`, å’Œ `20` æ­¥åçš„è½¬ç§»æ¦‚ç‡ï¼Œç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...
- en: 'In *Step 2*, we calculated the transition probability after k steps, which
    is the k^(th) power of the transition matrix. You will see the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*æ­¥éª¤ 2*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»è¿‡ k æ­¥åçš„è½¬ç§»æ¦‚ç‡ï¼Œå³è½¬ç§»çŸ©é˜µçš„kæ¬¡å¹‚ã€‚ç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that, after 10 to 15 steps, the transition probability converges.
    This means that, no matter what state the process is in, it has the same probability
    of transitioning to s0 (57.14%) and s1 (42.86%).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡10åˆ°15æ­¥ï¼Œè¿‡æ¸¡æ¦‚ç‡ä¼šæ”¶æ•›ã€‚è¿™æ„å‘³ç€æ— è®ºè¿‡ç¨‹å¤„äºä»€ä¹ˆçŠ¶æ€ï¼Œè½¬ç§»åˆ°s0ï¼ˆ57.14%ï¼‰å’Œs1ï¼ˆ42.86%ï¼‰çš„æ¦‚ç‡éƒ½ç›¸åŒã€‚
- en: 'In *Step 4*, we calculated the state distribution after k = `1`, `2`, `5`,
    `10`, `15`, and `20` steps, which is the multiplication of the initial state distribution
    and the transition probability. You can see the results here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*æ­¥éª¤4*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†k = `1`ï¼Œ`2`ï¼Œ`5`ï¼Œ`10`ï¼Œ`15`å’Œ`20`æ­¥åçš„çŠ¶æ€åˆ†å¸ƒï¼Œè¿™æ˜¯åˆå§‹çŠ¶æ€åˆ†å¸ƒå’Œè¿‡æ¸¡æ¦‚ç‡çš„ä¹˜ç§¯ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ç»“æœï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that, after 10 steps, the state distribution converges. The probability
    of being in s0 (57.14%) and the probability of being in s1 (42.86%) remain unchanged
    in the long run.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡10æ­¥åï¼ŒçŠ¶æ€åˆ†å¸ƒä¼šæ”¶æ•›ã€‚é•¿æœŸå†…å¤„äºs0ï¼ˆ57.14%ï¼‰å’Œs1ï¼ˆ42.86%ï¼‰çš„æ¦‚ç‡ä¿æŒä¸å˜ã€‚
- en: 'Starting with [0.7, 0.3], the state distribution after one iteration becomes
    [0.52, 0.48]. Details of its calculation are illustrated in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä»[0.7, 0.3]å¼€å§‹ï¼Œç»è¿‡ä¸€æ¬¡è¿­ä»£åçš„çŠ¶æ€åˆ†å¸ƒå˜ä¸º[0.52, 0.48]ã€‚å…¶è¯¦ç»†è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)'
- en: 'After another iteration, the state distribution becomes [0.592, 0.408] as calculated
    in the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡å¦ä¸€æ¬¡è¿­ä»£ï¼ŒçŠ¶æ€åˆ†å¸ƒå¦‚ä¸‹[0.592, 0.408]ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºè®¡ç®—ï¼š
- en: '![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)'
- en: As time progresses, the state distribution reaches equilibrium.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼ŒçŠ¶æ€åˆ†å¸ƒè¾¾åˆ°å¹³è¡¡ã€‚
- en: There's more...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤š...
- en: In fact, irrespective of the initial state the process was in, the state distribution
    will always converge to [0.5714, 0.4286]. You could test with other initial distributions,
    such as [0.2, 0.8] and [1, 0]. The distribution will remain [0.5714, 0.4286] after
    10 steps.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œæ— è®ºåˆå§‹çŠ¶æ€å¦‚ä½•ï¼ŒçŠ¶æ€åˆ†å¸ƒéƒ½å°†å§‹ç»ˆæ”¶æ•›åˆ°[0.5714, 0.4286]ã€‚æ‚¨å¯ä»¥å°è¯•å…¶ä»–åˆå§‹åˆ†å¸ƒï¼Œä¾‹å¦‚[0.2, 0.8]å’Œ[1, 0]ã€‚åˆ†å¸ƒåœ¨ç»è¿‡10æ­¥åä»å°†ä¿æŒä¸º[0.5714,
    0.4286]ã€‚
- en: A Markov chain does not necessarily converge, especially when it contains transient
    or current states. But if it does converge, it will reach the same equilibrium
    regardless of the starting distribution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«é“¾ä¸ä¸€å®šä¼šæ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯å½“åŒ…å«ç¬æ€æˆ–å½“å‰çŠ¶æ€æ—¶ã€‚ä½†å¦‚æœå®ƒç¡®å®æ”¶æ•›ï¼Œæ— è®ºèµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œå®ƒå°†è¾¾åˆ°ç›¸åŒçš„å¹³è¡¡ã€‚
- en: See also
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦è§
- en: 'If you want to read more about Markov chains, the following are globally two
    great blog articles with nice visualizations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³é˜…è¯»æ›´å¤šå…³äºé©¬å°”å¯å¤«é“¾çš„å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ä¸¤ç¯‡å…·æœ‰è‰¯å¥½å¯è§†åŒ–æ•ˆæœçš„åšå®¢æ–‡ç« ï¼š
- en: '[https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)'
- en: '[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)'
- en: Creating an MDP
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºMDP
- en: Developed upon the Markov chain, an MDP involves an agent and a decision-making
    process. Let's go ahead with developing an MDP and calculating the value function
    under the optimal policy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé©¬å°”å¯å¤«é“¾çš„å‘å±•ï¼ŒMDPæ¶‰åŠä»£ç†å’Œå†³ç­–è¿‡ç¨‹ã€‚è®©æˆ‘ä»¬ç»§ç»­å‘å±•ä¸€ä¸ªMDPï¼Œå¹¶è®¡ç®—æœ€ä¼˜ç­–ç•¥ä¸‹çš„å€¼å‡½æ•°ã€‚
- en: Besides a set of possible states, *S = {s0, s1, ... , sm}*, an MDP is defined
    by a set of actions, *A = {a0, a1, ... , an}*; a transition model, *T(s, a, s')*;
    a reward function, *R(s)*; and a discount factor, ğ². The transition matrix, *T(s,
    a, s')*, contains the probabilities of taking action a from state s then landing
    in s'. The discount factor, ğ², controls the tradeoff between future rewards and
    immediate ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸€ç»„å¯èƒ½çš„çŠ¶æ€ï¼Œ*S = {s0, s1, ... , sm}*ï¼ŒMDPç”±ä¸€ç»„åŠ¨ä½œï¼Œ*A = {a0, a1, ... , an}*ï¼›è¿‡æ¸¡æ¨¡å‹ï¼Œ*T(s,
    a, s')*ï¼›å¥–åŠ±å‡½æ•°ï¼Œ*R(s)*ï¼›å’ŒæŠ˜ç°å› å­ğ²å®šä¹‰ã€‚è¿‡æ¸¡çŸ©é˜µï¼Œ*T(s, a, s')*ï¼ŒåŒ…å«ä»çŠ¶æ€sé‡‡å–åŠ¨ä½œaç„¶åè½¬ç§»åˆ°s'çš„æ¦‚ç‡ã€‚æŠ˜ç°å› å­ğ²æ§åˆ¶æœªæ¥å¥–åŠ±å’Œå³æ—¶å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ã€‚
- en: 'To make our MDP slightly more complicated, we extend the study and sleep process
    with one more state, `s2 play` games. Let''s say we have two actions, `a0 work`
    and `a1 slack`. The *3 * 2 * 3* transition matrix *T(s, a, s'')* is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æˆ‘ä»¬çš„MDPç¨å¾®å¤æ‚åŒ–ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å’Œç¡çœ è¿‡ç¨‹å»¶ä¼¸åˆ°å¦ä¸€ä¸ªçŠ¶æ€ï¼Œ`s2 play` æ¸¸æˆã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªåŠ¨ä½œï¼Œ`a0 work` å’Œ `a1 slack`ã€‚*3
    * 2 * 3* è¿‡æ¸¡çŸ©é˜µ *T(s, a, s')* å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)'
- en: This means, for example, that when taking the a1 slack action from state s0
    study, there is a 60% chance that it will become s1 sleep (maybe getting tired
    ) and a 30% chance that it will become s2 play games (maybe wanting to relax ),
    and that there is a 10% chance of keeping on studying (maybe a true workaholic
    ). We define the reward function as [+1, 0, -1] for three states, to compensate
    for the hard work. Obviously, the **optimal policy**, in this case, is choosing
    a0 work for each step (keep on studying â€“ no pain no gain, right?). Also, we choose
    0.5 as the discount factor, to begin with. In the next section, we will compute
    the **state-value function** (also called the **value function**, just the **value**
    for short, or **expected utility**) under the optimal policy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œå½“ä»çŠ¶æ€s0 studyä¸­é‡‡å–a1 slackè¡ŒåŠ¨æ—¶ï¼Œæœ‰60%çš„æœºä¼šå®ƒå°†å˜æˆs1 sleepï¼ˆå¯èƒ½ä¼šç´¯ï¼‰ï¼Œæœ‰30%çš„æœºä¼šå®ƒå°†å˜æˆs2 play
    gamesï¼ˆå¯èƒ½æƒ³æ”¾æ¾ï¼‰ï¼Œè¿˜æœ‰10%çš„æœºä¼šç»§ç»­å­¦ä¹ ï¼ˆå¯èƒ½æ˜¯çœŸæ­£çš„å·¥ä½œç‹‚ï¼‰ã€‚æˆ‘ä»¬ä¸ºä¸‰ä¸ªçŠ¶æ€å®šä¹‰å¥–åŠ±å‡½æ•°ä¸º[+1, 0, -1]ï¼Œä»¥è¡¥å¿è¾›å‹¤å·¥ä½œã€‚æ˜¾ç„¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**æœ€ä¼˜ç­–ç•¥**æ˜¯åœ¨æ¯ä¸ªæ­¥éª¤é€‰æ‹©a0å·¥ä½œï¼ˆç»§ç»­å­¦ä¹ â€”â€”ä¸åŠªåŠ›å°±æ²¡æœ‰æ”¶è·ï¼Œå¯¹å§ï¼Ÿï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€‰æ‹©0.5ä½œä¸ºèµ·å§‹æŠ˜æ‰£å› å­ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—**çŠ¶æ€å€¼å‡½æ•°**ï¼ˆä¹Ÿç§°ä¸º**å€¼å‡½æ•°**ï¼Œç®€ç§°**å€¼**æˆ–**æœŸæœ›æ•ˆç”¨**ï¼‰åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹çš„å€¼ã€‚
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åš...
- en: 'Creating an MDP can be done via the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºMDPå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®Œæˆï¼š
- en: 'Import PyTorch and define the transition matrix:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥PyTorchå¹¶å®šä¹‰è½¬ç§»çŸ©é˜µï¼š
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the reward function and the discount factor:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜æ‰£å› å­ï¼š
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The optimal policy in this case is selecting action `a0` in all circumstances:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€ä¼˜ç­–ç•¥æ˜¯åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ`a0`ï¼š
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We calculate the value, `V`, of the optimal policy using the **matrix inversion**
    method in the following function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µæ±‚é€†**æ–¹æ³•è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼`V`ï¼š
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will demonstrate how to derive the value in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­å±•ç¤ºå¦‚ä½•æ¨å¯¼ä¸‹ä¸€ä¸ªéƒ¨åˆ†çš„å€¼ã€‚
- en: 'We feed all variables we have to the function, including the transition probabilities
    associated with action `a0`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ‰€æœ‰å˜é‡è¾“å…¥å‡½æ•°ä¸­ï¼ŒåŒ…æ‹¬ä¸åŠ¨ä½œ`a0`ç›¸å…³çš„è½¬ç§»æ¦‚ç‡ï¼š
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...
- en: In this oversimplified study-sleep-game process, the optimal policy, that is,
    the policy that achieves the highest total reward, is choosing action a0 in all
    steps. However, it won't be that straightforward in most cases. Also, the actions
    taken in individual steps won't necessarily be the same. They are usually dependent
    on states. So, we will have to solve an MDP by finding the optimal policy in real-world
    cases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡äºç®€åŒ–çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œæœ€ä¼˜ç­–ç•¥ï¼Œå³è·å¾—æœ€é«˜æ€»å¥–åŠ±çš„ç­–ç•¥ï¼Œæ˜¯åœ¨æ‰€æœ‰æ­¥éª¤ä¸­é€‰æ‹©åŠ¨ä½œa0ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæƒ…å†µä¸ä¼šé‚£ä¹ˆç®€å•ã€‚æ­¤å¤–ï¼Œä¸ªåˆ«æ­¥éª¤ä¸­é‡‡å–çš„è¡ŒåŠ¨ä¸ä¸€å®šç›¸åŒã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºçŠ¶æ€ã€‚å› æ­¤ï¼Œåœ¨å®é™…æƒ…å†µä¸­ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸é€šè¿‡æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥æ¥è§£å†³ä¸€ä¸ªMDPé—®é¢˜ã€‚
- en: The value function of a policy measures how good it is for an agent to be in
    each state, given the policy being followed. The greater the value, the better
    the state.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥çš„å€¼å‡½æ•°è¡¡é‡äº†åœ¨éµå¾ªç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¯¹äºä¸€ä¸ªagentè€Œè¨€å¤„äºæ¯ä¸ªçŠ¶æ€çš„å¥½å¤„ã€‚å€¼è¶Šå¤§ï¼ŒçŠ¶æ€è¶Šå¥½ã€‚
- en: 'In *Step 4*, we calculated the value, `V`, of the optimal policy using **matrix
    inversion**. According to the **Bellman Equation**, the relationship between the
    value at step *t+1* and that at step *t* can be expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬4æ­¥*ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µæ±‚é€†æ³•**è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼`V`ã€‚æ ¹æ®**è´å°”æ›¼æ–¹ç¨‹**ï¼Œæ­¥éª¤*t+1*çš„å€¼ä¸æ­¥éª¤*t*çš„å€¼ä¹‹é—´çš„å…³ç³»å¯ä»¥è¡¨è¾¾å¦‚ä¸‹ï¼š
- en: '![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)'
- en: 'When the value converges, which means *Vt+1 = Vt*, we can derive the value,
    `V`, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å€¼æ”¶æ•›æ—¶ï¼Œä¹Ÿå°±æ˜¯*Vt+1 = Vt*æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºå€¼`V`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)'
- en: Here, *I* is the identity matrix with 1s on the main diagonal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*I*æ˜¯å…·æœ‰ä¸»å¯¹è§’çº¿ä¸Šçš„1çš„å•ä½çŸ©é˜µã€‚
- en: One advantage of solving an MDP with matrix inversion is that you always get
    an exact answer. But the downside is its scalability. As we need to compute the
    inversion of an m * m matrix (where *m* is the number of possible states), the
    computation will become costly if there is a large number of states.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çŸ©é˜µæ±‚é€†è§£å†³MDPçš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ä½ æ€»æ˜¯å¾—åˆ°ä¸€ä¸ªç¡®åˆ‡çš„ç­”æ¡ˆã€‚ä½†å…¶å¯æ‰©å±•æ€§æœ‰é™ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦è®¡ç®—ä¸€ä¸ªm * mçŸ©é˜µçš„æ±‚é€†ï¼ˆå…¶ä¸­*m*æ˜¯å¯èƒ½çš„çŠ¶æ€æ•°é‡ï¼‰ï¼Œå¦‚æœæœ‰å¤§é‡çŠ¶æ€ï¼Œè®¡ç®—æˆæœ¬ä¼šå˜å¾—å¾ˆé«˜æ˜‚ã€‚
- en: There's more...
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤š...
- en: 'We decide to experiment with different values for the discount factor. Let''s
    start with 0, which means we only care about the immediate reward:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†³å®šå°è¯•ä¸åŒçš„æŠ˜æ‰£å› å­å€¼ã€‚è®©æˆ‘ä»¬ä»0å¼€å§‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªå…³å¿ƒå³æ—¶å¥–åŠ±ï¼š
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is consistent with the reward function as we only look at the reward received
    in the next move.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å¥–åŠ±å‡½æ•°ä¸€è‡´ï¼Œå› ä¸ºæˆ‘ä»¬åªçœ‹ä¸‹ä¸€æ­¥çš„å¥–åŠ±ã€‚
- en: 'As the discount factor increases toward 1, future rewards are considered. Let''s
    take a look at ğ²=0.99:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æŠ˜ç°å› å­å‘ 1 é æ‹¢ï¼Œæœªæ¥çš„å¥–åŠ±è¢«è€ƒè™‘ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ ğ²=0.99ï¼š
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: See also
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…
- en: This cheatsheet, [https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html),
    serves as a quick reference for MDPs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé€ŸæŸ¥è¡¨ï¼Œ[https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html)ï¼Œä½œä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å¿«é€Ÿå‚è€ƒã€‚
- en: Performing policy evaluation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰§è¡Œç­–ç•¥è¯„ä¼°
- en: We have just developed an MDP and computed the value function of the optimal
    policy using matrix inversion. We also mentioned the limitation of inverting an
    m * m matrix with a large m value (let's say 1,000, 10,000, or 100,000). In this
    recipe, we will talk about a simpler approach called **policy evaluation**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšå¼€å‘äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨çŸ©é˜µæ±‚é€†è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼å‡½æ•°ã€‚æˆ‘ä»¬è¿˜æåˆ°äº†é€šè¿‡æ±‚é€†å¤§å‹ m * m çŸ©é˜µï¼ˆä¾‹å¦‚ 1,000ã€10,000 æˆ–
    100,000ï¼‰çš„é™åˆ¶ã€‚åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼Œç§°ä¸º**ç­–ç•¥è¯„ä¼°**ã€‚
- en: 'Policy evaluation is an iterative algorithm. It starts with arbitrary policy
    values and then iteratively updates the values based on the **Bellman expectation
    equation** until they converge. In each iteration, the value of a policy, *Ï€*,
    for a state, *s*, is updated as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥è¯„ä¼°æ˜¯ä¸€ä¸ªè¿­ä»£ç®—æ³•ã€‚å®ƒä»ä»»æ„çš„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åæ ¹æ®**è´å°”æ›¼æœŸæœ›æ–¹ç¨‹**è¿­ä»£æ›´æ–°å€¼ï¼Œç›´åˆ°æ”¶æ•›ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒçŠ¶æ€ *s* ä¸‹ç­–ç•¥ *Ï€* çš„å€¼æ›´æ–°å¦‚ä¸‹ï¼š
- en: '![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)'
- en: Here, *Ï€(s, a)* denotes the probability of taking action *a* in state *s* under
    policy *Ï€*. *T(s, a, s')* is the transition probability from state *s* to state
    *s'* by taking action *a*, and *R(s, a)* is the reward received in state *s* by
    taking action *a*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*Ï€(s, a)* è¡¨ç¤ºåœ¨ç­–ç•¥ *Ï€* ä¸‹åœ¨çŠ¶æ€ *s* ä¸­é‡‡å–åŠ¨ä½œ *a* çš„æ¦‚ç‡ã€‚*T(s, a, s')* æ˜¯é€šè¿‡é‡‡å–åŠ¨ä½œ *a* ä»çŠ¶æ€
    *s* è½¬ç§»åˆ°çŠ¶æ€ *s'* çš„è½¬ç§»æ¦‚ç‡ï¼Œ*R(s, a)* æ˜¯åœ¨çŠ¶æ€ *s* ä¸­é‡‡å–åŠ¨ä½œ *a* åè·å¾—çš„å¥–åŠ±ã€‚
- en: There are two ways to terminate an iterative updating process. One is by setting
    a fixed number of iterations, such as 1,000 and 10,000, which might be difficult
    to control sometimes. Another one involves specifying a threshold (usually 0.0001,
    0.00001, or something similar) and terminating the process only if the values
    of all states change to an extent that is lower than the threshold specified.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§æ–¹æ³•æ¥ç»ˆæ­¢è¿­ä»£æ›´æ–°è¿‡ç¨‹ã€‚ä¸€ç§æ˜¯è®¾ç½®ä¸€ä¸ªå›ºå®šçš„è¿­ä»£æ¬¡æ•°ï¼Œæ¯”å¦‚ 1,000 å’Œ 10,000ï¼Œæœ‰æ—¶å¯èƒ½éš¾ä»¥æ§åˆ¶ã€‚å¦ä¸€ç§æ˜¯æŒ‡å®šä¸€ä¸ªé˜ˆå€¼ï¼ˆé€šå¸¸æ˜¯ 0.0001ã€0.00001
    æˆ–ç±»ä¼¼çš„å€¼ï¼‰ï¼Œä»…åœ¨æ‰€æœ‰çŠ¶æ€çš„å€¼å˜åŒ–ç¨‹åº¦ä½äºæŒ‡å®šçš„é˜ˆå€¼æ—¶ç»ˆæ­¢è¿‡ç¨‹ã€‚
- en: In the next section, we will perform policy evaluation on the study-sleep-game
    process under the optimal policy and a random policy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®æœ€ä¼˜ç­–ç•¥å’Œéšæœºç­–ç•¥å¯¹å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹æ‰§è¡Œç­–ç•¥è¯„ä¼°ã€‚
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ“ä½œ...
- en: 'Let''s develop a policy evaluation algorithm and apply it to our study-sleep-game
    process as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¯„ä¼°ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: 'Import PyTorch and define the transition matrix:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥ PyTorch å¹¶å®šä¹‰è¿‡æ¸¡çŸ©é˜µï¼š
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the reward function and the discount factor (let''s use `0.5` for now):'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜ç°å› å­ï¼ˆç°åœ¨ä½¿ç”¨ `0.5`ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the threshold used to determine when to stop the evaluation process:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰ç”¨äºç¡®å®šä½•æ—¶åœæ­¢è¯„ä¼°è¿‡ç¨‹çš„é˜ˆå€¼ï¼š
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the optimal policy where action a0 is chosen under all circumstances:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰æœ€ä¼˜ç­–ç•¥ï¼Œå…¶ä¸­åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ a0ï¼š
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Develop a policy evaluation function that takes in a policy, transition matrix,
    rewards, discount factor, and a threshold and computes the `value` function:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼€å‘ä¸€ä¸ªç­–ç•¥è¯„ä¼°å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªç­–ç•¥ã€è¿‡æ¸¡çŸ©é˜µã€å¥–åŠ±ã€æŠ˜ç°å› å­å’Œé˜ˆå€¼ï¼Œå¹¶è®¡ç®— `value` å‡½æ•°ï¼š
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s plug in the optimal policy and all other variables:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ’å…¥æœ€ä¼˜ç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡ï¼š
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is almost the same as what we got using matrix inversion.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬ä½¿ç”¨çŸ©é˜µæ±‚é€†å¾—åˆ°çš„ç»“æœå‡ ä¹ç›¸åŒã€‚
- en: 'We now experiment with another policy, a random policy where actions are picked
    with the same probabilities:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°è¯•å¦ä¸€ä¸ªç­–ç•¥ï¼Œä¸€ä¸ªéšæœºç­–ç•¥ï¼Œå…¶ä¸­åŠ¨ä½œä»¥ç›¸åŒçš„æ¦‚ç‡é€‰æ‹©ï¼š
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Plug in the random policy and all other variables:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ’å…¥éšæœºç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡ï¼š
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥ä½œåŸç†...
- en: We have just seen how effective it is to compute the value of a policy using
    policy evaluation. It is a simple convergent iterative approach, in the **dynamic
    programming family**, or to be more specific, **approximate dynamic programming**.
    It starts with random guesses as to the values and then iteratively updates them
    according to the Bellman expectation equation until they converge.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšçœ‹åˆ°äº†ä½¿ç”¨ç­–ç•¥è¯„ä¼°è®¡ç®—ç­–ç•¥å€¼çš„æ•ˆæœæœ‰å¤šä¹ˆæœ‰æ•ˆã€‚è¿™æ˜¯ä¸€ç§ç®€å•çš„æ”¶æ•›è¿­ä»£æ–¹æ³•ï¼Œåœ¨**åŠ¨æ€è§„åˆ’å®¶æ—**ä¸­ï¼Œæˆ–è€…æ›´å…·ä½“åœ°è¯´æ˜¯**è¿‘ä¼¼åŠ¨æ€è§„åˆ’**ã€‚å®ƒä»å¯¹å€¼çš„éšæœºçŒœæµ‹å¼€å§‹ï¼Œç„¶åæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è¿­ä»£æ›´æ–°ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚
- en: 'In Step 5, the policy evaluation function does the following tasks:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ 5æ­¥ä¸­ï¼Œç­–ç•¥è¯„ä¼°å‡½æ•°æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
- en: Initializes the policy values as all zeros.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨é›¶ã€‚
- en: Updates the values based on the Bellman expectation equation.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ›´æ–°å€¼ã€‚
- en: Computes the maximal change of the values across all states.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ‰€æœ‰çŠ¶æ€ä¸­å€¼çš„æœ€å¤§å˜åŒ–ã€‚
- en: If the maximal change is greater than the threshold, it keeps updating the values.
    Otherwise, it terminates the evaluation process and returns the latest values.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœ€å¤§å˜åŒ–å¤§äºé˜ˆå€¼ï¼Œåˆ™ç»§ç»­æ›´æ–°å€¼ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹å¹¶è¿”å›æœ€æ–°çš„å€¼ã€‚
- en: Since policy evaluation uses iterative approximation, its result might not be
    exactly the same as the result of the matrix inversion method, which uses exact
    computation. In fact, we don't really need the value function to be that precise.
    Also, it can solve the **curses** **of dimensionality** problem, which can result
    in scaling up the computation to thousands of millions of states. Therefore, we
    usually prefer policy evaluation over the other.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç­–ç•¥è¯„ä¼°ä½¿ç”¨è¿­ä»£é€¼è¿‘ï¼Œå…¶ç»“æœå¯èƒ½ä¸ä½¿ç”¨ç²¾ç¡®è®¡ç®—çš„çŸ©é˜µæ±‚é€†æ–¹æ³•çš„ç»“æœä¸å®Œå…¨ç›¸åŒã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¹¶ä¸çœŸçš„éœ€è¦ä»·å€¼å‡½æ•°é‚£ä¹ˆç²¾ç¡®ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥è§£å†³**ç»´åº¦è¯…å’’**é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è®¡ç®—æ‰©å±•åˆ°æ•°ä»¥åƒè®¡çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢ç­–ç•¥è¯„ä¼°è€Œä¸æ˜¯å…¶ä»–æ–¹æ³•ã€‚
- en: One more thing to remember is that policy evaluation is used to **predict**
    how great a we will get from a given policy; it is not used for **control** problems.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä»¶äº‹è¦è®°ä½ï¼Œç­–ç•¥è¯„ä¼°ç”¨äº**é¢„æµ‹**ç»™å®šç­–ç•¥çš„é¢„æœŸå›æŠ¥æœ‰å¤šå¤§ï¼›å®ƒä¸ç”¨äº**æ§åˆ¶**é—®é¢˜ã€‚
- en: There's more...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šå†…å®¹...
- en: To take a closer look, we also plot the policy values over the whole evaluation
    process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ä»”ç»†åœ°è§‚å¯Ÿï¼Œæˆ‘ä»¬è¿˜ä¼šç»˜åˆ¶æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹ä¸­çš„ç­–ç•¥å€¼ã€‚
- en: 'We first need to record the value for each iteration in the `policy_evaluation`
    function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ `policy_evaluation` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®°å½•æ¯æ¬¡è¿­ä»£çš„å€¼ï¼š
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we feed the `policy_evaluation_history` function with the optimal policy,
    a discount factor of `0.5`, and other variables:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°† `policy_evaluation_history` å‡½æ•°åº”ç”¨äºæœ€ä¼˜ç­–ç•¥ï¼ŒæŠ˜ç°å› å­ä¸º `0.5`ï¼Œä»¥åŠå…¶ä»–å˜é‡ï¼š
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then plot the resulting history of values using the following lines of code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç ç»˜åˆ¶äº†å€¼çš„å†å²ç»“æœï¼š
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We see the following result:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°äº†ä»¥ä¸‹ç»“æœï¼š
- en: '![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)'
- en: It is interesting to see the stabilization between iterations 10 to 14 during
    the convergence.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ”¶æ•›æœŸé—´ï¼Œä»ç¬¬ 10 åˆ°ç¬¬ 14æ¬¡è¿­ä»£ä¹‹é—´çš„ç¨³å®šæ€§æ˜¯éå¸¸æœ‰è¶£çš„ã€‚
- en: 'Next, we run the same code but with two different discount factors, 0.2 and
    0.99\. We get the following plot with the discount factor at 0.2:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æŠ˜ç°å› å­ï¼Œ0.2 å’Œ 0.99ï¼Œè¿è¡Œç›¸åŒçš„ä»£ç ã€‚æˆ‘ä»¬å¾—åˆ°äº†æŠ˜ç°å› å­ä¸º 0.2 æ—¶çš„ä»¥ä¸‹ç»˜å›¾ï¼š
- en: '![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)'
- en: Comparing the plot with a discount factor of 0.5 with this one, we can see that
    the smaller the factor, the faster the policy values converge.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æŠ˜ç°å› å­ä¸º 0.5 çš„ç»˜å›¾ä¸è¿™ä¸ªè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å› å­è¶Šå°ï¼Œç­–ç•¥å€¼æ”¶æ•›å¾—è¶Šå¿«ã€‚
- en: 'We also get the following plot with a discount factor of 0.99:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¾—åˆ°äº†æŠ˜ç°å› å­ä¸º 0.99 æ—¶çš„ä»¥ä¸‹ç»˜å›¾ï¼š
- en: '![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)'
- en: By comparing the plot with a discount factor of 0.5 to the plot with a discount
    factor of 0.99, we can see that the larger the factor, the longer it takes for
    policy values to converge. The discount factor is a tradeoff between rewards now
    and rewards in the future.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æŠ˜ç°å› å­ä¸º 0.5 çš„ç»˜å›¾ä¸æŠ˜ç°å› å­ä¸º 0.99 çš„ç»˜å›¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å› å­è¶Šå¤§ï¼Œç­–ç•¥å€¼æ”¶æ•›æ‰€éœ€çš„æ—¶é—´è¶Šé•¿ã€‚æŠ˜ç°å› å­æ˜¯å³æ—¶å¥–åŠ±ä¸æœªæ¥å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ã€‚
- en: Simulating the FrozenLake environment
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿ FrozenLake ç¯å¢ƒ
- en: The optimal policies for the MDPs we have dealt with so far are pretty intuitive.
    However, it won't be that straightforward in most cases, such as the FrozenLake
    environment. In this recipe, let's play around with the FrozenLake environment
    and get ready for upcoming recipes where we will find its optimal policy.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¤„ç†è¿‡çš„MDPçš„æœ€ä¼˜ç­–ç•¥éƒ½ç›¸å½“ç›´è§‚ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¦‚FrozenLakeç¯å¢ƒï¼Œæƒ…å†µå¹¶ä¸é‚£ä¹ˆç®€å•ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬ç©ä¸€ä¸‹FrozenLakeç¯å¢ƒï¼Œå¹¶å‡†å¤‡å¥½æ¥ä¸‹æ¥çš„æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†æ‰¾åˆ°å®ƒçš„æœ€ä¼˜ç­–ç•¥ã€‚
- en: FrozenLake is a typical Gym environment with a **discrete** state space. It
    is about moving an agent from the starting location to the goal location in a
    grid world, and at the same time avoiding traps. The grid is either four by four
    ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/))
    or eight by eigh.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLakeæ˜¯ä¸€ä¸ªå…¸å‹çš„Gymç¯å¢ƒï¼Œå…·æœ‰**ç¦»æ•£**çŠ¶æ€ç©ºé—´ã€‚å®ƒæ˜¯å…³äºåœ¨ç½‘æ ¼ä¸–ç•Œä¸­å°†ä»£ç†ç¨‹åºä»èµ·å§‹ä½ç½®ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼Œå¹¶åŒæ—¶é¿å¼€é™·é˜±ã€‚ç½‘æ ¼å¯ä»¥æ˜¯å››ä¹˜å››
    ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/))
    æˆ–è€…å…«ä¹˜å…«ã€‚
- en: 't ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
    The grid is made up of the following four types of tiles:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: t ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚ç½‘æ ¼ç”±ä»¥ä¸‹å››ç§ç±»å‹çš„æ–¹å—ç»„æˆï¼š
- en: '**S**: The starting location'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S**ï¼šä»£è¡¨èµ·å§‹ä½ç½®'
- en: '**G**: The goal location, which terminates an episode'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G**ï¼šä»£è¡¨ç›®æ ‡ä½ç½®ï¼Œè¿™ä¼šç»ˆæ­¢ä¸€ä¸ªå›åˆ'
- en: '**F**: The frozen tile, which is a walkable location'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**ï¼šä»£è¡¨å†°é¢æ–¹å—ï¼Œå¯ä»¥è¡Œèµ°çš„ä½ç½®'
- en: '**H**: The hole location, which terminates an episode'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H**ï¼šä»£è¡¨ä¸€ä¸ªåœ°æ´ä½ç½®ï¼Œè¿™ä¼šç»ˆæ­¢ä¸€ä¸ªå›åˆ'
- en: 'There are four actions, obviously: moving left (0), moving down (1), moving
    right (2), and moving up (3). The reward is +1 if the agent successfully reaches
    the goal location, and 0 otherwise. Also, the observation space is represented
    in a 16-dimensional integer array, and there are 4 possible actions (which makes
    sense).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶æœ‰å››ç§åŠ¨ä½œï¼šå‘å·¦ç§»åŠ¨ï¼ˆ0ï¼‰ã€å‘ä¸‹ç§»åŠ¨ï¼ˆ1ï¼‰ã€å‘å³ç§»åŠ¨ï¼ˆ2ï¼‰å’Œå‘ä¸Šç§»åŠ¨ï¼ˆ3ï¼‰ã€‚å¦‚æœä»£ç†ç¨‹åºæˆåŠŸåˆ°è¾¾ç›®æ ‡ä½ç½®ï¼Œå¥–åŠ±ä¸º+1ï¼Œå¦åˆ™ä¸º0ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿç©ºé—´ç”±ä¸€ä¸ª16ç»´æ•´æ•°æ•°ç»„è¡¨ç¤ºï¼Œæœ‰4ç§å¯èƒ½çš„åŠ¨ä½œï¼ˆè¿™æ˜¯æœ‰é“ç†çš„ï¼‰ã€‚
- en: What is tricky in this environment is that, as the ice surface is slippery,
    the agent won't always move in the direction it intends. For example, it may move
    to the left or to the right when it intends to move down.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¯å¢ƒçš„æ£˜æ‰‹ä¹‹å¤„åœ¨äºå†°é¢å¾ˆæ»‘ï¼Œä»£ç†ç¨‹åºå¹¶ä¸æ€»æ˜¯æŒ‰å…¶æ„å›¾ç§»åŠ¨ã€‚ä¾‹å¦‚ï¼Œå½“å®ƒæ‰“ç®—å‘ä¸‹ç§»åŠ¨æ—¶ï¼Œå¯èƒ½ä¼šå‘å·¦æˆ–å‘å³ç§»åŠ¨ã€‚
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡†å¤‡å·¥ä½œ
- en: 'To run the FrozenLake environment, let''s first search for it in the table
    of environments here: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    The search gives us `FrozenLake-v0`.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡ŒFrozenLakeç¯å¢ƒï¼Œè®©æˆ‘ä»¬é¦–å…ˆåœ¨è¿™é‡Œçš„ç¯å¢ƒè¡¨ä¸­æœç´¢å®ƒï¼š[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)ã€‚æœç´¢ç»“æœç»™å‡ºäº†`FrozenLake-v0`ã€‚
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€ä¹ˆåšâ€¦â€¦
- en: 'Let''s simulate the four-by-four FrozenLake environment in the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŒ‰ä»¥ä¸‹æ­¥éª¤æ¨¡æ‹Ÿå››ä¹˜å››çš„FrozenLakeç¯å¢ƒï¼š
- en: 'We import the `gym` library and create an instance of the FrozenLake environment:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¼å…¥`gym`åº“ï¼Œå¹¶åˆ›å»ºFrozenLakeç¯å¢ƒçš„ä¸€ä¸ªå®ä¾‹ï¼š
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Reset the environment:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡ç½®ç¯å¢ƒï¼š
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The agent starts with state `0`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†ç¨‹åºä»çŠ¶æ€`0`å¼€å§‹ã€‚
- en: 'Render the environment:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¸²æŸ“ç¯å¢ƒï¼š
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s make a down movement since it is walkable:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åšä¸€ä¸ªå‘ä¸‹çš„åŠ¨ä½œï¼Œå› ä¸ºè¿™æ˜¯å¯è¡Œèµ°çš„ï¼š
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Print out all the returning information to confirm that the agent lands in
    state `4` with a probability of 33.33%:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰“å°å‡ºæ‰€æœ‰è¿”å›çš„ä¿¡æ¯ï¼Œç¡®è®¤ä»£ç†ç¨‹åºä»¥33.33%çš„æ¦‚ç‡è½åœ¨çŠ¶æ€`4`ï¼š
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You get `0` as a reward, since it has not reached the goal and the episode is
    not done yet. Again, you might see the agent landing in state 1, or staying in
    state 0 because of the slippery surface.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¾—åˆ°äº†`0`ä½œä¸ºå¥–åŠ±ï¼Œå› ä¸ºå®ƒå°šæœªåˆ°è¾¾ç›®æ ‡ï¼Œå¹¶ä¸”å›åˆå°šæœªç»“æŸã€‚å†æ¬¡çœ‹åˆ°ä»£ç†ç¨‹åºå¯èƒ½ä¼šé™·å…¥çŠ¶æ€1ï¼Œæˆ–è€…å› ä¸ºè¡¨é¢å¤ªæ»‘è€Œåœç•™åœ¨çŠ¶æ€0ã€‚
- en: 'To demonstrate how difficult it is to walk on the frozen lake, implement a
    random policy and calculate the average total reward over 1,000 episodes. First,
    define a function that simulates a FrozenLake episode given a policy and returns
    the total reward (we know it is either 0 or 1):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºåœ¨å†°é¢ä¸Šè¡Œèµ°æœ‰å¤šå›°éš¾ï¼Œå®ç°ä¸€ä¸ªéšæœºç­–ç•¥å¹¶è®¡ç®—1,000ä¸ªå›åˆçš„å¹³å‡æ€»å¥–åŠ±ã€‚é¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®ç»™å®šçš„ç­–ç•¥æ¨¡æ‹Ÿä¸€ä¸ªFrozenLakeå›åˆå¹¶è¿”å›æ€»å¥–åŠ±ï¼ˆæˆ‘ä»¬çŸ¥é“è¿™è¦ä¹ˆæ˜¯0ï¼Œè¦ä¹ˆæ˜¯1ï¼‰ï¼š
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now run `1000` episodes, and a policy will be randomly generated and will be
    used in each episode:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿è¡Œ`1000`ä¸ªå›åˆï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå›åˆä¸­éƒ½ä¼šéšæœºç”Ÿæˆå¹¶ä½¿ç”¨ä¸€ä¸ªç­–ç•¥ï¼š
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This basically means there is only a 1.4% chance on average that the agent can
    reach the goal if we randomize the actions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬éšæœºæ‰§è¡ŒåŠ¨ä½œï¼Œå¹³å‡åªæœ‰1.4%çš„æœºä¼šä»£ç†ç¨‹åºèƒ½å¤Ÿåˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚
- en: 'Next, we experiment with a random search policy. In the training phase, we
    randomly generate a bunch of policies and record the first one that reaches the
    goal:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éšæœºæœç´¢ç­–ç•¥è¿›è¡Œå®éªŒã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆä¸€å †ç­–ç•¥ï¼Œå¹¶è®°å½•ç¬¬ä¸€ä¸ªè¾¾åˆ°ç›®æ ‡çš„ç­–ç•¥ï¼š
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Take a look at the best policy:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æœ€ä½³ç­–ç•¥ï¼š
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now run 1,000 episodes with the policy we just cherry-picked:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿è¡Œ 1,000 ä¸ªå›åˆï¼Œä½¿ç”¨æˆ‘ä»¬åˆšæŒ‘é€‰å‡ºçš„ç­–ç•¥ï¼š
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Using the random search algorithm, the goal will be reached 20.8% of the time
    on average.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºæœç´¢ç®—æ³•ï¼Œå¹³å‡æƒ…å†µä¸‹ä¼šæœ‰ 20.8% çš„æ¦‚ç‡è¾¾åˆ°ç›®æ ‡ã€‚
- en: Note that this result could vary a lot, as the policy we picked might happen
    to reach the goal because of the slippery ice and might not be the optimal one.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬é€‰æ‹©çš„ç­–ç•¥å¯èƒ½ç”±äºå†°é¢æ»‘åŠ¨è€Œè¾¾åˆ°ç›®æ ‡ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç»“æœå˜åŒ–å¾ˆå¤§ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥ä½œåŸç†â€¦â€¦
- en: In this recipe, we randomly generated a policy that was composed of 16 actions
    for the 16 states. Keep in mind that in FrozenLake, the movement direction is
    only partially dependent on the chosen action. This increases the uncertainty
    of control.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆäº†ä¸€ä¸ªç”± 16 ä¸ªåŠ¨ä½œç»„æˆçš„ç­–ç•¥ï¼Œå¯¹åº” 16 ä¸ªçŠ¶æ€ã€‚è¯·è®°ä½ï¼Œåœ¨ FrozenLake ä¸­ï¼Œç§»åŠ¨æ–¹å‘ä»…éƒ¨åˆ†ä¾èµ–äºé€‰æ‹©çš„åŠ¨ä½œï¼Œè¿™å¢åŠ äº†æ§åˆ¶çš„ä¸ç¡®å®šæ€§ã€‚
- en: 'After running the code in *Step 4*, you will see a 4 * 4 matrix as follows,
    representing the frozen lake and the tile (state 0) where the agent stands:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œ *Step 4* ä¸­çš„ä»£ç åï¼Œä½ å°†çœ‹åˆ°ä¸€ä¸ª 4 * 4 çš„çŸ©é˜µï¼Œä»£è¡¨å†°æ¹–å’Œä»£ç†ç«™ç«‹çš„ç“·ç –ï¼ˆçŠ¶æ€ 0ï¼‰ï¼š
- en: '![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)'
- en: 'After running the lines of code in *Step 5*, you will see the resulting grid
    as follows, where the agent moves down to state 4:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œ *Step 5* ä¸­çš„ä»£ç è¡Œåï¼Œä½ å°†çœ‹åˆ°å¦‚ä¸‹ç»“æœç½‘æ ¼ï¼Œä»£ç†å‘ä¸‹ç§»åŠ¨åˆ°çŠ¶æ€ 4ï¼š
- en: '![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)'
- en: 'An episode will terminate if either of the following two conditions is met:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ä¹‹ä¸€ï¼Œä¸€ä¸ªå›åˆå°†ç»ˆæ­¢ï¼š
- en: Moving to an H tile (state 5, 7, 11, 12 ). This will generate a total reward
    of 0.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç§»åŠ¨åˆ° H æ ¼ï¼ˆçŠ¶æ€ 5ã€7ã€11ã€12ï¼‰ã€‚è¿™å°†ç”Ÿæˆæ€»å¥–åŠ± 0ã€‚
- en: Moving to the G tile (state 15). This will generate a total reward of +1.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç§»åŠ¨åˆ° G æ ¼ï¼ˆçŠ¶æ€ 15ï¼‰ã€‚è¿™å°†äº§ç”Ÿæ€»å¥–åŠ± +1ã€‚
- en: There's more...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šå†…å®¹â€¦â€¦
- en: 'We can look into the details of the FrozenLake environment, including the transformation
    matrix and rewards for each state and action, by using the P attribute. For example,
    for state 6, we can do the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ P å±æ€§æŸ¥çœ‹ FrozenLake ç¯å¢ƒçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬è½¬ç§»çŸ©é˜µå’Œæ¯ä¸ªçŠ¶æ€åŠåŠ¨ä½œçš„å¥–åŠ±ã€‚ä¾‹å¦‚ï¼Œå¯¹äºçŠ¶æ€ 6ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This returns a dictionary with keys 0, 1, 2, and 3, representing four possible
    actions. The value is a list of movements after taking an action. The movement
    list is in the following format: (transformation probability, new state, reward
    received, is done). For instance, if the agent resides in state 6 and intends
    to take action 1 (down), there is a 33.33% chance that it will land in state 5,
    receiving a reward of 0 and terminating the episode; there is a 33.33% chance
    that it will land in state 10 and receive a reward of 0; and there is a 33.33%
    chance that it will land in state 7, receiving a reward of 0 and terminating the
    episode.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶é”®ä¸º 0ã€1ã€2 å’Œ 3ï¼Œåˆ†åˆ«ä»£è¡¨å››ç§å¯èƒ½çš„åŠ¨ä½œã€‚å€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«åœ¨æ‰§è¡ŒåŠ¨ä½œåçš„ç§»åŠ¨ã€‚ç§»åŠ¨åˆ—è¡¨çš„æ ¼å¼å¦‚ä¸‹ï¼šï¼ˆè½¬ç§»æ¦‚ç‡ï¼Œæ–°çŠ¶æ€ï¼Œè·å¾—çš„å¥–åŠ±ï¼Œæ˜¯å¦ç»“æŸï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä»£ç†å¤„äºçŠ¶æ€
    6 å¹¶æ‰“ç®—æ‰§è¡ŒåŠ¨ä½œ 1ï¼ˆå‘ä¸‹ï¼‰ï¼Œæœ‰ 33.33% çš„æ¦‚ç‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 5ï¼Œè·å¾—å¥–åŠ± 0 å¹¶ç»ˆæ­¢è¯¥å›åˆï¼›æœ‰ 33.33% çš„æ¦‚ç‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 10ï¼Œè·å¾—å¥–åŠ±
    0ï¼›æœ‰ 33.33% çš„æ¦‚ç‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 7ï¼Œè·å¾—å¥–åŠ± 0 å¹¶ç»ˆæ­¢è¯¥å›åˆã€‚
- en: 'For state 11, we can do the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçŠ¶æ€ 11ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As stepping on a hole will terminate an episode, it wonâ€™t make any movement
    afterward.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¸©åˆ°æ´ä¼šç»ˆæ­¢ä¸€ä¸ªå›åˆï¼Œæ‰€ä»¥ä¸ä¼šå†æœ‰ä»»ä½•ç§»åŠ¨ã€‚
- en: Feel free to check out the other states.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ„æŸ¥çœ‹å…¶ä»–çŠ¶æ€ã€‚
- en: Solving an MDP with a value iteration algorithm
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP
- en: An MDP is considered solved if its optimal policy is found. In this recipe,
    we will figure out the optimal policy for the FrozenLake environment using a **value
    iteration** algorithm.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¾åˆ°å…¶æœ€ä¼˜ç­–ç•¥ï¼Œåˆ™è®¤ä¸º MDP å·²è§£å†³ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **å€¼è¿­ä»£** ç®—æ³•æ‰¾å‡º FrozenLake ç¯å¢ƒçš„æœ€ä¼˜ç­–ç•¥ã€‚
- en: 'The idea behind value iteration is quite similar to that of policy evaluation.
    It is also an iterative algorithm. It starts with arbitrary policy values and
    then iteratively updates the values based on the **Bellman optimality equation**
    until they converge. So in each iteration, instead of taking the expectation (average)
    of values across all actions, it picks the action that achieves the maximal policy
    values:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£çš„æ€æƒ³ä¸ç­–ç•¥è¯„ä¼°éå¸¸ç›¸ä¼¼ã€‚å®ƒä¹Ÿæ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ã€‚å®ƒä»ä»»æ„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹è¿­ä»£æ›´æ–°å€¼ï¼Œç›´åˆ°æ”¶æ•›ã€‚å› æ­¤ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä¸æ˜¯é‡‡ç”¨è·¨æ‰€æœ‰åŠ¨ä½œçš„å€¼çš„æœŸæœ›ï¼ˆå¹³å‡å€¼ï¼‰ï¼Œè€Œæ˜¯é€‰æ‹©å®ç°æœ€å¤§ç­–ç•¥å€¼çš„åŠ¨ä½œï¼š
- en: '![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)'
- en: Here, V*(s) denotes the optimal value, which is the value of the optimal policy;
    T(s, a, s') is the transition probability from state s to state sâ€™ by taking action
    a; and R(s, a) is the reward received in state s by taking action a.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼ŒV*(s)è¡¨ç¤ºæœ€ä¼˜å€¼ï¼Œå³æœ€ä¼˜ç­–ç•¥çš„å€¼ï¼›T(s, a, s')æ˜¯é‡‡å–åŠ¨ä½œaä»çŠ¶æ€sè½¬ç§»åˆ°çŠ¶æ€sâ€™çš„è½¬ç§»æ¦‚ç‡ï¼›è€ŒR(s, a)æ˜¯é‡‡å–åŠ¨ä½œaæ—¶åœ¨çŠ¶æ€sä¸­æ”¶åˆ°çš„å¥–åŠ±ã€‚
- en: 'Once the optimal values are computed, we can easily obtain the optimal policy
    accordingly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å‡ºæœ€ä¼˜å€¼åï¼Œæˆ‘ä»¬å¯ä»¥ç›¸åº”åœ°è·å¾—æœ€ä¼˜ç­–ç•¥ï¼š
- en: '![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)'
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åšâ€¦
- en: 'Let''s solve the FrozenLake environment using a value iteration algorithm as
    follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³FrozenLakeç¯å¢ƒå¦‚ä¸‹ï¼š
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»ºFrozenLakeç¯å¢ƒçš„å®ä¾‹ï¼š
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æŠ˜æ‰£å› å­è®¾ä¸º`0.99`ï¼Œæ”¶æ•›é˜ˆå€¼è®¾ä¸º`0.0001`ã€‚
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now define the function that computes optimal values based on the value iteration
    algorithm:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä¼˜å€¼ï¼š
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Plug in the environment, discount factor, and convergence threshold, then print
    the optimal values:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ’å…¥ç¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼Œç„¶åæ‰“å°æœ€ä¼˜å€¼ï¼š
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now that we have the optimal values, we develop the function that extracts
    the optimal policy out of them:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æœ€ä¼˜å€¼ï¼Œæˆ‘ä»¬å¼€å‘æå–æœ€ä¼˜ç­–ç•¥çš„å‡½æ•°ï¼š
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Plug in the environment, discount factor, and optimal values, then print the
    optimal policy:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ’å…¥ç¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæœ€ä¼˜å€¼ï¼Œç„¶åæ‰“å°æœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We want to gauge how good the optimal policy is. So, let''s run 1,000 episodes
    with the optimal policy and check the average reward. Here, we will reuse the
    `run_episode` function we defined in the previous recipe:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦è¯„ä¼°æœ€ä¼˜ç­–ç•¥çš„å¥½åç¨‹åº¦ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æœ€ä¼˜ç­–ç•¥è¿è¡Œ1,000æ¬¡æƒ…èŠ‚ï¼Œå¹¶æ£€æŸ¥å¹³å‡å¥–åŠ±ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é‡å¤ä½¿ç”¨æˆ‘ä»¬åœ¨å‰é¢çš„é…æ–¹ä¸­å®šä¹‰çš„`run_episode`å‡½æ•°ï¼š
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Under the optimal policy, the agent will reach the goal 75% of the time, on
    average. This is the best we are able to get since the ice is slippery.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹ï¼Œä»£ç†å°†å¹³å‡75%çš„æ—¶é—´åˆ°è¾¾ç›®æ ‡ã€‚è¿™æ˜¯æˆ‘ä»¬èƒ½å¤Ÿåšåˆ°çš„æœ€å¥½ç»“æœï¼Œå› ä¸ºå†°å¾ˆæ»‘ã€‚
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥ä½œåŸç†â€¦
- en: In a value iteration algorithm, we get the optimal value function by iteratively
    applying the Bellman optimality equation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å€¼è¿­ä»£ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¿­ä»£åº”ç”¨è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ¥è·å¾—æœ€ä¼˜å€¼å‡½æ•°ã€‚
- en: 'The following is another version of the Bellman optimality equation, which
    can deal with environments where rewards are partially dependent on the new state:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„å¦ä¸€ç‰ˆæœ¬ï¼Œé€‚ç”¨äºå¥–åŠ±éƒ¨åˆ†ä¾èµ–äºæ–°çŠ¶æ€çš„ç¯å¢ƒï¼š
- en: '![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)'
- en: 'Here, R(s, a, s'') is the reward received as a result of moving to state s''
    from state s by taking action a. As this version is more compatible, we developed
    our `value_iteration` function according to it. As you saw in *Step 3*, we perform
    the following tasks:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼ŒR(s, a, s')è¡¨ç¤ºé€šè¿‡é‡‡å–åŠ¨ä½œaä»çŠ¶æ€sç§»åŠ¨åˆ°çŠ¶æ€s'è€Œæ”¶åˆ°çš„å¥–åŠ±ã€‚ç”±äºè¿™ä¸ªç‰ˆæœ¬æ›´å…¼å®¹ï¼Œæˆ‘ä»¬æ ¹æ®å®ƒå¼€å‘äº†æˆ‘ä»¬çš„`value_iteration`å‡½æ•°ã€‚æ­£å¦‚æ‚¨åœ¨*Step
    3*ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
- en: Initialize the policy values as all zeros.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨éƒ¨ä¸ºé›¶ã€‚
- en: Update the values based on the Bellman optimality equation.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ›´æ–°å€¼ã€‚
- en: Compute the maximal change of the values across all states.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ‰€æœ‰çŠ¶æ€çš„å€¼çš„æœ€å¤§å˜åŒ–ã€‚
- en: If the maximal change is greater than the threshold, we keep updating the values.
    Otherwise, we terminate the evaluation process and return the latest values as
    the optimal values.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœ€å¤§å˜åŒ–å¤§äºé˜ˆå€¼ï¼Œåˆ™ç»§ç»­æ›´æ–°å€¼ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶è¿”å›æœ€æ–°çš„å€¼ä½œä¸ºæœ€ä¼˜å€¼ã€‚
- en: There's more...
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šâ€¦
- en: 'We obtained a success rate of 75% with a discount factor of 0.99\. How does
    the discount factor affect the performance? Let''s do some experiments with different
    factors, including `0`, `0.2`, `0.4`, `0.6`, `0.8`, `0.99`, and `1.`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æŠ˜æ‰£å› å­ä¸º0.99æ—¶è·å¾—äº†75%çš„æˆåŠŸç‡ã€‚æŠ˜æ‰£å› å­å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿè®©æˆ‘ä»¬ç”¨ä¸åŒçš„å› å­è¿›è¡Œä¸€äº›å®éªŒï¼ŒåŒ…æ‹¬`0`ã€`0.2`ã€`0.4`ã€`0.6`ã€`0.8`ã€`0.99`å’Œ`1.`ï¼š
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'For each discount factor, we compute the average success rate over 10,000 episodes:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæŠ˜æ‰£å› å­ï¼Œæˆ‘ä»¬è®¡ç®—äº†10,000ä¸ªå‘¨æœŸçš„å¹³å‡æˆåŠŸç‡ï¼š
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We draw a plot of the average success rate versus the discount factor:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç»˜åˆ¶äº†å¹³å‡æˆåŠŸç‡ä¸æŠ˜æ‰£å› å­çš„å›¾è¡¨ï¼š
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We get the following plot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹çš„ç»˜å›¾ï¼š
- en: '![](img/650d388b-391a-4be4-8537-d49d31b08315.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/650d388b-391a-4be4-8537-d49d31b08315.png)'
- en: The result shows that the performance improves when there is an increase in
    the discount factor. This verifies the fact that a small discount factor values
    the reward now and a large discount factor values a better reward in the future.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¾ç¤ºï¼Œå½“æŠ˜æ‰£å› å­å¢åŠ æ—¶ï¼Œæ€§èƒ½æœ‰æ‰€æå‡ã€‚è¿™è¯å®äº†ä¸€ä¸ªå°çš„æŠ˜æ‰£å› å­ç›®å‰ä»·å€¼å¥–åŠ±ï¼Œè€Œä¸€ä¸ªå¤§çš„æŠ˜æ‰£å› å­åˆ™æ›´çœ‹é‡æœªæ¥çš„æ›´å¥½å¥–åŠ±ã€‚
- en: Solving an MDP with a policy iteration algorithm
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³MDP
- en: Another approach to solving an MDP is by using a **policy iteration** algorithm,
    which we will discuss in this recipe.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³MDPçš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨**ç­–ç•¥è¿­ä»£**ç®—æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬é…æ–¹ä¸­è®¨è®ºå®ƒã€‚
- en: 'A policy iteration algorithm can be subdivided into two components: policy
    evaluation and policy improvement. It starts with an arbitrary policy. And in
    each iteration, it first computes the policy values given the latest policy, based
    on the Bellman expectation equation; it then extracts an improved policy out of
    the resulting policy values, based on the Bellman optimality equation. It iteratively
    evaluates the policy and generates an improved version until the policy doesn''t
    change any more.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥è¿­ä»£ç®—æ³•å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚å®ƒä»ä»»æ„ç­–ç•¥å¼€å§‹ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒé¦–å…ˆæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—ç»™å®šæœ€æ–°ç­–ç•¥çš„ç­–ç•¥å€¼ï¼›ç„¶åæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä»ç»“æœç­–ç•¥å€¼ä¸­æå–ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥ã€‚å®ƒåå¤è¯„ä¼°ç­–ç•¥å¹¶ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬ï¼Œç›´åˆ°ç­–ç•¥ä¸å†æ”¹å˜ä¸ºæ­¢ã€‚
- en: Let's develop a policy iteration algorithm and use it to solve the FrozenLake
    environment. After that, we will explain how it works.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¿­ä»£ç®—æ³•ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è§£å†³FrozenLakeç¯å¢ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†è§£é‡Šå®ƒçš„å·¥ä½œåŸç†ã€‚
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åšâ€¦
- en: 'Let''s solve the FrozenLake environment using a policy iteration algorithm
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³FrozenLakeç¯å¢ƒï¼š
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»ºFrozenLakeç¯å¢ƒçš„å®ä¾‹ï¼š
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæš‚å°†æŠ˜æ‰£å› å­è®¾å®šä¸º`0.99`ï¼Œæ”¶æ•›é˜ˆå€¼è®¾å®šä¸º`0.0001`ï¼š
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now we define the `policy_evaluation` function that computes the values given
    a policy:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰`policy_evaluation`å‡½æ•°ï¼Œå®ƒè®¡ç®—ç»™å®šç­–ç•¥çš„å€¼ï¼š
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is similar to what we did in the *Performing policy evaluation* recipe,
    but with the Gym environment as an input.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬åœ¨*æ‰§è¡Œç­–ç•¥è¯„ä¼°*é…æ–¹ä¸­æ‰€åšçš„ç±»ä¼¼ï¼Œä½†è¾“å…¥æ˜¯Gymç¯å¢ƒã€‚
- en: 'Next, we develop the second main component of the policy iteration algorithm,
    the policy improvement part:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ç¬¬äºŒä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå³ç­–ç•¥æ”¹è¿›éƒ¨åˆ†ï¼š
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This extracts an improved policy from the given policy values, based on the
    Bellman optimality equation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä»ç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–äº†ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥ã€‚
- en: 'Now that we have both components ready, we develop the policy iteration algorithm
    as follows:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä¸¤ä¸ªç»„ä»¶éƒ½å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬æŒ‰ä»¥ä¸‹æ–¹å¼å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•ï¼š
- en: '[PRE50]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Plug in the environment, discount factor, and convergence threshold:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ’å…¥ç¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼š
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We''ve obtained the optimal values and optimal policy. Let''s take a look at
    them:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è·å¾—äº†æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚è®©æˆ‘ä»¬çœ‹ä¸€çœ‹å®ƒä»¬ï¼š
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: They are exactly the same as what we got using the value iteration algorithm.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•å¾—åˆ°çš„ç»“æœå®Œå…¨ä¸€æ ·ã€‚
- en: How it works...
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„â€¦
- en: 'Policy iteration combines policy evaluation and policy improvement in each
    iteration. In policy evaluation, the values for a given policy (not the optimal
    one) are calculated based on the Bellman expectation equation until they converge:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥è¿­ä»£ç»“åˆäº†æ¯æ¬¡è¿­ä»£ä¸­çš„ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚åœ¨ç­–ç•¥è¯„ä¼°ä¸­ï¼Œæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—ç»™å®šç­–ç•¥ï¼ˆè€Œéæœ€ä¼˜ç­–ç•¥ï¼‰çš„å€¼ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ï¼š
- en: '![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)'
- en: Here, a = Ï€(s), which is the action taken under policy Ï€ in state s.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œa = Ï€(s)ï¼Œå³åœ¨çŠ¶æ€sä¸‹æ ¹æ®ç­–ç•¥Ï€é‡‡å–çš„åŠ¨ä½œã€‚
- en: 'In policy improvement, the policy is updated using the resulting converged
    policy values, V(s), based on the Bellman optimality equation:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥æ”¹è¿›ä¸­ï¼Œæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä½¿ç”¨æ”¶æ•›çš„ç­–ç•¥å€¼V(s)æ›´æ–°ç­–ç•¥ï¼š
- en: '![](img/f60962b4-570b-4954-8da6-79733626a594.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f60962b4-570b-4954-8da6-79733626a594.png)'
- en: 'This repeats the policy evaluation and policy improvement steps until the policy
    converges. At convergence, the latest policy and its value function are the optimal
    policy and the optimal value function. Hence, in Step 5, the `policy_iteration`
    function does the following tasks:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡å¤ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ­¥éª¤ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›ã€‚åœ¨æ”¶æ•›æ—¶ï¼Œæœ€æ–°çš„ç­–ç•¥å’Œå…¶å€¼å‡½æ•°æ˜¯æœ€ä¼˜ç­–ç•¥å’Œæœ€ä¼˜å€¼å‡½æ•°ã€‚å› æ­¤ï¼Œåœ¨ç¬¬5æ­¥ï¼Œ`policy_iteration`å‡½æ•°æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
- en: Initializes a random policy.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ä¸€ä¸ªéšæœºç­–ç•¥ã€‚
- en: Computes the values of the policy with the policy evaluation algorithm.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­–ç•¥è¯„ä¼°ç®—æ³•è®¡ç®—ç­–ç•¥çš„å€¼ã€‚
- en: Obtains an improved policy based on the policy values.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºç­–ç•¥å€¼è·å–æ”¹è¿›çš„ç­–ç•¥ã€‚
- en: If the new policy is different from the old one, it updates the policy and runs
    another iteration. Otherwise, it terminates the iteration process and returns
    the policy values and the policy.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ–°ç­–ç•¥ä¸æ—§ç­–ç•¥ä¸åŒï¼Œåˆ™æ›´æ–°ç­–ç•¥å¹¶è¿è¡Œå¦ä¸€æ¬¡è¿­ä»£ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¿­ä»£è¿‡ç¨‹å¹¶è¿”å›ç­–ç•¥å€¼å’Œç­–ç•¥ã€‚
- en: There's more...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤š...
- en: 'We have just solved the FrozenLake environment with a policy iteration algorithm.
    So, you may wonder when it is better to use policy iteration over value iteration
    and vice versa. There are basically three scenarios where one has the edge over
    the other:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³äº†FrozenLakeç¯å¢ƒã€‚å› æ­¤ï¼Œæ‚¨å¯èƒ½æƒ³çŸ¥é“ä½•æ—¶æœ€å¥½ä½¿ç”¨ç­–ç•¥è¿­ä»£è€Œä¸æ˜¯å€¼è¿­ä»£ï¼Œåä¹‹äº¦ç„¶ã€‚åŸºæœ¬ä¸Šæœ‰ä¸‰ç§æƒ…å†µå…¶ä¸­ä¸€ç§æ¯”å¦ä¸€ç§æ›´å ä¼˜åŠ¿ï¼š
- en: If there is a large number of actions, use policy iteration, as it can converge
    faster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰å¤§é‡çš„åŠ¨ä½œï¼Œè¯·ä½¿ç”¨ç­–ç•¥è¿­ä»£ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´å¿«åœ°æ”¶æ•›ã€‚
- en: If there is a small number of actions, use value iteration.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœåŠ¨ä½œæ•°é‡è¾ƒå°‘ï¼Œè¯·ä½¿ç”¨å€¼è¿­ä»£ã€‚
- en: If there is already a viable policy (obtained either by intuition or domain
    knowledge), use policy iteration.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå·²ç»æœ‰ä¸€ä¸ªå¯è¡Œçš„ç­–ç•¥ï¼ˆé€šè¿‡ç›´è§‰æˆ–é¢†åŸŸçŸ¥è¯†è·å¾—ï¼‰ï¼Œè¯·ä½¿ç”¨ç­–ç•¥è¿­ä»£ã€‚
- en: Outside those scenarios, policy iteration and value iteration are generally
    comparable.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›æƒ…å†µä¹‹å¤–ï¼Œç­–ç•¥è¿­ä»£å’Œå€¼è¿­ä»£é€šå¸¸æ˜¯å¯æ¯”è¾ƒçš„ã€‚
- en: In the next recipe, we will apply each algorithm to solve the coin-flipping-gamble
    problem. We will see which algorithm converges faster.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨æ¯ç§ç®—æ³•æ¥è§£å†³ç¡¬å¸æŠ›æ·èµŒåšé—®é¢˜ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å“ªç§ç®—æ³•æ”¶æ•›å¾—æ›´å¿«ã€‚
- en: See also
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è§
- en: Feel free to use what we've learned in these two recipes to solve a bigger ice
    grid, the `FrozenLake8x8-v0` environment ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·éšæ„ä½¿ç”¨æˆ‘ä»¬åœ¨è¿™ä¸¤ä¸ªæ¡ˆä¾‹ä¸­å­¦åˆ°çš„çŸ¥è¯†æ¥è§£å†³ä¸€ä¸ªæ›´å¤§çš„å†°æ ¼ï¼Œå³ `FrozenLake8x8-v0` ç¯å¢ƒ ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚
- en: Solving the coin-flipping gamble problem
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£å†³ç¡¬å¸æŠ›æ·èµŒåšé—®é¢˜
- en: Gambling on coin flipping should sound familiar to everyone. In each round of
    the game, the gambler can make a bet on whether a coin flip will show heads. If
    it turns out heads, the gambler will win the same amount they bet; otherwise,
    they will lose this amount. The game continues until the gambler loses (ends up
    with nothing) or wins (wins more than 100 dollars, let's say). Let's say the coin
    is unfair and it lands on heads 40% of the time. In order to maximize the chance
    of winning, how much should the gambler bet based on their current capital in
    each round? This will definitely be an interesting problem to solve.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç¡¬å¸æŠ›æ·èµŒåšåº”è¯¥å¯¹æ¯ä¸ªäººéƒ½å¾ˆç†Ÿæ‚‰ã€‚åœ¨æ¸¸æˆçš„æ¯ä¸€è½®ä¸­ï¼ŒèµŒå¾’å¯ä»¥æ‰“èµŒç¡¬å¸æ˜¯å¦ä¼šæ­£é¢æœä¸Šã€‚å¦‚æœç»“æœæ˜¯æ­£é¢ï¼ŒèµŒå¾’å°†èµ¢å¾—ä»–ä»¬ä¸‹æ³¨çš„ç›¸åŒé‡‘é¢ï¼›å¦åˆ™ï¼Œä»–ä»¬å°†å¤±å»è¿™ç¬”é‡‘é¢ã€‚æ¸¸æˆå°†ç»§ç»­ï¼Œç›´åˆ°èµŒå¾’è¾“æ‰ï¼ˆæœ€ç»ˆä¸€æ— æ‰€æœ‰ï¼‰æˆ–èµ¢å¾—ï¼ˆèµ¢å¾—è¶…è¿‡100ç¾å…ƒï¼Œå‡è®¾ï¼‰ã€‚å‡è®¾ç¡¬å¸ä¸å…¬å¹³ï¼Œå¹¶ä¸”æœ‰40%çš„æ¦‚ç‡æ­£é¢æœä¸Šã€‚ä¸ºäº†æœ€å¤§åŒ–èµ¢çš„æœºä¼šï¼ŒèµŒå¾’åº”è¯¥æ ¹æ®å½“å‰èµ„æœ¬åœ¨æ¯ä¸€è½®ä¸‹æ³¨å¤šå°‘ï¼Ÿè¿™ç»å¯¹æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜è¦è§£å†³ã€‚
- en: If the coin lands on heads more than 50% of the time, there is nothing to discuss.
    The gambler can just keep betting one dollar each round and should win the game
    most of the time. If it is a fair coin, the gambler could bet one dollar each
    round and end up winning around 50% of the time. It gets tricky when the probability
    of heads is lower than 50%; the safe-bet strategy wouldn't work anymore. Nor would
    a random strategy, either. We need to resort to the reinforcement learning techniques
    we've learned in this chapter to make smart bets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡è¶…è¿‡ 50%ï¼Œå°±æ²¡ä»€ä¹ˆå¥½è®¨è®ºçš„ã€‚èµŒå¾’å¯ä»¥æ¯è½®ä¸‹æ³¨ä¸€ç¾å…ƒï¼Œå¹¶ä¸”å¤§å¤šæ•°æƒ…å†µä¸‹åº”è¯¥èƒ½èµ¢å¾—æ¸¸æˆã€‚å¦‚æœæ˜¯å…¬å¹³ç¡¬å¸ï¼ŒèµŒå¾’æ¯è½®ä¸‹æ³¨ä¸€ç¾å…ƒæ—¶ï¼Œå¤§çº¦ä¸€åŠçš„æ—¶é—´ä¼šèµ¢ã€‚å½“æ­£é¢æœä¸Šçš„æ¦‚ç‡ä½äº
    50% æ—¶ï¼Œä¿å®ˆçš„ç­–ç•¥å°±è¡Œä¸é€šäº†ã€‚éšæœºç­–ç•¥ä¹Ÿä¸è¡Œã€‚æˆ‘ä»¬éœ€è¦ä¾é æœ¬ç« å­¦åˆ°çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ¥åšå‡ºæ˜æ™ºçš„æŠ•æ³¨ã€‚
- en: 'Let''s get started by formulating the coin-flipping gamble problem as an MDP.
    It is basically an undiscounted, episodic, and finite MDP with the following properties:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å°†æŠ›ç¡¬å¸èµŒåšé—®é¢˜åˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªæ— æŠ˜æ‰£ã€å‘¨æœŸæ€§çš„æœ‰é™ MDPï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
- en: 'The state is the gambler''s capital in dollars. There are 101 states: 0, 1,
    2, â€¦, 98, 99, and 100+.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŠ¶æ€æ˜¯èµŒå¾’çš„ç¾å…ƒèµ„æœ¬ã€‚æ€»å…±æœ‰ 101 ä¸ªçŠ¶æ€ï¼š0ã€1ã€2ã€â€¦ã€98ã€99 å’Œ 100+ã€‚
- en: The reward is 1 if the state 100+ is reached; otherwise, the reward is 0.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè¾¾åˆ°çŠ¶æ€ 100+ï¼Œåˆ™å¥–åŠ±ä¸º 1ï¼›å¦åˆ™ï¼Œå¥–åŠ±ä¸º 0ã€‚
- en: The action is the possible amount the gambler bets in a round. Given state s,
    the possible actions include 1, 2, â€¦, and min(s, 100 - s). For example, when the
    gambler has 60 dollars, they can bet any amount from 1 to 40\. Any amount above
    40 doesn't make any sense as it increases the loss and doesn't increase the chance
    of winning.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¡ŒåŠ¨æ˜¯èµŒå¾’åœ¨ä¸€è½®ä¸­å¯èƒ½ä¸‹æ³¨çš„é‡‘é¢ã€‚å¯¹äºçŠ¶æ€ sï¼Œå¯èƒ½çš„è¡ŒåŠ¨åŒ…æ‹¬ 1ã€2ã€â€¦ï¼Œä»¥åŠ min(s, 100 - s)ã€‚ä¾‹å¦‚ï¼Œå½“èµŒå¾’æœ‰ 60 ç¾å…ƒæ—¶ï¼Œä»–ä»¬å¯ä»¥ä¸‹æ³¨ä»
    1 åˆ° 40 çš„ä»»æ„é‡‘é¢ã€‚è¶…è¿‡ 40 çš„ä»»ä½•é‡‘é¢éƒ½æ²¡æœ‰æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¢åŠ äº†æŸå¤±å¹¶ä¸”ä¸å¢åŠ èµ¢å¾—æ¸¸æˆçš„æœºä¼šã€‚
- en: The next state after taking an action depends on the probability of the coin
    coming up heads. Let's say it is 40%. So, the next state of state s after taking
    action *a* will be *s+a* by 40%, *s-a* by 60%.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é‡‡å–è¡ŒåŠ¨åï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€å–å†³äºç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ã€‚å‡è®¾æ˜¯ 40%ã€‚å› æ­¤ï¼Œåœ¨é‡‡å–è¡ŒåŠ¨ *a* åï¼ŒçŠ¶æ€ s çš„ä¸‹ä¸€ä¸ªçŠ¶æ€å°†ä»¥ 40% çš„æ¦‚ç‡ä¸º *s+a*ï¼Œä»¥
    60% çš„æ¦‚ç‡ä¸º *s-a*ã€‚
- en: The process terminates at state 0 and state 100+.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡ç¨‹åœ¨çŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+ å¤„ç»ˆæ­¢ã€‚
- en: How to do it...
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åšâ€¦
- en: 'We first solve the coin-flipping gamble problem by using a value iteration
    algorithm and performing the following steps:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³æŠ›ç¡¬å¸èµŒåšé—®é¢˜ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: 'Import PyTorch:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥ PyTorchï¼š
- en: '[PRE53]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Specify the discount factor and convergence threshold:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‡å®šæŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼š
- en: '[PRE54]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Here, we set 1 as the discount factor as the MDP is an undiscounted process;
    we set a small threshold as we expect small policy values since all rewards are
    0 except the last state.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŠ˜æ‰£å› å­è®¾ä¸º 1ï¼Œå› ä¸ºè¿™ä¸ª MDP æ˜¯ä¸€ä¸ªæ— æŠ˜æ‰£çš„è¿‡ç¨‹ï¼›æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªå°é˜ˆå€¼ï¼Œå› ä¸ºæˆ‘ä»¬é¢„æœŸç­–ç•¥å€¼è¾ƒå°ï¼Œæ‰€æœ‰å¥–åŠ±éƒ½æ˜¯ 0ï¼Œé™¤äº†æœ€åä¸€ä¸ªçŠ¶æ€ã€‚
- en: Define the following environment variables.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰ä»¥ä¸‹ç¯å¢ƒå˜é‡ã€‚
- en: 'There are 101 states in total:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å…±æœ‰ 101 ä¸ªçŠ¶æ€ï¼š
- en: '[PRE55]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The corresponding reward is displayed as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åº”çš„å¥–åŠ±æ˜¾ç¤ºå¦‚ä¸‹ï¼š
- en: '[PRE56]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s say the probability of getting heads is 40%:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ­£é¢æœä¸Šçš„æ¦‚ç‡æ˜¯ 40%ï¼š
- en: '[PRE57]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Put these variables into a dictionary:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™äº›å˜é‡æ”¾å…¥å­—å…¸ä¸­ï¼š
- en: '[PRE58]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now we develop a function that computes optimal values based on the value iteration
    algorithm:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¼€å‘ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä¼˜å€¼ï¼š
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We only need to compute the values for states 1 to 99, as the values for state
    0 and state 100+ are 0\. And given state *s*, the possible actions can be anything
    from 1 up to *min(s, 100 - s)*. We should keep this in mind while computing the
    Bellman optimality equation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€è®¡ç®—çŠ¶æ€ 1 åˆ° 99 çš„å€¼ï¼Œå› ä¸ºçŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+ çš„å€¼ä¸º 0ã€‚è€Œç»™å®šçŠ¶æ€ *s*ï¼Œå¯èƒ½çš„è¡ŒåŠ¨å¯ä»¥æ˜¯ä» 1 åˆ° *min(s, 100
    - s)*ã€‚åœ¨è®¡ç®—è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥ç‰¢è®°è¿™ä¸€ç‚¹ã€‚
- en: 'Next, we develop a function that extracts the optimal policy based on the optimal
    values:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®æœ€ä¼˜å€¼æå–æœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE60]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Finally, we can plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and optimal policy after . Also, we time how long
    it takes to solve the gamble MDP with value iteration; we will compare this with
    the time it takes for policy iteration to complete:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼è¾“å…¥ï¼Œè®¡ç®—å‡ºæœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¡æ—¶äº†ä½¿ç”¨å€¼è¿­ä»£è§£å†³èµŒåš MDP æ‰€éœ€çš„æ—¶é—´ï¼›æˆ‘ä»¬å°†å…¶ä¸ç­–ç•¥è¿­ä»£å®Œæˆæ‰€éœ€çš„æ—¶é—´è¿›è¡Œæ¯”è¾ƒï¼š
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We solved the gamble problem with value iteration in `4.717` seconds.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ `4.717` ç§’å†…ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³äº†èµŒåšé—®é¢˜ã€‚
- en: 'Take a look at the optimal policy values and the optimal policy we got:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æˆ‘ä»¬å¾—åˆ°çš„æœ€ä¼˜ç­–ç•¥å€¼å’Œæœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE62]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can plot the policy value versus state as follows:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ç­–ç•¥å€¼ä¸çŠ¶æ€çš„å›¾è¡¨å¦‚ä¸‹ï¼š
- en: '[PRE63]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now that we've solved the gamble problem with value iteration, how about policy
    iteration? Let's see.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡å€¼è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ï¼Œæ¥ä¸‹æ¥æ˜¯ç­–ç•¥è¿­ä»£ï¼Ÿæˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: 'We start by developing the `policy_evaluation` function that computes the values
    given a policy:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¼€å‘`policy_evaluation`å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®ç­–ç•¥è®¡ç®—å€¼ï¼š
- en: '[PRE64]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we develop another main component of the policy iteration algorithm,
    the policy improvement part:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„å¦ä¸€ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå³ç­–ç•¥æ”¹è¿›éƒ¨åˆ†ï¼š
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'With both components ready, we can develop the main entry to the policy iteration
    algorithm as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸¤ä¸ªç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ä¸»è¦å…¥å£å¦‚ä¸‹ï¼š
- en: '[PRE66]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and the optimal policy. We record the time spent
    solving the MDP as well:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ç¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼æ’å…¥ä»¥è®¡ç®—æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚æˆ‘ä»¬è®°å½•è§£å†³MDPæ‰€èŠ±è´¹çš„æ—¶é—´ï¼š
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Check out the optimal values and optimal policy we just obtained:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹åˆšåˆšè·å¾—çš„æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE68]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: How it works...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•è¿ä½œçš„â€¦â€¦
- en: 'After executing the lines of code in *Step 7*, you will see the optimal policy
    values:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œ*ç¬¬ 7 æ­¥*ä¸­çš„ä»£ç è¡Œåï¼Œæ‚¨å°†çœ‹åˆ°æœ€ä¼˜ç­–ç•¥å€¼ï¼š
- en: '[PRE69]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You will also see the optimal policy:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å°†çœ‹åˆ°æœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE70]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '*Step 8* generates the following plot for the optimal policy values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬ 8 æ­¥* ç”Ÿæˆäº†ä»¥ä¸‹æœ€ä¼˜ç­–ç•¥å€¼çš„å›¾è¡¨ï¼š'
- en: '![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)'
- en: We can see that, as the capital (state) increases, the estimated reward (policy
    value) also increases, which makes sense.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€èµ„æœ¬ï¼ˆçŠ¶æ€ï¼‰çš„å¢åŠ ï¼Œä¼°è®¡çš„å¥–åŠ±ï¼ˆç­–ç•¥å€¼ï¼‰ä¹Ÿåœ¨å¢åŠ ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ã€‚
- en: What we did in *Step 9* is very similar to what we did in the *Solving an MDP
    with a policy iteration algorithm* recipe, but for the coin-flipping gamble environment
    this time.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬ 9 æ­¥*ä¸­æˆ‘ä»¬æ‰€åšçš„äº‹æƒ…ä¸*Solving an MDP with a policy iteration algorithm*é…æ–¹ä¸­çš„æ‰€åšçš„éå¸¸ç›¸ä¼¼ï¼Œä½†è¿™æ¬¡æ˜¯é’ˆå¯¹æŠ›ç¡¬å¸èµŒåšç¯å¢ƒã€‚
- en: In *Step 10*, the policy improvement function extracts an improved policy out
    of the given policy values, based on the Bellman optimality equation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬ 10 æ­¥*ä¸­ï¼Œç­–ç•¥æ”¹è¿›å‡½æ•°ä»ç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–å‡ºæ”¹è¿›çš„ç­–ç•¥ï¼ŒåŸºäºè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ã€‚
- en: As you can see in *Step 12*, we solved the gamble problem with policy iteration
    in `2.002` seconds, which is less than half the time it took with value iteration.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨åœ¨*ç¬¬ 12 æ­¥*ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬é€šè¿‡ç­–ç•¥è¿­ä»£åœ¨`2.002`ç§’å†…è§£å†³äº†èµŒåšé—®é¢˜ï¼Œæ¯”å€¼è¿­ä»£æ‰€èŠ±è´¹çš„æ—¶é—´å°‘äº†ä¸€åŠã€‚
- en: 'The results we got from *Step 13* include the following optimal values:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»*ç¬¬ 13 æ­¥*å¾—åˆ°çš„ç»“æœåŒ…æ‹¬ä»¥ä¸‹æœ€ä¼˜å€¼ï¼š
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'They also include the optimal policy:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬è¿˜åŒ…æ‹¬æœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE72]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The results from the two approaches, value iteration and policy iteration, are
    consistent.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£çš„ä¸¤ç§æ–¹æ³•çš„ç»“æœæ˜¯ä¸€è‡´çš„ã€‚
- en: We have solved the gamble problem by using value iteration and policy iteration.
    To deal with a reinforcement learning problem, one of the trickiest tasks is to
    formulate the process into an MDP. In our case, the policy is transformed from
    the current capital (states) to the new capital (new states) by betting certain
    stakes (actions). The optimal policy maximizes the probability of winning the
    game (+1 reward), and evaluates the probability of winning under the optimal policy.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ã€‚å¤„ç†å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­æœ€æ£˜æ‰‹çš„ä»»åŠ¡ä¹‹ä¸€æ˜¯å°†è¿‡ç¨‹å½¢å¼åŒ–ä¸ºMDPã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œé€šè¿‡ä¸‹æ³¨ä¸€å®šçš„èµŒæ³¨ï¼ˆåŠ¨ä½œï¼‰ï¼Œå°†å½“å‰èµ„æœ¬ï¼ˆçŠ¶æ€ï¼‰çš„ç­–ç•¥è½¬åŒ–ä¸ºæ–°èµ„æœ¬ï¼ˆæ–°çŠ¶æ€ï¼‰ã€‚æœ€ä¼˜ç­–ç•¥æœ€å¤§åŒ–äº†èµ¢å¾—æ¸¸æˆçš„æ¦‚ç‡ï¼ˆ+1
    å¥–åŠ±ï¼‰ï¼Œå¹¶åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹è¯„ä¼°äº†èµ¢å¾—æ¸¸æˆçš„æ¦‚ç‡ã€‚
- en: 'Another interesting thing to note is how the transformation probabilities and
    new states are determined in the Bellman equation in our example. Taking action
    a in state s (having capital s and making a bet of 1 dollar) will have two possible
    outcomes:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰è¶£çš„äº‹æƒ…æ˜¯æ³¨æ„æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­å¦‚ä½•ç¡®å®šè´å°”æ›¼æ–¹ç¨‹ä¸­çš„è½¬æ¢æ¦‚ç‡å’Œæ–°çŠ¶æ€ã€‚åœ¨çŠ¶æ€ s ä¸­é‡‡å–åŠ¨ä½œ aï¼ˆæ‹¥æœ‰èµ„æœ¬ s å¹¶ä¸‹æ³¨ 1 ç¾å…ƒï¼‰ï¼Œå°†æœ‰ä¸¤ç§å¯èƒ½çš„ç»“æœï¼š
- en: Moving to new state s+a, if the coin lands on heads. Hence, the transformation
    probability is equal to the probability of heads.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœç¡¬å¸æ­£é¢æœä¸Šï¼Œåˆ™ç§»åŠ¨åˆ°æ–°çŠ¶æ€ s+aã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚ç‡ç­‰äºæ­£é¢æœä¸Šçš„æ¦‚ç‡ã€‚
- en: Moving to new state s-a, if the coin lands on tails. Therefore, the transformation
    probability is equal to the probability of tails.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœç¡¬å¸åé¢æœä¸Šï¼Œåˆ™ç§»åŠ¨åˆ°æ–°çŠ¶æ€ s-aã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚ç‡ç­‰äºåé¢æœä¸Šçš„æ¦‚ç‡ã€‚
- en: This is quite similar to the FrozenLake environment, where the agent lands on
    the intended tile only by a certain probability.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸FrozenLakeç¯å¢ƒéå¸¸ç›¸ä¼¼ï¼Œä»£ç†äººåªæœ‰ä»¥ä¸€å®šæ¦‚ç‡ç€é™†åœ¨é¢„æœŸçš„ç“¦ç‰‡ä¸Šã€‚
- en: We also verified that policy iteration converges faster than value iteration
    in this case. This is because there are up to 50 possible actions, which is more
    than the 4 actions in FrozenLake. For MDPs with a large number of actions, solving
    with policy iteration is more efficient than doing so with value iteration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éªŒè¯äº†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç­–ç•¥è¿­ä»£æ¯”å€¼è¿­ä»£æ”¶æ•›æ›´å¿«ã€‚è¿™æ˜¯å› ä¸ºå¯èƒ½æœ‰å¤šè¾¾ 50 ä¸ªå¯èƒ½çš„è¡ŒåŠ¨ï¼Œè¿™æ¯” FrozenLake ä¸­çš„ 4 ä¸ªè¡ŒåŠ¨æ›´å¤šã€‚å¯¹äºå…·æœ‰å¤§é‡è¡ŒåŠ¨çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œç”¨ç­–ç•¥è¿­ä»£è§£å†³æ¯”å€¼è¿­ä»£æ›´æœ‰æ•ˆç‡ã€‚
- en: There's more...
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤š...
- en: 'You may want to know whether the optimal policy really works. Let''s act like
    smart gamblers and play 10,000 episodes of the game. We are going to compare the
    optimal policy with two other strategies: conservative (betting one dollar each
    round) and random (betting a random amount):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½æƒ³çŸ¥é“æœ€ä¼˜ç­–ç•¥æ˜¯å¦çœŸçš„æœ‰æ•ˆã€‚è®©æˆ‘ä»¬åƒèªæ˜çš„èµŒå¾’ä¸€æ ·ç© 10,000 ä¸ªå‰§é›†çš„æ¸¸æˆã€‚æˆ‘ä»¬å°†æ¯”è¾ƒæœ€ä¼˜ç­–ç•¥ä¸å¦å¤–ä¸¤ç§ç­–ç•¥ï¼šä¿å®ˆç­–ç•¥ï¼ˆæ¯è½®ä¸‹æ³¨ä¸€ç¾å…ƒï¼‰å’Œéšæœºç­–ç•¥ï¼ˆä¸‹æ³¨éšæœºé‡‘é¢ï¼‰ï¼š
- en: We start by defining the three aforementioned betting strategies.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆé€šè¿‡å®šä¹‰ä¸‰ç§ä¸Šè¿°çš„æŠ•æ³¨ç­–ç•¥å¼€å§‹ã€‚
- en: 'We define the optimal strategy first:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰æœ€ä¼˜ç­–ç•¥ï¼š
- en: '[PRE73]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Then we define the conservative strategy:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰ä¿å®ˆç­–ç•¥ï¼š
- en: '[PRE74]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, we define the random strategy:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å®šä¹‰éšæœºç­–ç•¥ï¼š
- en: '[PRE75]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a wrapper function that runs one episode with a strategy and returns
    whether or not the game was won:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œç”¨ä¸€ç§ç­–ç•¥è¿è¡Œä¸€ä¸ªå‰§é›†ï¼Œå¹¶è¿”å›æ¸¸æˆæ˜¯å¦è·èƒœï¼š
- en: '[PRE76]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Specify a starting capital (letâ€™s say `50` dollars) and a number of episodes
    (`10000`):'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‡å®šä¸€ä¸ªèµ·å§‹èµ„æœ¬ï¼ˆå‡è®¾æ˜¯`50`ç¾å…ƒï¼‰å’Œä¸€å®šæ•°é‡çš„å‰§é›†ï¼ˆ`10000`ï¼‰ï¼š
- en: '[PRE77]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Run 10,000 episodes and keep track of the winning times:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿è¡Œ 10,000 ä¸ªå‰§é›†å¹¶è·Ÿè¸ªè·èƒœæ¬¡æ•°ï¼š
- en: '[PRE78]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Print out the winning probabilities for the three strategies:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰“å°å‡ºä¸‰ç§ç­–ç•¥çš„è·èƒœæ¦‚ç‡ï¼š
- en: '[PRE79]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Our optimal policy is clearly the winner!
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æœ€ä¼˜ç­–ç•¥æ˜¾ç„¶æ˜¯èµ¢å®¶ï¼
