- en: Markov Decision Processes and Dynamic Programming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程和动态规划
- en: 'In this chapter, we will continue our practical reinforcement learning journey
    with PyTorch by looking at **Markov decision processes** (**MDPs**) and dynamic
    programming. This chapter will start with the creation of a Markov chain and an
    MDP, which is the core of most reinforcement learning algorithms. You will also
    become more familiar with Bellman equations by practicing policy evaluation. We
    will then move on and apply two approaches to solving an MDP: value iteration
    and policy iteration. We will use the FrozenLake environment as an example. At
    the end of the chapter, we will demonstrate how to solve the interesting coin-flipping
    gamble problem with dynamic programming step by step.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过观察**马尔可夫决策过程**（MDPs）和动态规划来继续我们的实践强化学习旅程。本章将从创建马尔可夫链和MDP开始，这是大多数强化学习算法的核心。您还将通过实践策略评估更加熟悉贝尔曼方程。然后我们将继续并应用两种方法解决MDP问题：值迭代和策略迭代。我们将以FrozenLake环境作为示例。在本章的最后，我们将逐步展示如何使用动态规划解决有趣的硬币抛掷赌博问题。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下示例：
- en: Creating a Markov chain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建马尔可夫链
- en: Creating an MDP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 MDP
- en: Performing policy evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行策略评估
- en: Simulating the FrozenLake environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟 FrozenLake 环境
- en: Solving an MDP with a value iteration algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用值迭代算法解决 MDP
- en: Solving an MDP with a policy iteration algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略迭代算法解决 MDP
- en: Solving the coin-flipping gamble problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用值迭代算法解决 MDP
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You will need the following programs installed on your system to successfully
    execute the recipes in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功执行本章中的示例，请确保系统中安装了以下程序：
- en: Python 3.6, 3.7, or above
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.6, 3.7 或更高版本
- en: Anaconda
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: PyTorch 1.0 or above
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 1.0 或更高版本
- en: OpenAI Gym
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: Creating a Markov chain
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建马尔可夫链
- en: Let's get started by creating a Markov chain, on which the MDP is developed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个马尔可夫链开始，以便于开发 MDP。
- en: 'A Markov chain describes a sequence of events that comply with the **Markov
    property**. It is defined by a set of possible states, *S = {s0, s1, ... , sm}*,
    and a transition matrix, *T(s, s'')*, consisting of the probabilities of state
    *s* transitioning to state s''. With the Markov property, the future state of
    the process, given the present state, is conditionally independent of past states.
    In other words, the state of the process at *t+1* is dependent only on the state
    at *t*. Here, we use a process of study and sleep as an example and create a Markov
    chain based on two states, *s0* (study) and *s1* (sleep). Let''s say we have the
    following transition matrix:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链描述了遵守**马尔可夫性质**的事件序列。它由一组可能的状态 *S = {s0, s1, ... , sm}* 和转移矩阵 *T(s, s')*
    定义，其中包含状态 *s* 转移到状态 *s'* 的概率。根据马尔可夫性质，过程的未来状态，在给定当前状态的情况下，与过去状态是条件独立的。换句话说，过程在
    *t+1* 时刻的状态仅依赖于 *t* 时刻的状态。在这里，我们以学习和睡眠过程为例，基于两个状态 *s0*（学习）和 *s1*（睡眠），创建了一个马尔可夫链。假设我们有以下转移矩阵：
- en: '![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)'
- en: In the next section, we will compute the transition matrix after k steps, and
    the probabilities of being in each state given an initial distribution of states,
    such as *[0.7, 0.3]*, meaning there is a 70% chance that the process starts with
    study and a 30% chance that it starts with sleep.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将计算经过 k 步后的转移矩阵，以及在初始状态分布（如 *[0.7, 0.3]*，表示有 70% 的概率从学习开始，30% 的概率从睡眠开始）下各个状态的概率。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To create a Markov chain for the study - and - sleep process and conduct some
    analysis on it, perform the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要为学习 - 睡眠过程创建一个马尔可夫链，并对其进行一些分析，请执行以下步骤：
- en: 'Import the library and define the transition matrix:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并定义转移矩阵：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Calculate the transition probability after k steps. Here, we use k = `2`, `5`,
    `10`, `15`, and `20` as examples:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算经过 k 步后的转移概率。这里，我们以 k = `2`, `5`, `10`, `15`, 和 `20` 为例：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the initial distribution of two states:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义两个状态的初始分布：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Calculate the state distribution after k = `1`, `2`, `5`, `10`, `15`, and `20`
    steps:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们计算了经过 k = `1`, `2`, `5`, `10`, `15`, 和 `20` 步后的转移概率，结果如下：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In *Step 2*, we calculated the transition probability after k steps, which
    is the k^(th) power of the transition matrix. You will see the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们计算了经过 k 步后的转移概率，即转移矩阵的k次幂。结果如下：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that, after 10 to 15 steps, the transition probability converges.
    This means that, no matter what state the process is in, it has the same probability
    of transitioning to s0 (57.14%) and s1 (42.86%).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过10到15步，过渡概率会收敛。这意味着无论过程处于什么状态，转移到s0（57.14%）和s1（42.86%）的概率都相同。
- en: 'In *Step 4*, we calculated the state distribution after k = `1`, `2`, `5`,
    `10`, `15`, and `20` steps, which is the multiplication of the initial state distribution
    and the transition probability. You can see the results here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，我们计算了k = `1`，`2`，`5`，`10`，`15`和`20`步后的状态分布，这是初始状态分布和过渡概率的乘积。您可以在这里看到结果：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that, after 10 steps, the state distribution converges. The probability
    of being in s0 (57.14%) and the probability of being in s1 (42.86%) remain unchanged
    in the long run.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过10步后，状态分布会收敛。长期内处于s0（57.14%）和s1（42.86%）的概率保持不变。
- en: 'Starting with [0.7, 0.3], the state distribution after one iteration becomes
    [0.52, 0.48]. Details of its calculation are illustrated in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从[0.7, 0.3]开始，经过一次迭代后的状态分布变为[0.52, 0.48]。其详细计算过程如下图所示：
- en: '![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)'
- en: 'After another iteration, the state distribution becomes [0.592, 0.408] as calculated
    in the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 经过另一次迭代，状态分布如下[0.592, 0.408]，如下图所示计算：
- en: '![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)'
- en: As time progresses, the state distribution reaches equilibrium.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，状态分布达到平衡。
- en: There's more...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In fact, irrespective of the initial state the process was in, the state distribution
    will always converge to [0.5714, 0.4286]. You could test with other initial distributions,
    such as [0.2, 0.8] and [1, 0]. The distribution will remain [0.5714, 0.4286] after
    10 steps.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，无论初始状态如何，状态分布都将始终收敛到[0.5714, 0.4286]。您可以尝试其他初始分布，例如[0.2, 0.8]和[1, 0]。分布在经过10步后仍将保持为[0.5714,
    0.4286]。
- en: A Markov chain does not necessarily converge, especially when it contains transient
    or current states. But if it does converge, it will reach the same equilibrium
    regardless of the starting distribution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链不一定会收敛，特别是当包含瞬态或当前状态时。但如果它确实收敛，无论起始分布如何，它将达到相同的平衡。
- en: See also
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you want to read more about Markov chains, the following are globally two
    great blog articles with nice visualizations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想阅读更多关于马尔可夫链的内容，以下是两篇具有良好可视化效果的博客文章：
- en: '[https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)'
- en: '[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)'
- en: Creating an MDP
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建MDP
- en: Developed upon the Markov chain, an MDP involves an agent and a decision-making
    process. Let's go ahead with developing an MDP and calculating the value function
    under the optimal policy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于马尔可夫链的发展，MDP涉及代理和决策过程。让我们继续发展一个MDP，并计算最优策略下的值函数。
- en: Besides a set of possible states, *S = {s0, s1, ... , sm}*, an MDP is defined
    by a set of actions, *A = {a0, a1, ... , an}*; a transition model, *T(s, a, s')*;
    a reward function, *R(s)*; and a discount factor, 𝝲. The transition matrix, *T(s,
    a, s')*, contains the probabilities of taking action a from state s then landing
    in s'. The discount factor, 𝝲, controls the tradeoff between future rewards and
    immediate ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一组可能的状态，*S = {s0, s1, ... , sm}*，MDP由一组动作，*A = {a0, a1, ... , an}*；过渡模型，*T(s,
    a, s')*；奖励函数，*R(s)*；和折现因子𝝲定义。过渡矩阵，*T(s, a, s')*，包含从状态s采取动作a然后转移到s'的概率。折现因子𝝲控制未来奖励和即时奖励之间的权衡。
- en: 'To make our MDP slightly more complicated, we extend the study and sleep process
    with one more state, `s2 play` games. Let''s say we have two actions, `a0 work`
    and `a1 slack`. The *3 * 2 * 3* transition matrix *T(s, a, s'')* is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的MDP稍微复杂化，我们将学习和睡眠过程延伸到另一个状态，`s2 play` 游戏。假设我们有两个动作，`a0 work` 和 `a1 slack`。*3
    * 2 * 3* 过渡矩阵 *T(s, a, s')* 如下所示：
- en: '![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)'
- en: This means, for example, that when taking the a1 slack action from state s0
    study, there is a 60% chance that it will become s1 sleep (maybe getting tired
    ) and a 30% chance that it will become s2 play games (maybe wanting to relax ),
    and that there is a 10% chance of keeping on studying (maybe a true workaholic
    ). We define the reward function as [+1, 0, -1] for three states, to compensate
    for the hard work. Obviously, the **optimal policy**, in this case, is choosing
    a0 work for each step (keep on studying – no pain no gain, right?). Also, we choose
    0.5 as the discount factor, to begin with. In the next section, we will compute
    the **state-value function** (also called the **value function**, just the **value**
    for short, or **expected utility**) under the optimal policy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，例如，当从状态s0 study中采取a1 slack行动时，有60%的机会它将变成s1 sleep（可能会累），有30%的机会它将变成s2 play
    games（可能想放松），还有10%的机会继续学习（可能是真正的工作狂）。我们为三个状态定义奖励函数为[+1, 0, -1]，以补偿辛勤工作。显然，在这种情况下，**最优策略**是在每个步骤选择a0工作（继续学习——不努力就没有收获，对吧？）。此外，我们选择0.5作为起始折扣因子。在下一节中，我们将计算**状态值函数**（也称为**值函数**，简称**值**或**期望效用**）在最优策略下的值。
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Creating an MDP can be done via the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建MDP可以通过以下步骤完成：
- en: 'Import PyTorch and define the transition matrix:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch并定义转移矩阵：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the reward function and the discount factor:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义奖励函数和折扣因子：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The optimal policy in this case is selecting action `a0` in all circumstances:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，最优策略是在所有情况下选择动作`a0`：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We calculate the value, `V`, of the optimal policy using the **matrix inversion**
    method in the following function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**矩阵求逆**方法计算了最优策略的值`V`：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will demonstrate how to derive the value in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中展示如何推导下一个部分的值。
- en: 'We feed all variables we have to the function, including the transition probabilities
    associated with action `a0`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有变量输入函数中，包括与动作`a0`相关的转移概率：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this oversimplified study-sleep-game process, the optimal policy, that is,
    the policy that achieves the highest total reward, is choosing action a0 in all
    steps. However, it won't be that straightforward in most cases. Also, the actions
    taken in individual steps won't necessarily be the same. They are usually dependent
    on states. So, we will have to solve an MDP by finding the optimal policy in real-world
    cases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过于简化的学习-睡眠-游戏过程中，最优策略，即获得最高总奖励的策略，是在所有步骤中选择动作a0。然而，在大多数情况下，情况不会那么简单。此外，个别步骤中采取的行动不一定相同。它们通常依赖于状态。因此，在实际情况中，我们将不得不通过找到最优策略来解决一个MDP问题。
- en: The value function of a policy measures how good it is for an agent to be in
    each state, given the policy being followed. The greater the value, the better
    the state.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的值函数衡量了在遵循策略的情况下，对于一个agent而言处于每个状态的好处。值越大，状态越好。
- en: 'In *Step 4*, we calculated the value, `V`, of the optimal policy using **matrix
    inversion**. According to the **Bellman Equation**, the relationship between the
    value at step *t+1* and that at step *t* can be expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4步*中，我们使用**矩阵求逆法**计算了最优策略的值`V`。根据**贝尔曼方程**，步骤*t+1*的值与步骤*t*的值之间的关系可以表达如下：
- en: '![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)'
- en: 'When the value converges, which means *Vt+1 = Vt*, we can derive the value,
    `V`, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当值收敛时，也就是*Vt+1 = Vt*时，我们可以推导出值`V`，如下所示：
- en: '![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)'
- en: Here, *I* is the identity matrix with 1s on the main diagonal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*I*是具有主对角线上的1的单位矩阵。
- en: One advantage of solving an MDP with matrix inversion is that you always get
    an exact answer. But the downside is its scalability. As we need to compute the
    inversion of an m * m matrix (where *m* is the number of possible states), the
    computation will become costly if there is a large number of states.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵求逆解决MDP的一个优点是你总是得到一个确切的答案。但其可扩展性有限。因为我们需要计算一个m * m矩阵的求逆（其中*m*是可能的状态数量），如果有大量状态，计算成本会变得很高昂。
- en: There's more...
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We decide to experiment with different values for the discount factor. Let''s
    start with 0, which means we only care about the immediate reward:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定尝试不同的折扣因子值。让我们从0开始，这意味着我们只关心即时奖励：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is consistent with the reward function as we only look at the reward received
    in the next move.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这与奖励函数一致，因为我们只看下一步的奖励。
- en: 'As the discount factor increases toward 1, future rewards are considered. Let''s
    take a look at 𝝲=0.99:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着折现因子向 1 靠拢，未来的奖励被考虑。让我们看看 𝝲=0.99：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: See also
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: This cheatsheet, [https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html),
    serves as a quick reference for MDPs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个速查表，[https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html)，作为马尔可夫决策过程的快速参考。
- en: Performing policy evaluation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行策略评估
- en: We have just developed an MDP and computed the value function of the optimal
    policy using matrix inversion. We also mentioned the limitation of inverting an
    m * m matrix with a large m value (let's say 1,000, 10,000, or 100,000). In this
    recipe, we will talk about a simpler approach called **policy evaluation**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开发了一个马尔可夫决策过程，并使用矩阵求逆计算了最优策略的值函数。我们还提到了通过求逆大型 m * m 矩阵（例如 1,000、10,000 或
    100,000）的限制。在这个方案中，我们将讨论一个更简单的方法，称为**策略评估**。
- en: 'Policy evaluation is an iterative algorithm. It starts with arbitrary policy
    values and then iteratively updates the values based on the **Bellman expectation
    equation** until they converge. In each iteration, the value of a policy, *π*,
    for a state, *s*, is updated as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估是一个迭代算法。它从任意的策略值开始，然后根据**贝尔曼期望方程**迭代更新值，直到收敛。在每次迭代中，状态 *s* 下策略 *π* 的值更新如下：
- en: '![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)'
- en: Here, *π(s, a)* denotes the probability of taking action *a* in state *s* under
    policy *π*. *T(s, a, s')* is the transition probability from state *s* to state
    *s'* by taking action *a*, and *R(s, a)* is the reward received in state *s* by
    taking action *a*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*π(s, a)* 表示在策略 *π* 下在状态 *s* 中采取动作 *a* 的概率。*T(s, a, s')* 是通过采取动作 *a* 从状态
    *s* 转移到状态 *s'* 的转移概率，*R(s, a)* 是在状态 *s* 中采取动作 *a* 后获得的奖励。
- en: There are two ways to terminate an iterative updating process. One is by setting
    a fixed number of iterations, such as 1,000 and 10,000, which might be difficult
    to control sometimes. Another one involves specifying a threshold (usually 0.0001,
    0.00001, or something similar) and terminating the process only if the values
    of all states change to an extent that is lower than the threshold specified.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法来终止迭代更新过程。一种是设置一个固定的迭代次数，比如 1,000 和 10,000，有时可能难以控制。另一种是指定一个阈值（通常是 0.0001、0.00001
    或类似的值），仅在所有状态的值变化程度低于指定的阈值时终止过程。
- en: In the next section, we will perform policy evaluation on the study-sleep-game
    process under the optimal policy and a random policy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将根据最优策略和随机策略对学习-睡眠-游戏过程执行策略评估。
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s develop a policy evaluation algorithm and apply it to our study-sleep-game
    process as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开发一个策略评估算法，并将其应用于我们的学习-睡眠-游戏过程如下：
- en: 'Import PyTorch and define the transition matrix:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 并定义过渡矩阵：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the reward function and the discount factor (let''s use `0.5` for now):'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义奖励函数和折现因子（现在使用 `0.5`）：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the threshold used to determine when to stop the evaluation process:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于确定何时停止评估过程的阈值：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the optimal policy where action a0 is chosen under all circumstances:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义最优策略，其中在所有情况下选择动作 a0：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Develop a policy evaluation function that takes in a policy, transition matrix,
    rewards, discount factor, and a threshold and computes the `value` function:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个策略评估函数，接受一个策略、过渡矩阵、奖励、折现因子和阈值，并计算 `value` 函数：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s plug in the optimal policy and all other variables:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们插入最优策略和所有其他变量：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is almost the same as what we got using matrix inversion.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们使用矩阵求逆得到的结果几乎相同。
- en: 'We now experiment with another policy, a random policy where actions are picked
    with the same probabilities:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在尝试另一个策略，一个随机策略，其中动作以相同的概率选择：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Plug in the random policy and all other variables:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入随机策略和所有其他变量：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We have just seen how effective it is to compute the value of a policy using
    policy evaluation. It is a simple convergent iterative approach, in the **dynamic
    programming family**, or to be more specific, **approximate dynamic programming**.
    It starts with random guesses as to the values and then iteratively updates them
    according to the Bellman expectation equation until they converge.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了使用策略评估计算策略值的效果有多么有效。这是一种简单的收敛迭代方法，在**动态规划家族**中，或者更具体地说是**近似动态规划**。它从对值的随机猜测开始，然后根据贝尔曼期望方程迭代更新，直到它们收敛。
- en: 'In Step 5, the policy evaluation function does the following tasks:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5步中，策略评估函数执行以下任务：
- en: Initializes the policy values as all zeros.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将策略值初始化为全零。
- en: Updates the values based on the Bellman expectation equation.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据贝尔曼期望方程更新值。
- en: Computes the maximal change of the values across all states.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有状态中值的最大变化。
- en: If the maximal change is greater than the threshold, it keeps updating the values.
    Otherwise, it terminates the evaluation process and returns the latest values.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最大变化大于阈值，则继续更新值。否则，终止评估过程并返回最新的值。
- en: Since policy evaluation uses iterative approximation, its result might not be
    exactly the same as the result of the matrix inversion method, which uses exact
    computation. In fact, we don't really need the value function to be that precise.
    Also, it can solve the **curses** **of dimensionality** problem, which can result
    in scaling up the computation to thousands of millions of states. Therefore, we
    usually prefer policy evaluation over the other.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于策略评估使用迭代逼近，其结果可能与使用精确计算的矩阵求逆方法的结果不完全相同。事实上，我们并不真的需要价值函数那么精确。此外，它可以解决**维度诅咒**问题，这可能导致计算扩展到数以千计的状态。因此，我们通常更喜欢策略评估而不是其他方法。
- en: One more thing to remember is that policy evaluation is used to **predict**
    how great a we will get from a given policy; it is not used for **control** problems.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事要记住，策略评估用于**预测**给定策略的预期回报有多大；它不用于**控制**问题。
- en: There's more...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: To take a closer look, we also plot the policy values over the whole evaluation
    process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地观察，我们还会绘制整个评估过程中的策略值。
- en: 'We first need to record the value for each iteration in the `policy_evaluation`
    function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `policy_evaluation` 函数中，我们首先需要记录每次迭代的值：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we feed the `policy_evaluation_history` function with the optimal policy,
    a discount factor of `0.5`, and other variables:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将 `policy_evaluation_history` 函数应用于最优策略，折现因子为 `0.5`，以及其他变量：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then plot the resulting history of values using the following lines of code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码绘制了值的历史结果：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We see the following result:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了以下结果：
- en: '![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)'
- en: It is interesting to see the stabilization between iterations 10 to 14 during
    the convergence.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在收敛期间，从第 10 到第 14次迭代之间的稳定性是非常有趣的。
- en: 'Next, we run the same code but with two different discount factors, 0.2 and
    0.99\. We get the following plot with the discount factor at 0.2:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用两个不同的折现因子，0.2 和 0.99，运行相同的代码。我们得到了折现因子为 0.2 时的以下绘图：
- en: '![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)'
- en: Comparing the plot with a discount factor of 0.5 with this one, we can see that
    the smaller the factor, the faster the policy values converge.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将折现因子为 0.5 的绘图与这个进行比较，我们可以看到因子越小，策略值收敛得越快。
- en: 'We also get the following plot with a discount factor of 0.99:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们也得到了折现因子为 0.99 时的以下绘图：
- en: '![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)'
- en: By comparing the plot with a discount factor of 0.5 to the plot with a discount
    factor of 0.99, we can see that the larger the factor, the longer it takes for
    policy values to converge. The discount factor is a tradeoff between rewards now
    and rewards in the future.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将折现因子为 0.5 的绘图与折现因子为 0.99 的绘图进行比较，我们可以看到因子越大，策略值收敛所需的时间越长。折现因子是即时奖励与未来奖励之间的权衡。
- en: Simulating the FrozenLake environment
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟 FrozenLake 环境
- en: The optimal policies for the MDPs we have dealt with so far are pretty intuitive.
    However, it won't be that straightforward in most cases, such as the FrozenLake
    environment. In this recipe, let's play around with the FrozenLake environment
    and get ready for upcoming recipes where we will find its optimal policy.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理过的MDP的最优策略都相当直观。然而，在大多数情况下，如FrozenLake环境，情况并不那么简单。在这个教程中，让我们玩一下FrozenLake环境，并准备好接下来的教程，我们将找到它的最优策略。
- en: FrozenLake is a typical Gym environment with a **discrete** state space. It
    is about moving an agent from the starting location to the goal location in a
    grid world, and at the same time avoiding traps. The grid is either four by four
    ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/))
    or eight by eigh.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLake是一个典型的Gym环境，具有**离散**状态空间。它是关于在网格世界中将代理程序从起始位置移动到目标位置，并同时避开陷阱。网格可以是四乘四
    ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/))
    或者八乘八。
- en: 't ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
    The grid is made up of the following four types of tiles:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: t ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))。网格由以下四种类型的方块组成：
- en: '**S**: The starting location'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S**：代表起始位置'
- en: '**G**: The goal location, which terminates an episode'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G**：代表目标位置，这会终止一个回合'
- en: '**F**: The frozen tile, which is a walkable location'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**：代表冰面方块，可以行走的位置'
- en: '**H**: The hole location, which terminates an episode'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H**：代表一个地洞位置，这会终止一个回合'
- en: 'There are four actions, obviously: moving left (0), moving down (1), moving
    right (2), and moving up (3). The reward is +1 if the agent successfully reaches
    the goal location, and 0 otherwise. Also, the observation space is represented
    in a 16-dimensional integer array, and there are 4 possible actions (which makes
    sense).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 显然有四种动作：向左移动（0）、向下移动（1）、向右移动（2）和向上移动（3）。如果代理程序成功到达目标位置，奖励为+1，否则为0。此外，观察空间由一个16维整数数组表示，有4种可能的动作（这是有道理的）。
- en: What is tricky in this environment is that, as the ice surface is slippery,
    the agent won't always move in the direction it intends. For example, it may move
    to the left or to the right when it intends to move down.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境的棘手之处在于冰面很滑，代理程序并不总是按其意图移动。例如，当它打算向下移动时，可能会向左或向右移动。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To run the FrozenLake environment, let''s first search for it in the table
    of environments here: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    The search gives us `FrozenLake-v0`.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行FrozenLake环境，让我们首先在这里的环境表中搜索它：[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)。搜索结果给出了`FrozenLake-v0`。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做……
- en: 'Let''s simulate the four-by-four FrozenLake environment in the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按以下步骤模拟四乘四的FrozenLake环境：
- en: 'We import the `gym` library and create an instance of the FrozenLake environment:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入`gym`库，并创建FrozenLake环境的一个实例：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Reset the environment:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The agent starts with state `0`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代理程序从状态`0`开始。
- en: 'Render the environment:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s make a down movement since it is walkable:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们做一个向下的动作，因为这是可行走的：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Print out all the returning information to confirm that the agent lands in
    state `4` with a probability of 33.33%:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出所有返回的信息，确认代理程序以33.33%的概率落在状态`4`：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You get `0` as a reward, since it has not reached the goal and the episode is
    not done yet. Again, you might see the agent landing in state 1, or staying in
    state 0 because of the slippery surface.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到了`0`作为奖励，因为它尚未到达目标，并且回合尚未结束。再次看到代理程序可能会陷入状态1，或者因为表面太滑而停留在状态0。
- en: 'To demonstrate how difficult it is to walk on the frozen lake, implement a
    random policy and calculate the average total reward over 1,000 episodes. First,
    define a function that simulates a FrozenLake episode given a policy and returns
    the total reward (we know it is either 0 or 1):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了展示在冰面上行走有多困难，实现一个随机策略并计算1,000个回合的平均总奖励。首先，定义一个函数，该函数根据给定的策略模拟一个FrozenLake回合并返回总奖励（我们知道这要么是0，要么是1）：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now run `1000` episodes, and a policy will be randomly generated and will be
    used in each episode:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行`1000`个回合，并且在每个回合中都会随机生成并使用一个策略：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This basically means there is only a 1.4% chance on average that the agent can
    reach the goal if we randomize the actions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着，如果我们随机执行动作，平均只有1.4%的机会代理程序能够到达目标位置。
- en: 'Next, we experiment with a random search policy. In the training phase, we
    randomly generate a bunch of policies and record the first one that reaches the
    goal:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用随机搜索策略进行实验。在训练阶段，我们随机生成一堆策略，并记录第一个达到目标的策略：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Take a look at the best policy:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳策略：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now run 1,000 episodes with the policy we just cherry-picked:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行 1,000 个回合，使用我们刚挑选出的策略：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Using the random search algorithm, the goal will be reached 20.8% of the time
    on average.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机搜索算法，平均情况下会有 20.8% 的概率达到目标。
- en: Note that this result could vary a lot, as the policy we picked might happen
    to reach the goal because of the slippery ice and might not be the optimal one.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们选择的策略可能由于冰面滑动而达到目标，这可能会导致结果变化很大，可能不是最优策略。
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: In this recipe, we randomly generated a policy that was composed of 16 actions
    for the 16 states. Keep in mind that in FrozenLake, the movement direction is
    only partially dependent on the chosen action. This increases the uncertainty
    of control.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们随机生成了一个由 16 个动作组成的策略，对应 16 个状态。请记住，在 FrozenLake 中，移动方向仅部分依赖于选择的动作，这增加了控制的不确定性。
- en: 'After running the code in *Step 4*, you will see a 4 * 4 matrix as follows,
    representing the frozen lake and the tile (state 0) where the agent stands:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 *Step 4* 中的代码后，你将看到一个 4 * 4 的矩阵，代表冰湖和代理站立的瓷砖（状态 0）：
- en: '![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)'
- en: 'After running the lines of code in *Step 5*, you will see the resulting grid
    as follows, where the agent moves down to state 4:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 *Step 5* 中的代码行后，你将看到如下结果网格，代理向下移动到状态 4：
- en: '![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)'
- en: 'An episode will terminate if either of the following two conditions is met:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下两个条件之一，一个回合将终止：
- en: Moving to an H tile (state 5, 7, 11, 12 ). This will generate a total reward
    of 0.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动到 H 格（状态 5、7、11、12）。这将生成总奖励 0。
- en: Moving to the G tile (state 15). This will generate a total reward of +1.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动到 G 格（状态 15）。这将产生总奖励 +1。
- en: There's more...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'We can look into the details of the FrozenLake environment, including the transformation
    matrix and rewards for each state and action, by using the P attribute. For example,
    for state 6, we can do the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 P 属性查看 FrozenLake 环境的详细信息，包括转移矩阵和每个状态及动作的奖励。例如，对于状态 6，我们可以执行以下操作：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This returns a dictionary with keys 0, 1, 2, and 3, representing four possible
    actions. The value is a list of movements after taking an action. The movement
    list is in the following format: (transformation probability, new state, reward
    received, is done). For instance, if the agent resides in state 6 and intends
    to take action 1 (down), there is a 33.33% chance that it will land in state 5,
    receiving a reward of 0 and terminating the episode; there is a 33.33% chance
    that it will land in state 10 and receive a reward of 0; and there is a 33.33%
    chance that it will land in state 7, receiving a reward of 0 and terminating the
    episode.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回一个字典，其键为 0、1、2 和 3，分别代表四种可能的动作。值是一个列表，包含在执行动作后的移动。移动列表的格式如下：（转移概率，新状态，获得的奖励，是否结束）。例如，如果代理处于状态
    6 并打算执行动作 1（向下），有 33.33% 的概率它会进入状态 5，获得奖励 0 并终止该回合；有 33.33% 的概率它会进入状态 10，获得奖励
    0；有 33.33% 的概率它会进入状态 7，获得奖励 0 并终止该回合。
- en: 'For state 11, we can do the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于状态 11，我们可以执行以下操作：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As stepping on a hole will terminate an episode, it won’t make any movement
    afterward.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于踩到洞会终止一个回合，所以不会再有任何移动。
- en: Feel free to check out the other states.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 随意查看其他状态。
- en: Solving an MDP with a value iteration algorithm
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用值迭代算法解决 MDP
- en: An MDP is considered solved if its optimal policy is found. In this recipe,
    we will figure out the optimal policy for the FrozenLake environment using a **value
    iteration** algorithm.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到其最优策略，则认为 MDP 已解决。在这个示例中，我们将使用 **值迭代** 算法找出 FrozenLake 环境的最优策略。
- en: 'The idea behind value iteration is quite similar to that of policy evaluation.
    It is also an iterative algorithm. It starts with arbitrary policy values and
    then iteratively updates the values based on the **Bellman optimality equation**
    until they converge. So in each iteration, instead of taking the expectation (average)
    of values across all actions, it picks the action that achieves the maximal policy
    values:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代的思想与策略评估非常相似。它也是一种迭代算法。它从任意策略值开始，然后根据贝尔曼最优方程迭代更新值，直到收敛。因此，在每次迭代中，它不是采用跨所有动作的值的期望（平均值），而是选择实现最大策略值的动作：
- en: '![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)'
- en: Here, V*(s) denotes the optimal value, which is the value of the optimal policy;
    T(s, a, s') is the transition probability from state s to state s’ by taking action
    a; and R(s, a) is the reward received in state s by taking action a.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，V*(s)表示最优值，即最优策略的值；T(s, a, s')是采取动作a从状态s转移到状态s’的转移概率；而R(s, a)是采取动作a时在状态s中收到的奖励。
- en: 'Once the optimal values are computed, we can easily obtain the optimal policy
    accordingly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出最优值后，我们可以相应地获得最优策略：
- en: '![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)'
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s solve the FrozenLake environment using a value iteration algorithm as
    follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用值迭代算法解决FrozenLake环境如下：
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库并创建FrozenLake环境的实例：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将折扣因子设为`0.99`，收敛阈值设为`0.0001`。
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now define the function that computes optimal values based on the value iteration
    algorithm:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在定义一个函数，根据值迭代算法计算最优值：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Plug in the environment, discount factor, and convergence threshold, then print
    the optimal values:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入环境、折扣因子和收敛阈值，然后打印最优值：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now that we have the optimal values, we develop the function that extracts
    the optimal policy out of them:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了最优值，我们开发提取最优策略的函数：
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Plug in the environment, discount factor, and optimal values, then print the
    optimal policy:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入环境、折扣因子和最优值，然后打印最优策略：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We want to gauge how good the optimal policy is. So, let''s run 1,000 episodes
    with the optimal policy and check the average reward. Here, we will reuse the
    `run_episode` function we defined in the previous recipe:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要评估最优策略的好坏程度。因此，让我们使用最优策略运行1,000次情节，并检查平均奖励。在这里，我们将重复使用我们在前面的配方中定义的`run_episode`函数：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Under the optimal policy, the agent will reach the goal 75% of the time, on
    average. This is the best we are able to get since the ice is slippery.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在最优策略下，代理将平均75%的时间到达目标。这是我们能够做到的最好结果，因为冰很滑。
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: In a value iteration algorithm, we get the optimal value function by iteratively
    applying the Bellman optimality equation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代算法中，我们通过迭代应用贝尔曼最优方程来获得最优值函数。
- en: 'The following is another version of the Bellman optimality equation, which
    can deal with environments where rewards are partially dependent on the new state:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是贝尔曼最优方程的另一版本，适用于奖励部分依赖于新状态的环境：
- en: '![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)'
- en: 'Here, R(s, a, s'') is the reward received as a result of moving to state s''
    from state s by taking action a. As this version is more compatible, we developed
    our `value_iteration` function according to it. As you saw in *Step 3*, we perform
    the following tasks:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，R(s, a, s')表示通过采取动作a从状态s移动到状态s'而收到的奖励。由于这个版本更兼容，我们根据它开发了我们的`value_iteration`函数。正如您在*Step
    3*中看到的，我们执行以下任务：
- en: Initialize the policy values as all zeros.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将策略值初始化为全部为零。
- en: Update the values based on the Bellman optimality equation.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据贝尔曼最优方程更新值。
- en: Compute the maximal change of the values across all states.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有状态的值的最大变化。
- en: If the maximal change is greater than the threshold, we keep updating the values.
    Otherwise, we terminate the evaluation process and return the latest values as
    the optimal values.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最大变化大于阈值，则继续更新值。否则，终止评估过程，并返回最新的值作为最优值。
- en: There's more...
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We obtained a success rate of 75% with a discount factor of 0.99\. How does
    the discount factor affect the performance? Let''s do some experiments with different
    factors, including `0`, `0.2`, `0.4`, `0.6`, `0.8`, `0.99`, and `1.`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在折扣因子为0.99时获得了75%的成功率。折扣因子如何影响性能？让我们用不同的因子进行一些实验，包括`0`、`0.2`、`0.4`、`0.6`、`0.8`、`0.99`和`1.`：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'For each discount factor, we compute the average success rate over 10,000 episodes:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个折扣因子，我们计算了10,000个周期的平均成功率：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We draw a plot of the average success rate versus the discount factor:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了平均成功率与折扣因子的图表：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We get the following plot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下的绘图：
- en: '![](img/650d388b-391a-4be4-8537-d49d31b08315.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/650d388b-391a-4be4-8537-d49d31b08315.png)'
- en: The result shows that the performance improves when there is an increase in
    the discount factor. This verifies the fact that a small discount factor values
    the reward now and a large discount factor values a better reward in the future.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，当折扣因子增加时，性能有所提升。这证实了一个小的折扣因子目前价值奖励，而一个大的折扣因子则更看重未来的更好奖励。
- en: Solving an MDP with a policy iteration algorithm
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略迭代算法解决MDP
- en: Another approach to solving an MDP is by using a **policy iteration** algorithm,
    which we will discuss in this recipe.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 解决MDP的另一种方法是使用**策略迭代**算法，我们将在本配方中讨论它。
- en: 'A policy iteration algorithm can be subdivided into two components: policy
    evaluation and policy improvement. It starts with an arbitrary policy. And in
    each iteration, it first computes the policy values given the latest policy, based
    on the Bellman expectation equation; it then extracts an improved policy out of
    the resulting policy values, based on the Bellman optimality equation. It iteratively
    evaluates the policy and generates an improved version until the policy doesn''t
    change any more.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代算法可以分为两个部分：策略评估和策略改进。它从任意策略开始。每次迭代中，它首先根据贝尔曼期望方程计算给定最新策略的策略值；然后根据贝尔曼最优性方程从结果策略值中提取一个改进的策略。它反复评估策略并生成改进版本，直到策略不再改变为止。
- en: Let's develop a policy iteration algorithm and use it to solve the FrozenLake
    environment. After that, we will explain how it works.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开发一个策略迭代算法，并使用它来解决FrozenLake环境。之后，我们将解释它的工作原理。
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s solve the FrozenLake environment using a policy iteration algorithm
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用策略迭代算法解决FrozenLake环境：
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的库并创建FrozenLake环境的实例：
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，暂将折扣因子设定为`0.99`，收敛阈值设定为`0.0001`：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now we define the `policy_evaluation` function that computes the values given
    a policy:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义`policy_evaluation`函数，它计算给定策略的值：
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is similar to what we did in the *Performing policy evaluation* recipe,
    but with the Gym environment as an input.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在*执行策略评估*配方中所做的类似，但输入是Gym环境。
- en: 'Next, we develop the second main component of the policy iteration algorithm,
    the policy improvement part:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开发策略迭代算法的第二个主要组成部分，即策略改进部分：
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This extracts an improved policy from the given policy values, based on the
    Bellman optimality equation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这根据贝尔曼最优性方程从给定的策略值中提取了一个改进的策略。
- en: 'Now that we have both components ready, we develop the policy iteration algorithm
    as follows:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们两个组件都准备好了，我们按以下方式开发策略迭代算法：
- en: '[PRE50]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Plug in the environment, discount factor, and convergence threshold:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入环境、折扣因子和收敛阈值：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We''ve obtained the optimal values and optimal policy. Let''s take a look at
    them:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经获得了最优值和最优策略。让我们看一看它们：
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: They are exactly the same as what we got using the value iteration algorithm.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这与使用值迭代算法得到的结果完全一样。
- en: How it works...
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Policy iteration combines policy evaluation and policy improvement in each
    iteration. In policy evaluation, the values for a given policy (not the optimal
    one) are calculated based on the Bellman expectation equation until they converge:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代结合了每次迭代中的策略评估和策略改进。在策略评估中，根据贝尔曼期望方程计算给定策略（而非最优策略）的值，直到它们收敛：
- en: '![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)'
- en: Here, a = π(s), which is the action taken under policy π in state s.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，a = π(s)，即在状态s下根据策略π采取的动作。
- en: 'In policy improvement, the policy is updated using the resulting converged
    policy values, V(s), based on the Bellman optimality equation:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略改进中，根据贝尔曼最优性方程使用收敛的策略值V(s)更新策略：
- en: '![](img/f60962b4-570b-4954-8da6-79733626a594.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f60962b4-570b-4954-8da6-79733626a594.png)'
- en: 'This repeats the policy evaluation and policy improvement steps until the policy
    converges. At convergence, the latest policy and its value function are the optimal
    policy and the optimal value function. Hence, in Step 5, the `policy_iteration`
    function does the following tasks:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这重复策略评估和策略改进步骤，直到策略收敛。在收敛时，最新的策略和其值函数是最优策略和最优值函数。因此，在第5步，`policy_iteration`函数执行以下任务：
- en: Initializes a random policy.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个随机策略。
- en: Computes the values of the policy with the policy evaluation algorithm.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略评估算法计算策略的值。
- en: Obtains an improved policy based on the policy values.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略值获取改进的策略。
- en: If the new policy is different from the old one, it updates the policy and runs
    another iteration. Otherwise, it terminates the iteration process and returns
    the policy values and the policy.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果新策略与旧策略不同，则更新策略并运行另一次迭代。否则，终止迭代过程并返回策略值和策略。
- en: There's more...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We have just solved the FrozenLake environment with a policy iteration algorithm.
    So, you may wonder when it is better to use policy iteration over value iteration
    and vice versa. There are basically three scenarios where one has the edge over
    the other:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚用策略迭代算法解决了FrozenLake环境。因此，您可能想知道何时最好使用策略迭代而不是值迭代，反之亦然。基本上有三种情况其中一种比另一种更占优势：
- en: If there is a large number of actions, use policy iteration, as it can converge
    faster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有大量的动作，请使用策略迭代，因为它可以更快地收敛。
- en: If there is a small number of actions, use value iteration.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动作数量较少，请使用值迭代。
- en: If there is already a viable policy (obtained either by intuition or domain
    knowledge), use policy iteration.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果已经有一个可行的策略（通过直觉或领域知识获得），请使用策略迭代。
- en: Outside those scenarios, policy iteration and value iteration are generally
    comparable.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况之外，策略迭代和值迭代通常是可比较的。
- en: In the next recipe, we will apply each algorithm to solve the coin-flipping-gamble
    problem. We will see which algorithm converges faster.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个案例中，我们将应用每种算法来解决硬币抛掷赌博问题。我们将看到哪种算法收敛得更快。
- en: See also
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Feel free to use what we've learned in these two recipes to solve a bigger ice
    grid, the `FrozenLake8x8-v0` environment ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意使用我们在这两个案例中学到的知识来解决一个更大的冰格，即 `FrozenLake8x8-v0` 环境 ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))。
- en: Solving the coin-flipping gamble problem
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决硬币抛掷赌博问题
- en: Gambling on coin flipping should sound familiar to everyone. In each round of
    the game, the gambler can make a bet on whether a coin flip will show heads. If
    it turns out heads, the gambler will win the same amount they bet; otherwise,
    they will lose this amount. The game continues until the gambler loses (ends up
    with nothing) or wins (wins more than 100 dollars, let's say). Let's say the coin
    is unfair and it lands on heads 40% of the time. In order to maximize the chance
    of winning, how much should the gambler bet based on their current capital in
    each round? This will definitely be an interesting problem to solve.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 对硬币抛掷赌博应该对每个人都很熟悉。在游戏的每一轮中，赌徒可以打赌硬币是否会正面朝上。如果结果是正面，赌徒将赢得他们下注的相同金额；否则，他们将失去这笔金额。游戏将继续，直到赌徒输掉（最终一无所有）或赢得（赢得超过100美元，假设）。假设硬币不公平，并且有40%的概率正面朝上。为了最大化赢的机会，赌徒应该根据当前资本在每一轮下注多少？这绝对是一个有趣的问题要解决。
- en: If the coin lands on heads more than 50% of the time, there is nothing to discuss.
    The gambler can just keep betting one dollar each round and should win the game
    most of the time. If it is a fair coin, the gambler could bet one dollar each
    round and end up winning around 50% of the time. It gets tricky when the probability
    of heads is lower than 50%; the safe-bet strategy wouldn't work anymore. Nor would
    a random strategy, either. We need to resort to the reinforcement learning techniques
    we've learned in this chapter to make smart bets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果硬币正面朝上的概率超过 50%，就没什么好讨论的。赌徒可以每轮下注一美元，并且大多数情况下应该能赢得游戏。如果是公平硬币，赌徒每轮下注一美元时，大约一半的时间会赢。当正面朝上的概率低于
    50% 时，保守的策略就行不通了。随机策略也不行。我们需要依靠本章学到的强化学习技术来做出明智的投注。
- en: 'Let''s get started by formulating the coin-flipping gamble problem as an MDP.
    It is basically an undiscounted, episodic, and finite MDP with the following properties:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始将抛硬币赌博问题制定为马尔可夫决策过程（MDP）。它基本上是一个无折扣、周期性的有限 MDP，具有以下特性：
- en: 'The state is the gambler''s capital in dollars. There are 101 states: 0, 1,
    2, …, 98, 99, and 100+.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态是赌徒的美元资本。总共有 101 个状态：0、1、2、…、98、99 和 100+。
- en: The reward is 1 if the state 100+ is reached; otherwise, the reward is 0.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果达到状态 100+，则奖励为 1；否则，奖励为 0。
- en: The action is the possible amount the gambler bets in a round. Given state s,
    the possible actions include 1, 2, …, and min(s, 100 - s). For example, when the
    gambler has 60 dollars, they can bet any amount from 1 to 40\. Any amount above
    40 doesn't make any sense as it increases the loss and doesn't increase the chance
    of winning.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动是赌徒在一轮中可能下注的金额。对于状态 s，可能的行动包括 1、2、…，以及 min(s, 100 - s)。例如，当赌徒有 60 美元时，他们可以下注从
    1 到 40 的任意金额。超过 40 的任何金额都没有意义，因为它增加了损失并且不增加赢得游戏的机会。
- en: The next state after taking an action depends on the probability of the coin
    coming up heads. Let's say it is 40%. So, the next state of state s after taking
    action *a* will be *s+a* by 40%, *s-a* by 60%.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在采取行动后，下一个状态取决于硬币正面朝上的概率。假设是 40%。因此，在采取行动 *a* 后，状态 s 的下一个状态将以 40% 的概率为 *s+a*，以
    60% 的概率为 *s-a*。
- en: The process terminates at state 0 and state 100+.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过程在状态 0 和状态 100+ 处终止。
- en: How to do it...
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We first solve the coin-flipping gamble problem by using a value iteration
    algorithm and performing the following steps:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用值迭代算法解决抛硬币赌博问题，并执行以下步骤：
- en: 'Import PyTorch:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch：
- en: '[PRE53]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Specify the discount factor and convergence threshold:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定折扣因子和收敛阈值：
- en: '[PRE54]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Here, we set 1 as the discount factor as the MDP is an undiscounted process;
    we set a small threshold as we expect small policy values since all rewards are
    0 except the last state.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将折扣因子设为 1，因为这个 MDP 是一个无折扣的过程；我们设置了一个小阈值，因为我们预期策略值较小，所有奖励都是 0，除了最后一个状态。
- en: Define the following environment variables.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义以下环境变量。
- en: 'There are 101 states in total:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有 101 个状态：
- en: '[PRE55]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The corresponding reward is displayed as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的奖励显示如下：
- en: '[PRE56]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s say the probability of getting heads is 40%:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 假设正面朝上的概率是 40%：
- en: '[PRE57]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Put these variables into a dictionary:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些变量放入字典中：
- en: '[PRE58]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now we develop a function that computes optimal values based on the value iteration
    algorithm:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开发一个函数，根据值迭代算法计算最优值：
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We only need to compute the values for states 1 to 99, as the values for state
    0 and state 100+ are 0\. And given state *s*, the possible actions can be anything
    from 1 up to *min(s, 100 - s)*. We should keep this in mind while computing the
    Bellman optimality equation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需计算状态 1 到 99 的值，因为状态 0 和状态 100+ 的值为 0。而给定状态 *s*，可能的行动可以是从 1 到 *min(s, 100
    - s)*。在计算贝尔曼最优方程时，我们应该牢记这一点。
- en: 'Next, we develop a function that extracts the optimal policy based on the optimal
    values:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开发一个函数，根据最优值提取最优策略：
- en: '[PRE60]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Finally, we can plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and optimal policy after . Also, we time how long
    it takes to solve the gamble MDP with value iteration; we will compare this with
    the time it takes for policy iteration to complete:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将环境、折扣因子和收敛阈值输入，计算出最优值和最优策略。此外，我们还计时了使用值迭代解决赌博 MDP 所需的时间；我们将其与策略迭代完成所需的时间进行比较：
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We solved the gamble problem with value iteration in `4.717` seconds.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `4.717` 秒内使用值迭代算法解决了赌博问题。
- en: 'Take a look at the optimal policy values and the optimal policy we got:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看我们得到的最优策略值和最优策略：
- en: '[PRE62]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can plot the policy value versus state as follows:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以绘制策略值与状态的图表如下：
- en: '[PRE63]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now that we've solved the gamble problem with value iteration, how about policy
    iteration? Let's see.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过值迭代解决了赌博问题，接下来是策略迭代？我们来看看。
- en: 'We start by developing the `policy_evaluation` function that computes the values
    given a policy:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先开发`policy_evaluation`函数，该函数根据策略计算值：
- en: '[PRE64]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we develop another main component of the policy iteration algorithm,
    the policy improvement part:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开发策略迭代算法的另一个主要组成部分，即策略改进部分：
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'With both components ready, we can develop the main entry to the policy iteration
    algorithm as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这两个组件，我们可以开发策略迭代算法的主要入口如下：
- en: '[PRE66]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and the optimal policy. We record the time spent
    solving the MDP as well:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将环境、折扣因子和收敛阈值插入以计算最优值和最优策略。我们记录解决MDP所花费的时间：
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Check out the optimal values and optimal policy we just obtained:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看刚刚获得的最优值和最优策略：
- en: '[PRE68]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: How it works...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何运作的……
- en: 'After executing the lines of code in *Step 7*, you will see the optimal policy
    values:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行*第 7 步*中的代码行后，您将看到最优策略值：
- en: '[PRE69]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You will also see the optimal policy:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 您还将看到最优策略：
- en: '[PRE70]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '*Step 8* generates the following plot for the optimal policy values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 8 步* 生成了以下最优策略值的图表：'
- en: '![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)'
- en: We can see that, as the capital (state) increases, the estimated reward (policy
    value) also increases, which makes sense.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着资本（状态）的增加，估计的奖励（策略值）也在增加，这是有道理的。
- en: What we did in *Step 9* is very similar to what we did in the *Solving an MDP
    with a policy iteration algorithm* recipe, but for the coin-flipping gamble environment
    this time.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 9 步*中我们所做的事情与*Solving an MDP with a policy iteration algorithm*配方中的所做的非常相似，但这次是针对抛硬币赌博环境。
- en: In *Step 10*, the policy improvement function extracts an improved policy out
    of the given policy values, based on the Bellman optimality equation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 10 步*中，策略改进函数从给定的策略值中提取出改进的策略，基于贝尔曼最优方程。
- en: As you can see in *Step 12*, we solved the gamble problem with policy iteration
    in `2.002` seconds, which is less than half the time it took with value iteration.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*第 12 步*中所看到的，我们通过策略迭代在`2.002`秒内解决了赌博问题，比值迭代所花费的时间少了一半。
- en: 'The results we got from *Step 13* include the following optimal values:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*第 13 步*得到的结果包括以下最优值：
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'They also include the optimal policy:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还包括最优策略：
- en: '[PRE72]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The results from the two approaches, value iteration and policy iteration, are
    consistent.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 来自值迭代和策略迭代的两种方法的结果是一致的。
- en: We have solved the gamble problem by using value iteration and policy iteration.
    To deal with a reinforcement learning problem, one of the trickiest tasks is to
    formulate the process into an MDP. In our case, the policy is transformed from
    the current capital (states) to the new capital (new states) by betting certain
    stakes (actions). The optimal policy maximizes the probability of winning the
    game (+1 reward), and evaluates the probability of winning under the optimal policy.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过值迭代和策略迭代解决了赌博问题。处理强化学习问题中最棘手的任务之一是将过程形式化为MDP。在我们的例子中，通过下注一定的赌注（动作），将当前资本（状态）的策略转化为新资本（新状态）。最优策略最大化了赢得游戏的概率（+1
    奖励），并在最优策略下评估了赢得游戏的概率。
- en: 'Another interesting thing to note is how the transformation probabilities and
    new states are determined in the Bellman equation in our example. Taking action
    a in state s (having capital s and making a bet of 1 dollar) will have two possible
    outcomes:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的事情是注意我们的示例中如何确定贝尔曼方程中的转换概率和新状态。在状态 s 中采取动作 a（拥有资本 s 并下注 1 美元），将有两种可能的结果：
- en: Moving to new state s+a, if the coin lands on heads. Hence, the transformation
    probability is equal to the probability of heads.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果硬币正面朝上，则移动到新状态 s+a。因此，转换概率等于正面朝上的概率。
- en: Moving to new state s-a, if the coin lands on tails. Therefore, the transformation
    probability is equal to the probability of tails.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果硬币反面朝上，则移动到新状态 s-a。因此，转换概率等于反面朝上的概率。
- en: This is quite similar to the FrozenLake environment, where the agent lands on
    the intended tile only by a certain probability.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这与FrozenLake环境非常相似，代理人只有以一定概率着陆在预期的瓦片上。
- en: We also verified that policy iteration converges faster than value iteration
    in this case. This is because there are up to 50 possible actions, which is more
    than the 4 actions in FrozenLake. For MDPs with a large number of actions, solving
    with policy iteration is more efficient than doing so with value iteration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还验证了在这种情况下，策略迭代比值迭代收敛更快。这是因为可能有多达 50 个可能的行动，这比 FrozenLake 中的 4 个行动更多。对于具有大量行动的马尔可夫决策过程，用策略迭代解决比值迭代更有效率。
- en: There's more...
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You may want to know whether the optimal policy really works. Let''s act like
    smart gamblers and play 10,000 episodes of the game. We are going to compare the
    optimal policy with two other strategies: conservative (betting one dollar each
    round) and random (betting a random amount):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道最优策略是否真的有效。让我们像聪明的赌徒一样玩 10,000 个剧集的游戏。我们将比较最优策略与另外两种策略：保守策略（每轮下注一美元）和随机策略（下注随机金额）：
- en: We start by defining the three aforementioned betting strategies.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过定义三种上述的投注策略开始。
- en: 'We define the optimal strategy first:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义最优策略：
- en: '[PRE73]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Then we define the conservative strategy:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义保守策略：
- en: '[PRE74]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, we define the random strategy:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义随机策略：
- en: '[PRE75]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a wrapper function that runs one episode with a strategy and returns
    whether or not the game was won:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包装函数，用一种策略运行一个剧集，并返回游戏是否获胜：
- en: '[PRE76]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Specify a starting capital (let’s say `50` dollars) and a number of episodes
    (`10000`):'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个起始资本（假设是`50`美元）和一定数量的剧集（`10000`）：
- en: '[PRE77]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Run 10,000 episodes and keep track of the winning times:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 10,000 个剧集并跟踪获胜次数：
- en: '[PRE78]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Print out the winning probabilities for the three strategies:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出三种策略的获胜概率：
- en: '[PRE79]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Our optimal policy is clearly the winner!
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最优策略显然是赢家！
