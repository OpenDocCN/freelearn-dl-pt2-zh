- en: Markov Decision Processes and Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will continue our practical reinforcement learning journey
    with PyTorch by looking at **Markov decision processes** (**MDPs**) and dynamic
    programming. This chapter will start with the creation of a Markov chain and an
    MDP, which is the core of most reinforcement learning algorithms. You will also
    become more familiar with Bellman equations by practicing policy evaluation. We
    will then move on and apply two approaches to solving an MDP: value iteration
    and policy iteration. We will use the FrozenLake environment as an example. At
    the end of the chapter, we will demonstrate how to solve the interesting coin-flipping
    gamble problem with dynamic programming step by step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Markov chain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an MDP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing policy evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating the FrozenLake environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving an MDP with a value iteration algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving an MDP with a policy iteration algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the coin-flipping gamble problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following programs installed on your system to successfully
    execute the recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.6, 3.7, or above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch 1.0 or above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Markov chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started by creating a Markov chain, on which the MDP is developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Markov chain describes a sequence of events that comply with the **Markov
    property**. It is defined by a set of possible states, *S = {s0, s1, ... , sm}*,
    and a transition matrix, *T(s, s'')*, consisting of the probabilities of state
    *s* transitioning to state s''. With the Markov property, the future state of
    the process, given the present state, is conditionally independent of past states.
    In other words, the state of the process at *t+1* is dependent only on the state
    at *t*. Here, we use a process of study and sleep as an example and create a Markov
    chain based on two states, *s0* (study) and *s1* (sleep). Let''s say we have the
    following transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will compute the transition matrix after k steps, and
    the probabilities of being in each state given an initial distribution of states,
    such as *[0.7, 0.3]*, meaning there is a 70% chance that the process starts with
    study and a 30% chance that it starts with sleep.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a Markov chain for the study - and - sleep process and conduct some
    analysis on it, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and define the transition matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the transition probability after k steps. Here, we use k = `2`, `5`,
    `10`, `15`, and `20` as examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the initial distribution of two states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the state distribution after k = `1`, `2`, `5`, `10`, `15`, and `20`
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 2*, we calculated the transition probability after k steps, which
    is the k^(th) power of the transition matrix. You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, after 10 to 15 steps, the transition probability converges.
    This means that, no matter what state the process is in, it has the same probability
    of transitioning to s0 (57.14%) and s1 (42.86%).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, we calculated the state distribution after k = `1`, `2`, `5`,
    `10`, `15`, and `20` steps, which is the multiplication of the initial state distribution
    and the transition probability. You can see the results here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, after 10 steps, the state distribution converges. The probability
    of being in s0 (57.14%) and the probability of being in s1 (42.86%) remain unchanged
    in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with [0.7, 0.3], the state distribution after one iteration becomes
    [0.52, 0.48]. Details of its calculation are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After another iteration, the state distribution becomes [0.592, 0.408] as calculated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)'
  prefs: []
  type: TYPE_IMG
- en: As time progresses, the state distribution reaches equilibrium.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In fact, irrespective of the initial state the process was in, the state distribution
    will always converge to [0.5714, 0.4286]. You could test with other initial distributions,
    such as [0.2, 0.8] and [1, 0]. The distribution will remain [0.5714, 0.4286] after
    10 steps.
  prefs: []
  type: TYPE_NORMAL
- en: A Markov chain does not necessarily converge, especially when it contains transient
    or current states. But if it does converge, it will reach the same equilibrium
    regardless of the starting distribution.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to read more about Markov chains, the following are globally two
    great blog articles with nice visualizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an MDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed upon the Markov chain, an MDP involves an agent and a decision-making
    process. Let's go ahead with developing an MDP and calculating the value function
    under the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Besides a set of possible states, *S = {s0, s1, ... , sm}*, an MDP is defined
    by a set of actions, *A = {a0, a1, ... , an}*; a transition model, *T(s, a, s')*;
    a reward function, *R(s)*; and a discount factor, ùù≤. The transition matrix, *T(s,
    a, s')*, contains the probabilities of taking action a from state s then landing
    in s'. The discount factor, ùù≤, controls the tradeoff between future rewards and
    immediate ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our MDP slightly more complicated, we extend the study and sleep process
    with one more state, `s2 play` games. Let''s say we have two actions, `a0 work`
    and `a1 slack`. The *3 * 2 * 3* transition matrix *T(s, a, s'')* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)'
  prefs: []
  type: TYPE_IMG
- en: This means, for example, that when taking the a1 slack action from state s0
    study, there is a 60% chance that it will become s1 sleep (maybe getting tired
    ) and a 30% chance that it will become s2 play games (maybe wanting to relax ),
    and that there is a 10% chance of keeping on studying (maybe a true workaholic
    ). We define the reward function as [+1, 0, -1] for three states, to compensate
    for the hard work. Obviously, the **optimal policy**, in this case, is choosing
    a0 work for each step (keep on studying ‚Äì no pain no gain, right?). Also, we choose
    0.5 as the discount factor, to begin with. In the next section, we will compute
    the **state-value function** (also called the **value function**, just the **value**
    for short, or **expected utility**) under the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating an MDP can be done via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and define the transition matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the reward function and the discount factor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal policy in this case is selecting action `a0` in all circumstances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the value, `V`, of the optimal policy using the **matrix inversion**
    method in the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We will demonstrate how to derive the value in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We feed all variables we have to the function, including the transition probabilities
    associated with action `a0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this oversimplified study-sleep-game process, the optimal policy, that is,
    the policy that achieves the highest total reward, is choosing action a0 in all
    steps. However, it won't be that straightforward in most cases. Also, the actions
    taken in individual steps won't necessarily be the same. They are usually dependent
    on states. So, we will have to solve an MDP by finding the optimal policy in real-world
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: The value function of a policy measures how good it is for an agent to be in
    each state, given the policy being followed. The greater the value, the better
    the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, we calculated the value, `V`, of the optimal policy using **matrix
    inversion**. According to the **Bellman Equation**, the relationship between the
    value at step *t+1* and that at step *t* can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the value converges, which means *Vt+1 = Vt*, we can derive the value,
    `V`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I* is the identity matrix with 1s on the main diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of solving an MDP with matrix inversion is that you always get
    an exact answer. But the downside is its scalability. As we need to compute the
    inversion of an m * m matrix (where *m* is the number of possible states), the
    computation will become costly if there is a large number of states.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We decide to experiment with different values for the discount factor. Let''s
    start with 0, which means we only care about the immediate reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is consistent with the reward function as we only look at the reward received
    in the next move.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the discount factor increases toward 1, future rewards are considered. Let''s
    take a look at ùù≤=0.99:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This cheatsheet, [https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html),
    serves as a quick reference for MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: Performing policy evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just developed an MDP and computed the value function of the optimal
    policy using matrix inversion. We also mentioned the limitation of inverting an
    m * m matrix with a large m value (let's say 1,000, 10,000, or 100,000). In this
    recipe, we will talk about a simpler approach called **policy evaluation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy evaluation is an iterative algorithm. It starts with arbitrary policy
    values and then iteratively updates the values based on the **Bellman expectation
    equation** until they converge. In each iteration, the value of a policy, *œÄ*,
    for a state, *s*, is updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *œÄ(s, a)* denotes the probability of taking action *a* in state *s* under
    policy *œÄ*. *T(s, a, s')* is the transition probability from state *s* to state
    *s'* by taking action *a*, and *R(s, a)* is the reward received in state *s* by
    taking action *a*.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to terminate an iterative updating process. One is by setting
    a fixed number of iterations, such as 1,000 and 10,000, which might be difficult
    to control sometimes. Another one involves specifying a threshold (usually 0.0001,
    0.00001, or something similar) and terminating the process only if the values
    of all states change to an extent that is lower than the threshold specified.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will perform policy evaluation on the study-sleep-game
    process under the optimal policy and a random policy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s develop a policy evaluation algorithm and apply it to our study-sleep-game
    process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and define the transition matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the reward function and the discount factor (let''s use `0.5` for now):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the threshold used to determine when to stop the evaluation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the optimal policy where action a0 is chosen under all circumstances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Develop a policy evaluation function that takes in a policy, transition matrix,
    rewards, discount factor, and a threshold and computes the `value` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s plug in the optimal policy and all other variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is almost the same as what we got using matrix inversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now experiment with another policy, a random policy where actions are picked
    with the same probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Plug in the random policy and all other variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just seen how effective it is to compute the value of a policy using
    policy evaluation. It is a simple convergent iterative approach, in the **dynamic
    programming family**, or to be more specific, **approximate dynamic programming**.
    It starts with random guesses as to the values and then iteratively updates them
    according to the Bellman expectation equation until they converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Step 5, the policy evaluation function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializes the policy values as all zeros.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the values based on the Bellman expectation equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the maximal change of the values across all states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the maximal change is greater than the threshold, it keeps updating the values.
    Otherwise, it terminates the evaluation process and returns the latest values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since policy evaluation uses iterative approximation, its result might not be
    exactly the same as the result of the matrix inversion method, which uses exact
    computation. In fact, we don't really need the value function to be that precise.
    Also, it can solve the **curses** **of dimensionality** problem, which can result
    in scaling up the computation to thousands of millions of states. Therefore, we
    usually prefer policy evaluation over the other.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to remember is that policy evaluation is used to **predict**
    how great a we will get from a given policy; it is not used for **control** problems.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To take a closer look, we also plot the policy values over the whole evaluation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to record the value for each iteration in the `policy_evaluation`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we feed the `policy_evaluation_history` function with the optimal policy,
    a discount factor of `0.5`, and other variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then plot the resulting history of values using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)'
  prefs: []
  type: TYPE_IMG
- en: It is interesting to see the stabilization between iterations 10 to 14 during
    the convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we run the same code but with two different discount factors, 0.2 and
    0.99\. We get the following plot with the discount factor at 0.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the plot with a discount factor of 0.5 with this one, we can see that
    the smaller the factor, the faster the policy values converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also get the following plot with a discount factor of 0.99:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)'
  prefs: []
  type: TYPE_IMG
- en: By comparing the plot with a discount factor of 0.5 to the plot with a discount
    factor of 0.99, we can see that the larger the factor, the longer it takes for
    policy values to converge. The discount factor is a tradeoff between rewards now
    and rewards in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the FrozenLake environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The optimal policies for the MDPs we have dealt with so far are pretty intuitive.
    However, it won't be that straightforward in most cases, such as the FrozenLake
    environment. In this recipe, let's play around with the FrozenLake environment
    and get ready for upcoming recipes where we will find its optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: FrozenLake is a typical Gym environment with a **discrete** state space. It
    is about moving an agent from the starting location to the goal location in a
    grid world, and at the same time avoiding traps. The grid is either four by four
    ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/))
    or eight by eigh.
  prefs: []
  type: TYPE_NORMAL
- en: 't ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
    The grid is made up of the following four types of tiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S**: The starting location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G**: The goal location, which terminates an episode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F**: The frozen tile, which is a walkable location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H**: The hole location, which terminates an episode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are four actions, obviously: moving left (0), moving down (1), moving
    right (2), and moving up (3). The reward is +1 if the agent successfully reaches
    the goal location, and 0 otherwise. Also, the observation space is represented
    in a 16-dimensional integer array, and there are 4 possible actions (which makes
    sense).'
  prefs: []
  type: TYPE_NORMAL
- en: What is tricky in this environment is that, as the ice surface is slippery,
    the agent won't always move in the direction it intends. For example, it may move
    to the left or to the right when it intends to move down.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the FrozenLake environment, let''s first search for it in the table
    of environments here: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    The search gives us `FrozenLake-v0`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s simulate the four-by-four FrozenLake environment in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `gym` library and create an instance of the FrozenLake environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The agent starts with state `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Render the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a down movement since it is walkable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out all the returning information to confirm that the agent lands in
    state `4` with a probability of 33.33%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You get `0` as a reward, since it has not reached the goal and the episode is
    not done yet. Again, you might see the agent landing in state 1, or staying in
    state 0 because of the slippery surface.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how difficult it is to walk on the frozen lake, implement a
    random policy and calculate the average total reward over 1,000 episodes. First,
    define a function that simulates a FrozenLake episode given a policy and returns
    the total reward (we know it is either 0 or 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run `1000` episodes, and a policy will be randomly generated and will be
    used in each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This basically means there is only a 1.4% chance on average that the agent can
    reach the goal if we randomize the actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we experiment with a random search policy. In the training phase, we
    randomly generate a bunch of policies and record the first one that reaches the
    goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the best policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run 1,000 episodes with the policy we just cherry-picked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Using the random search algorithm, the goal will be reached 20.8% of the time
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this result could vary a lot, as the policy we picked might happen
    to reach the goal because of the slippery ice and might not be the optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we randomly generated a policy that was composed of 16 actions
    for the 16 states. Keep in mind that in FrozenLake, the movement direction is
    only partially dependent on the chosen action. This increases the uncertainty
    of control.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the code in *Step 4*, you will see a 4 * 4 matrix as follows,
    representing the frozen lake and the tile (state 0) where the agent stands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After running the lines of code in *Step 5*, you will see the resulting grid
    as follows, where the agent moves down to state 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An episode will terminate if either of the following two conditions is met:'
  prefs: []
  type: TYPE_NORMAL
- en: Moving to an H tile (state 5, 7, 11, 12 ). This will generate a total reward
    of 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving to the G tile (state 15). This will generate a total reward of +1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can look into the details of the FrozenLake environment, including the transformation
    matrix and rewards for each state and action, by using the P attribute. For example,
    for state 6, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns a dictionary with keys 0, 1, 2, and 3, representing four possible
    actions. The value is a list of movements after taking an action. The movement
    list is in the following format: (transformation probability, new state, reward
    received, is done). For instance, if the agent resides in state 6 and intends
    to take action 1 (down), there is a 33.33% chance that it will land in state 5,
    receiving a reward of 0 and terminating the episode; there is a 33.33% chance
    that it will land in state 10 and receive a reward of 0; and there is a 33.33%
    chance that it will land in state 7, receiving a reward of 0 and terminating the
    episode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For state 11, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As stepping on a hole will terminate an episode, it won‚Äôt make any movement
    afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check out the other states.
  prefs: []
  type: TYPE_NORMAL
- en: Solving an MDP with a value iteration algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An MDP is considered solved if its optimal policy is found. In this recipe,
    we will figure out the optimal policy for the FrozenLake environment using a **value
    iteration** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind value iteration is quite similar to that of policy evaluation.
    It is also an iterative algorithm. It starts with arbitrary policy values and
    then iteratively updates the values based on the **Bellman optimality equation**
    until they converge. So in each iteration, instead of taking the expectation (average)
    of values across all actions, it picks the action that achieves the maximal policy
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, V*(s) denotes the optimal value, which is the value of the optimal policy;
    T(s, a, s') is the transition probability from state s to state s‚Äô by taking action
    a; and R(s, a) is the reward received in state s by taking action a.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the optimal values are computed, we can easily obtain the optimal policy
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s solve the FrozenLake environment using a value iteration algorithm as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now define the function that computes optimal values based on the value iteration
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Plug in the environment, discount factor, and convergence threshold, then print
    the optimal values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the optimal values, we develop the function that extracts
    the optimal policy out of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Plug in the environment, discount factor, and optimal values, then print the
    optimal policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to gauge how good the optimal policy is. So, let''s run 1,000 episodes
    with the optimal policy and check the average reward. Here, we will reuse the
    `run_episode` function we defined in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Under the optimal policy, the agent will reach the goal 75% of the time, on
    average. This is the best we are able to get since the ice is slippery.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a value iteration algorithm, we get the optimal value function by iteratively
    applying the Bellman optimality equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is another version of the Bellman optimality equation, which
    can deal with environments where rewards are partially dependent on the new state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, R(s, a, s'') is the reward received as a result of moving to state s''
    from state s by taking action a. As this version is more compatible, we developed
    our `value_iteration` function according to it. As you saw in *Step 3*, we perform
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy values as all zeros.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the values based on the Bellman optimality equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the maximal change of the values across all states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the maximal change is greater than the threshold, we keep updating the values.
    Otherwise, we terminate the evaluation process and return the latest values as
    the optimal values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We obtained a success rate of 75% with a discount factor of 0.99\. How does
    the discount factor affect the performance? Let''s do some experiments with different
    factors, including `0`, `0.2`, `0.4`, `0.6`, `0.8`, `0.99`, and `1.`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'For each discount factor, we compute the average success rate over 10,000 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We draw a plot of the average success rate versus the discount factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/650d388b-391a-4be4-8537-d49d31b08315.png)'
  prefs: []
  type: TYPE_IMG
- en: The result shows that the performance improves when there is an increase in
    the discount factor. This verifies the fact that a small discount factor values
    the reward now and a large discount factor values a better reward in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Solving an MDP with a policy iteration algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to solving an MDP is by using a **policy iteration** algorithm,
    which we will discuss in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'A policy iteration algorithm can be subdivided into two components: policy
    evaluation and policy improvement. It starts with an arbitrary policy. And in
    each iteration, it first computes the policy values given the latest policy, based
    on the Bellman expectation equation; it then extracts an improved policy out of
    the resulting policy values, based on the Bellman optimality equation. It iteratively
    evaluates the policy and generates an improved version until the policy doesn''t
    change any more.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's develop a policy iteration algorithm and use it to solve the FrozenLake
    environment. After that, we will explain how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s solve the FrozenLake environment using a policy iteration algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary libraries and create an instance of the FrozenLake
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `0.99` as the discount factor for now, and `0.0001` as the convergence
    threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define the `policy_evaluation` function that computes the values given
    a policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to what we did in the *Performing policy evaluation* recipe,
    but with the Gym environment as an input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we develop the second main component of the policy iteration algorithm,
    the policy improvement part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This extracts an improved policy from the given policy values, based on the
    Bellman optimality equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have both components ready, we develop the policy iteration algorithm
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Plug in the environment, discount factor, and convergence threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve obtained the optimal values and optimal policy. Let''s take a look at
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: They are exactly the same as what we got using the value iteration algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Policy iteration combines policy evaluation and policy improvement in each
    iteration. In policy evaluation, the values for a given policy (not the optimal
    one) are calculated based on the Bellman expectation equation until they converge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, a = œÄ(s), which is the action taken under policy œÄ in state s.
  prefs: []
  type: TYPE_NORMAL
- en: 'In policy improvement, the policy is updated using the resulting converged
    policy values, V(s), based on the Bellman optimality equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f60962b4-570b-4954-8da6-79733626a594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This repeats the policy evaluation and policy improvement steps until the policy
    converges. At convergence, the latest policy and its value function are the optimal
    policy and the optimal value function. Hence, in Step 5, the `policy_iteration`
    function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializes a random policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the values of the policy with the policy evaluation algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtains an improved policy based on the policy values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the new policy is different from the old one, it updates the policy and runs
    another iteration. Otherwise, it terminates the iteration process and returns
    the policy values and the policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have just solved the FrozenLake environment with a policy iteration algorithm.
    So, you may wonder when it is better to use policy iteration over value iteration
    and vice versa. There are basically three scenarios where one has the edge over
    the other:'
  prefs: []
  type: TYPE_NORMAL
- en: If there is a large number of actions, use policy iteration, as it can converge
    faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a small number of actions, use value iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is already a viable policy (obtained either by intuition or domain
    knowledge), use policy iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outside those scenarios, policy iteration and value iteration are generally
    comparable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, we will apply each algorithm to solve the coin-flipping-gamble
    problem. We will see which algorithm converges faster.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feel free to use what we've learned in these two recipes to solve a bigger ice
    grid, the `FrozenLake8x8-v0` environment ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/)).
  prefs: []
  type: TYPE_NORMAL
- en: Solving the coin-flipping gamble problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gambling on coin flipping should sound familiar to everyone. In each round of
    the game, the gambler can make a bet on whether a coin flip will show heads. If
    it turns out heads, the gambler will win the same amount they bet; otherwise,
    they will lose this amount. The game continues until the gambler loses (ends up
    with nothing) or wins (wins more than 100 dollars, let's say). Let's say the coin
    is unfair and it lands on heads 40% of the time. In order to maximize the chance
    of winning, how much should the gambler bet based on their current capital in
    each round? This will definitely be an interesting problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: If the coin lands on heads more than 50% of the time, there is nothing to discuss.
    The gambler can just keep betting one dollar each round and should win the game
    most of the time. If it is a fair coin, the gambler could bet one dollar each
    round and end up winning around 50% of the time. It gets tricky when the probability
    of heads is lower than 50%; the safe-bet strategy wouldn't work anymore. Nor would
    a random strategy, either. We need to resort to the reinforcement learning techniques
    we've learned in this chapter to make smart bets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by formulating the coin-flipping gamble problem as an MDP.
    It is basically an undiscounted, episodic, and finite MDP with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The state is the gambler''s capital in dollars. There are 101 states: 0, 1,
    2, ‚Ä¶, 98, 99, and 100+.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is 1 if the state 100+ is reached; otherwise, the reward is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action is the possible amount the gambler bets in a round. Given state s,
    the possible actions include 1, 2, ‚Ä¶, and min(s, 100 - s). For example, when the
    gambler has 60 dollars, they can bet any amount from 1 to 40\. Any amount above
    40 doesn't make any sense as it increases the loss and doesn't increase the chance
    of winning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next state after taking an action depends on the probability of the coin
    coming up heads. Let's say it is 40%. So, the next state of state s after taking
    action *a* will be *s+a* by 40%, *s-a* by 60%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process terminates at state 0 and state 100+.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first solve the coin-flipping gamble problem by using a value iteration
    algorithm and performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the discount factor and convergence threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set 1 as the discount factor as the MDP is an undiscounted process;
    we set a small threshold as we expect small policy values since all rewards are
    0 except the last state.
  prefs: []
  type: TYPE_NORMAL
- en: Define the following environment variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are 101 states in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding reward is displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s say the probability of getting heads is 40%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Put these variables into a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we develop a function that computes optimal values based on the value iteration
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We only need to compute the values for states 1 to 99, as the values for state
    0 and state 100+ are 0\. And given state *s*, the possible actions can be anything
    from 1 up to *min(s, 100 - s)*. We should keep this in mind while computing the
    Bellman optimality equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we develop a function that extracts the optimal policy based on the optimal
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and optimal policy after . Also, we time how long
    it takes to solve the gamble MDP with value iteration; we will compare this with
    the time it takes for policy iteration to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We solved the gamble problem with value iteration in `4.717` seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the optimal policy values and the optimal policy we got:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the policy value versus state as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've solved the gamble problem with value iteration, how about policy
    iteration? Let's see.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by developing the `policy_evaluation` function that computes the values
    given a policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we develop another main component of the policy iteration algorithm,
    the policy improvement part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'With both components ready, we can develop the main entry to the policy iteration
    algorithm as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plug in the environment, discount factor, and convergence threshold
    to compute the optimal values and the optimal policy. We record the time spent
    solving the MDP as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Check out the optimal values and optimal policy we just obtained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After executing the lines of code in *Step 7*, you will see the optimal policy
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also see the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 8* generates the following plot for the optimal policy values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that, as the capital (state) increases, the estimated reward (policy
    value) also increases, which makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: What we did in *Step 9* is very similar to what we did in the *Solving an MDP
    with a policy iteration algorithm* recipe, but for the coin-flipping gamble environment
    this time.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, the policy improvement function extracts an improved policy out
    of the given policy values, based on the Bellman optimality equation.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Step 12*, we solved the gamble problem with policy iteration
    in `2.002` seconds, which is less than half the time it took with value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results we got from *Step 13* include the following optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'They also include the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The results from the two approaches, value iteration and policy iteration, are
    consistent.
  prefs: []
  type: TYPE_NORMAL
- en: We have solved the gamble problem by using value iteration and policy iteration.
    To deal with a reinforcement learning problem, one of the trickiest tasks is to
    formulate the process into an MDP. In our case, the policy is transformed from
    the current capital (states) to the new capital (new states) by betting certain
    stakes (actions). The optimal policy maximizes the probability of winning the
    game (+1 reward), and evaluates the probability of winning under the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting thing to note is how the transformation probabilities and
    new states are determined in the Bellman equation in our example. Taking action
    a in state s (having capital s and making a bet of 1 dollar) will have two possible
    outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: Moving to new state s+a, if the coin lands on heads. Hence, the transformation
    probability is equal to the probability of heads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving to new state s-a, if the coin lands on tails. Therefore, the transformation
    probability is equal to the probability of tails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is quite similar to the FrozenLake environment, where the agent lands on
    the intended tile only by a certain probability.
  prefs: []
  type: TYPE_NORMAL
- en: We also verified that policy iteration converges faster than value iteration
    in this case. This is because there are up to 50 possible actions, which is more
    than the 4 actions in FrozenLake. For MDPs with a large number of actions, solving
    with policy iteration is more efficient than doing so with value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may want to know whether the optimal policy really works. Let''s act like
    smart gamblers and play 10,000 episodes of the game. We are going to compare the
    optimal policy with two other strategies: conservative (betting one dollar each
    round) and random (betting a random amount):'
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining the three aforementioned betting strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We define the optimal strategy first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the conservative strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the random strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a wrapper function that runs one episode with a strategy and returns
    whether or not the game was won:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a starting capital (let‚Äôs say `50` dollars) and a number of episodes
    (`10000`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Run 10,000 episodes and keep track of the winning times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the winning probabilities for the three strategies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Our optimal policy is clearly the winner!
  prefs: []
  type: TYPE_NORMAL
