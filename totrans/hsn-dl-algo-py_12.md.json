["```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\ntf.reset_default_graph()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython import display\n```", "```py\ndata = input_data.read_data_sets(\"data/mnist\",one_hot=True)\n```", "```py\ndef generator(z, c,reuse=False):\n    with tf.variable_scope('generator', reuse=reuse):\n```", "```py\n            w_init = tf.contrib.layers.xavier_initializer()\n```", "```py\n            inputs = tf.concat([z, c], 1)\n```", "```py\n            dense1 = tf.layers.dense(inputs, 128, kernel_initializer=w_init)\n            relu1 = tf.nn.relu(dense1)\n```", "```py\n            logits = tf.layers.dense(relu1, 784, kernel_initializer=w_init)\n            output = tf.nn.tanh(logits)\n\n            return output\n```", "```py\ndef discriminator(x, c, reuse=False):\n    with tf.variable_scope('discriminator', reuse=reuse):\n```", "```py\n            w_init = tf.contrib.layers.xavier_initializer()\n```", "```py\n            inputs = tf.concat([x, c], 1)\n```", "```py\n            dense1 = tf.layers.dense(inputs, 128, kernel_initializer=w_init)\n            relu1 = tf.nn.relu(dense1)\n```", "```py\n             logits = tf.layers.dense(relu1, 1, kernel_initializer=w_init)\n             output = tf.nn.sigmoid(logits)\n\n             return output\n```", "```py\nx = tf.placeholder(tf.float32, shape=(None, 784))\nc = tf.placeholder(tf.float32, shape=(None, 10))\nz = tf.placeholder(tf.float32, shape=(None, 100))\n```", "```py\nfake_x = generator(z, c)\n```", "```py\nD_logits_real = discriminator(x,c)\n```", "```py\nD_logits_fake = discriminator(fake_x, c, reuse=True)\n```", "```py\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real,\n               labels=tf.ones_like(D_logits_real)))\n```", "```py\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,  \n               labels=tf.zeros_like(D_logits_fake)))\n```", "```py\nD_loss = D_loss_real + D_loss_fake\n```", "```py\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,\n               labels=tf.ones_like(D_logits_fake)))\n```", "```py\ntraining_vars = tf.trainable_variables()\ntheta_D = [var for var in training_vars if var.name.startswith('discriminator')]\ntheta_G = [var for var in training_vars if var.name.startswith('generator')]\n```", "```py\nlearning_rate = 0.001\n\nD_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(D_loss,         \n                 var_list=theta_D)\nG_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(G_loss, \n                       var_list=theta_G)\n```", "```py\nsession = tf.InteractiveSession()\ntf.global_variables_initializer().run()\n```", "```py\nbatch_size = 128\n```", "```py\nnum_epochs = 500\nnum_classes = 10\n```", "```py\nimages = (data.train.images)\nlabels = data.train.labels\n```", "```py\nlabel_to_generate = 7\nonehot = np.eye(10)\n```", "```py\nfor epoch in range(num_epochs):\n\n    for i in range(len(images) // batch_size):\n```", "```py\n        batch_image = images[i * batch_size:(i + 1) * batch_size]\n```", "```py\n        batch_c = labels[i * batch_size:(i + 1) * batch_size]\n```", "```py\n        batch_noise = np.random.normal(0, 1, (batch_size, 100))\n```", "```py\n        generator_loss, _ = session.run([D_loss, D_optimizer], {x: batch_image, c: batch_c, z: batch_noise})    \n```", "```py\n        discriminator_loss, _ = session.run([G_loss, G_optimizer], {x: batch_image, c: batch_c, z: batch_noise})\n```", "```py\n    noise = np.random.rand(1,100)\n```", "```py\n    gen_label = np.array([[label_to_generate]]).reshape(-1)\n```", "```py\n    one_hot_targets = np.eye(num_classes)[gen_label]\n```", "```py\n    _fake_x = session.run(fake_x, {z: noise, c: one_hot_targets})\n    _fake_x = _fake_x.reshape(28,28)\n```", "```py\n    print(\"Epoch: {},Discriminator Loss:{}, Generator Loss: {}\".format(epoch,discriminator_loss,generator_loss))\n\n    #plot the generated image\n    display.clear_output(wait=True)\n    plt.imshow(_fake_x) \n    plt.show()\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\ndata = input_data.read_data_sets(\"data/mnist\",one_hot=True)\n```", "```py\ndef lrelu(X, leak=0.2):\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * X + f2 * tf.abs(X)\n```", "```py\ndef generator(c, z,reuse=None):\n```", "```py\n    input_combined = tf.concat([c, z], axis=1)\n```", "```py\n    fuly_connected1 = tf.layers.dense(input_combined, 1024)\n    batch_norm1 = tf.layers.batch_normalization(fuly_connected1, training=is_train)\n    relu1 = tf.nn.relu(batch_norm1)\n```", "```py\n    fully_connected2 = tf.layers.dense(relu1, 7 * 7 * 128)\n    batch_norm2 = tf.layers.batch_normalization(fully_connected2, training=is_train)\n    relu2 = tf.nn.relu(batch_norm2)\n```", "```py\n    relu_flat = tf.reshape(relu2, [batch_size, 7, 7, 128])\n```", "```py\n    deconv1 = tf.layers.conv2d_transpose(relu_flat, \n                                          filters=64,\n                                          kernel_size=4,\n                                          strides=2,\n                                          padding='same',\n                                          activation=None)\n    batch_norm3 = tf.layers.batch_normalization(deconv1, training=is_train)\n    relu3 = tf.nn.relu(batch_norm3)\n```", "```py\n    deconv2 = tf.layers.conv2d_transpose(relu3, \n                                          filters=1,\n                                          kernel_size=4,\n                                          strides=2,\n                                          padding='same',\n                                          activation=None)\n```", "```py\n    output = tf.nn.sigmoid(deconv2) \n\n    return output\n```", "```py\ndef discriminator(x,reuse=None):\n```", "```py\n    conv1 = tf.layers.conv2d(x, \n                             filters=64, \n                             kernel_size=4,\n                             strides=2,\n                             padding='same',\n                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                             activation=None)\n    lrelu1 = lrelu(conv1, 0.2)\n```", "```py\n    conv2 = tf.layers.conv2d(lrelu1, \n                             filters=128,\n                             kernel_size=4,\n                             strides=2,\n                             padding='same',\n                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                             activation=None)\n    batch_norm2 = tf.layers.batch_normalization(conv2, training=is_train)\n    lrelu2 = lrelu(batch_norm2, 0.2)\n```", "```py\n   lrelu2_flat = tf.reshape(lrelu2, [batch_size, -1])\n```", "```py\n    full_connected = tf.layers.dense(lrelu2_flat, \n                          units=1024, \n                          activation=None)\n    batch_norm_3 = tf.layers.batch_normalization(full_connected, training=is_train)\n    lrelu3 = lrelu(batch_norm_3, 0.2)\n```", "```py\n    d_logits = tf.layers.dense(lrelu3, units=1, activation=None)\n```", "```py\n    full_connected_2 = tf.layers.dense(lrelu3, \n                                     units=128, \n                                     activation=None)\n\n    batch_norm_4 = tf.layers.batch_normalization(full_connected_2, training=is_train)\n    lrelu4 = lrelu(batch_norm_4, 0.2)\n```", "```py\n    q_net_latent = tf.layers.dense(lrelu4, \n                                    units=74, \n                                    activation=None)\n```", "```py\n    q_latents_categoricals_raw = q_net_latent[:,0:10]\n\n    c_estimates = tf.nn.softmax(q_latents_categoricals_raw, dim=1)\n```", "```py\n    return d_logits, c_estimates\n```", "```py\nbatch_size = 64\ninput_shape = [batch_size, 28,28,1]\n\nx = tf.placeholder(tf.float32, input_shape)\nz = tf.placeholder(tf.float32, [batch_size, 64])\nc = tf.placeholder(tf.float32, [batch_size, 10])\n\nis_train = tf.placeholder(tf.bool)\n```", "```py\nfake_x = generator(c, z)\n```", "```py\nD_logits_real, c_posterior_real = discriminator(x)\n```", "```py\nD_logits_fake, c_posterior_fake = discriminator(fake_x,reuse=True)\n```", "```py\n#real loss\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real, \n labels=tf.ones(dtype=tf.float32, shape=[batch_size, 1])))\n\n#fake loss\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, \n labels=tf.zeros(dtype=tf.float32, shape=[batch_size, 1])))\n\n#final discriminator loss\nD_loss = D_loss_real + D_loss_fake\n```", "```py\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, \n labels=tf.ones(dtype=tf.float32, shape=[batch_size, 1])))\n```", "```py\nc_prior = 0.10 * tf.ones(dtype=tf.float32, shape=[batch_size, 10])\n```", "```py\nentropy_of_c = tf.reduce_mean(-tf.reduce_sum(c * tf.log(tf.clip_by_value(c_prior, 1e-12, 1.0)),axis=-1))\n```", "```py\nlog_q_c_given_x = tf.reduce_mean(tf.reduce_sum(c * tf.log(tf.clip_by_value(c_posterior_fake, 1e-12, 1.0)), axis=-1))\n```", "```py\nmutual_information = entropy_of_c + log_q_c_given_x\n```", "```py\nD_loss = D_loss - mutual_information\nG_loss = G_loss - mutual_information\n```", "```py\ntraining_vars = tf.trainable_variables()\n\ntheta_D = [var for var in training_vars if 'discriminator' in var.name]\ntheta_G = [var for var in training_vars if 'generator' in var.name]\n```", "```py\nlearning_rate = 0.001\n\nD_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss,var_list = theta_D)\nG_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list = theta_G)\n```", "```py\nnum_epochs = 100\nsession = tf.InteractiveSession()\nsession.run(tf.global_variables_initializer())\n```", "```py\ndef plot(c, x):\n\n    c_ = np.argmax(c, 1)\n\n    sort_indices = np.argsort(c_, 0)\n\n    x_reshape = np.reshape(x[sort_indices], [batch_size, 28, 28])\n\n    x_reshape = np.reshape( np.expand_dims(x_reshape, axis=0), [4, (batch_size // 4), 28, 28])\n\n    values = []\n\n    for i in range(0,4):\n        row = np.concatenate( [x_reshape[i,j,:,:] for j in range(0,(batch_size // 4))], axis=1)\n        values.append(row)\n\n    return np.concatenate(values, axis=0)\n```", "```py\nonehot = np.eye(10)\n\nfor epoch in range(num_epochs):\n\n    for i in range(0, data.train.num_examples // batch_size):\n```", "```py\n        x_batch, _ = data.train.next_batch(batch_size)\n        x_batch = np.reshape(x_batch, (batch_size, 28, 28, 1))\n```", "```py\n        c_ = np.random.randint(low=0, high=10, size=(batch_size,))\n        c_one_hot = onehot[c_]\n```", "```py\n        z_batch = np.random.uniform(low=-1.0, high=1.0, size=(batch_size,64))\n```", "```py\n        feed_dict={x: x_batch, c: c_one_hot, z: z_batch, is_train: True}\n\n        _ = session.run(D_optimizer, feed_dict=feed_dict)\n        _ = session.run(G_optimizer, feed_dict=feed_dict)\n```", "```py\n\n        if i % 100 == 0:\n\n            discriminator_loss = D_loss.eval(feed_dict)\n            generator_loss = G_loss.eval(feed_dict)\n\n            _fake_x = fake_x.eval(feed_dict)\n\n            print(\"Epoch: {}, iteration: {}, Discriminator Loss:{}, Generator Loss: {}\".format(epoch,i,discriminator_loss,generator_loss))\n            plt.imshow(plot(c_one_hot, _fake_x))\n            plt.show() \n```", "```py\nclass CycleGAN:\n        def __init__(self):\n```", "```py\n        self.X = tf.placeholder(\"float\", shape=[batchsize, image_height, image_width, 3])\n        self.Y = tf.placeholder(\"float\", shape=[batchsize, image_height, image_width, 3])\n```", "```py\n        G = generator(\"G\")\n```", "```py\n        F = generator(\"F\")\n```", "```py\n         self.Dx = discriminator(\"Dx\")       \n```", "```py\n        self.Dy = discriminator(\"Dy\")\n```", "```py\n        self.fake_X = F(self.Y)\n```", "```py\n        self.fake_Y = G(self.X)        \n```", "```py\n        #real source image logits\n        self.Dx_logits_real = self.Dx(self.X) \n\n        #fake source image logits\n        self.Dx_logits_fake = self.Dx(self.fake_X, True)\n\n        #real target image logits\n        self.Dy_logits_fake = self.Dy(self.fake_Y, True)\n\n        #fake target image logits\n        self.Dy_logits_real = self.Dy(self.Y)\n```", "```py\n        self.cycle_loss = tf.reduce_mean(tf.abs(F(self.fake_Y, True) - self.X)) + \\\n                        tf.reduce_mean(tf.abs(G(self.fake_X, True) - self.Y))\n```", "```py\n        self.Dx_loss = -tf.reduce_mean(self.Dx_logits_real) + tf.reduce_mean(self.Dx_logits_fake) \n\n        self.Dy_loss = -tf.reduce_mean(self.Dy_logits_real) + tf.reduce_mean(self.Dy_logits_fake)\n```", "```py\n        self.G_loss = -tf.reduce_mean(self.Dy_logits_fake) + 10\\. * self.cycle_loss\n\n        self.F_loss = -tf.reduce_mean(self.Dx_logits_fake) + 10\\. * self.cycle_loss\n```", "```py\n       #optimize the discriminator\n        self.Dx_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.Dx_loss, var_list=[self.Dx.var])\n\n        self.Dy_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.Dy_loss, var_list=[self.Dy.var])\n\n        #optimize the generator\n        self.G_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.G_loss, var_list=[G.var])\n\n        self.F_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.F_loss, var_list=[F.var])\n```", "```py\nEpoch: 0, iteration: 0, Dx Loss: -0.6229429245, Dy Loss: -2.42867970467, G Loss: 1385.33557129, F Loss: 1383.81530762, Cycle Loss: 138.448059082\n\nEpoch: 0, iteration: 50, Dx Loss: -6.46077537537, Dy Loss: -7.29514217377, G Loss: 629.768066406, F Loss: 615.080932617, Cycle Loss: 62.6807098389\n\nEpoch: 1, iteration: 100, Dx Loss: -16.5891685486, Dy Loss: -16.0576553345, G Loss: 645.53137207, F Loss: 649.854919434, Cycle Loss: 63.9096908569\n```"]