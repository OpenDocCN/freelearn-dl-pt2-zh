["```py\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom torchvision import datasets\n```", "```py\nnum_eps=10\nbsize=32\nlrate=0.001\nlat_dimension=64\nimage_sz=64\nchnls=1\nlogging_intv=200\n```", "```py\nclass GANGenerator(nn.Module):\n    def __init__(self):\n        super(GANGenerator, self).__init__()\n        self.inp_sz = image_sz // 4\n        self.lin =   nn.Sequential(nn.Linear(lat_dimension, 128 * self.inp_sz ** 2))\n        self.bn1 = nn.BatchNorm2d(128)\n        self.up1 = nn.Upsample(scale_factor=2)\n        self.cn1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(128, 0.8)\n        self.rl1 = nn.LeakyReLU(0.2, inplace=True)\n        self.up2 = nn.Upsample(scale_factor=2)\n        self.cn2 = nn.Conv2d(128, 64, 3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(64, 0.8)\n        self.rl2 = nn.LeakyReLU(0.2, inplace=True)\n        self.cn3 = nn.Conv2d(64, chnls, 3, stride=1, padding=1)\n        self.act = nn.Tanh()\n```", "```py\n def forward(self, x):\n        x = self.lin(x)\n        x = x.view(x.shape[0], 128, self.inp_sz, self.inp_sz)\n        x = self.bn1(x)\n        x = self.up1(x)\n        x = self.cn1(x)\n        x = self.bn2(x)\n        x = self.rl1(x)\n        x = self.up2(x)\n        x = self.cn2(x)\n        x = self.bn3(x)\n        x = self.rl2(x)\n        x = self.cn3(x)\n        out = self.act(x)\n        return out\n```", "```py\nclass GANDiscriminator(nn.Module):\n    def __init__(self):\n        super(GANDiscriminator, self).__init__()\n        def disc_module(ip_chnls, op_chnls, bnorm=True):\n            mod = [nn.Conv2d(ip_chnls, op_chnls, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n                   nn.Dropout2d(0.25)] if bnorm:\n                mod += [nn.BatchNorm2d(op_chnls, 0.8)]\n            return mod\n        self.disc_model = nn.Sequential(\n            *disc_module(chnls, 16, bnorm=False),\n            *disc_module(16, 32),\n            *disc_module(32, 64),\n            *disc_module(64, 128),\n        )\n        # width and height of the down-sized image\n        ds_size = image_sz // 2 ** 4\n        self.adverse_lyr = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n```", "```py\n def forward(self, x):\n        x = self.disc_model(x)\n        x = x.view(x.shape[0], -1)\n        out = self.adverse_lyr(x)\n        return out\n```", "```py\n# instantiate the discriminator and generator models\ngen = GANGenerator()\ndisc = GANDiscriminator()\n# define the loss metric\nadv_loss_func = torch.nn.BCELoss()\n```", "```py\n# define the dataset and corresponding dataloader\ndloader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        \"./data/mnist/\", download=True,\n        transform=transforms.Compose(\n            [transforms.Resize((image_sz, image_sz)),\n             transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]),), batch_size=bsize, shuffle=True,)\n```", "```py\n# define the optimization schedule for both G and D\nopt_gen = torch.optim.Adam(gen.parameters(), lr=lrate)\nopt_disc = torch.optim.Adam(disc.parameters(), lr=lrate)\n```", "```py\nos.makedirs(\"./images_mnist\", exist_ok=True)\nfor ep in range(num_eps):\n    for idx, (images, _) in enumerate(dloader):\n        # generate ground truths for real and fake images\n        good_img = Variable(torch.FloatTensor(images.shape[0], 1).fill_(1.0), requires_grad=False)\n        bad_img = Variable(torch.FloatTensor(images.shape[0], 1) .fill_(0.0), requires_grad=False)\n        # get a real image\n        actual_images = Variable(images.type(torch.FloatTensor))\n        # train the generator model\n        opt_gen.zero_grad()\n        # generate a batch of images based on random noise as input\n        noise = Variable(torch.FloatTensor(np.random.normal(0, 1, (images.shape[0], lat_dimension))))\n        gen_images = gen(noise)\n        # generator model optimization - how well can it fool the discriminator\n        generator_loss = adv_loss_func(disc(gen_images), good_img)\n        generator_loss.backward()\n        opt_gen.step()\n```", "```py\n # train the discriminator model\n        opt_disc.zero_grad()\n        # calculate discriminator loss as average of mistakes(losses) in confusing real images as fake and vice versa\n        actual_image_loss = adv_loss_func(disc(actual_images), good_img)\n        fake_image_loss = adv_loss_func(disc(gen_images.detach()), bad_img)\n        discriminator_loss = (actual_image_loss + fake_image_loss) / 2\n        # discriminator model optimization\n        discriminator_loss.backward()\n        opt_disc.step()\n        batches_completed = ep * len(dloader) + idx\n        if batches_completed % logging_intv == 0:\n            print(f\"epoch number {ep} | batch number {idx} | generator loss = {generator_loss.item()} \\\n            | discriminator loss = {discriminator_loss.item()}\")\n            save_image(gen_images.data[:25], f\"images_mnist/{batches_completed}.png\", nrow=5, normalize=True)\n```", "```py\nclass UNetGenerator(nn.Module):\n    def __init__(self, chnls_in=3, chnls_op=3):\n        super(UNetGenerator, self).__init__()\n        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n        self.down_conv_layer_2 = DownConvBlock(64, 128)\n        self.down_conv_layer_3 = DownConvBlock(128, 256)\n        self.down_conv_layer_4 = DownConvBlock(256, 512, dropout=0.5)\n        self.down_conv_layer_5 = DownConvBlock(512, 512, dropout=0.5)\n        self.down_conv_layer_6 = DownConvBlock(512, 512, dropout=0.5)\n        self.down_conv_layer_7 = DownConvBlock(512, 512, dropout=0.5)\n        self.down_conv_layer_8 = DownConvBlock(512, 512, norm=False, dropout=0.5)\n        self.up_conv_layer_1 = UpConvBlock(512, 512, dropout=0.5)\n        self.up_conv_layer_2 = UpConvBlock(1024, 512, dropout=0.5)\n        self.up_conv_layer_3 = UpConvBlock(1024, 512, dropout=0.5)\n        self.up_conv_layer_4 = UpConvBlock(1024, 512, dropout=0.5)\n        self.up_conv_layer_5 = UpConvBlock(1024, 256)\n        self.up_conv_layer_6 = UpConvBlock(512, 128)\n        self.up_conv_layer_7 = UpConvBlock(256, 64)\n        self.upsample_layer = nn.Upsample(scale_factor=2)\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.conv_layer_1 = nn.Conv2d(128, chnls_op, 4, padding=1)\n        self.activation = nn.Tanh()\n```", "```py\nclass UpConvBlock(nn.Module):\n    def __init__(self, ip_sz, op_sz, dropout=0.0):\n        super(UpConvBlock, self).__init__()\n        self.layers = [\n            nn.ConvTranspose2d(ip_sz, op_sz, 4, 2, 1),\n            nn.InstanceNorm2d(op_sz), nn.ReLU(),]\n        if dropout:\n            self.layers += [nn.Dropout(dropout)]\n    def forward(self, x, enc_ip):\n        x = nn.Sequential(*(self.layers))(x)\n        op = torch.cat((x, enc_ip), 1)\n        return op\n```", "```py\nclass DownConvBlock(nn.Module):\n    def __init__(self, ip_sz, op_sz, norm=True, dropout=0.0):\n        super(DownConvBlock, self).__init__()\n        self.layers = [nn.Conv2d(ip_sz, op_sz, 4, 2, 1)]\n        if norm:\n            self.layers.append(nn.InstanceNorm2d(op_sz))\n        self.layers += [nn.LeakyReLU(0.2)]\n        if dropout:\n            self.layers += [nn.Dropout(dropout)]\n    def forward(self, x):\n        op = nn.Sequential(*(self.layers))(x)\n        return op\n```", "```py\n def forward(self, x):\n        enc1 = self.down_conv_layer_1(x)\n        enc2 = self.down_conv_layer_2(enc1)\n        enc3 = self.down_conv_layer_3(enc2)\n        enc4 = self.down_conv_layer_4(enc3)\n        enc5 = self.down_conv_layer_5(enc4)\n        enc6 = self.down_conv_layer_6(enc5)\n        enc7 = self.down_conv_layer_7(enc6)\n        enc8 = self.down_conv_layer_8(enc7)\n        dec1 = self.up_conv_layer_1(enc8, enc7)\n        dec2 = self.up_conv_layer_2(dec1, enc6)\n        dec3 = self.up_conv_layer_3(dec2, enc5)\n        dec4 = self.up_conv_layer_4(dec3, enc4)\n        dec5 = self.up_conv_layer_5(dec4, enc3)\n        dec6 = self.up_conv_layer_6(dec5, enc2)\n        dec7 = self.up_conv_layer_7(dec6, enc1)\n        final = self.upsample_layer(dec7)\n        final = self.zero_pad(final)\n        final = self.conv_layer_1(final)\n        return self.activation(final)\n```", "```py\nclass Pix2PixDiscriminator(nn.Module):\n    def __init__(self, chnls_in=3):\n        super(Pix2PixDiscriminator, self).__init__()\n        def disc_conv_block(chnls_in, chnls_op, norm=1):\n            layers = [nn.Conv2d(chnls_in, chnls_op, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(chnls_op))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        self.lyr1 = disc_conv_block(chnls_in * 2, 64, norm=0)\n        self.lyr2 = disc_conv_block(64, 128)\n        self.lyr3 = disc_conv_block(128, 256)\n        self.lyr4 = disc_conv_block(256, 512)\n```", "```py\n def forward(self, real_image, translated_image):\n        ip = torch.cat((real_image, translated_image), 1)\n        op = self.lyr1(ip)\n        op = self.lyr2(op)\n        op = self.lyr3(op)\n        op = self.lyr4(op)\n        op = nn.ZeroPad2d((1, 0, 1, 0))(op)\n        op = nn.Conv2d(512, 1, 4, padding=1)(op)\n        return op\n```"]