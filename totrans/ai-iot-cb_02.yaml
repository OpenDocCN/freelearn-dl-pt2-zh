- en: Handling Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据
- en: The technique used to collect data often determines the type of models that
    can be utilized. If a seismograph only reported the current reading of seismic
    activity once an hour, it would be meaningless. The data would not be high fidelity
    enough to predict earthquakes. The job of a data scientist in an IoT project does
    not start after the data is collected but rather, the data scientist needs to
    be part of the building of the device. When a device is built, the data scientist
    needs to determine whether the device is emitting the type of data that is appropriate
    for machine learning. Next, the data scientist helps the electrical engineer determine
    whether the sensors are in the right places and whether there is a correlation
    between sensors, and finally, the data scientist needs to store data in a way
    that is efficient to perform analytics. By doing so, we avoid the first major
    pitfall of IoT, which is collecting and storing data that is, in the end, useless
    for machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 用于收集数据的技术通常决定了可以使用的模型类型。如果地震仪每小时只报告一次地震活动的当前读数，那就毫无意义。数据的高保真度不足以预测地震。在 IoT 项目中，数据科学家的工作并不是在收集数据之后开始，而是需要参与设备的建造。当设备建成时，数据科学家需要确定设备是否发出适合机器学习的数据。接下来，数据科学家帮助电气工程师确定传感器是否放置在正确的位置，传感器之间是否存在相关性，最后，数据科学家需要以高效的方式存储数据以进行分析。通过这样做，我们避免了
    IoT 的第一个主要陷阱，即收集和存储最终对机器学习无用的数据。
- en: This chapter examines storing, collecting, and analyzing data to ensure that
    there is enough data to perform effective and efficient machine learning. We are
    going to start by looking at how data is stored and accessed. Then, we are going
    to look at data collection design to ensure that the data coming off the device
    is feasible for machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将检查存储、收集和分析数据，以确保有足够的数据进行有效和高效的机器学习。我们将首先看看数据的存储和访问方式。然后，我们将查看数据收集设计，以确保设备上的数据适合机器学习。
- en: 'This chapter will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下教程：
- en: Storing data for analysis using Delta Lake
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Delta Lake 存储分析数据
- en: Data collection design
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集设计
- en: Windowing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口化
- en: Exploratory factor analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索因子分析
- en: Implementing analytic queries in Mongo/hot path storage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Mongo/hot path 存储中实现分析查询
- en: Ingesting IoT data into Spark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 IoT 数据导入 Spark
- en: Storing data for analysis using Delta Lake
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Delta Lake 存储分析数据
- en: Today, there are many options for dealing with data for analysis. You can store
    it in a data lake, Delta Lake, or a NoSQL database. This recipe covers data storage
    and retrieval and using Delta Lake. Delta Lake provides the fastest way to work
    with data and the most efficient way to store data. It also allows you to look
    at data as it existed at any given time in the past.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，有许多处理分析数据的选择。您可以将其存储在数据湖、Delta Lake 或 NoSQL 数据库中。本教程涵盖数据存储和检索，并使用 Delta Lake。Delta
    Lake 提供了处理数据的最快方式和最有效的存储数据方式。它还允许您查看过去任意时间点存在的数据。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: While Delta Lake is an open source project, the easiest way to store files in
    Delta Lake is through Databricks. The setup of Databricks was discussed in [Chapter
    1](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml), *Setting Up the IoT and AI Environment*.
    This recipe assumes you have Databricks set up and running.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Delta Lake 是一个开源项目，但在 Delta Lake 中存储文件的最简单方法是通过 Databricks。Databricks 的设置在
    [第1章](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml) 中讨论过，《设置 IoT 和 AI 环境》。本教程假设您已经设置和运行了
    Databricks。
- en: How to do it...
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'Importing files into Delta Lake is easy. Data can be imported through files
    or streaming. The steps for this recipe are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件导入 Delta Lake 很容易。数据可以通过文件或流导入。本教程的步骤如下：
- en: In Databricks, open the data panel by clicking on the Data button, click on
    the Add Data button, and drag your file into the Upload section.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Databricks 中，点击“数据”按钮打开数据面板，然后点击“添加数据”按钮，将文件拖入上传区域。
- en: 'Click on Create Table in Notebook. The code generated for you will start with
    this:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Notebook 中点击“创建表”。为您生成的代码将从这里开始：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Review the data and when you are ready to save to Delta Lake, uncomment the
    last line:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看数据，并在准备好保存到 Delta Lake 时，取消注释最后一行：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, change `"parquet"` to `"delta"`:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将 `"parquet"` 更改为 `"delta"`：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From here, query the data:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里查询数据：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Alternatively, you can optimize how Delta Lake saves the file, making querying
    faster:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，您可以优化 Delta Lake 保存文件的方式，使查询速度更快：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Delta Lake data can be updated, filtered, and aggregated. In addition, it can
    be turned into a Spark or Koalas DataFrame easily.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delta Lake is built on top of Parquet. Utilizing columnar compression and metadata
    storage, it can make the retrieval of data 10 times faster than standard Parquet. In
    addition to faster performance, Delta Lake's data versioning allows data scientists
    to look at how data was at a particular time, allowing data scientists to perform
    root cause analysis when their models drift.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Data collection design
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The single most important factor in machine learning and IoT is data collection
    design. If the data collected is *garbage **data*, then no machine learning can
    be done on top of it. Suppose you are looking at vibrations of a pump (shown in
    the following graph) to determine whether the pump is having issues with its mechanics
    or ball bearings so that preventive maintenance can be performed before serious
    damage is done to the machine:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aee68c95-aca7-4fac-91d6-982d39cad99a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'Importantly, real-time data at 100 Hz is prohibitively expensive to store in
    the cloud. To keep costs down, engineers often send data at frequencies of 1 minute. Low-frequency
    sensor data often cannot accurately represent the issue that is being looked at. The
    next chart shows how the data looks when only sampled once per minute:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf631676-cc3d-478b-a535-8ad59666185c.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'Here, we see vibrometer data overlaid with the data that is being collected
    in 1-minute intervals. The data has some use but it is not accurate as it does
    not show the true magnitude of what is going on with the data. Using the mean
    is worse. The following chart shows the average reading of the vibrometer''s mean
    over 1 minute:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5912086c-58db-4c63-816e-b33ea05898b8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Taking the average reading windowed over 1 minute is an even worse solution
    because the average value is not changing when there is a problem with the pump. The
    following chart shows the vibrometer''s standard reading over 1 minute:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ce3db09-44b8-47e9-a1a4-5cad48fb0978.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Using a standard deviation technique shows variance compared to the mean to
    determine whether there is an issue with the pump. This is a more accurate solution
    over the average technique.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Using minimum and maximum windowed over a 1-minute window can present the best
    representation of the magnitude of the situation. The following chart shows what
    the reading will look like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dea0117e-528e-4efa-9597-f7c608fce056.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Because IoT machines can work correctly for years before having issues and forwarding
    high-frequency data in the cloud is cost-prohibitive, other measurements are used
    to determine whether the device needs maintenance. Techniques such as *min/max*,
    *standard deviation*, or *spikes* can be used to trigger a cloud-to-device message
    telling the device to send data at a much higher frequency. High-frequency diagnostic
    data can use blob storage to store large files.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges of IoT is finding meaningful data in a sea of data. In
    this recipe, we shall demonstrate techniques to mine for valuable data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: IoT 的一个挑战之一是在海量数据中找到有意义的数据。在本章中，我们将展示挖掘有价值数据的技术。
- en: Getting ready
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To get ready for data collection design, you will need a device streaming data
    at a high rate. In [Chapter 1](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml), *Setting
    Up the IoT and AI Environment*, we discussed getting a device streaming data into
    IoT Hub. Often in production, device data is sent in intervals of 15 seconds or
    1 minute. But for data collection design, one device is sending data at a high
    rate of 10 Hz, or 10 times a second. Once that data is flowing in, you can pull
    it into Databricks for real-time analysis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备好数据收集设计，您需要一个以高速率流式传输数据的设备。在《第 1 章》中，*设置 IoT 和 AI 环境*，我们讨论了将设备数据流入 IoT Hub
    的过程。通常在生产中，设备数据以 15 秒或 1 分钟的间隔发送。但是在数据收集设计中，一个设备以 10 Hz 的高速率发送数据，即每秒 10 次。一旦数据流入，您可以将其拉入
    Databricks 进行实时分析。
- en: How to do it...
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: In our Databricks notebooks, we will analyze of the IoT data using techniques
    such as variance, Z-Spikes, and min/max.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Databricks 笔记本中，我们将使用方差、Z-尖峰和最小/最大值技术分析 IoT 数据。
- en: Variance
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差
- en: 'Variance is the measure of how much the data varies from the mean. In the code
    that follows, we are using Koalas, a distributed clone of `pandas`, to do our
    basic data engineering tasks, such as determining variance. The following code
    uses standard deviation over a rolling window to show data spike issues:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是数据与均值的变化程度的度量。在接下来的代码中，我们使用 Koalas，即 `pandas` 的分布式克隆，来执行基本的数据工程任务，如确定方差。以下代码使用滚动窗口上的标准偏差来显示数据尖峰问题：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Duty cycles are used on IoT product lines before enough data is collected for
    machine learning. They are often simple measures, such as whether the device is
    too hot or there are too many vibrations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IoT 产品线上使用占空比，直到收集到足够的数据用于机器学习。它们通常是简单的措施，例如设备是否过热或者振动过多。
- en: 'We can also look at high and low values such as maximum to show whether the
    sensor is throwing out appropriate readings. The following code shows the maximum
    reading of our dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看高低值，例如最大值，以确定传感器是否输出适当的读数。以下代码显示了我们数据集的最大读数：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Z-Spikes
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Z-尖峰
- en: Spikes can help determine whether there is an issue by looking at how rapidly
    a reading is changing. For example, an outdoor IoT device may have a different
    operating temperature in the South Pole compared to one in direct sun in Death
    Valley. One way of finding out whether there is an issue with the device is by
    looking at how fast the temperature is changing. Z-Spikes are a typical time-based
    anomaly detection. It is used because it only looks at that device's readings
    and can give a value independent of environmental factors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尖峰可以帮助确定是否存在问题，方法是查看读数变化的快速程度。例如，户外 IoT 设备在南极和死亡谷直射阳光下的工作温度可能不同。找出设备是否存在问题的一种方法是查看温度变化的速度。Z-尖峰是典型的基于时间的异常检测方法。它的使用原因是它仅查看该设备的读数，并且可以提供独立于环境因素的数值。
- en: Z-Spikes look at how the spike differs from the standard deviation. They use
    a statistical z-test to determine whether a spike is greater than 99.5% of the
    population.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Z-尖峰查看尖峰与标准偏差的差异。它使用统计 z 检验来确定尖峰是否大于人群中 99.5% 的尖峰。
- en: Min/max
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小/最大值
- en: 'Mins and maxes can show the value that shows the most stress on the system.
    The following code shows how to get the min and max values of a 1-minute window:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值和最大值可以显示系统上的最大压力值。以下代码展示了如何获取一个 1 分钟窗口的最小值和最大值：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Minimum and maximum values can emphasize outliers. This can be useful in determining
    anomalies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值和最大值可以突出异常值。这在确定异常时非常有用。
- en: Windowing
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口操作
- en: 'There are three primary windowing functions: tumbling, hopping, and sliding.
    Both Spark and Stream Analytics can do windowing. Windowing allows you to look
    at aggregate functions such as average, count, and sum. It also allows you to
    look at minimum and maximum values. Windowing is a feature engineering technique
    to help make data more manageable. In this recipe, we are going to cover several
    tools for windowing and the ways to window.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种主要的窗口函数：滚动窗口、跳跃窗口和滑动窗口。Spark 和 Stream Analytics 都可以进行窗口操作。窗口操作允许您查看聚合函数，如平均值、计数和总和。它还允许您查看最小值和最大值。窗口操作是一种特征工程技术，有助于使数据更易管理。在本章中，我们将介绍几种窗口操作的工具和窗口的方式。
- en: Getting ready
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To get ready, you will also need a device streaming data to IoT Hub. That stream
    will need to be ingested by either Azure's Stream Analytics, Spark, or Databricks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要准备好，您还需要一个将数据流式传输到IoT Hub的设备。该流需要被Azure的流分析、Spark或Databricks摄取。
- en: How to do it...
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: Utilize a Databricks notebook or Stream Analytics workspace to perform the recipes. Windowing
    turns the static of large datasets into meaningful features of your machine learning
    model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Databricks笔记本或流分析工作空间执行配方。窗口化可以将大数据集的静态转化为机器学习模型的有意义特征。
- en: Tumbling
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动
- en: 'Tumbling window functions group data streams into time segments (as shown in
    the following diagram). Tumbling windows means that the window does not repeat
    or overlap data from one segment waterfall into the next:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动窗口函数将数据流分组为时间段（如下图所示）。滚动窗口意味着窗口不会重复或将数据从一个段落传递到下一个段落：
- en: '![](img/5ff1a9bc-3d58-4028-9c43-f7cb51b62575.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ff1a9bc-3d58-4028-9c43-f7cb51b62575.png)'
- en: '**Stream Analytics**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**流分析**'
- en: 'In Stream Analytics, one way to use a tumbling window to count the events that
    happen every 10 seconds would be to do the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在流分析中，使用滚动窗口每10秒计算事件发生的一种方法如下：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Spark**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark**'
- en: 'In Spark, to do the same count of events happening every 10 minutes, you would
    do the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，要每10分钟计算事件发生的次数，可以按以下步骤进行：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Hopping
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跳跃
- en: 'Hopping windows are tumbling windows that overlap. They allow you to set specific
    commands and conditions, such as *every 5 minutes, give me the count of the sensor
    readings over the last 10 minutes*. To make a hopping window the same as a tumbling
    window, you would make the hop size the same as the window size, as shown in the
    following diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃窗口是重叠的滚动窗口。它们允许您设置特定的命令和条件，例如*每5分钟，给我过去10分钟内传感器读数的计数*。要使跳跃窗口与滚动窗口相同，需要将跳跃大小设置为窗口大小，如下图所示：
- en: '![](img/f9e39cf6-ea93-4c18-8f2c-51f1c1149ff3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9e39cf6-ea93-4c18-8f2c-51f1c1149ff3.png)'
- en: '**Stream Analytics**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**流分析**'
- en: 'The following Stream Analytics example shows a count of messages over a 10-minute
    window. This count happens every 5 minutes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的流分析示例显示了在10分钟窗口内的消息计数。每5分钟进行一次计数：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Spark**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark**'
- en: 'In PySpark, this would be done through a window function. The following example
    shows a Spark DataFrame that is windowed, producing an entry in a new entry in
    a DataFrame for every 5 minutes spanning a 10-minute period:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中，可以通过窗口函数来完成这一操作。下面的示例显示了一个窗口化的Spark DataFrame，在10分钟时间段内每5分钟生成一个新的DataFrame条目：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Sliding
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滑动
- en: 'Sliding windows produce an output when an event occurs. The following diagram
    illustrates this concept:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口在事件发生时产生输出。下图说明了这个概念：
- en: '![](img/8f2c30b0-381a-4e41-a981-07e4b9efeed6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f2c30b0-381a-4e41-a981-07e4b9efeed6.png)'
- en: '**Stream Analytics**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**流分析**'
- en: 'In the Stream Analytics example, by using a sliding window, we only receive
    a result when there are more than 100 messages over a 10-minute window. Unlike
    other methods that look at an exact window of time and show one message for that
    window, in sliding windows, we would receive a message on every input message.
    Another use of this would be to show a rolling average:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在流分析示例中，通过使用滑动窗口，只有在10分钟窗口内的消息数超过100条时才会收到结果。与查看确切时间窗口并显示该窗口的消息不同，在滑动窗口中，我们将在每个输入消息上收到一条消息。另一个用途是显示滚动平均值：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: Using windowing, IoT data can show factors such as frequency, sums, standard
    deviation, and percentile distribution over a period of time. Windowing can be
    used to enrich the data with feature engineering or can transform the data into
    an aggregate dataset. Windowing, for example, can show how many devices were produced
    in a factory or show the modulation in a sensor reading.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用窗口化，IoT数据可以显示诸如频率、总和、标准差和百分位分布等因素，时间段内的窗口化可以用于丰富数据的特征工程或将数据转换为聚合数据集。例如，窗口化可以显示工厂生产了多少设备或显示传感器读数中的调制。
- en: Exploratory factor analysis
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性因子分析
- en: Garbage data is one of the key issues that plague IoT. Data is often not validated
    before it is collected. Often, there are issues with bad sensor placement or data
    that appears to be random because it is not an appropriate measure for the type
    of data being used. For example, a vibrometer may show, because of the central
    limit theorem, that the data is centered around the mean, whereas the data is actually
    showing a large increase in magnitude. To combat this, it is important to do exploratory
    factor analysis on the device data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾数据是困扰物联网的主要问题之一。在收集数据之前，数据通常没有经过验证。常常存在与错误的传感器放置或数据看起来随机的问题，因为它不适合所使用的数据类型的适当度量。例如，由于中心极限定理，振动计可能显示数据集围绕平均值集中，而实际上数据显示了一个数量级的大增加。为了应对这些问题，对设备数据进行探索性因子分析至关重要。
- en: In this recipe, we will explore several techniques of factor analysis. Aggregate
    data and raw telemetry data are used in Databricks notebooks to perform this analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探讨几种因子分析技术。在Databricks笔记本中使用聚合数据和原始遥测数据来执行此分析。
- en: Getting ready
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to have data in a table in Databricks, which we implemented in
    the *Storing data for analysis using Delta Lake* recipe. Once data is in a Spark
    data table, it is ready for factor analysis.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在Databricks的表中拥有数据，我们在*使用Delta Lake存储数据进行分析*示例中实现了这一点。一旦数据存储在Spark数据表中，即可进行因子分析。
- en: How to do it...
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: This recipe is composed of two sections. The first is performing a visual inspection
    of data. Visual inspection can reveal software bugs, learn about how the device
    behaves, and determine device data patterns. The second part looks at correlation
    and co-variance. These techniques are often used to determine whether a sensor
    is redundant.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例由两个部分组成。第一部分是对数据进行视觉检查。视觉检查可以揭示软件缺陷，了解设备行为以及确定设备数据模式。第二部分则关注相关性和协方差。这些技术通常用于确定传感器是否冗余。
- en: Visual exploration
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉探索
- en: 'Spark allows you to look at basic charts without much code. Using the magic
    symbol at the top of the notebook segment, you can change language easily from
    Python to Scala or SQL. One word of caution about using Databricks'' built-in
    charting system is that it only looks at the first 10,000 records. For a large
    dataset, there are other charting libraries. The steps are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Spark允许您查看基本的图表而无需太多代码。使用笔记本段落顶部的魔术符号，您可以轻松地从Python切换到Scala或SQL。关于使用Databricks内置的图表系统的一个警告是，它只查看前10,000条记录。对于大型数据集，可以使用其他图表库。步骤如下：
- en: 'Query the data in Databricks using the `%sql` magic, as shown:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `%sql` 魔法在Databricks中查询数据，如下所示：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Select the chart icon at the bottom of the returned data grid. It will bring
    up the chart builder UI, as shown in the following screenshot:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择返回数据网格底部的图表图标。这将显示图表构建器UI，如下屏幕截图所示：
- en: '![](img/7c94b847-460b-4ae5-a586-9362af8bd951.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c94b847-460b-4ae5-a586-9362af8bd951.png)'
- en: Select the chart type that best represents the data. Some charts are better
    suited for variable comparison while others can help reveal trends.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最能代表数据的图表类型。有些图表更适合变量比较，而其他图表则可以帮助揭示趋势。
- en: The following section reviews when and why you would use different chart types.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将审查何时以及为什么使用不同类型的图表。
- en: Chart types
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图表类型
- en: Different types of charts illuminate different aspects of the data, such as
    comparison, composition, relationship, and distribution. Relationship charts are
    used to test a hypothesis or look at how one factor affects other factors. Composition
    shows the percentage breakdown of a dataset. It is often used to show how factors
    compare against others. A pie chart is a simple composition chart. Distribution
    charts are used to show distributions of a population. They are often used to
    determine whether the data is random, has a large spread, or is normalized. Comparison
    charts are used to compare one value against others.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的图表阐明了数据的不同方面，比如比较、组成、关系和分布。关系图表用于测试假设或查看一个因素如何影响其他因素。组成显示数据集的百分比分布。它通常用于显示各因素之间的比较情况。饼图是一种简单的组成图表。分布图表用于显示人群的分布情况。它们通常用于确定数据是否随机，是否具有较大的扩展或是否已归一化。比较图表用于将一个值与其他值进行比较。
- en: '**Bar and column charts**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**条形和柱形图**'
- en: 'Bar and column charts are used to make a comparison between items. Bar charts
    can have many items simply because of the page layout. Column and bar charts can
    also show change over time. The following chart is an example of a bar and column
    chart:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图和柱形图用于比较项目之间的差异。条形图由于页面布局的关系可以有很多项。柱形图和条形图还可以显示随时间变化的情况。以下图表是条形图和柱形图的一个例子：
- en: '![](img/c2226842-3041-4082-9376-26ebeb67e63a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2226842-3041-4082-9376-26ebeb67e63a.png)'
- en: '**Scatter plot**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**散点图**'
- en: 'Scatter plots can show the relationship between two variables. It also can
    show a trend line. The following is an example of a scatter plot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图可以显示两个变量之间的关系。它还可以显示趋势线。以下是散点图的一个例子：
- en: '![](img/19b383d7-7dc0-4255-be46-d9cfa6bc339d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19b383d7-7dc0-4255-be46-d9cfa6bc339d.png)'
- en: '**Bubble charts**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**气泡图**'
- en: 'When you want to show the relationship between three variables, you can use
    bubble charts. This can be used to show anomalous behavior. The following is an
    example of a bubble chart:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要展示三个变量之间的关系时，可以使用气泡图。这可以用来显示异常行为。以下是气泡图的一个例子：
- en: '![](img/7cb50415-dc6f-435f-b26e-23e7877699ed.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cb50415-dc6f-435f-b26e-23e7877699ed.png)'
- en: '**Line charts**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**折线图**'
- en: 'These charts show changes over time and can be used to show how a device''s
    data changes over a day. If a device has seasonal data, you may need to include
    the time of day as part of the algorithm or use de-seasonal algorithms. The following
    is an example of a line chart:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表显示随时间变化的情况，并可用于显示设备数据在一天内的变化。如果设备具有季节性数据，则可能需要将时间作为算法的一部分或使用去季节性算法。以下是折线图的一个例子：
- en: '![](img/344a11c7-0cd7-43b2-bf57-1f91cad705c2.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/344a11c7-0cd7-43b2-bf57-1f91cad705c2.png)'
- en: '**Area charts**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**面积图**'
- en: 'Area charts are like line charts but are used to show how the volume of one
    segment compares to another. The following is an example of an area chart:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 面积图类似于折线图，但用于显示一个部分的体积与另一个部分的比较。以下是面积图的一个例子：
- en: '![](img/0718e81c-7dbe-4eea-9e6e-7c282680507a.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0718e81c-7dbe-4eea-9e6e-7c282680507a.png)'
- en: '**Quantile plot**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**分位数图**'
- en: 'Help determine population shape by spitting data into segments (quantiles).
    Common quantiles are 25%, 50%, and 75%, or 33% and 66%, or 5% and 95% (percentages
    in general are quartiles). Understanding whether data is behaving within expected
    parameters is important in understanding whether a device is having problems.
    The following is an example of a quantile plot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据分成分段（分位数）来帮助确定人口形状。常见的分位数是25%，50%和75%，或33%和66%，或5%和95%（一般百分比是四分位数）。了解数据是否符合预期参数是理解设备是否存在问题的重要因素。以下是分位数图的一个例子：
- en: '![](img/1c26cdc0-52e2-458a-8d07-e803ea703e9c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c26cdc0-52e2-458a-8d07-e803ea703e9c.png)'
- en: Redundant sensors
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冗余传感器
- en: 'One of the challenges of IoT is determining where to place the sensors and
    how many sensors are needed. Take pumps, for example: one way of determining whether
    a pump''s bearings are going out is to use a microphone to listen for a high-pitched
    squeal. Another way is to use a parameter to determine whether it is vibrating
    more. Yet another way is to measure the current and see whether it is fluctuating.
    There is no one right way to determine whether a pump''s ball bearings are going
    out; however, implementing all three techniques may be cost-prohibitive and redundant. A
    common way of looking at the correlation between different sensors is using a
    heat map. In the following code, we use a heat map to find the correlation between
    sensors. In other words, we are looking for sensors that are transmitting redundant
    information:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网的一个挑战是确定传感器放置的位置以及需要多少个传感器。以泵为例：判断泵轴承是否损坏的一种方法是使用麦克风听高音尖叫声。另一种方法是使用参数来判断是否振动更多。还有一种方法是测量电流是否波动。没有一种确定泵轴承是否损坏的正确方法；然而，实施这三种技术可能成本过高且冗余。评估不同传感器之间的相关性的常见方法是使用热图。在下面的代码中，我们使用热图来找出传输冗余信息的传感器：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following screenshot shows the heat map:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了热图：
- en: '![](img/8521be2b-043f-4e66-996a-0f10235b8078.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8521be2b-043f-4e66-996a-0f10235b8078.png)'
- en: In the preceding example, we can see that `count` and `registered` have a very
    high correlation because both numbers are close to 1\. Similarly, we can see that
    `temp` and `atemp` have a high degree of correlation. Using this data without
    pruning out the corollary data can give a weighted effect to machine learning
    models training on the dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们可以看到`count`和`registered`具有非常高的相关性，因为这两个数字接近1。同样，我们可以看到`temp`和`atemp`之间有很高的相关性。如果在不去除相关数据的情况下使用这些数据，可能会给在数据集上训练的机器学习模型带来加权影响。
- en: '[](http://en.wikipedia.org/wiki/Contingency_table) When a device has very little
    data, it still may be valuable to perform analysis of variance, distribution,
    and deviation. Because it has a lower bar of entry than machine learning, it can
    be deployed at an earlier phase in the machine''s life cycle. Doing statistical
    analysis helps ensure that the device is setting proper data that is not duplicated
    or false and can be used for machine learning.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://en.wikipedia.org/wiki/Contingency_table) 当设备数据很少时，仍然进行方差分析、分布和偏差分析可能很有价值。因为它的进入门槛低于机器学习，所以可以在机器生命周期的早期阶段部署。进行统计分析有助于确保设备设置的数据正确，不重复或虚假，并可用于机器学习。'
- en: 'Cross-tabulation provides a table of the frequency distributions. This can
    be used to determine whether two different sensors are counting the same. The
    following is the code to display the cross-tabulation table:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉制表提供频率分布表。这可以用于确定两个不同传感器是否计算相同的情况。以下是显示交叉制表表格的代码：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Sample co-variance and correlation
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本协方差和相关性
- en: 'Co-variance measures the joint variability change of two sensors with respect
    to each other. Positive numbers would indicate that the sensors are reporting
    the same data. Negative numbers indicate that there is an inverse relationship
    between the sensors. The co-variance of two sensors can be calculated using the
    DataFrame `stat.cov` function in a Spark DataFrame:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差测量两个传感器相对于彼此的联合变化性。正数表明传感器报告相同的数据。负数表明传感器之间存在反向关系。可以使用Spark DataFrame中的DataFrame
    `stat.cov`函数计算两个传感器的协方差：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Modifying physical devices after they have been produced can be costly. This
    recipe shows how to inspect a prototype device to make sure the data produced
    by it will not be meaningless. Using data analysis tools such as Databricks for
    preliminary data analysis can save us from issues that plague IoT and AI, such
    as bad sensor placement, under- or overcommunication, and data that is not usable
    for machine learning. Performing standard machine learning tasks such as predictive
    maintenance, anomaly detection, or remaining useful life is dependent on good
    data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产后修改物理设备可能很昂贵。本文展示了如何检查原型设备，以确保其产生的数据不会毫无意义。使用数据分析工具如Databricks进行初步数据分析，可以避免物联网和人工智能中常见的问题，如传感器放置不当、通信过度或不足以及无法用于机器学习的数据。执行标准的机器学习任务，如预测维护、异常检测或剩余有用寿命，取决于良好的数据。
- en: There's more...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'You can further explore data by creating a filtering widget. For example, you
    could use the `CREATE WIDGET DROPDOWN` query as shown:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建一个过滤小部件来进一步探索数据。例如，您可以使用如下所示的`CREATE WIDGET DROPDOWN`查询：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Creating a widget allows you to create a data query that can be easily segmented,
    as shown in the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个小部件允许您创建一个数据查询，可以轻松分段，如下代码所示：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Other widget types, such as text, combo box, and multi-select, also are available.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其他小部件类型，如文本、组合框和多选框，也是可用的。
- en: Implementing analytic queries in Mongo/hot path storage
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo/hot路径存储中实现分析查询
- en: In IoT architectures, there is hot and cold path data. Hot path data can be
    accessed immediately. This is typically stored in a NoSQL or time-series database.
    An example of this would be to use a time-series database such as InfluxDB to
    count the number of resets per device over the last hour. This could be used to
    aid in feature engineering. Another use of hot data is precision analysis. If
    a machine breaks in the field, a database such as MongoDB can be queried for just
    the data that that machine has generated over the last month.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在物联网架构中，存在热路径数据和冷路径数据。热路径数据可以立即访问。这通常存储在NoSQL或时间序列数据库中。例如，可以使用时间序列数据库如InfluxDB来计算每小时每台设备的重置次数。这可以用于辅助特征工程。热数据的另一个用途是精确分析。如果一个设备在现场故障，可以查询诸如MongoDB之类的数据库，仅获取该设备在过去一个月内生成的数据。
- en: Cold path data is typically used for batch processing, such as machine learning
    and monthly reports. Cold path data is primarily data stored in a blob, S3 storage,
    or HDFS-compliant data store. Separating a hot path from a cold path is usually
    a factor of cost and scalability. IoT data generally falls into the category of
    big data. If a data scientist queries years' worth of data from a NoSQL database,
    the web application that is using it can falter. The same is not true for data
    stored in the cold path on a disk. On the other hand, if the data scientist needs
    to query a few hundred records from billions of records, a NoSQL database would
    be appropriate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 冷路径数据通常用于批处理，如机器学习和月度报告。冷路径数据主要存储在Blob、S3存储或HDFS兼容数据存储中。将热路径与冷路径分开通常涉及成本和可扩展性因素。IoT数据通常属于大数据范畴。如果数据科学家从NoSQL数据库中查询多年的数据，使用它的Web应用程序可能会崩溃。对于存储在磁盘上的冷路径数据，这种情况并不适用。另一方面，如果数据科学家需要从数十亿条记录中查询几百条记录，那么NoSQL数据库就是合适的选择。
- en: This recipe is focused on working with hot data. This recipe's primary focus
    is on extracting IoT data from MongoDB. First, we extract data from one device,
    and then we will aggregate it across multiple devices.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点是处理热数据。本文主要关注从MongoDB提取IoT数据。首先，我们从一个设备中提取数据，然后我们将其在多个设备间进行聚合。
- en: Getting ready
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Stream Analytics can get IoT data into MongoDB. To do this, start up MongoDB.
    This can be done through Azure Kubernetes Service or using the Atlas MongoDB cloud
    provider. Once you have a database, you can use a function app to move data between
    IoT Hub and MongoDB.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析可以将IoT数据传输到MongoDB。要做到这一点，请启动MongoDB。可以通过Azure Kubernetes Service或使用Atlas
    MongoDB云提供商来完成此操作。一旦有了数据库，您可以使用函数应用程序在IoT Hub和MongoDB之间移动数据。
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Mongo has a list of filtering options comparable to SQL. The following code
    shows how to connect to a local version of Mongo and query for all products with
    an inventory status of `A`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo具有一系列与SQL可比较的过滤选项。以下代码显示了如何连接到Mongo的本地版本，并查询所有库存状态为`A`的产品：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next example shows how to do a complicated filter followed by a group by
    operation. It finally sums the information. The output will show the count of
    items with a status of `A`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了如何执行复杂的过滤操作，接着进行分组操作。最终输出将显示状态为`A`的项目计数：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Mongo stores indexed data on multiple computers or partitions. This allows retrieval
    of specific data to be done with latency times in milliseconds. NoSQL databases
    can provide fast lookup for data. In this recipe, we discussed how to query data
    from MongoDB into Databricks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo将索引数据存储在多台计算机或分区上。这使得以毫秒为单位的延迟时间内检索特定数据成为可能。NoSQL数据库可以为数据提供快速查找。在本文中，我们讨论了如何从MongoDB查询数据到Databricks。
- en: Ingesting IoT data into Spark
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将IoT数据导入Spark
- en: To connect Spark to IoT Hub, first, create a consumer group. A consumer group
    is a pointer to the current position in the journal that the consumer has reached.
    There can be multiple consumers on the same journal of data. The consumer group
    is paralyzed and distributable, enabling you to write programs that can remain
    stable even on a massive scale.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要将Spark连接到IoT Hub，首先创建一个消费者组。消费者组是指示消费者已达到的日志当前位置的指针。可以在同一数据日志的多个消费者之间存在多个消费者。消费者组是并行和可分布的，使您能够编写即使在大规模的情况下也能保持稳定的程序。
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, go into the Azure IoT Hub portal and click on the Build-in
    endpoints menu option. Then, add a consumer group by entering some text. While
    still on that screen, copy the Event Hub-compatible endpoint connection string.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文，进入Azure IoT Hub门户，单击内置终端菜单选项。然后，通过输入一些文本来添加一个消费者组。在仍在该屏幕上的情况下，复制事件中心兼容端点连接字符串。
- en: How to do it...
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的步骤如下：
- en: 'In Databricks, start a new notebook and enter the information needed to connect
    to IoT Hub. Then, enter the following code:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Databricks中，启动一个新的笔记本，并输入连接到IoT Hub所需的信息。然后，输入以下代码：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Put the data into a Spark DataFrame:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据放入Spark DataFrame中：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The next step is to apply a structure to the data so that you can use structured
    streaming:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是对数据应用结构，以便您可以使用结构化流处理：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The final step is to apply the schema to a DataFrame. This allows you to work
    with the data as if it were a table:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是将架构应用到DataFrame中。这样可以使您像处理表格一样处理数据：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we connected to IoT Hub and put the data into a DataFrame. Later,
    we added a structure to that frame, allowing us to query data similarly to the
    way we query a database table.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们连接到了 IoT Hub 并将数据放入了一个 DataFrame 中。稍后，我们为该数据框架添加了结构，使我们能够像查询数据库表一样查询数据。
- en: In the next few chapters, we will discuss how to create models. After creating
    models using cold path data, you can perform near-real-time machine learning on
    it by pushing those trained models into Databricks structured streaming.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将讨论如何创建模型。在使用冷路径数据创建模型之后，您可以通过将这些训练好的模型推送到 Databricks 结构化流中，实现几乎实时的机器学习。
