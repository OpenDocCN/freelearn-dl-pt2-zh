- en: 8 Neural Style Transfer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 神经风格转移
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区 Discord
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file64.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file64.png)'
- en: In the previous chapter, we started exploring generative models using PyTorch.
    We built machine learning models that can generate text and music by training
    the models without supervision on text and music data, respectively. We will continue
    exploring generative modeling in this chapter by applying a similar methodology
    to image data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们开始探索使用 PyTorch 的生成模型。我们构建了可以在文本和音乐数据上无监督训练的机器学习模型，从而能够生成文本和音乐。在本章中，我们将继续探索生成建模，通过类似的方法应用于图像数据。
- en: 'We will mix different aspects of two different images, **A** and **B**, to
    generate a resultant image, **C**, that contains the content of image **A** and
    the style of image **B**. This task is also popularly known as **neural style
    transfer** because, in a way, we are transferring the style of image **B** to
    image **A** in order to achieve image **C**, as illustrated in the following figure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将混合两幅不同图像**A**和**B**的不同方面，生成一幅结果图像**C**，其中包含图像**A**的内容和图像**B**的风格。这项任务也被称为**神经风格转移**，因为在某种程度上，我们正在将图像**B**的风格转移到图像**A**，以实现图像**C**，如下图所示：
- en: '![Figure 8.1 – Neural style transfer example](img/file65.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 神经风格转移示例](img/file65.jpg)'
- en: Figure 8.1 – Neural style transfer example
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 神经风格转移示例
- en: First, we will briefly discuss how to approach this problem and understand the
    idea behind achieving style transfer. Using PyTorch, we will then implement our
    own neural style transfer system and apply it to a pair of images. Through this
    implementation exercise, we will also try to understand the effects of different
    parameters in the style transfer mechanism.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将简要讨论如何解决这个问题，并理解实现风格转移背后的想法。然后，我们将使用 PyTorch 实现自己的神经风格转移系统，并将其应用于一对图像。通过这个实现练习，我们还将试图理解风格转移机制中不同参数的影响。
- en: By the end of this chapter, you will understand the concepts behind neural style
    transfer and be able to build and test your own neural style transfer model using
    PyTorch.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章末，您将理解神经风格转移背后的概念，并能够使用 PyTorch 构建和测试自己的神经风格转移模型。
- en: 'This chapter covers the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Understanding how to transfer style between images
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何在图像之间转移风格
- en: Implementing neural style transfer using PyTorch
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现神经风格转移
- en: Understanding how to transfer style between images
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何在图像之间转移风格
- en: In *Chapter 3*, *Deep CNN Architectures*, we discussed **convolutional neural
    networks** (**CNNs**) in detail. CNNs are largely the most successful class of
    models when working with image data. We have seen how CNN-based architectures
    are among the best-performing architectures of neural networks on tasks such as
    image classification, object detection, and so on. One of the core reasons behind
    this success is the ability of convolutional layers to learn spatial representations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*深度 CNN 结构*中，我们详细讨论了**卷积神经网络**（**CNNs**）。当处理图像数据时，CNNs 是最成功的模型类之一。我们已经看到，基于
    CNN 的架构在图像分类、物体检测等任务上是表现最佳的神经网络架构之一。这一成功的核心原因之一是卷积层学习空间表示的能力。
- en: For example, in a dog versus cat classifier, the CNN model is essentially able
    to capture the content of an image in its higher-level features, which helps it
    detect dog-specific features against cat-specific features. We will leverage this
    ability of an image classifier CNN to grasp the content of an image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在狗与猫分类器中，CNN 模型基本上能够捕捉图像中的内容在其更高级别的特征中，这帮助它检测狗特有的特征与猫特有的特征。我们将利用图像分类器 CNN
    的这种能力来把握图像的内容。
- en: We know that VGG is a powerful image classification model, as discussed in *Chapter
    3*, *Deep CNN Architectures*. We are going to use the convolutional part of the
    VGG model (excluding the linear layers) to extract content-related features from
    an image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 VGG 是一个强大的图像分类模型，如*第 3 章*，*深度 CNN 结构*中所述。我们将使用 VGG 模型的卷积部分（不包括线性层）来从图像中提取与内容相关的特征。
- en: We know that each convolutional layer produces, say, *N* feature maps of dimensions
    *X*Y* each. For example, let's say we have a single channel (grayscale) input
    image of size (3,3) and a convolutional layer where the number of output channels
    (*N*) is 3, the kernel size is (2,2) with a stride of (1,1), and there's no padding.
    This convolutional layer will produce 3 (*N*) feature maps each of size 2x2, hence
    *X*=2 and *Y*=2 in this case.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每个卷积层都会生成 *N* 个尺寸为 *X*Y* 的特征图。例如，假设我们有一个单通道（灰度）输入图像尺寸为（3,3），一个卷积层的输出通道数
    (*N*) 为3，核大小为（2,2），步幅为（1,1），且没有填充。这个卷积层将产生3个尺寸为2x2的特征图，因此在这种情况下 *X*=2，*Y*=2。
- en: 'We can represent these *N* feature maps produced by the convolutional layer
    as a 2D matrix of size *N*M*, where *M=X*Y*. By defining the output of each convolutional
    layer as a 2D matrix, we can define a loss function that''s attached to each convolutional
    layer. This loss function, called the **content loss**, is the squared loss between
    the expected and predicted outputs of the convolutional layers, as demonstrated
    in the following diagram, with *N*=3, *X*=2, and *Y*=2:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将卷积层产生的这些 *N* 个特征图表示为大小为 *N*M* 的2D矩阵，其中 *M=X*Y*。通过定义每个卷积层的输出为2D矩阵，我们可以定义一个损失函数，将其附加到每个卷积层上。这个损失函数称为**内容损失**，是预期输出与卷积层预测输出之间的平方损失，如下图所示，其中
    *N*=3，*X*=2，*Y*=2：
- en: '![Figure 8\. 2 – Content loss schematic](img/file66.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 2 – 内容损失示意图](img/file66.jpg)'
- en: Figure 8\. 2 – Content loss schematic
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 2 – 内容损失示意图
- en: As we can see, the input image (image *C*, as per our notation in *Figure 8.*
    *1*) in this example is transformed into **three feature maps** by the **convolutional**
    (**conv**) **layer**. These three feature maps, of size 2x2 each, are formatted
    into a 3x4 matrix. This matrix is compared with the expected output, which is
    obtained by passing image *A* (the content image) through the same flow. The pixel-wise
    squared summed loss is then calculated, which we call the **content loss**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，输入图像（图像 *C*，如我们在 *图 8.* *1* 中的标记）在本示例中通过**卷积层**转换为**三个特征图**。这三个尺寸为2x2的特征图每个都被格式化为一个3x4的矩阵。该矩阵与通过相同流程将图像
    *A*（内容图像）通过的预期输出进行比较。然后计算像素逐点的平方和损失，我们称之为**内容损失**。
- en: 'Now, for extracting style from an image, we will use gram matrices [8.1] derived
    from the inner product between the rows of the reduced 2D matrix representations,
    as demonstrated in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了从图像中提取风格，我们将使用由减少的2D矩阵表示的行之间内积得出的格拉姆矩阵 [8.1]，如下图所示：
- en: '![Figure 8\. 3 – Style loss schematic](img/file67.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 3 – 风格损失示意图](img/file67.jpg)'
- en: Figure 8\. 3 – Style loss schematic
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 3 – 风格损失示意图
- en: The **gram matrix** computation is the only extra step here compared to the
    content loss calculations. Also, as we can see, the output of the pixel-wise squared
    summed loss is quite a large number compared to the content loss. Hence, this
    number is normalized by dividing it by *N*X*Y*; that is, the number of feature
    maps (*N*) times the length (*X*) times the breadth (*Y*) of a feature map. This
    also helps standardize the **style loss** metric across different convolutional
    layers, which have a different *N*, *X*, and *Y*. Details of the implementation
    can be found in the original paper that introduced neural style transfer [8.2]
    .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与内容损失计算相比，格拉姆矩阵的计算是唯一的额外步骤。同时，正如我们所见，像素逐点平方和损失的输出数值相比内容损失而言相当大。因此，通过将其除以
    *N*X*Y*，即特征图的数量 (*N*) 乘以长度 (*X*) 乘以宽度 (*Y*)，来对这个数值进行标准化。这也有助于在具有不同 *N*、*X* 和 *Y*
    的不同卷积层之间标准化**风格损失**指标。关于实现的详细信息可以在引入神经风格迁移的原始论文 [8.2] 中找到。
- en: 'Now that we understand the concept of content and style loss, let''s take a
    look at how neural style transfer works, as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了内容和风格损失的概念，让我们来看看神经风格迁移的工作原理，如下所示：
- en: For the given VGG (or any other CNN) network, we define which convolutional
    layers in the network should have a content loss attached to them. Repeat this
    exercise for style loss.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的VGG（或任何其他CNN）网络，我们定义网络中哪些卷积层应该附加内容损失。重复此操作以进行风格损失。
- en: Once we have those lists, we pass the content image through the network and
    compute the expected convolutional outputs (2D matrices) at the convolutional
    layers where the content loss is to be calculated.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了这些列表，我们将内容图像通过网络，并计算在应计算内容损失的卷积层处的预期卷积输出（2D矩阵）。
- en: Next, we pass the style image through the network and compute the expected gram
    matrices at the convolutional layers. This is where the style loss is to be calculated,
    as demonstrated in the following diagram.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将风格图像通过网络并在卷积层计算预期的格拉姆矩阵。这就是风格损失将被计算的地方，如下图所示。
- en: 'In the following diagram, for example, the content loss is to be calculated
    at the second and third convolutional layers, while the style loss is to be calculated
    at the second, third, and fifth convolutional layers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，例如，将在第二和第三个卷积层计算内容损失，同时在第二、第三和第五个卷积层计算风格损失：
- en: '![Figure 8\. 4 – Style transfer architecture schematic](img/file68.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 4 – 风格转移架构图示](img/file68.jpg)'
- en: Figure 8\. 4 – Style transfer architecture schematic
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 4 – 风格转移架构图示
- en: Now that we have the content and style targets at the decided convolutional
    layers, we are all set to generate an image that contains the content of the content
    image and the style of the style image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在决定的卷积层具有内容和风格目标后，我们准备好生成一幅图像，其中包含内容图像的内容和风格图像的风格。
- en: For initialization, we can either use a random noise matrix as our starting
    point for the generated image, or directly use the content image to start with.
    We pass this image through the network and compute the style and content losses
    at the pre-selected convolutional layers. We add style losses to get the total
    style loss and content losses to get the total content loss. Finally, we obtain
    a total loss by summing these two components in a weighted fashion.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初始化，我们可以使用随机噪声矩阵作为生成图像的起始点，或直接使用内容图像作为起点。我们将此图像通过网络并在预选卷积层计算风格和内容损失。我们将风格损失相加以获得总风格损失，并将内容损失相加以获得总内容损失。最后，通过加权的方式将这两个组件相加，我们获得总损失。
- en: If we give more weight to the style component, the generated image will have
    more style reflected on it and vice versa. Using gradient descent, we backpropagate
    the loss all the way back to the input in order to update our generated image.
    After a few epochs, the generated image should evolve in a way that it produces
    the content and style representations that minimize the respective losses, thereby
    producing a style transferred image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更注重风格组件，生成的图像将更多地反映其风格，反之亦然。使用梯度下降，我们将损失反向传播到输入，以更新我们生成的图像。几个时期后，生成的图像应该以一种方式演变，以产生最小化相应损失的内容和风格表示，从而产生风格转移的图像。
- en: In the preceding diagram, the pooling layer is average pooling-based instead
    of the traditional max pooling. Average pooling is deliberately used for style
    transfer to ensure smooth gradient flow. We want the generated images not to have
    sharp changes between pixels. Also, it is worth noticing that the network in the
    preceding diagram ends at the layer where the last style or content loss is calculated.
    Hence, in this case, because there is no loss associated with the sixth convolutional
    layer of the original network, it is meaningless to talk about layers beyond the
    fifth convolutional layer in the context of style transfer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，池化层是基于平均池化而不是传统的最大池化。平均池化被有意地用于风格转移，以确保平滑的梯度流。我们希望生成的图像不会在像素之间产生剧烈变化。此外，值得注意的是，前面图表中的网络在计算最后一个风格或内容损失的层结束。因此，在这种情况下，因为原始网络的第六个卷积层没有关联的损失，所以在风格转移的背景下谈论第五个卷积层之后的层是没有意义的。
- en: In the next section, we will implement our own neural style transfer system
    using PyTorch. With the help of a pre-trained VGG model, we will use the concepts
    we've discussed in this section to generate artistically styled images. We will
    also explore the impact of tuning the various model parameters on the content
    and texture/style of generated images.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用PyTorch实现自己的神经风格转移系统。借助预训练的VGG模型，我们将使用本节讨论的概念生成艺术风格的图像。我们还将探讨调整各种模型参数对生成图像的内容和纹理/风格的影响。
- en: Implementing neural style transfer using PyTorch
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch实现神经风格转移
- en: Having discussed the internals of a neural style transfer system, we are all
    set to build one using PyTorch. In the form of an exercise, we will load a style
    and a content image. Then, we will load the pre-trained VGG model. After defining
    which layers to compute the style and content loss on, we will trim the model
    so that it only retains the relevant layers. Finally, we will train the neural
    style transfer model in order to refine the generated image epoch by epoch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了神经风格迁移系统的内部之后，我们已经准备好使用PyTorch构建一个系统。作为练习，我们将加载一个风格图像和一个内容图像。然后，我们将加载预训练的VGG模型。在定义要计算风格和内容损失的层之后，我们将修剪模型，使其仅保留相关层。最后，我们将训练神经风格迁移模型，逐步改进生成的图像。
- en: Loading the content and style images
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载内容和风格图像
- en: 'In this exercise, we will only show the important parts of the code for demonstration
    purposes. To access the full code, go to our github repository [8.3] . Follow
    these steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们只会展示代码的重要部分以示例。要获取完整的代码，请访问我们的 github 代码库 [8.3] 。请按照以下步骤进行：
- en: 'Firstly, we need to import the necessary libraries :'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的库：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Amng other libraries, we import the `torchvision` library to load the pre-trained
    VGG model and other computer vision-related utilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其他库外，我们导入`torchvision`库以加载预训练的VGG模型和其他计算机视觉相关的工具。
- en: 'Next, we need a style and a content image. We will use the unsplash website
    [8.4] to download an image of each kind. The downloaded images are included in
    the code repository of this book. In the following code, we are writing a function
    that will load the images as tensors:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个风格图像和一个内容图像。我们将使用unsplash网站 [8.4] 下载这两种图像。这些下载的图像已包含在本书的代码库中。在下面的代码中，我们编写一个函数来将图像加载为张量：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This should give us the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 8\. 5 – Style and content images](img/file69.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 5 – 风格和内容图像](img/file69.jpg)'
- en: Figure 8\. 5 – Style and content images
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 5 – 风格和内容图像
- en: So, the content image is a real-life photograph of the *Taj Mahal*, whereas
    the style image is an art painting. Using style transfer, we hope to generate
    an artistic *Taj Mahal* painting. However, before we do that, we need to load
    and trim the VGG19 model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，内容图像是*泰姬陵*的真实照片，而风格图像是一幅艺术画作。通过风格迁移，我们希望生成一幅艺术性的*泰姬陵*画作。然而，在此之前，我们需要加载并修剪VGG19模型。
- en: Loading and trimming the pre-trained VGG19 model
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载并修剪预训练的VGG19模型
- en: 'In this part of the exercise, we will use a pre-trained VGG model and retain
    its convolutional layers. We will make some minor changes to the model to make
    it usable for neural style transfer. Let''s get started:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分练习中，我们将使用预训练的VGG模型并保留其卷积层。我们将对模型进行一些小的更改，使其适用于神经风格迁移。让我们开始吧：
- en: 'We will first load the pre-trained VGG19 model and use its convolutional layers
    to generate the content and style targets to yield the content and style losses,
    respectively:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载预训练的VGG19模型，并使用其卷积层生成内容和风格目标，从而产生内容和风格损失：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should be as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 8\. 6 – VGG19 model](img/file70.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 6 – VGG19 模型](img/file70.jpg)'
- en: Figure 8\. 6 – VGG19 model
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 6 – VGG19 模型
- en: 'We do not need the linear layers; that is, we only need the convolutional part
    of the model. In the preceding code, this can be achieved by only retaining the
    `features` attribute of the model object, as follows:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不需要线性层；也就是说，我们只需要模型的卷积部分。在前面的代码中，可以通过仅保留模型对象的`features`属性来实现：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this exercise, we are not going to tune the parameters of the VGG model.
    All we are going to tune is the pixels of the generated image, right at the input
    end of the model. Hence, we will ensure that the parameters of the loaded VGG
    model are fixed.
  id: totrans-62
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个练习中，我们不会调整VGG模型的参数。我们只会调整生成图像的像素，即模型输入端。因此，我们将确保加载的VGG模型的参数是固定的。
- en: 'We must freeze the parameters of the VGG model with the following code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须使用以下代码冻结VGG模型的参数：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we''ve loaded the relevant section of the VGG model, we need to change
    the `maxpool` layers into average pooling layers, as discussed in the previous
    section. While doing so, we will take note of where the convolutional layers are
    located in the model:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经加载了VGG模型的相关部分，我们需要将`maxpool`层改为平均池化层，如前面讨论的那样。在此过程中，我们将注意到模型中卷积层的位置：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output should be as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 8\. 7 – Modified VGG19 model](img/file71.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 7 – 修改后的VGG19 模型](img/file71.jpg)'
- en: Figure 8\. 7 – Modified VGG19 model
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 7 – 修改后的VGG19模型
- en: As we can see, the linear layers have been removed and the max pooling layers
    have been replaced by average pooling layers, as indicated by the red boxes in
    the preceding figure.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，线性层已被移除，并且最大池化层已被替换为平均池化层，如前图中的红色框所示。
- en: In the preceding steps, we loaded a pre-trained VGG model and modified it in
    order to use it as a neural style transfer model. Next, we will transform this
    modified VGG model into a neural style transfer model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们加载了一个预训练的VGG模型，并对其进行了修改，以便将其用作神经风格迁移模型。接下来，我们将把这个修改后的VGG模型转换成一个神经风格迁移模型。
- en: Building the neural style transfer model
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建神经风格迁移模型
- en: 'At this point, we can define which convolutional layers we want the content
    and style losses to be calculated on. In the original paper, style loss was calculated
    on the first five convolutional layers, while content loss was calculated on the
    fourth convolutional layer only. We will follow the same convention, although
    you are welcome to try out different combinations and observe their effects on
    the generated image. Follow these steps:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以定义希望计算内容和风格损失的卷积层。在原始论文中，风格损失是在前五个卷积层上计算的，而内容损失仅在第四个卷积层上计算。我们将遵循相同的惯例，尽管您可以尝试不同的组合并观察它们对生成图像的影响。请按照以下步骤进行：
- en: 'First, we list the layers we need to have the style and content loss on:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们列出我们需要在其上进行风格和内容损失的层：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we have defined the first to fifth convolutional layers, which are attached
    to the style loss, and the fourth convolutional layer, which is attached to the
    content loss.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了第一到第五个卷积层，这些层与风格损失相关联，并且第四个卷积层与内容损失相关联。
- en: 'Now, let''s remove the unnecessary parts of the VGG model. We shall only retain
    it until the fifth convolutional layer, as shown here:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们删除VGG模型中不必要的部分。我们将仅保留它到第五个卷积层，如下所示：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This should give us the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们以下输出：
- en: '![Figure 8\. 8 – Neural style transfer model object](img/file72.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 8 – 神经风格迁移模型对象](img/file72.jpg)'
- en: Figure 8\. 8 – Neural style transfer model object
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 8 – 神经风格迁移模型对象
- en: As we can see, we have transformed the VGG model with 16 convolutional layers
    into a neural style transfer model with five convolutional layers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们已经将具有16个卷积层的VGG模型转换为具有五个卷积层的神经风格迁移模型。
- en: Training the style transfer model
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练风格迁移模型
- en: 'In this section, we''ll start working on the image that will be generated.
    We can initialize this image in many ways, such as by using a random noise image
    or using the content image as the initial image. Currently, we are going to start
    with random noise. Later, we will also see how using the content image as the
    starting point impacts the results. Follow these steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始处理将生成的图像。我们可以通过多种方式初始化这个图像，例如使用随机噪声图像或使用内容图像作为初始图像。目前，我们将从随机噪声开始。稍后，我们还将看到使用内容图像作为起点对结果的影响。请按照以下步骤进行：
- en: 'The following code demonstrates the process of initializing a `torch` tensor
    with random numbers:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码演示了使用随机数初始化`torch`张量的过程：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This should give us the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们以下输出：
- en: '![Figure 8\. 9 – Random noise image](img/file73.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 9 – 随机噪声图像](img/file73.jpg)'
- en: Figure 8\. 9 – Random noise image
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 9 – 随机噪声图像
- en: 'Finally, we can start the model training loop. First, we will define the number
    of epochs to train for, the relative weightage to provide for the style and content
    losses, and instantiate the Adam optimizer for gradient descent-based optimization
    with a learning rate of `0.1`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始模型训练循环。首先，我们将定义训练的时代数，为风格和内容损失提供的相对权重，并使用学习率为`0.1`的Adam优化器进行基于梯度下降的优化实例化：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Upon starting the training loop, we initialize the style and content losses
    to zero at the beginning of the epoch, and then clip the pixel values of the input
    image between `0` and `1` for numerical stability:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始训练循环时，我们在时代开始时将风格和内容损失初始化为零，然后为了数值稳定性将输入图像的像素值剪切在`0`和`1`之间。
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At this stage, we have reached a crucial step in the training iteration. Here,
    we must calculate the style and content losses for each of the pre-defined style
    and content convolutional layers. The individual style losses and content losses
    for each of the respective layers are added together to get the total style and
    content loss for the current epoch:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经达到了训练迭代的关键步骤。在这里，我们必须计算每个预定义的风格和内容卷积层的风格和内容损失。将各自层的单独风格损失和内容损失相加，得到当前时代的总风格和内容损失：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As shown in the preceding code, for both the style and content losses, first,
    we compute the style and content targets (ground truths) using the style and content
    image. We use `.detach()` for the targets to indicate that these are not trainable
    but just fixed target values. Next, we compute the predicted style and content
    outputs based on the generated image as input, at each of the style and content
    layers. Finally, we compute the style and content losses.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，对于风格和内容损失，首先，我们使用风格和内容图像计算风格和内容目标（地面真值）。我们使用`.detach()`来表示这些目标不可训练，而只是固定的目标值。接下来，我们根据生成的图像作为输入，在每个风格和内容层计算预测的风格和内容输出。最后，我们计算风格和内容损失。
- en: 'For the style loss, we also need to compute the gram matrix using a pre-defined
    gram matrix function, as shown in the following code:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于风格损失，我们还需要使用预定义的Gram矩阵函数来计算Gram矩阵，如下面的代码所示：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we mentioned earlier, we can compute an inner dot product using the `torch.mm`
    function. This computes the gram matrix and normalizes the matrix by dividing
    it by the number of feature maps times the width times the height of each feature
    map.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们可以使用`torch.mm`函数计算内部点积。这将计算Gram矩阵并通过特征映射数乘以每个特征映射的宽度和高度来归一化矩阵。
- en: 'Moving on in our training loop, now that we''ve computed the total style and
    content losses, we need to compute the final total loss as a weighted sum of these
    two, using the weights we defined earlier:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的训练循环中继续进行，现在我们已经计算出了总风格和内容损失，我们需要计算最终的总损失，作为这两者的加权和，使用我们之前定义的权重：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, at every *k* epochs, we can see the progression of our training by
    looking at the losses as well as looking at the generated image. The following
    figure shows the evolution of the generated style transferred image for the previous
    code for a total of 180 epochs recorded at every 20 epochs:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在每*k*个时代，我们可以通过查看损失以及查看生成的图像来看到我们训练的进展。以下图表显示了前一个代码的生成风格转移图像的演变，总共记录了180个时代，每20个时代一次：
- en: '![Figure 8\. 10 – Neural style transfer epoch-wise generated image](img/file74.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8\. 10 – 神经风格转移逐时代生成的图像](img/file74.jpg)'
- en: Figure 8\. 10 – Neural style transfer epoch-wise generated image
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 10 – 神经风格转移逐时代生成的图像
- en: 'It is quite clear that the model begins by applying the style from the style
    image to the random noise. As training proceeds, the content loss starts playing
    its role, thereby imparting content to the styled image. By epoch **180**, we
    can see the generated image, which looks like a good approximation of an artistic
    painting of the *Taj Mahal*. The following graph shows the decreasing style and
    content losses as the epochs progress from **0** to **180**:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，模型开始时将风格从风格图像应用于随机噪声。随着训练的进行，内容损失开始发挥作用，从而为风格化图像赋予内容。到第**180**个时代，我们可以看到生成的图像，看起来像是塔吉马哈尔的艺术绘画的良好近似。以下图表显示了从**0**到**180**个时代随着时代的推移逐渐减少的风格和内容损失：
- en: '![Figure 8\. 11 – Style and content loss curves](img/file75.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图8\. 11 – 风格和内容损失曲线](img/file75.jpg)'
- en: Figure 8\. 11 – Style and content loss curves
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 11 – 风格和内容损失曲线
- en: Noticeably, the style loss sharply goes down initially, which is also evident
    in *Figure 8.* *10* in that the initial epochs mark the imposition of style on
    the image more than the content. At the advanced stages of training, both losses
    decline together gradually, resulting in a style transferred image, which is a
    decent compromise between the artwork of the style image and the realism of a
    photograph that's been taken with a camera.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，风格损失在最初急剧下降，这也在*图8.* *10*中有所体现，即初始时期更多地将风格施加在图像上而不是内容。在训练的高级阶段，两种损失逐渐下降，导致风格转移图像，这是风格图像艺术性和以相机拍摄的照片逼真性之间的一个不错的折衷。
- en: Experimenting with the style transfer system
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对风格转移系统进行实验
- en: 'Having successfully trained a style transfer system in the previous section,
    we will now look at how the system responds to different hyperparameter settings.
    Follow these steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节成功训练了样式迁移系统后，我们现在将看看系统如何响应不同的超参数设置。按照以下步骤进行：
- en: 'In the preceding section, we set the content weight to `1` and the style weight
    to `1e6`. Let''s increase the style weight 10x further – that is, to `1e7` – and
    observe how it affects the style transfer process. Upon training with the new
    weights for 600 epochs, we get the following progression of style transfer:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前一节中，我们将内容权重设置为`1`，将样式权重设置为`1e6`。让我们进一步增加样式权重10倍，即到`1e7`，并观察它如何影响样式迁移过程。使用新权重进行600个时期的训练后，我们得到了以下样式迁移的进展：
- en: '![Figure 8\. 12 – Style transfer epochs with higher style weights](img/file76.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 12 – 高风格权重的样式迁移时期](img/file76.jpg)'
- en: Figure 8\. 12 – Style transfer epochs with higher style weights
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 12 – 高风格权重的样式迁移时期
- en: Here, we can see that initially, it required many more epochs than in the previous
    scenario to reach a reasonable result. More importantly, the higher style weight
    does seem to have an effect on the generated image. When we look at the images
    in the preceding figure compared to the ones in *Figure 8.* *10*, we find that
    the former have a stronger resemblance to the style image shown in *Figure 8.*
    *5*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们可以看到，与之前的情况相比，最初需要更多的时期才能达到合理的结果。更重要的是，较高的样式权重似乎对生成的图像有影响。当我们将前一张图像与*图 8.*
    *10*中的图像进行比较时，我们发现前者更像*图 8.* *5*中展示的样式图像。
- en: 'Likewise, reducing the style weight from `1e6` to `1e5` produces a more content-focused
    result, as can be seen in the following screenshot:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，将样式权重从`1e6`减少到`1e5`会产生更加注重内容的结果，如下图所示：
- en: '![Figure 8\. 13 – Style transfer epochs with lower style weights](img/file77.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 13 – 低风格权重的样式迁移时期](img/file77.jpg)'
- en: Figure 8\. 13 – Style transfer epochs with lower style weights
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 13 – 低风格权重的样式迁移时期
- en: Compared to the scenario with a higher style weight, having a lower style weight
    means it takes far fewer epochs to get a reasonable-looking result. The amount
    of style in the generated image is much smaller and is mostly filled with the
    content image data. We only trained this scenario for 6 epochs as the results
    saturate after that point.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与较高样式权重的情况相比，降低样式权重意味着需要更少的时期才能得到看起来合理的结果。生成图像中的样式量要小得多，主要填充了内容图像数据。我们仅对此情况进行了6个时期的训练，因为在那之后结果就会饱和。
- en: 'A final change could be to initialize the generated image with the content
    image instead of the random noise, while using the original style and content
    weights of `1e6` and `1`, respectively. The following figure shows the epoch-wise
    progression in this scenario:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的改变可能是将生成的图像初始化为内容图像，而不是随机噪声，同时使用原始的样式和内容权重`1e6`和`1`。以下图显示了这种情况下的时期逐步进展：
- en: '![Figure 8\. 14 – Style transfer epochs with content image initialization](img/file78.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 14 – 使用内容图像初始化的样式迁移时期](img/file78.jpg)'
- en: Figure 8\. 14 – Style transfer epochs with content image initialization
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 14 – 使用内容图像初始化的样式迁移时期
- en: 'By comparing the preceding figure to *Figure 8.* *10*, we can see that having
    the content image as a starting point gives us a different path of progression
    to getting a reasonable style transferred image. It seems that both the content
    and style components are being imposed on the generated image more simultaneously
    than in *Figure 8.* *10*, where the style got imposed first, followed by the content.
    The following graph confirms this hypothesis:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较前一张图与*图 8.* *10*，我们可以看到，将内容图像作为起点确实为我们得到合理的样式迁移图像提供了不同的进展路径。似乎生成图像上同时施加了内容和样式组件，而不像*图
    8.* *10*中那样，先施加样式，然后是内容。以下图表证实了这一假设：
- en: '![Figure 8\. 15 – Style and content loss curves with content image initialization](img/file79.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 8\. 15 – 使用内容图像初始化的样式和内容损失曲线](img/file79.jpg)'
- en: Figure 8\. 15 – Style and content loss curves with content image initialization
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 15 – 使用内容图像初始化的样式和内容损失曲线
- en: As we can see, both style and content losses are decreasing together as the
    epochs progress, eventually saturating toward the end. Nonetheless, the end results
    in both *Figures 8.* *10* and 8\. *14* or even *Figures 8.* *12* and 8\. *13*
    all represent reasonable artistic impressions of the *Taj Mahal*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，随着训练周期的推进，风格损失和内容损失一起减少，最终朝向饱和状态发展。尽管如此，*图8.* *10*和8\. *14*甚至*图8.*
    *12*和8\. *13*的最终结果都展示了*泰姬陵*的合理艺术印象。
- en: We have successfully built a neural style transfer model using PyTorch, wherein
    using a content image – a photograph of the beautiful *Taj Mahal* – and a style
    image – a canvas painting – we generated a reasonable approximation of an artistic
    painting of the *Taj Mahal*. This application can be extended to various other
    combinations. Swapping the content and style images could also produce interesting
    results and give more insight into the inner workings of the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地使用PyTorch构建了一个神经风格转移模型，在这个模型中，使用了一个内容图像——*泰姬陵*的照片——和一个风格图像——一幅画布绘画——我们生成了*泰姬陵*的一个合理的艺术画作近似。这个应用可以扩展到各种其他组合。交换内容和风格图像也可能产生有趣的结果，并更深入地了解模型的内部工作原理。
- en: 'You are encouraged to extend the exercise we discussed in this chapter by doing
    the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您通过以下方式扩展本章中讨论的练习：
- en: Changing the list of style and content layers
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改风格和内容层列表
- en: Using larger image sizes
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的图像尺寸
- en: Trying more combinations of style and content loss weights
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试更多的风格和内容损失权重组合
- en: Using other optimizers, such as SGD and LBFGS
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他优化器，如SGD和LBFGS
- en: Training for longer epochs with different learning rates, in order to observe
    the differences in the generated images across all these approaches
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的学习率进行更长的训练周期，以便观察所有这些方法生成的图像之间的差异
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we applied the concept of generative machine learning to images
    by generating an image that contains the content of one image and the style of
    another – a task known as neural style transfer. In the next chapter, we will
    expand on this paradigm, where we'll have a generator that generates *fake* data
    and there is a discriminator that tells apart *fake* data from *real* data. Such
    models are popularly known as **generative adversarial networks (GANs)**. We will
    be exploring deep convolutional GANs (DCGANs) in the next chapter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将生成式机器学习的概念应用于图像，通过生成一幅包含一张图像内容和另一张风格的图像，这被称为神经风格转移的任务。在下一章中，我们将扩展这一范式，我们将拥有一个生成器生成*虚假*数据，还有一个鉴别器区分*虚假*数据和*真实*数据。这样的模型通常被称为**生成对抗网络（GANs）**。在下一章中，我们将探索深度卷积GANs（DCGANs）。
