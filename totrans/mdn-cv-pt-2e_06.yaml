- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Introducing Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入卷积神经网络
- en: 'So far, we’ve learned how to build deep neural networks and the impact of tweaking
    their various hyperparameters. In this chapter, we will learn about where traditional
    deep neural networks do not work. We’ll then learn about the inner workings of
    **convolutional neural networks** (**CNNs**) by using a toy example before understanding
    some of their major hyperparameters, including stride, pooling, and filters. Next,
    we will leverage CNNs, along with various data augmentation techniques, to solve
    the issue of traditional deep neural networks not having good accuracy. Following
    this, we will learn what the outcome of a feature learning process in a CNN looks
    like. Finally, we’ll bring our learning together to solve a use case: we’ll be
    classifying an image by stating whether the image contains a dog or a cat. By
    doing this, we’ll be able to understand how the accuracy of prediction varies
    by the amount of data available for training.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学习了如何构建深度神经网络以及调整它们的各种超参数所产生的影响。在本章中，我们将了解传统深度神经网络无法解决的问题。然后，我们将通过一个玩具例子了解**卷积神经网络**（**CNNs**）的内部工作原理，然后理解它们的一些主要超参数，包括步幅、池化和滤波器。接下来，我们将利用CNN以及各种数据增强技术来解决传统深度神经网络精度不高的问题。然后，我们将了解CNN中特征学习过程的结果是什么样子。最后，我们将把学到的东西结合起来解决一个用例：我们将通过声明图像中包含狗还是猫来分类图像。通过这样做，我们将能够理解预测准确性如何随着用于训练的数据量的变化而变化。
- en: By the end of this chapter, you will have a deep understanding of CNNs, which
    form the backbone of multiple model architectures that are used for various tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章末尾，您将对CNN有深入的理解，CNN是多个模型架构的基础，用于各种任务。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The problem with traditional deep neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统深度神经网络的问题
- en: Building blocks of a CNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN的构建模块
- en: Implementing a CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 CNN
- en: Classifying images using deep CNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度CNN分类图像
- en: Implementing data augmentation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据增强
- en: Visualizing the outcome of feature learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化特征学习的结果
- en: Building a CNN for classifying real-world images
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了对真实世界图像进行分类构建CNN
- en: Let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'All code in this chapter is available for reference in the `Chapter04` folder
    of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码都可在此书的GitHub存储库的`Chapter04`文件夹中找到：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: The problem with traditional deep neural networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统深度神经网络的问题
- en: Before we dive into CNNs, let’s look at the major problem that’s faced when
    using traditional deep neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究CNN之前，让我们看看在使用传统深度神经网络时面临的主要问题。
- en: 'Let’s reconsider the model we built on the Fashion-MNIST dataset in *Chapter
    3*. We will fetch a random image and predict the class that corresponds to that
    image, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新考虑我们在*第 3 章*中在Fashion-MNIST数据集上构建的模型。我们将获取一个随机图像并预测与该图像对应的类别，如下所示：
- en: The following code can be found in the `Issues_with_image_translation.ipynb`
    file located in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Only the additional code corresponding to the issue of image translation will
    be discussed here for brevity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在GitHub上`Chapter04`文件夹中的`Issues_with_image_translation.ipynb`文件中找到：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。这里仅讨论与图像翻译问题对应的附加代码，以保持简洁。
- en: 'Fetch a random image from the available training images:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取可用训练图像中的随机图像：
- en: '[PRE0]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code results in the following output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将产生以下输出：
- en: '![](img/B18457_04_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_01.png)'
- en: 'Figure 4.1: Image corresponding to index 24300'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：与索引 24300 相对应的图像
- en: 'Pass the image through the **trained model** (continue using the model we trained
    in the *Batch size of 32* section of *Chapter 3*):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像通过**训练好的模型**传递（继续使用我们在*第 3 章*的*批量大小为 32*节中训练的模型）：
- en: 'Preprocess the image so it goes through the same preprocessing steps we performed
    while building the model:'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理图像，使其经过与构建模型时执行的相同预处理步骤：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Extract the probabilities associated with the various classes:'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取与各种类别相关联的概率：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code results in the following output, where we can see that the
    highest probability is for the first index, which is of the `Trouser` class:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码将产生以下输出，我们可以看到最高概率是第一个索引，即`裤子`类：
- en: '![](img/B18457_04_02.png)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18457_04_02.png)'
- en: 'Figure 4.2: Probabilities of different classes'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.2：不同类别的概率
- en: 'Translate (roll/slide) the image multiple times (one pixel at a time) from
    a translation of 5 pixels to the left to 5 pixels to the right and store the predictions
    in a list:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a list that stores predictions:'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a loop that translates (rolls) an image from -5 pixels (5 pixels to
    the left) to +5 pixels (5 pixels to the right) of the original position (which
    is at the center of the image):'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, we specified 6 as the upper bound even though we are
    interested in translating until +5 pixels since the output of the range would
    be from -5 to +5 when (-5,6) is the specified range.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocess the image as we did in *step 2*:'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Roll the image by a value equal to `px` within the `for` loop:'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we specified `axis=1` since we want the image pixels to be moving horizontally
    and not vertically.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Store the rolled image as a tensor object and register it to `device`:'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Pass `img3` through the trained model to predict the class of the translated
    (rolled) image and append it to the list that stores predictions for various translations:'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Visualize the predictions of the model for all the translations (-5 pixels
    to +5 pixels):'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code results in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Probability of each class for different translations'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: There was no change in the image’s content since we only translated the images
    from 5 pixels to the left and 5 pixels to the right. However, the predicted class
    of the image changed when the translation was beyond 2 pixels. This is because,
    while the model was being trained, the content in all the training and testing
    images was at the center. This differs from the preceding scenario where we tested
    with translated images that are off-center (by a margin of 5 pixels), resulting
    in an incorrectly predicted class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about a scenario where a traditional neural network
    fails, we will learn how CNNs help address this problem. But before we do this,
    let’s first look at the building blocks of a CNN.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks of a CNN
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are the most prominent architectures that are used when working on images.
    They address the major limitations of deep neural networks, like the one we saw
    in the previous section. Besides image classification, they also help with object
    detection, image segmentation, GANs, and much more – essentially, wherever we
    use images. Furthermore, there are different ways of constructing a CNN, and there
    are multiple pre-trained models that leverage CNNs to perform various tasks. Starting
    with this chapter, we will be using CNNs extensively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming subsections, we will understand the fundamental building blocks
    of a CNN, which are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strides and padding
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convolution is basically a multiplication between two matrices. As you saw
    in the previous chapter, matrix multiplication is a key ingredient in training
    a neural network. (We perform matrix multiplication when we calculate hidden layer
    values – which is a matrix multiplication of the input values and weight values
    connecting the input to the hidden layer. Similarly, we perform matrix multiplication
    to calculate output layer values.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that we have a solid understanding of the convolution process, let’s
    go through an example. Let’s assume we have two matrices we can use to perform
    convolution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Matrix A:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Matrix A'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Matrix B:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Matrix B'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'While performing the convolution operation, we are sliding Matrix B (the smaller
    matrix) over Matrix A (the bigger matrix). Furthermore, we are performing element-to-element
    multiplication between Matrix A and Matrix B, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {1,2,5,6} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {2,3,6,7} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*2*1 + 3*2 + 6*3 + 7*4 = 54*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {3,4,7,8} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {5,6,9,10} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*5*1 + 6*2 + 9*3 + 10*4 = 84*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {6,7,10,11} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*6*1 + 7*2 + 10*3 + 11*4 = 94*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {7,8,11,12} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*7*1 + 8*2 + 11*3 + 12*4 = 104*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {9,10,13,14} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {10,11,14,15} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*10*1 + 11*2 + 14*3 + 15*4 = 134*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {11,12,15,16} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of performing the preceding operations is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Output of convolution operation'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The smaller matrix is typically called a **filter** or a kernel, while the bigger
    matrix is the original image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Filters
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A filter is a matrix of weights that is initialized randomly at the start.
    The model learns the optimal weight values of a filter over increasing epochs.
    The concept of filters brings us to two different aspects:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: What the filters learn about
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How filters are represented
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the more filters there are in a CNN, the more features of an image
    the model can learn about. We will learn about what various filters learn in the
    *Visualizing the outcome of feature learning* section of this chapter. For now,
    we’ll proceed with the intermediate understanding that the filters learn about
    different features present in the image. For example, a certain filter might learn
    about the ears of a cat and provide high activation (a matrix multiplication value)
    when the part of the image it is convolving with contains the ear of a cat.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, when we convolved one filter that has a size of 2 x
    2 with a matrix that has a size of 4 x 4, we got an output that is 3 x 3 in dimension.
    However, if 10 different filters multiply the bigger matrix (original image),
    the result is 10 sets of the 3 x 3 output matrices.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding case, a 4 x 4 image is convolved with 10 filters that are 2
    x 2 in size, resulting in 3 x 3 x 10 output values. Essentially, when an image
    is convolved by multiple filters, the output has as many channels as there are
    filters that the image is convolved with.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in a scenario where we are dealing with color images where there
    are three channels, the filter that is convolving with the original image would
    also have three channels, resulting in a single scalar output per convolution.
    Also, if the filters are convolving with an intermediate output – let’s say 64
    x 112 x 112 in shape – the filter would have 64 channels to fetch a scalar output.
    In addition, if there are 512 filters that are convolving with the output that
    was obtained in the intermediate layer, the output post-convolution with 512 filters
    would be 512 x 112 x 112 in shape.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'To solidify our understanding of the output of filters further, let’s take
    a look at the following diagram:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_07.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Output of convolution operation with multiple filters'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see that the input image is multiplied by the
    filters that have the same depth as that of the input (which the filters are convolving
    with) and that the number of channels in the output of a convolution is as many
    as there are filters.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Strides and padding
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, each filter strode across the image – one column and
    one row at a time (after exhausting all possible columns by the end of the image).
    This also resulted in the output size being 1 pixel less than the input image
    size – both in terms of height and width. This results in a partial loss of information
    and can limit the possibility of us adding the output of the convolution operation
    to the original image if the output of convolution and the original image are
    not of the same shape. This is known as residual addition and will be discussed
    in detail in the next chapter. For now, however, let’s learn how strides and padding
    influence the output shape of convolutions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s understand the impact of strides by leveraging the same example we saw
    in the *Filters* section. We’ll move Matrix B with a stride of 2 over Matrix A.
    As a result, the output of convolution with a stride of 2 is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '{1,2,5,6} of the bigger matrix is multiplied by {1,2,3,4} of the smaller matrix:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '{3,4,7,8} of the bigger matrix is multiplied by {1,2,3,4} of the smaller matrix:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '{9,10,13,14} of the bigger matrix is multiplied by {1,2,3,4} of the smaller
    matrix:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '{11,12,15,16} of the bigger matrix is multiplied by {1,2,3,4} of the smaller
    matrix:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of performing the preceding operations is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_08.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Output of convolution with stride'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Since we now have a stride of 2, note that the preceding output has a lower
    dimension compared to the scenario where the stride was 1 (where the output shape
    was 3 x 3).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding case, we could not multiply the leftmost elements of the filter
    by the rightmost elements of the image. If we were to perform such matrix multiplication,
    we would pad the image with zeros. This would ensure that we can perform element-to-element
    multiplication of all the elements within an image with a filter.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand padding by using the same example we used in the *Convolution*
    section. Once we add padding on top of Matrix A, the revised version of Matrix
    A will look as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_09.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Padding on Matrix A'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have padded Matrix A with zeros, and the convolution with
    Matrix B will not result in the output dimension being smaller than the input
    dimension. This aspect comes in handy when we are working on a residual network
    where we must add the output of the convolution to the original image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve done this, we can perform activation on top of the convolution operation’s
    output. We could use any of the activation functions we saw in *Chapter 3* for
    this.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pooling aggregates information in a small patch. Imagine a scenario where the
    output of convolution activation is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Output of convolution operation'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The max pooling for this patch is 4 – as that is the maximum value across the
    values in the patch. Let’s understand the max pooling for a bigger matrix:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_11.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Output of convolution operation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding case, if the pooling stride has a length of 2, the max pooling
    operation is calculated as follows, where we divide the input image by a stride
    of 2 (that is, we have divided the image into 2 x 2 divisions):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_12.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Output of convolution with stride highlighted'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'For the four sub-portions of the matrix, the maximum values in the pool of
    elements are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Max pool values'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is not necessary to always have a stride of 2; this has just
    been used for illustration purposes here. Other variants of pooling are sum and
    average pooling. However, in practice, max pooling is used more often.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，并非总是需要步幅为2；这里仅用于举例说明。池化的其他变体包括求和池化和平均池化。然而，在实践中，最大池化更常用。
- en: Note that by the end of performing the convolution and pooling operations, the
    size of the original matrix is reduced from 4 x 4 to 2 x 2\. In a realistic scenario,
    if the original image is of shape 200 x 200 and the filter is of shape 3 x 3,
    the output of the convolution operation would be 198 x 198\. Following that, the
    output of the pooling operation with a stride of 2 is 99 x 99\. This way, by leveraging
    pooling, we preserve the more important features while reducing the dimension
    of inputs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过执行卷积和池化操作后，原始矩阵的大小从4 x 4减小为2 x 2。在现实情况下，如果原始图像的形状为200 x 200并且滤波器的形状为3
    x 3，则卷积操作的输出将是198 x 198。随后，步长为2的池化操作的输出将是99 x 99。这样，通过利用池化，我们保留了更重要的特征同时减少了输入的维度。
- en: Putting them all together
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将它们整合在一起
- en: So far, we have learned about convolution, filters, strides, padding, and pooling,
    and their impact on reducing the dimension of an image. Now, we will learn about
    another critical component of a CNN – the flatten layer (fully connected layer)
    – before putting the three pieces we have learned about together.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了卷积、滤波器、步幅、填充和池化，以及它们在减少图像维度中的作用。现在，我们将了解CNN的另一个关键组成部分——平坦化层（全连接层）——然后将我们学到的三个部分整合在一起。
- en: To understand the flattening process, we’ll take the output of the pooling layer
    in the previous section and flatten the output. The output of flattening the pooling
    layer is {6, 8, 14, 16}.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解平坦化过程，我们将使用前一节中池化层的输出，并对其进行平坦化处理。平坦化池化层的输出是 {6, 8, 14, 16}。
- en: By doing this, we’ll see that the flatten layer can be treated as equivalent
    to the input layer (where we flattened the input image into a 784-dimensional
    input in *Chapter 3*). Once the flatten layer’s (fully connected layer) values
    have been obtained, we can pass it through the hidden layer and then obtain the
    output for predicting the class of an image.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们会看到平坦化层可以等同于输入层（在*第三章*中我们将输入图像压平为784维输入）。一旦获取了平坦化层（全连接层）的值，我们可以将其传递到隐藏层然后获取用于预测图像类别的输出。
- en: 'The overall flow of a CNN is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的整体流程如下：
- en: '![](img/B18457_04_14.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_14.png)'
- en: 'Figure 4.14: CNN workflow'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：CNN工作流程
- en: We can see the overall flow of a CNN model, where we are passing an image through
    convolution via multiple filters and then pooling (and in the preceding case,
    repeating the convolution and pooling process twice), before flattening the output
    of the final pooling layer. This forms the **feature learning** part of the preceding
    image, where we are taking an image and reducing it to a lower dimension (the
    flattened output) while restoring the required information.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到CNN模型的整体流程，我们通过多个滤波器将图像通过卷积传递，然后通过池化（在前述情况下，重复两次卷积和池化过程），最后将最终池化层的输出进行平坦化处理。这形成了前述图像的**特征学习**部分，其中我们将图像转换为较低维度（平坦化输出），同时恢复所需的信息。
- en: The operations of convolution and pooling constitute the feature learning section,
    as filters help in extracting relevant features from images and pooling helps
    in aggregating information and thereby reducing the number of nodes at the flatten
    layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化操作构成了特征学习部分，滤波器帮助从图像中提取相关特征，池化则有助于聚合信息，从而减少在平坦化层的节点数量。
- en: If we directly flatten the input image (which is 300 x 300 pixels in size, for
    example), we are dealing with 90K input values. If we have 90K input pixel values
    and 100K nodes in a hidden layer, we are looking at ~9 billion parameters, which
    is huge in terms of computation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果直接将输入图像（例如尺寸为300 x 300像素）进行平坦化处理，我们处理的是90K个输入值。如果输入有90K个像素值并且隐藏层有100K个节点，我们需要处理约9亿个参数，这在计算上是巨大的。
- en: Convolution and pooling help in fetching a flatten layer that has a much smaller
    representation than the original image.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化有助于获取一个比原始图像更小的平坦化层表示。
- en: Finally, the last part of the classification is similar to the way we classified
    images in *Chapter 3*, where we had a hidden layer and then obtained the output
    layer.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分类的最后部分类似于我们在*第三章*中对图像进行分类的方式，那里有一个隐藏层然后获得输出层。
- en: How convolution and pooling help in image translation
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积和池化在图像翻译中的帮助
- en: When we perform pooling, we can consider the output of the operation as an abstraction
    of a region (a small patch). This phenomenon comes in handy, especially when images
    are being translated.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行池化操作时，可以将操作的输出视为一个区域（一个小补丁）的抽象。特别是在图像被翻译时，这种现象非常方便。
- en: Think of a scenario where an image is translated by 1 pixel to the left. Once
    we perform convolution, activation, and pooling on top of it, we’ll have reduced
    the dimension of the image (due to pooling), which means that a fewer number of
    pixels store the majority of the information from the original image. Moreover,
    given that pooling stores information of a region (patch), the information within
    a pixel of the pooled image would not vary, even if the original image is translated
    by 1 unit. This is because the maximum value of that region is likely to get captured
    in the pooled image.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情景，图像向左平移了1个像素。一旦我们对其执行卷积、激活和池化，我们将减少图像的维度（由于池化），这意味着较少数量的像素存储了来自原始图像的大部分信息。此外，由于池化存储了区域（补丁）的信息，即使原始图像平移了1个单位，池化图像中一个像素的信息也不会变化。这是因为该区域的最大值很可能已被捕获在池化图像中。
- en: Convolution and pooling can also help us with the **receptive field**. To understand
    the receptive field, let’s imagine a scenario where we perform a convolution +
    pooling operation twice on an image that is 100 x 100 in shape. The output at
    the end of the two convolution pooling operations is of the shape 25 x 25 (if
    the convolution operation was done with padding). Each cell in the 25 x 25 output
    now corresponds to a larger 4 x 4 portion of the original image. Thus, because
    of the convolution and pooling operations, each cell in the resulting image contains
    key information within a patch of the original image.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化还可以帮助我们实现**感受野**。要理解感受野，让我们想象一个场景，我们在一个形状为100 x 100的图像上进行两次卷积 + 池化操作。如果卷积操作进行了填充，那么在两次卷积池化操作结束时的输出形状将为25
    x 25。25 x 25输出中的每个单元格现在对应于原始图像中一个较大的4 x 4部分。因此，由于卷积和池化操作，结果图像中的每个单元格都包含原始图像的一个补丁内的关键信息。
- en: Now that we have learned about the core components of a CNN, let’s apply them
    all to a toy example to understand how they work together.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了CNN的核心组件，让我们通过一个玩具示例将它们应用起来，以理解它们如何一起工作。
- en: Implementing a CNN
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施CNN
- en: A CNN is one of the foundational blocks of computer vision techniques, and it
    is important for you to have a solid understanding of how they work. While we
    already know that a CNN constitutes convolution, pooling, flattening, and then
    the final classification layer, in this section, we will understand the various
    operations that occur during the forward pass of a CNN through code.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是计算机视觉技术的基础组成部分之一，对于您深入理解它们的工作原理非常重要。虽然我们已经知道CNN包括卷积、池化、展平，然后是最终的分类层，但在本节中，我们将通过代码了解CNN前向传播期间发生的各种操作。
- en: 'To gain a solid understanding of this, first, we will build a CNN architecture
    on a toy example using PyTorch and then match the output by building the feed-forward
    propagation from scratch in Python. The CNN architecture will differ from the
    neural network architecture that we built in the previous chapter in that a CNN
    constitutes the following in addition to what a typical vanilla deep neural network
    would have:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要对此有一个坚实的理解，首先，我们将使用PyTorch在一个玩具示例上构建一个CNN架构，然后通过Python从头开始构建前向传播以匹配输出。CNN架构将与我们在上一章中构建的神经网络架构有所不同，因为CNN除了典型的香草深度神经网络外，还包括以下内容：
- en: Convolution operation
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积操作
- en: Pooling operation
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化操作
- en: Flatten layer
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平层
- en: 'In the following code, we will build a CNN model on a toy dataset, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将在玩具数据集上构建一个CNN模型，如下所示：
- en: The following code can be found in the `CNN_working_details.ipynb` file located
    in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可在GitHub的`Chapter04`文件夹中找到`CNN_working_details.ipynb`文件中的以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'First, we need to import the relevant libraries:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入相关的库：
- en: '[PRE10]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we need to create the dataset using the following steps:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要按照以下步骤创建数据集：
- en: '[PRE11]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that PyTorch expects the input to be of the shape **N x C x H x W**, where
    *N* is the number (batch size) of images, *C* is the number of channels, *H* is
    the height, and *W* is the width of the image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，PyTorch期望输入的形状为**N x C x H x W**，其中*N*是图像的数量（批量大小），*C*是通道数，*H*是高度，*W*是图像的宽度。
- en: Here, we are scaling the input dataset so that it has a range between -1 and
    +1 by dividing the input data by the maximum input value – that is, 8\. The shape
    of the input dataset is (2,1,4,4) since there are two data points, where each
    is 4 x 4 in shape and has 1 channel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将输入数据集进行缩放，使其范围在-1到+1之间，通过将输入数据除以最大输入值即8。输入数据集的形状为(2,1,4,4)，因为有两个数据点，每个数据点的形状为4
    x 4，并且有1个通道。
- en: 'Define the model architecture:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: '[PRE12]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that in the preceding model, we are specifying that there is 1 channel
    in the input and that we are extracting 1 channel from the output post-convolution
    (that is, we have 1 filter with a size of 3 x 3) using the `nn.Conv2d` method.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上述模型中，我们指定输入中有1个通道，并且我们使用`nn.Conv2d`方法在卷积后提取1个通道的输出（即，我们有1个大小为3 x 3的滤波器）。
- en: After this, we perform max pooling using `nn.MaxPool2d` and ReLU activation
    (using `nn.Relu()`) prior to flattening and connecting to the final layer, which
    has one output per data point.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`nn.MaxPool2d`进行最大池化和ReLU激活（使用`nn.Relu()`），然后扁平化并连接到最终层，每个数据点有一个输出。
- en: Furthermore, note that the loss function is binary cross-entropy loss (`nn.BCELoss()`)
    since the output is from a binary class. We are also specifying that the optimization
    will be done using the Adam optimizer with a learning rate of 0.001.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，损失函数是二元交叉熵损失(`nn.BCELoss()`)，因为输出来自二元类别。我们还指定优化将使用学习率为0.001的Adam优化器进行。
- en: 'Summarize the architecture of the model using the `summary` method that’s available
    in the `torch_summary` package post fetching our `model`, loss function (`loss_fn`),
    and `optimizer` by calling the `get_model` function:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`torch_summary`包中的`summary`方法对我们的`model`、损失函数(`loss_fn`)和`optimizer`进行获取后，总结模型的架构：
- en: '[PRE13]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code results in the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![](img/B18457_04_15.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_15.png)'
- en: 'Figure 4.15: Summary of model architecture'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15：模型架构摘要
- en: 'Let’s understand the reason why each layer contains as many parameters. The
    arguments of the `Conv2d` class are as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解每一层包含多少参数的原因。`Conv2d`类的参数如下：
- en: '![](img/B18457_04_16.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_16.png)'
- en: 'Figure 4.16: Arguments within Conv2d'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16：Conv2d中的参数说明
- en: In the preceding case, we are specifying that the size of the convolving kernel
    (`kernel_size`) is 3 and that the number of `out_channels` is 1 (essentially,
    the number of filters is 1), where the number of initial (input) channels is 1\.
    Thus, for each input image, we are convolving a filter of shape 3 x 3 on a shape
    of 1 x 4 x 4, which results in an output of the shape 1 x 2 x 2\. There are 10
    parameters since we are learning the nine weight parameters (3 x 3) and the one
    bias of the convolving kernel. For the `MaxPool2d`, ReLU, and flatten layers,
    there are no parameters as these are operations that are performed on top of the
    output of the convolution layer; no weights or biases are involved.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们指定卷积核的大小(`kernel_size`)为3，并且`out_channels`的数量为1（本质上，滤波器的数量为1），其中初始（输入）通道的数量为1。因此，对于每个输入图像，我们在一个形状为1
    x 4 x 4的图像上卷积一个形状为3 x 3的滤波器，得到一个形状为1 x 2 x 2的输出。有10个参数，因为我们正在学习九个权重参数（3 x 3）和卷积核的一个偏置。对于`MaxPool2d`、ReLU和flatten层，没有参数，因为这些是在卷积层输出之上执行的操作；不涉及权重或偏置。
- en: The linear layer has two parameters – one weight and one bias – which means
    there’s a total of 12 parameters (10 from the convolution operation and two from
    the linear layer).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层有两个参数 - 一个权重和一个偏置 - 这意味着总共有12个参数（来自卷积操作的10个和线性层的两个）。
- en: 'Train the model using the same model training code we used in *Chapter 3*,
    where we defined the function that will train on batches of data (`train_batch`).
    Then, fetch the `DataLoader` and train it on batches of data over 2,000 epochs
    (we’re only using 2,000 because this is a small toy dataset), as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在*第3章*中使用的相同模型训练代码来训练模型，定义将对数据批次进行训练的函数(`train_batch`)。然后，获取`DataLoader`并在2,000个epochs中对数据批次进行训练（我们只使用2,000个是因为这是一个小型玩具数据集），如下所示：
- en: 'Define the function that will train on batches of data (`train_batch`):'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将对数据批次进行训练的函数(`train_batch`)：
- en: '[PRE14]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the training DataLoader by specifying the dataset using the `TensorDataset`
    method and then loading it using `DataLoader`:'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `TensorDataset` 方法指定数据集来定义训练 DataLoader，然后使用 `DataLoader` 加载它：
- en: '[PRE15]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Given that we are not modifying the input data by a lot, we won’t be building
    a class separately, but instead, leveraging the `TensorDataset` method directly,
    which provides an object that corresponds to the input data.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们没有对输入数据进行大量修改，我们不会单独构建一个类，而是直接利用 `TensorDataset` 方法，该方法提供了与输入数据对应的对象。
- en: 'Train the model over 2,000 epochs:'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 2,000 个 epochs 上训练模型：
- en: '[PRE16]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the preceding code, we have trained the CNN model on our toy dataset.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述代码，我们已经在我们的玩具数据集上训练了 CNN 模型。
- en: 'Perform a forward pass on top of the first data point:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对第一个数据点执行前向传播：
- en: '[PRE17]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output of the preceding code is `0.1625`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是 `0.1625`。
- en: Note that you might have a different output value owing to a different random
    weight initialization when you execute the preceding code.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于执行上述代码时随机权重初始化可能不同，因此您可能会有不同的输出值。
- en: Using the `CNN from scratch in Python.pdf` file in the GitHub repository, we
    can learn how forward propagation in CNNs works from scratch and replicate the
    output of 0.1625 on the first data point.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GitHub 仓库中的 `CNN from scratch in Python.pdf` 文件，我们可以学习如何从头开始构建 CNN 中的前向传播，并在第一个数据点上复制输出
    0.1625。
- en: In the next section, we’ll apply this to the Fashion-MNIST dataset and see how
    it fares on translated images.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将把这个应用到 Fashion-MNIST 数据集，并看看它在翻译后的图像上表现如何。
- en: Classifying images using deep CNNs
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度 CNN 对图像进行分类
- en: So far, we have seen that the traditional neural network predicts incorrectly
    for translated images. This needs to be addressed because, in real-world scenarios,
    various augmentations will need to be applied, such as translation and rotation,
    that were not seen during the training phase. In this section, we will understand
    how CNNs address the problem of incorrect predictions when image translation happens
    on images in the Fashion-MNIST dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到传统神经网络对翻译图像预测不正确。这需要解决，因为在实际情况中，会需要应用各种增强技术，如翻译和旋转，这些在训练阶段没有看到。在本节中，我们将了解
    CNN 如何解决在 Fashion-MNIST 数据集的图像发生翻译时预测不正确的问题。
- en: 'The preprocessing portion of the Fashion-MNIST dataset remains the same as
    in the previous chapter, except when we reshape (`.view`) the input data, where
    instead of flattening the input to 28 x 28 = 784 dimensions, we reshape the input
    to a shape of (1,28,28) for each image (remember, channels are to be specified
    first, followed by their height and width, in PyTorch):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST 数据集的预处理部分与上一章节相同，除了当我们对输入数据进行重塑（`.view`）时，我们不再将输入展平为 28 x 28 =
    784 维度，而是将每个图像重塑为形状为 (1,28,28)（请记住，首先需要指定通道，然后是它们的高度和宽度，在 PyTorch 中）：
- en: The following code can be found in the `CNN_on_FashionMNIST.ipynb` file located
    in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)**.**
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 可在 GitHub 的 `Chapter04` 文件夹中的 `CNN_on_FashionMNIST.ipynb` 文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)**.**
- en: 'Import the necessary packages:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE18]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Fashion-MNIST dataset class is defined as follows. Remember, the `Dataset`
    object will **always** need the `__init__`, `__getitem__`, and `__len__` methods
    we’ve defined:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fashion-MNIST 数据集类定义如下。请记住，`Dataset` 对象将**总是**需要我们定义的 `__init__`、`__getitem__`
    和 `__len__` 方法：
- en: '[PRE19]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The line of code in bold is where we are reshaping each input image (differently
    from what we did in the previous chapter) since we are providing data to a CNN
    that expects each input to have a shape of batch size x channels x height x width.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 粗体的代码行是我们重塑每个输入图像的地方（与前一章节所做的不同），因为我们正在为期望每个输入具有批大小 x 通道 x 高度 x 宽度形状的 CNN 提供数据。
- en: 'The CNN model architecture is defined as follows:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN 模型架构定义如下：
- en: '[PRE20]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A summary of the model can be created using the following code:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下代码创建模型总结：
- en: '[PRE21]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![](img/B18457_04_17.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_17.png)'
- en: 'Figure 4.17: Summary of model architecture'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17：模型架构总结
- en: 'To solidify our understanding of CNNs, let’s understand the reason why the
    numbers of parameters have been set the way they have in the preceding output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加深我们对 CNN 的理解，让我们了解在前面的输出中参数数量被设定为什么样的原因：
- en: '**Layer 1**: Given that there are 64 filters with a kernel size of 3, we have
    64 x 3 x 3 weights and 64 x 1 biases, resulting in a total of 640 parameters.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 4**: Given that there are 128 filters with a kernel size of 3, we have
    128 x 64 x 3 x 3 weights and 128 x 1 biases, resulting in a total of 73,856 parameters.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 8**: Given that a layer with 3,200 nodes is getting connected to another
    layer with 256 nodes, we have a total of 3,200 x 256 weights and 256 biases, resulting
    in a total of 819,456 parameters.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 10**: Given that a layer with 256 nodes is getting connected to a layer
    with 10 nodes, we have a total of 256 x 10 weights and 10 biases, resulting in
    a total of 2,570 parameters.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we train the model, just like we trained it in the previous chapter.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code is available in this book’s GitHub repository: `https://bit.ly/mcvp-2e`.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained, you’ll notice that the variation of accuracy
    and loss over the training and test datasets is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_18.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Training and validation loss and accuracy over increasing epochs'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding scenario, the accuracy of the validation dataset
    is ~92% within the first five epochs, which is already better than the accuracy
    we saw across various techniques in the previous chapter, even without additional
    regularization.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s translate the image and predict the class of translated images:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Translate the image between -5 pixels to +5 pixels and predict its class:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, we reshaped the image (`img3`) so it has a shape of `(-1,1,28,28)`,
    which will enable us to pass the image to a CNN model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the probability of the classes across various translations:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code results in the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_19.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Probability of each class with varying translation'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this scenario, even when the image was translated by 4 pixels,
    the prediction was correct, while in the scenario where we did not use a CNN,
    the prediction was incorrect when the image was translated by 4 pixels. Furthermore,
    when the image was translated by 5 pixels, the probability of **Trouser** dropped
    considerably.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, while CNNs help in addressing the challenge of image translation,
    they don’t solve the problem at hand completely. We will learn how to address
    such a scenario by leveraging data augmentation alongside CNNs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Given the different techniques that can be leveraged for data augmentation,
    we have provided exhaustive information on data augmentation in the GitHub repository
    in the `implementing data augmentation.pdf` file within the `Chapter04` folder.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the outcome of feature learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how CNNs help us classify images, even when the objects
    in the images have been translated. We have also learned that filters play a key
    role in learning the features of an image, which, in turn, helps in classifying
    the image into the right class. However, we haven’t mentioned what the filters
    learn that makes them powerful. In this section, we will learn about what these
    filters learn that enables CNNs to classify an image correctly by classifying
    a dataset that contains images of Xs and Os. We will also examine the fully connected
    layer (flatten layer) to understand what their activations look like.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at what the filters learn:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Visualizing_the_features'_learning.ipynb`
    file located in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the images in the folder are named as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_20.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: Naming convention of images'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The class of an image can be obtained from the image’s name, where the first
    character of the image’s name specifies the class the image belongs to.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define a class that fetches data. Also, ensure that the images have been resized
    to a shape of 28 x 28, batches have been shaped with three channels, and that
    the dependent variable is fetched as a numeric value. We’ll do this in the following
    code, one step at a time:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the image augmented method, which resizes the image to a shape of 28
    x 28:'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define a class that takes the folder path as input and loops through the files
    in that path in the `__init__` method:'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the `__len__` method, which returns the lengths of the files that are
    to be considered:'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the `__getitem__` method, which we use to fetch an index that returns
    the file present at that index, read the file, and then perform augmentation on
    the image. We have not used `collate_fn` here because this is a small dataset
    and it wouldn’t affect the training time significantly:'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Given that each image is of the shape 28 x 28, we’ll now create a dummy channel
    dimension at the beginning of the shape – that is, before the height and width
    of an image:'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can assign the class of each image based on the character post `''/''`
    and prior to `''@''` in the filename:'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we return the image and the corresponding class:'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Inspect a sample of the images you’ve obtained. In the following code, we’re
    extracting the images and their corresponding classes by fetching data from the
    class we defined previously:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we can plot a sample of the images from the dataset we’ve obtained:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code results in the following output:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_21.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Sample images'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture, loss function, and the optimizer:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note that the loss function is binary cross-entropy loss (`nn.BCELoss()`) since
    the output provided is from a binary class. A summary of the preceding model can
    be obtained as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This results in the following output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_22.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Summary of model architecture'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function for training on batches that takes images and their classes
    as input and returns their loss values and accuracy after backpropagation has
    been performed on top of the given batch of data:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define a `DataLoader` where the input is the `Dataset` class:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initialize the model:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Train the model over `5` epochs:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Fetch an image to check what the filters learn about the image:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This results in the following output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_23.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: Sample image'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the image through the trained model and fetch the output of the first
    layer. Then, store it in the `intermediate_output` variable:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Plot the output of the 64 filters. Each channel in `intermediate_output` is
    the output of the convolution for each filter:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This results in the following output:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_24.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: Activations of the 64 filters'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Notice that certain filters, such as filters 0, 4, 6, and 7, learn about the
    edges present in the network, while other filters, such as filter 54, learn to
    invert the image.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass multiple O images and inspect the output of the fourth filter across the
    images (we are only using the fourth filter for illustration purposes; you can
    choose a different filter if you wish):'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fetch multiple O images from the data:'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Reshape `x2` so that it has a proper input shape for a CNN model – that is,
    batch size x channels x height x width:'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define a variable that stores the model until the first layer:'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Extract the output of passing the O images (`x2`) through the model until the
    first layer (`first_layer`), as defined previously:'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Plot the output of passing multiple images through the `first_layer` model:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code results in the following output:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_25.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: Activations of the fourth filter when multiple O images are passed'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Note that the behavior of a given filter (in this case, the fourth filter of
    the first layer) has remained consistent across images.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create another model that extracts layers until the second convolution
    layer (that is, until the four layers defined in the preceding model) and then
    extracts the output of passing the original O image. We will then plot the output
    of convolving the filters in the second layer with the input O image:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Plot the output of convolving the filters with the respective image:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code results in the following output:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18457_04_26.png)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Activations of the 128 filters in the second convolution layer'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s use the 34th filter’s output in the preceding image as an example.
    When we pass multiple O images through filter 34, we should see similar activations
    across images. Let’s test this, as follows:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The preceding code results in the following output:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_27.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: Activations of the 34th filter when multiple O images are passed'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Note that, even here, the activations of the 34th filter on different images
    are similar in that the left half of O was activating the filter.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the activations of a fully connected layer, as follows:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, fetch a larger sample of images:'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, choose only the O images from the dataset and then reshape them so that
    they can be passed as input to our CNN model:'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Fetch the flatten (fully connected) layer and pass the preceding images through
    the model until they reach the flatten layer:'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Plot the flatten layer:'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The preceding code results in the following output:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_28.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.28: Activations of the fully connected layer'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Note that the shape of the output is 1,245 x 3,200 since there are 1,245 **O**
    images in our dataset and there are 3,200 dimensions for each image in the flatten
    layer.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: It’s also interesting to note that certain values in the fully connected layer
    are highlighted when the input is **O** (here, we can see white lines, where each
    dot represents an activation value greater than zero).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Note that the model has learned to bring some structure to the fully connected
    layer, even though the input images – while all belonging to the same class –
    differ in style considerably.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how CNNs work and how filters aid in this process,
    we will apply this so that we can classify images of cats and dogs.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN for classifying real-world images
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to perform image classification on the Fashion-MNIST
    dataset. In this section, we’ll do the same for a more real-world scenario, where
    the task is to classify images containing cats or dogs. We will also learn how
    the accuracy of the dataset varies when we change the number of images available
    for training.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be working on a dataset available in Kaggle at [https://www.kaggle.com/tongpython/cat-and-dog](https://www.kaggle.com/tongpython/cat-and-dog):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Cats_Vs_Dogs.ipynb` file located in
    the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy the URL from the notebook on GitHub to avoid any issues while
    reproducing the results.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Download the dataset, as follows:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We must download the dataset that’s available in the `colab` environment. First,
    however, we must upload our Kaggle authentication file:'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: You will have to upload your `kaggle.json` file for this step, which can be
    obtained from your Kaggle account. Details on how to obtain the `kaggle.json`
    file is provided in the associated notebook on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, specify that we’re moving to the Kaggle folder and copy the `kaggle.json`
    file to it:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, download the cats and dogs dataset and unzip it:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Provide the training and test dataset folders:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Build a class that fetches data from the preceding folders. Then, based on
    the directory the image corresponds to, provide a label of 1 for dog images and
    a label of 0 for cat images. Furthermore, ensure that the fetched image has been
    normalized to a scale between 0 and 1 and permute it so that channels are provided
    first (as PyTorch models expect to have channels specified first, before the height
    and width of the image) – performed as follows:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `__init__` method, which takes a folder as input and stores the
    file paths (image paths) corresponding to the images in the `cats` and `dogs`
    folders in separate objects, post concatenating the file paths into a single list:'
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, randomize the file paths and create target variables based on the folder
    corresponding to these file paths:'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Define the `__len__` method, which corresponds to the `self` class:'
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Define the `__getitem__` method, which we use to specify a random file path
    from the list of file paths, read the image, and resize all the images so that
    they’re 224 x 224 in size. Given that our CNN expects the inputs from the channel
    to be specified first for each image, we will permute the resized image so that
    channels are provided first before we return the scaled image and the corresponding
    `target` value:'
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Inspect a random image:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We need to permute the image we’ve obtained to our channels last. This is because
    `matplotlib` expects an image to have the channels specified after the height
    and width of the image have been provided:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This results in the following output:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_29.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.29: Sample dog image'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a model, loss function, and optimizer, as follows:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we must define the `conv_layer` function, where we perform convolution,
    ReLU activation, batch normalization, and max pooling in that order. This method
    will be reused in the final model, which we will define in the next step:'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In the preceding code, we are taking the number of input channels (`ni`), number
    of output channels (`no`), `kernel_size`, and the `stride` of filters as input
    for the `conv_layer` function.
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `get_model` function, which performs multiple convolutions and pooling
    operations (by calling the `conv_layer` method), flattens the output, and connects
    a hidden layer to it prior to connecting to the output layer:'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: You can chain `nn.Sequential` inside `nn.Sequential` with as much depth as you
    want. In the preceding code, we used `conv_layer` as if it were any other `nn.Module`
    layer.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must call the `get_model` function to fetch the model, loss function
    (`loss_fn`), and `optimizer` and then summarize the model using the `summary`
    method that we imported from the `torchsummary` package:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The preceding code results in the following output:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_30.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.30: Summary of model architecture'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `get_data` function, which creates an object of the `cats_dogs`
    class and creates a `DataLoader` with a `batch_size` of 32 for both the training
    and validation folders:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In the preceding code, we are ignoring the last batch of data by specifying
    that `drop_last = True`. We’re doing this because the last batch might not be
    the same size as the other batches.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们通过指定`drop_last = True`来忽略最后一个数据批次。我们这样做是因为最后一个批次的大小可能与其他批次不同。
- en: 'Define the function that will train the model on a batch of data, as we’ve
    done in previous sections:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将在数据批次上训练模型的函数，就像我们在前面的章节中所做的一样：
- en: '[PRE71]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Define the functions for calculating accuracy and validation loss, as we’ve
    done in previous sections:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于计算准确性和验证损失的函数，就像我们在前面的章节中所做的一样：
- en: 'Define the `accuracy` function:'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`accuracy`函数：
- en: '[PRE72]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Note that the preceding code for accuracy calculation is different from the
    code in the Fashion-MNIST classification because the current model (cats versus
    dogs classification) is being built for binary classification, while the Fashion-MNIST
    model was built for multi-class classification.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，用于精度计算的前述代码与Fashion-MNIST分类中的代码不同，因为当前模型（猫与狗分类）是为二元分类构建的，而Fashion-MNIST模型是为多类分类构建的。
- en: 'Define the validation loss calculation function:'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义验证损失计算函数：
- en: '[PRE73]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Train the model for `5` epochs and check the accuracy of the test data at the
    end of each epoch, as we’ve done in previous sections:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在5个epoch上训练模型，并在每个epoch结束时检查测试数据的准确性，就像我们在前面的章节中所做的一样：
- en: 'Define the model and fetch the required DataLoaders:'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型并获取所需的DataLoaders：
- en: '[PRE74]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Train the model over increasing epochs:'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的epoch上训练模型：
- en: '[PRE75]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Plot the variation of the training and validation accuracies over increasing
    epochs:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制随着epoch增加，训练和验证准确性的变化：
- en: '[PRE76]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The preceding code results in the following output:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![](img/B18457_04_31.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_31.png)'
- en: 'Figure 4.31: Training and validation accuracy over increasing epochs'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.31：随着epoch增加，训练和验证准确性的变化
- en: Note that the classification accuracy at the end of `5` epochs is ~86%.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在5个epoch结束时的分类准确性约为86%。
- en: As we discussed in the previous chapter, batch normalization has a great impact
    on improving classification accuracy – check this out for yourself by training
    the model without batch normalization. Furthermore, the model can be trained without
    batch normalization if you use fewer parameters. You can do this by reducing the
    number of layers, increasing the stride, increasing the pooling, or resizing the
    image to a number that’s lower than 224 x 224.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中讨论的，批量归一化对提高分类准确性有很大影响——通过训练模型而不使用批量归一化来自行验证这一点。此外，如果您使用较少的参数，模型也可以在没有批量归一化的情况下进行训练。您可以通过减少层数、增加步幅、增加池化或将图像调整为低于224
    x 224的数字来实现这一点。
- en: So far, the training we’ve done has been based on ~8K examples, where 4K examples
    have been from the `cat` class and the rest have been from the `dog` class. In
    the next section, we will learn about the impact that having a reduced number
    of training examples has on each class when it comes to the classification accuracy
    of the test dataset.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的训练基于约8K个示例，其中4K个示例来自“cat”类，其余来自“dog”类。在接下来的章节中，我们将了解在分类测试数据集的分类准确性中，训练示例数量减少对每个类别的影响。
- en: Impact on the number of images used for training
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练图像数量的影响
- en: We know that, generally, the more training examples we use, the better our classification
    accuracy is. In this section, we will learn what impact using different numbers
    of available images has on training accuracy by artificially reducing the number
    of images available for training and then testing the model’s accuracy when classifying
    the test dataset.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，通常使用的训练示例越多，我们的分类准确性就越好。在本节中，我们将通过人为减少可用于训练的图像数量，然后在分类测试数据集时测试模型的准确性，来了解使用不同数量的可用图像对训练准确性的影响。
- en: The following code can be found in the `Cats_Vs_Dogs.ipynb` file located in
    the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Given that the majority of the code that will be provided here is similar to what
    we have seen in the previous section, we have only provided the modified code
    for brevity. The respective notebook on GitHub will contain the full code.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 可在GitHub上的`Chapter04`文件夹中的`Cats_Vs_Dogs.ipynb`文件中找到以下代码，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。鉴于这里提供的大部分代码与我们在前一节中看到的类似，我们仅为简洁起见提供了修改后的代码。相应的GitHub笔记本将包含完整的代码。
- en: 'Here, we only want to have 500 data points for each class in the training dataset.
    We can do this by limiting the number of files to only the first 500 image paths
    in each folder in the `__init__` method and ensuring that the rest remain as they
    were in the previous section:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只希望在训练数据集中有每类 500 个数据点。我们可以通过在 `__init__` 方法中限制文件数到每个文件夹中的前 500 个图像路径，并确保其余保持与上一节相同的方式来实现这一点：
- en: '[PRE77]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: In the preceding code, the only difference from the initialization we performed
    in the previous section is in `self.paths`, where we are now limiting the number
    of file paths to be considered in each folder to only the first 500.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，与我们在前一节中执行的初始化唯一区别在于 `self.paths`，我们现在将考虑的文件路径数量限制为每个文件夹中的前 500 个。
- en: 'Now, once we execute the rest of the code, as we did in the previous section,
    the accuracy of the model that’s been built on 1,000 images (500 of each class)
    in the test dataset will be as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们执行其余代码，就像在前一节中所做的那样，构建在 1,000 张图像（每类 500 张）上的模型在测试数据集上的准确率如下：
- en: '![](img/B18457_04_32.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_32.png)'
- en: 'Figure 4.32: Training and validation accuracy with 1K data points'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.32：使用 1K 数据点的训练和验证准确率
- en: We can see that because we had fewer examples of images in training, the accuracy
    of the model on the test dataset reduced considerably – that is, down to ~66%.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，由于训练中图像样本较少，在测试数据集上模型的准确率显著降低，即下降至约 66%。
- en: Now, let’s see how the number of training data points impacts the accuracy of
    the test dataset by varying the number of available training examples that will
    be used to train the model (where we build a model for each scenario).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看训练数据点数量如何影响测试数据集的准确性，通过改变用于训练模型的可用训练示例的数量（我们为每种情况构建一个模型）。
- en: 'We’ll use the same code we used for the 1K (500 per class) data point training
    example but will vary the number of available images (to 2K, 4K, and 8K total
    data points, respectively). For brevity, we will only look at the output of running
    the model on a varying number of images available for training. This results in
    the following output:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与 1K（每类 500 个）数据点训练示例相同的代码，但会改变可用图像的数量（分别为 2K、4K 和 8K 总数据点）。为简洁起见，我们只关注在不同训练图像数量下运行模型的输出。结果如下：
- en: '![](img/B18457_04_33.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_04_33.png)'
- en: 'Figure 4.33: Training and validation accuracy with varying number of data points'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.33：使用不同数据点数量的训练和验证准确率
- en: As you can see, the more training data that’s available, the higher the accuracy
    of the model on test data. However, we might not have a large enough amount of
    training data in every scenario that we encounter. The next chapter, which will
    cover transfer learning, will address this problem by walking you through various
    techniques you can use to attain high accuracy, even on a small amount of training
    data.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，可用的训练数据越多，模型在测试数据上的准确率就越高。然而，在我们遇到的每种情况下，我们可能没有足够大量的训练数据。下一章将涵盖迁移学习，通过引导您了解各种技术，即使在少量训练数据的情况下也能获得高准确率。
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Traditional neural networks fail when new images that are very similar to previously
    seen images that have been translated are fed as input to the model. CNNs play
    a key role in addressing this shortcoming. This is enabled through the various
    mechanisms that are present in CNNs, including filters, strides, and pooling.
    Initially, we built a toy example to learn how CNNs work. Then, we learned how
    data augmentation helps in increasing the accuracy of the model by creating translated
    augmentations on top of the original image. After that, we learned about what
    different filters learn in the feature learning process so that we could implement
    a CNN to classify images.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当将与先前看到的已被翻译的类似的新图像作为模型的输入时，传统神经网络会失败。CNN 在解决此缺陷中发挥了关键作用。这是通过 CNN 中存在的各种机制实现的，包括滤波器、步幅和池化。最初，我们建立了一个玩具示例来学习
    CNN 的工作原理。然后，我们学习了数据增强如何通过在原始图像上创建翻译增强来增加模型的准确性。之后，我们了解了不同滤波器在特征学习过程中学到的内容，以便我们能够实现一个用于图像分类的
    CNN。
- en: Finally, we saw the impact that differing amounts of training data have on the
    accuracy of test data. Here, we saw that the more training data that is available,
    the better the accuracy of the test data. In the next chapter, we will learn how
    to leverage various transfer learning techniques to increase the accuracy of the
    test dataset, even when we have just a small amount of training data.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why was the prediction on the translated image in the first section of the chapter
    low when using traditional neural networks?
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is convolution done?
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are optimal weight values in a filter identified?
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the combination of convolution and pooling help in addressing the issue
    of image translation?
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do the convolution filters in layers closer to the input layer learn?
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What functionality does pooling have that helps in building a model?
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can’t we take an input image, flatten it just like we did on the Fashion-MNIST
    dataset, and train a model for real-world images?
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does data augmentation help in improving image translation?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In what scenario do we leverage `collate_fn` for dataloaders?
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What impact does varying the number of training data points have on the classification
    accuracy of the validation dataset?
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
