- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve learned how to build deep neural networks and the impact of tweaking
    their various hyperparameters. In this chapter, we will learn about where traditional
    deep neural networks do not work. We’ll then learn about the inner workings of
    **convolutional neural networks** (**CNNs**) by using a toy example before understanding
    some of their major hyperparameters, including stride, pooling, and filters. Next,
    we will leverage CNNs, along with various data augmentation techniques, to solve
    the issue of traditional deep neural networks not having good accuracy. Following
    this, we will learn what the outcome of a feature learning process in a CNN looks
    like. Finally, we’ll bring our learning together to solve a use case: we’ll be
    classifying an image by stating whether the image contains a dog or a cat. By
    doing this, we’ll be able to understand how the accuracy of prediction varies
    by the amount of data available for training.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a deep understanding of CNNs, which
    form the backbone of multiple model architectures that are used for various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with traditional deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building blocks of a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying images using deep CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the outcome of feature learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN for classifying real-world images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'All code in this chapter is available for reference in the `Chapter04` folder
    of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with traditional deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into CNNs, let’s look at the major problem that’s faced when
    using traditional deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s reconsider the model we built on the Fashion-MNIST dataset in *Chapter
    3*. We will fetch a random image and predict the class that corresponds to that
    image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Issues_with_image_translation.ipynb`
    file located in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Only the additional code corresponding to the issue of image translation will
    be discussed here for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch a random image from the available training images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Image corresponding to index 24300'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the image through the **trained model** (continue using the model we trained
    in the *Batch size of 32* section of *Chapter 3*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocess the image so it goes through the same preprocessing steps we performed
    while building the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the probabilities associated with the various classes:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output, where we can see that the
    highest probability is for the first index, which is of the `Trouser` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18457_04_02.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.2: Probabilities of different classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Translate (roll/slide) the image multiple times (one pixel at a time) from
    a translation of 5 pixels to the left to 5 pixels to the right and store the predictions
    in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a list that stores predictions:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a loop that translates (rolls) an image from -5 pixels (5 pixels to
    the left) to +5 pixels (5 pixels to the right) of the original position (which
    is at the center of the image):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we specified 6 as the upper bound even though we are
    interested in translating until +5 pixels since the output of the range would
    be from -5 to +5 when (-5,6) is the specified range.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocess the image as we did in *step 2*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Roll the image by a value equal to `px` within the `for` loop:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specified `axis=1` since we want the image pixels to be moving horizontally
    and not vertically.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Store the rolled image as a tensor object and register it to `device`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass `img3` through the trained model to predict the class of the translated
    (rolled) image and append it to the list that stores predictions for various translations:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the predictions of the model for all the translations (-5 pixels
    to +5 pixels):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Probability of each class for different translations'
  prefs: []
  type: TYPE_NORMAL
- en: There was no change in the image’s content since we only translated the images
    from 5 pixels to the left and 5 pixels to the right. However, the predicted class
    of the image changed when the translation was beyond 2 pixels. This is because,
    while the model was being trained, the content in all the training and testing
    images was at the center. This differs from the preceding scenario where we tested
    with translated images that are off-center (by a margin of 5 pixels), resulting
    in an incorrectly predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about a scenario where a traditional neural network
    fails, we will learn how CNNs help address this problem. But before we do this,
    let’s first look at the building blocks of a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks of a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are the most prominent architectures that are used when working on images.
    They address the major limitations of deep neural networks, like the one we saw
    in the previous section. Besides image classification, they also help with object
    detection, image segmentation, GANs, and much more – essentially, wherever we
    use images. Furthermore, there are different ways of constructing a CNN, and there
    are multiple pre-trained models that leverage CNNs to perform various tasks. Starting
    with this chapter, we will be using CNNs extensively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming subsections, we will understand the fundamental building blocks
    of a CNN, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strides and padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convolution is basically a multiplication between two matrices. As you saw
    in the previous chapter, matrix multiplication is a key ingredient in training
    a neural network. (We perform matrix multiplication when we calculate hidden layer
    values – which is a matrix multiplication of the input values and weight values
    connecting the input to the hidden layer. Similarly, we perform matrix multiplication
    to calculate output layer values.)
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that we have a solid understanding of the convolution process, let’s
    go through an example. Let’s assume we have two matrices we can use to perform
    convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Matrix A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Matrix A'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is Matrix B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Matrix B'
  prefs: []
  type: TYPE_NORMAL
- en: 'While performing the convolution operation, we are sliding Matrix B (the smaller
    matrix) over Matrix A (the bigger matrix). Furthermore, we are performing element-to-element
    multiplication between Matrix A and Matrix B, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {1,2,5,6} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {2,3,6,7} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*2*1 + 3*2 + 6*3 + 7*4 = 54*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {3,4,7,8} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {5,6,9,10} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*5*1 + 6*2 + 9*3 + 10*4 = 84*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {6,7,10,11} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*6*1 + 7*2 + 10*3 + 11*4 = 94*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {7,8,11,12} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*7*1 + 8*2 + 11*3 + 12*4 = 104*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {9,10,13,14} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {10,11,14,15} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*10*1 + 11*2 + 14*3 + 15*4 = 134*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply {11,12,15,16} of the bigger matrix by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of performing the preceding operations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Output of convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: The smaller matrix is typically called a **filter** or a kernel, while the bigger
    matrix is the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A filter is a matrix of weights that is initialized randomly at the start.
    The model learns the optimal weight values of a filter over increasing epochs.
    The concept of filters brings us to two different aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: What the filters learn about
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How filters are represented
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the more filters there are in a CNN, the more features of an image
    the model can learn about. We will learn about what various filters learn in the
    *Visualizing the outcome of feature learning* section of this chapter. For now,
    we’ll proceed with the intermediate understanding that the filters learn about
    different features present in the image. For example, a certain filter might learn
    about the ears of a cat and provide high activation (a matrix multiplication value)
    when the part of the image it is convolving with contains the ear of a cat.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, when we convolved one filter that has a size of 2 x
    2 with a matrix that has a size of 4 x 4, we got an output that is 3 x 3 in dimension.
    However, if 10 different filters multiply the bigger matrix (original image),
    the result is 10 sets of the 3 x 3 output matrices.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding case, a 4 x 4 image is convolved with 10 filters that are 2
    x 2 in size, resulting in 3 x 3 x 10 output values. Essentially, when an image
    is convolved by multiple filters, the output has as many channels as there are
    filters that the image is convolved with.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in a scenario where we are dealing with color images where there
    are three channels, the filter that is convolving with the original image would
    also have three channels, resulting in a single scalar output per convolution.
    Also, if the filters are convolving with an intermediate output – let’s say 64
    x 112 x 112 in shape – the filter would have 64 channels to fetch a scalar output.
    In addition, if there are 512 filters that are convolving with the output that
    was obtained in the intermediate layer, the output post-convolution with 512 filters
    would be 512 x 112 x 112 in shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solidify our understanding of the output of filters further, let’s take
    a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Output of convolution operation with multiple filters'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see that the input image is multiplied by the
    filters that have the same depth as that of the input (which the filters are convolving
    with) and that the number of channels in the output of a convolution is as many
    as there are filters.
  prefs: []
  type: TYPE_NORMAL
- en: Strides and padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, each filter strode across the image – one column and
    one row at a time (after exhausting all possible columns by the end of the image).
    This also resulted in the output size being 1 pixel less than the input image
    size – both in terms of height and width. This results in a partial loss of information
    and can limit the possibility of us adding the output of the convolution operation
    to the original image if the output of convolution and the original image are
    not of the same shape. This is known as residual addition and will be discussed
    in detail in the next chapter. For now, however, let’s learn how strides and padding
    influence the output shape of convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s understand the impact of strides by leveraging the same example we saw
    in the *Filters* section. We’ll move Matrix B with a stride of 2 over Matrix A.
    As a result, the output of convolution with a stride of 2 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{1,2,5,6} of the bigger matrix is multiplied by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  prefs: []
  type: TYPE_NORMAL
- en: '{3,4,7,8} of the bigger matrix is multiplied by {1,2,3,4} of the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  prefs: []
  type: TYPE_NORMAL
- en: '{9,10,13,14} of the bigger matrix is multiplied by {1,2,3,4} of the smaller
    matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  prefs: []
  type: TYPE_NORMAL
- en: '{11,12,15,16} of the bigger matrix is multiplied by {1,2,3,4} of the smaller
    matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of performing the preceding operations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Output of convolution with stride'
  prefs: []
  type: TYPE_NORMAL
- en: Since we now have a stride of 2, note that the preceding output has a lower
    dimension compared to the scenario where the stride was 1 (where the output shape
    was 3 x 3).
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding case, we could not multiply the leftmost elements of the filter
    by the rightmost elements of the image. If we were to perform such matrix multiplication,
    we would pad the image with zeros. This would ensure that we can perform element-to-element
    multiplication of all the elements within an image with a filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand padding by using the same example we used in the *Convolution*
    section. Once we add padding on top of Matrix A, the revised version of Matrix
    A will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Padding on Matrix A'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have padded Matrix A with zeros, and the convolution with
    Matrix B will not result in the output dimension being smaller than the input
    dimension. This aspect comes in handy when we are working on a residual network
    where we must add the output of the convolution to the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve done this, we can perform activation on top of the convolution operation’s
    output. We could use any of the activation functions we saw in *Chapter 3* for
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pooling aggregates information in a small patch. Imagine a scenario where the
    output of convolution activation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Output of convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The max pooling for this patch is 4 – as that is the maximum value across the
    values in the patch. Let’s understand the max pooling for a bigger matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Output of convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding case, if the pooling stride has a length of 2, the max pooling
    operation is calculated as follows, where we divide the input image by a stride
    of 2 (that is, we have divided the image into 2 x 2 divisions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Output of convolution with stride highlighted'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the four sub-portions of the matrix, the maximum values in the pool of
    elements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Max pool values'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is not necessary to always have a stride of 2; this has just
    been used for illustration purposes here. Other variants of pooling are sum and
    average pooling. However, in practice, max pooling is used more often.
  prefs: []
  type: TYPE_NORMAL
- en: Note that by the end of performing the convolution and pooling operations, the
    size of the original matrix is reduced from 4 x 4 to 2 x 2\. In a realistic scenario,
    if the original image is of shape 200 x 200 and the filter is of shape 3 x 3,
    the output of the convolution operation would be 198 x 198\. Following that, the
    output of the pooling operation with a stride of 2 is 99 x 99\. This way, by leveraging
    pooling, we preserve the more important features while reducing the dimension
    of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Putting them all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned about convolution, filters, strides, padding, and pooling,
    and their impact on reducing the dimension of an image. Now, we will learn about
    another critical component of a CNN – the flatten layer (fully connected layer)
    – before putting the three pieces we have learned about together.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the flattening process, we’ll take the output of the pooling layer
    in the previous section and flatten the output. The output of flattening the pooling
    layer is {6, 8, 14, 16}.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we’ll see that the flatten layer can be treated as equivalent
    to the input layer (where we flattened the input image into a 784-dimensional
    input in *Chapter 3*). Once the flatten layer’s (fully connected layer) values
    have been obtained, we can pass it through the hidden layer and then obtain the
    output for predicting the class of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall flow of a CNN is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: CNN workflow'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the overall flow of a CNN model, where we are passing an image through
    convolution via multiple filters and then pooling (and in the preceding case,
    repeating the convolution and pooling process twice), before flattening the output
    of the final pooling layer. This forms the **feature learning** part of the preceding
    image, where we are taking an image and reducing it to a lower dimension (the
    flattened output) while restoring the required information.
  prefs: []
  type: TYPE_NORMAL
- en: The operations of convolution and pooling constitute the feature learning section,
    as filters help in extracting relevant features from images and pooling helps
    in aggregating information and thereby reducing the number of nodes at the flatten
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: If we directly flatten the input image (which is 300 x 300 pixels in size, for
    example), we are dealing with 90K input values. If we have 90K input pixel values
    and 100K nodes in a hidden layer, we are looking at ~9 billion parameters, which
    is huge in terms of computation.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution and pooling help in fetching a flatten layer that has a much smaller
    representation than the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last part of the classification is similar to the way we classified
    images in *Chapter 3*, where we had a hidden layer and then obtained the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: How convolution and pooling help in image translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we perform pooling, we can consider the output of the operation as an abstraction
    of a region (a small patch). This phenomenon comes in handy, especially when images
    are being translated.
  prefs: []
  type: TYPE_NORMAL
- en: Think of a scenario where an image is translated by 1 pixel to the left. Once
    we perform convolution, activation, and pooling on top of it, we’ll have reduced
    the dimension of the image (due to pooling), which means that a fewer number of
    pixels store the majority of the information from the original image. Moreover,
    given that pooling stores information of a region (patch), the information within
    a pixel of the pooled image would not vary, even if the original image is translated
    by 1 unit. This is because the maximum value of that region is likely to get captured
    in the pooled image.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution and pooling can also help us with the **receptive field**. To understand
    the receptive field, let’s imagine a scenario where we perform a convolution +
    pooling operation twice on an image that is 100 x 100 in shape. The output at
    the end of the two convolution pooling operations is of the shape 25 x 25 (if
    the convolution operation was done with padding). Each cell in the 25 x 25 output
    now corresponds to a larger 4 x 4 portion of the original image. Thus, because
    of the convolution and pooling operations, each cell in the resulting image contains
    key information within a patch of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the core components of a CNN, let’s apply them
    all to a toy example to understand how they work together.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CNN is one of the foundational blocks of computer vision techniques, and it
    is important for you to have a solid understanding of how they work. While we
    already know that a CNN constitutes convolution, pooling, flattening, and then
    the final classification layer, in this section, we will understand the various
    operations that occur during the forward pass of a CNN through code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain a solid understanding of this, first, we will build a CNN architecture
    on a toy example using PyTorch and then match the output by building the feed-forward
    propagation from scratch in Python. The CNN architecture will differ from the
    neural network architecture that we built in the previous chapter in that a CNN
    constitutes the following in addition to what a typical vanilla deep neural network
    would have:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flatten layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code, we will build a CNN model on a toy dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `CNN_working_details.ipynb` file located
    in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to create the dataset using the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that PyTorch expects the input to be of the shape **N x C x H x W**, where
    *N* is the number (batch size) of images, *C* is the number of channels, *H* is
    the height, and *W* is the width of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are scaling the input dataset so that it has a range between -1 and
    +1 by dividing the input data by the maximum input value – that is, 8\. The shape
    of the input dataset is (2,1,4,4) since there are two data points, where each
    is 4 x 4 in shape and has 1 channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding model, we are specifying that there is 1 channel
    in the input and that we are extracting 1 channel from the output post-convolution
    (that is, we have 1 filter with a size of 3 x 3) using the `nn.Conv2d` method.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we perform max pooling using `nn.MaxPool2d` and ReLU activation
    (using `nn.Relu()`) prior to flattening and connecting to the final layer, which
    has one output per data point.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, note that the loss function is binary cross-entropy loss (`nn.BCELoss()`)
    since the output is from a binary class. We are also specifying that the optimization
    will be done using the Adam optimizer with a learning rate of 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarize the architecture of the model using the `summary` method that’s available
    in the `torch_summary` package post fetching our `model`, loss function (`loss_fn`),
    and `optimizer` by calling the `get_model` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the reason why each layer contains as many parameters. The
    arguments of the `Conv2d` class are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Arguments within Conv2d'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding case, we are specifying that the size of the convolving kernel
    (`kernel_size`) is 3 and that the number of `out_channels` is 1 (essentially,
    the number of filters is 1), where the number of initial (input) channels is 1\.
    Thus, for each input image, we are convolving a filter of shape 3 x 3 on a shape
    of 1 x 4 x 4, which results in an output of the shape 1 x 2 x 2\. There are 10
    parameters since we are learning the nine weight parameters (3 x 3) and the one
    bias of the convolving kernel. For the `MaxPool2d`, ReLU, and flatten layers,
    there are no parameters as these are operations that are performed on top of the
    output of the convolution layer; no weights or biases are involved.
  prefs: []
  type: TYPE_NORMAL
- en: The linear layer has two parameters – one weight and one bias – which means
    there’s a total of 12 parameters (10 from the convolution operation and two from
    the linear layer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model using the same model training code we used in *Chapter 3*,
    where we defined the function that will train on batches of data (`train_batch`).
    Then, fetch the `DataLoader` and train it on batches of data over 2,000 epochs
    (we’re only using 2,000 because this is a small toy dataset), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the function that will train on batches of data (`train_batch`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training DataLoader by specifying the dataset using the `TensorDataset`
    method and then loading it using `DataLoader`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given that we are not modifying the input data by a lot, we won’t be building
    a class separately, but instead, leveraging the `TensorDataset` method directly,
    which provides an object that corresponds to the input data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train the model over 2,000 epochs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the preceding code, we have trained the CNN model on our toy dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform a forward pass on top of the first data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding code is `0.1625`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you might have a different output value owing to a different random
    weight initialization when you execute the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `CNN from scratch in Python.pdf` file in the GitHub repository, we
    can learn how forward propagation in CNNs works from scratch and replicate the
    output of 0.1625 on the first data point.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll apply this to the Fashion-MNIST dataset and see how
    it fares on translated images.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images using deep CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen that the traditional neural network predicts incorrectly
    for translated images. This needs to be addressed because, in real-world scenarios,
    various augmentations will need to be applied, such as translation and rotation,
    that were not seen during the training phase. In this section, we will understand
    how CNNs address the problem of incorrect predictions when image translation happens
    on images in the Fashion-MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preprocessing portion of the Fashion-MNIST dataset remains the same as
    in the previous chapter, except when we reshape (`.view`) the input data, where
    instead of flattening the input to 28 x 28 = 784 dimensions, we reshape the input
    to a shape of (1,28,28) for each image (remember, channels are to be specified
    first, followed by their height and width, in PyTorch):'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `CNN_on_FashionMNIST.ipynb` file located
    in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Fashion-MNIST dataset class is defined as follows. Remember, the `Dataset`
    object will **always** need the `__init__`, `__getitem__`, and `__len__` methods
    we’ve defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The line of code in bold is where we are reshaping each input image (differently
    from what we did in the previous chapter) since we are providing data to a CNN
    that expects each input to have a shape of batch size x channels x height x width.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNN model architecture is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A summary of the model can be created using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solidify our understanding of CNNs, let’s understand the reason why the
    numbers of parameters have been set the way they have in the preceding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer 1**: Given that there are 64 filters with a kernel size of 3, we have
    64 x 3 x 3 weights and 64 x 1 biases, resulting in a total of 640 parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 4**: Given that there are 128 filters with a kernel size of 3, we have
    128 x 64 x 3 x 3 weights and 128 x 1 biases, resulting in a total of 73,856 parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 8**: Given that a layer with 3,200 nodes is getting connected to another
    layer with 256 nodes, we have a total of 3,200 x 256 weights and 256 biases, resulting
    in a total of 819,456 parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 10**: Given that a layer with 256 nodes is getting connected to a layer
    with 10 nodes, we have a total of 256 x 10 weights and 10 biases, resulting in
    a total of 2,570 parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we train the model, just like we trained it in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code is available in this book’s GitHub repository: `https://bit.ly/mcvp-2e`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained, you’ll notice that the variation of accuracy
    and loss over the training and test datasets is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Training and validation loss and accuracy over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding scenario, the accuracy of the validation dataset
    is ~92% within the first five epochs, which is already better than the accuracy
    we saw across various techniques in the previous chapter, even without additional
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s translate the image and predict the class of translated images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Translate the image between -5 pixels to +5 pixels and predict its class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we reshaped the image (`img3`) so it has a shape of `(-1,1,28,28)`,
    which will enable us to pass the image to a CNN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the probability of the classes across various translations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Probability of each class with varying translation'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this scenario, even when the image was translated by 4 pixels,
    the prediction was correct, while in the scenario where we did not use a CNN,
    the prediction was incorrect when the image was translated by 4 pixels. Furthermore,
    when the image was translated by 5 pixels, the probability of **Trouser** dropped
    considerably.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, while CNNs help in addressing the challenge of image translation,
    they don’t solve the problem at hand completely. We will learn how to address
    such a scenario by leveraging data augmentation alongside CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Given the different techniques that can be leveraged for data augmentation,
    we have provided exhaustive information on data augmentation in the GitHub repository
    in the `implementing data augmentation.pdf` file within the `Chapter04` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the outcome of feature learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how CNNs help us classify images, even when the objects
    in the images have been translated. We have also learned that filters play a key
    role in learning the features of an image, which, in turn, helps in classifying
    the image into the right class. However, we haven’t mentioned what the filters
    learn that makes them powerful. In this section, we will learn about what these
    filters learn that enables CNNs to classify an image correctly by classifying
    a dataset that contains images of Xs and Os. We will also examine the fully connected
    layer (flatten layer) to understand what their activations look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at what the filters learn:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Visualizing_the_features'_learning.ipynb`
    file located in the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the images in the folder are named as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: Naming convention of images'
  prefs: []
  type: TYPE_NORMAL
- en: The class of an image can be obtained from the image’s name, where the first
    character of the image’s name specifies the class the image belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a class that fetches data. Also, ensure that the images have been resized
    to a shape of 28 x 28, batches have been shaped with three channels, and that
    the dependent variable is fetched as a numeric value. We’ll do this in the following
    code, one step at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the image augmented method, which resizes the image to a shape of 28
    x 28:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a class that takes the folder path as input and loops through the files
    in that path in the `__init__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__len__` method, which returns the lengths of the files that are
    to be considered:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__getitem__` method, which we use to fetch an index that returns
    the file present at that index, read the file, and then perform augmentation on
    the image. We have not used `collate_fn` here because this is a small dataset
    and it wouldn’t affect the training time significantly:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Given that each image is of the shape 28 x 28, we’ll now create a dummy channel
    dimension at the beginning of the shape – that is, before the height and width
    of an image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can assign the class of each image based on the character post `''/''`
    and prior to `''@''` in the filename:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we return the image and the corresponding class:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect a sample of the images you’ve obtained. In the following code, we’re
    extracting the images and their corresponding classes by fetching data from the
    class we defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can plot a sample of the images from the dataset we’ve obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Sample images'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture, loss function, and the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the loss function is binary cross-entropy loss (`nn.BCELoss()`) since
    the output provided is from a binary class. A summary of the preceding model can
    be obtained as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function for training on batches that takes images and their classes
    as input and returns their loss values and accuracy after backpropagation has
    been performed on top of the given batch of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a `DataLoader` where the input is the `Dataset` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over `5` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch an image to check what the filters learn about the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: Sample image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the image through the trained model and fetch the output of the first
    layer. Then, store it in the `intermediate_output` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the output of the 64 filters. Each channel in `intermediate_output` is
    the output of the convolution for each filter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: Activations of the 64 filters'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that certain filters, such as filters 0, 4, 6, and 7, learn about the
    edges present in the network, while other filters, such as filter 54, learn to
    invert the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass multiple O images and inspect the output of the fourth filter across the
    images (we are only using the fourth filter for illustration purposes; you can
    choose a different filter if you wish):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fetch multiple O images from the data:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape `x2` so that it has a proper input shape for a CNN model – that is,
    batch size x channels x height x width:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a variable that stores the model until the first layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the output of passing the O images (`x2`) through the model until the
    first layer (`first_layer`), as defined previously:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the output of passing multiple images through the `first_layer` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: Activations of the fourth filter when multiple O images are passed'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the behavior of a given filter (in this case, the fourth filter of
    the first layer) has remained consistent across images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create another model that extracts layers until the second convolution
    layer (that is, until the four layers defined in the preceding model) and then
    extracts the output of passing the original O image. We will then plot the output
    of convolving the filters in the second layer with the input O image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the output of convolving the filters with the respective image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18457_04_26.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Activations of the 128 filters in the second convolution layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s use the 34th filter’s output in the preceding image as an example.
    When we pass multiple O images through filter 34, we should see similar activations
    across images. Let’s test this, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: Activations of the 34th filter when multiple O images are passed'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, even here, the activations of the 34th filter on different images
    are similar in that the left half of O was activating the filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the activations of a fully connected layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, fetch a larger sample of images:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, choose only the O images from the dataset and then reshape them so that
    they can be passed as input to our CNN model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch the flatten (fully connected) layer and pass the preceding images through
    the model until they reach the flatten layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the flatten layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.28: Activations of the fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the shape of the output is 1,245 x 3,200 since there are 1,245 **O**
    images in our dataset and there are 3,200 dimensions for each image in the flatten
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also interesting to note that certain values in the fully connected layer
    are highlighted when the input is **O** (here, we can see white lines, where each
    dot represents an activation value greater than zero).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the model has learned to bring some structure to the fully connected
    layer, even though the input images – while all belonging to the same class –
    differ in style considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how CNNs work and how filters aid in this process,
    we will apply this so that we can classify images of cats and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN for classifying real-world images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to perform image classification on the Fashion-MNIST
    dataset. In this section, we’ll do the same for a more real-world scenario, where
    the task is to classify images containing cats or dogs. We will also learn how
    the accuracy of the dataset varies when we change the number of images available
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be working on a dataset available in Kaggle at [https://www.kaggle.com/tongpython/cat-and-dog](https://www.kaggle.com/tongpython/cat-and-dog):'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Cats_Vs_Dogs.ipynb` file located in
    the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy the URL from the notebook on GitHub to avoid any issues while
    reproducing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We must download the dataset that’s available in the `colab` environment. First,
    however, we must upload our Kaggle authentication file:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will have to upload your `kaggle.json` file for this step, which can be
    obtained from your Kaggle account. Details on how to obtain the `kaggle.json`
    file is provided in the associated notebook on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, specify that we’re moving to the Kaggle folder and copy the `kaggle.json`
    file to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Finally, download the cats and dogs dataset and unzip it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Provide the training and test dataset folders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a class that fetches data from the preceding folders. Then, based on
    the directory the image corresponds to, provide a label of 1 for dog images and
    a label of 0 for cat images. Furthermore, ensure that the fetched image has been
    normalized to a scale between 0 and 1 and permute it so that channels are provided
    first (as PyTorch models expect to have channels specified first, before the height
    and width of the image) – performed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `__init__` method, which takes a folder as input and stores the
    file paths (image paths) corresponding to the images in the `cats` and `dogs`
    folders in separate objects, post concatenating the file paths into a single list:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, randomize the file paths and create target variables based on the folder
    corresponding to these file paths:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__len__` method, which corresponds to the `self` class:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__getitem__` method, which we use to specify a random file path
    from the list of file paths, read the image, and resize all the images so that
    they’re 224 x 224 in size. Given that our CNN expects the inputs from the channel
    to be specified first for each image, we will permute the resized image so that
    channels are provided first before we return the scaled image and the corresponding
    `target` value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect a random image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to permute the image we’ve obtained to our channels last. This is because
    `matplotlib` expects an image to have the channels specified after the height
    and width of the image have been provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.29: Sample dog image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a model, loss function, and optimizer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we must define the `conv_layer` function, where we perform convolution,
    ReLU activation, batch normalization, and max pooling in that order. This method
    will be reused in the final model, which we will define in the next step:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are taking the number of input channels (`ni`), number
    of output channels (`no`), `kernel_size`, and the `stride` of filters as input
    for the `conv_layer` function.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `get_model` function, which performs multiple convolutions and pooling
    operations (by calling the `conv_layer` method), flattens the output, and connects
    a hidden layer to it prior to connecting to the output layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can chain `nn.Sequential` inside `nn.Sequential` with as much depth as you
    want. In the preceding code, we used `conv_layer` as if it were any other `nn.Module`
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must call the `get_model` function to fetch the model, loss function
    (`loss_fn`), and `optimizer` and then summarize the model using the `summary`
    method that we imported from the `torchsummary` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.30: Summary of model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `get_data` function, which creates an object of the `cats_dogs`
    class and creates a `DataLoader` with a `batch_size` of 32 for both the training
    and validation folders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are ignoring the last batch of data by specifying
    that `drop_last = True`. We’re doing this because the last batch might not be
    the same size as the other batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the function that will train the model on a batch of data, as we’ve
    done in previous sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the functions for calculating accuracy and validation loss, as we’ve
    done in previous sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `accuracy` function:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the preceding code for accuracy calculation is different from the
    code in the Fashion-MNIST classification because the current model (cats versus
    dogs classification) is being built for binary classification, while the Fashion-MNIST
    model was built for multi-class classification.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the validation loss calculation function:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model for `5` epochs and check the accuracy of the test data at the
    end of each epoch, as we’ve done in previous sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the model and fetch the required DataLoaders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over increasing epochs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the variation of the training and validation accuracies over increasing
    epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: Training and validation accuracy over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the classification accuracy at the end of `5` epochs is ~86%.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous chapter, batch normalization has a great impact
    on improving classification accuracy – check this out for yourself by training
    the model without batch normalization. Furthermore, the model can be trained without
    batch normalization if you use fewer parameters. You can do this by reducing the
    number of layers, increasing the stride, increasing the pooling, or resizing the
    image to a number that’s lower than 224 x 224.
  prefs: []
  type: TYPE_NORMAL
- en: So far, the training we’ve done has been based on ~8K examples, where 4K examples
    have been from the `cat` class and the rest have been from the `dog` class. In
    the next section, we will learn about the impact that having a reduced number
    of training examples has on each class when it comes to the classification accuracy
    of the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Impact on the number of images used for training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that, generally, the more training examples we use, the better our classification
    accuracy is. In this section, we will learn what impact using different numbers
    of available images has on training accuracy by artificially reducing the number
    of images available for training and then testing the model’s accuracy when classifying
    the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Cats_Vs_Dogs.ipynb` file located in
    the `Chapter04` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Given that the majority of the code that will be provided here is similar to what
    we have seen in the previous section, we have only provided the modified code
    for brevity. The respective notebook on GitHub will contain the full code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we only want to have 500 data points for each class in the training dataset.
    We can do this by limiting the number of files to only the first 500 image paths
    in each folder in the `__init__` method and ensuring that the rest remain as they
    were in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the only difference from the initialization we performed
    in the previous section is in `self.paths`, where we are now limiting the number
    of file paths to be considered in each folder to only the first 500.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, once we execute the rest of the code, as we did in the previous section,
    the accuracy of the model that’s been built on 1,000 images (500 of each class)
    in the test dataset will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.32: Training and validation accuracy with 1K data points'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that because we had fewer examples of images in training, the accuracy
    of the model on the test dataset reduced considerably – that is, down to ~66%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how the number of training data points impacts the accuracy of
    the test dataset by varying the number of available training examples that will
    be used to train the model (where we build a model for each scenario).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the same code we used for the 1K (500 per class) data point training
    example but will vary the number of available images (to 2K, 4K, and 8K total
    data points, respectively). For brevity, we will only look at the output of running
    the model on a varying number of images available for training. This results in
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_04_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.33: Training and validation accuracy with varying number of data points'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the more training data that’s available, the higher the accuracy
    of the model on test data. However, we might not have a large enough amount of
    training data in every scenario that we encounter. The next chapter, which will
    cover transfer learning, will address this problem by walking you through various
    techniques you can use to attain high accuracy, even on a small amount of training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional neural networks fail when new images that are very similar to previously
    seen images that have been translated are fed as input to the model. CNNs play
    a key role in addressing this shortcoming. This is enabled through the various
    mechanisms that are present in CNNs, including filters, strides, and pooling.
    Initially, we built a toy example to learn how CNNs work. Then, we learned how
    data augmentation helps in increasing the accuracy of the model by creating translated
    augmentations on top of the original image. After that, we learned about what
    different filters learn in the feature learning process so that we could implement
    a CNN to classify images.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw the impact that differing amounts of training data have on the
    accuracy of test data. Here, we saw that the more training data that is available,
    the better the accuracy of the test data. In the next chapter, we will learn how
    to leverage various transfer learning techniques to increase the accuracy of the
    test dataset, even when we have just a small amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why was the prediction on the translated image in the first section of the chapter
    low when using traditional neural networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is convolution done?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are optimal weight values in a filter identified?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the combination of convolution and pooling help in addressing the issue
    of image translation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do the convolution filters in layers closer to the input layer learn?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What functionality does pooling have that helps in building a model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can’t we take an input image, flatten it just like we did on the Fashion-MNIST
    dataset, and train a model for real-world images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does data augmentation help in improving image translation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In what scenario do we leverage `collate_fn` for dataloaders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What impact does varying the number of training data points have on the classification
    accuracy of the validation dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
