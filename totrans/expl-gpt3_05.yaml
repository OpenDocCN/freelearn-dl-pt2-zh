- en: '*Chapter 3*: Working with the OpenAI Playground'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we briefly introduced the Playground. Chances are, you'll
    be spending a lot of time in the Playground because it's a great tool, both for
    learning and for rapidly prototyping and testing prompts and settings. So, in
    this chapter, we're going to take a closer look at the Playground with an emphasis
    on the Playground settings. We'll also look at other OpenAI developer tools and
    resources that you'll want to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will be covering in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the OpenAI developer console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving deeper into the Playground
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with presets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to the **OpenAI API**. You can request
    access by visiting [https://openapi.com](https://openapi.com).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the OpenAI developer console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Playground is part of the OpenAI developer console. The developer console
    is a private web-based portal that provides developer resources and tools – the
    Playground being one of them. To access the developer console, you'll need a valid
    OpenAI developer account. While this chapter will largely focus on the Playground
    and, more specifically, the Playground settings, it's worth taking a minute to
    review some of the other resources available in the OpenAI developer console,
    starting with the developer documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Developer documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you're working with new technologies, good documentation is very often
    not available. Fortunately, that's not the case with GPT-3\. The OpenAI documentation
    is extremely well done. It's complete, easy to follow, and provides a number of
    very useful examples. We looked at one of those examples – a classification prompt
    example – in [*Chapter 1*](B16854_01_ePub_AM.xhtml#_idTextAnchor016), *Introducing
    GPT 3 and the OpenAI API*. But let's take a look at another great example from
    the documentation, the Factual responses example.
  prefs: []
  type: TYPE_NORMAL
- en: Factual responses example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following prompt example is from the OpenAI developer documentation. It
    is located at [https://beta.openai.com/docs/introduction/prompt-design-101](https://beta.openai.com/docs/introduction/prompt-design-101).
    It provides an example QA prompt that shows the model that a question mark should
    be returned for questions it likely doesn't have the correct answer to. This directs
    the model to *not* make up an answer, which it would likely do by default. This
    is an important example because although GPT-3 responses are almost always grammatically
    correct, they are very often not factual. So, even though they might sound good,
    they could be completely fabricated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key thing to note in the following prompt is that the examples provided
    show GPT-3 how to deal with questions that don''t have a factual answer, or that
    GPT-3 doesn''t know how to answer. There are also some settings used to help ensure
    a factual response, but we''ll talk about the settings a bit later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The point of highlighting the OpenAI documentation, as you can hopefully see,
    is because it can be an extremely valuable resource. But it's just one of many.
  prefs: []
  type: TYPE_NORMAL
- en: Developer resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The documentation is just one of the available resources in the developer portal.
    Other resources are available, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FAQs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pricing Details**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video Tutorials**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community Examples**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive Tools**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidelines and Legal Documents**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logo Assets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you become familiar with GPT-3 and the OpenAI API, you'll want to spend time
    reviewing all of the available developer resources. We will dive deeper into a
    few of them, but they are all valuable and worth reviewing.
  prefs: []
  type: TYPE_NORMAL
- en: Accounts and organizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important area to point out in the developer console is the account
    profile section. This is where you edit your developer account and organization
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Developer accounts are used to authenticate and identify individual developers.
    By default, when a developer account is created, an organization named Personal
    is also created. An organization is used for billing purposes and grouping users,
    meaning that users can create, or be associated with, multiple organizations that
    each get billed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Each organization has a title (name) that you can specify and an organization
    ID that is automatically generated. The organization ID is a unique identifier
    used to associate usage with the proper organization for billing purposes. So,
    when you're logged in to the developer console, any usage will be associated with
    the organization you're working under. We'll discuss the organization ID again
    in [*Chapter 4*](B16854_04_ePub_AM.xhtml#_idTextAnchor074), *Working with the
    OpenAI API*, where we'll look at associating API calls with a specific organization,
    but you can also associate usage with an organization in the developer console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to see the organizations your account is
    associated with and how to switch between organizations in the developer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Switching between organizations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Switching between organizations
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, organizations are used for billing purposes, and an organization
    named **Personal** is created along with your user account. You can change the
    organization title to something other than **Personal** if you prefer. The following
    screenshot shows where you can change the name of your personal organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Personal organization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Personal organization
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you're the owner of your personal organization and this is the organization
    you would set up billing for if you're using the API for your own individual use.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing and billing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get into pricing, it's important to note that pricing could change
    at any time, so you'll want to visit https://beta.openai.com/pricing for the most
    current pricing details. With that disclaimer out of the way, let's continue.
  prefs: []
  type: TYPE_NORMAL
- en: For starters, the OpenAI API is priced on a per-usage basis. So, you only pay
    for the resources that you use. There are no setup fees or recurring charges.
    The usage fees are based on tokens used. The cost per token depends on the engine
    you're using. We discussed tokens and introduced the available engines in [*Chapter
    1*](B16854_01_ePub_AM.xhtml#_idTextAnchor016), *Introducing GPT-3 and the OpenAI
    API.*
  prefs: []
  type: TYPE_NORMAL
- en: Davinci is the largest model and the most capable engine, hence, it is also
    the most expensive engine to use. At the other end of the price spectrum is ada.
    This is the smallest model, which limits its capabilities. However, ada is the
    most efficient engine, and therefore the least expensive one to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the pricing per engine at the time this book
    was published. Again, the pricing could change at any time, so be sure to verify
    the current pricing as it may very well have changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Pricing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Pricing
  prefs: []
  type: TYPE_NORMAL
- en: Pay-per-usage pricing is nice because if you're not doing anything, it's not
    costing you anything. That said, it can also be a bit scary not knowing what your
    bill might be. Thankfully, however, you can set a hard limit or a soft limit to
    manage your spending. This is done in your billing settings. The hard limit prevents
    the API from using more tokens once the limit is met. Of course, this will render
    the API unusable, which could be a problem in production apps. So, there is also
    the option of setting a soft limit. This will send an email alert when usage limits
    are hit.
  prefs: []
  type: TYPE_NORMAL
- en: Usage reporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to setting hard or soft limits to manage your costs, you also have
    access to usage reporting. You''ll find usage reporting in the organization settings
    under the **Usage** menu. The following screenshot shows an example usage report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Usage reporting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Usage reporting
  prefs: []
  type: TYPE_NORMAL
- en: The main chart in usage reporting defaults to show the total tokens used per
    day for the current month. Each bar also shows the tokens used for prompts and
    completions. From this chart, you can also view usage by a dollar amount and display
    the cumulative total rather than daily totals. In addition, below the main chart,
    you can view the total usage along with detailed usage per day by engine.
  prefs: []
  type: TYPE_NORMAL
- en: Member management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, when you get a developer account, an organization is set up for
    your personal use. But you might also want to have an organization for a team
    of users. To do that, you can request an organization account for multiple users
    by sending an email to `support@openai.com`. When a new organization is created,
    you''ll be able to invite other users to the organization. This is done under
    the **Members** menu within the organization. The following screenshot shows the
    member management page for an organization. From this page, you can invite new
    members, remove members, or change member permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Member management'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Member management
  prefs: []
  type: TYPE_NORMAL
- en: Outside of the Playground, that should cover the essential things you'll need
    to know about the developer console. So, let's get back to the Playground and
    take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into the Playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you should understand the basics of using the Playground. But
    we're going to cover the Playground in more depth now and discuss all of the available
    options and settings. [*Chapter 2*](B16854_02_ePub_AM.xhtml#_idTextAnchor038),
    *GPT 3 Applications and Use Cases*, provided a quick overview of the available
    settings, but let's take a closer look at each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the settings in the Playground. They are located
    just to the right of the large text input box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Playground settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Playground settings
  prefs: []
  type: TYPE_NORMAL
- en: The first setting is the **Engine** setting, so we'll start there.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, we refer to the OpenAI language model as just GPT-3\. But, as you'll
    recall from [*Chapter 1*](B16854_01_ePub_AM.xhtml#_idTextAnchor016), *Introducing
    GPT-3 and the OpenAI API*, there are multiple models/engines.
  prefs: []
  type: TYPE_NORMAL
- en: When you first open the Playground, the davinci engine is selected by default.
    This will usually be the engine you'll want to start testing prompts with. The
    reason you'll want to start with davinci is that it's the largest model and therefore
    the most capable engine. The davinci engine can do anything that any of the other
    engines can do. However, other engines might be able to perform the specific tasks
    faster or more cost-effectively. So, an alternative approach could be to start
    with the least expensive engine first and then test the next most expensive engine
    when a less expensive engine is unable to complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: So, start with davinci. Then, when you're getting the results you want from
    davinci, test your prompt with the other engines to see whether you're also able
    to get acceptable results. Or start with ada, the least expensive engine, and
    then progress up if you fail to obtain acceptable results. Let's look at an example
    using a simple classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a prompt classifying items as a tool, food, clothing, or something
    else:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results when davinci is used as the engine.
    Note that the new items added to the list (**Shirt**, **Hammer**, **Apple**, and
    **Airplane**) are all categorized correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Classification example with the davinci engine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Classification example with the davinci engine
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the results when the engine is changed from davinci to
    ada. You''ll notice in the following screenshot that the new items added to the
    list (**Socks**, **Pliers**, **Hamburger**, and **House**) are also correctly
    classified by ada:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Classification example with the ada engine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Classification example with the ada engine
  prefs: []
  type: TYPE_NORMAL
- en: So, as you can see from the previous example, there will be tasks that don't
    require davinci to get acceptable results. If that's the case, choosing another
    engine will reduce your usage costs and often also improve response times. Of
    course, if costs and performance aren't a concern, you can always stick with davinci.
    But again, depending on the task, davinci might not be the only option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list provides an idea of what each engine does generally well.
    These aren''t hard and fast rules, merely a guideline. So, you''ll always want
    to test to determine the best fit based on the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Davinci**: Complex intent, cause and effect, summarization for age.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Curie**: Language translation, complex classification, text sentiment, summarization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Babbage**: Moderate classification, semantic search classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ada**: Text parsing, simple classification, address correction, keywords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The response length setting is fairly self-explanatory. It controls the length
    of the completion that will be generated. The main thing to keep in mind with
    the response length is that the value relates to a number of tokens to be returned.
    Recall from [*Chapter 1*](B16854_01_ePub_AM.xhtml#_idTextAnchor016), *Introducing
    GPT-3 and the OpenAI API*, that tokens can represent words or parts of words.
    So, don't mistake the response length for a word count or character count.
  prefs: []
  type: TYPE_NORMAL
- en: The other thing to keep in mind is that you get billed for tokens used – including
    tokens that are used for completions, meaning the larger your response length,
    the more tokens you'll use. So, if you're trying to optimize costs, set the response
    length as short as possible for the given task. For example, if the task is to
    provide sentiment analysis on a block of text, the response length only needs
    to be long enough to display the sentiment result.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Top P
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next two settings are **Temperature** and **Top P**. These are two of the
    most important settings, but they can also be the most confusing ones to understand.
    At a high level, they both influence the randomness or diversity of the response
    that is generated. But knowing how and when to use one or the other can be tricky.
  prefs: []
  type: TYPE_NORMAL
- en: To make sense of the temperature and Top P settings, it's helpful to know that
    machine learning systems could process the same input differently. This means
    that the output can vary even when the input provided hasn't changed. This is
    because machine learning systems such as GPT-3 use heuristics (educated guesses)
    rather than concrete logic to generate results. So, instead of trying to find
    the perfect solution, machine learning systems try to identify the best possible
    options based on the data it was trained with.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of GPT-3, the dataset it was trained on is extremely large and diverse.
    Therefore, most inputs (prompts) will result in a variety of possible completions.
    Again, this could be a benefit or a challenge depending on the task. For example,
    if you're using GPT-3 to generate ideas for a book title, you want a lot of different
    options to choose from. However, if you want GPT-3 to accurately answer history
    questions, you want responses that are consistent and factual. This is where the
    temperature and Top P settings come in. The temperature and Top P settings can
    be used to help control the variability and number of options that are used to
    generate a completion.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The temperature setting influences how deterministic the model will be when
    generating a result. So, the temperature provides some control over how likely
    the results are to vary. A lower value will direct the model to be more deterministic
    (less variable), while a higher value will cause the model to be less deterministic,
    or more variable. The range can be between 0 and 1\. To see the effects of the
    temperature setting let's look at some examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with an example that uses the default Playground temperature of
    **0.7**. In this example, we''ll look at the default stochastic (random) nature
    of most completions. We''ll start with a prompt that contains the words **Once
    upon a time** and nothing else, like the prompt shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Temperature example 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Temperature example 1
  prefs: []
  type: TYPE_NORMAL
- en: As you might guess, there are a lot of possible completions for this prompt.
    So, when we submit the prompt three times, we get the following three completions,
    and each one is different.
  prefs: []
  type: TYPE_NORMAL
- en: '*Once upon a time*, a little princess was born.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there were three little pigs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a girl who saw a boy run past her house every
    day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example is a simple one and the results aren't surprising. But understanding
    why the same prompt resulted in three different completions is important. So,
    let's talk a bit more about our first example.
  prefs: []
  type: TYPE_NORMAL
- en: There are actually three reasons why we got different responses in our previous
    example. The first reason is that the underlying model can come up with a lot
    of different ways to complete this prompt. That's because there are a lot of stories
    that start with *Once upon a time* and the data used to train the model contains
    plenty of examples of those stories.
  prefs: []
  type: TYPE_NORMAL
- en: The second reason is that the default temperature setting is relatively high
    (0.7 out of 1). So, the model is being directed to take more risks and to be more
    random when generating the response.
  prefs: []
  type: TYPE_NORMAL
- en: The last reason has to do with the Top P setting, but we'll talk about that
    a little later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the same example again, but this time we''ll change the
    temperature setting to 0\. Again, we''ll submit the prompt three times, but this
    time, the results are as follows – the same each time:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what's happening? The lower temperature value is telling GPT-3 to be less
    variable in how it processes the prompt, so the completion is staying consistent.
    Because we're using zero for the temperature (the lowest value), the result will
    likely always be the same. However, you can use a value between zero and one (for
    example 0.2) to gain more control over the randomness of the result. However,
    changing the temperature won't always affect the completion because again, the
    completion also depends on the examples in the data the model was trained on.
    To illustrate this, let's look at another example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we''ll use a prompt that just includes the words **A robot
    may not injure** with the default temperature setting of **0.7**, as we have in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 3.10 – Temperature example 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Temperature example 2
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, when we submit the prompt in the previous screenshot three times,
    we get the following results – the same completion each time:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A robot may not injure* a human being or, through inaction, allow a human
    being to come to harm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A robot may not injure* a human being or, through inaction, allow a human
    being to come to harm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A robot may not injure* a human being or, through inaction, allow a human
    being to come to harm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, now you might be wondering what's going on. Shouldn't the completion have
    varied because of the higher temperature setting? Again, the completion also depends
    on the data the model was trained on. In this case, the reason the completion
    isn't changing (and probably wouldn't irrespective of what the temperature setting
    was) is because the five words (or five tokens), *A robot may not injure*, are
    most often seen as part of **Isaac Asimov**'s **laws of robotics**. So, because
    of the training, regardless of the temperature, the best possible result is almost
    always the same. So, keep in mind that the temperature setting will only have
    a noticeable effect when there are a variety of different ways in which a prompt
    could be completed.
  prefs: []
  type: TYPE_NORMAL
- en: Top P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the temperature controls the randomness of results generated based on
    the model, the Top P setting controls how many of those results (or tokens) are
    considered for completion. The value can be between 0-1, where a higher value
    considers a higher number of tokens. For example, a value of 1 would consider
    all of the likely options, whereas a value of 0.5 would limit the options by half.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like temperature, Top P can be used to increase or limit the seeming randomness
    of a completion. However, unlike the temperature, it''s influencing the randomness
    by limiting the scope of the possible results that should be considered. To illustrate
    the point, imagine that 100 potential token options could be selected as the next
    token in a completion. A Top P value of 0.1 could be used to limit the options
    to 10, thereby reducing the number of tokens that could be selected. But this
    is still dependent on the number of possible token options derived from the model.
    So, if there were only 10 potential options, a 0.5 Top P setting would limit that
    to five options – reducing the variability. Furthermore, a Top P value of 0 will
    always reduce the options to the top token, meaning that even if there are a lot
    of possible options and the temperature setting was 1 (which will generate the
    most options), if the Top P setting is 0, only the best option will be selected,
    which will have the same effect as setting the temperature to 0 – you''ll most
    likely always get the same result. To illustrate this, let''s look at our **Once
    upon a time** prompt again. This time, we''ll set the temperature to 1 and the
    Top P value to 0, as we have in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Top P example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Top P example
  prefs: []
  type: TYPE_NORMAL
- en: 'If we submit the prompt with the settings in the previous screenshot three
    times, we get the following results (the same completion), even though the temperature
    is set to 1\. This is because Top P is limiting the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Once upon a time*, there was a little girl who was very sad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, although the temperature and Top P settings both influence the seeming randomness
    of a completion, they are interrelated, and each can affect the other. This is
    what makes them a bit confusing if you're not clear on how they work. Because
    of this, you're usually best off using each setting separately. So, if you want
    to influence the randomness with the temperature, make the Top P setting 1 and
    only vary the temperature. If you want to influence the randomness with the Top
    P value, set the temperature to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency and presence penalty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Frequency** and **Presence Penalty** settings can also be a bit confusing
    because they can seem similar to temperature or Top P in that they are settings
    to control variability. However, rather than considering the model, the frequency
    and presence penalty settings consider the prompt text and previous completion
    text to influence the tokens that are selected for the next completion. So, these
    two settings can provide some control over what new text is generated based on
    existing text.
  prefs: []
  type: TYPE_NORMAL
- en: The frequency and presence penalty settings can be useful for preventing the
    same completion text from being repeated across multiple requests. The two settings
    are very similar to one another, with the only difference being that the frequency
    penalty is applied if the text exists multiple times, whereas the presence penalty
    is applied if the text exists at all. Let's take a look at another example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of the **Once upon a time example**
    with a temperature setting of 1, a Top P setting of 0, and no frequency or presence
    penalty. The submit button was clicked 5 times and each completion generated a
    new sentence. While no sentences are repeated verbatim, there are a number of
    repeated token sequences – notice that each completion sentence begins with the
    words **She was sad because she had no**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Frequency and presence example 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Frequency and presence example 1
  prefs: []
  type: TYPE_NORMAL
- en: For the previous example, we could have added the presence or frequency penalty
    to limit the likelihood that each completion would be so similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next screenshot shows the results after adding a presence penalty and clicking
    **Submit** five times (like we did for the last example). This time, you can see
    that the completion sentences are not as similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Frequency and presence example 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – Frequency and presence example 2
  prefs: []
  type: TYPE_NORMAL
- en: The frequency penalty penalizes new tokens based on how often the token already
    exists in the text. The presence penalty, on the other hand, penalizes tokens
    if they exist in the text at all. In both cases, the value can be between 0 and
    1, and a higher value increases the penalty, thereby reducing the likelihood of
    duplications.
  prefs: []
  type: TYPE_NORMAL
- en: Best of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **best of** setting will cause the model to generate multiple completions
    on the server side and return the best of *x* completions (where *x* is the *best
    of* value). This can be used to help get the best quality results without having
    to make multiple requests to the API. However, the thing to consider when using
    *best of* is that you get charged for the tokens used for each of the completions
    generated. For example, if your response length was 50 and you set the best of
    value to 10, the response would consume 500 tokens. So, if you're providing a
    best of value, make sure to set the response length value as low as possible to
    minimize the number of tokens used. You can also use the stop sequence setting
    to help limit unnecessary tokens being used.
  prefs: []
  type: TYPE_NORMAL
- en: Stop sequence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **stop sequence** is a text sequence that will cause a completion to end when
    the sequence is encountered in the completion. You can provide up to four sequences.
    For example, if you wanted to limit a completion to text that came before a period
    followed by a carriage return, you would provide a period and a return as stop
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Playground, you enter a stop sequence by typing the stop sequences followed
    by the tab button to complete your entry. The following screenshot shows a carriage
    return as a stop sequence. For this example, the return button was entered followed
    by the tab key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Stop sequence'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Stop sequence
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Inject Start Text and Inject Restart Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Inject Start Text** and **Inject Restart Text** inputs insert text at
    the beginning or end of a completion, respectively. These settings can be used
    to help ensure that the desired pattern is continued as part of a completion.
    Often, these settings are most helpful when they are used in conjunction with
    a stop sequence. Let''s take a look at an example. For example, let''s start with
    the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the default Playground settings, the completions might look something
    like the completion in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Default settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Default settings
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous screenshot, you can see that the engine did a good job figuring
    out that this was a conversation and continued with a dialog. However, suppose
    you don''t want the completion to generate the human side of the conversation
    and you want to use the label `AI:` rather than `Assistant:`? If that was the
    case, you could use a stop sequence to end the completion before the human side
    of the conversation is generated. Then you could use an inject restart text value
    to prompt for the human input. Finally, the inject start text value could be set
    to a carriage return followed by `AI:` to begin the assistant''s response. The
    following screenshot shows what a completion might look like with those settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Using Stop Sequences, Inject Start Text, and Inject Restart
    Text together'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – Using Stop Sequences, Inject Start Text, and Inject Restart Text
    together
  prefs: []
  type: TYPE_NORMAL
- en: With the settings used in the previous screenshot, you can see that the completion
    ends before the human side of the conversation is generated. Then, the restart
    text, **Human:**, is appended to the completion, prompting human input.
  prefs: []
  type: TYPE_NORMAL
- en: Show Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the playground, the **Show Probabilities** option toggles on text highlighting,
    showing how likely a token was to be generated. This lets you examine the options
    that could have been used in the completion, which can be helpful when you''re
    trying to troubleshoot a completion. It can also help see alternative options
    that you might want to use. To use show probabilities, you toggle it on by selecting
    one of the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Most Likely**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Least Likely**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full Spectrum**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **most likely** value will show the most likely tokens to be selected,
    **least likely** will show the least likely tokens that could have been selected,
    and **full spectrum** will show the range of tokens that could have been selected.
    The following screenshot shows an example. In this example, the input prompt was
    just **Hi,**. The settings used were the defaults, except the response length,
    which was set to 1, and the show probabilities were set to **Most Likely**. You
    can see that the completion was the word **my**, but some other likely options
    were considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Show Probabilities – most likely tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Show Probabilities – most likely tokens
  prefs: []
  type: TYPE_NORMAL
- en: The settings provide a lot of control over how a completion is generated, but
    selecting the right setting can take a bit of trial and error. Thankfully, the
    Playground includes presets to help you understand how to best select the right
    combination of settings for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: Working with presets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16854_02_ePub_AM.xhtml#_idTextAnchor038), *GPT 3 Applications
    and Use Cases*, we briefly introduced presets in the Playground. Specifically,
    we looked at the English to French preset, but that's just one of many. Presets
    are like templates that provide an example prompt, along with the Playground settings.
    They are a great starting point for creating new prompts or as a tool for getting
    familiar with prompt design and setting usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of presets available, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chat**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q&A**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grammatical Standard English**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarize for a 2nd grader**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to command**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**English to French**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parse unstructured data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You''ll find a drop-down list of the presets in the Playground just above the
    large input box. The following screenshot shows the location:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Presets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_03_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – Presets
  prefs: []
  type: TYPE_NORMAL
- en: We won't review all of the presets, but let's take a look at a few to see how
    settings are used to help get the best possible completion from a prompt. The
    first one we'll review is the Grammatical Standard English preset.
  prefs: []
  type: TYPE_NORMAL
- en: Grammatical Standard English
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Grammatical Standard English** preset demonstrates a use case where non-standard
    US English text is transformed into standard US English text. The following is
    the preset''s prompt text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, presets also include settings. So, after selecting the Grammatical
    Standard English preset, you'll notice that some of the default Playground settings
    have changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default Playground settings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Engine*: davinci'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Response* *Length*: 64'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temperature*: 0.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top* *P*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Frequency Penalty*: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Presence Penalty*: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Best Of*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop Sequences*: empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Start Text*: empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Restart Text*: empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Show Probabilities*: Off'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But when you select a preset, some of the defaults will be updated. For the
    Grammatical Standard English preset, the following settings are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response Length*: 120'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temperature*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top P*: 0.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop Sequences*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Start Text*: Standard American English:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Restart Text*: Non-standard English:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the temperature is set to 1 and Top P is used to limit the results
    considered to 70% of the possible options. Also notice that the stop sequence
    is used along with the inject start text and inject restart text to keep the completion
    short while continuing the prompt pattern for the next phrase to standardize.
  prefs: []
  type: TYPE_NORMAL
- en: Text to command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Text to command** preset provides an example that shows how an English
    command could be converted to a machine command to send a message. The following
    is the prompt text for the Text to command preset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated settings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response Length*: 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temperature*: 0.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top P*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Frequency Penalty*: 0.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop Sequences*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Start Text*: A:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inject Restart Text*: Q:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this preset, notice that the temperature is set to 0.5 and a slight frequency
    penalty of 0.2 is used.
  prefs: []
  type: TYPE_NORMAL
- en: Parse unstructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Parse unstructured data** preset provides an example that shows how to
    extract values from unstructured text. The prompt provides a block of text, instructions,
    and a couple of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The settings used for the Parse unstructured data preset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response Length*: 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temperature*: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top P*: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop Sequences*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The settings worth noting in this preset are the temperature and Top P settings.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided an overview of the tools and resources available
    in the OpenAI developer console. We also took a closer look at the Playground
    and reviewed the Playground settings in more depth. We learned how to select the
    right engine, and also learned about using temperature and Top P, as well as frequency
    and presence penalties, along with other options. Finally, we looked at some of
    the presets to further understand how the settings can be used.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move beyond the Playground and start looking at
    using the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
