- en: Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter starts with a basic introduction to **Reinforcement Learning**
    (**RL**), including agents, state, actions, rewards, and policies. It extends
    to **Deep Learning** (**DL**)-based architectures for RL problems such as policy
    gradient methods, Deep-Q networks, and actor-critic models. This chapter will
    explain how to use these deep learning architectures with hands-on code to solve
    sequential decision-making problems in the OpenAI Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DL to tackle RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradients and code walk-through in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep-Q networks and code walk-through in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actor-critic networks and code walk-through in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of RL in the real world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL is a branch of machine learning where an agent learns to behave optimally
    in a given environment. The agent performs certain actions and observes the rewards/results.
    It learns the process of mapping situations to actions to maximize a reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RL process can be modeled as an iterative loop and can be represented as
    a mathematical framework called the **Markov Decision Process** (**MDP**). The
    following steps outline the process that takes place:'
  prefs: []
  type: TYPE_NORMAL
- en: RL agent receives state (*s[0]*) from the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RL agent takes an action (*a[0]*)¬†given the current state (*s[0]*). At this
    stage, the action it takes is random as the agent does not have any previous knowledge
    about the reward it could receive if it performs the action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the first action has taken place, the agent can now be considered to be
    in state *s[1]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, the environment gives a reward (*r[1]*) to the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loop is repeated continuously; it outputs a sequence of state and action
    and observes the reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is essentially an algorithm for learning an MDP policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cumulative rewards at each time step with respect to the given action can
    be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)'
  prefs: []
  type: TYPE_IMG
- en: In some applications, it may be beneficial to give more weight to rewards that
    are received sooner in time. For example, it would be better for you to receive
    ¬£100 today rather than in 5 years' time. To incorporate this, it is common to
    introduce a discount factor, ùõæ.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cumulative discounted rewards are represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)'
  prefs: []
  type: TYPE_IMG
- en: An important consideration in RL is the fact that rewards may be infrequent
    and delayed. In cases where there is a long-delayed reward, it can be challenging
    to trace back which sequence of actions contributed to the award.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model-based RL mimics the behavior of the environment. It predicts the next
    state after taking an action. It can be represented mathematically as a probability
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9b54c93-04ea-441c-a726-90485becbb49.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p* denotes the model, *x* is the state, and *a* is the control or action.
  prefs: []
  type: TYPE_NORMAL
- en: 'This notion can be demonstrated by considering the cart-pole example. The goal
    is for the pole attached to the cart to stay upright and the agent can decide
    between two possible actions: to move the cart left or to move the cart right.
    In the following screenshot, P models the angle of the pole after taking an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram depicts the probability distribution output for Œ∏ in
    the subsequent time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d739342a-0b14-480d-b1d7-0d627b8fab81.png)'
  prefs: []
  type: TYPE_IMG
- en: In this instance, the model describes the law of physics; however, the model
    could be anything depending on the application. Another example is that the model
    could be built on¬†the rules of a game of chess.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core concept of model-based RL is to use the model along with the cost
    function to locate the optimal path of actions or, in other words, the trajectory
    of states and actions, ùùâ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The drawback of model-based algorithms is that they can become impractical as
    the state space and action space become larger.
  prefs: []
  type: TYPE_NORMAL
- en: Model-free RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model-free algorithms rely on trial and error to update their knowledge. As
    such, they do not require space to store all of the combinations of states and
    actions. Policy gradients, value learning, or other model-free RL are used to
    find a policy that takes the best actions for maximum rewards. A key difference
    between model-free and model-based methods is that model-free methods act in the
    real environment to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing on-policy and off-policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The policy defines how the agent behaves; it tells the agent how to act in each
    state. Every RL algorithm must follow some type of policy to decide how it will
    act.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy function the agent is trying to learn can be represented as follows,
    where *Œ∏* is the parameter vector, *s* is a particular state, and *a* is an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)'
  prefs: []
  type: TYPE_IMG
- en: An on-policy agent learns the value (the expected discounted rewards) based
    on its current action and is derived from the current policy. Off-policy learns
    the value based on the action obtained from another policy such as a greed policy
    as in Q-learning, which we introduce next.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is a model-free RL algorithm whereby a table is created that calculates
    the maximum expected future reward for each action at each state. It is considered
    off-policy because the Q-learning function learns from actions that are outside
    of the current policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'When Q-learning is performed, a Q-table is created whereby the columns represent
    the possible actions and the rows represent the states. The value of each cell
    in the Q-table will be the maximum expected future reward for that given state
    and action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)'
  prefs: []
  type: TYPE_IMG
- en: Each Q-table score will be the maximum expected future reward if taking the
    action from the best policy. To learn each value in the Q-table, the Q-learning
    algorithm is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Q-function** (or **action-value function**) takes two inputs: state and
    reward. The Q-function returns the expected future reward of that action at that
    state. It can be represented as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)'
  prefs: []
  type: TYPE_IMG
- en: The Q-function essentially scrolls through the Q-table to find the row associated
    with the current state and the column associated with the action. From here, it
    returns the Q value that is the corresponding expected future reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this in the cart-pole example shown in the following figure. In its
    current state, moving left should have a higher Q-value than moving right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f82716fa-e071-471f-a14b-d0a302ed4792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the environment is explored, the Q-table is updated to give better approximations.
    The process of the Q-learning algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q-table is initialized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An action in the current state (*s*) is chosen based on the current Q-value
    estimates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An action (*a*) is performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward (*r*) is measured.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q is updated using the Bellman equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Steps 2-5* are repeated until the maximum number of episodes is reached or
    until the training is manually stopped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q-learning algorithm can be expressed as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Value methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Value learning is an approach that can often be a key building block in many
    RL methods. A value function,¬†*V(s)*, represents how good the state is that the
    agent is in. It is equal to the expected total reward for an agent starting from
    the state,¬†*s*. The total expected reward is dependent on the policy by which
    the agent chooses actions to perform. If the agent uses a given policy (ùõë) to
    select its actions, the corresponding value function is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07835c7c-d756-46bf-b0fb-282981fb0c05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering this in the cart-pole example, we could use the length of time
    the pole stays up in order to measure the rewards. In the following screenshot,
    there is a higher probability that the pole will stay upright for state s1 compared
    to state s2\. As such, for most policies, the state s1 is likely to have a higher
    value function (a higher expected future reward):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is an optimal value function that has a higher value than other functions
    for all states and this can be represented as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is an optimal policy that corresponds to the optimal value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)'
  prefs: []
  type: TYPE_IMG
- en: There are several different ways in which the optimal policy can be found. This
    is referred to as policy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Value iteration is a process that computes the optimal state-value function
    by iteratively improving estimates of *V(s)*. Firstly, the algorithm initializes
    *V(s)* to be random values. From there, it repeatedly updates the values of *Q(s,
    a)* and *V(s)* until they converge. Value iteration converges to the optimal values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the value iteration pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)'
  prefs: []
  type: TYPE_IMG
- en: Coded example ‚Äì value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To consider an example of this, we will use the Frozen Lake environment in
    OpenAI Gym. In the environment, the player is to imagine they are standing on
    a frozen lake where not all of it is frozen. The goal is to move from place S
    to place¬†G without falling into the holes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The letters on the grid represent the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'S: Starting point, safe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F: Frozen surface, safe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'H: Hole, not safe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'G: Goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are four possible moves that the agent can take: left, right, down, and
    up, which are represented as 0, 1, 2, and 3 respectively. As such, there are 16
    possible states to be in (4 x 4). For every H state, the agent receives a reward
    of -1 and upon reaching the goal, receives a reward of +1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement value-iteration in code, we first import the relevant libraries
    we wish to use and initialize the `FrozenLake` environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we assign the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we initialize the Q-table where `env.observation_space.n` is the
    number of states and `env.action_space.n` is the number of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the functions for the agent to choose an action and learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can begin running the episodes and export the Q-table to a pickled
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the process in action by running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In policy-based RL, the goal is to find the policy that makes the most rewarding
    decisions and can be represented mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49c33b1b-779e-4963-8486-f7788f2e7333.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Policies in RL can be either deterministic or stochastic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Stochastic policies output a probability distribution rather than a single
    discrete value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can represent the objective mathematically as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During value iteration, sometimes the optimal policy converges before the value
    function does since it only cares about finding the optimal policy. Another algorithm
    called policy iteration can be performed to reach the optimal policy. This is
    when, after each policy evaluation has taken place, the policy of the next is
    based on the value function until the policy converges. The following diagram
    illustrates the policy iteration process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)'
  prefs: []
  type: TYPE_IMG
- en: As such, the policy iteration algorithm is guaranteed to converge to the optimal
    policy, unlike the value iteration algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the policy iteration pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)'
  prefs: []
  type: TYPE_IMG
- en: Coded example¬†‚Äì policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we consider a coded example of this using the Frozen Lake environment
    as earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the relevant libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the functions to run an episode and return the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can define functions to evaluate the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, define the functions to extract the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, define the function to compute the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can run policy iteration on the Frozen Lake environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We observe that it has converged at *step 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)'
  prefs: []
  type: TYPE_IMG
- en: Value iteration versus policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Where the agent is assumed to have some prior knowledge about the effects of
    its actions on the environment, both value and policy iteration algorithms can
    be used. The algorithms assume that the MDP model is known. Policy iteration,
    however, is more computationally efficient as it often takes a lower number of
    iterations to converge.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Policy gradient is also an approach to solve reinforcement learning problems
    and aims to model and optimize the policy directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In policy gradients, the following steps are taken:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent observes the state of the environment (*s*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent takes action *u*¬†based on their instincts (a policy, œÄ) about the
    state (*s*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent moves and the environment changes; a new state is formed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent takes further actions based on the observed state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After a trajectory (œÑ) of motions, the agent adjusts its instinct based on the
    total rewards,¬†*R(œÑ)*¬†,received.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The policy gradient theorem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The derivative of the expected reward is the expectation of the product of
    the reward and gradient of the log of the policy œÄ[Œ∏]‚Äã*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86474d27-5e94-4d57-bf41-ee2165f51e90.png)'
  prefs: []
  type: TYPE_IMG
- en: Coded example¬†‚Äì policy gradient algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we use the OpenAI environment named CartPole where the goal
    is for the pole attached to the cart to stay upright for as long as possible.
    The agent receives a reward for every time step taken in which the pole remains
    balanced. If the pole falls over, the episode ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7df31996-e074-44c3-8f28-cfa7e066dd53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At any point in time, the cart and pole are in a state, *s*. This state is
    represented by a vector of four elements, namely, pole angle, pole velocity, cart
    position, and cart velocity. The agent can decide between two possible actions:
    to move the cart left or to move the cart right.'
  prefs: []
  type: TYPE_NORMAL
- en: A policy gradient takes small steps and updates the policy based on the reward
    that is associated with the step. It does this so that it can train the agent
    without having to map the value for every pair of states and actions in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will apply a technique called the Monte-Carlo policy gradient.
    Using this approach, the agent will update the policy at the end of each episode
    based on the rewards obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first import the relevant libraries that we plan to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define a feedforward neural network with one hidden layer of 128 neurons
    and a dropout of 0.5\. We use Adam as the optimizer and a learning rate of 0.02\.
    Using a dropout significantly improves the performance of the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define a `choose_action` function. This function chooses an action
    based on the policy distribution with the aid of the PyTorch distributions package.
    The policy returns a probability for each possible action on the action space
    as an array. In our example, this is to move left or to move right so the output
    could be [0.1, 0.9]. Based on these probabilities, the action is chosen, the history
    is recorded, and the action is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To update the policy, we take a sample of the Q-function (action-value function).
    Recall that this is the expected return by taking an action in a state by following
    the policy, œÄ. We can calculate the policy gradient at each time step using the
    fact that there is a reward of 1 for every step the pole remains vertical. We
    use the long term reward (*vt*) where this is the discounted sum of all future
    rewards for the length of the episode. As such, the longer the episode, the greater
    the reward for a state-action pair in the present where gamma is the discount
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The discounted reward vector is denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if an episode lasts 4 time steps, the reward for each step would
    be [4.90, 3.94, 2.97, 1.99]. From here, we can scale the reward vector by subtracting
    the mean and dividing by the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the policy after each episode, we apply the Monte-Carlo policy gradient,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01176f23-63b1-47ff-9cee-789c93f290af.png)'
  prefs: []
  type: TYPE_IMG
- en: This policy is then multiplied by the rewards and fed into the optimizer and
    the weights of the neural network are updated using stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function defines how we can update the policy in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we define the main policy training loop. At every step in a training
    episode, an action is chosen and the new state and reward are recorded. The `update_policy`
    function is called at the end of each episode to feed the episode history into
    the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Deep Q-networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep-Q Networks** (**DQNs**) combine deep learning and RL to learn in several
    different applications, particularly computer games. Let''s consider a simplified
    example of a game where there is a mouse in a maze and where the goal is for the
    mouse to eat as much cheese as possible. The more cheese the mouse eats, the more
    points it gets in the game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, the RL terms would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent: The mouse as this is controlled by the computer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State: The current moment in the game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action: A decision made by the mouse (to move left, right, up, or down)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward: The score in the game/amount of cheese the mouse has eaten‚Äîin other
    words, the value that the agent is trying to maximize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DQNs use Q-learning to learn the best action in a given state. They use **convolutional
    neural networks** (**ConvNets**) as a function approximator for the Q-learning
    function. ConvNets use convolutional layers to find spatial features such as where
    the mouse currently is in the grid. This means that the agent only has to learn
    Q-values for a few million rather than billions of different game states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/048486f3-6d05-4c7b-b741-dab2757bf1af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of a DQN architecture when learning the mouse maze game is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The current state (maze screen) is fed as input into the DQN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input is passed through convolutional layers to find spatial patterns in
    the image. Note that no pooling is used as it is important to know the spatial
    position when modeling computer games.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the convolutional layers is fed into fully connected linear layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the linear layers gives the probability that the DQN will take
    an action given its current state (move up, down, left, or right).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DQN loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The DQN requires a loss function for it to improve and get a higher score.
    This function can be mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76163220-a598-499f-98fa-842db8be916f.png)'
  prefs: []
  type: TYPE_IMG
- en: It is the Q-network that chooses which actions to take. The target network is
    what is used as an approximation for the ground truth. If we consider a scenario
    where the Q-network predicted that the correction action in a particular state
    was to move left with 80% certainty and the target network advises to move left,
    we could tweak the parameters of the Q-network using backpropagation to make it
    more likely to predict "move left" in that state. In other words, we backpropagate
    the loss through the DQN and adjust the weights of the Q-network to reduce the
    overall loss. The loss equation aims to make the probabilities of the move to
    be nearer 100% certainty of the correct choice.
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An experience consists of the current state, the action, the reward, and the
    next state. Each experience the agents get is recorded in the experience replay
    memory. An experience from the replay memory is sampled at random to train the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay has some key advantages when compared to traditional Q-learning.
    One such advantage is that, as each experience has the potential to be used to
    train the DQN's neural network multiple times, there is greater data efficiency.
    Another advantage is that, when it is learning from experiences as soon as they
    are obtained, it is the current parameters that determine the next sample that
    the parameters are trained on. If we consider this in the maze example, if the
    next best action is to move left, then the training samples will be dominated
    by those from the left-hand side of the screen. Such behavior can cause the DQN
    to get stuck in local minima. With the incorporation of experience replay, the
    experiences used to train the DQN originate from many different points in time,
    smoothing out the learning and helping to avoid poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example¬†‚Äì DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will again consider the `CartPole-v0` environment from OpenAI
    Gym.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a class that will permit us to incorporate experience replay
    when training the DQN. This essentially stores the transitions observed by the
    agent. The transitions that build up a batch are decorrelated by the sampling
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the ConvNet model whereby the difference between the current and previous
    screen patches are fed into it. The model has two outputs‚Äî*Q(s,left) and Q(s,right)*.
    The network is trying to predict the expected reward/return of taking an action
    given the current input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the hyperparameters of the model along with some utilities for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have the code for training our model. This function performs a
    single step of the optimization. First, it samples a batch and concatenates all
    of the tensors into a single one. It computes *Q(st,at)* and *V(st+1)=maxaQ(st+1,a)*
    and combines them into a loss. By definition, we set *V(s)=0* if *s* is a terminal
    state. We also use a target network to compute *V(st+1)* for added stability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute a mask of non-final states. After this, we concatenate the batch elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute *Q(s_t, a)*, then select the columns of actions taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the expected Q values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then compute the Huber loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we optimize the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we observe the new state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We store the transition in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform one step of the optimization (on the target network):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the target network; copy all weights and biases in the DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs some visualizations to give insight into how the model performs
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d983eb9e-2dd3-407d-9965-184d98e4f351.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram summarizes what the model in this coded example is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)'
  prefs: []
  type: TYPE_IMG
- en: Double deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Double deep Q-learning generally leads to better performance of the AI agents
    when compared to vanilla DQNs. A common problem with deep Q-learning is that,
    sometimes, the agents can learn unrealistically high action values because it
    includes a maximization step over estimated action values. This tends to prefer
    overestimated to underestimated values. If overestimations are not uniform and
    not concentrated at states about which we wish to learn more, then these can negatively
    affect the quality of the resulting policy.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of double Q-learning is to reduce these overestimations. It does this
    by decomposing the max operation in the target into action selection and action
    evaluation. In the vanilla DQN implementation, the action selection and action
    evaluation are coupled. It uses the target network to select the action and, at
    the same time, to estimate the quality of the action.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the target-network to select the action and at the same time to
    estimate the quality of the action. Double Q-learning essentially tries to decouple
    both procedures from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In double Q-learning, the temporal difference (TD) target looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70934206-aa2d-46c4-b277-fbc831cef8f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The calculation of new TD target can be summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Q-Network uses the next state, *s'*, to calculate qualities, *Q(s',a)*, for
    each possible action, *a*, in the state, *s'*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `argmax` operation applied on *Q(s',a)* chooses the action, *a**, that belongs
    to the highest quality (action selection).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The quality *Q(s',a*)*, that belongs to the action, *a**, is selected for the
    calculation of the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The process of double Q-learning can be visualized as per the following diagram.
    An AI agent is at the initial in state s; it knows, based on some previous calculations,
    the qualities Q(s, a[1]) and Q(s, a[2]) for possible two actions in that state.
    The agent then decides to take action a[1] and ends up in state s'':'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e883c737-107a-4652-b211-e9d4815d0c46.png)'
  prefs: []
  type: TYPE_IMG
- en: Actor-critic methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Actor-critic methods aim to incorporate the advantages of both value- and policy-based
    methods while eliminating their drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0968abce-a6e7-461f-8549-37d45a09c41e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The fundamental idea behind actor-critics is to split the model into two parts:
    one for computing an action based on a state and another to produce the Q-values
    of the action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor is a neural network that takes the state as input and outputs the
    best action. By learning the optimal policy, it controls how the agent behaves.
    The critic evaluates the action by computing the value function. In other words,
    the actor tries to optimize the policy and the critic tries to optimize the value.
    The two models improve over time at their individual roles and, as such, the overall
    architecture learns more efficiently than the two methods separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f65da315-87b4-4a85-9b63-15004169d6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The two models are essentially competing with one another. Such an approach
    is becoming increasingly popular in the field of machine learning; for example,
    this is also present in generative adversarial networks.
  prefs: []
  type: TYPE_NORMAL
- en: The nature of the actor's role is to be exploratory. It frequently tries new
    things and explores the environment. The role of the critic is to either criticize
    or compliment the actions of the actor. The actor takes this feedback on board
    and adjusts its behavior accordingly. As the actor receives more and more feedback,
    it becomes better at deciding which actions to take.
  prefs: []
  type: TYPE_NORMAL
- en: Like a neural network, the actor can be a function approximator where its task
    is to produce the best action for a given state. This could be a fully connected
    or convolutional neural network, for example. The critic is also a function approximator,
    which receives the environment as input along with the action by the actor. It
    concatenates these inputs and outputs the action value (Q-value).
  prefs: []
  type: TYPE_NORMAL
- en: The two networks are trained separately and, to update their weights, they use
    gradient ascent as opposed to descent as it aims to determine the global maximum
    rather than minimum. Weights are updated at each step rather than at the end of
    an episode as opposed to policy gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critics have been proven to learn complex environments and have been used
    in many 2D and 3D computer games such as *Super Mario* and *Doom*.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example¬†‚Äì actor-critic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will consider a coded implementation example in PyTorch. First, we
    define the `ActorCritic` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we initialize the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that will choose the best action based on the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we need to define the function that calculates the total returns
    and considers the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train the model and review how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/776751b2-6167-4fa2-945b-a9722ced0530.png)'
  prefs: []
  type: TYPE_IMG
- en: Asynchronous actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Asynchronous Advantage Actor-Critic** or **A3C** is an algorithm proposed
    by Google''s DeepMind. The algorithm has been proven to outperform other algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In A3C, there are multiple instances of agents, each of which has been initialized
    differently in their own separate environments. Each individual agent begins to
    take actions and go through the reinforcement learning process to gather their
    own unique experiences. These unique experiences are then used to update the global
    neural network. This global neural network is shared by all of the agents and
    it influences all of the actions of the agents, and every new experience from
    each agent improves the overall network speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Advantage** term in the name is the value that states whether or not
    there is an improvement in an action compared to the expected average value of
    that state based on. The advantage formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A (s,a) = Q(s,a) - V(s)*'
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RL methods have been applied to solve problems in a multitude of areas in the
    real world. Here, we consider some examples of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Robotics**: There has been a significant amount of work on applying RL in
    the field of robotics. In the present day, manufacturing facilities are full of
    robots performing a variety of tasks, the foundations of which are RL methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/feda1bf9-3714-4899-9f98-97915440d76d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Traffic light control**: In the paper¬†*Reinforcement learning-based multi-agent
    system for network traffic signal control*, researchers designed a traffic light
    controller to solve congestion problems that demonstrated superior results to
    other methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Personalized recommendations**: RL has been applied in news recommendation
    systems to account for the fact that news changes rapidly and, as users tend to
    have a short attention span, the click-through rate alone cannot reflect the retention
    rate of the users:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/293bd127-5ca4-4eb7-8524-78d8385046ea.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Generating images**: There has been a lot of research into combining RL with
    other deep learning architectures for a host of different applications. Many of
    these have shown some impressive results. DeepMind showed that using generative
    models and RL, they were able to successfully generate images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3e043c7a-f238-4bee-83d6-3918928f53e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began by covering the basics of RL and introduced more advanced
    algorithms that have proven an ability to outperform humans in real-world scenarios.
    We also gave examples as to how these can be implemented in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, there will be an overview of this book, along
    with some tips on how you can keep yourself up to date with recent advancements
    in the data science space.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Brief Survey of Deep Reinforcement Learning*: [https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Playing Atari with Deep Reinforcement Learning*: [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning with Double Q-learning*: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous Control with Deep Reinforcement Learning*: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Asynchronous Methods for Deep Reinforcement Learning*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor*: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reinforcement learning-based multi-agent system for network traffic signal
    control*: [http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf](http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*End-to-End Training of Deep Visuomotor Policies*: [https://arxiv.org/pdf/1504.00702.pdf](https://arxiv.org/pdf/1504.00702.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DRN: A Deep Reinforcement Learning Framework for News Recommendation*: [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synthesizing Programs for Images using Reinforced Adversarial Learning*: [https://arxiv.org/pdf/1804.01118.pdf](https://arxiv.org/pdf/1804.01118.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
