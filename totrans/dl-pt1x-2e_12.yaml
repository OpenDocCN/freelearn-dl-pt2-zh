- en: Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: This chapter starts with a basic introduction to **Reinforcement Learning**
    (**RL**), including agents, state, actions, rewards, and policies. It extends
    to **Deep Learning** (**DL**)-based architectures for RL problems such as policy
    gradient methods, Deep-Q networks, and actor-critic models. This chapter will
    explain how to use these deep learning architectures with hands-on code to solve
    sequential decision-making problems in the OpenAI Gym environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从基本介绍**强化学习**（**RL**）开始，包括代理、状态、动作、奖励和策略。它扩展到基于**深度学习**（**DL**）的架构，用于解决RL问题，如策略梯度方法、深度Q网络和演员-评论家模型。本章将解释如何使用这些深度学习架构及其手动代码在OpenAI
    Gym环境中解决序列决策问题。
- en: 'Specifically, the following will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，将涵盖以下内容：
- en: Introduction to RL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Using DL to tackle RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习解决强化学习问题
- en: Policy gradients and code walk-through in PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度和PyTorch中的代码演示
- en: Deep-Q networks and code walk-through in PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q网络和PyTorch中的代码演示
- en: Actor-critic networks and code walk-through in PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-critic 网络和 PyTorch 中的代码演示
- en: Applications of RL in the real world
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习在现实世界中的应用
- en: Introduction to RL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: RL is a branch of machine learning where an agent learns to behave optimally
    in a given environment. The agent performs certain actions and observes the rewards/results.
    It learns the process of mapping situations to actions to maximize a reward.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，其中代理程序学习在给定环境中的最佳行为方式。代理程序执行某些动作并观察奖励/结果。它学习将情况映射到行动的过程，以最大化奖励。
- en: 'The RL process can be modeled as an iterative loop and can be represented as
    a mathematical framework called the **Markov Decision Process** (**MDP**). The
    following steps outline the process that takes place:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程可以建模为一个迭代循环，并可以表示为称为**马尔可夫决策过程**（**MDP**）的数学框架。以下步骤概述了该过程的进行：
- en: RL agent receives state (*s[0]*) from the environment.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习代理从环境中接收状态（*s[0]*）。
- en: The RL agent takes an action (*a[0]*) given the current state (*s[0]*). At this
    stage, the action it takes is random as the agent does not have any previous knowledge
    about the reward it could receive if it performs the action.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习代理根据当前状态（*s[0]*）采取行动（*a[0]*）。在此阶段，由于代理没有关于可能获得的奖励的先前知识，它采取的行动是随机的。
- en: After the first action has taken place, the agent can now be considered to be
    in state *s[1]*.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次行动发生后，代理现在可以被认为处于状态*s[1]*。
- en: At this point, the environment gives a reward (*r[1]*) to the agent.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，环境向代理提供奖励（*r[1]*）。
- en: This loop is repeated continuously; it outputs a sequence of state and action
    and observes the reward.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一循环持续重复；它输出一个状态和动作序列，并观察奖励。
- en: 'This process is essentially an algorithm for learning an MDP policy:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程本质上是一种学习MDP策略的算法：
- en: '![](img/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)'
- en: 'The cumulative rewards at each time step with respect to the given action can
    be represented as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步的累积奖励与给定行动相关的表示如下：
- en: '![](img/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)'
- en: In some applications, it may be beneficial to give more weight to rewards that
    are received sooner in time. For example, it would be better for you to receive
    £100 today rather than in 5 years' time. To incorporate this, it is common to
    introduce a discount factor, 𝛾.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，可能有利于更多地重视及时收到的奖励。例如，今天收到£100比5年后收到要好。为了纳入这一点，引入一个折扣因子𝛾是很常见的。
- en: 'The cumulative discounted rewards are represented as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 累积折扣奖励表示如下：
- en: '![](img/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)'
- en: An important consideration in RL is the fact that rewards may be infrequent
    and delayed. In cases where there is a long-delayed reward, it can be challenging
    to trace back which sequence of actions contributed to the award.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个重要考虑因素是奖励可能是不频繁和延迟的。在存在长时间延迟奖励的情况下，追溯哪些动作序列导致了奖励可能是具有挑战性的。
- en: Model-based RL
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的强化学习
- en: 'Model-based RL mimics the behavior of the environment. It predicts the next
    state after taking an action. It can be represented mathematically as a probability
    distribution:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习模拟环境的行为。它预测采取行动后的下一个状态。可以用概率分布的形式数学表示如下：
- en: '![](img/e9b54c93-04ea-441c-a726-90485becbb49.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9b54c93-04ea-441c-a726-90485becbb49.png)'
- en: Here, *p* denotes the model, *x* is the state, and *a* is the control or action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，*p*表示模型，*x*是状态，*a*是控制或动作。
- en: 'This notion can be demonstrated by considering the cart-pole example. The goal
    is for the pole attached to the cart to stay upright and the agent can decide
    between two possible actions: to move the cart left or to move the cart right.
    In the following screenshot, P models the angle of the pole after taking an action:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以通过考虑平衡杆示例来进行演示。目标是让附着在小车上的杆保持竖直，代理可以决定两种可能的动作之间的选择：将小车向左移动或将小车向右移动。在下面的截图中，P模拟了采取行动后杆的角度：
- en: '![](img/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)'
- en: 'The following diagram depicts the probability distribution output for θ in
    the subsequent time step:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了下一时间步中θ的概率分布输出：
- en: '![](img/d739342a-0b14-480d-b1d7-0d627b8fab81.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d739342a-0b14-480d-b1d7-0d627b8fab81.png)'
- en: In this instance, the model describes the law of physics; however, the model
    could be anything depending on the application. Another example is that the model
    could be built on the rules of a game of chess.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型描述了物理定律；然而，模型可能是任何应用的依据。另一个例子是模型可以建立在国际象棋游戏的规则上。
- en: 'The core concept of model-based RL is to use the model along with the cost
    function to locate the optimal path of actions or, in other words, the trajectory
    of states and actions, 𝝉:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习的核心概念是使用模型和成本函数来定位最佳路径的行动或者说状态和行动的轨迹，𝝉：
- en: '![](img/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)'
- en: The drawback of model-based algorithms is that they can become impractical as
    the state space and action space become larger.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的算法的缺点是，随着状态空间和动作空间的扩大，它们可能变得不切实际。
- en: Model-free RL
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无模型强化学习
- en: Model-free algorithms rely on trial and error to update their knowledge. As
    such, they do not require space to store all of the combinations of states and
    actions. Policy gradients, value learning, or other model-free RL are used to
    find a policy that takes the best actions for maximum rewards. A key difference
    between model-free and model-based methods is that model-free methods act in the
    real environment to learn.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型算法依赖试验和错误来更新其知识。因此，它们不需要空间来存储所有状态和动作的组合。策略梯度、值学习或其他无模型强化学习方法用于找到一个最大化奖励的最佳行动策略。无模型和基于模型方法的一个关键区别在于，无模型方法在真实环境中行动学习。
- en: Comparing on-policy and off-policy
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较基于策略和离策略
- en: The policy defines how the agent behaves; it tells the agent how to act in each
    state. Every RL algorithm must follow some type of policy to decide how it will
    act.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 策略定义了代理如何行动；它告诉代理在每个状态下应该如何行动。每个强化学习算法必须遵循某种策略来决定其行为方式。
- en: 'The policy function the agent is trying to learn can be represented as follows,
    where *θ* is the parameter vector, *s* is a particular state, and *a* is an action:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代理试图学习的策略函数可以表示如下，其中*θ*是参数向量，*s*是特定状态，*a*是一个动作：
- en: '![](img/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)'
- en: An on-policy agent learns the value (the expected discounted rewards) based
    on its current action and is derived from the current policy. Off-policy learns
    the value based on the action obtained from another policy such as a greed policy
    as in Q-learning, which we introduce next.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的代理学习值（期望的折扣奖励），基于当前动作并源于当前策略。离策略学习值则基于从另一策略（如贪婪策略，如我们接下来介绍的 Q-learning
    中）获得的动作。
- en: Q-learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is a model-free RL algorithm whereby a table is created that calculates
    the maximum expected future reward for each action at each state. It is considered
    off-policy because the Q-learning function learns from actions that are outside
    of the current policy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是一种无模型强化学习算法，通过创建一个表格来计算每个状态下每个动作的最大预期未来奖励。它被认为是离策略的，因为 Q-learning
    函数从当前策略之外的动作中学习。
- en: 'When Q-learning is performed, a Q-table is created whereby the columns represent
    the possible actions and the rows represent the states. The value of each cell
    in the Q-table will be the maximum expected future reward for that given state
    and action:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行 Q-learning 时，会创建一个 Q-表格，其中列代表可能的动作，行代表状态。Q-表格中每个单元格的值将是给定状态和动作的最大预期未来奖励：
- en: '![](img/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)'
- en: Each Q-table score will be the maximum expected future reward if taking the
    action from the best policy. To learn each value in the Q-table, the Q-learning
    algorithm is used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Q-表格分数将是从最佳策略中获取的动作的最大预期未来奖励。Q-learning 算法用于学习 Q-表格中的每个值。
- en: 'The **Q-function** (or **action-value function**) takes two inputs: state and
    reward. The Q-function returns the expected future reward of that action at that
    state. It can be represented as shown:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q 函数**（或**动作值函数**）接受两个输入：状态和奖励。Q 函数返回该状态下执行该动作的预期未来奖励。它可以如下表示：'
- en: '![](img/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)'
- en: The Q-function essentially scrolls through the Q-table to find the row associated
    with the current state and the column associated with the action. From here, it
    returns the Q value that is the corresponding expected future reward.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Q 函数基本上通过滚动 Q 表来查找与当前状态相关的行和与动作相关的列。从这里，它返回相应的预期未来奖励 Q 值。
- en: 'Consider this in the cart-pole example shown in the following figure. In its
    current state, moving left should have a higher Q-value than moving right:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在下图中展示的倒立摆示例中。在当前状态下，向左移动应该比向右移动具有更高的 Q 值：
- en: '![](img/f82716fa-e071-471f-a14b-d0a302ed4792.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f82716fa-e071-471f-a14b-d0a302ed4792.png)'
- en: 'As the environment is explored, the Q-table is updated to give better approximations.
    The process of the Q-learning algorithm is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随着环境的探索，Q 表将被更新以提供更好的近似值。Q 学习算法的过程如下：
- en: The Q-table is initialized.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 表。
- en: An action in the current state (*s*) is chosen based on the current Q-value
    estimates.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前的 Q 值估计，选择当前状态（*s*）中的一个动作。
- en: An action (*a*) is performed.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一个动作（*a*）。
- en: The reward (*r*) is measured.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励（*r*）被测量。
- en: Q is updated using the Bellman equation.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程更新 Q 值。
- en: 'The following is the Bellman equation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程如下所示：
- en: '![](img/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)'
- en: '*Steps 2-5* are repeated until the maximum number of episodes is reached or
    until the training is manually stopped.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 2-5* 重复执行，直到达到最大的回合数或手动停止训练。'
- en: 'The Q-learning algorithm can be expressed as the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习算法可以表示为以下方程：
- en: '![](img/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)'
- en: Value methods
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值方法
- en: 'Value learning is an approach that can often be a key building block in many
    RL methods. A value function, *V(s)*, represents how good the state is that the
    agent is in. It is equal to the expected total reward for an agent starting from
    the state, *s*. The total expected reward is dependent on the policy by which
    the agent chooses actions to perform. If the agent uses a given policy (𝛑) to
    select its actions, the corresponding value function is given by the following
    formula:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 值学习是许多强化学习方法的关键构建模块。值函数 *V(s)* 表示代理所处状态的好坏程度。它等于从状态 *s* 开始代理预期的总奖励。总预期奖励取决于代理通过选择动作执行的策略。如果代理使用给定策略（𝛑）选择其动作，则相应的值函数由以下公式给出：
- en: '![](img/07835c7c-d756-46bf-b0fb-282981fb0c05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07835c7c-d756-46bf-b0fb-282981fb0c05.png)'
- en: 'Considering this in the cart-pole example, we could use the length of time
    the pole stays up in order to measure the rewards. In the following screenshot,
    there is a higher probability that the pole will stay upright for state s1 compared
    to state s2\. As such, for most policies, the state s1 is likely to have a higher
    value function (a higher expected future reward):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在倒立摆示例中考虑到这一点，我们可以利用杆子保持直立的时间长度来衡量奖励。在下面的截图中，与状态 s2 相比，状态 s1 杆子保持直立的概率更高。因此，对于大多数策略而言，状态
    s1 的值函数可能更高（即更高的期望未来奖励）：
- en: '![](img/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)'
- en: 'There is an optimal value function that has a higher value than other functions
    for all states and this can be represented as the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个优化的值函数，其对所有状态都具有比其他函数更高的值，可以表示为以下形式：
- en: '![](img/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)'
- en: 'There is an optimal policy that corresponds to the optimal value function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个与最优值函数对应的最优策略：
- en: '![](img/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)'
- en: There are several different ways in which the optimal policy can be found. This
    is referred to as policy evaluation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过几种不同的方式找到最优策略。这称为策略评估。
- en: Value iteration
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代
- en: Value iteration is a process that computes the optimal state-value function
    by iteratively improving estimates of *V(s)*. Firstly, the algorithm initializes
    *V(s)* to be random values. From there, it repeatedly updates the values of *Q(s,
    a)* and *V(s)* until they converge. Value iteration converges to the optimal values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代是一个通过迭代改进*V(s)*估计来计算最优状态值函数的过程。首先，算法将*V(s)*初始化为随机值。然后，它重复更新*Q(s, a)*和*V(s)*的值，直到它们收敛。值迭代收敛到最优值。
- en: 'The following is the value iteration pseudocode:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是值迭代的伪代码：
- en: '![](img/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)'
- en: Coded example – value iteration
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 – 值迭代
- en: 'To consider an example of this, we will use the Frozen Lake environment in
    OpenAI Gym. In the environment, the player is to imagine they are standing on
    a frozen lake where not all of it is frozen. The goal is to move from place S
    to place G without falling into the holes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将使用OpenAI Gym中的Frozen Lake环境作为示例。在这个环境中，玩家需要想象自己站在一个部分冻结的湖面上。目标是从起点S移动到终点G而不掉入洞中：
- en: '![](img/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)'
- en: 'The letters on the grid represent the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 网格上的字母代表以下内容：
- en: 'S: Starting point, safe'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S：起点，安全
- en: 'F: Frozen surface, safe'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F：冻结表面，安全
- en: 'H: Hole, not safe'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H：洞，不安全
- en: 'G: Goal'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G：目标
- en: 'There are four possible moves that the agent can take: left, right, down, and
    up, which are represented as 0, 1, 2, and 3 respectively. As such, there are 16
    possible states to be in (4 x 4). For every H state, the agent receives a reward
    of -1 and upon reaching the goal, receives a reward of +1.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以采取四种可能的移动：左、右、下、上，分别表示为0、1、2、3。因此，有16种可能的状态（4 x 4）。对于每个H状态，代理会收到-1的奖励，并在达到目标时收到+1的奖励。
- en: 'To implement value-iteration in code, we first import the relevant libraries
    we wish to use and initialize the `FrozenLake` environment:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要在代码中实现值迭代，我们首先导入希望使用的相关库，并初始化`FrozenLake`环境：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we assign the variables:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们为变量赋值：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From here, we initialize the Q-table where `env.observation_space.n` is the
    number of states and `env.action_space.n` is the number of actions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们初始化Q表，其中`env.observation_space.n`是状态数，`env.action_space.n`是动作数。
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the functions for the agent to choose an action and learn:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 定义代理选择动作和学习的函数：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From here, we can begin running the episodes and export the Q-table to a pickled
    file:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以开始运行回合并将Q表导出到一个pickle文件中：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see the process in action by running the preceding code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行前面的代码来看到这个过程：
- en: '![](img/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)'
- en: Policy methods
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略方法
- en: 'In policy-based RL, the goal is to find the policy that makes the most rewarding
    decisions and can be represented mathematically as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的强化学习中，目标是找到能做出最有益决策的策略，可以数学表示如下：
- en: '![](img/49c33b1b-779e-4963-8486-f7788f2e7333.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49c33b1b-779e-4963-8486-f7788f2e7333.png)'
- en: 'Policies in RL can be either deterministic or stochastic:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的策略可以是确定性的，也可以是随机的：
- en: '![](img/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)'
- en: 'Stochastic policies output a probability distribution rather than a single
    discrete value:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略输出的是概率分布而不是单一的离散值：
- en: '![](img/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)'
- en: 'We can represent the objective mathematically as shown:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个目标数学化地表示如下：
- en: '![](img/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)'
- en: Policy iteration
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略迭代
- en: 'During value iteration, sometimes the optimal policy converges before the value
    function does since it only cares about finding the optimal policy. Another algorithm
    called policy iteration can be performed to reach the optimal policy. This is
    when, after each policy evaluation has taken place, the policy of the next is
    based on the value function until the policy converges. The following diagram
    illustrates the policy iteration process:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在值迭代过程中，有时最优策略在值函数之前收敛，因为它只关心找到最优策略。还可以执行另一个称为策略迭代的算法来达到最优策略。这是在每次策略评估之后，下一个策略基于值函数直到策略收敛的过程。以下图示展示了策略迭代过程：
- en: '![](img/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)'
- en: As such, the policy iteration algorithm is guaranteed to converge to the optimal
    policy, unlike the value iteration algorithm.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，策略迭代算法保证收敛到最优策略，而值迭代算法则不一定。
- en: 'The following is the policy iteration pseudocode:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是策略迭代的伪代码：
- en: '![](img/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)'
- en: Coded example – policy iteration
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - 策略迭代
- en: Here, we consider a coded example of this using the Frozen Lake environment
    as earlier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们考虑使用之前介绍的Frozen Lake环境的编码示例。
- en: 'First, import the relevant libraries:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入相关的库：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we define the functions to run an episode and return the reward:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义函数来运行一个episode并返回奖励：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From here, we can define functions to evaluate the policy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以定义评估策略的函数：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, define the functions to extract the policy:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义提取策略的函数：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, define the function to compute the policy:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定义计算策略的函数：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From here, we can run policy iteration on the Frozen Lake environment:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以在Frozen Lake环境上运行策略迭代：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We observe that it has converged at *step 5*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到它在 *步骤 5* 处收敛：
- en: '![](img/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)'
- en: Value iteration versus policy iteration
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价值迭代与策略迭代的比较
- en: Where the agent is assumed to have some prior knowledge about the effects of
    its actions on the environment, both value and policy iteration algorithms can
    be used. The algorithms assume that the MDP model is known. Policy iteration,
    however, is more computationally efficient as it often takes a lower number of
    iterations to converge.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理假定对其在环境中的行为影响有一些先验知识时，可以使用价值迭代和策略迭代算法。这些算法假定已知马尔可夫决策过程（MDP）模型。然而，策略迭代通常更加计算效率高，因为它往往需要更少的迭代次数才能收敛。
- en: Policy gradient algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度算法
- en: 'Policy gradient is also an approach to solve reinforcement learning problems
    and aims to model and optimize the policy directly:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度也是解决强化学习问题的一种方法，旨在直接建模和优化策略：
- en: '![](img/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)'
- en: 'In policy gradients, the following steps are taken:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度中，采取以下步骤：
- en: The agent observes the state of the environment (*s*).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理观察环境的状态 (*s*)。
- en: The agent takes action *u* based on their instincts (a policy, π) about the
    state (*s*).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理根据他们对状态 (*s*) 的本能（即策略π）采取行动 *u*。
- en: The agent moves and the environment changes; a new state is formed.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理移动并且环境改变；形成一个新的状态。
- en: The agent takes further actions based on the observed state.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理根据观察到的环境状态进一步采取行动。
- en: After a trajectory (τ) of motions, the agent adjusts its instinct based on the
    total rewards, *R(τ)* ,received.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运动轨迹（τ）之后，代理根据所获得的总奖励 *R(τ)* 调整其本能。
- en: 'The policy gradient theorem is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定理如下：
- en: '*The derivative of the expected reward is the expectation of the product of
    the reward and gradient of the log of the policy π[θ]​*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*预期奖励的导数是策略π[θ]​的对数梯度与奖励乘积的期望*。'
- en: '![](img/86474d27-5e94-4d57-bf41-ee2165f51e90.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86474d27-5e94-4d57-bf41-ee2165f51e90.png)'
- en: Coded example – policy gradient algorithm
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - 策略梯度算法
- en: 'In this example, we use the OpenAI environment named CartPole where the goal
    is for the pole attached to the cart to stay upright for as long as possible.
    The agent receives a reward for every time step taken in which the pole remains
    balanced. If the pole falls over, the episode ends:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用名为CartPole的OpenAI环境，目标是让连接到小车上的杆尽可能长时间保持直立。代理在每个时间步长内保持杆平衡时会获得奖励。如果杆倒下，那么这一集结束：
- en: '![](img/7df31996-e074-44c3-8f28-cfa7e066dd53.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7df31996-e074-44c3-8f28-cfa7e066dd53.png)'
- en: 'At any point in time, the cart and pole are in a state, *s*. This state is
    represented by a vector of four elements, namely, pole angle, pole velocity, cart
    position, and cart velocity. The agent can decide between two possible actions:
    to move the cart left or to move the cart right.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时刻，小车和杆都处于一个状态 *s*。该状态由四个元素的向量表示，即杆角度、杆速度、小车位置和小车速度。代理可以选择两种可能的动作：向左移动小车或向右移动小车。
- en: A policy gradient takes small steps and updates the policy based on the reward
    that is associated with the step. It does this so that it can train the agent
    without having to map the value for every pair of states and actions in the environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度采取小步骤，并根据与步骤相关的奖励更新策略。这样做可以训练代理，而无需在环境中为每对状态和动作映射价值。
- en: In this example, we will apply a technique called the Monte-Carlo policy gradient.
    Using this approach, the agent will update the policy at the end of each episode
    based on the rewards obtained.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将应用一种称为蒙特卡洛策略梯度的技术。使用这种方法，代理将根据获得的奖励在每个 episode 结束时更新策略。
- en: 'We will first import the relevant libraries that we plan to use:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入我们计划使用的相关库：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we define a feedforward neural network with one hidden layer of 128 neurons
    and a dropout of 0.5\. We use Adam as the optimizer and a learning rate of 0.02\.
    Using a dropout significantly improves the performance of the policy:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个前馈神经网络，包含一个隐藏层，有 128 个神经元和 0.5 的 dropout。我们使用 Adam 作为优化器，学习率为 0.02。使用
    dropout 显著提高了策略的性能：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we define a `choose_action` function. This function chooses an action
    based on the policy distribution with the aid of the PyTorch distributions package.
    The policy returns a probability for each possible action on the action space
    as an array. In our example, this is to move left or to move right so the output
    could be [0.1, 0.9]. Based on these probabilities, the action is chosen, the history
    is recorded, and the action is returned:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个`choose_action`函数。这个函数根据策略分布选择动作，使用了 PyTorch 分布包。策略返回一个数组，表示动作空间上每个可能动作的概率。在我们的例子中，可以是向左移动或向右移动，因此输出可能是[0.1,
    0.9]。根据这些概率选择动作，记录历史并返回动作：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To update the policy, we take a sample of the Q-function (action-value function).
    Recall that this is the expected return by taking an action in a state by following
    the policy, π. We can calculate the policy gradient at each time step using the
    fact that there is a reward of 1 for every step the pole remains vertical. We
    use the long term reward (*vt*) where this is the discounted sum of all future
    rewards for the length of the episode. As such, the longer the episode, the greater
    the reward for a state-action pair in the present where gamma is the discount
    factor.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新策略，我们从 Q 函数（动作值函数）中取样。回想一下，这是通过遵循策略 π 在状态中采取行动来预期的回报。我们可以使用每个时间步长的策略梯度来计算，其中在杆保持垂直的每个步骤都有
    1 的奖励。我们使用长期奖励 (*vt*)，这是整个 episode 期间所有未来奖励的折现总和。因此，episode 越长，当前状态-动作对的奖励越大，其中
    gamma 是折现因子。
- en: 'The discounted reward vector is denoted as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 折现奖励向量表示如下：
- en: '![](img/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)'
- en: For example, if an episode lasts 4 time steps, the reward for each step would
    be [4.90, 3.94, 2.97, 1.99]. From here, we can scale the reward vector by subtracting
    the mean and dividing by the standard deviation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个 episode 持续 4 个时间步，每步的奖励将分别是 [4.90, 3.94, 2.97, 1.99]。从这里，我们可以通过减去均值并除以标准差来缩放奖励向量。
- en: 'To improve the policy after each episode, we apply the Monte-Carlo policy gradient,
    as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 episode 结束后，我们应用蒙特卡洛策略梯度来改进策略，如下所示：
- en: '![](img/01176f23-63b1-47ff-9cee-789c93f290af.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01176f23-63b1-47ff-9cee-789c93f290af.png)'
- en: This policy is then multiplied by the rewards and fed into the optimizer and
    the weights of the neural network are updated using stochastic gradient descent.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个策略乘以奖励值，输入优化器，使用随机梯度下降更新神经网络的权重。
- en: 'The following function defines how we can update the policy in code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数定义了我们如何在代码中更新策略：
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'From here, we define the main policy training loop. At every step in a training
    episode, an action is chosen and the new state and reward are recorded. The `update_policy`
    function is called at the end of each episode to feed the episode history into
    the neural network:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义主策略训练循环。在每个训练 episode 的每一步，选择一个动作并记录新状态和奖励。在每个 episode 结束时调用`update_policy`函数，将
    episode 历史传入神经网络：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Deep Q-networks
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 网络
- en: '**Deep-Q Networks** (**DQNs**) combine deep learning and RL to learn in several
    different applications, particularly computer games. Let''s consider a simplified
    example of a game where there is a mouse in a maze and where the goal is for the
    mouse to eat as much cheese as possible. The more cheese the mouse eats, the more
    points it gets in the game:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度 Q 网络** (**DQNs**) 结合了深度学习和强化学习，在多个不同的应用中学习，尤其是在电子游戏中。让我们考虑一个简化的游戏示例，在迷宫中有一只老鼠，目标是让老鼠尽可能多地吃奶酪。老鼠吃的奶酪越多，游戏得分就越高：'
- en: '![](img/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)'
- en: 'In this example, the RL terms would be as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，RL 术语如下：
- en: 'Agent: The mouse as this is controlled by the computer'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：由计算机控制的老鼠
- en: 'State: The current moment in the game'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态：游戏中的当前时刻
- en: 'Action: A decision made by the mouse (to move left, right, up, or down)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作：老鼠做出的决策（向左、向右、向上或向下移动）
- en: 'Reward: The score in the game/amount of cheese the mouse has eaten—in other
    words, the value that the agent is trying to maximize'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励：游戏中的分数/老鼠吃掉的奶酪数量，换句话说，代理试图最大化的值
- en: 'DQNs use Q-learning to learn the best action in a given state. They use **convolutional
    neural networks** (**ConvNets**) as a function approximator for the Q-learning
    function. ConvNets use convolutional layers to find spatial features such as where
    the mouse currently is in the grid. This means that the agent only has to learn
    Q-values for a few million rather than billions of different game states:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DQN使用Q-learning来学习给定状态的最佳动作。它们使用卷积神经网络作为Q-learning函数的函数逼近器。ConvNets使用卷积层来查找空间特征，例如老鼠当前在网格中的位置。这意味着代理只需学习数百万而不是数十亿种不同的游戏状态的Q值：
- en: '![](img/048486f3-6d05-4c7b-b741-dab2757bf1af.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/048486f3-6d05-4c7b-b741-dab2757bf1af.png)'
- en: 'An example of a DQN architecture when learning the mouse maze game is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 学习鼠迷宫游戏时DQN架构的示例如下：
- en: The current state (maze screen) is fed as input into the DQN.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前状态（迷宫屏幕）作为输入输入到DQN中。
- en: The input is passed through convolutional layers to find spatial patterns in
    the image. Note that no pooling is used as it is important to know the spatial
    position when modeling computer games.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入通过卷积层传递，以找出图像中的空间模式。请注意，这里没有使用池化，因为在建模电脑游戏时知道空间位置很重要。
- en: The output of the convolutional layers is fed into fully connected linear layers.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层的输出被馈送到全连接线性层。
- en: The output of the linear layers gives the probability that the DQN will take
    an action given its current state (move up, down, left, or right).
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性层的输出给出了DQN在当前状态下采取行动的概率（向上、向下、向左或向右）。
- en: DQN loss function
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN损失函数
- en: 'The DQN requires a loss function for it to improve and get a higher score.
    This function can be mathematically represented as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DQN需要一种损失函数以提高得分。该函数可以数学表示如下：
- en: '![](img/76163220-a598-499f-98fa-842db8be916f.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76163220-a598-499f-98fa-842db8be916f.png)'
- en: It is the Q-network that chooses which actions to take. The target network is
    what is used as an approximation for the ground truth. If we consider a scenario
    where the Q-network predicted that the correction action in a particular state
    was to move left with 80% certainty and the target network advises to move left,
    we could tweak the parameters of the Q-network using backpropagation to make it
    more likely to predict "move left" in that state. In other words, we backpropagate
    the loss through the DQN and adjust the weights of the Q-network to reduce the
    overall loss. The loss equation aims to make the probabilities of the move to
    be nearer 100% certainty of the correct choice.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 是Q网络选择要采取的动作。目标网络是用作地面真实值的近似值。如果我们考虑这样一个情况，即Q网络预测在特定状态下正确行动是向左移动的概率为80%，而目标网络建议向左移动，我们可以通过反向传播调整Q网络的参数，使其更有可能在该状态下预测“向左移动”。换句话说，我们通过DQN反向传播损失并调整Q网络的权重，以减少总体损失。损失方程旨在使移动的概率更接近于100%的确定性选择。
- en: Experience replay
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验回放
- en: An experience consists of the current state, the action, the reward, and the
    next state. Each experience the agents get is recorded in the experience replay
    memory. An experience from the replay memory is sampled at random to train the
    network.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验包括当前状态、动作、奖励和下一个状态。代理获得的每个经验都记录在经验回放内存中。从回放内存中随机抽取一个经验来训练网络。
- en: Experience replay has some key advantages when compared to traditional Q-learning.
    One such advantage is that, as each experience has the potential to be used to
    train the DQN's neural network multiple times, there is greater data efficiency.
    Another advantage is that, when it is learning from experiences as soon as they
    are obtained, it is the current parameters that determine the next sample that
    the parameters are trained on. If we consider this in the maze example, if the
    next best action is to move left, then the training samples will be dominated
    by those from the left-hand side of the screen. Such behavior can cause the DQN
    to get stuck in local minima. With the incorporation of experience replay, the
    experiences used to train the DQN originate from many different points in time,
    smoothing out the learning and helping to avoid poor performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的Q-learning相比，经验回放具有一些关键优势。其中一个优势是，由于每个经验可能被用来多次训练DQN的神经网络，因此具有更高的数据效率。另一个优势是，在它学习到经验之后，下一个样本的训练是由当前参数决定的。如果我们在迷宫的例子中考虑这一点，如果下一个最佳行动是向左移动，那么训练样本将主要来自屏幕左侧。这种行为可能导致DQN陷入局部最小值。通过引入经验回放，用于训练DQN的经验来源于时间的许多不同点，从而平滑学习过程并帮助避免性能不佳。
- en: Coded example – DQN
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - DQN
- en: In this example, we will again consider the `CartPole-v0` environment from OpenAI
    Gym.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将再次考虑来自OpenAI Gym的`CartPole-v0`环境。
- en: 'First, we create a class that will permit us to incorporate experience replay
    when training the DQN. This essentially stores the transitions observed by the
    agent. The transitions that build up a batch are decorrelated by the sampling
    process:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个类，它将允许我们在训练DQN时引入经验回放。这本质上存储了智能体观察到的转换。通过采样过程，构建一个批次的转换是不相关的：
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the ConvNet model whereby the difference between the current and previous
    screen patches are fed into it. The model has two outputs—*Q(s,left) and Q(s,right)*.
    The network is trying to predict the expected reward/return of taking an action
    given the current input:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 定义ConvNet模型，其中当前和先前的屏幕补丁之间的差异被馈送进去。模型有两个输出—*Q(s,left)和Q(s,right)*。网络试图预测在给定当前输入时采取行动的预期奖励/回报：
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Set the hyperparameters of the model along with some utilities for training:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 设置模型的超参数以及一些用于训练的实用程序：
- en: '[PRE18]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we have the code for training our model. This function performs a
    single step of the optimization. First, it samples a batch and concatenates all
    of the tensors into a single one. It computes *Q(st,at)* and *V(st+1)=maxaQ(st+1,a)*
    and combines them into a loss. By definition, we set *V(s)=0* if *s* is a terminal
    state. We also use a target network to compute *V(st+1)* for added stability:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有训练模型的代码。此函数执行优化的单步。首先，它对批次进行采样并将所有张量连接成一个单一张量。它计算*Q(st,at)*和*V(st+1)=maxaQ(st+1,a)*，并将它们结合成一个损失。根据定义，如果*s*是终端状态，则设置*V(s)=0*。我们还使用目标网络来计算*V(st+1)*以提高稳定性：
- en: '[PRE19]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compute a mask of non-final states. After this, we concatenate the batch elements:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 计算非最终状态的掩码。之后，我们将批次元素连接起来：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Compute *Q(s_t, a)*, then select the columns of actions taken:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*Q(s_t, a)*，然后选择所采取的动作列：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We compute the expected Q values:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算预期的Q值：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then compute the Huber loss function:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算Huber损失函数：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we optimize the model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们优化模型：
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we observe the new state:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们观察新状态：
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We store the transition in memory:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将转换存储在内存中：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s perform one step of the optimization (on the target network):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行优化的一步（在目标网络上）：
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Update the target network; copy all weights and biases in the DQN:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 更新目标网络；复制所有DQN中的权重和偏置：
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This outputs some visualizations to give insight into how the model performs
    during training:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出一些可视化，以便了解模型在训练过程中的表现：
- en: '![](img/d983eb9e-2dd3-407d-9965-184d98e4f351.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d983eb9e-2dd3-407d-9965-184d98e4f351.png)'
- en: 'The following diagram summarizes what the model in this coded example is doing:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了此编码示例中模型的操作：
- en: '![](img/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)'
- en: Double deep Q-learning
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重深度Q学习
- en: Double deep Q-learning generally leads to better performance of the AI agents
    when compared to vanilla DQNs. A common problem with deep Q-learning is that,
    sometimes, the agents can learn unrealistically high action values because it
    includes a maximization step over estimated action values. This tends to prefer
    overestimated to underestimated values. If overestimations are not uniform and
    not concentrated at states about which we wish to learn more, then these can negatively
    affect the quality of the resulting policy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 双深度Q学习通常比纯深度Q网络表现更好。深度Q学习的一个常见问题是，有时代理可以学习到不切实际地高的行动价值，因为它包括对估计行动价值的最大化步骤。这倾向于偏爱过高估计的值而不是低估值。如果过高估计不均匀且不集中在我们希望更多了解的状态上，则可能会对结果策略的质量产生负面影响。
- en: The idea of double Q-learning is to reduce these overestimations. It does this
    by decomposing the max operation in the target into action selection and action
    evaluation. In the vanilla DQN implementation, the action selection and action
    evaluation are coupled. It uses the target network to select the action and, at
    the same time, to estimate the quality of the action.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 双Q学习的思想是减少这些过高估计。它通过将目标中的max操作分解为行动选择和行动评估来实现。在纯深度Q网络实现中，行动选择和行动评估是耦合的。它使用目标网络来选择行动，并同时估计行动的质量。
- en: We are using the target-network to select the action and at the same time to
    estimate the quality of the action. Double Q-learning essentially tries to decouple
    both procedures from each other.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用目标网络来选择行动，并同时估计行动的质量。双Q学习本质上试图将这两个过程解耦。
- en: 'In double Q-learning, the temporal difference (TD) target looks as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在双Q学习中，时序差分（TD）目标如下所示：
- en: '![](img/70934206-aa2d-46c4-b277-fbc831cef8f5.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70934206-aa2d-46c4-b277-fbc831cef8f5.png)'
- en: 'The calculation of new TD target can be summarized in the following steps:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 新的TD目标的计算可以总结为以下步骤：
- en: Q-Network uses the next state, *s'*, to calculate qualities, *Q(s',a)*, for
    each possible action, *a*, in the state, *s'*.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q网络使用下一个状态*s'*来计算在状态*s'*中每个可能的行动*a*的质量*Q(s',a)*。
- en: The `argmax` operation applied on *Q(s',a)* chooses the action, *a**, that belongs
    to the highest quality (action selection).
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用于*Q(s',a)*的`argmax`操作选择了属于最高质量的行动*a**（行动选择）。
- en: The quality *Q(s',a*)*, that belongs to the action, *a**, is selected for the
    calculation of the target.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行动的质量 *Q(s',a*)*，属于行动*a**，被选为目标的计算。
- en: 'The process of double Q-learning can be visualized as per the following diagram.
    An AI agent is at the initial in state s; it knows, based on some previous calculations,
    the qualities Q(s, a[1]) and Q(s, a[2]) for possible two actions in that state.
    The agent then decides to take action a[1] and ends up in state s'':'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 双Q学习的过程可以如下图所示。AI代理处于初始状态*s*，基于一些先前的计算，它知道该状态中可能的两个行动*a[1]*和*a[2]*的质量*Q(s, a[1])*和*Q(s,
    a[2])*。然后代理决定采取行动*a[1]*并进入状态*s'*：
- en: '![](img/e883c737-107a-4652-b211-e9d4815d0c46.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e883c737-107a-4652-b211-e9d4815d0c46.png)'
- en: Actor-critic methods
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家方法
- en: 'Actor-critic methods aim to incorporate the advantages of both value- and policy-based
    methods while eliminating their drawbacks:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法旨在结合值和基于策略的方法的优势，同时消除它们的缺点：
- en: '![](img/0968abce-a6e7-461f-8549-37d45a09c41e.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0968abce-a6e7-461f-8549-37d45a09c41e.png)'
- en: 'The fundamental idea behind actor-critics is to split the model into two parts:
    one for computing an action based on a state and another to produce the Q-values
    of the action.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家的基本思想是将模型分为两部分：一部分用于根据状态计算行动，另一部分用于生成行动的Q值。
- en: 'The actor is a neural network that takes the state as input and outputs the
    best action. By learning the optimal policy, it controls how the agent behaves.
    The critic evaluates the action by computing the value function. In other words,
    the actor tries to optimize the policy and the critic tries to optimize the value.
    The two models improve over time at their individual roles and, as such, the overall
    architecture learns more efficiently than the two methods separately:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 演员是一个神经网络，将状态作为输入并输出最佳行动。通过学习最优策略，它控制代理的行为方式。评论家通过计算价值函数评估行动。换句话说，演员尝试优化策略，评论家尝试优化价值。这两个模型随着时间的推移在各自的角色上得到改进，因此整体架构的学习效率高于单独使用这两种方法：
- en: '![](img/f65da315-87b4-4a85-9b63-15004169d6e1.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f65da315-87b4-4a85-9b63-15004169d6e1.png)'
- en: The two models are essentially competing with one another. Such an approach
    is becoming increasingly popular in the field of machine learning; for example,
    this is also present in generative adversarial networks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型本质上是相互竞争的。这种方法在机器学习领域越来越流行；例如，在生成对抗网络中也有这种情况。
- en: The nature of the actor's role is to be exploratory. It frequently tries new
    things and explores the environment. The role of the critic is to either criticize
    or compliment the actions of the actor. The actor takes this feedback on board
    and adjusts its behavior accordingly. As the actor receives more and more feedback,
    it becomes better at deciding which actions to take.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的角色性质是探索性的。它经常尝试新事物并探索环境。评论者的角色是要么批评，要么赞扬演员的行动。演员接受这些反馈并相应地调整其行为。随着演员获得越来越多的反馈，它在决定采取哪些行动时变得越来越好。
- en: Like a neural network, the actor can be a function approximator where its task
    is to produce the best action for a given state. This could be a fully connected
    or convolutional neural network, for example. The critic is also a function approximator,
    which receives the environment as input along with the action by the actor. It
    concatenates these inputs and outputs the action value (Q-value).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 就像神经网络一样，演员可以是一个函数逼近器，其任务是为给定的状态生成最佳动作。例如，这可以是一个完全连接的或卷积神经网络。评论者也是一个函数逼近器，它接收环境和演员的动作作为输入。它连接这些输入并输出动作值（Q值）。
- en: The two networks are trained separately and, to update their weights, they use
    gradient ascent as opposed to descent as it aims to determine the global maximum
    rather than minimum. Weights are updated at each step rather than at the end of
    an episode as opposed to policy gradients.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络分别进行训练，并使用梯度上升而不是下降来更新它们的权重，因为它旨在确定全局最大值而不是最小值。权重在每个步骤而不是在每个策略梯度末尾更新。
- en: Actor-critics have been proven to learn complex environments and have been used
    in many 2D and 3D computer games such as *Super Mario* and *Doom*.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 演员评论已被证明能够学习复杂的环境，并已在许多二维和三维电脑游戏中使用，例如*超级马里奥*和*Doom*。
- en: Coded example – actor-critic model
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - 演员评论模型
- en: 'Here, we will consider a coded implementation example in PyTorch. First, we
    define the `ActorCritic` class:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将考虑一个在PyTorch中的编码实现示例。首先，我们定义`ActorCritic`类：
- en: '[PRE29]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we initialize the model:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们初始化模型：
- en: '[PRE30]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define a function that will choose the best action based on the state:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个基于状态选择最佳动作的函数：
- en: '[PRE31]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From here, we need to define the function that calculates the total returns
    and considers the loss function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们需要定义计算总回报并考虑损失函数的函数：
- en: '[PRE32]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we can train the model and review how it performs:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以训练模型并查看其表现：
- en: '[PRE33]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/776751b2-6167-4fa2-945b-a9722ced0530.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/776751b2-6167-4fa2-945b-a9722ced0530.png)'
- en: Asynchronous actor-critic algorithm
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步演员评论算法
- en: '**Asynchronous Advantage Actor-Critic** or **A3C** is an algorithm proposed
    by Google''s DeepMind. The algorithm has been proven to outperform other algorithms.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**异步优势演员评论**或**A3C**是由谷歌的DeepMind提出的一种算法。该算法已被证明优于其他算法。'
- en: 'In A3C, there are multiple instances of agents, each of which has been initialized
    differently in their own separate environments. Each individual agent begins to
    take actions and go through the reinforcement learning process to gather their
    own unique experiences. These unique experiences are then used to update the global
    neural network. This global neural network is shared by all of the agents and
    it influences all of the actions of the agents, and every new experience from
    each agent improves the overall network speed:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在A3C中，有多个代理实例，每个代理实例在其自己的独立环境中进行不同的初始化。每个个体代理开始采取行动，并通过强化学习过程来收集自己独特的经验。然后，这些独特的经验用于更新全局神经网络。这个全局神经网络被所有代理共享，它影响所有代理的行动，每个代理的每个新经验都提高了整体网络的速度：
- en: '![](img/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)'
- en: 'The **Advantage** term in the name is the value that states whether or not
    there is an improvement in an action compared to the expected average value of
    that state based on. The advantage formula is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 名称中的**优势**术语是指状态的预期平均值与该状态的行动相比是否有改进的价值。优势公式如下：
- en: '*A (s,a) = Q(s,a) - V(s)*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*A (s,a) = Q(s,a) - V(s)*'
- en: Practical applications
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用
- en: 'RL methods have been applied to solve problems in a multitude of areas in the
    real world. Here, we consider some examples of these:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法已被应用于解决现实世界中多种领域的问题。在这里，我们考虑了其中一些例子。
- en: '**Robotics**: There has been a significant amount of work on applying RL in
    the field of robotics. In the present day, manufacturing facilities are full of
    robots performing a variety of tasks, the foundations of which are RL methods:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**: 在机器人领域应用强化学习的工作已经取得了显著进展。如今，制造设施充斥着执行各种任务的机器人，其基础是强化学习方法：'
- en: '![](img/feda1bf9-3714-4899-9f98-97915440d76d.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/feda1bf9-3714-4899-9f98-97915440d76d.png)'
- en: '**Traffic light control**: In the paper *Reinforcement learning-based multi-agent
    system for network traffic signal control*, researchers designed a traffic light
    controller to solve congestion problems that demonstrated superior results to
    other methods:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交通信号灯控制**: 在论文*基于强化学习的多智能体网络交通信号控制系统*中，研究人员设计了一个交通信号灯控制器来解决拥堵问题，表现优异，超过了其他方法：'
- en: '![](img/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)'
- en: '**Personalized recommendations**: RL has been applied in news recommendation
    systems to account for the fact that news changes rapidly and, as users tend to
    have a short attention span, the click-through rate alone cannot reflect the retention
    rate of the users:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化推荐**: 强化学习已经应用于新闻推荐系统中，以应对新闻快速变化的特点，用户的注意力不集中，仅凭点击率无法反映用户的留存率：'
- en: '![](img/293bd127-5ca4-4eb7-8524-78d8385046ea.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/293bd127-5ca4-4eb7-8524-78d8385046ea.png)'
- en: '**Generating images**: There has been a lot of research into combining RL with
    other deep learning architectures for a host of different applications. Many of
    these have shown some impressive results. DeepMind showed that using generative
    models and RL, they were able to successfully generate images:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成图像**: 对于将强化学习与其他深度学习架构结合进行研究已经有很多成果。DeepMind展示了使用生成模型和强化学习成功生成图像的能力：'
- en: '![](img/3e043c7a-f238-4bee-83d6-3918928f53e7.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e043c7a-f238-4bee-83d6-3918928f53e7.png)'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we began by covering the basics of RL and introduced more advanced
    algorithms that have proven an ability to outperform humans in real-world scenarios.
    We also gave examples as to how these can be implemented in PyTorch.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了强化学习的基础，并介绍了一些在真实场景中表现出超越人类能力的先进算法。同时，我们还展示了这些算法如何在PyTorch中实现。
- en: In the next and final chapter, there will be an overview of this book, along
    with some tips on how you can keep yourself up to date with recent advancements
    in the data science space.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的最后一章中，将概述本书内容，并提供如何保持与数据科学领域最新进展的技巧。
- en: Further reading
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links for more information:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下链接获取更多信息：
- en: '*A Brief Survey of Deep Reinforcement Learning*: [https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习简要概述*: [https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)'
- en: '*Playing Atari with Deep Reinforcement Learning*: [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用深度强化学习玩Atari游戏*: [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
- en: '*Deep Reinforcement Learning with Double Q-learning*: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*双Q学习的深度强化学习*: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
- en: '*Continuous Control with Deep Reinforcement Learning*: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习中的连续控制*: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
- en: '*Asynchronous Methods for Deep Reinforcement Learning*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习中的异步方法*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
- en: '*Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor*: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*软演员-评论家算法：基于最大熵深度强化学习的离策略方法*: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
- en: '*Reinforcement learning-based multi-agent system for network traffic signal
    control*: [http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf](http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于强化学习的多智能体网络交通信号控制系统*: [http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf](http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf)'
- en: '*End-to-End Training of Deep Visuomotor Policies*: [https://arxiv.org/pdf/1504.00702.pdf](https://arxiv.org/pdf/1504.00702.pdf)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*端到端训练深度视觉运动策略*: [https://arxiv.org/pdf/1504.00702.pdf](https://arxiv.org/pdf/1504.00702.pdf)'
- en: '*DRN: A Deep Reinforcement Learning Framework for News Recommendation*: [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DRN: 一种用于新闻推荐的深度强化学习框架*: [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
- en: '*Synthesizing Programs for Images using Reinforced Adversarial Learning*: [https://arxiv.org/pdf/1804.01118.pdf](https://arxiv.org/pdf/1804.01118.pdf)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用增强对抗学习合成图像程序*: [https://arxiv.org/pdf/1804.01118.pdf](https://arxiv.org/pdf/1804.01118.pdf)'
