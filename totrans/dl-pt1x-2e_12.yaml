- en: Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- en: This chapter starts with a basic introduction to **Reinforcement Learning**
    (**RL**), including agents, state, actions, rewards, and policies. It extends
    to **Deep Learning** (**DL**)-based architectures for RL problems such as policy
    gradient methods, Deep-Q networks, and actor-critic models. This chapter will
    explain how to use these deep learning architectures with hands-on code to solve
    sequential decision-making problems in the OpenAI Gym environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä»åŸºæœ¬ä»‹ç»**å¼ºåŒ–å­¦ä¹ **ï¼ˆ**RL**ï¼‰å¼€å§‹ï¼ŒåŒ…æ‹¬ä»£ç†ã€çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œç­–ç•¥ã€‚å®ƒæ‰©å±•åˆ°åŸºäº**æ·±åº¦å­¦ä¹ **ï¼ˆ**DL**ï¼‰çš„æ¶æ„ï¼Œç”¨äºè§£å†³RLé—®é¢˜ï¼Œå¦‚ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€æ·±åº¦Qç½‘ç»œå’Œæ¼”å‘˜-è¯„è®ºå®¶æ¨¡å‹ã€‚æœ¬ç« å°†è§£é‡Šå¦‚ä½•ä½¿ç”¨è¿™äº›æ·±åº¦å­¦ä¹ æ¶æ„åŠå…¶æ‰‹åŠ¨ä»£ç åœ¨OpenAI
    Gymç¯å¢ƒä¸­è§£å†³åºåˆ—å†³ç­–é—®é¢˜ã€‚
- en: 'Specifically, the following will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“è€Œè¨€ï¼Œå°†æ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š
- en: Introduction to RL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ç®€ä»‹
- en: Using DL to tackle RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ·±åº¦å­¦ä¹ è§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜
- en: Policy gradients and code walk-through in PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦å’ŒPyTorchä¸­çš„ä»£ç æ¼”ç¤º
- en: Deep-Q networks and code walk-through in PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·±åº¦Qç½‘ç»œå’ŒPyTorchä¸­çš„ä»£ç æ¼”ç¤º
- en: Actor-critic networks and code walk-through in PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-critic ç½‘ç»œå’Œ PyTorch ä¸­çš„ä»£ç æ¼”ç¤º
- en: Applications of RL in the real world
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨
- en: Introduction to RL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ç®€ä»‹
- en: RL is a branch of machine learning where an agent learns to behave optimally
    in a given environment. The agent performs certain actions and observes the rewards/results.
    It learns the process of mapping situations to actions to maximize a reward.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå…¶ä¸­ä»£ç†ç¨‹åºå­¦ä¹ åœ¨ç»™å®šç¯å¢ƒä¸­çš„æœ€ä½³è¡Œä¸ºæ–¹å¼ã€‚ä»£ç†ç¨‹åºæ‰§è¡ŒæŸäº›åŠ¨ä½œå¹¶è§‚å¯Ÿå¥–åŠ±/ç»“æœã€‚å®ƒå­¦ä¹ å°†æƒ…å†µæ˜ å°„åˆ°è¡ŒåŠ¨çš„è¿‡ç¨‹ï¼Œä»¥æœ€å¤§åŒ–å¥–åŠ±ã€‚
- en: 'The RL process can be modeled as an iterative loop and can be represented as
    a mathematical framework called the **Markov Decision Process** (**MDP**). The
    following steps outline the process that takes place:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹å¯ä»¥å»ºæ¨¡ä¸ºä¸€ä¸ªè¿­ä»£å¾ªç¯ï¼Œå¹¶å¯ä»¥è¡¨ç¤ºä¸ºç§°ä¸º**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**ï¼ˆ**MDP**ï¼‰çš„æ•°å­¦æ¡†æ¶ã€‚ä»¥ä¸‹æ­¥éª¤æ¦‚è¿°äº†è¯¥è¿‡ç¨‹çš„è¿›è¡Œï¼š
- en: RL agent receives state (*s[0]*) from the environment.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä»£ç†ä»ç¯å¢ƒä¸­æ¥æ”¶çŠ¶æ€ï¼ˆ*s[0]*ï¼‰ã€‚
- en: The RL agent takes an action (*a[0]*)Â given the current state (*s[0]*). At this
    stage, the action it takes is random as the agent does not have any previous knowledge
    about the reward it could receive if it performs the action.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä»£ç†æ ¹æ®å½“å‰çŠ¶æ€ï¼ˆ*s[0]*ï¼‰é‡‡å–è¡ŒåŠ¨ï¼ˆ*a[0]*ï¼‰ã€‚åœ¨æ­¤é˜¶æ®µï¼Œç”±äºä»£ç†æ²¡æœ‰å…³äºå¯èƒ½è·å¾—çš„å¥–åŠ±çš„å…ˆå‰çŸ¥è¯†ï¼Œå®ƒé‡‡å–çš„è¡ŒåŠ¨æ˜¯éšæœºçš„ã€‚
- en: After the first action has taken place, the agent can now be considered to be
    in state *s[1]*.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡è¡ŒåŠ¨å‘ç”Ÿåï¼Œä»£ç†ç°åœ¨å¯ä»¥è¢«è®¤ä¸ºå¤„äºçŠ¶æ€*s[1]*ã€‚
- en: At this point, the environment gives a reward (*r[1]*) to the agent.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œç¯å¢ƒå‘ä»£ç†æä¾›å¥–åŠ±ï¼ˆ*r[1]*ï¼‰ã€‚
- en: This loop is repeated continuously; it outputs a sequence of state and action
    and observes the reward.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€å¾ªç¯æŒç»­é‡å¤ï¼›å®ƒè¾“å‡ºä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œåºåˆ—ï¼Œå¹¶è§‚å¯Ÿå¥–åŠ±ã€‚
- en: 'This process is essentially an algorithm for learning an MDP policy:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯ä¸€ç§å­¦ä¹ MDPç­–ç•¥çš„ç®—æ³•ï¼š
- en: '![](img/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)'
- en: 'The cumulative rewards at each time step with respect to the given action can
    be represented as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ—¶é—´æ­¥çš„ç´¯ç§¯å¥–åŠ±ä¸ç»™å®šè¡ŒåŠ¨ç›¸å…³çš„è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)'
- en: In some applications, it may be beneficial to give more weight to rewards that
    are received sooner in time. For example, it would be better for you to receive
    Â£100 today rather than in 5 years' time. To incorporate this, it is common to
    introduce a discount factor, ğ›¾.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›åº”ç”¨ä¸­ï¼Œå¯èƒ½æœ‰åˆ©äºæ›´å¤šåœ°é‡è§†åŠæ—¶æ”¶åˆ°çš„å¥–åŠ±ã€‚ä¾‹å¦‚ï¼Œä»Šå¤©æ”¶åˆ°Â£100æ¯”5å¹´åæ”¶åˆ°è¦å¥½ã€‚ä¸ºäº†çº³å…¥è¿™ä¸€ç‚¹ï¼Œå¼•å…¥ä¸€ä¸ªæŠ˜æ‰£å› å­ğ›¾æ˜¯å¾ˆå¸¸è§çš„ã€‚
- en: 'The cumulative discounted rewards are represented as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)'
- en: An important consideration in RL is the fact that rewards may be infrequent
    and delayed. In cases where there is a long-delayed reward, it can be challenging
    to trace back which sequence of actions contributed to the award.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦è€ƒè™‘å› ç´ æ˜¯å¥–åŠ±å¯èƒ½æ˜¯ä¸é¢‘ç¹å’Œå»¶è¿Ÿçš„ã€‚åœ¨å­˜åœ¨é•¿æ—¶é—´å»¶è¿Ÿå¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œè¿½æº¯å“ªäº›åŠ¨ä½œåºåˆ—å¯¼è‡´äº†å¥–åŠ±å¯èƒ½æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚
- en: Model-based RL
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ 
- en: 'Model-based RL mimics the behavior of the environment. It predicts the next
    state after taking an action. It can be represented mathematically as a probability
    distribution:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿç¯å¢ƒçš„è¡Œä¸ºã€‚å®ƒé¢„æµ‹é‡‡å–è¡ŒåŠ¨åçš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å¯ä»¥ç”¨æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼æ•°å­¦è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/e9b54c93-04ea-441c-a726-90485becbb49.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9b54c93-04ea-441c-a726-90485becbb49.png)'
- en: Here, *p* denotes the model, *x* is the state, and *a* is the control or action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ï¼Œ*p*è¡¨ç¤ºæ¨¡å‹ï¼Œ*x*æ˜¯çŠ¶æ€ï¼Œ*a*æ˜¯æ§åˆ¶æˆ–åŠ¨ä½œã€‚
- en: 'This notion can be demonstrated by considering the cart-pole example. The goal
    is for the pole attached to the cart to stay upright and the agent can decide
    between two possible actions: to move the cart left or to move the cart right.
    In the following screenshot, P models the angle of the pole after taking an action:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¦‚å¿µå¯ä»¥é€šè¿‡è€ƒè™‘å¹³è¡¡æ†ç¤ºä¾‹æ¥è¿›è¡Œæ¼”ç¤ºã€‚ç›®æ ‡æ˜¯è®©é™„ç€åœ¨å°è½¦ä¸Šçš„æ†ä¿æŒç«–ç›´ï¼Œä»£ç†å¯ä»¥å†³å®šä¸¤ç§å¯èƒ½çš„åŠ¨ä½œä¹‹é—´çš„é€‰æ‹©ï¼šå°†å°è½¦å‘å·¦ç§»åŠ¨æˆ–å°†å°è½¦å‘å³ç§»åŠ¨ã€‚åœ¨ä¸‹é¢çš„æˆªå›¾ä¸­ï¼ŒPæ¨¡æ‹Ÿäº†é‡‡å–è¡ŒåŠ¨åæ†çš„è§’åº¦ï¼š
- en: '![](img/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)'
- en: 'The following diagram depicts the probability distribution output for Î¸ in
    the subsequent time step:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾æè¿°äº†ä¸‹ä¸€æ—¶é—´æ­¥ä¸­Î¸çš„æ¦‚ç‡åˆ†å¸ƒè¾“å‡ºï¼š
- en: '![](img/d739342a-0b14-480d-b1d7-0d627b8fab81.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d739342a-0b14-480d-b1d7-0d627b8fab81.png)'
- en: In this instance, the model describes the law of physics; however, the model
    could be anything depending on the application. Another example is that the model
    could be built onÂ the rules of a game of chess.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹æè¿°äº†ç‰©ç†å®šå¾‹ï¼›ç„¶è€Œï¼Œæ¨¡å‹å¯èƒ½æ˜¯ä»»ä½•åº”ç”¨çš„ä¾æ®ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯æ¨¡å‹å¯ä»¥å»ºç«‹åœ¨å›½é™…è±¡æ£‹æ¸¸æˆçš„è§„åˆ™ä¸Šã€‚
- en: 'The core concept of model-based RL is to use the model along with the cost
    function to locate the optimal path of actions or, in other words, the trajectory
    of states and actions, ğ‰:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä½¿ç”¨æ¨¡å‹å’Œæˆæœ¬å‡½æ•°æ¥å®šä½æœ€ä½³è·¯å¾„çš„è¡ŒåŠ¨æˆ–è€…è¯´çŠ¶æ€å’Œè¡ŒåŠ¨çš„è½¨è¿¹ï¼Œğ‰ï¼š
- en: '![](img/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)'
- en: The drawback of model-based algorithms is that they can become impractical as
    the state space and action space become larger.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¨¡å‹çš„ç®—æ³•çš„ç¼ºç‚¹æ˜¯ï¼Œéšç€çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„æ‰©å¤§ï¼Œå®ƒä»¬å¯èƒ½å˜å¾—ä¸åˆ‡å®é™…ã€‚
- en: Model-free RL
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ 
- en: Model-free algorithms rely on trial and error to update their knowledge. As
    such, they do not require space to store all of the combinations of states and
    actions. Policy gradients, value learning, or other model-free RL are used to
    find a policy that takes the best actions for maximum rewards. A key difference
    between model-free and model-based methods is that model-free methods act in the
    real environment to learn.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æ¨¡å‹ç®—æ³•ä¾èµ–è¯•éªŒå’Œé”™è¯¯æ¥æ›´æ–°å…¶çŸ¥è¯†ã€‚å› æ­¤ï¼Œå®ƒä»¬ä¸éœ€è¦ç©ºé—´æ¥å­˜å‚¨æ‰€æœ‰çŠ¶æ€å’ŒåŠ¨ä½œçš„ç»„åˆã€‚ç­–ç•¥æ¢¯åº¦ã€å€¼å­¦ä¹ æˆ–å…¶ä»–æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç”¨äºæ‰¾åˆ°ä¸€ä¸ªæœ€å¤§åŒ–å¥–åŠ±çš„æœ€ä½³è¡ŒåŠ¨ç­–ç•¥ã€‚æ— æ¨¡å‹å’ŒåŸºäºæ¨¡å‹æ–¹æ³•çš„ä¸€ä¸ªå…³é”®åŒºåˆ«åœ¨äºï¼Œæ— æ¨¡å‹æ–¹æ³•åœ¨çœŸå®ç¯å¢ƒä¸­è¡ŒåŠ¨å­¦ä¹ ã€‚
- en: Comparing on-policy and off-policy
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒåŸºäºç­–ç•¥å’Œç¦»ç­–ç•¥
- en: The policy defines how the agent behaves; it tells the agent how to act in each
    state. Every RL algorithm must follow some type of policy to decide how it will
    act.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥å®šä¹‰äº†ä»£ç†å¦‚ä½•è¡ŒåŠ¨ï¼›å®ƒå‘Šè¯‰ä»£ç†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹åº”è¯¥å¦‚ä½•è¡ŒåŠ¨ã€‚æ¯ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å¿…é¡»éµå¾ªæŸç§ç­–ç•¥æ¥å†³å®šå…¶è¡Œä¸ºæ–¹å¼ã€‚
- en: 'The policy function the agent is trying to learn can be represented as follows,
    where *Î¸* is the parameter vector, *s* is a particular state, and *a* is an action:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†è¯•å›¾å­¦ä¹ çš„ç­–ç•¥å‡½æ•°å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼Œå…¶ä¸­*Î¸*æ˜¯å‚æ•°å‘é‡ï¼Œ*s*æ˜¯ç‰¹å®šçŠ¶æ€ï¼Œ*a*æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼š
- en: '![](img/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)'
- en: An on-policy agent learns the value (the expected discounted rewards) based
    on its current action and is derived from the current policy. Off-policy learns
    the value based on the action obtained from another policy such as a greed policy
    as in Q-learning, which we introduce next.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºç­–ç•¥çš„ä»£ç†å­¦ä¹ å€¼ï¼ˆæœŸæœ›çš„æŠ˜æ‰£å¥–åŠ±ï¼‰ï¼ŒåŸºäºå½“å‰åŠ¨ä½œå¹¶æºäºå½“å‰ç­–ç•¥ã€‚ç¦»ç­–ç•¥å­¦ä¹ å€¼åˆ™åŸºäºä»å¦ä¸€ç­–ç•¥ï¼ˆå¦‚è´ªå©ªç­–ç•¥ï¼Œå¦‚æˆ‘ä»¬æ¥ä¸‹æ¥ä»‹ç»çš„ Q-learning
    ä¸­ï¼‰è·å¾—çš„åŠ¨ä½œã€‚
- en: Q-learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is a model-free RL algorithm whereby a table is created that calculates
    the maximum expected future reward for each action at each state. It is considered
    off-policy because the Q-learning function learns from actions that are outside
    of the current policy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning æ˜¯ä¸€ç§æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªè¡¨æ ¼æ¥è®¡ç®—æ¯ä¸ªçŠ¶æ€ä¸‹æ¯ä¸ªåŠ¨ä½œçš„æœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±ã€‚å®ƒè¢«è®¤ä¸ºæ˜¯ç¦»ç­–ç•¥çš„ï¼Œå› ä¸º Q-learning
    å‡½æ•°ä»å½“å‰ç­–ç•¥ä¹‹å¤–çš„åŠ¨ä½œä¸­å­¦ä¹ ã€‚
- en: 'When Q-learning is performed, a Q-table is created whereby the columns represent
    the possible actions and the rows represent the states. The value of each cell
    in the Q-table will be the maximum expected future reward for that given state
    and action:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿›è¡Œ Q-learning æ—¶ï¼Œä¼šåˆ›å»ºä¸€ä¸ª Q-è¡¨æ ¼ï¼Œå…¶ä¸­åˆ—ä»£è¡¨å¯èƒ½çš„åŠ¨ä½œï¼Œè¡Œä»£è¡¨çŠ¶æ€ã€‚Q-è¡¨æ ¼ä¸­æ¯ä¸ªå•å…ƒæ ¼çš„å€¼å°†æ˜¯ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œçš„æœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±ï¼š
- en: '![](img/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)'
- en: Each Q-table score will be the maximum expected future reward if taking the
    action from the best policy. To learn each value in the Q-table, the Q-learning
    algorithm is used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª Q-è¡¨æ ¼åˆ†æ•°å°†æ˜¯ä»æœ€ä½³ç­–ç•¥ä¸­è·å–çš„åŠ¨ä½œçš„æœ€å¤§é¢„æœŸæœªæ¥å¥–åŠ±ã€‚Q-learning ç®—æ³•ç”¨äºå­¦ä¹  Q-è¡¨æ ¼ä¸­çš„æ¯ä¸ªå€¼ã€‚
- en: 'The **Q-function** (or **action-value function**) takes two inputs: state and
    reward. The Q-function returns the expected future reward of that action at that
    state. It can be represented as shown:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q å‡½æ•°**ï¼ˆæˆ–**åŠ¨ä½œå€¼å‡½æ•°**ï¼‰æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼šçŠ¶æ€å’Œå¥–åŠ±ã€‚Q å‡½æ•°è¿”å›è¯¥çŠ¶æ€ä¸‹æ‰§è¡Œè¯¥åŠ¨ä½œçš„é¢„æœŸæœªæ¥å¥–åŠ±ã€‚å®ƒå¯ä»¥å¦‚ä¸‹è¡¨ç¤ºï¼š'
- en: '![](img/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)'
- en: The Q-function essentially scrolls through the Q-table to find the row associated
    with the current state and the column associated with the action. From here, it
    returns the Q value that is the corresponding expected future reward.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Q å‡½æ•°åŸºæœ¬ä¸Šé€šè¿‡æ»šåŠ¨ Q è¡¨æ¥æŸ¥æ‰¾ä¸å½“å‰çŠ¶æ€ç›¸å…³çš„è¡Œå’Œä¸åŠ¨ä½œç›¸å…³çš„åˆ—ã€‚ä»è¿™é‡Œï¼Œå®ƒè¿”å›ç›¸åº”çš„é¢„æœŸæœªæ¥å¥–åŠ± Q å€¼ã€‚
- en: 'Consider this in the cart-pole example shown in the following figure. In its
    current state, moving left should have a higher Q-value than moving right:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°åœ¨ä¸‹å›¾ä¸­å±•ç¤ºçš„å€’ç«‹æ‘†ç¤ºä¾‹ä¸­ã€‚åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œå‘å·¦ç§»åŠ¨åº”è¯¥æ¯”å‘å³ç§»åŠ¨å…·æœ‰æ›´é«˜çš„ Q å€¼ï¼š
- en: '![](img/f82716fa-e071-471f-a14b-d0a302ed4792.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f82716fa-e071-471f-a14b-d0a302ed4792.png)'
- en: 'As the environment is explored, the Q-table is updated to give better approximations.
    The process of the Q-learning algorithm is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ç¯å¢ƒçš„æ¢ç´¢ï¼ŒQ è¡¨å°†è¢«æ›´æ–°ä»¥æä¾›æ›´å¥½çš„è¿‘ä¼¼å€¼ã€‚Q å­¦ä¹ ç®—æ³•çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: The Q-table is initialized.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ– Q è¡¨ã€‚
- en: An action in the current state (*s*) is chosen based on the current Q-value
    estimates.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®å½“å‰çš„ Q å€¼ä¼°è®¡ï¼Œé€‰æ‹©å½“å‰çŠ¶æ€ï¼ˆ*s*ï¼‰ä¸­çš„ä¸€ä¸ªåŠ¨ä½œã€‚
- en: An action (*a*) is performed.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼ˆ*a*ï¼‰ã€‚
- en: The reward (*r*) is measured.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¥–åŠ±ï¼ˆ*r*ï¼‰è¢«æµ‹é‡ã€‚
- en: Q is updated using the Bellman equation.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q å€¼ã€‚
- en: 'The following is the Bellman equation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è´å°”æ›¼æ–¹ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)'
- en: '*Steps 2-5* are repeated until the maximum number of episodes is reached or
    until the training is manually stopped.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­¥éª¤ 2-5* é‡å¤æ‰§è¡Œï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§çš„å›åˆæ•°æˆ–æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚'
- en: 'The Q-learning algorithm can be expressed as the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Q å­¦ä¹ ç®—æ³•å¯ä»¥è¡¨ç¤ºä¸ºä»¥ä¸‹æ–¹ç¨‹ï¼š
- en: '![](img/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)'
- en: Value methods
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å€¼æ–¹æ³•
- en: 'Value learning is an approach that can often be a key building block in many
    RL methods. A value function,Â *V(s)*, represents how good the state is that the
    agent is in. It is equal to the expected total reward for an agent starting from
    the state,Â *s*. The total expected reward is dependent on the policy by which
    the agent chooses actions to perform. If the agent uses a given policy (ğ›‘) to
    select its actions, the corresponding value function is given by the following
    formula:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å­¦ä¹ æ˜¯è®¸å¤šå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å…³é”®æ„å»ºæ¨¡å—ã€‚å€¼å‡½æ•° *V(s)* è¡¨ç¤ºä»£ç†æ‰€å¤„çŠ¶æ€çš„å¥½åç¨‹åº¦ã€‚å®ƒç­‰äºä»çŠ¶æ€ *s* å¼€å§‹ä»£ç†é¢„æœŸçš„æ€»å¥–åŠ±ã€‚æ€»é¢„æœŸå¥–åŠ±å–å†³äºä»£ç†é€šè¿‡é€‰æ‹©åŠ¨ä½œæ‰§è¡Œçš„ç­–ç•¥ã€‚å¦‚æœä»£ç†ä½¿ç”¨ç»™å®šç­–ç•¥ï¼ˆğ›‘ï¼‰é€‰æ‹©å…¶åŠ¨ä½œï¼Œåˆ™ç›¸åº”çš„å€¼å‡½æ•°ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
- en: '![](img/07835c7c-d756-46bf-b0fb-282981fb0c05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07835c7c-d756-46bf-b0fb-282981fb0c05.png)'
- en: 'Considering this in the cart-pole example, we could use the length of time
    the pole stays up in order to measure the rewards. In the following screenshot,
    there is a higher probability that the pole will stay upright for state s1 compared
    to state s2\. As such, for most policies, the state s1 is likely to have a higher
    value function (a higher expected future reward):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å€’ç«‹æ‘†ç¤ºä¾‹ä¸­è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ†å­ä¿æŒç›´ç«‹çš„æ—¶é—´é•¿åº¦æ¥è¡¡é‡å¥–åŠ±ã€‚åœ¨ä¸‹é¢çš„æˆªå›¾ä¸­ï¼Œä¸çŠ¶æ€ s2 ç›¸æ¯”ï¼ŒçŠ¶æ€ s1 æ†å­ä¿æŒç›´ç«‹çš„æ¦‚ç‡æ›´é«˜ã€‚å› æ­¤ï¼Œå¯¹äºå¤§å¤šæ•°ç­–ç•¥è€Œè¨€ï¼ŒçŠ¶æ€
    s1 çš„å€¼å‡½æ•°å¯èƒ½æ›´é«˜ï¼ˆå³æ›´é«˜çš„æœŸæœ›æœªæ¥å¥–åŠ±ï¼‰ï¼š
- en: '![](img/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)'
- en: 'There is an optimal value function that has a higher value than other functions
    for all states and this can be represented as the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨ä¸€ä¸ªä¼˜åŒ–çš„å€¼å‡½æ•°ï¼Œå…¶å¯¹æ‰€æœ‰çŠ¶æ€éƒ½å…·æœ‰æ¯”å…¶ä»–å‡½æ•°æ›´é«˜çš„å€¼ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºä»¥ä¸‹å½¢å¼ï¼š
- en: '![](img/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)'
- en: 'There is an optimal policy that corresponds to the optimal value function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨ä¸€ä¸ªä¸æœ€ä¼˜å€¼å‡½æ•°å¯¹åº”çš„æœ€ä¼˜ç­–ç•¥ï¼š
- en: '![](img/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)'
- en: There are several different ways in which the optimal policy can be found. This
    is referred to as policy evaluation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡å‡ ç§ä¸åŒçš„æ–¹å¼æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚è¿™ç§°ä¸ºç­–ç•¥è¯„ä¼°ã€‚
- en: Value iteration
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£
- en: Value iteration is a process that computes the optimal state-value function
    by iteratively improving estimates of *V(s)*. Firstly, the algorithm initializes
    *V(s)* to be random values. From there, it repeatedly updates the values of *Q(s,
    a)* and *V(s)* until they converge. Value iteration converges to the optimal values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼è¿­ä»£æ˜¯ä¸€ä¸ªé€šè¿‡è¿­ä»£æ”¹è¿›*V(s)*ä¼°è®¡æ¥è®¡ç®—æœ€ä¼˜çŠ¶æ€å€¼å‡½æ•°çš„è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œç®—æ³•å°†*V(s)*åˆå§‹åŒ–ä¸ºéšæœºå€¼ã€‚ç„¶åï¼Œå®ƒé‡å¤æ›´æ–°*Q(s, a)*å’Œ*V(s)*çš„å€¼ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚å€¼è¿­ä»£æ”¶æ•›åˆ°æœ€ä¼˜å€¼ã€‚
- en: 'The following is the value iteration pseudocode:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯å€¼è¿­ä»£çš„ä¼ªä»£ç ï¼š
- en: '![](img/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)'
- en: Coded example â€“ value iteration
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ â€“ å€¼è¿­ä»£
- en: 'To consider an example of this, we will use the Frozen Lake environment in
    OpenAI Gym. In the environment, the player is to imagine they are standing on
    a frozen lake where not all of it is frozen. The goal is to move from place S
    to placeÂ G without falling into the holes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨OpenAI Gymä¸­çš„Frozen Lakeç¯å¢ƒä½œä¸ºç¤ºä¾‹ã€‚åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œç©å®¶éœ€è¦æƒ³è±¡è‡ªå·±ç«™åœ¨ä¸€ä¸ªéƒ¨åˆ†å†»ç»“çš„æ¹–é¢ä¸Šã€‚ç›®æ ‡æ˜¯ä»èµ·ç‚¹Sç§»åŠ¨åˆ°ç»ˆç‚¹Gè€Œä¸æ‰å…¥æ´ä¸­ï¼š
- en: '![](img/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)'
- en: 'The letters on the grid represent the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘æ ¼ä¸Šçš„å­—æ¯ä»£è¡¨ä»¥ä¸‹å†…å®¹ï¼š
- en: 'S: Starting point, safe'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sï¼šèµ·ç‚¹ï¼Œå®‰å…¨
- en: 'F: Frozen surface, safe'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fï¼šå†»ç»“è¡¨é¢ï¼Œå®‰å…¨
- en: 'H: Hole, not safe'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hï¼šæ´ï¼Œä¸å®‰å…¨
- en: 'G: Goal'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gï¼šç›®æ ‡
- en: 'There are four possible moves that the agent can take: left, right, down, and
    up, which are represented as 0, 1, 2, and 3 respectively. As such, there are 16
    possible states to be in (4 x 4). For every H state, the agent receives a reward
    of -1 and upon reaching the goal, receives a reward of +1.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†å¯ä»¥é‡‡å–å››ç§å¯èƒ½çš„ç§»åŠ¨ï¼šå·¦ã€å³ã€ä¸‹ã€ä¸Šï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º0ã€1ã€2ã€3ã€‚å› æ­¤ï¼Œæœ‰16ç§å¯èƒ½çš„çŠ¶æ€ï¼ˆ4 x 4ï¼‰ã€‚å¯¹äºæ¯ä¸ªHçŠ¶æ€ï¼Œä»£ç†ä¼šæ”¶åˆ°-1çš„å¥–åŠ±ï¼Œå¹¶åœ¨è¾¾åˆ°ç›®æ ‡æ—¶æ”¶åˆ°+1çš„å¥–åŠ±ã€‚
- en: 'To implement value-iteration in code, we first import the relevant libraries
    we wish to use and initialize the `FrozenLake` environment:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ä»£ç ä¸­å®ç°å€¼è¿­ä»£ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¼å…¥å¸Œæœ›ä½¿ç”¨çš„ç›¸å…³åº“ï¼Œå¹¶åˆå§‹åŒ–`FrozenLake`ç¯å¢ƒï¼š
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we assign the variables:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬ä¸ºå˜é‡èµ‹å€¼ï¼š
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From here, we initialize the Q-table where `env.observation_space.n` is the
    number of states and `env.action_space.n` is the number of actions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬åˆå§‹åŒ–Qè¡¨ï¼Œå…¶ä¸­`env.observation_space.n`æ˜¯çŠ¶æ€æ•°ï¼Œ`env.action_space.n`æ˜¯åŠ¨ä½œæ•°ã€‚
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the functions for the agent to choose an action and learn:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä»£ç†é€‰æ‹©åŠ¨ä½œå’Œå­¦ä¹ çš„å‡½æ•°ï¼š
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From here, we can begin running the episodes and export the Q-table to a pickled
    file:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è¿è¡Œå›åˆå¹¶å°†Qè¡¨å¯¼å‡ºåˆ°ä¸€ä¸ªpickleæ–‡ä»¶ä¸­ï¼š
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see the process in action by running the preceding code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œå‰é¢çš„ä»£ç æ¥çœ‹åˆ°è¿™ä¸ªè¿‡ç¨‹ï¼š
- en: '![](img/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)'
- en: Policy methods
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ–¹æ³•
- en: 'In policy-based RL, the goal is to find the policy that makes the most rewarding
    decisions and can be represented mathematically as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½åšå‡ºæœ€æœ‰ç›Šå†³ç­–çš„ç­–ç•¥ï¼Œå¯ä»¥æ•°å­¦è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/49c33b1b-779e-4963-8486-f7788f2e7333.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49c33b1b-779e-4963-8486-f7788f2e7333.png)'
- en: 'Policies in RL can be either deterministic or stochastic:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯éšæœºçš„ï¼š
- en: '![](img/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)'
- en: 'Stochastic policies output a probability distribution rather than a single
    discrete value:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºç­–ç•¥è¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒè€Œä¸æ˜¯å•ä¸€çš„ç¦»æ•£å€¼ï¼š
- en: '![](img/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)'
- en: 'We can represent the objective mathematically as shown:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªç›®æ ‡æ•°å­¦åŒ–åœ°è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)'
- en: Policy iteration
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥è¿­ä»£
- en: 'During value iteration, sometimes the optimal policy converges before the value
    function does since it only cares about finding the optimal policy. Another algorithm
    called policy iteration can be performed to reach the optimal policy. This is
    when, after each policy evaluation has taken place, the policy of the next is
    based on the value function until the policy converges. The following diagram
    illustrates the policy iteration process:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å€¼è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæœ‰æ—¶æœ€ä¼˜ç­–ç•¥åœ¨å€¼å‡½æ•°ä¹‹å‰æ”¶æ•›ï¼Œå› ä¸ºå®ƒåªå…³å¿ƒæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚è¿˜å¯ä»¥æ‰§è¡Œå¦ä¸€ä¸ªç§°ä¸ºç­–ç•¥è¿­ä»£çš„ç®—æ³•æ¥è¾¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚è¿™æ˜¯åœ¨æ¯æ¬¡ç­–ç•¥è¯„ä¼°ä¹‹åï¼Œä¸‹ä¸€ä¸ªç­–ç•¥åŸºäºå€¼å‡½æ•°ç›´åˆ°ç­–ç•¥æ”¶æ•›çš„è¿‡ç¨‹ã€‚ä»¥ä¸‹å›¾ç¤ºå±•ç¤ºäº†ç­–ç•¥è¿­ä»£è¿‡ç¨‹ï¼š
- en: '![](img/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)'
- en: As such, the policy iteration algorithm is guaranteed to converge to the optimal
    policy, unlike the value iteration algorithm.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç­–ç•¥è¿­ä»£ç®—æ³•ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œè€Œå€¼è¿­ä»£ç®—æ³•åˆ™ä¸ä¸€å®šã€‚
- en: 'The following is the policy iteration pseudocode:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç­–ç•¥è¿­ä»£çš„ä¼ªä»£ç ï¼š
- en: '![](img/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)'
- en: Coded exampleÂ â€“ policy iteration
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - ç­–ç•¥è¿­ä»£
- en: Here, we consider a coded example of this using the Frozen Lake environment
    as earlier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨ä¹‹å‰ä»‹ç»çš„Frozen Lakeç¯å¢ƒçš„ç¼–ç ç¤ºä¾‹ã€‚
- en: 'First, import the relevant libraries:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå¯¼å…¥ç›¸å…³çš„åº“ï¼š
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we define the functions to run an episode and return the reward:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰å‡½æ•°æ¥è¿è¡Œä¸€ä¸ªepisodeå¹¶è¿”å›å¥–åŠ±ï¼š
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From here, we can define functions to evaluate the policy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰è¯„ä¼°ç­–ç•¥çš„å‡½æ•°ï¼š
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, define the functions to extract the policy:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå®šä¹‰æå–ç­–ç•¥çš„å‡½æ•°ï¼š
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, define the function to compute the policy:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå®šä¹‰è®¡ç®—ç­–ç•¥çš„å‡½æ•°ï¼š
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From here, we can run policy iteration on the Frozen Lake environment:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨Frozen Lakeç¯å¢ƒä¸Šè¿è¡Œç­–ç•¥è¿­ä»£ï¼š
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We observe that it has converged at *step 5*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒåœ¨ *æ­¥éª¤ 5* å¤„æ”¶æ•›ï¼š
- en: '![](img/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)'
- en: Value iteration versus policy iteration
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»·å€¼è¿­ä»£ä¸ç­–ç•¥è¿­ä»£çš„æ¯”è¾ƒ
- en: Where the agent is assumed to have some prior knowledge about the effects of
    its actions on the environment, both value and policy iteration algorithms can
    be used. The algorithms assume that the MDP model is known. Policy iteration,
    however, is more computationally efficient as it often takes a lower number of
    iterations to converge.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä»£ç†å‡å®šå¯¹å…¶åœ¨ç¯å¢ƒä¸­çš„è¡Œä¸ºå½±å“æœ‰ä¸€äº›å…ˆéªŒçŸ¥è¯†æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ä»·å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£ç®—æ³•ã€‚è¿™äº›ç®—æ³•å‡å®šå·²çŸ¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œç­–ç•¥è¿­ä»£é€šå¸¸æ›´åŠ è®¡ç®—æ•ˆç‡é«˜ï¼Œå› ä¸ºå®ƒå¾€å¾€éœ€è¦æ›´å°‘çš„è¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›ã€‚
- en: Policy gradient algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦ç®—æ³•
- en: 'Policy gradient is also an approach to solve reinforcement learning problems
    and aims to model and optimize the policy directly:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦ä¹Ÿæ˜¯è§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨ç›´æ¥å»ºæ¨¡å’Œä¼˜åŒ–ç­–ç•¥ï¼š
- en: '![](img/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)'
- en: 'In policy gradients, the following steps are taken:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥æ¢¯åº¦ä¸­ï¼Œé‡‡å–ä»¥ä¸‹æ­¥éª¤ï¼š
- en: The agent observes the state of the environment (*s*).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†è§‚å¯Ÿç¯å¢ƒçš„çŠ¶æ€ (*s*)ã€‚
- en: The agent takes action *u*Â based on their instincts (a policy, Ï€) about the
    state (*s*).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†æ ¹æ®ä»–ä»¬å¯¹çŠ¶æ€ (*s*) çš„æœ¬èƒ½ï¼ˆå³ç­–ç•¥Ï€ï¼‰é‡‡å–è¡ŒåŠ¨ *u*ã€‚
- en: The agent moves and the environment changes; a new state is formed.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†ç§»åŠ¨å¹¶ä¸”ç¯å¢ƒæ”¹å˜ï¼›å½¢æˆä¸€ä¸ªæ–°çš„çŠ¶æ€ã€‚
- en: The agent takes further actions based on the observed state.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»£ç†æ ¹æ®è§‚å¯Ÿåˆ°çš„ç¯å¢ƒçŠ¶æ€è¿›ä¸€æ­¥é‡‡å–è¡ŒåŠ¨ã€‚
- en: After a trajectory (Ï„) of motions, the agent adjusts its instinct based on the
    total rewards,Â *R(Ï„)*Â ,received.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿åŠ¨è½¨è¿¹ï¼ˆÏ„ï¼‰ä¹‹åï¼Œä»£ç†æ ¹æ®æ‰€è·å¾—çš„æ€»å¥–åŠ± *R(Ï„)* è°ƒæ•´å…¶æœ¬èƒ½ã€‚
- en: 'The policy gradient theorem is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦å®šç†å¦‚ä¸‹ï¼š
- en: '*The derivative of the expected reward is the expectation of the product of
    the reward and gradient of the log of the policy Ï€[Î¸]â€‹*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*é¢„æœŸå¥–åŠ±çš„å¯¼æ•°æ˜¯ç­–ç•¥Ï€[Î¸]â€‹çš„å¯¹æ•°æ¢¯åº¦ä¸å¥–åŠ±ä¹˜ç§¯çš„æœŸæœ›*ã€‚'
- en: '![](img/86474d27-5e94-4d57-bf41-ee2165f51e90.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86474d27-5e94-4d57-bf41-ee2165f51e90.png)'
- en: Coded exampleÂ â€“ policy gradient algorithm
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - ç­–ç•¥æ¢¯åº¦ç®—æ³•
- en: 'In this example, we use the OpenAI environment named CartPole where the goal
    is for the pole attached to the cart to stay upright for as long as possible.
    The agent receives a reward for every time step taken in which the pole remains
    balanced. If the pole falls over, the episode ends:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åä¸ºCartPoleçš„OpenAIç¯å¢ƒï¼Œç›®æ ‡æ˜¯è®©è¿æ¥åˆ°å°è½¦ä¸Šçš„æ†å°½å¯èƒ½é•¿æ—¶é—´ä¿æŒç›´ç«‹ã€‚ä»£ç†åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿å†…ä¿æŒæ†å¹³è¡¡æ—¶ä¼šè·å¾—å¥–åŠ±ã€‚å¦‚æœæ†å€’ä¸‹ï¼Œé‚£ä¹ˆè¿™ä¸€é›†ç»“æŸï¼š
- en: '![](img/7df31996-e074-44c3-8f28-cfa7e066dd53.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7df31996-e074-44c3-8f28-cfa7e066dd53.png)'
- en: 'At any point in time, the cart and pole are in a state, *s*. This state is
    represented by a vector of four elements, namely, pole angle, pole velocity, cart
    position, and cart velocity. The agent can decide between two possible actions:
    to move the cart left or to move the cart right.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»»ä½•æ—¶åˆ»ï¼Œå°è½¦å’Œæ†éƒ½å¤„äºä¸€ä¸ªçŠ¶æ€ *s*ã€‚è¯¥çŠ¶æ€ç”±å››ä¸ªå…ƒç´ çš„å‘é‡è¡¨ç¤ºï¼Œå³æ†è§’åº¦ã€æ†é€Ÿåº¦ã€å°è½¦ä½ç½®å’Œå°è½¦é€Ÿåº¦ã€‚ä»£ç†å¯ä»¥é€‰æ‹©ä¸¤ç§å¯èƒ½çš„åŠ¨ä½œï¼šå‘å·¦ç§»åŠ¨å°è½¦æˆ–å‘å³ç§»åŠ¨å°è½¦ã€‚
- en: A policy gradient takes small steps and updates the policy based on the reward
    that is associated with the step. It does this so that it can train the agent
    without having to map the value for every pair of states and actions in the environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦é‡‡å–å°æ­¥éª¤ï¼Œå¹¶æ ¹æ®ä¸æ­¥éª¤ç›¸å…³çš„å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚è¿™æ ·åšå¯ä»¥è®­ç»ƒä»£ç†ï¼Œè€Œæ— éœ€åœ¨ç¯å¢ƒä¸­ä¸ºæ¯å¯¹çŠ¶æ€å’ŒåŠ¨ä½œæ˜ å°„ä»·å€¼ã€‚
- en: In this example, we will apply a technique called the Monte-Carlo policy gradient.
    Using this approach, the agent will update the policy at the end of each episode
    based on the rewards obtained.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨ä¸€ç§ç§°ä¸ºè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦çš„æŠ€æœ¯ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä»£ç†å°†æ ¹æ®è·å¾—çš„å¥–åŠ±åœ¨æ¯ä¸ª episode ç»“æŸæ—¶æ›´æ–°ç­–ç•¥ã€‚
- en: 'We will first import the relevant libraries that we plan to use:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æˆ‘ä»¬è®¡åˆ’ä½¿ç”¨çš„ç›¸å…³åº“ï¼š
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we define a feedforward neural network with one hidden layer of 128 neurons
    and a dropout of 0.5\. We use Adam as the optimizer and a learning rate of 0.02\.
    Using a dropout significantly improves the performance of the policy:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ŒåŒ…å«ä¸€ä¸ªéšè—å±‚ï¼Œæœ‰ 128 ä¸ªç¥ç»å…ƒå’Œ 0.5 çš„ dropoutã€‚æˆ‘ä»¬ä½¿ç”¨ Adam ä½œä¸ºä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.02ã€‚ä½¿ç”¨
    dropout æ˜¾è‘—æé«˜äº†ç­–ç•¥çš„æ€§èƒ½ï¼š
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we define a `choose_action` function. This function chooses an action
    based on the policy distribution with the aid of the PyTorch distributions package.
    The policy returns a probability for each possible action on the action space
    as an array. In our example, this is to move left or to move right so the output
    could be [0.1, 0.9]. Based on these probabilities, the action is chosen, the history
    is recorded, and the action is returned:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`choose_action`å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ ¹æ®ç­–ç•¥åˆ†å¸ƒé€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨äº† PyTorch åˆ†å¸ƒåŒ…ã€‚ç­–ç•¥è¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œè¡¨ç¤ºåŠ¨ä½œç©ºé—´ä¸Šæ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„æ¦‚ç‡ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¯ä»¥æ˜¯å‘å·¦ç§»åŠ¨æˆ–å‘å³ç§»åŠ¨ï¼Œå› æ­¤è¾“å‡ºå¯èƒ½æ˜¯[0.1,
    0.9]ã€‚æ ¹æ®è¿™äº›æ¦‚ç‡é€‰æ‹©åŠ¨ä½œï¼Œè®°å½•å†å²å¹¶è¿”å›åŠ¨ä½œï¼š
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To update the policy, we take a sample of the Q-function (action-value function).
    Recall that this is the expected return by taking an action in a state by following
    the policy, Ï€. We can calculate the policy gradient at each time step using the
    fact that there is a reward of 1 for every step the pole remains vertical. We
    use the long term reward (*vt*) where this is the discounted sum of all future
    rewards for the length of the episode. As such, the longer the episode, the greater
    the reward for a state-action pair in the present where gamma is the discount
    factor.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ›´æ–°ç­–ç•¥ï¼Œæˆ‘ä»¬ä» Q å‡½æ•°ï¼ˆåŠ¨ä½œå€¼å‡½æ•°ï¼‰ä¸­å–æ ·ã€‚å›æƒ³ä¸€ä¸‹ï¼Œè¿™æ˜¯é€šè¿‡éµå¾ªç­–ç•¥ Ï€ åœ¨çŠ¶æ€ä¸­é‡‡å–è¡ŒåŠ¨æ¥é¢„æœŸçš„å›æŠ¥ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„ç­–ç•¥æ¢¯åº¦æ¥è®¡ç®—ï¼Œå…¶ä¸­åœ¨æ†ä¿æŒå‚ç›´çš„æ¯ä¸ªæ­¥éª¤éƒ½æœ‰
    1 çš„å¥–åŠ±ã€‚æˆ‘ä»¬ä½¿ç”¨é•¿æœŸå¥–åŠ± (*vt*)ï¼Œè¿™æ˜¯æ•´ä¸ª episode æœŸé—´æ‰€æœ‰æœªæ¥å¥–åŠ±çš„æŠ˜ç°æ€»å’Œã€‚å› æ­¤ï¼Œepisode è¶Šé•¿ï¼Œå½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¥–åŠ±è¶Šå¤§ï¼Œå…¶ä¸­
    gamma æ˜¯æŠ˜ç°å› å­ã€‚
- en: 'The discounted reward vector is denoted as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ˜ç°å¥–åŠ±å‘é‡è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)'
- en: For example, if an episode lasts 4 time steps, the reward for each step would
    be [4.90, 3.94, 2.97, 1.99]. From here, we can scale the reward vector by subtracting
    the mean and dividing by the standard deviation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ª episode æŒç»­ 4 ä¸ªæ—¶é—´æ­¥ï¼Œæ¯æ­¥çš„å¥–åŠ±å°†åˆ†åˆ«æ˜¯ [4.90, 3.94, 2.97, 1.99]ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®æ¥ç¼©æ”¾å¥–åŠ±å‘é‡ã€‚
- en: 'To improve the policy after each episode, we apply the Monte-Carlo policy gradient,
    as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ª episode ç»“æŸåï¼Œæˆ‘ä»¬åº”ç”¨è’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦æ¥æ”¹è¿›ç­–ç•¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/01176f23-63b1-47ff-9cee-789c93f290af.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01176f23-63b1-47ff-9cee-789c93f290af.png)'
- en: This policy is then multiplied by the rewards and fed into the optimizer and
    the weights of the neural network are updated using stochastic gradient descent.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™ä¸ªç­–ç•¥ä¹˜ä»¥å¥–åŠ±å€¼ï¼Œè¾“å…¥ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚
- en: 'The following function defines how we can update the policy in code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å‡½æ•°å®šä¹‰äº†æˆ‘ä»¬å¦‚ä½•åœ¨ä»£ç ä¸­æ›´æ–°ç­–ç•¥ï¼š
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'From here, we define the main policy training loop. At every step in a training
    episode, an action is chosen and the new state and reward are recorded. The `update_policy`
    function is called at the end of each episode to feed the episode history into
    the neural network:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰ä¸»ç­–ç•¥è®­ç»ƒå¾ªç¯ã€‚åœ¨æ¯ä¸ªè®­ç»ƒ episode çš„æ¯ä¸€æ­¥ï¼Œé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œå¹¶è®°å½•æ–°çŠ¶æ€å’Œå¥–åŠ±ã€‚åœ¨æ¯ä¸ª episode ç»“æŸæ—¶è°ƒç”¨`update_policy`å‡½æ•°ï¼Œå°†
    episode å†å²ä¼ å…¥ç¥ç»ç½‘ç»œï¼š
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Deep Q-networks
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦ Q ç½‘ç»œ
- en: '**Deep-Q Networks** (**DQNs**) combine deep learning and RL to learn in several
    different applications, particularly computer games. Let''s consider a simplified
    example of a game where there is a mouse in a maze and where the goal is for the
    mouse to eat as much cheese as possible. The more cheese the mouse eats, the more
    points it gets in the game:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ·±åº¦ Q ç½‘ç»œ** (**DQNs**) ç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œåœ¨å¤šä¸ªä¸åŒçš„åº”ç”¨ä¸­å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨ç”µå­æ¸¸æˆä¸­ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç®€åŒ–çš„æ¸¸æˆç¤ºä¾‹ï¼Œåœ¨è¿·å®«ä¸­æœ‰ä¸€åªè€é¼ ï¼Œç›®æ ‡æ˜¯è®©è€é¼ å°½å¯èƒ½å¤šåœ°åƒå¥¶é…ªã€‚è€é¼ åƒçš„å¥¶é…ªè¶Šå¤šï¼Œæ¸¸æˆå¾—åˆ†å°±è¶Šé«˜ï¼š'
- en: '![](img/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)'
- en: 'In this example, the RL terms would be as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒRL æœ¯è¯­å¦‚ä¸‹ï¼š
- en: 'Agent: The mouse as this is controlled by the computer'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†ï¼šç”±è®¡ç®—æœºæ§åˆ¶çš„è€é¼ 
- en: 'State: The current moment in the game'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŠ¶æ€ï¼šæ¸¸æˆä¸­çš„å½“å‰æ—¶åˆ»
- en: 'Action: A decision made by the mouse (to move left, right, up, or down)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨ä½œï¼šè€é¼ åšå‡ºçš„å†³ç­–ï¼ˆå‘å·¦ã€å‘å³ã€å‘ä¸Šæˆ–å‘ä¸‹ç§»åŠ¨ï¼‰
- en: 'Reward: The score in the game/amount of cheese the mouse has eatenâ€”in other
    words, the value that the agent is trying to maximize'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥–åŠ±ï¼šæ¸¸æˆä¸­çš„åˆ†æ•°/è€é¼ åƒæ‰çš„å¥¶é…ªæ•°é‡ï¼Œæ¢å¥è¯è¯´ï¼Œä»£ç†è¯•å›¾æœ€å¤§åŒ–çš„å€¼
- en: 'DQNs use Q-learning to learn the best action in a given state. They use **convolutional
    neural networks** (**ConvNets**) as a function approximator for the Q-learning
    function. ConvNets use convolutional layers to find spatial features such as where
    the mouse currently is in the grid. This means that the agent only has to learn
    Q-values for a few million rather than billions of different game states:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DQNä½¿ç”¨Q-learningæ¥å­¦ä¹ ç»™å®šçŠ¶æ€çš„æœ€ä½³åŠ¨ä½œã€‚å®ƒä»¬ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œä½œä¸ºQ-learningå‡½æ•°çš„å‡½æ•°é€¼è¿‘å™¨ã€‚ConvNetsä½¿ç”¨å·ç§¯å±‚æ¥æŸ¥æ‰¾ç©ºé—´ç‰¹å¾ï¼Œä¾‹å¦‚è€é¼ å½“å‰åœ¨ç½‘æ ¼ä¸­çš„ä½ç½®ã€‚è¿™æ„å‘³ç€ä»£ç†åªéœ€å­¦ä¹ æ•°ç™¾ä¸‡è€Œä¸æ˜¯æ•°åäº¿ç§ä¸åŒçš„æ¸¸æˆçŠ¶æ€çš„Qå€¼ï¼š
- en: '![](img/048486f3-6d05-4c7b-b741-dab2757bf1af.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/048486f3-6d05-4c7b-b741-dab2757bf1af.png)'
- en: 'An example of a DQN architecture when learning the mouse maze game is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ é¼ è¿·å®«æ¸¸æˆæ—¶DQNæ¶æ„çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š
- en: The current state (maze screen) is fed as input into the DQN.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“å‰çŠ¶æ€ï¼ˆè¿·å®«å±å¹•ï¼‰ä½œä¸ºè¾“å…¥è¾“å…¥åˆ°DQNä¸­ã€‚
- en: The input is passed through convolutional layers to find spatial patterns in
    the image. Note that no pooling is used as it is important to know the spatial
    position when modeling computer games.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾“å…¥é€šè¿‡å·ç§¯å±‚ä¼ é€’ï¼Œä»¥æ‰¾å‡ºå›¾åƒä¸­çš„ç©ºé—´æ¨¡å¼ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œæ²¡æœ‰ä½¿ç”¨æ± åŒ–ï¼Œå› ä¸ºåœ¨å»ºæ¨¡ç”µè„‘æ¸¸æˆæ—¶çŸ¥é“ç©ºé—´ä½ç½®å¾ˆé‡è¦ã€‚
- en: The output of the convolutional layers is fed into fully connected linear layers.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å·ç§¯å±‚çš„è¾“å‡ºè¢«é¦ˆé€åˆ°å…¨è¿æ¥çº¿æ€§å±‚ã€‚
- en: The output of the linear layers gives the probability that the DQN will take
    an action given its current state (move up, down, left, or right).
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: çº¿æ€§å±‚çš„è¾“å‡ºç»™å‡ºäº†DQNåœ¨å½“å‰çŠ¶æ€ä¸‹é‡‡å–è¡ŒåŠ¨çš„æ¦‚ç‡ï¼ˆå‘ä¸Šã€å‘ä¸‹ã€å‘å·¦æˆ–å‘å³ï¼‰ã€‚
- en: DQN loss function
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQNæŸå¤±å‡½æ•°
- en: 'The DQN requires a loss function for it to improve and get a higher score.
    This function can be mathematically represented as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DQNéœ€è¦ä¸€ç§æŸå¤±å‡½æ•°ä»¥æé«˜å¾—åˆ†ã€‚è¯¥å‡½æ•°å¯ä»¥æ•°å­¦è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/76163220-a598-499f-98fa-842db8be916f.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76163220-a598-499f-98fa-842db8be916f.png)'
- en: It is the Q-network that chooses which actions to take. The target network is
    what is used as an approximation for the ground truth. If we consider a scenario
    where the Q-network predicted that the correction action in a particular state
    was to move left with 80% certainty and the target network advises to move left,
    we could tweak the parameters of the Q-network using backpropagation to make it
    more likely to predict "move left" in that state. In other words, we backpropagate
    the loss through the DQN and adjust the weights of the Q-network to reduce the
    overall loss. The loss equation aims to make the probabilities of the move to
    be nearer 100% certainty of the correct choice.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯Qç½‘ç»œé€‰æ‹©è¦é‡‡å–çš„åŠ¨ä½œã€‚ç›®æ ‡ç½‘ç»œæ˜¯ç”¨ä½œåœ°é¢çœŸå®å€¼çš„è¿‘ä¼¼å€¼ã€‚å¦‚æœæˆ‘ä»¬è€ƒè™‘è¿™æ ·ä¸€ä¸ªæƒ…å†µï¼Œå³Qç½‘ç»œé¢„æµ‹åœ¨ç‰¹å®šçŠ¶æ€ä¸‹æ­£ç¡®è¡ŒåŠ¨æ˜¯å‘å·¦ç§»åŠ¨çš„æ¦‚ç‡ä¸º80%ï¼Œè€Œç›®æ ‡ç½‘ç»œå»ºè®®å‘å·¦ç§»åŠ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è°ƒæ•´Qç½‘ç»œçš„å‚æ•°ï¼Œä½¿å…¶æ›´æœ‰å¯èƒ½åœ¨è¯¥çŠ¶æ€ä¸‹é¢„æµ‹â€œå‘å·¦ç§»åŠ¨â€ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é€šè¿‡DQNåå‘ä¼ æ’­æŸå¤±å¹¶è°ƒæ•´Qç½‘ç»œçš„æƒé‡ï¼Œä»¥å‡å°‘æ€»ä½“æŸå¤±ã€‚æŸå¤±æ–¹ç¨‹æ—¨åœ¨ä½¿ç§»åŠ¨çš„æ¦‚ç‡æ›´æ¥è¿‘äº100%çš„ç¡®å®šæ€§é€‰æ‹©ã€‚
- en: Experience replay
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»éªŒå›æ”¾
- en: An experience consists of the current state, the action, the reward, and the
    next state. Each experience the agents get is recorded in the experience replay
    memory. An experience from the replay memory is sampled at random to train the
    network.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç»éªŒåŒ…æ‹¬å½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ä»£ç†è·å¾—çš„æ¯ä¸ªç»éªŒéƒ½è®°å½•åœ¨ç»éªŒå›æ”¾å†…å­˜ä¸­ã€‚ä»å›æ”¾å†…å­˜ä¸­éšæœºæŠ½å–ä¸€ä¸ªç»éªŒæ¥è®­ç»ƒç½‘ç»œã€‚
- en: Experience replay has some key advantages when compared to traditional Q-learning.
    One such advantage is that, as each experience has the potential to be used to
    train the DQN's neural network multiple times, there is greater data efficiency.
    Another advantage is that, when it is learning from experiences as soon as they
    are obtained, it is the current parameters that determine the next sample that
    the parameters are trained on. If we consider this in the maze example, if the
    next best action is to move left, then the training samples will be dominated
    by those from the left-hand side of the screen. Such behavior can cause the DQN
    to get stuck in local minima. With the incorporation of experience replay, the
    experiences used to train the DQN originate from many different points in time,
    smoothing out the learning and helping to avoid poor performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„Q-learningç›¸æ¯”ï¼Œç»éªŒå›æ”¾å…·æœ‰ä¸€äº›å…³é”®ä¼˜åŠ¿ã€‚å…¶ä¸­ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œç”±äºæ¯ä¸ªç»éªŒå¯èƒ½è¢«ç”¨æ¥å¤šæ¬¡è®­ç»ƒDQNçš„ç¥ç»ç½‘ç»œï¼Œå› æ­¤å…·æœ‰æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œåœ¨å®ƒå­¦ä¹ åˆ°ç»éªŒä¹‹åï¼Œä¸‹ä¸€ä¸ªæ ·æœ¬çš„è®­ç»ƒæ˜¯ç”±å½“å‰å‚æ•°å†³å®šçš„ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿·å®«çš„ä¾‹å­ä¸­è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œå¦‚æœä¸‹ä¸€ä¸ªæœ€ä½³è¡ŒåŠ¨æ˜¯å‘å·¦ç§»åŠ¨ï¼Œé‚£ä¹ˆè®­ç»ƒæ ·æœ¬å°†ä¸»è¦æ¥è‡ªå±å¹•å·¦ä¾§ã€‚è¿™ç§è¡Œä¸ºå¯èƒ½å¯¼è‡´DQNé™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚é€šè¿‡å¼•å…¥ç»éªŒå›æ”¾ï¼Œç”¨äºè®­ç»ƒDQNçš„ç»éªŒæ¥æºäºæ—¶é—´çš„è®¸å¤šä¸åŒç‚¹ï¼Œä»è€Œå¹³æ»‘å­¦ä¹ è¿‡ç¨‹å¹¶å¸®åŠ©é¿å…æ€§èƒ½ä¸ä½³ã€‚
- en: Coded exampleÂ â€“ DQN
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - DQN
- en: In this example, we will again consider the `CartPole-v0` environment from OpenAI
    Gym.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å†æ¬¡è€ƒè™‘æ¥è‡ªOpenAI Gymçš„`CartPole-v0`ç¯å¢ƒã€‚
- en: 'First, we create a class that will permit us to incorporate experience replay
    when training the DQN. This essentially stores the transitions observed by the
    agent. The transitions that build up a batch are decorrelated by the sampling
    process:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç±»ï¼Œå®ƒå°†å…è®¸æˆ‘ä»¬åœ¨è®­ç»ƒDQNæ—¶å¼•å…¥ç»éªŒå›æ”¾ã€‚è¿™æœ¬è´¨ä¸Šå­˜å‚¨äº†æ™ºèƒ½ä½“è§‚å¯Ÿåˆ°çš„è½¬æ¢ã€‚é€šè¿‡é‡‡æ ·è¿‡ç¨‹ï¼Œæ„å»ºä¸€ä¸ªæ‰¹æ¬¡çš„è½¬æ¢æ˜¯ä¸ç›¸å…³çš„ï¼š
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the ConvNet model whereby the difference between the current and previous
    screen patches are fed into it. The model has two outputsâ€”*Q(s,left) and Q(s,right)*.
    The network is trying to predict the expected reward/return of taking an action
    given the current input:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ConvNetæ¨¡å‹ï¼Œå…¶ä¸­å½“å‰å’Œå…ˆå‰çš„å±å¹•è¡¥ä¸ä¹‹é—´çš„å·®å¼‚è¢«é¦ˆé€è¿›å»ã€‚æ¨¡å‹æœ‰ä¸¤ä¸ªè¾“å‡ºâ€”*Q(s,left)å’ŒQ(s,right)*ã€‚ç½‘ç»œè¯•å›¾é¢„æµ‹åœ¨ç»™å®šå½“å‰è¾“å…¥æ—¶é‡‡å–è¡ŒåŠ¨çš„é¢„æœŸå¥–åŠ±/å›æŠ¥ï¼š
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Set the hyperparameters of the model along with some utilities for training:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æ¨¡å‹çš„è¶…å‚æ•°ä»¥åŠä¸€äº›ç”¨äºè®­ç»ƒçš„å®ç”¨ç¨‹åºï¼š
- en: '[PRE18]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we have the code for training our model. This function performs a
    single step of the optimization. First, it samples a batch and concatenates all
    of the tensors into a single one. It computes *Q(st,at)* and *V(st+1)=maxaQ(st+1,a)*
    and combines them into a loss. By definition, we set *V(s)=0* if *s* is a terminal
    state. We also use a target network to compute *V(st+1)* for added stability:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æœ‰è®­ç»ƒæ¨¡å‹çš„ä»£ç ã€‚æ­¤å‡½æ•°æ‰§è¡Œä¼˜åŒ–çš„å•æ­¥ã€‚é¦–å…ˆï¼Œå®ƒå¯¹æ‰¹æ¬¡è¿›è¡Œé‡‡æ ·å¹¶å°†æ‰€æœ‰å¼ é‡è¿æ¥æˆä¸€ä¸ªå•ä¸€å¼ é‡ã€‚å®ƒè®¡ç®—*Q(st,at)*å’Œ*V(st+1)=maxaQ(st+1,a)*ï¼Œå¹¶å°†å®ƒä»¬ç»“åˆæˆä¸€ä¸ªæŸå¤±ã€‚æ ¹æ®å®šä¹‰ï¼Œå¦‚æœ*s*æ˜¯ç»ˆç«¯çŠ¶æ€ï¼Œåˆ™è®¾ç½®*V(s)=0*ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ç›®æ ‡ç½‘ç»œæ¥è®¡ç®—*V(st+1)*ä»¥æé«˜ç¨³å®šæ€§ï¼š
- en: '[PRE19]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compute a mask of non-final states. After this, we concatenate the batch elements:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—éæœ€ç»ˆçŠ¶æ€çš„æ©ç ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†æ‰¹æ¬¡å…ƒç´ è¿æ¥èµ·æ¥ï¼š
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Compute *Q(s_t, a)*, then select the columns of actions taken:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—*Q(s_t, a)*ï¼Œç„¶åé€‰æ‹©æ‰€é‡‡å–çš„åŠ¨ä½œåˆ—ï¼š
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We compute the expected Q values:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—é¢„æœŸçš„Qå€¼ï¼š
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then compute the Huber loss function:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¡ç®—HuberæŸå¤±å‡½æ•°ï¼š
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we optimize the model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬ä¼˜åŒ–æ¨¡å‹ï¼š
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we observe the new state:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬è§‚å¯Ÿæ–°çŠ¶æ€ï¼š
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We store the transition in memory:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è½¬æ¢å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼š
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s perform one step of the optimization (on the target network):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰§è¡Œä¼˜åŒ–çš„ä¸€æ­¥ï¼ˆåœ¨ç›®æ ‡ç½‘ç»œä¸Šï¼‰ï¼š
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Update the target network; copy all weights and biases in the DQN:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°ç›®æ ‡ç½‘ç»œï¼›å¤åˆ¶æ‰€æœ‰DQNä¸­çš„æƒé‡å’Œåç½®ï¼š
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This outputs some visualizations to give insight into how the model performs
    during training:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¾“å‡ºä¸€äº›å¯è§†åŒ–ï¼Œä»¥ä¾¿äº†è§£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼š
- en: '![](img/d983eb9e-2dd3-407d-9965-184d98e4f351.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d983eb9e-2dd3-407d-9965-184d98e4f351.png)'
- en: 'The following diagram summarizes what the model in this coded example is doing:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾è¡¨æ€»ç»“äº†æ­¤ç¼–ç ç¤ºä¾‹ä¸­æ¨¡å‹çš„æ“ä½œï¼š
- en: '![](img/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)'
- en: Double deep Q-learning
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŒé‡æ·±åº¦Qå­¦ä¹ 
- en: Double deep Q-learning generally leads to better performance of the AI agents
    when compared to vanilla DQNs. A common problem with deep Q-learning is that,
    sometimes, the agents can learn unrealistically high action values because it
    includes a maximization step over estimated action values. This tends to prefer
    overestimated to underestimated values. If overestimations are not uniform and
    not concentrated at states about which we wish to learn more, then these can negatively
    affect the quality of the resulting policy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ·±åº¦Qå­¦ä¹ é€šå¸¸æ¯”çº¯æ·±åº¦Qç½‘ç»œè¡¨ç°æ›´å¥½ã€‚æ·±åº¦Qå­¦ä¹ çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼Œæœ‰æ—¶ä»£ç†å¯ä»¥å­¦ä¹ åˆ°ä¸åˆ‡å®é™…åœ°é«˜çš„è¡ŒåŠ¨ä»·å€¼ï¼Œå› ä¸ºå®ƒåŒ…æ‹¬å¯¹ä¼°è®¡è¡ŒåŠ¨ä»·å€¼çš„æœ€å¤§åŒ–æ­¥éª¤ã€‚è¿™å€¾å‘äºåçˆ±è¿‡é«˜ä¼°è®¡çš„å€¼è€Œä¸æ˜¯ä½ä¼°å€¼ã€‚å¦‚æœè¿‡é«˜ä¼°è®¡ä¸å‡åŒ€ä¸”ä¸é›†ä¸­åœ¨æˆ‘ä»¬å¸Œæœ›æ›´å¤šäº†è§£çš„çŠ¶æ€ä¸Šï¼Œåˆ™å¯èƒ½ä¼šå¯¹ç»“æœç­–ç•¥çš„è´¨é‡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚
- en: The idea of double Q-learning is to reduce these overestimations. It does this
    by decomposing the max operation in the target into action selection and action
    evaluation. In the vanilla DQN implementation, the action selection and action
    evaluation are coupled. It uses the target network to select the action and, at
    the same time, to estimate the quality of the action.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: åŒQå­¦ä¹ çš„æ€æƒ³æ˜¯å‡å°‘è¿™äº›è¿‡é«˜ä¼°è®¡ã€‚å®ƒé€šè¿‡å°†ç›®æ ‡ä¸­çš„maxæ“ä½œåˆ†è§£ä¸ºè¡ŒåŠ¨é€‰æ‹©å’Œè¡ŒåŠ¨è¯„ä¼°æ¥å®ç°ã€‚åœ¨çº¯æ·±åº¦Qç½‘ç»œå®ç°ä¸­ï¼Œè¡ŒåŠ¨é€‰æ‹©å’Œè¡ŒåŠ¨è¯„ä¼°æ˜¯è€¦åˆçš„ã€‚å®ƒä½¿ç”¨ç›®æ ‡ç½‘ç»œæ¥é€‰æ‹©è¡ŒåŠ¨ï¼Œå¹¶åŒæ—¶ä¼°è®¡è¡ŒåŠ¨çš„è´¨é‡ã€‚
- en: We are using the target-network to select the action and at the same time to
    estimate the quality of the action. Double Q-learning essentially tries to decouple
    both procedures from each other.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ç›®æ ‡ç½‘ç»œæ¥é€‰æ‹©è¡ŒåŠ¨ï¼Œå¹¶åŒæ—¶ä¼°è®¡è¡ŒåŠ¨çš„è´¨é‡ã€‚åŒQå­¦ä¹ æœ¬è´¨ä¸Šè¯•å›¾å°†è¿™ä¸¤ä¸ªè¿‡ç¨‹è§£è€¦ã€‚
- en: 'In double Q-learning, the temporal difference (TD) target looks as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŒQå­¦ä¹ ä¸­ï¼Œæ—¶åºå·®åˆ†ï¼ˆTDï¼‰ç›®æ ‡å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/70934206-aa2d-46c4-b277-fbc831cef8f5.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70934206-aa2d-46c4-b277-fbc831cef8f5.png)'
- en: 'The calculation of new TD target can be summarized in the following steps:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°çš„TDç›®æ ‡çš„è®¡ç®—å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š
- en: Q-Network uses the next state, *s'*, to calculate qualities, *Q(s',a)*, for
    each possible action, *a*, in the state, *s'*.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Qç½‘ç»œä½¿ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€*s'*æ¥è®¡ç®—åœ¨çŠ¶æ€*s'*ä¸­æ¯ä¸ªå¯èƒ½çš„è¡ŒåŠ¨*a*çš„è´¨é‡*Q(s',a)*ã€‚
- en: The `argmax` operation applied on *Q(s',a)* chooses the action, *a**, that belongs
    to the highest quality (action selection).
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åº”ç”¨äº*Q(s',a)*çš„`argmax`æ“ä½œé€‰æ‹©äº†å±äºæœ€é«˜è´¨é‡çš„è¡ŒåŠ¨*a**ï¼ˆè¡ŒåŠ¨é€‰æ‹©ï¼‰ã€‚
- en: The quality *Q(s',a*)*, that belongs to the action, *a**, is selected for the
    calculation of the target.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¡ŒåŠ¨çš„è´¨é‡ *Q(s',a*)*ï¼Œå±äºè¡ŒåŠ¨*a**ï¼Œè¢«é€‰ä¸ºç›®æ ‡çš„è®¡ç®—ã€‚
- en: 'The process of double Q-learning can be visualized as per the following diagram.
    An AI agent is at the initial in state s; it knows, based on some previous calculations,
    the qualities Q(s, a[1]) and Q(s, a[2]) for possible two actions in that state.
    The agent then decides to take action a[1] and ends up in state s'':'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: åŒQå­¦ä¹ çš„è¿‡ç¨‹å¯ä»¥å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚AIä»£ç†å¤„äºåˆå§‹çŠ¶æ€*s*ï¼ŒåŸºäºä¸€äº›å…ˆå‰çš„è®¡ç®—ï¼Œå®ƒçŸ¥é“è¯¥çŠ¶æ€ä¸­å¯èƒ½çš„ä¸¤ä¸ªè¡ŒåŠ¨*a[1]*å’Œ*a[2]*çš„è´¨é‡*Q(s, a[1])*å’Œ*Q(s,
    a[2])*ã€‚ç„¶åä»£ç†å†³å®šé‡‡å–è¡ŒåŠ¨*a[1]*å¹¶è¿›å…¥çŠ¶æ€*s'*ï¼š
- en: '![](img/e883c737-107a-4652-b211-e9d4815d0c46.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e883c737-107a-4652-b211-e9d4815d0c46.png)'
- en: Actor-critic methods
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•
- en: 'Actor-critic methods aim to incorporate the advantages of both value- and policy-based
    methods while eliminating their drawbacks:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•æ—¨åœ¨ç»“åˆå€¼å’ŒåŸºäºç­–ç•¥çš„æ–¹æ³•çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æ¶ˆé™¤å®ƒä»¬çš„ç¼ºç‚¹ï¼š
- en: '![](img/0968abce-a6e7-461f-8549-37d45a09c41e.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0968abce-a6e7-461f-8549-37d45a09c41e.png)'
- en: 'The fundamental idea behind actor-critics is to split the model into two parts:
    one for computing an action based on a state and another to produce the Q-values
    of the action.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå®¶çš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ¨¡å‹åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†ç”¨äºæ ¹æ®çŠ¶æ€è®¡ç®—è¡ŒåŠ¨ï¼Œå¦ä¸€éƒ¨åˆ†ç”¨äºç”Ÿæˆè¡ŒåŠ¨çš„Qå€¼ã€‚
- en: 'The actor is a neural network that takes the state as input and outputs the
    best action. By learning the optimal policy, it controls how the agent behaves.
    The critic evaluates the action by computing the value function. In other words,
    the actor tries to optimize the policy and the critic tries to optimize the value.
    The two models improve over time at their individual roles and, as such, the overall
    architecture learns more efficiently than the two methods separately:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå°†çŠ¶æ€ä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºæœ€ä½³è¡ŒåŠ¨ã€‚é€šè¿‡å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå®ƒæ§åˆ¶ä»£ç†çš„è¡Œä¸ºæ–¹å¼ã€‚è¯„è®ºå®¶é€šè¿‡è®¡ç®—ä»·å€¼å‡½æ•°è¯„ä¼°è¡ŒåŠ¨ã€‚æ¢å¥è¯è¯´ï¼Œæ¼”å‘˜å°è¯•ä¼˜åŒ–ç­–ç•¥ï¼Œè¯„è®ºå®¶å°è¯•ä¼˜åŒ–ä»·å€¼ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹éšç€æ—¶é—´çš„æ¨ç§»åœ¨å„è‡ªçš„è§’è‰²ä¸Šå¾—åˆ°æ”¹è¿›ï¼Œå› æ­¤æ•´ä½“æ¶æ„çš„å­¦ä¹ æ•ˆç‡é«˜äºå•ç‹¬ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•ï¼š
- en: '![](img/f65da315-87b4-4a85-9b63-15004169d6e1.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f65da315-87b4-4a85-9b63-15004169d6e1.png)'
- en: The two models are essentially competing with one another. Such an approach
    is becoming increasingly popular in the field of machine learning; for example,
    this is also present in generative adversarial networks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªæ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ç›¸äº’ç«äº‰çš„ã€‚è¿™ç§æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸè¶Šæ¥è¶Šæµè¡Œï¼›ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸­ä¹Ÿæœ‰è¿™ç§æƒ…å†µã€‚
- en: The nature of the actor's role is to be exploratory. It frequently tries new
    things and explores the environment. The role of the critic is to either criticize
    or compliment the actions of the actor. The actor takes this feedback on board
    and adjusts its behavior accordingly. As the actor receives more and more feedback,
    it becomes better at deciding which actions to take.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜çš„è§’è‰²æ€§è´¨æ˜¯æ¢ç´¢æ€§çš„ã€‚å®ƒç»å¸¸å°è¯•æ–°äº‹ç‰©å¹¶æ¢ç´¢ç¯å¢ƒã€‚è¯„è®ºè€…çš„è§’è‰²æ˜¯è¦ä¹ˆæ‰¹è¯„ï¼Œè¦ä¹ˆèµæ‰¬æ¼”å‘˜çš„è¡ŒåŠ¨ã€‚æ¼”å‘˜æ¥å—è¿™äº›åé¦ˆå¹¶ç›¸åº”åœ°è°ƒæ•´å…¶è¡Œä¸ºã€‚éšç€æ¼”å‘˜è·å¾—è¶Šæ¥è¶Šå¤šçš„åé¦ˆï¼Œå®ƒåœ¨å†³å®šé‡‡å–å“ªäº›è¡ŒåŠ¨æ—¶å˜å¾—è¶Šæ¥è¶Šå¥½ã€‚
- en: Like a neural network, the actor can be a function approximator where its task
    is to produce the best action for a given state. This could be a fully connected
    or convolutional neural network, for example. The critic is also a function approximator,
    which receives the environment as input along with the action by the actor. It
    concatenates these inputs and outputs the action value (Q-value).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒç¥ç»ç½‘ç»œä¸€æ ·ï¼Œæ¼”å‘˜å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°é€¼è¿‘å™¨ï¼Œå…¶ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„çŠ¶æ€ç”Ÿæˆæœ€ä½³åŠ¨ä½œã€‚ä¾‹å¦‚ï¼Œè¿™å¯ä»¥æ˜¯ä¸€ä¸ªå®Œå…¨è¿æ¥çš„æˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚è¯„è®ºè€…ä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°é€¼è¿‘å™¨ï¼Œå®ƒæ¥æ”¶ç¯å¢ƒå’Œæ¼”å‘˜çš„åŠ¨ä½œä½œä¸ºè¾“å…¥ã€‚å®ƒè¿æ¥è¿™äº›è¾“å…¥å¹¶è¾“å‡ºåŠ¨ä½œå€¼ï¼ˆQå€¼ï¼‰ã€‚
- en: The two networks are trained separately and, to update their weights, they use
    gradient ascent as opposed to descent as it aims to determine the global maximum
    rather than minimum. Weights are updated at each step rather than at the end of
    an episode as opposed to policy gradients.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªç½‘ç»œåˆ†åˆ«è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦ä¸Šå‡è€Œä¸æ˜¯ä¸‹é™æ¥æ›´æ–°å®ƒä»¬çš„æƒé‡ï¼Œå› ä¸ºå®ƒæ—¨åœ¨ç¡®å®šå…¨å±€æœ€å¤§å€¼è€Œä¸æ˜¯æœ€å°å€¼ã€‚æƒé‡åœ¨æ¯ä¸ªæ­¥éª¤è€Œä¸æ˜¯åœ¨æ¯ä¸ªç­–ç•¥æ¢¯åº¦æœ«å°¾æ›´æ–°ã€‚
- en: Actor-critics have been proven to learn complex environments and have been used
    in many 2D and 3D computer games such as *Super Mario* and *Doom*.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜è¯„è®ºå·²è¢«è¯æ˜èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„ç¯å¢ƒï¼Œå¹¶å·²åœ¨è®¸å¤šäºŒç»´å’Œä¸‰ç»´ç”µè„‘æ¸¸æˆä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚*è¶…çº§é©¬é‡Œå¥¥*å’Œ*Doom*ã€‚
- en: Coded exampleÂ â€“ actor-critic model
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - æ¼”å‘˜è¯„è®ºæ¨¡å‹
- en: 'Here, we will consider a coded implementation example in PyTorch. First, we
    define the `ActorCritic` class:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€ä¸ªåœ¨PyTorchä¸­çš„ç¼–ç å®ç°ç¤ºä¾‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰`ActorCritic`ç±»ï¼š
- en: '[PRE29]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we initialize the model:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE30]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define a function that will choose the best action based on the state:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªåŸºäºçŠ¶æ€é€‰æ‹©æœ€ä½³åŠ¨ä½œçš„å‡½æ•°ï¼š
- en: '[PRE31]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From here, we need to define the function that calculates the total returns
    and considers the loss function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰è®¡ç®—æ€»å›æŠ¥å¹¶è€ƒè™‘æŸå¤±å‡½æ•°çš„å‡½æ•°ï¼š
- en: '[PRE32]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we can train the model and review how it performs:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ¨¡å‹å¹¶æŸ¥çœ‹å…¶è¡¨ç°ï¼š
- en: '[PRE33]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç»™å‡ºä»¥ä¸‹è¾“å‡ºï¼š
- en: '![](img/776751b2-6167-4fa2-945b-a9722ced0530.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/776751b2-6167-4fa2-945b-a9722ced0530.png)'
- en: Asynchronous actor-critic algorithm
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼‚æ­¥æ¼”å‘˜è¯„è®ºç®—æ³•
- en: '**Asynchronous Advantage Actor-Critic** or **A3C** is an algorithm proposed
    by Google''s DeepMind. The algorithm has been proven to outperform other algorithms.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®º**æˆ–**A3C**æ˜¯ç”±è°·æ­Œçš„DeepMindæå‡ºçš„ä¸€ç§ç®—æ³•ã€‚è¯¥ç®—æ³•å·²è¢«è¯æ˜ä¼˜äºå…¶ä»–ç®—æ³•ã€‚'
- en: 'In A3C, there are multiple instances of agents, each of which has been initialized
    differently in their own separate environments. Each individual agent begins to
    take actions and go through the reinforcement learning process to gather their
    own unique experiences. These unique experiences are then used to update the global
    neural network. This global neural network is shared by all of the agents and
    it influences all of the actions of the agents, and every new experience from
    each agent improves the overall network speed:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨A3Cä¸­ï¼Œæœ‰å¤šä¸ªä»£ç†å®ä¾‹ï¼Œæ¯ä¸ªä»£ç†å®ä¾‹åœ¨å…¶è‡ªå·±çš„ç‹¬ç«‹ç¯å¢ƒä¸­è¿›è¡Œä¸åŒçš„åˆå§‹åŒ–ã€‚æ¯ä¸ªä¸ªä½“ä»£ç†å¼€å§‹é‡‡å–è¡ŒåŠ¨ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æ¥æ”¶é›†è‡ªå·±ç‹¬ç‰¹çš„ç»éªŒã€‚ç„¶åï¼Œè¿™äº›ç‹¬ç‰¹çš„ç»éªŒç”¨äºæ›´æ–°å…¨å±€ç¥ç»ç½‘ç»œã€‚è¿™ä¸ªå…¨å±€ç¥ç»ç½‘ç»œè¢«æ‰€æœ‰ä»£ç†å…±äº«ï¼Œå®ƒå½±å“æ‰€æœ‰ä»£ç†çš„è¡ŒåŠ¨ï¼Œæ¯ä¸ªä»£ç†çš„æ¯ä¸ªæ–°ç»éªŒéƒ½æé«˜äº†æ•´ä½“ç½‘ç»œçš„é€Ÿåº¦ï¼š
- en: '![](img/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)'
- en: 'The **Advantage** term in the name is the value that states whether or not
    there is an improvement in an action compared to the expected average value of
    that state based on. The advantage formula is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: åç§°ä¸­çš„**ä¼˜åŠ¿**æœ¯è¯­æ˜¯æŒ‡çŠ¶æ€çš„é¢„æœŸå¹³å‡å€¼ä¸è¯¥çŠ¶æ€çš„è¡ŒåŠ¨ç›¸æ¯”æ˜¯å¦æœ‰æ”¹è¿›çš„ä»·å€¼ã€‚ä¼˜åŠ¿å…¬å¼å¦‚ä¸‹ï¼š
- en: '*A (s,a) = Q(s,a) - V(s)*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*A (s,a) = Q(s,a) - V(s)*'
- en: Practical applications
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®é™…åº”ç”¨
- en: 'RL methods have been applied to solve problems in a multitude of areas in the
    real world. Here, we consider some examples of these:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ–¹æ³•å·²è¢«åº”ç”¨äºè§£å†³ç°å®ä¸–ç•Œä¸­å¤šç§é¢†åŸŸçš„é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘äº†å…¶ä¸­ä¸€äº›ä¾‹å­ã€‚
- en: '**Robotics**: There has been a significant amount of work on applying RL in
    the field of robotics. In the present day, manufacturing facilities are full of
    robots performing a variety of tasks, the foundations of which are RL methods:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœºå™¨äººæŠ€æœ¯**: åœ¨æœºå™¨äººé¢†åŸŸåº”ç”¨å¼ºåŒ–å­¦ä¹ çš„å·¥ä½œå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¦‚ä»Šï¼Œåˆ¶é€ è®¾æ–½å……æ–¥ç€æ‰§è¡Œå„ç§ä»»åŠ¡çš„æœºå™¨äººï¼Œå…¶åŸºç¡€æ˜¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼š'
- en: '![](img/feda1bf9-3714-4899-9f98-97915440d76d.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/feda1bf9-3714-4899-9f98-97915440d76d.png)'
- en: '**Traffic light control**: In the paperÂ *Reinforcement learning-based multi-agent
    system for network traffic signal control*, researchers designed a traffic light
    controller to solve congestion problems that demonstrated superior results to
    other methods:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äº¤é€šä¿¡å·ç¯æ§åˆ¶**: åœ¨è®ºæ–‡*åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“ç½‘ç»œäº¤é€šä¿¡å·æ§åˆ¶ç³»ç»Ÿ*ä¸­ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸€ä¸ªäº¤é€šä¿¡å·ç¯æ§åˆ¶å™¨æ¥è§£å†³æ‹¥å µé—®é¢˜ï¼Œè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼š'
- en: '![](img/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)'
- en: '**Personalized recommendations**: RL has been applied in news recommendation
    systems to account for the fact that news changes rapidly and, as users tend to
    have a short attention span, the click-through rate alone cannot reflect the retention
    rate of the users:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ªæ€§åŒ–æ¨è**: å¼ºåŒ–å­¦ä¹ å·²ç»åº”ç”¨äºæ–°é—»æ¨èç³»ç»Ÿä¸­ï¼Œä»¥åº”å¯¹æ–°é—»å¿«é€Ÿå˜åŒ–çš„ç‰¹ç‚¹ï¼Œç”¨æˆ·çš„æ³¨æ„åŠ›ä¸é›†ä¸­ï¼Œä»…å‡­ç‚¹å‡»ç‡æ— æ³•åæ˜ ç”¨æˆ·çš„ç•™å­˜ç‡ï¼š'
- en: '![](img/293bd127-5ca4-4eb7-8524-78d8385046ea.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/293bd127-5ca4-4eb7-8524-78d8385046ea.png)'
- en: '**Generating images**: There has been a lot of research into combining RL with
    other deep learning architectures for a host of different applications. Many of
    these have shown some impressive results. DeepMind showed that using generative
    models and RL, they were able to successfully generate images:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”Ÿæˆå›¾åƒ**: å¯¹äºå°†å¼ºåŒ–å­¦ä¹ ä¸å…¶ä»–æ·±åº¦å­¦ä¹ æ¶æ„ç»“åˆè¿›è¡Œç ”ç©¶å·²ç»æœ‰å¾ˆå¤šæˆæœã€‚DeepMindå±•ç¤ºäº†ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æˆåŠŸç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ï¼š'
- en: '![](img/3e043c7a-f238-4bee-83d6-3918928f53e7.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e043c7a-f238-4bee-83d6-3918928f53e7.png)'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, we began by covering the basics of RL and introduced more advanced
    algorithms that have proven an ability to outperform humans in real-world scenarios.
    We also gave examples as to how these can be implemented in PyTorch.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€ï¼Œå¹¶ä»‹ç»äº†ä¸€äº›åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºè¶…è¶Šäººç±»èƒ½åŠ›çš„å…ˆè¿›ç®—æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¿™äº›ç®—æ³•å¦‚ä½•åœ¨PyTorchä¸­å®ç°ã€‚
- en: In the next and final chapter, there will be an overview of this book, along
    with some tips on how you can keep yourself up to date with recent advancements
    in the data science space.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„æœ€åä¸€ç« ä¸­ï¼Œå°†æ¦‚è¿°æœ¬ä¹¦å†…å®¹ï¼Œå¹¶æä¾›å¦‚ä½•ä¿æŒä¸æ•°æ®ç§‘å­¦é¢†åŸŸæœ€æ–°è¿›å±•çš„æŠ€å·§ã€‚
- en: Further reading
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'Refer to the following links for more information:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚è€ƒä»¥ä¸‹é“¾æ¥è·å–æ›´å¤šä¿¡æ¯ï¼š
- en: '*A Brief Survey of Deep Reinforcement Learning*: [https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€è¦æ¦‚è¿°*: [https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)'
- en: '*Playing Atari with Deep Reinforcement Learning*: [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç©Atariæ¸¸æˆ*: [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
- en: '*Deep Reinforcement Learning with Double Q-learning*: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*åŒQå­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ *: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
- en: '*Continuous Control with Deep Reinforcement Learning*: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿ç»­æ§åˆ¶*: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
- en: '*Asynchronous Methods for Deep Reinforcement Learning*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å¼‚æ­¥æ–¹æ³•*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
- en: '*Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor*: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼šåŸºäºæœ€å¤§ç†µæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç¦»ç­–ç•¥æ–¹æ³•*: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
- en: '*Reinforcement learning-based multi-agent system for network traffic signal
    control*: [http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf](http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“ç½‘ç»œäº¤é€šä¿¡å·æ§åˆ¶ç³»ç»Ÿ*: [http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf](http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf)'
- en: '*End-to-End Training of Deep Visuomotor Policies*: [https://arxiv.org/pdf/1504.00702.pdf](https://arxiv.org/pdf/1504.00702.pdf)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç«¯åˆ°ç«¯è®­ç»ƒæ·±åº¦è§†è§‰è¿åŠ¨ç­–ç•¥*: [https://arxiv.org/pdf/1504.00702.pdf](https://arxiv.org/pdf/1504.00702.pdf)'
- en: '*DRN: A Deep Reinforcement Learning Framework for News Recommendation*: [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DRN: ä¸€ç§ç”¨äºæ–°é—»æ¨èçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶*: [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
- en: '*Synthesizing Programs for Images using Reinforced Adversarial Learning*: [https://arxiv.org/pdf/1804.01118.pdf](https://arxiv.org/pdf/1804.01118.pdf)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨å¢å¼ºå¯¹æŠ—å­¦ä¹ åˆæˆå›¾åƒç¨‹åº*: [https://arxiv.org/pdf/1804.01118.pdf](https://arxiv.org/pdf/1804.01118.pdf)'
