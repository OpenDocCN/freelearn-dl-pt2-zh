["```py\ndef new_board():\n   return ((0,0,0),\n          (0,0,0),\n          (0,0,0))\n```", "```py\ndef apply_move(board_state, move, side):\n    move_x, move_y = move\n    state_list = list(list(s) for s in board_state)\n    state_list[move_x][move_y] = side\n    return tuple(tuple(s) for s in state_list)\n```", "```py\nimport itertools\n\ndef available_moves(board_state):\n    for x, y in itertools.product(range(3), range(3)):\n            if board_state[x][y] == 0:\n                  yield (x, y)\n```", "```py\ndef has_3_in_a_line(line):\n  return all(x==-1 for x in line) | all(x==1 for x in line)\n```", "```py\ndef has_winner(board_state):\n    # check rows\n    for x in range(3):\n        if has_3_in_a_line (board_state[x]):\n            return board_state[x][0]\n    # check columns\n    for y in range(3):\n        if has_3_in_a_line([i[y] for i in board_state]):\n            return board_state[0][y]\n    # check diagonals\n    if has_3_in_a_line([board_state[i][i] for i in range(3)]):\n        return board_state[0][0]\n    if has_3_in_a_line([board_state[2 - i][i] for i in range(3)]):\n        return board_state[0][2]\n    return 0 # no one has won\n```", "```py\ndef play_game(plus_player_func, minus_player_func):\n\nboard_state = new_board()\n\nplayer_turn = 1\n```", "```py\n    while True:\n        _available_moves = list(available_moves(board_state))\n        if len(_available_moves) == 0:\n            print(\"no moves left, game ended a draw\")\n            return 0.\n```", "```py\n        if player_turn > 0:\n            move = plus_player_func(board_state, 1)\n        else:\n            move = minus_player_func(board_state, -1)\n```", "```py\n        if move not in _avialable_moves:\n            # if a player makes an invalid move the other player wins\n            print(\"illegal move \", move)\n            return -player_turn\n```", "```py\n        board_state = apply_move(board_state, move, player_turn)\n        print(board_state)\n\n        winner = has_winner(board_state)\n        if winner != 0:\n            print(\"we have a winner, side: %s\" % player_turn)\n            return winner\n        player_turn = -player_turn\n```", "```py\ndef random_player(board_state, side):\n    moves = list(available_moves(board_state))\n    return random.choice(moves)\n```", "```py\nplay_game(random_player, random_player)\n\n((0, 0, 0), (0, 0, 0), [1, 0, 0])\n([0, -1, 0], (0, 0, 0), [1, 0, 0])\n([0, -1, 0], [0, 1, 0], [1, 0, 0])\n([0, -1, 0], [0, 1, 0], [1, -1, 0])\n([0, -1, 0], [0, 1, 1], [1, -1, 0])\n([0, -1, 0], [0, 1, 1], [1, -1, -1])\n([0, -1, 1], [0, 1, 1], [1, -1, -1])\nwe have a winner, side: 1\n\n```", "```py\ndef score_line(line):\n    minus_count = line.count(-1)\n    plus_count = line.count(1)\n    if plus_count == 2 and minus_count == 0:\n        return 1\n    elif minus_count == 2 and plus_count == 0:\n        return -1\n    return 0\n```", "```py\ndef evaluate(board_state):\n    score = 0\n    for x in range(3):\n        score += score_line(board_state[x])\n    for y in range(3):\n        score += score_line([i[y] for i in board_state])\n    #diagonals\n    score += score_line([board_state[i][i] for i in range(3)])\n    score += score_line([board_state[2-i][i] for i in range(3)])\n\n    return score\n```", "```py\ndef min_max(board_state, side, max_depth):\n    best_score = None\n    best_score_move = None\n```", "```py\n    moves = list(available_moves(board_state))\n    if not moves:\n        return 0, None\n```", "```py\n    for move in moves:\n new_board_state = apply_move(board_state, move, side)\n```", "```py\n\nwinner = has_winner(new_board_state)\n        if winner != 0:\n            return winner * 10000, move\n```", "```py\n        else:\n            if max_depth <= 1:\n                score = evaluate(new_board_state)\n            else:\n                score, _ = min_max(new_board_state, -side, max_depth - 1)\n```", "```py\n            if side > 0:\n                if best_score is None or score > best_score:\n                    best_score = score\n                    best_score_move = move\n            else:\n                if best_score is None or score < best_score:\n                    best_score = score\n                    best_score_move = move\n       return best_score, best_score_move\n```", "```py\ndef min_max_player(board_state, side):\n    return min_max(board_state, side, 5)[1]\n```", "```py\nimport sys\n\ndef\n```", "```py\nmin_max_alpha_beta(board_state, side, max_depth, \n                       alpha=-sys.float_info.max,\n                       beta=sys.float_info.max):\n```", "```py\n    best_score_move = None\n    moves = list(available_moves(board_state))\n    if not moves:\n        return 0, None\n\n    for move in moves:\n        new_board_state = apply_move(board_state, move, side)\n        winner = has_winner(new_board_state)\n        if winner != 0:\n            return winner * 10000, move\n        else:\n            if max_depth <= 1:\n                score = evaluate(new_board_state)\n            else:\n                score, _ = min_max_alpha_beta(new_board_state, -side, max_depth - 1, alpha, beta)\n```", "```py\n        if side > 0:\n            if score > alpha:\n                alpha = score\n                best_score_move = move\n```", "```py\n        else:\n            if score < beta:\n                beta = score\n                best_score_move = move\n```", "```py\n        if alpha >= beta:\n            break\n```", "```py\n    return alpha if side > 0 else beta, best_score_move\n```", "```py\nimport collections\n\ndef monte_carlo_sample(board_state, side):\n    result = has_winner(board_state)\n    if result != 0:\n        return result, None\n    moves = list(available_moves(board_state))\n    if not moves:\n        return 0, None\n```", "```py\n    # select a random move\n    move = random.choice(moves)\n    result, next_move = monte_carlo_sample(apply_move(board_state, move, side), -side)\n    return result, move\n```", "```py\ndef monte_carlo_tree_search(board_state, side, number_of_samples):\n    results_per_move = collections.defaultdict(lambda: [0, 0])\n    for _ in range(number_of_samples):\n        result, move = monte_carlo_sample(board_state, side)\n        results_per_move[move][0] += result\n        results_per_move[move][1] += 1\n```", "```py\n    move = max(results_per_move, \n        key=lambda x: results_per_move.get(x)[0] /\n                results_per_move[move][1])\n```", "```py\n    return results_per_move[move][0] / results_per_move[move][1], move\n```", "```py\n    def upper_confidence_bounds(payout, samples_for_this_machine, log_total_samples):\n        return payout / samples_for_this_machine \n               + math.sqrt((2 * log_total_samples)\n                           / samples_for_this_machine)\n    ```", "```py\ndef monte_carlo_tree_search_uct(board_state, side, number_of_rollouts):\n\nstate_results = collections.defaultdict(float)\n\nstate_samples = collections.defaultdict(float)\n```", "```py\n    for _ in range(number_of_rollouts):\n        current_side = side\n        current_board_state = board_state\n\nfirst_unvisited_node = True\n\nrollout_path = []\n        result = 0\n```", "```py\n while result == 0:\n\nmove_states = {move: apply_move(current_board_state, move, current_side)\n                           for move in available_moves(current_board_state)}\n\n            if not move_states:\n\nresult = 0\n                break\n```", "```py\n            if all((state in state_samples) for _, state in move_states):\n                log_total_samples = math.log(sum(state_samples[s] for s in move_states.values()))\n                move, state = max(move_states, key=lambda _, s:\n                upper_confidence_bounds(state_results[s], state_samples[s], log_total_samples))\n            else:\n                move = random.choice(list(move_states.keys()))\n```", "```py\n\ncurrent_board_state = move_states[move]\n```", "```py\n            if first_unvisited_node:\n\nrollout_path.append((current_board_state, current_side))\n                if current_board_state not in state_samples:\n                    first_unvisited_node = False\n```", "```py\n            current_side = -current_side\n            result = has_winner(current_board_state)\n```", "```py\n        for path_board_state, path_side in rollout_path:\n            state_samples[path_board_state] += 1.\n\nresult = result*path_side/2.+.5\n            state_results[path_board_state] += result\n\n```", "```py\n    move_states = {move: apply_move(board_state, move, side) for move in available_moves(board_state)}\n\n    move = max(move_states, key=lambda x: state_results[move_states[x]] / state_samples[move_states[x]])\n\n    return state_results[move_states[move]] / state_samples[move_states[move]], move\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nHIDDEN_NODES = (100, 100, 100) \nINPUT_NODES = 3 * 3 \nLEARN_RATE = 1e-4\nOUTPUT_NODES = INPUT_NODES\n```", "```py\ninput_placeholder = tf.placeholder(\"float\", shape=(None, INPUT_NODES))\n```", "```py\nhidden_weights_1 = tf.Variable(tf.truncated_normal((INPUT_NODES, HIDDEN_NODES[0]), stddev=1\\. / np.sqrt(INPUT_NODES)))\nhidden_weights_2 = tf.Variable(\ntf.truncated_normal((HIDDEN_NODES[0], HIDDEN_NODES[1]), stddev=1\\. / np.sqrt(HIDDEN_NODES[0])))\nhidden_weights_3 = tf.Variable(\ntf.truncated_normal((HIDDEN_NODES[1], HIDDEN_NODES[2]), stddev=1\\. / np.sqrt(HIDDEN_NODES[1])))\noutput_weights = tf.Variable(tf.truncated_normal((HIDDEN_NODES[-1], OUTPUT_NODES), stddev=1\\. / np.sqrt(OUTPUT_NODES)))\n```", "```py\nhidden_layer_1 = tf.nn.relu(\n    tf.matmul(input_placeholder, hidden_weights_1) +\n tf.Variable(tf.constant(0.01, shape=(HIDDEN_NODES[0],))))\n```", "```py\nhidden_layer_2 = tf.nn.relu(\ntf.matmul(hidden_layer_1, hidden_weights_2) + \ntf.Variable(tf.truncated_normal((HIDDEN_NODES[1],), stddev=0.001)))\nhidden_layer_3 = tf.nn.relu(\ntf.matmul(hidden_layer_2, hidden_weights_3) + tf.Variable(tf.truncated_normal((HIDDEN_NODES[2],), stddev=0.001)))\noutput_layer = tf.nn.softmax(tf.matmul(hidden_layer_3, output_weights) + tf.Variable(tf.truncated_normal((OUTPUT_NODES,), stddev=0.001)))\n```", "```py\nreward_placeholder = tf.placeholder(\"float\", shape=(None,))\nactual_move_placeholder = tf.placeholder(\"float\", shape=(None, OUTPUT_NODES))\n```", "```py\npolicy_gradient = tf.reduce_sum(\n    tf.reshape(reward_placeholder, (-1, 1)) * \nactual_move_placeholder * output_layer)\n\ntrain_step = tf.train.RMSPropOptimizer(LEARN_RATE).minimize(-policy_gradient)\n```", "```py\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n```", "```py\nboard_states, actual_moves, rewards = [], [], []\n\ndef make_move(board_state):\n    board_state_flat = np.ravel(board_state)\n    board_states.append(board_state_flat)\n probability_of_actions = sess.run(output_layer, feed_dict={input_placeholder: [board_state_flat]})[0]\n```", "```py\ntry:\n        move = np.random.multinomial(1, probability_of_actions)\nexcept ValueError:\n        move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-7))\n```", "```py\n    actual_moves.append(move)\n\n    move_index = move.argmax()\n    return move_index / 3, move_index % 3\n```", "```py\nBATCH_SIZE = 100\nepisode_number = 1\n```", "```py\nwhile True:\n    reward = play_game(make_move, random_player)\n```", "```py\n\nlast_game_length = len(board_states) - len(rewards)\n\n    # we scale here \n    reward /= float(last_game_length)\n\nrewards += ([reward] * last_game_length)\n```", "```py\n\nepisode_number += 1\n\n    if episode_number % BATCH_SIZE == 0:\n        normalized_rewards = rewards - np.mean(rewards)\n        normalized_rewards /= np.std(normalized_rewards)\n\n        sess.run(train_step, feed_dict={input_placeholder: board_states, reward_placeholder: normalized_rewards, actual_move_placeholder: actual_moves})\n```", "```py\n        del board_states[:]\n        del actual_moves[:]\n        del rewards[:]\n```", "```py\nmove = random.choice(list(move_states.keys()))\n\n```", "```py\nprobability_of_move = fast_rollout_policy.run(board_state)\nmove = np.random.binomial(1, probability_of_move)\n```"]