- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Downstream NLP Tasks with Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用转换器的下游 NLP 任务
- en: Transformers reveal their full potential when we unleash pretrained models and
    watch them perform downstream **Natural Language Understanding** (**NLU**) tasks.
    It takes a lot of time and effort to pretrain and fine-tune a transformer model,
    but the effort is worthwhile when we see a multi-million parameter transformer
    model in action on a range of NLU tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们释放预训练模型并观察它们在下游**自然语言理解**（**NLU**）任务中表现时，转换器展现出其全部潜力。对一个庞大的参数转换器模型进行预训练和微调需要大量时间和精力，但当我们看到一个模型在一系列
    NLU 任务中发挥作用时，这些努力是值得的。
- en: We will begin this chapter with the quest of outperforming the human baseline.
    The human baseline represents the performance of humans on an NLU task. Humans
    learn transduction at an early age and quickly develop inductive thinking. We
    humans perceive the world directly with our senses. Machine intelligence relies
    entirely on our perceptions transcribed into words to make sense of our language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从超越人类基线的探索开始这一章。人类基线代表人类在NLU任务上的表现。人类在幼年时学会了转导，并迅速发展出归纳思维。我们人类通过感官直接感知世界。机器智能完全依赖我们的感知被转录为文字来理解我们的语言。
- en: We will then see how to measure the performance of transformers. Measuring **Natural
    Language Processing** (**NLP**) tasks remains a straightforward approach involving
    accuracy scores in various forms based on true and false results. These results
    are obtained through benchmark tasks and datasets. SuperGLUE, for example, is
    a wonderful example of how Google DeepMind, Facebook AI, the University of New
    York, the University of Washington, and others worked together to set high standards
    to measure NLP performances.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后将看到如何衡量转换器的性能。衡量**自然语言处理**（**NLP**）任务仍然是一种直观的方法，涉及基于真假结果的各种形式的准确性得分。这些结果是通过基准任务和数据集获得的。例如，SuperGLUE是一个很好的例子，展示了谷歌DeepMind、Facebook
    AI、纽约大学、华盛顿大学等共同努力设定了用于衡量NLP性能的高标准。
- en: Finally, we will explore several downstream tasks, such as the **Standard Sentiment
    TreeBank** (**SST-2**), linguistic acceptability, and Winograd schemas.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探索几个下游任务，如**标准情感树库**（**SST-2**）、语言可接受性和温格拉德模式。
- en: Transformers are rapidly taking NLP to the next level by outperforming other
    models on well-designed benchmark tasks. Alternative transformer architectures
    will continue to emerge and evolve.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器正在通过在设计良好的基准任务上胜过其他模型来迅速将NLP推向下一级。替代的转换器体系结构将继续出现和演化。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Machine versus human intelligence for transduction and induction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器与人类智能的转导和归纳比较
- en: The NLP transduction and induction process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理的转导和归纳过程
- en: Measuring transformer performance versus Human Baselines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量转换器性能与人类基线
- en: Measurement methods (Accuracy, F1-score, and MCC)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量方法（准确性、F1 分数和MCC）
- en: Benchmark tasks and datasets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准任务和数据集
- en: SuperGLUE downstream tasks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SuperGLUE 下游任务
- en: Linguistic acceptability with CoLA
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CoLA 进行语言可接受性评估
- en: Sentiment analysis with SST-2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SST-2 进行情感分析
- en: Winograd schemas
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温格拉德模式
- en: Let’s start by understanding how humans and machines represent language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解人类和机器如何表示语言。
- en: Transduction and the inductive inheritance of transformers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转导和变压器的归纳继承
- en: The emergence of **Automated Machine Learning** (**AutoML**), meaning APIs in
    automated cloud AI platforms, has deeply changed the job description of every
    AI specialist. Google Vertex, for example, boasts a reduction of 80% of the development
    required to implement ML. This suggests that anybody can implement ML with ready-to-use
    systems. Does that mean an 80% reduction of the workforce of developers? I don’t
    think so. I see an Industry 4.0 AI specialist assemble AI with added value to
    a project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动机器学习**（**AutoML**）的出现，意味着自动化云AI平台中的API，已经深刻改变了每个AI专家的工作描述。例如，Google Vertex
    宣称实现 ML 所需的开发量减少了 80%。这表明任何人都可以使用现成的系统实现 ML。这是否意味着开发人员的工作减少了 80%？我认为不是。我认为工业4.0
    AI 专家会组装具有附加价值的AI项目。'
- en: Industry 4.0\. NLP AI specialists invest less in source code and more in knowledge
    to become the AI guru of a team.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 工业4.0。NLP AI 专家更多地投资于知识而不是源代码，以成为团队中的AI专家。
- en: Transformers possess the unique ability to apply their knowledge to tasks they
    did not learn. A BERT transformer, for example, acquires language through sequence-to-sequence
    and masked language modeling. The BERT transformer can then be fine-tuned to perform
    downstream tasks that it did not learn from scratch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器拥有独特的能力，可以将它们的知识应用于它们没有学习过的任务。例如，BERT 变压器通过序列到序列和掩码语言建模获得语言。然后可以对 BERT 变压器进行微调，以执行它从头开始没有学习的下游任务。
- en: In this section, we will do a mind experiment. We will use the graph of a transformer
    to represent how humans and machines make sense of information using language.
    Machines make sense of information in a different way from humans but reach very
    efficient results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行一次心灵实验。我们将使用变压器的图表来表示人类和机器如何使用语言理解信息。机器以与人类不同的方式理解信息，但达到了非常高效的结果。
- en: '*Figure 5.1*, a mind experiment designed with transformer architecture layers
    and sublayers, shows the deceptive similarity between humans and machines. Let’s
    study the learning process of transformer models to understand downstream tasks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.1*，一个使用变压器架构层和子层设计的心灵实验，显示了人类和机器之间的欺骗性相似性。让我们研究变压器模型的学习过程，以了解下游任务：'
- en: '![](img/B17948_05_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_01.png)'
- en: 'Figure 5.1: Human and ML methods'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.1: 人类和 ML 方法'
- en: For our example, `N=2`. This conceptual representation has two layers. The two
    layers show that humans build on accumulated knowledge from generation to generation.
    Machines only process what we give them. Machines use our outputs as inputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，`N=2`。这个概念性表示有两层。这两层显示了人类从一代到下一代积累的知识。机器只处理我们给它们的东西。机器将我们的输出用作输入。
- en: The human intelligence stack
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类智能栈
- en: On the left side of *Figure 5.1*, we can see that the input for humans is the
    perception of raw events for layer 0, and the output is language. We first perceive
    events with our senses as children. Gradually, the output becomes burbling language
    and then structured language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.1* 的左侧，我们可以看到人类的输入是层 0 的原始事件的感知，输出是语言。我们首先作为孩子用感官感知事件。逐渐地，输出变成了潺潺的语言，然后是结构化的语言。
- en: For humans, *transduction* goes through a trial-and-error process. Transduction
    means that we take structures we perceive and represent them with patterns, for
    example. We make representations of the world that we apply to our inductive thinking.
    Our inductive thinking relies on the quality of our transductions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，*转导* 经历了一个反复试错的过程。转导意味着我们将我们感知到的结构并用模式来表示它们，例如。我们制作了我们应用于归纳思维的世界的表示。我们的归纳思维依赖于我们转导的质量。
- en: For example, as children, we were often forced to take a nap early in the afternoon.
    Famous child psychologist Piaget found that this could lead to some children saying,
    for example, “I haven’t taken a nap, so it’s not the afternoon.” The child sees
    two events, creates a link between them with transduction, and then makes an inference
    to generalize and make an induction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作为孩子，我们经常被迫在下午早些时候午睡。著名的儿童心理学家皮亚杰发现，这可能导致一些孩子说，例如，“我没有午睡，所以现在还不是下午。”孩子看到两个事件，用转导之间创建了链接，然后进行推理概括和归纳。
- en: 'At first, humans notice these patterns through transduction and generalize
    them through *inductions*. We are trained by trial and error to understand that
    many events are related:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，人类通过转导注意到这些模式，并通过 *归纳* 将它们概括。我们通过反复试错来训练，以理解许多事件是相关的：
- en: '*Trained_related events = {sunrise – light, sunset – dark, dark clouds – rain,
    blue sky – running, food – good, fire – warm, snow – cold}*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*已训练的相关事件 = {日出 – 光，日落 – 黑暗，乌云 – 雨，蓝天 – 跑步，食物 – 好，火 – 温暖，雪 – 寒冷}*'
- en: 'Over time, we are trained to understand millions of related events. New generations
    of humans did not have to start from scratch. They were only *fine-tuned for many
    tasks by previous generations*. They were taught that “fire burns you,” for example.
    From then on, a child knew that this knowledge could be fine-tuned to any form
    of “fire”: candles, wildfires, volcanoes, and every instance of “fire.”'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们被训练去理解数百万相关事件。新一代人类不必从头开始。他们只是由上一代对许多任务进行了 *微调*。例如，他们被教导“火会烧伤你”。从那时起，一个孩子就知道这个知识可以被微调为任何形式的“火”：蜡烛、森林火灾、火山和每一种“火”的实例。
- en: Finally, humans transcribed everything they knew, imagined, or predicted into
    written *language*. The output of layer 0 was born.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，人类将他们所知道、想象到的或预测到的一切都记录到书面 *语言* 中。第 0 层的输出诞生了。
- en: For humans, the input of the next layer, layer 1, is the vast amount of trained
    and fine-tuned knowledge. On top of that, humans perceive massive amounts of events
    that then go through the transduction, induction, training, and fine-tuning of
    sublayers, along with previous transcribed knowledge.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，下一层（第 1 层）的输入是大量经过训练和微调的知识。此外，人类感知大量事件，然后通过转导、归纳、训练和微调子层以及先前的转录知识。
- en: Many of these events stem from odors, emotions, situations, experiences, and
    everything that makes a human unique. Machines do not have access to this individual
    identity. Humans have a personal perception of a word in a homogeneous way specific
    for each individual.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事件很多源自气味、情绪、情境、经历，以及构成人类独特性的一切。机器无法获取这种个体身份。人类对单词有一种个人的感知方式，对每个人来说都是特定的。
- en: A machine takes what we give it through masses of heterogeneous unfiltered impersonal
    data. The goal of a machine is to perform an impersonal, efficient task. The goal
    of a human being is personal wellbeing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器接收我们通过大量异质未经过滤的非个人数据给予它的内容。机器的目标是执行非个人的高效任务。人类的目标是个人的福祉。
- en: Our infinite approach loop goes from layer 0 to layer 1 and back to layer 0
    with more raw and processed information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的无限方法循环从第 0 层到第 1 层，然后回到第 0 层，带着更多的原始和处理后的信息。
- en: The result is fascinating! We do not need to learn (train on) our native language
    from scratch to acquire summarization abilities. We use our pretrained knowledge
    to adjust (fine-tune) to summarization tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人着迷！我们不需要从头学习（训练）我们的母语就能获得摘要能力。我们利用预先训练的知识来调整（微调）以适应摘要任务。
- en: Transformers go through the same process but in a different way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器通过不同的方式经历相同的过程。
- en: The machine intelligence stack
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器智能栈
- en: Machines learn basic tasks like the ones described in this chapter, and then
    perform hundreds of tasks using the sequences they learned how to predict.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习基本任务，就像本章描述的任务一样，然后使用它们学习如何预测的序列来执行数百个任务。
- en: On the right side of *Figure 5.1*, we can see that the input for machines is
    second-hand information in the form of language. *Our output is the only input
    machines have to analyze language*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.1*的右侧，我们可以看到机器的输入是语言形式的二手信息。*我们的输出是机器分析语言的唯一输入*。
- en: At this point in human and machine history, computer vision identifies images
    but does not contain the grammatical structure of language. Speech recognition
    converts sound into words, which brings us back to written language. Music pattern
    recognition cannot lead to objective concepts expressed in words.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类和机器历史的这一阶段，计算机视觉识别图像，但没有包含语言的语法结构。语音识别将声音转换成单词，这将我们带回书面语言。音乐模式识别不能导致用单词表达的客观概念。
- en: 'Machines start with a handicap. We impose an artificial disadvantage on them.
    Machines must rely on our random quality language outputs to:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 机器起步有点吃力。我们对它们施加了人为的劣势。机器必须依赖我们的随机质量语言输出来：
- en: Perform transductions connecting all the tokens (subwords) that occur together
    in language sequences
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行连接语言序列中所有一起出现的令牌（子词）的转导
- en: Build inductions from these transductions
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这些转导中建立归纳
- en: Train those inductions based on tokens to produce patterns of tokens
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于令牌训练这些归纳，以产生令牌的模式
- en: 'Let’s stop at this point and peek into the process of the attention sublayer,
    which works hard to produce valid inductions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这一点上停下来，窥探一下注意子层的过程，它努力产生有效的归纳：
- en: The transformer model excluded the former recurrence-based learning operations
    and used self-attention to heighten the vision of the model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器模型排除了以前基于循环的学习操作，并使用自注意力来提高模型的视野
- en: 'Attention sublayers have an advantage over humans at this point: they can process
    millions of examples for their inductive thinking operations'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意子层在这一点上比人类有优势：它们可以处理数百万个示例来进行归纳思维操作
- en: Like us, they find patterns in sequences through transduction and induction
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像我们一样，它们通过转导和归纳在序列中找到模式
- en: They memorize these patterns using parameters that are stored with their model
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们使用存储在模型中的参数来记忆这些模式。
- en: 'They have acquired language understanding by using their abilities: substantial
    data volumes, excellent NLP transformer algorithms, and computer power. *Thanks
    to their deep understanding of language, they are ready to run hundreds of tasks
    they were not trained for*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用其能力：大量数据、优秀的NLP变压器算法和计算能力，它们已经获得了语言理解。*由于它们对语言的深刻理解，它们已经准备好运行数百个它们未经过训练的任务*。
- en: Transformers, like humans, acquire language understanding through a limited
    number of tasks. Like us, they detect connections through transduction and then
    generalize them through inductive operations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 像人类一样，变压器通过有限数量的任务来获得语言理解。像我们一样，它们通过转导检测到连接，然后通过归纳操作对其进行概括。
- en: When the transformer model reaches the fine-tuning sub-layer of machine intelligence,
    it reacts like us. It does not start training from scratch to perform a new task.
    Like us, it considers it as a downstream task that only requires fine-tuning.
    If it needs to learn how to answer a question, it does not start learning a language
    from scratch. A transformer model just fine-tunes its parameters like us.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当变压器模型达到机器智能的微调子层时，它的反应就像我们一样。它不是从头开始训练以执行新任务。像我们一样，它认为这只是一个只需要微调的下游任务。如果它需要学习如何回答问题，它不会从头开始学习一种语言。变压器模型只是像我们一样微调其参数。
- en: In this section, we saw that transformer models struggle to learn in the way
    we do. They start with a handicap from the moment they rely on our perceptions
    transcribed into language. However, they have access to infinitely more data than
    we do with massive computing power.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到变压器模型在学习方式上存在困难。一旦它们依赖于我们用语言转录的感知，它们就从一开始就处于劣势。然而，它们可以利用无限多的数据和大量的计算能力。
- en: Let’s now see how to measure transformer performances versus Human Baselines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何衡量变压器性能与人类基线。
- en: Transformer performances versus Human Baselines
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器性能与人类基线
- en: Transformers, like humans, can be fine-tuned to perform downstream tasks by
    inheriting the properties of a pretrained model. The pretrained model provides
    its architecture and language representations through its parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 像人类一样，变压器可以通过继承预训练模型的属性来进行微调，以执行下游任务。预训练模型通过其参数提供其架构和语言表示。
- en: A pretrained model trains on key tasks to acquire a general knowledge of the
    language. A fine-tuned model trains on downstream tasks. Not every transformer
    model uses the same tasks for pretraining. Potentially, all tasks can be pretrained
    or fine-tuned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型在关键任务上训练，以获得对语言的一般知识。微调模型在下游任务上训练。并非每个变压器模型都使用相同的任务进行预训练。潜在地，所有任务都可以预先训练或微调。
- en: Every NLP model needs to be evaluated with a standard method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个NLP模型都需要使用标准方法进行评估。
- en: This section will first go through some of the key measurement methods. Then,
    we will go through some of the main benchmark tasks and datasets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先将介绍一些关键的测量方法。然后，我们将介绍一些主要的基准任务和数据集。
- en: Let’s start by going through some of the key metric methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些关键的度量方法开始。
- en: Evaluating models with metrics
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用度量标准评估模型
- en: It is impossible to compare one transformer model to another transformer model
    (or any other NLP model) without a universal measurement system that uses metrics.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 没有使用度量标准的通用测量系统，不可能将一个变压器模型与另一个变压器模型（或任何其他NLP模型）进行比较。
- en: In this section, we will analyze three measurement scoring methods that are
    used by GLUE and SuperGLUE.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析GLUE和SuperGLUE使用的三种测量评分方法。
- en: Accuracy score
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确度得分
- en: 'The accuracy score, in whatever variant you use, is a practical evaluation.
    The score function calculates a straightforward true or false value for each result.
    Either the model’s outputs, ![](img/B17948_05_001.png), match the correct predictions,
    ![](img/B17948_05_002.png), for a given subset, *samples*[i], of a set of samples
    or not. The basic function is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用的是哪种变体，准确度得分都是一个实用的评估标准。得分函数为给定子集的每个结果计算一个简单的真值或假值。要么模型的输出，![](img/B17948_05_001.png)，与给定样本集的正确预测，![](img/B17948_05_002.png)，匹配，要么不匹配。基本函数为：
- en: '![](img/B17948_05_08.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_08.png)'
- en: We will obtain `1` if the result for the subset is correct and `0` if it is
    false.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果子集的结果是正确的，则我们将获得`1`，如果是错误的，则为`0`。
- en: Let’s now examine the more flexible F1-score.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看更灵活的F1分数。
- en: F1-score
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F1分数
- en: The F1-score introduces a more flexible approach that can help when faced with
    datasets containing uneven class distributions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数引入了一种更灵活的方法，可以在面对包含不均匀类分布的数据集时提供帮助。
- en: 'The F1-score uses the weighted values of precision and recall. It is a weighted
    average of precision and recall values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数使用精确率和召回率的加权值。它是精确率和召回率值的加权平均值：
- en: '*F1score= 2** (*precision * recall*)/(*precision + recall*)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1score= 2** (*precision * recall*)/(*precision + recall*)'
- en: 'In this equation, true (*T*) positives (*p*), false (*F*) positives (*p*),
    and false (*F*) negatives (*n*) are plugged into the precision (*P*) and recall
    (*R*) equations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，真正例（*p*）、假正例（*p*）和假负例（*n*）被放入精确率（*P*）和召回率（*R*）的方程中：
- en: '![](img/B17948_05_09.png)![](img/B17948_05_10.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_09.png)![](img/B17948_05_10.png)'
- en: 'The F1-score can thus be viewed as the harmonic mean (reciprocal of the arithmetic
    mean) of precision (*P*) and recall (*R*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，F1 分数可以视为精确率（*P*）和召回率（*R*）的调和平均值（算术平均值的倒数）：
- en: '![](img/B17948_05_003.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_003.png)'
- en: Let’s now review the MCC approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下 MCC 方法。
- en: Matthews Correlation Coefficient (MCC)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matthews 相关系数（MCC）
- en: MCC was described and implemented in the *Evaluating using Matthews Correlation
    Coefficient* section in *Chapter 3*, *Fine-Tuning BERT Models*. MCC computes a
    measurement with true positives (*T*[P]), true negatives (*T*[N]), false positives
    (*F*[P]), and false negatives (*F*[N]).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 在 *Fine-Tuning BERT Models* 章节的 *Evaluating using Matthews Correlation Coefficient*
    部分进行了描述和实现。MCC 计算出一个值，其中包括真正例（*T*[P]）、真负例（*T*[N]）、假正例（*F*[P]）和假负例（*F*[N]）。
- en: 'The MCC can be summarized by the following equation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 可以用以下方程总结：
- en: '![](img/B17948_05_11.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_11.png)'
- en: MCC provides an excellent metric for binary classification models, even if the
    sizes of the classes are different.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 为二元分类模型提供了一个出色的度量标准，即使类别的大小不同也可以。
- en: We now have a good idea of how to measure a given transformer model’s results
    and compare them to other transformer models or NLP models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地了解了如何衡量给定 transformer 模型的结果，并将其与其他 transformer 模型或 NLP 模型进行比较。
- en: With measurement scoring methods in mind, let’s now look into benchmark tasks
    and datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有了测量评分方法的考虑，现在让我们来看看基准任务和数据集。
- en: Benchmark tasks and datasets
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准任务和数据集
- en: 'Three prerequisites are required to prove that transformers have reached state-of-the-art
    performance levels:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个前提条件需要证明变压器已经达到了最先进的性能水平：
- en: A model
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型
- en: A dataset-driven task
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个以数据集驱动的任务
- en: A metric as described in the *Evaluating models with metrics* section of this
    chapter
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章的 *Evaluating models with metrics* 部分描述的度量标准
- en: We will begin by exploring the SuperGLUE benchmark to illustrate the evaluation
    process of a transformer model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从探索 SuperGLUE 基准开始，以说明一个 transformer 模型的评估过程。
- en: From GLUE to SuperGLUE
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 GLUE 到 SuperGLUE
- en: The SuperGLUE benchmark was designed and made public by *Wang* et al. (2019).
    *Wang* et al. (2019) first designed the **General Language Understanding Evaluation**
    (**GLUE**) benchmark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE 基准是由 *Wang* 等人（2019 年）设计并公开的。*Wang* 等人（2019 年）首次设计了 **General Language
    Understanding Evaluation** (**GLUE**) 基准。
- en: The motivation of the GLUE benchmark was to show that to be useful, NLU has
    to be applicable to a wide range of tasks. Relatively small GLUE datasets were
    designed to encourage an NLU model to solve a set of tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE 基准的动机是要表明，为了有用，NLU 必须适用于广泛的任务。相对较小的 GLUE 数据集旨在鼓励 NLU 模型解决一系列任务。
- en: However, the performance of NLU models, boosted by the arrival of transformers,
    began to exceed the level of the average human, as we can see in the GLUE leaderboard
    (December 2021). The GLUE leaderboard, available at [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard),
    shows a remarkable display of NLU talent, retaining some of the former RNN/CNN
    ideas while mainly focusing on the ground-breaking transformer models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着变压器的到来，NLU 模型的性能开始超越平均人类水平，正如我们可以在 GLUE 排行榜（2021 年 12 月）中看到的。GLUE 排行榜可在
    [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)
    查看，展示了 NLU 天才的显著表现，保留了一些以前的 RNN/CNN 思想，同时主要关注突破性的变压器模型。
- en: 'The following excerpt of the leaderboard shows the top leaders and the position
    of GLUE’s Human Baselines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 排行榜的下面摘录显示了顶尖领导者和 GLUE 人类基线的位置：
- en: '![](img/B17948_05_02.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_02.png)'
- en: 'Figure 5.2: GLUE Leaderboard – December 2021'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：GLUE 排行榜 - 2021 年 12 月
- en: New models and the Human Baselines ranking will constantly change. These rankings
    just give an idea of how far classical NLP and transformers have taken us!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型和人类基线的排名将不断变化。这些排名只是给出了经典 NLP 和变压器带领我们走向何方的一个概念！
- en: We first notice the GLUE Human Baselines are not in a top position, which shows
    that NLU models have surpassed non-expert humans on GLUE tasks. Human Baselines
    represent what we humans can achieve. AI can now outperform humans. In December
    2021, the Human Baselines are only in position 17\. This is a problem. Without
    a standard to beat, it is challenging to fish around for benchmark datasets to
    improve our models blindly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到 GLUE 人类基线并不处于顶级位置，这表明自然语言理解模型已经在 GLUE 任务上超越了非专业人类。人类基线代表了我们人类能够达到的水平。人工智能现在能够胜过人类。到了
    2021 年 12 月，人类基线仅排在第 17 位。这是一个问题。如果没有一个要超越的标准，就很难盲目地寻找基准数据集来改进我们的模型。
- en: We also notice that transformer models have taken the lead.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到 transformer 模型已经领先。
- en: I like to think of GLUE and SuperGLUE as the point when words go from chaos
    to order with language understanding. For me, understanding is the glue that makes
    words fit together and become a language.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将 GLUE 和 SuperGLUE 看作是语言理解从混乱到有序的转折点。对我来说，理解是让词语彼此契合并形成一种语言的粘合剂。
- en: The GLUE leaderboard will continuously evolve as NLU progresses. However, *Wang*
    et al. (2019) introduced SuperGLUE to set a higher standard for Human Baselines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言理解的进步，GLUE 排行榜将不断发展。然而，*Wang* 等人（2019）引入 SuperGLUE 是为了为人类基线设立更高的标准。
- en: Introducing higher Human Baselines standards
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入更高的人类基线标准
- en: '*Wang* et al. (2019) recognized the limits of GLUE. They designed SuperGLUE
    for more difficult NLU tasks.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Wang* 等人（2019）认识到了 GLUE 的局限性。他们为更困难的自然语言理解任务设计了 SuperGLUE。'
- en: 'SuperGLUE immediately re-established Human Baselines at rank #1 (December 2020),
    as shown in the following excerpt of the leaderboard, [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE 立即将人类基线重新确定为第一名（2020 年 12 月），如下面排行榜摘录所示，[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)：
- en: '![](img/B17948_05_03.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_03.png)'
- en: 'Figure 5.3: SuperGLUE Leaderboard 2.0 – December 2020'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：SuperGLUE 排行榜 2.0 – 2020 年 12 月
- en: 'However, the SuperGLUE leaderboard evolved as we produced better NLU models.
    In 2021, transformers had already surpassed Human Baselines. In December 2021,
    Human Baselines have gone down to rank #5, as shown in *Figure 5.4*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着我们生产出更好的自然语言理解（NLU）模型，SuperGLUE 排行榜也在发展。在 2021 年，transformers 已经超过了人类基线。到了
    2021 年 12 月，人类基线已经下降到第 5 位，如 *图 5.4* 所示：
- en: '![](img/B17948_05_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_04.png)'
- en: 'Figure 5.4: SuperGLUE Leaderboard 2.0 – December 2021'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：SuperGLUE 排行榜 2.0 – 2021 年 12 月
- en: AI algorithm rankings will constantly change as new innovative models arrive.
    These rankings just give an idea of how hard the battle for NLP supremacy is being
    fought!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新的创新模型的到来，人工智能算法排名将不断变化。这些排名只是给出了自然语言处理至高无上的战斗有多么艰难的一个概念！
- en: Let’s now see how the evaluation process works.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看评估过程是如何工作的。
- en: The SuperGLUE evaluation process
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SuperGLUE 评估过程
- en: Wang et al. (2019) selected eight tasks for their SuperGLUE benchmark. The selection
    criteria for these tasks were stricter than for GLUE. For example, the tasks had
    to not only understand texts but also to reason. The level of reasoning is not
    that of a top human expert. However, the level of performance is sufficient to
    replace many human tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人（2019）为他们的 SuperGLUE 基准选择了八个任务。这些任务的选择标准比 GLUE 更严格。例如，这些任务不仅要理解文本，还要进行推理。推理的水平不是顶级专家的水平。然而，性能水平已足以替代许多人类任务。
- en: 'The eight SuperGLUE tasks are presented in a ready-to-use list:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这八个 SuperGLUE 任务被呈现在一个现成的列表中：
- en: '![](img/B17948_05_05.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_05.png)'
- en: 'Figure 5.5: SuperGLUE tasks'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：SuperGLUE 任务
- en: 'The task list is interactive: [https://super.gluebenchmark.com/tasks](https://super.gluebenchmark.com/tasks).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 任务列表是交互式的：[https://super.gluebenchmark.com/tasks](https://super.gluebenchmark.com/tasks)。
- en: 'Each task contains links to the required information to perform that task:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务都包含执行该任务所需信息的链接：
- en: '**Name** is the name of the downstream task of a fine-tuned, pretrained model'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名称** 是一个微调、预训练模型的下游任务的名称'
- en: '**Identifier** is the abbreviation or short version of the name'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标识符** 是名称的缩写或简称'
- en: '**Download** is the download link to the datasets'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载** 是数据集的下载链接'
- en: '**More Info** offers greater detail through a link to the paper or website
    of the team that designed the dataset-driven task(s)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多信息** 通过链接到设计数据集驱动任务的团队的论文或网站提供更多细节'
- en: '**Metric** is the measurement score used to evaluate the model'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量** 是用于评估模型的测量分数'
- en: 'SuperGLUE provides the task instructions, the software, the datasets, and papers
    or websites describing the problem to be solved. Once a team runs the benchmark
    tasks and reaches the leaderboard, the results are displayed:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE提供任务说明、软件、数据集以及描述要解决问题的论文或网站。一旦团队运行基准任务并达到排行榜，结果将显示如下：
- en: '![](img/B17948_05_06.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_06.png)'
- en: 'Figure 5.6: SuperGLUE task scores'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：SuperGLUE任务分数
- en: SuperGLUE displays the overall score and the score for each task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE显示了整体评分以及每个任务的评分。
- en: For example, let’s take the instructions *Wang* et al. (2019) provided for the
    **Choice of Plausible Answers** (**COPA**) task in *Table 6* of their paper.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看*王*等人(2019年)在他们的论文的*表6*中为**选择合理答案**(**COPA**)任务提供的说明。
- en: The first step is to read the remarkable paper written by *Roemmele* et al.
    (2011). In a nutshell, the goal is for the NLU model to demonstrate its machine
    thinking (not human thinking, of course) potential. In our case, the transformer
    must choose the most plausible answer to a question. The dataset provides a premise,
    and the transformer model must find the most plausible answer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是阅读由*Roemmele*等人(2011年)撰写的杰出论文。简言之，目标是让NLU模型展示它的机器思考能力（当然不是人类思考）。在我们的案例中，transformer必须选择最合理的答案来回答问题。数据集提供前提，transformer模型必须找出最合理的答案。
- en: 'For example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '`Premise: I knocked on my neighbor''s door.`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`前提：我敲了敲邻居的门。`'
- en: '`What happened as a result?`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`结果是什么？`'
- en: '`Alternative 1: My neighbor invited me in.`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`另一种选择：我的邻居邀请我进去了。`'
- en: '`Alternative 2: My neighbor left his house.`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`另一种选择：我的邻居离开了他的房子。`'
- en: This question requires a second or two for a human to answer, which shows that
    it requires some commonsense machine thinking. `COPA.zip`, a ready-to-use dataset,
    can be downloaded directly from the SuperGLUE task page. The metric provided makes
    the process equal and reliable for all participants in the benchmark race.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题需要人类花上一两秒钟来回答，这表明它需要一些常识机器思考。`COPA.zip`是一个可以直接从SuperGLUE任务页面下载的现成数据集，所提供的度量使得这个过程对于所有参与基准竞赛的人员都是公平和可靠的。
- en: 'The examples might seem difficult. However, transformers are nearing COPA Human
    Baselines (*Figure 5.7*), which is only at rank #5 if we take all of the tasks
    into account:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子可能看起来很困难。然而，如果我们考虑所有的任务，transformers接近了COPA人类基准线(*图5.7*)，它只排在第5名：
- en: '![](img/B17948_05_07.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_07.png)'
- en: 'Figure 5.7: SuperGLUE results for COPA'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：COPA的SuperGLUE结果
- en: As incredible as it seems, transformers climbed the leaderboard ladder in a
    very short time! And this is just the beginning. New ideas are emerging nearly
    every month!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看上去难以置信，transformers在很短的时间内就攀登到了排行榜梯子上！而且这只是一个开始。新的想法几乎每个月都在出现！
- en: We have introduced COPA. Let’s define the seven other SuperGLUE benchmark tasks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了COPA。让我们定义另外七个SuperGLUE基准任务。
- en: Defining the SuperGLUE benchmark tasks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义SuperGLUE基准任务
- en: A task can be a pretraining task to generate a trained model. That same task
    can be a downstream task for another model that will fine-tune it. However, the
    goal of SuperGLUE is to show that a given NLU model can perform multiple downstream
    tasks with fine-tuning. Multi-task models are the ones that prove the thinking
    power of transformers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个任务可以是一个用于生成训练模型的预训练任务。同一个任务可以是另一个模型的下游任务，该模型将对其进行微调。然而，SuperGLUE的目标是展示给定的NLU模型可以执行多个下游任务并进行微调。多任务模型证明了transformers的思考能力。
- en: The power of any transformer resides in its ability to perform multiple tasks
    using a pretrained model and then applying it to fine-tuned downstream tasks.
    The original Transformer model and its variants now lead in all the GLUE and SuperGLUE
    tasks. We will continue to focus on SuperGLUE downstream tasks for which Human
    Baselines is tough to beat.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 任何transformer的力量都在于它能够使用预训练模型执行多个任务，然后将其应用于微调的下游任务。原始Transformer模型及其变体现在所有GLUE和SuperGLUE任务中处于领先地位。我们将继续关注SuperGLUE下游任务，而人类基准线很难击败。
- en: In the previous section, we went through COPA. In this section, we will go through
    the seven other tasks defined by *Wang* et al. (2019) in *Table 2* of their paper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们介绍了COPA。在本节中，我们将介绍*王*等人(2019年)在其论文的*表2*中定义的其他七个任务。
- en: Let’s continue with a Boolean question task.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行一个布尔问题任务。
- en: BoolQ
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BoolQ
- en: 'BoolQ is a Boolean yes-or-no answer task. The dataset, as defined on SuperGLUE,
    contains 15,942 naturally occurring examples. A raw sample of line `#3` of the
    `train.jsonl` dataset contains a passage, a question, and the answer (`true`):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ是一个布尔型的是或否回答任务。如在SuperGLUE中所定义的，数据集包含15,942个自然发生的例子。`train.jsonl` 数据集的第3行原始样本包含一段文章、一个问题和答案（`true`）：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The datasets provided may change in time, but the concepts remain the same.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的数据集可能会随时间改变，但概念保持不变。
- en: Now, let’s examine CB, a task that requires both humans and machines to focus.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看CB，这是一个需要人类和机器共同关注的任务。
- en: Commitment Bank (CB)
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Commitment Bank（CB）
- en: '**Commitment Bank** (**CB**) is a difficult *entailment* task. We are asking
    the transformer model to read a *premise*, then examine a *hypothesis* built on
    the premise. For example, the hypothesis will confirm the premise or contradict
    it. Then the transformer model must *label* the hypothesis as *neutral*, an *entailment*,
    or a *contradiction* of the premise, for example.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**承诺银行**（**CB**）是一个困难的*蕴含*任务。我们要求变压器模型阅读一个*前提*，然后检验基于前提构建的一个*假设*。例如，假设会确认前提或与之矛盾。然后变压器模型必须将假设标记为*中性*、*蕴含*或*矛盾*，例如。'
- en: The dataset contains discourses, which are natural discourses.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含自然语篇。
- en: 'The following sample `#77`, taken from the `train.jsonl` training dataset,
    shows how difficult the CB task is:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中的 `#77`，取自 `train.jsonl` 训练数据集，展示了CB任务的难度：
- en: '[PRE1]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will now have a look at the multi-sentence problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在来看看多句问题。 '
- en: Multi-Sentence Reading Comprehension (MultiRC)
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多句阅读理解（MultiRC）
- en: '**Multi-Sentence Reading Comprehension** (**MultiRC**) asks the model to read
    a text and choose from several possible choices. The task is difficult for both
    humans and machines. The model is presented with a *text*, several *questions*,
    and possible *answers* to each question with a `0` (false) or `1` (true) *label*.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**多句阅读理解**（**MultiRC**）要求模型阅读一段文本，并从几个可能的选择中作出选择。这个任务对人类和机器来说都很困难。模型需要面对一个*文本*，多个*问题*，以及每个问题可能对应的*答案*，并带有
    `0`（错误）或 `1`（正确）的*标签*。'
- en: 'Let’s take the second sample in `train.jsonl`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看 `train.jsonl` 中的第二个样本：
- en: '[PRE2]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The sample contains four questions. To illustrate the task, we will just investigate
    two of them. The model must predict the correct labels. Notice how the information
    that the model is asked to obtain is distributed throughout the text:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 样本包含四个问题。为了说明这个任务，我们只研究其中两个。模型必须预测正确的标签。请注意，模型被要求获取的信息分布在整个文本中：
- en: '[PRE3]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, one can only admire the performance of a single fine-tuned, pretrained
    model on these difficult downstream tasks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，人们只能欣赏一个优秀的、经过精确调校的预训练模型在这些困难的下游任务上的表现。
- en: Now, let’s see the reading comprehension task.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看阅读理解任务。
- en: Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Reading Comprehension with Commonsense Reasoning Dataset（ReCoRD）
- en: '**Reading Comprehension with Commonsense Reasoning Dataset** (**ReCoRD**) represents
    another challenging task. The dataset contains over 120,000 queries from more
    than 70,000 news articles. The transformer must use common-sense reasoning to
    solve this problem.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用常识推理数据集**（**ReCoRD**）代表着另一个具有挑战性的任务。该数据集包含来自超过70,000篇新闻文章的120,000多个查询。变压器必须运用常识推理来解决这个问题。'
- en: 'Let’s examine a sample from `train.jsonl`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来查看 `train.jsonl` 中的一个样本：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The *entities* are indicated, as shown in the following excerpt:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体*如下摘录所示：'
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, the model must *answer* a *query* by finding the proper value for
    the *placeholder*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型必须通过找到适当的值来*回答*一个*查询*，填写*占位符*：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once the transformer model has gone through this problem, it must now face an
    entailment task.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦变压器模型经历了这个问题，它现在必须面对一个包含的任务。
- en: Recognizing Textual Entailment (RTE)
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本蕴含识别（RTE）
- en: For **Recognizing Textual Entailment** (**RTE**), the transformer model must
    read the *premise*, examine a *hypothesis*, and predict the *label* of the *entailment
    hypothesis status*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**文本蕴涵识别**（**RTE**），变压器模型必须阅读*前提*，检查一个*假设*，并预测*蕴含假设状态*的*标签*。
- en: 'Let’s examine sample `#19` of the `train.jsonl` dataset:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 `train.jsonl` 数据集的样本 `#19`：
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: RTE requires understanding and logic. Let’s now see the Words in Context task.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: RTE需要理解和逻辑推理。现在让我们来看Words in Context任务。
- en: Words in Context (WiC)
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Words in Context（WiC）
- en: '**Words in Context** (**WiC**) and the following Winograd task test a model’s
    ability to process an ambiguous word. In WiC, the multi-task transformer will
    have to analyze two sentences and determine whether the target word has the same
    meaning in both sentences.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文中的词语**（**WiC**）和以下的Winograd任务测试模型处理模棱两可的词语的能力。在WiC中，多任务transformer必须分析两个句子，并确定目标词在两个句子中是否具有相同的含义。'
- en: Let’s examine the first sample of the `train.jsonl` dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查`train.jsonl`数据集的第一个样本。
- en: 'First, the target word is specified:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先指定目标词：
- en: '[PRE8]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The model has to read two sentences containing the target word:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须读取包含目标词的两个句子：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`train.jsonl` specifies the sample index, the value of the label, and the position
    of the target word in `sentence1(start1, end1)` and `sentence2(start2, end2)`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.jsonl`指定了样本索引、标签的值以及`sentence1(start1, end1)`和`sentence2(start2, end2)`中目标词的位置：'
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After this daunting task, the transformer model has to face the Winograd task.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这个艰巨的任务后，transformer模型将面临Winograd任务。
- en: The Winograd schema challenge (WSC)
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Winograd模式挑战（WSC）
- en: The Winograd schema task is named after Terry Winograd. If a transformer is
    well trained, it should be able to solve disambiguation problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Winograd模式任务以Terry Winograd命名。如果一个transformer经过良好训练，它应该能够解决消歧问题。
- en: The dataset contains sentences that target slight differences in the gender
    of a pronoun.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含针对代词性别细微差异的句子。
- en: This constitutes a coreference resolution problem, which is one of the most
    challenging tasks to perform. However, the transformer architecture, which allows
    self-attention, is ideal for this task.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这构成了一个共指解析问题，这是执行的最具挑战性的任务之一。然而，允许自注意的transformer架构对于这个任务来说是理想的。
- en: Each sentence contains an *occupation*, a *participant*, and a *pronoun*. The
    problem to solve is to find whether the pronoun is *coreferent* with the occupation
    or the participant.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子包含一个*职业*、一个*参与者*和一个*代词*。要解决的问题是找出代词是否与职业或参与者*共指*。
- en: Let’s examine a sample taken from `train.jsonl`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从`train.jsonl`中取出的样本。
- en: 'First, the sample asks the model to read a *text*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，样本要求模型阅读一段*文本*：
- en: '[PRE11]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have gone through some of the main SuperGLUE tasks. There are many other
    tasks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了一些主要的SuperGLUE任务。还有许多其他任务。
- en: However, once you understand the architecture of transformers and the mechanism
    of the benchmark tasks, you will rapidly adapt to any model and benchmark.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦你了解transformer的架构和基准任务的机制，你将迅速适应任何模型和基准。
- en: Let’s now run some downstream tasks.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行一些下游任务。
- en: Running downstream tasks
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行下游任务
- en: In this section, we will just jump into some transformer cars and drive them
    around a bit to see what they do. There are many models and tasks. We will run
    a few of them in this section. Once you understand the process of running a few
    tasks, you will quickly understand all of them. *After all, the human baseline
    for all these tasks is us!*
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将随便选几辆transformer车，开一下看看它们是如何运行的。有许多模型和任务。我们将在本节运行其中几个。一旦你了解了运行几个任务的过程，你会很快理解所有任务。*毕竟，所有这些任务的人类基线就是我们自己！*
- en: A downstream task is a fine-tuned transformer task that inherits the model and
    parameters from a pretrained transformer model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下游任务是从预训练transformer模型继承模型和参数的微调transformer任务。
- en: A downstream task is thus the perspective of a pretrained model running fine-tuned
    tasks. That means, depending on the model, a task is downstream if it was not
    used to fully pretrain the model. In this section, we will consider all the tasks
    as downstream since we did not pretrain them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下游任务是一个从预训练模型运行微调任务的视角。这意味着，根据模型的不同，如果任务没有用于完全预训练该模型，则该任务是下游任务。在本节中，我们将考虑所有任务都是下游任务，因为我们没有预训练它们。
- en: Models will evolve, as will databases, benchmark methods, accuracy measurement
    methods, and leaderboard criteria. But the structure of human thought reflected
    through the downstream tasks in this chapter will remain.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将会发展，数据库也会发展，基准方法、准确性测量方法和排行榜标准也会发展。但是反映在本章的下游任务中的人类思维结构将会保持不变。
- en: Let’s start with CoLA.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从CoLA开始。
- en: The Corpus of Linguistic Acceptability (CoLA)
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言接受性语料库（CoLA）
- en: The **Corpus of Linguistic Acceptability** (**CoLA**), a GLUE task, [https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks),
    contains thousands of samples of English sentences annotated for grammatical acceptability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言接受性语料库** (**CoLA**)，一个GLUE任务，[https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks)，包含了标记为语法接受性的英语句子的成千上万个样本。'
- en: The goal of *Alex Warstadt* et al. (2019) was to evaluate the linguistic competence
    of an NLP model to judge the linguistic acceptability of a sentence. The NLP model
    is expected to classify the sentences accordingly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*Alex Warstadt*等人（2019）的目标是评估 NLP 模型对句子的语言能力，以判断句子的语言可接受性。NLP 模型应该按照预期对句子进行分类。'
- en: 'The sentences are labeled as grammatical or ungrammatical. The sentence is
    labeled `0` if the sentence is not grammatically acceptable. The sentence is labeled
    `1` if the sentence is grammatically acceptable. For example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 句子被标记为语法正确或不符合语法规范。如果句子不符合语法规范，则标记为 `0`。如果句子在语法上是正确的，则标记为 `1`。例如：
- en: Classification = `1` for ‘we yelled ourselves hoarse.’
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 分类 = `1` 代表 ‘我们喊得嘶哑了.’
- en: Classification = `0` for ‘we yelled ourselves.’
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 分类 = `0` 代表 ‘我们在喊.’
- en: 'You can go through `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb` in
    *Chapter 3*, *Fine-Tuning BERT Models*, to view the BERT model that we fine-tuned
    on CoLA datasets. We used CoLA data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *第三章*，*Fine-Tuning BERT Models* 中查看我们在 CoLA 数据集上对 BERT 模型进行微调的内容，打开 `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb`。我们使用了
    CoLA 数据：
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also load a pretrained BERT model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还加载了一个预训练的 BERT 模型：
- en: '[PRE13]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the measurement method, or metric, we used is MCC, which was described
    in the *Evaluating using Matthews Correlation Coefficient* section of *Chapter
    3*, *Fine-Tuning BERT Models*, and earlier in this chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用的度量方法是 MCC，在 *第三章*，*Fine-Tuning BERT Models* 的 *使用 Matthews 相关系数进行评估*
    部分和本章中有详细描述。
- en: You can refer to that section for the mathematical description of MCC and take
    the time to rerun the source code if necessary.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以参考该部分以获取 MCC 的数学描述，并有必要时重新运行源代码。
- en: A sentence can be grammatically unacceptable but still convey a sentiment. Sentiment
    analysis can add some form of empathy to a machine.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一句话在语法上可能不可接受，但仍然传达情绪。情感分析可以为机器增加某种程度的同理心。
- en: Stanford Sentiment TreeBank (SST-2)
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 斯坦福情感树库（**SST-2**）
- en: '**Stanford Sentiment TreeBank** (**SST-2**) contains movie reviews. In this
    section, we will describe the SST-2 (binary classification) task. However, the
    datasets go beyond that, and it is possible to classify sentiments in a range
    of *0* (negative) to *n* (positive).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**斯坦福情感树库**（**SST-2**）包含电影评论。在这一部分，我们将描述 SST-2（二元分类）任务。然而，数据集远不止如此，可以将情感分类为
    *0*（负面）到 *n*（正面）。'
- en: '*Socher* et al. (2013) took sentiment analysis beyond the binary positive-negative
    NLP classification. We will explore the SST-2 multi-label sentiment classification
    with a transformer model in *Chapter 12*, *Detecting Customer Emotions to Make
    Predictions*.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*Socher*等人(2013)将情感分析发展到了超越二元积极-消极 NLP 分类的程度。我们将在 *第十二章*，*检测客户情绪以做出预测* 中使用
    Transformer 模型探索 SST-2 多标签情感分类。'
- en: In this section, we will run a sample taken from SST on a Hugging Face transformer
    pipeline model to illustrate binary classification.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将在 Hugging Face transformer pipeline 模型上运行从 SST 中取出的样本以说明二元分类。
- en: 'Open `Transformer_tasks.ipynb` and run the following cell, which contains positive
    and negative movie reviews taken from SST:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Transformer_tasks.ipynb`并运行以下单元格，其中包含从**SST**获取的正面和负面电影评论：
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is accurate:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是准确的：
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The SST-2 task is evaluated using the accuracy metric.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**SST-2** 任务使用准确度指标进行评估。'
- en: We classify sentiments of a sequence. Let’s now see whether two sentences in
    a sequence are paraphrases or not.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对一个序列的情感进行分类。现在让我们看看一个序列中的两个句子是否是释义。
- en: Microsoft Research Paraphrase Corpus (MRPC)
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软研究释义语料库（MRPC）
- en: 'The **Microsoft Research Paraphrase Corpus** (**MRPC**), a GLUE task, contains
    pairs of sentences extracted from new sources on the web. Each pair has been annotated
    by a human to indicate whether the sentences are equivalent based on two closely
    related properties:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**微软研究释义语料库**（**MRPC**），一个 GLUE 任务，包含从网上新闻来源中提取的句子对。每对句子都经过人工标注，标明句子是否基于两个密切相关属性等价：'
- en: Paraphrase equivalent
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释等价
- en: Semantic equivalent
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义等价
- en: 'Let’s run a sample using the Hugging Face BERT model. Open `Transformer_tasks.ipynb`
    and go to the following cell, and then run the sample taken from MRPC:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Hugging Face BERT 模型运行一个样本。打开`Transformer_tasks.ipynb`并前往以下单元格，然后运行从 MRPC
    中提取的样本：
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is accurate, though you may get messages warning you that the model
    needs more downstream training:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是准确的，尽管可能会收到警告消息，表明模型需要更多的下游训练：
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The MRPC task is measured with the F1/Accuracy score method.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**MRPC** 任务使用 F1/准确率分数方法进行度量。'
- en: Let’s now run a Winograd schema.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行一个**Winograd schema**。
- en: Winograd schemas
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Winograd模式
- en: We described the Winograd schemas in this chapter’s *The Winograd schema challenge
    (WSC)* section. The training set was in English.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的*The Winograd schema challenge (WSC)*部分描述了Winograd模式。训练集是英文的。
- en: But what happens if we ask a transformer model to solve a pronoun gender problem
    in an English-French translation? French has different spellings for nouns that
    have grammatical genders (feminine, masculine).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们要求变压器模型解决英法翻译中的代词性别问题会发生什么？法语对具有语法性别（阴性、阳性）的名词有不同的拼写。
- en: The following sentence contains the pronoun *it*, which can refer to the words
    *car* or *garage*. Can a transformer disambiguate this pronoun?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子包含代词*it*，可以指代*car*或*garage*。变压器能消除这个代词的歧义吗？
- en: 'Open `Transformer_tasks.ipynb`, go to the `#Winograd` cell, and run our example:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Transformer_tasks.ipynb`，转到`#Winograd`单元格，并运行我们的示例：
- en: '[PRE18]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The translation is perfect:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译是完美的：
- en: '[PRE19]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The transformer detected that the word *it* refers to the word *car*, which
    is a feminine form. The feminine form applies to *it* and the adjective *big*:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器检测到单词*it*指的是单词*car*，这是女性形式。女性形式适用于*it*和形容词*big*：
- en: '*elle* means *she* in French, which is the translation of *it*. The masculine
    form would have been *il*, which means *he*.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*elle*在法语中意味着*she*，这是*it*的翻译。阳性形式将是*il*，意思是*he*。'
- en: '*grosse* is the feminine form of the translation of the word *big*. Otherwise,
    the masculine form would have been *gros*.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*grosse*是单词*big*的翻译的女性形式。否则，阳性形式将是*gros*。'
- en: We gave the transformer a difficult Winograd schema to solve, and it produced
    the right answer.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给变压器一个困难的Winograd模式来解决，它给出了正确的答案。
- en: There are many more dataset-driven NLU tasks available. We will explore some
    of them throughout this book to add more building blocks to our toolbox of transformers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多基于数据集的NLU任务可用。我们将在本书中探索其中一些，以将更多构建块添加到我们的transformer工具箱中。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: This chapter analyzed the difference between the human language representation
    process and the way machine intelligence performs transduction. We saw that transformers
    must rely on the outputs of our incredibly complex thought processes expressed
    in written language. Language remains the most precise way to express a massive
    amount of information. The machine has no senses and must convert speech to text
    to extract meaning from raw datasets.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分析了人类语言表征过程与机器智能执行传导的方式之间的差异。我们看到，变压器必须依赖于我们在书面语言中表达的极其复杂的思维过程的输出。语言仍然是表达大量信息的最精确的方式。机器没有感觉，必须将语音转换为文本，以从原始数据集中提取含义。
- en: We then explored how to measure the performance of multi-task transformers.
    Transformers’ ability to obtain top-ranking results for downstream tasks is unique
    in NLP history. We went through the tough SuperGLUE tasks that brought transformers
    up to the top ranks of the GLUE and SuperGLUE leaderboards.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了如何衡量多任务变压器的性能。变压器在下游任务中获得顶级结果的能力在NLP历史上是独一无二的。我们经历了许多艰难的SuperGLUE任务，这些任务将变压器带到了GLUE和SuperGLUE排行榜的前列。
- en: BoolQ, CB, WiC, and the many other tasks we covered are by no means easy to
    process, even for humans. We went through an example of several downstream tasks
    that show the difficulty transformer models face in proving their efficiency.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ、CB、WiC等我们涵盖的许多任务，甚至对人类来说都不容易处理。我们通过几个下游任务的示例展示了变压器模型在证明其效率时面临的困难。
- en: Transformers have proven their value by outperforming the former NLU architectures.
    To illustrate how simple it is to implement downstream fine-tuned tasks, we then
    ran several tasks in a Google Colaboratory notebook using Hugging Face’s pipeline
    for transformers.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器已经通过胜过以前的NLU架构证明了其价值。为了说明实施下游微调任务是多么简单，我们在Google Colaboratory笔记本中使用Hugging
    Face的transformers管道运行了几个任务。
- en: In *Winograd schemas*, we gave the transformer the difficult task of solving
    a Winograd disambiguation problem for an English-French translation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Winograd模式*中，我们给变压器一个困难的任务，即解决英法翻译中的Winograd消歧问题。
- en: In the next chapter, *Chapter 6*, *Machine Translation with the Transformer*,
    we will take translation tasks a step further and build a translation model with
    Trax.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*第6章*，*使用变压器进行机器翻译*，我们将进一步开展翻译任务，并使用Trax构建翻译模型。
- en: Questions
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Machine intelligence uses the same data as humans to make predictions. (True/False)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器智能使用与人类相同的数据进行预测。（是/否）
- en: SuperGLUE is more difficult than GLUE for NLP models. (True/False)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于NLP模型来说，SuperGLUE比GLUE更难。（是/否）
- en: BoolQ expects a binary answer. (True/False)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BoolQ期望一个二进制答案。（True/False）
- en: WiC stands for Words in Context. (True/False)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WiC代表上下文中的单词。（True/False）
- en: '**Recognizing Textual Entailment** (**RTE**) detects whether one sequence entails
    another sequence. (True/False)'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本蕴含识别**（**RTE**）检测一个序列是否蕴含另一个序列。（True/False）'
- en: A Winograd schema predicts whether a verb is spelled correctly. (True/False)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Winograd模式预测动词是否拼写正确。（True/False）
- en: Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer模型现在占据了GLUE和SuperGLUE的前几名。（True/False）
- en: Human Baselines standards are not defined once and for all. They were made tougher
    to attain by SuperGLUE. (True/False)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类基准标准并非一劳永逸。它们被SuperGLUE制定得更难达到。（True/False）
- en: Transformer models will never beat SuperGLUE Human Baselines standards. (True/False)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer模型永远不会达到SuperGLUE人类基准标准。（True/False）
- en: Variants of transformer models have outperformed RNN and CNN models. (True/False)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变种的Transformer模型已经超过了RNN和CNN模型。（True/False）
- en: References
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alex Wang*、*Yada Pruksachatkun*、*Nikita Nangia*、*Amanpreet Singh*、*Julian
    Michael*、*Felix Hill*、*Omer Levy*、*Samuel R. Bowman*，2019年，《SuperGLUE：通用语言理解系统的更棘手的基准》：[https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE：用于自然语言理解的多任务基准和分析平台*'
- en: '*Yu Sun*, *Shuohuan Wang*, *Yukun Li*, *Shikun Feng*, *Hao Tian*, *Hua Wu*,
    *Haifeng Wang*, 2019, *ERNIE 2.0: A Continual Pretraining Framework for Language
    Understanding*: [https://arxiv.org/pdf/1907.12412.pdf](https://arxiv.org/pdf/1907.12412.pdf)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Yu Sun*、*Shuohuan Wang*、*Yukun Li*、*Shikun Feng*、*Hao Tian*、*Hua Wu*、*Haifeng
    Wang*，2019年，《ERNIE 2.0：语言理解的持续预训练框架》：[https://arxiv.org/pdf/1907.12412.pdf](https://arxiv.org/pdf/1907.12412.pdf)'
- en: '*Melissa Roemmele*, *Cosmin Adrian Bejan*, and *Andrew S. Gordon*, 2011, *Choice
    of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning*: [https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Melissa Roemmele*、*Cosmin Adrian Bejan*和*Andrew S. Gordon*，2011年，《可选择的合理替代方案：常识因果推理评估》：[https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)'
- en: '*Richard Socher*, *Alex Perelygin*, *Jean Y. Wu*, *Jason Chuang*, *Christopher
    D. Manning*, *Andrew Y. Ng*, and *Christopher Potts*, 2013, *Recursive Deep Models
    for Semantic Compositionality Over a Sentiment Treebank*: [https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Richard Socher*、*Alex Perelygin*、*Jean Y. Wu*、*Jason Chuang*、*Christopher
    D. Manning*、*Andrew Y. Ng*和*Christopher Potts*，2013年，《递归深度模型用于情感树库的语义组合》：[https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
- en: '*Thomas Wolf*, *Lysandre Debut*, *Victor Sanh*, *Julien Chaumond*, *Clement
    Delangue*, *Anthony Moi*, *Pierric Cistac*, *Tim Rault*, *Rémi Louf*, *Morgan
    Funtowicz*, *Jamie Brew*, 2019, *HuggingFace’s Transformers: State-of-the-art
    Natural Language Processing*: [https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Thomas Wolf*、*Lysandre Debut*、*Victor Sanh*、*Julien Chaumond*、*Clement Delangue*、*Anthony
    Moi*、*Pierric Cistac*、*Tim Rault*、*Rémi Louf*、*Morgan Funtowicz*和*Jamie Brew*，2019年，《HuggingFace的Transformer：最先进的自然语言处理》：[https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)'
- en: 'Hugging Face Transformer Usage: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugging Face Transformer 用法: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
- en: Join our book’s Discord space
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 加入该书的Discord工作区，与作者进行每月的*问我任何事*会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
