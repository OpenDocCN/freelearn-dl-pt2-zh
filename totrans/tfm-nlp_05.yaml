- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downstream NLP Tasks with Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers reveal their full potential when we unleash pretrained models and
    watch them perform downstream **Natural Language Understanding** (**NLU**) tasks.
    It takes a lot of time and effort to pretrain and fine-tune a transformer model,
    but the effort is worthwhile when we see a multi-million parameter transformer
    model in action on a range of NLU tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter with the quest of outperforming the human baseline.
    The human baseline represents the performance of humans on an NLU task. Humans
    learn transduction at an early age and quickly develop inductive thinking. We
    humans perceive the world directly with our senses. Machine intelligence relies
    entirely on our perceptions transcribed into words to make sense of our language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We will then see how to measure the performance of transformers. Measuring **Natural
    Language Processing** (**NLP**) tasks remains a straightforward approach involving
    accuracy scores in various forms based on true and false results. These results
    are obtained through benchmark tasks and datasets. SuperGLUE, for example, is
    a wonderful example of how Google DeepMind, Facebook AI, the University of New
    York, the University of Washington, and others worked together to set high standards
    to measure NLP performances.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will explore several downstream tasks, such as the **Standard Sentiment
    TreeBank** (**SST-2**), linguistic acceptability, and Winograd schemas.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are rapidly taking NLP to the next level by outperforming other
    models on well-designed benchmark tasks. Alternative transformer architectures
    will continue to emerge and evolve.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Machine versus human intelligence for transduction and induction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLP transduction and induction process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring transformer performance versus Human Baselines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurement methods (Accuracy, F1-score, and MCC)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark tasks and datasets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuperGLUE downstream tasks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic acceptability with CoLA
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis with SST-2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winograd schemas
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by understanding how humans and machines represent language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Transduction and the inductive inheritance of transformers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The emergence of **Automated Machine Learning** (**AutoML**), meaning APIs in
    automated cloud AI platforms, has deeply changed the job description of every
    AI specialist. Google Vertex, for example, boasts a reduction of 80% of the development
    required to implement ML. This suggests that anybody can implement ML with ready-to-use
    systems. Does that mean an 80% reduction of the workforce of developers? I don’t
    think so. I see an Industry 4.0 AI specialist assemble AI with added value to
    a project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Industry 4.0\. NLP AI specialists invest less in source code and more in knowledge
    to become the AI guru of a team.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Transformers possess the unique ability to apply their knowledge to tasks they
    did not learn. A BERT transformer, for example, acquires language through sequence-to-sequence
    and masked language modeling. The BERT transformer can then be fine-tuned to perform
    downstream tasks that it did not learn from scratch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will do a mind experiment. We will use the graph of a transformer
    to represent how humans and machines make sense of information using language.
    Machines make sense of information in a different way from humans but reach very
    efficient results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.1*, a mind experiment designed with transformer architecture layers
    and sublayers, shows the deceptive similarity between humans and machines. Let’s
    study the learning process of transformer models to understand downstream tasks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Human and ML methods'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: For our example, `N=2`. This conceptual representation has two layers. The two
    layers show that humans build on accumulated knowledge from generation to generation.
    Machines only process what we give them. Machines use our outputs as inputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The human intelligence stack
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the left side of *Figure 5.1*, we can see that the input for humans is the
    perception of raw events for layer 0, and the output is language. We first perceive
    events with our senses as children. Gradually, the output becomes burbling language
    and then structured language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: For humans, *transduction* goes through a trial-and-error process. Transduction
    means that we take structures we perceive and represent them with patterns, for
    example. We make representations of the world that we apply to our inductive thinking.
    Our inductive thinking relies on the quality of our transductions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: For example, as children, we were often forced to take a nap early in the afternoon.
    Famous child psychologist Piaget found that this could lead to some children saying,
    for example, “I haven’t taken a nap, so it’s not the afternoon.” The child sees
    two events, creates a link between them with transduction, and then makes an inference
    to generalize and make an induction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, humans notice these patterns through transduction and generalize
    them through *inductions*. We are trained by trial and error to understand that
    many events are related:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '*Trained_related events = {sunrise – light, sunset – dark, dark clouds – rain,
    blue sky – running, food – good, fire – warm, snow – cold}*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, we are trained to understand millions of related events. New generations
    of humans did not have to start from scratch. They were only *fine-tuned for many
    tasks by previous generations*. They were taught that “fire burns you,” for example.
    From then on, a child knew that this knowledge could be fine-tuned to any form
    of “fire”: candles, wildfires, volcanoes, and every instance of “fire.”'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Finally, humans transcribed everything they knew, imagined, or predicted into
    written *language*. The output of layer 0 was born.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: For humans, the input of the next layer, layer 1, is the vast amount of trained
    and fine-tuned knowledge. On top of that, humans perceive massive amounts of events
    that then go through the transduction, induction, training, and fine-tuning of
    sublayers, along with previous transcribed knowledge.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Many of these events stem from odors, emotions, situations, experiences, and
    everything that makes a human unique. Machines do not have access to this individual
    identity. Humans have a personal perception of a word in a homogeneous way specific
    for each individual.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: A machine takes what we give it through masses of heterogeneous unfiltered impersonal
    data. The goal of a machine is to perform an impersonal, efficient task. The goal
    of a human being is personal wellbeing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Our infinite approach loop goes from layer 0 to layer 1 and back to layer 0
    with more raw and processed information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The result is fascinating! We do not need to learn (train on) our native language
    from scratch to acquire summarization abilities. We use our pretrained knowledge
    to adjust (fine-tune) to summarization tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Transformers go through the same process but in a different way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The machine intelligence stack
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machines learn basic tasks like the ones described in this chapter, and then
    perform hundreds of tasks using the sequences they learned how to predict.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: On the right side of *Figure 5.1*, we can see that the input for machines is
    second-hand information in the form of language. *Our output is the only input
    machines have to analyze language*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: At this point in human and machine history, computer vision identifies images
    but does not contain the grammatical structure of language. Speech recognition
    converts sound into words, which brings us back to written language. Music pattern
    recognition cannot lead to objective concepts expressed in words.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Machines start with a handicap. We impose an artificial disadvantage on them.
    Machines must rely on our random quality language outputs to:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Perform transductions connecting all the tokens (subwords) that occur together
    in language sequences
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build inductions from these transductions
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train those inductions based on tokens to produce patterns of tokens
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s stop at this point and peek into the process of the attention sublayer,
    which works hard to produce valid inductions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The transformer model excluded the former recurrence-based learning operations
    and used self-attention to heighten the vision of the model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention sublayers have an advantage over humans at this point: they can process
    millions of examples for their inductive thinking operations'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like us, they find patterns in sequences through transduction and induction
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They memorize these patterns using parameters that are stored with their model
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They have acquired language understanding by using their abilities: substantial
    data volumes, excellent NLP transformer algorithms, and computer power. *Thanks
    to their deep understanding of language, they are ready to run hundreds of tasks
    they were not trained for*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Transformers, like humans, acquire language understanding through a limited
    number of tasks. Like us, they detect connections through transduction and then
    generalize them through inductive operations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: When the transformer model reaches the fine-tuning sub-layer of machine intelligence,
    it reacts like us. It does not start training from scratch to perform a new task.
    Like us, it considers it as a downstream task that only requires fine-tuning.
    If it needs to learn how to answer a question, it does not start learning a language
    from scratch. A transformer model just fine-tunes its parameters like us.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw that transformer models struggle to learn in the way
    we do. They start with a handicap from the moment they rely on our perceptions
    transcribed into language. However, they have access to infinitely more data than
    we do with massive computing power.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how to measure transformer performances versus Human Baselines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Transformer performances versus Human Baselines
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers, like humans, can be fine-tuned to perform downstream tasks by
    inheriting the properties of a pretrained model. The pretrained model provides
    its architecture and language representations through its parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: A pretrained model trains on key tasks to acquire a general knowledge of the
    language. A fine-tuned model trains on downstream tasks. Not every transformer
    model uses the same tasks for pretraining. Potentially, all tasks can be pretrained
    or fine-tuned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Every NLP model needs to be evaluated with a standard method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: This section will first go through some of the key measurement methods. Then,
    we will go through some of the main benchmark tasks and datasets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by going through some of the key metric methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models with metrics
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is impossible to compare one transformer model to another transformer model
    (or any other NLP model) without a universal measurement system that uses metrics.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will analyze three measurement scoring methods that are
    used by GLUE and SuperGLUE.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy score
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The accuracy score, in whatever variant you use, is a practical evaluation.
    The score function calculates a straightforward true or false value for each result.
    Either the model’s outputs, ![](img/B17948_05_001.png), match the correct predictions,
    ![](img/B17948_05_002.png), for a given subset, *samples*[i], of a set of samples
    or not. The basic function is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_08.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: We will obtain `1` if the result for the subset is correct and `0` if it is
    false.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now examine the more flexible F1-score.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: F1-score
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The F1-score introduces a more flexible approach that can help when faced with
    datasets containing uneven class distributions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The F1-score uses the weighted values of precision and recall. It is a weighted
    average of precision and recall values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '*F1score= 2** (*precision * recall*)/(*precision + recall*)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation, true (*T*) positives (*p*), false (*F*) positives (*p*),
    and false (*F*) negatives (*n*) are plugged into the precision (*P*) and recall
    (*R*) equations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_09.png)![](img/B17948_05_10.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'The F1-score can thus be viewed as the harmonic mean (reciprocal of the arithmetic
    mean) of precision (*P*) and recall (*R*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_003.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Let’s now review the MCC approach.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Matthews Correlation Coefficient (MCC)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MCC was described and implemented in the *Evaluating using Matthews Correlation
    Coefficient* section in *Chapter 3*, *Fine-Tuning BERT Models*. MCC computes a
    measurement with true positives (*T*[P]), true negatives (*T*[N]), false positives
    (*F*[P]), and false negatives (*F*[N]).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The MCC can be summarized by the following equation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_11.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: MCC provides an excellent metric for binary classification models, even if the
    sizes of the classes are different.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good idea of how to measure a given transformer model’s results
    and compare them to other transformer models or NLP models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: With measurement scoring methods in mind, let’s now look into benchmark tasks
    and datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark tasks and datasets
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Three prerequisites are required to prove that transformers have reached state-of-the-art
    performance levels:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: A model
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset-driven task
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metric as described in the *Evaluating models with metrics* section of this
    chapter
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will begin by exploring the SuperGLUE benchmark to illustrate the evaluation
    process of a transformer model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: From GLUE to SuperGLUE
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SuperGLUE benchmark was designed and made public by *Wang* et al. (2019).
    *Wang* et al. (2019) first designed the **General Language Understanding Evaluation**
    (**GLUE**) benchmark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of the GLUE benchmark was to show that to be useful, NLU has
    to be applicable to a wide range of tasks. Relatively small GLUE datasets were
    designed to encourage an NLU model to solve a set of tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: However, the performance of NLU models, boosted by the arrival of transformers,
    began to exceed the level of the average human, as we can see in the GLUE leaderboard
    (December 2021). The GLUE leaderboard, available at [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard),
    shows a remarkable display of NLU talent, retaining some of the former RNN/CNN
    ideas while mainly focusing on the ground-breaking transformer models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'The following excerpt of the leaderboard shows the top leaders and the position
    of GLUE’s Human Baselines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_02.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: GLUE Leaderboard – December 2021'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: New models and the Human Baselines ranking will constantly change. These rankings
    just give an idea of how far classical NLP and transformers have taken us!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: We first notice the GLUE Human Baselines are not in a top position, which shows
    that NLU models have surpassed non-expert humans on GLUE tasks. Human Baselines
    represent what we humans can achieve. AI can now outperform humans. In December
    2021, the Human Baselines are only in position 17\. This is a problem. Without
    a standard to beat, it is challenging to fish around for benchmark datasets to
    improve our models blindly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: We also notice that transformer models have taken the lead.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: I like to think of GLUE and SuperGLUE as the point when words go from chaos
    to order with language understanding. For me, understanding is the glue that makes
    words fit together and become a language.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The GLUE leaderboard will continuously evolve as NLU progresses. However, *Wang*
    et al. (2019) introduced SuperGLUE to set a higher standard for Human Baselines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Introducing higher Human Baselines standards
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Wang* et al. (2019) recognized the limits of GLUE. They designed SuperGLUE
    for more difficult NLU tasks.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'SuperGLUE immediately re-established Human Baselines at rank #1 (December 2020),
    as shown in the following excerpt of the leaderboard, [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_03.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: SuperGLUE Leaderboard 2.0 – December 2020'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the SuperGLUE leaderboard evolved as we produced better NLU models.
    In 2021, transformers had already surpassed Human Baselines. In December 2021,
    Human Baselines have gone down to rank #5, as shown in *Figure 5.4*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: SuperGLUE Leaderboard 2.0 – December 2021'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: AI algorithm rankings will constantly change as new innovative models arrive.
    These rankings just give an idea of how hard the battle for NLP supremacy is being
    fought!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the evaluation process works.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The SuperGLUE evaluation process
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wang et al. (2019) selected eight tasks for their SuperGLUE benchmark. The selection
    criteria for these tasks were stricter than for GLUE. For example, the tasks had
    to not only understand texts but also to reason. The level of reasoning is not
    that of a top human expert. However, the level of performance is sufficient to
    replace many human tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The eight SuperGLUE tasks are presented in a ready-to-use list:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_05.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: SuperGLUE tasks'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The task list is interactive: [https://super.gluebenchmark.com/tasks](https://super.gluebenchmark.com/tasks).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Each task contains links to the required information to perform that task:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Name** is the name of the downstream task of a fine-tuned, pretrained model'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifier** is the abbreviation or short version of the name'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Download** is the download link to the datasets'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More Info** offers greater detail through a link to the paper or website
    of the team that designed the dataset-driven task(s)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric** is the measurement score used to evaluate the model'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SuperGLUE provides the task instructions, the software, the datasets, and papers
    or websites describing the problem to be solved. Once a team runs the benchmark
    tasks and reaches the leaderboard, the results are displayed:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE提供任务说明、软件、数据集以及描述要解决问题的论文或网站。一旦团队运行基准任务并达到排行榜，结果将显示如下：
- en: '![](img/B17948_05_06.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_06.png)'
- en: 'Figure 5.6: SuperGLUE task scores'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：SuperGLUE任务分数
- en: SuperGLUE displays the overall score and the score for each task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE显示了整体评分以及每个任务的评分。
- en: For example, let’s take the instructions *Wang* et al. (2019) provided for the
    **Choice of Plausible Answers** (**COPA**) task in *Table 6* of their paper.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看*王*等人(2019年)在他们的论文的*表6*中为**选择合理答案**(**COPA**)任务提供的说明。
- en: The first step is to read the remarkable paper written by *Roemmele* et al.
    (2011). In a nutshell, the goal is for the NLU model to demonstrate its machine
    thinking (not human thinking, of course) potential. In our case, the transformer
    must choose the most plausible answer to a question. The dataset provides a premise,
    and the transformer model must find the most plausible answer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是阅读由*Roemmele*等人(2011年)撰写的杰出论文。简言之，目标是让NLU模型展示它的机器思考能力（当然不是人类思考）。在我们的案例中，transformer必须选择最合理的答案来回答问题。数据集提供前提，transformer模型必须找出最合理的答案。
- en: 'For example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '`Premise: I knocked on my neighbor''s door.`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`前提：我敲了敲邻居的门。`'
- en: '`What happened as a result?`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`结果是什么？`'
- en: '`Alternative 1: My neighbor invited me in.`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`另一种选择：我的邻居邀请我进去了。`'
- en: '`Alternative 2: My neighbor left his house.`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`另一种选择：我的邻居离开了他的房子。`'
- en: This question requires a second or two for a human to answer, which shows that
    it requires some commonsense machine thinking. `COPA.zip`, a ready-to-use dataset,
    can be downloaded directly from the SuperGLUE task page. The metric provided makes
    the process equal and reliable for all participants in the benchmark race.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题需要人类花上一两秒钟来回答，这表明它需要一些常识机器思考。`COPA.zip`是一个可以直接从SuperGLUE任务页面下载的现成数据集，所提供的度量使得这个过程对于所有参与基准竞赛的人员都是公平和可靠的。
- en: 'The examples might seem difficult. However, transformers are nearing COPA Human
    Baselines (*Figure 5.7*), which is only at rank #5 if we take all of the tasks
    into account:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子可能看起来很困难。然而，如果我们考虑所有的任务，transformers接近了COPA人类基准线(*图5.7*)，它只排在第5名：
- en: '![](img/B17948_05_07.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_05_07.png)'
- en: 'Figure 5.7: SuperGLUE results for COPA'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：COPA的SuperGLUE结果
- en: As incredible as it seems, transformers climbed the leaderboard ladder in a
    very short time! And this is just the beginning. New ideas are emerging nearly
    every month!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看上去难以置信，transformers在很短的时间内就攀登到了排行榜梯子上！而且这只是一个开始。新的想法几乎每个月都在出现！
- en: We have introduced COPA. Let’s define the seven other SuperGLUE benchmark tasks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了COPA。让我们定义另外七个SuperGLUE基准任务。
- en: Defining the SuperGLUE benchmark tasks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义SuperGLUE基准任务
- en: A task can be a pretraining task to generate a trained model. That same task
    can be a downstream task for another model that will fine-tune it. However, the
    goal of SuperGLUE is to show that a given NLU model can perform multiple downstream
    tasks with fine-tuning. Multi-task models are the ones that prove the thinking
    power of transformers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个任务可以是一个用于生成训练模型的预训练任务。同一个任务可以是另一个模型的下游任务，该模型将对其进行微调。然而，SuperGLUE的目标是展示给定的NLU模型可以执行多个下游任务并进行微调。多任务模型证明了transformers的思考能力。
- en: The power of any transformer resides in its ability to perform multiple tasks
    using a pretrained model and then applying it to fine-tuned downstream tasks.
    The original Transformer model and its variants now lead in all the GLUE and SuperGLUE
    tasks. We will continue to focus on SuperGLUE downstream tasks for which Human
    Baselines is tough to beat.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 任何transformer的力量都在于它能够使用预训练模型执行多个任务，然后将其应用于微调的下游任务。原始Transformer模型及其变体现在所有GLUE和SuperGLUE任务中处于领先地位。我们将继续关注SuperGLUE下游任务，而人类基准线很难击败。
- en: In the previous section, we went through COPA. In this section, we will go through
    the seven other tasks defined by *Wang* et al. (2019) in *Table 2* of their paper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们介绍了COPA。在本节中，我们将介绍*王*等人(2019年)在其论文的*表2*中定义的其他七个任务。
- en: Let’s continue with a Boolean question task.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行一个布尔问题任务。
- en: BoolQ
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BoolQ
- en: 'BoolQ is a Boolean yes-or-no answer task. The dataset, as defined on SuperGLUE,
    contains 15,942 naturally occurring examples. A raw sample of line `#3` of the
    `train.jsonl` dataset contains a passage, a question, and the answer (`true`):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: BoolQ是一个布尔型的是或否回答任务。如在SuperGLUE中所定义的，数据集包含15,942个自然发生的例子。`train.jsonl` 数据集的第3行原始样本包含一段文章、一个问题和答案（`true`）：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The datasets provided may change in time, but the concepts remain the same.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的数据集可能会随时间改变，但概念保持不变。
- en: Now, let’s examine CB, a task that requires both humans and machines to focus.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看CB，这是一个需要人类和机器共同关注的任务。
- en: Commitment Bank (CB)
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Commitment Bank（CB）
- en: '**Commitment Bank** (**CB**) is a difficult *entailment* task. We are asking
    the transformer model to read a *premise*, then examine a *hypothesis* built on
    the premise. For example, the hypothesis will confirm the premise or contradict
    it. Then the transformer model must *label* the hypothesis as *neutral*, an *entailment*,
    or a *contradiction* of the premise, for example.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**承诺银行**（**CB**）是一个困难的*蕴含*任务。我们要求变压器模型阅读一个*前提*，然后检验基于前提构建的一个*假设*。例如，假设会确认前提或与之矛盾。然后变压器模型必须将假设标记为*中性*、*蕴含*或*矛盾*，例如。'
- en: The dataset contains discourses, which are natural discourses.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含自然语篇。
- en: 'The following sample `#77`, taken from the `train.jsonl` training dataset,
    shows how difficult the CB task is:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中的 `#77`，取自 `train.jsonl` 训练数据集，展示了CB任务的难度：
- en: '[PRE1]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will now have a look at the multi-sentence problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在来看看多句问题。 '
- en: Multi-Sentence Reading Comprehension (MultiRC)
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多句阅读理解（MultiRC）
- en: '**Multi-Sentence Reading Comprehension** (**MultiRC**) asks the model to read
    a text and choose from several possible choices. The task is difficult for both
    humans and machines. The model is presented with a *text*, several *questions*,
    and possible *answers* to each question with a `0` (false) or `1` (true) *label*.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**多句阅读理解**（**MultiRC**）要求模型阅读一段文本，并从几个可能的选择中作出选择。这个任务对人类和机器来说都很困难。模型需要面对一个*文本*，多个*问题*，以及每个问题可能对应的*答案*，并带有
    `0`（错误）或 `1`（正确）的*标签*。'
- en: 'Let’s take the second sample in `train.jsonl`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看 `train.jsonl` 中的第二个样本：
- en: '[PRE2]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The sample contains four questions. To illustrate the task, we will just investigate
    two of them. The model must predict the correct labels. Notice how the information
    that the model is asked to obtain is distributed throughout the text:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 样本包含四个问题。为了说明这个任务，我们只研究其中两个。模型必须预测正确的标签。请注意，模型被要求获取的信息分布在整个文本中：
- en: '[PRE3]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, one can only admire the performance of a single fine-tuned, pretrained
    model on these difficult downstream tasks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，人们只能欣赏一个优秀的、经过精确调校的预训练模型在这些困难的下游任务上的表现。
- en: Now, let’s see the reading comprehension task.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看阅读理解任务。
- en: Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Reading Comprehension with Commonsense Reasoning Dataset（ReCoRD）
- en: '**Reading Comprehension with Commonsense Reasoning Dataset** (**ReCoRD**) represents
    another challenging task. The dataset contains over 120,000 queries from more
    than 70,000 news articles. The transformer must use common-sense reasoning to
    solve this problem.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用常识推理数据集**（**ReCoRD**）代表着另一个具有挑战性的任务。该数据集包含来自超过70,000篇新闻文章的120,000多个查询。变压器必须运用常识推理来解决这个问题。'
- en: 'Let’s examine a sample from `train.jsonl`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来查看 `train.jsonl` 中的一个样本：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The *entities* are indicated, as shown in the following excerpt:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体*如下摘录所示：'
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, the model must *answer* a *query* by finding the proper value for
    the *placeholder*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型必须通过找到适当的值来*回答*一个*查询*，填写*占位符*：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once the transformer model has gone through this problem, it must now face an
    entailment task.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦变压器模型经历了这个问题，它现在必须面对一个包含的任务。
- en: Recognizing Textual Entailment (RTE)
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本蕴含识别（RTE）
- en: For **Recognizing Textual Entailment** (**RTE**), the transformer model must
    read the *premise*, examine a *hypothesis*, and predict the *label* of the *entailment
    hypothesis status*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**文本蕴涵识别**（**RTE**），变压器模型必须阅读*前提*，检查一个*假设*，并预测*蕴含假设状态*的*标签*。
- en: 'Let’s examine sample `#19` of the `train.jsonl` dataset:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 `train.jsonl` 数据集的样本 `#19`：
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: RTE requires understanding and logic. Let’s now see the Words in Context task.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: RTE需要理解和逻辑推理。现在让我们来看Words in Context任务。
- en: Words in Context (WiC)
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Words in Context（WiC）
- en: '**Words in Context** (**WiC**) and the following Winograd task test a model’s
    ability to process an ambiguous word. In WiC, the multi-task transformer will
    have to analyze two sentences and determine whether the target word has the same
    meaning in both sentences.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the first sample of the `train.jsonl` dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the target word is specified:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The model has to read two sentences containing the target word:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`train.jsonl` specifies the sample index, the value of the label, and the position
    of the target word in `sentence1(start1, end1)` and `sentence2(start2, end2)`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After this daunting task, the transformer model has to face the Winograd task.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The Winograd schema challenge (WSC)
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Winograd schema task is named after Terry Winograd. If a transformer is
    well trained, it should be able to solve disambiguation problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains sentences that target slight differences in the gender
    of a pronoun.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: This constitutes a coreference resolution problem, which is one of the most
    challenging tasks to perform. However, the transformer architecture, which allows
    self-attention, is ideal for this task.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Each sentence contains an *occupation*, a *participant*, and a *pronoun*. The
    problem to solve is to find whether the pronoun is *coreferent* with the occupation
    or the participant.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine a sample taken from `train.jsonl`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the sample asks the model to read a *text*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have gone through some of the main SuperGLUE tasks. There are many other
    tasks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: However, once you understand the architecture of transformers and the mechanism
    of the benchmark tasks, you will rapidly adapt to any model and benchmark.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run some downstream tasks.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Running downstream tasks
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will just jump into some transformer cars and drive them
    around a bit to see what they do. There are many models and tasks. We will run
    a few of them in this section. Once you understand the process of running a few
    tasks, you will quickly understand all of them. *After all, the human baseline
    for all these tasks is us!*
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: A downstream task is a fine-tuned transformer task that inherits the model and
    parameters from a pretrained transformer model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: A downstream task is thus the perspective of a pretrained model running fine-tuned
    tasks. That means, depending on the model, a task is downstream if it was not
    used to fully pretrain the model. In this section, we will consider all the tasks
    as downstream since we did not pretrain them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Models will evolve, as will databases, benchmark methods, accuracy measurement
    methods, and leaderboard criteria. But the structure of human thought reflected
    through the downstream tasks in this chapter will remain.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with CoLA.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The Corpus of Linguistic Acceptability (CoLA)
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Corpus of Linguistic Acceptability** (**CoLA**), a GLUE task, [https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks),
    contains thousands of samples of English sentences annotated for grammatical acceptability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The goal of *Alex Warstadt* et al. (2019) was to evaluate the linguistic competence
    of an NLP model to judge the linguistic acceptability of a sentence. The NLP model
    is expected to classify the sentences accordingly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentences are labeled as grammatical or ungrammatical. The sentence is
    labeled `0` if the sentence is not grammatically acceptable. The sentence is labeled
    `1` if the sentence is grammatically acceptable. For example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Classification = `1` for ‘we yelled ourselves hoarse.’
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Classification = `0` for ‘we yelled ourselves.’
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'You can go through `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb` in
    *Chapter 3*, *Fine-Tuning BERT Models*, to view the BERT model that we fine-tuned
    on CoLA datasets. We used CoLA data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also load a pretrained BERT model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the measurement method, or metric, we used is MCC, which was described
    in the *Evaluating using Matthews Correlation Coefficient* section of *Chapter
    3*, *Fine-Tuning BERT Models*, and earlier in this chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to that section for the mathematical description of MCC and take
    the time to rerun the source code if necessary.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: A sentence can be grammatically unacceptable but still convey a sentiment. Sentiment
    analysis can add some form of empathy to a machine.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Stanford Sentiment TreeBank (SST-2)
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Stanford Sentiment TreeBank** (**SST-2**) contains movie reviews. In this
    section, we will describe the SST-2 (binary classification) task. However, the
    datasets go beyond that, and it is possible to classify sentiments in a range
    of *0* (negative) to *n* (positive).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '*Socher* et al. (2013) took sentiment analysis beyond the binary positive-negative
    NLP classification. We will explore the SST-2 multi-label sentiment classification
    with a transformer model in *Chapter 12*, *Detecting Customer Emotions to Make
    Predictions*.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will run a sample taken from SST on a Hugging Face transformer
    pipeline model to illustrate binary classification.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Transformer_tasks.ipynb` and run the following cell, which contains positive
    and negative movie reviews taken from SST:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is accurate:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The SST-2 task is evaluated using the accuracy metric.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: We classify sentiments of a sequence. Let’s now see whether two sentences in
    a sequence are paraphrases or not.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research Paraphrase Corpus (MRPC)
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Microsoft Research Paraphrase Corpus** (**MRPC**), a GLUE task, contains
    pairs of sentences extracted from new sources on the web. Each pair has been annotated
    by a human to indicate whether the sentences are equivalent based on two closely
    related properties:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Paraphrase equivalent
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic equivalent
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s run a sample using the Hugging Face BERT model. Open `Transformer_tasks.ipynb`
    and go to the following cell, and then run the sample taken from MRPC:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is accurate, though you may get messages warning you that the model
    needs more downstream training:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The MRPC task is measured with the F1/Accuracy score method.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run a Winograd schema.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Winograd schemas
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We described the Winograd schemas in this chapter’s *The Winograd schema challenge
    (WSC)* section. The training set was in English.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if we ask a transformer model to solve a pronoun gender problem
    in an English-French translation? French has different spellings for nouns that
    have grammatical genders (feminine, masculine).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The following sentence contains the pronoun *it*, which can refer to the words
    *car* or *garage*. Can a transformer disambiguate this pronoun?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Transformer_tasks.ipynb`, go to the `#Winograd` cell, and run our example:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The translation is perfect:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The transformer detected that the word *it* refers to the word *car*, which
    is a feminine form. The feminine form applies to *it* and the adjective *big*:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '*elle* means *she* in French, which is the translation of *it*. The masculine
    form would have been *il*, which means *he*.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '*grosse* is the feminine form of the translation of the word *big*. Otherwise,
    the masculine form would have been *gros*.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: We gave the transformer a difficult Winograd schema to solve, and it produced
    the right answer.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: There are many more dataset-driven NLU tasks available. We will explore some
    of them throughout this book to add more building blocks to our toolbox of transformers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter analyzed the difference between the human language representation
    process and the way machine intelligence performs transduction. We saw that transformers
    must rely on the outputs of our incredibly complex thought processes expressed
    in written language. Language remains the most precise way to express a massive
    amount of information. The machine has no senses and must convert speech to text
    to extract meaning from raw datasets.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: We then explored how to measure the performance of multi-task transformers.
    Transformers’ ability to obtain top-ranking results for downstream tasks is unique
    in NLP history. We went through the tough SuperGLUE tasks that brought transformers
    up to the top ranks of the GLUE and SuperGLUE leaderboards.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: BoolQ, CB, WiC, and the many other tasks we covered are by no means easy to
    process, even for humans. We went through an example of several downstream tasks
    that show the difficulty transformer models face in proving their efficiency.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have proven their value by outperforming the former NLU architectures.
    To illustrate how simple it is to implement downstream fine-tuned tasks, we then
    ran several tasks in a Google Colaboratory notebook using Hugging Face’s pipeline
    for transformers.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: In *Winograd schemas*, we gave the transformer the difficult task of solving
    a Winograd disambiguation problem for an English-French translation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Chapter 6*, *Machine Translation with the Transformer*,
    we will take translation tasks a step further and build a translation model with
    Trax.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine intelligence uses the same data as humans to make predictions. (True/False)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SuperGLUE is more difficult than GLUE for NLP models. (True/False)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BoolQ expects a binary answer. (True/False)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: WiC stands for Words in Context. (True/False)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recognizing Textual Entailment** (**RTE**) detects whether one sequence entails
    another sequence. (True/False)'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Winograd schema predicts whether a verb is spelled correctly. (True/False)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human Baselines standards are not defined once and for all. They were made tougher
    to attain by SuperGLUE. (True/False)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer models will never beat SuperGLUE Human Baselines standards. (True/False)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variants of transformer models have outperformed RNN and CNN models. (True/False)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yu Sun*, *Shuohuan Wang*, *Yukun Li*, *Shikun Feng*, *Hao Tian*, *Hua Wu*,
    *Haifeng Wang*, 2019, *ERNIE 2.0: A Continual Pretraining Framework for Language
    Understanding*: [https://arxiv.org/pdf/1907.12412.pdf](https://arxiv.org/pdf/1907.12412.pdf)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Melissa Roemmele*, *Cosmin Adrian Bejan*, and *Andrew S. Gordon*, 2011, *Choice
    of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning*: [https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Richard Socher*, *Alex Perelygin*, *Jean Y. Wu*, *Jason Chuang*, *Christopher
    D. Manning*, *Andrew Y. Ng*, and *Christopher Potts*, 2013, *Recursive Deep Models
    for Semantic Compositionality Over a Sentiment Treebank*: [https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thomas Wolf*, *Lysandre Debut*, *Victor Sanh*, *Julien Chaumond*, *Clement
    Delangue*, *Anthony Moi*, *Pierric Cistac*, *Tim Rault*, *Rémi Louf*, *Morgan
    Funtowicz*, *Jamie Brew*, 2019, *HuggingFace’s Transformers: State-of-the-art
    Natural Language Processing*: [https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformer Usage: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
