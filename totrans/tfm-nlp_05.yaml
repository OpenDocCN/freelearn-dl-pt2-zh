- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downstream NLP Tasks with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers reveal their full potential when we unleash pretrained models and
    watch them perform downstream **Natural Language Understanding** (**NLU**) tasks.
    It takes a lot of time and effort to pretrain and fine-tune a transformer model,
    but the effort is worthwhile when we see a multi-million parameter transformer
    model in action on a range of NLU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter with the quest of outperforming the human baseline.
    The human baseline represents the performance of humans on an NLU task. Humans
    learn transduction at an early age and quickly develop inductive thinking. We
    humans perceive the world directly with our senses. Machine intelligence relies
    entirely on our perceptions transcribed into words to make sense of our language.
  prefs: []
  type: TYPE_NORMAL
- en: We will then see how to measure the performance of transformers. Measuring **Natural
    Language Processing** (**NLP**) tasks remains a straightforward approach involving
    accuracy scores in various forms based on true and false results. These results
    are obtained through benchmark tasks and datasets. SuperGLUE, for example, is
    a wonderful example of how Google DeepMind, Facebook AI, the University of New
    York, the University of Washington, and others worked together to set high standards
    to measure NLP performances.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will explore several downstream tasks, such as the **Standard Sentiment
    TreeBank** (**SST-2**), linguistic acceptability, and Winograd schemas.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are rapidly taking NLP to the next level by outperforming other
    models on well-designed benchmark tasks. Alternative transformer architectures
    will continue to emerge and evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine versus human intelligence for transduction and induction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLP transduction and induction process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring transformer performance versus Human Baselines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurement methods (Accuracy, F1-score, and MCC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark tasks and datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuperGLUE downstream tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistic acceptability with CoLA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis with SST-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winograd schemas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by understanding how humans and machines represent language.
  prefs: []
  type: TYPE_NORMAL
- en: Transduction and the inductive inheritance of transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The emergence of **Automated Machine Learning** (**AutoML**), meaning APIs in
    automated cloud AI platforms, has deeply changed the job description of every
    AI specialist. Google Vertex, for example, boasts a reduction of 80% of the development
    required to implement ML. This suggests that anybody can implement ML with ready-to-use
    systems. Does that mean an 80% reduction of the workforce of developers? I don’t
    think so. I see an Industry 4.0 AI specialist assemble AI with added value to
    a project.
  prefs: []
  type: TYPE_NORMAL
- en: Industry 4.0\. NLP AI specialists invest less in source code and more in knowledge
    to become the AI guru of a team.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers possess the unique ability to apply their knowledge to tasks they
    did not learn. A BERT transformer, for example, acquires language through sequence-to-sequence
    and masked language modeling. The BERT transformer can then be fine-tuned to perform
    downstream tasks that it did not learn from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will do a mind experiment. We will use the graph of a transformer
    to represent how humans and machines make sense of information using language.
    Machines make sense of information in a different way from humans but reach very
    efficient results.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.1*, a mind experiment designed with transformer architecture layers
    and sublayers, shows the deceptive similarity between humans and machines. Let’s
    study the learning process of transformer models to understand downstream tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Human and ML methods'
  prefs: []
  type: TYPE_NORMAL
- en: For our example, `N=2`. This conceptual representation has two layers. The two
    layers show that humans build on accumulated knowledge from generation to generation.
    Machines only process what we give them. Machines use our outputs as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The human intelligence stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the left side of *Figure 5.1*, we can see that the input for humans is the
    perception of raw events for layer 0, and the output is language. We first perceive
    events with our senses as children. Gradually, the output becomes burbling language
    and then structured language.
  prefs: []
  type: TYPE_NORMAL
- en: For humans, *transduction* goes through a trial-and-error process. Transduction
    means that we take structures we perceive and represent them with patterns, for
    example. We make representations of the world that we apply to our inductive thinking.
    Our inductive thinking relies on the quality of our transductions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, as children, we were often forced to take a nap early in the afternoon.
    Famous child psychologist Piaget found that this could lead to some children saying,
    for example, “I haven’t taken a nap, so it’s not the afternoon.” The child sees
    two events, creates a link between them with transduction, and then makes an inference
    to generalize and make an induction.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, humans notice these patterns through transduction and generalize
    them through *inductions*. We are trained by trial and error to understand that
    many events are related:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trained_related events = {sunrise – light, sunset – dark, dark clouds – rain,
    blue sky – running, food – good, fire – warm, snow – cold}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, we are trained to understand millions of related events. New generations
    of humans did not have to start from scratch. They were only *fine-tuned for many
    tasks by previous generations*. They were taught that “fire burns you,” for example.
    From then on, a child knew that this knowledge could be fine-tuned to any form
    of “fire”: candles, wildfires, volcanoes, and every instance of “fire.”'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, humans transcribed everything they knew, imagined, or predicted into
    written *language*. The output of layer 0 was born.
  prefs: []
  type: TYPE_NORMAL
- en: For humans, the input of the next layer, layer 1, is the vast amount of trained
    and fine-tuned knowledge. On top of that, humans perceive massive amounts of events
    that then go through the transduction, induction, training, and fine-tuning of
    sublayers, along with previous transcribed knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these events stem from odors, emotions, situations, experiences, and
    everything that makes a human unique. Machines do not have access to this individual
    identity. Humans have a personal perception of a word in a homogeneous way specific
    for each individual.
  prefs: []
  type: TYPE_NORMAL
- en: A machine takes what we give it through masses of heterogeneous unfiltered impersonal
    data. The goal of a machine is to perform an impersonal, efficient task. The goal
    of a human being is personal wellbeing.
  prefs: []
  type: TYPE_NORMAL
- en: Our infinite approach loop goes from layer 0 to layer 1 and back to layer 0
    with more raw and processed information.
  prefs: []
  type: TYPE_NORMAL
- en: The result is fascinating! We do not need to learn (train on) our native language
    from scratch to acquire summarization abilities. We use our pretrained knowledge
    to adjust (fine-tune) to summarization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers go through the same process but in a different way.
  prefs: []
  type: TYPE_NORMAL
- en: The machine intelligence stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machines learn basic tasks like the ones described in this chapter, and then
    perform hundreds of tasks using the sequences they learned how to predict.
  prefs: []
  type: TYPE_NORMAL
- en: On the right side of *Figure 5.1*, we can see that the input for machines is
    second-hand information in the form of language. *Our output is the only input
    machines have to analyze language*.
  prefs: []
  type: TYPE_NORMAL
- en: At this point in human and machine history, computer vision identifies images
    but does not contain the grammatical structure of language. Speech recognition
    converts sound into words, which brings us back to written language. Music pattern
    recognition cannot lead to objective concepts expressed in words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machines start with a handicap. We impose an artificial disadvantage on them.
    Machines must rely on our random quality language outputs to:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform transductions connecting all the tokens (subwords) that occur together
    in language sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build inductions from these transductions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train those inductions based on tokens to produce patterns of tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s stop at this point and peek into the process of the attention sublayer,
    which works hard to produce valid inductions:'
  prefs: []
  type: TYPE_NORMAL
- en: The transformer model excluded the former recurrence-based learning operations
    and used self-attention to heighten the vision of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention sublayers have an advantage over humans at this point: they can process
    millions of examples for their inductive thinking operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like us, they find patterns in sequences through transduction and induction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They memorize these patterns using parameters that are stored with their model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They have acquired language understanding by using their abilities: substantial
    data volumes, excellent NLP transformer algorithms, and computer power. *Thanks
    to their deep understanding of language, they are ready to run hundreds of tasks
    they were not trained for*.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers, like humans, acquire language understanding through a limited
    number of tasks. Like us, they detect connections through transduction and then
    generalize them through inductive operations.
  prefs: []
  type: TYPE_NORMAL
- en: When the transformer model reaches the fine-tuning sub-layer of machine intelligence,
    it reacts like us. It does not start training from scratch to perform a new task.
    Like us, it considers it as a downstream task that only requires fine-tuning.
    If it needs to learn how to answer a question, it does not start learning a language
    from scratch. A transformer model just fine-tunes its parameters like us.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw that transformer models struggle to learn in the way
    we do. They start with a handicap from the moment they rely on our perceptions
    transcribed into language. However, they have access to infinitely more data than
    we do with massive computing power.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how to measure transformer performances versus Human Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer performances versus Human Baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers, like humans, can be fine-tuned to perform downstream tasks by
    inheriting the properties of a pretrained model. The pretrained model provides
    its architecture and language representations through its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A pretrained model trains on key tasks to acquire a general knowledge of the
    language. A fine-tuned model trains on downstream tasks. Not every transformer
    model uses the same tasks for pretraining. Potentially, all tasks can be pretrained
    or fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Every NLP model needs to be evaluated with a standard method.
  prefs: []
  type: TYPE_NORMAL
- en: This section will first go through some of the key measurement methods. Then,
    we will go through some of the main benchmark tasks and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by going through some of the key metric methods.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models with metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is impossible to compare one transformer model to another transformer model
    (or any other NLP model) without a universal measurement system that uses metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will analyze three measurement scoring methods that are
    used by GLUE and SuperGLUE.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The accuracy score, in whatever variant you use, is a practical evaluation.
    The score function calculates a straightforward true or false value for each result.
    Either the model’s outputs, ![](img/B17948_05_001.png), match the correct predictions,
    ![](img/B17948_05_002.png), for a given subset, *samples*[i], of a set of samples
    or not. The basic function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: We will obtain `1` if the result for the subset is correct and `0` if it is
    false.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now examine the more flexible F1-score.
  prefs: []
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The F1-score introduces a more flexible approach that can help when faced with
    datasets containing uneven class distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The F1-score uses the weighted values of precision and recall. It is a weighted
    average of precision and recall values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F1score= 2** (*precision * recall*)/(*precision + recall*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation, true (*T*) positives (*p*), false (*F*) positives (*p*),
    and false (*F*) negatives (*n*) are plugged into the precision (*P*) and recall
    (*R*) equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_09.png)![](img/B17948_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The F1-score can thus be viewed as the harmonic mean (reciprocal of the arithmetic
    mean) of precision (*P*) and recall (*R*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s now review the MCC approach.
  prefs: []
  type: TYPE_NORMAL
- en: Matthews Correlation Coefficient (MCC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MCC was described and implemented in the *Evaluating using Matthews Correlation
    Coefficient* section in *Chapter 3*, *Fine-Tuning BERT Models*. MCC computes a
    measurement with true positives (*T*[P]), true negatives (*T*[N]), false positives
    (*F*[P]), and false negatives (*F*[N]).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MCC can be summarized by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: MCC provides an excellent metric for binary classification models, even if the
    sizes of the classes are different.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good idea of how to measure a given transformer model’s results
    and compare them to other transformer models or NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: With measurement scoring methods in mind, let’s now look into benchmark tasks
    and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark tasks and datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Three prerequisites are required to prove that transformers have reached state-of-the-art
    performance levels:'
  prefs: []
  type: TYPE_NORMAL
- en: A model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset-driven task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metric as described in the *Evaluating models with metrics* section of this
    chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will begin by exploring the SuperGLUE benchmark to illustrate the evaluation
    process of a transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: From GLUE to SuperGLUE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SuperGLUE benchmark was designed and made public by *Wang* et al. (2019).
    *Wang* et al. (2019) first designed the **General Language Understanding Evaluation**
    (**GLUE**) benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of the GLUE benchmark was to show that to be useful, NLU has
    to be applicable to a wide range of tasks. Relatively small GLUE datasets were
    designed to encourage an NLU model to solve a set of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, the performance of NLU models, boosted by the arrival of transformers,
    began to exceed the level of the average human, as we can see in the GLUE leaderboard
    (December 2021). The GLUE leaderboard, available at [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard),
    shows a remarkable display of NLU talent, retaining some of the former RNN/CNN
    ideas while mainly focusing on the ground-breaking transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following excerpt of the leaderboard shows the top leaders and the position
    of GLUE’s Human Baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: GLUE Leaderboard – December 2021'
  prefs: []
  type: TYPE_NORMAL
- en: New models and the Human Baselines ranking will constantly change. These rankings
    just give an idea of how far classical NLP and transformers have taken us!
  prefs: []
  type: TYPE_NORMAL
- en: We first notice the GLUE Human Baselines are not in a top position, which shows
    that NLU models have surpassed non-expert humans on GLUE tasks. Human Baselines
    represent what we humans can achieve. AI can now outperform humans. In December
    2021, the Human Baselines are only in position 17\. This is a problem. Without
    a standard to beat, it is challenging to fish around for benchmark datasets to
    improve our models blindly.
  prefs: []
  type: TYPE_NORMAL
- en: We also notice that transformer models have taken the lead.
  prefs: []
  type: TYPE_NORMAL
- en: I like to think of GLUE and SuperGLUE as the point when words go from chaos
    to order with language understanding. For me, understanding is the glue that makes
    words fit together and become a language.
  prefs: []
  type: TYPE_NORMAL
- en: The GLUE leaderboard will continuously evolve as NLU progresses. However, *Wang*
    et al. (2019) introduced SuperGLUE to set a higher standard for Human Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing higher Human Baselines standards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Wang* et al. (2019) recognized the limits of GLUE. They designed SuperGLUE
    for more difficult NLU tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SuperGLUE immediately re-established Human Baselines at rank #1 (December 2020),
    as shown in the following excerpt of the leaderboard, [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: SuperGLUE Leaderboard 2.0 – December 2020'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the SuperGLUE leaderboard evolved as we produced better NLU models.
    In 2021, transformers had already surpassed Human Baselines. In December 2021,
    Human Baselines have gone down to rank #5, as shown in *Figure 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: SuperGLUE Leaderboard 2.0 – December 2021'
  prefs: []
  type: TYPE_NORMAL
- en: AI algorithm rankings will constantly change as new innovative models arrive.
    These rankings just give an idea of how hard the battle for NLP supremacy is being
    fought!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the evaluation process works.
  prefs: []
  type: TYPE_NORMAL
- en: The SuperGLUE evaluation process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wang et al. (2019) selected eight tasks for their SuperGLUE benchmark. The selection
    criteria for these tasks were stricter than for GLUE. For example, the tasks had
    to not only understand texts but also to reason. The level of reasoning is not
    that of a top human expert. However, the level of performance is sufficient to
    replace many human tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eight SuperGLUE tasks are presented in a ready-to-use list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: SuperGLUE tasks'
  prefs: []
  type: TYPE_NORMAL
- en: 'The task list is interactive: [https://super.gluebenchmark.com/tasks](https://super.gluebenchmark.com/tasks).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each task contains links to the required information to perform that task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name** is the name of the downstream task of a fine-tuned, pretrained model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifier** is the abbreviation or short version of the name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Download** is the download link to the datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More Info** offers greater detail through a link to the paper or website
    of the team that designed the dataset-driven task(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric** is the measurement score used to evaluate the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SuperGLUE provides the task instructions, the software, the datasets, and papers
    or websites describing the problem to be solved. Once a team runs the benchmark
    tasks and reaches the leaderboard, the results are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: SuperGLUE task scores'
  prefs: []
  type: TYPE_NORMAL
- en: SuperGLUE displays the overall score and the score for each task.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s take the instructions *Wang* et al. (2019) provided for the
    **Choice of Plausible Answers** (**COPA**) task in *Table 6* of their paper.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to read the remarkable paper written by *Roemmele* et al.
    (2011). In a nutshell, the goal is for the NLU model to demonstrate its machine
    thinking (not human thinking, of course) potential. In our case, the transformer
    must choose the most plausible answer to a question. The dataset provides a premise,
    and the transformer model must find the most plausible answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Premise: I knocked on my neighbor''s door.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`What happened as a result?`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Alternative 1: My neighbor invited me in.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Alternative 2: My neighbor left his house.`'
  prefs: []
  type: TYPE_NORMAL
- en: This question requires a second or two for a human to answer, which shows that
    it requires some commonsense machine thinking. `COPA.zip`, a ready-to-use dataset,
    can be downloaded directly from the SuperGLUE task page. The metric provided makes
    the process equal and reliable for all participants in the benchmark race.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples might seem difficult. However, transformers are nearing COPA Human
    Baselines (*Figure 5.7*), which is only at rank #5 if we take all of the tasks
    into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: SuperGLUE results for COPA'
  prefs: []
  type: TYPE_NORMAL
- en: As incredible as it seems, transformers climbed the leaderboard ladder in a
    very short time! And this is just the beginning. New ideas are emerging nearly
    every month!
  prefs: []
  type: TYPE_NORMAL
- en: We have introduced COPA. Let’s define the seven other SuperGLUE benchmark tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the SuperGLUE benchmark tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A task can be a pretraining task to generate a trained model. That same task
    can be a downstream task for another model that will fine-tune it. However, the
    goal of SuperGLUE is to show that a given NLU model can perform multiple downstream
    tasks with fine-tuning. Multi-task models are the ones that prove the thinking
    power of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The power of any transformer resides in its ability to perform multiple tasks
    using a pretrained model and then applying it to fine-tuned downstream tasks.
    The original Transformer model and its variants now lead in all the GLUE and SuperGLUE
    tasks. We will continue to focus on SuperGLUE downstream tasks for which Human
    Baselines is tough to beat.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we went through COPA. In this section, we will go through
    the seven other tasks defined by *Wang* et al. (2019) in *Table 2* of their paper.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue with a Boolean question task.
  prefs: []
  type: TYPE_NORMAL
- en: BoolQ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BoolQ is a Boolean yes-or-no answer task. The dataset, as defined on SuperGLUE,
    contains 15,942 naturally occurring examples. A raw sample of line `#3` of the
    `train.jsonl` dataset contains a passage, a question, and the answer (`true`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The datasets provided may change in time, but the concepts remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s examine CB, a task that requires both humans and machines to focus.
  prefs: []
  type: TYPE_NORMAL
- en: Commitment Bank (CB)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Commitment Bank** (**CB**) is a difficult *entailment* task. We are asking
    the transformer model to read a *premise*, then examine a *hypothesis* built on
    the premise. For example, the hypothesis will confirm the premise or contradict
    it. Then the transformer model must *label* the hypothesis as *neutral*, an *entailment*,
    or a *contradiction* of the premise, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains discourses, which are natural discourses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample `#77`, taken from the `train.jsonl` training dataset,
    shows how difficult the CB task is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will now have a look at the multi-sentence problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Sentence Reading Comprehension (MultiRC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Multi-Sentence Reading Comprehension** (**MultiRC**) asks the model to read
    a text and choose from several possible choices. The task is difficult for both
    humans and machines. The model is presented with a *text*, several *questions*,
    and possible *answers* to each question with a `0` (false) or `1` (true) *label*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the second sample in `train.jsonl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample contains four questions. To illustrate the task, we will just investigate
    two of them. The model must predict the correct labels. Notice how the information
    that the model is asked to obtain is distributed throughout the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, one can only admire the performance of a single fine-tuned, pretrained
    model on these difficult downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see the reading comprehension task.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reading Comprehension with Commonsense Reasoning Dataset** (**ReCoRD**) represents
    another challenging task. The dataset contains over 120,000 queries from more
    than 70,000 news articles. The transformer must use common-sense reasoning to
    solve this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine a sample from `train.jsonl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The *entities* are indicated, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the model must *answer* a *query* by finding the proper value for
    the *placeholder*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once the transformer model has gone through this problem, it must now face an
    entailment task.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing Textual Entailment (RTE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For **Recognizing Textual Entailment** (**RTE**), the transformer model must
    read the *premise*, examine a *hypothesis*, and predict the *label* of the *entailment
    hypothesis status*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine sample `#19` of the `train.jsonl` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: RTE requires understanding and logic. Let’s now see the Words in Context task.
  prefs: []
  type: TYPE_NORMAL
- en: Words in Context (WiC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Words in Context** (**WiC**) and the following Winograd task test a model’s
    ability to process an ambiguous word. In WiC, the multi-task transformer will
    have to analyze two sentences and determine whether the target word has the same
    meaning in both sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the first sample of the `train.jsonl` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the target word is specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The model has to read two sentences containing the target word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`train.jsonl` specifies the sample index, the value of the label, and the position
    of the target word in `sentence1(start1, end1)` and `sentence2(start2, end2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After this daunting task, the transformer model has to face the Winograd task.
  prefs: []
  type: TYPE_NORMAL
- en: The Winograd schema challenge (WSC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Winograd schema task is named after Terry Winograd. If a transformer is
    well trained, it should be able to solve disambiguation problems.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains sentences that target slight differences in the gender
    of a pronoun.
  prefs: []
  type: TYPE_NORMAL
- en: This constitutes a coreference resolution problem, which is one of the most
    challenging tasks to perform. However, the transformer architecture, which allows
    self-attention, is ideal for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Each sentence contains an *occupation*, a *participant*, and a *pronoun*. The
    problem to solve is to find whether the pronoun is *coreferent* with the occupation
    or the participant.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine a sample taken from `train.jsonl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the sample asks the model to read a *text*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have gone through some of the main SuperGLUE tasks. There are many other
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, once you understand the architecture of transformers and the mechanism
    of the benchmark tasks, you will rapidly adapt to any model and benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run some downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Running downstream tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will just jump into some transformer cars and drive them
    around a bit to see what they do. There are many models and tasks. We will run
    a few of them in this section. Once you understand the process of running a few
    tasks, you will quickly understand all of them. *After all, the human baseline
    for all these tasks is us!*
  prefs: []
  type: TYPE_NORMAL
- en: A downstream task is a fine-tuned transformer task that inherits the model and
    parameters from a pretrained transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: A downstream task is thus the perspective of a pretrained model running fine-tuned
    tasks. That means, depending on the model, a task is downstream if it was not
    used to fully pretrain the model. In this section, we will consider all the tasks
    as downstream since we did not pretrain them.
  prefs: []
  type: TYPE_NORMAL
- en: Models will evolve, as will databases, benchmark methods, accuracy measurement
    methods, and leaderboard criteria. But the structure of human thought reflected
    through the downstream tasks in this chapter will remain.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with CoLA.
  prefs: []
  type: TYPE_NORMAL
- en: The Corpus of Linguistic Acceptability (CoLA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Corpus of Linguistic Acceptability** (**CoLA**), a GLUE task, [https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks),
    contains thousands of samples of English sentences annotated for grammatical acceptability.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of *Alex Warstadt* et al. (2019) was to evaluate the linguistic competence
    of an NLP model to judge the linguistic acceptability of a sentence. The NLP model
    is expected to classify the sentences accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentences are labeled as grammatical or ungrammatical. The sentence is
    labeled `0` if the sentence is not grammatically acceptable. The sentence is labeled
    `1` if the sentence is grammatically acceptable. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification = `1` for ‘we yelled ourselves hoarse.’
  prefs: []
  type: TYPE_NORMAL
- en: Classification = `0` for ‘we yelled ourselves.’
  prefs: []
  type: TYPE_NORMAL
- en: 'You can go through `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb` in
    *Chapter 3*, *Fine-Tuning BERT Models*, to view the BERT model that we fine-tuned
    on CoLA datasets. We used CoLA data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We also load a pretrained BERT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the measurement method, or metric, we used is MCC, which was described
    in the *Evaluating using Matthews Correlation Coefficient* section of *Chapter
    3*, *Fine-Tuning BERT Models*, and earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to that section for the mathematical description of MCC and take
    the time to rerun the source code if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: A sentence can be grammatically unacceptable but still convey a sentiment. Sentiment
    analysis can add some form of empathy to a machine.
  prefs: []
  type: TYPE_NORMAL
- en: Stanford Sentiment TreeBank (SST-2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Stanford Sentiment TreeBank** (**SST-2**) contains movie reviews. In this
    section, we will describe the SST-2 (binary classification) task. However, the
    datasets go beyond that, and it is possible to classify sentiments in a range
    of *0* (negative) to *n* (positive).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Socher* et al. (2013) took sentiment analysis beyond the binary positive-negative
    NLP classification. We will explore the SST-2 multi-label sentiment classification
    with a transformer model in *Chapter 12*, *Detecting Customer Emotions to Make
    Predictions*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will run a sample taken from SST on a Hugging Face transformer
    pipeline model to illustrate binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Transformer_tasks.ipynb` and run the following cell, which contains positive
    and negative movie reviews taken from SST:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The SST-2 task is evaluated using the accuracy metric.
  prefs: []
  type: TYPE_NORMAL
- en: We classify sentiments of a sequence. Let’s now see whether two sentences in
    a sequence are paraphrases or not.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research Paraphrase Corpus (MRPC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Microsoft Research Paraphrase Corpus** (**MRPC**), a GLUE task, contains
    pairs of sentences extracted from new sources on the web. Each pair has been annotated
    by a human to indicate whether the sentences are equivalent based on two closely
    related properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Paraphrase equivalent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic equivalent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s run a sample using the Hugging Face BERT model. Open `Transformer_tasks.ipynb`
    and go to the following cell, and then run the sample taken from MRPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is accurate, though you may get messages warning you that the model
    needs more downstream training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The MRPC task is measured with the F1/Accuracy score method.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run a Winograd schema.
  prefs: []
  type: TYPE_NORMAL
- en: Winograd schemas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We described the Winograd schemas in this chapter’s *The Winograd schema challenge
    (WSC)* section. The training set was in English.
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if we ask a transformer model to solve a pronoun gender problem
    in an English-French translation? French has different spellings for nouns that
    have grammatical genders (feminine, masculine).
  prefs: []
  type: TYPE_NORMAL
- en: The following sentence contains the pronoun *it*, which can refer to the words
    *car* or *garage*. Can a transformer disambiguate this pronoun?
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Transformer_tasks.ipynb`, go to the `#Winograd` cell, and run our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The translation is perfect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformer detected that the word *it* refers to the word *car*, which
    is a feminine form. The feminine form applies to *it* and the adjective *big*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*elle* means *she* in French, which is the translation of *it*. The masculine
    form would have been *il*, which means *he*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*grosse* is the feminine form of the translation of the word *big*. Otherwise,
    the masculine form would have been *gros*.'
  prefs: []
  type: TYPE_NORMAL
- en: We gave the transformer a difficult Winograd schema to solve, and it produced
    the right answer.
  prefs: []
  type: TYPE_NORMAL
- en: There are many more dataset-driven NLU tasks available. We will explore some
    of them throughout this book to add more building blocks to our toolbox of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter analyzed the difference between the human language representation
    process and the way machine intelligence performs transduction. We saw that transformers
    must rely on the outputs of our incredibly complex thought processes expressed
    in written language. Language remains the most precise way to express a massive
    amount of information. The machine has no senses and must convert speech to text
    to extract meaning from raw datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored how to measure the performance of multi-task transformers.
    Transformers’ ability to obtain top-ranking results for downstream tasks is unique
    in NLP history. We went through the tough SuperGLUE tasks that brought transformers
    up to the top ranks of the GLUE and SuperGLUE leaderboards.
  prefs: []
  type: TYPE_NORMAL
- en: BoolQ, CB, WiC, and the many other tasks we covered are by no means easy to
    process, even for humans. We went through an example of several downstream tasks
    that show the difficulty transformer models face in proving their efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have proven their value by outperforming the former NLU architectures.
    To illustrate how simple it is to implement downstream fine-tuned tasks, we then
    ran several tasks in a Google Colaboratory notebook using Hugging Face’s pipeline
    for transformers.
  prefs: []
  type: TYPE_NORMAL
- en: In *Winograd schemas*, we gave the transformer the difficult task of solving
    a Winograd disambiguation problem for an English-French translation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Chapter 6*, *Machine Translation with the Transformer*,
    we will take translation tasks a step further and build a translation model with
    Trax.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine intelligence uses the same data as humans to make predictions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SuperGLUE is more difficult than GLUE for NLP models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BoolQ expects a binary answer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: WiC stands for Words in Context. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recognizing Textual Entailment** (**RTE**) detects whether one sequence entails
    another sequence. (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Winograd schema predicts whether a verb is spelled correctly. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human Baselines standards are not defined once and for all. They were made tougher
    to attain by SuperGLUE. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer models will never beat SuperGLUE Human Baselines standards. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variants of transformer models have outperformed RNN and CNN models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yu Sun*, *Shuohuan Wang*, *Yukun Li*, *Shikun Feng*, *Hao Tian*, *Hua Wu*,
    *Haifeng Wang*, 2019, *ERNIE 2.0: A Continual Pretraining Framework for Language
    Understanding*: [https://arxiv.org/pdf/1907.12412.pdf](https://arxiv.org/pdf/1907.12412.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Melissa Roemmele*, *Cosmin Adrian Bejan*, and *Andrew S. Gordon*, 2011, *Choice
    of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning*: [https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Richard Socher*, *Alex Perelygin*, *Jean Y. Wu*, *Jason Chuang*, *Christopher
    D. Manning*, *Andrew Y. Ng*, and *Christopher Potts*, 2013, *Recursive Deep Models
    for Semantic Compositionality Over a Sentiment Treebank*: [https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thomas Wolf*, *Lysandre Debut*, *Victor Sanh*, *Julien Chaumond*, *Clement
    Delangue*, *Anthony Moi*, *Pierric Cistac*, *Tim Rault*, *Rémi Louf*, *Morgan
    Funtowicz*, *Jamie Brew*, 2019, *HuggingFace’s Transformers: State-of-the-art
    Natural Language Processing*: [https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformer Usage: [https://huggingface.co/transformers/usage.html](https://huggingface.co/transformers/usage.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
