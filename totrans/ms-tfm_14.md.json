["```py\n!pip install bertviz ipywidgets\n```", "```py\nfrom bertviz import head_view\nfrom Transformers import BertTokenizer, BertModel\n```", "```py\n    def get_bert_attentions(model_path, sentence_a, sentence_b):\n        model = BertModel.from_pretrained(model_path,\n            output_attentions=True)\n        tokenizer = BertTokenizer.from_pretrained(model_path)\n        inputs = tokenizer.encode_plus(sentence_a,\n            sentence_b, return_tensors='pt',\n            add_special_tokens=True) \n        token_type_ids = inputs['token_type_ids']\n        input_ids = inputs['input_ids']\n        attention = model(input_ids,\n            token_type_ids=token_type_ids)[-1]\n        input_id_list = input_ids[0].tolist()\n        tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n        return attention, tokens\n    ```", "```py\n    model_path = 'bert-base-cased'\n    sentence_a = \"The cat is very sad.\"\n    sentence_b = \"Because it could not find food to eat.\"\n    attention, tokens=get_bert_attentions(model_path, \n        sentence_a, sentence_b)\n    head_view(attention, tokens)\n    ```", "```py\n    model_path = 'dbmdz/bert-base-turkish-cased'\n    sentence_a = \"Kedi çok üzgün.\"\n    sentence_b = \"Çünkü o her zamanki gibi çok fazla yemek yedi.\"\n    attention, tokens=\\\n    get_bert_attentions(model_path, sentence_a, sentence_b)\n    head_view(attention, tokens)\n    ```", "```py\n    model_path = 'bert-base-german-cased'\n    sentence_a = \"Die Katze ist sehr traurig.\"\n    sentence_b = \"Weil sie zu viel gegessen hat\"\n    attention, tokens=\\\n    get_bert_attentions(model_path, sentence_a, sentence_b)\n    head_view(attention, tokens)\n    ```", "```py\n    from bertviz import model_view\n    from Transformers import BertTokenizer, BertModel\n    ```", "```py\n    def show_model_view(model, tokenizer, sentence_a,\n         sentence_b=None, hide_delimiter_attn=False,\n         display_mode=\"dark\"):\n    . . . \n    ```", "```py\n    model_path='bert-base-german-cased'\n    sentence_a = \"Die Katze ist sehr traurig.\"\n    sentence_b = \"Weil sie zu viel gegessen hat\"\n    model = BertModel.from_pretrained(model_path, output_attentions=True)\n    tokenizer = BertTokenizer.from_pretrained(model_path)\n    show_model_view(model, tokenizer, sentence_a, sentence_b, \n        hide_delimiter_attn=False, \n        display_mode=\"light\")\n    ```", "```py\nfrom bertviz.Transformers_neuron_view import BertModel, BertTokenizer\nfrom bertviz.neuron_view import show\nmodel_path='bert-base-german-cased'\nsentence_a = \"Die Katze ist sehr traurig.\"\nsentence_b = \"Weil sie zu viel gegessen hat\"\nmodel = BertModel.from_pretrained(model_path, output_attentions=True)\ntokenizer = BertTokenizer.from_pretrained(model_path)\nmodel_type = 'bert'\nshow(model, model_type, tokenizer, sentence_a, sentence_b, layer=8, head=11)\n```", "```py\n    !pip install tensorboard \n    ```", "```py\n    from Transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(\n        output_dir='./MyIMDBModel', \n        do_train=True,\n        do_eval=True,\n        num_train_epochs=3, \n        per_device_train_batch_size=16, \n        per_device_eval_batch_size=32,\n        logging_strategy='steps', \n        logging_dir='./logs', \n        logging_steps=50,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"epoch\",\n        fp16=True,\n        load_best_model_at_end=True\n    )\n    ```", "```py\n    %reload_ext tensorboard\n    %tensorboard --logdir logs\n    ```", "```py\n    !pip install wandb\n    ```", "```py\n    import wandb\n    !wandb login\n    ```", "```py\n    !export WANDB_API_KEY=e7d*********\n    ```", "```py\n    training_args = TrainingArguments(\n        ...  the rest is same ...\n        run_name=\"IMDB-batch-32-lr-5e-5\",\n        report_to=\"wandb\"\n    )\n    ```", "```py\n    wandb.finish()\n    ```"]