["```py\n#The user can modify the values of the weight w \n#as well as biasValue1 and biasValue2 to observe\n#how this plots to different step functions\n\nimport numpy \nimport matplotlib.pyplot as plt\nweightValue = 1000\n#to be modified to change where the step function starts\nbiasValue1 = 5000 \n#to be modified to change where the step function ends \nbiasValue2 = -5000\n\nplt.axis([-10, 10, -1, 10])  \n\nprint (\"The step function starts at {0} and ends at {1}\"\n        .format(-biasValue1/weightValue, \n        -biasValue2/weightValue))  \n\ny1 = 1.0/(1.0 + numpy.exp(-weightValue*x - biasValue1)) \ny2 = 1.0/(1.0 + numpy.exp(-weightValue*x - biasValue2)) \n#to be modified to change the height of the step function \nw = 7 \ny = y1*w-y2*w  \nplt.plot(x, y, lw=2, color='black') \nplt.show()\n```", "```py\nimport numpy \nfrom matplotlib.colors import ListedColormap \nimport matplotlib.pyplot as plt \n```", "```py\ndef tanh(x):     \n    return (1.0 - numpy.exp(-2*x))/(1.0 + numpy.exp(-2*x))\n\ndef tanh_derivative(x):     \n    return (1 + tanh(x))*(1 - tanh(x))\n```", "```py\nclass NeuralNetwork:\n```", "```py\n    #net_arch consists of a list of integers, indicating \n    #the number of neurons in each layer, i.e. the network \n    #architecture\n    def __init__(self, net_arch): \n        self.activity = tanh\n        self.activity_derivative = tanh_derivative \n        self.layers = len(net_arch)         \n        self.steps_per_epoch = 1000   \n        self.arch = net_arch\n\n        self.weights = []         \n        #range of weight values (-1,1)         \n        for layer in range(self.layers - 1):             \n            w = 2*numpy.random.rand(net_arch[layer] + 1, net_arch[layer+1]) - 1\n            self.weights.append(w)\n```", "```py\n    #data is the set of all possible pairs of booleans\n    #True or False indicated by the integers 1 or 0 \n    #labels is the result of the logical operation 'xor' \n    #on each of those input pairs\n    def fit(self, data, labels, learning_rate=0.1, epochs=100):\n        #Add bias units to the input layer         \n        ones = numpy.ones((1, data.shape[0]))        \n        Z = numpy.concatenate((ones.T, data), axis=1)\n        training = epochs*self.steps_per_epoch\n        for k in range(training):             \n            if k % self.steps_per_epoch == 0:                  \n                print('epochs: {}'.format(k/self.steps_per_epoch))\n                for s in data:                     \n                    print(s, nn.predict(s))\n```", "```py\n               sample = numpy.random.randint(data.shape[0])\n               y = [Z[sample]]\n               for i in range(len(self.weights)-1):\n                   activation = numpy.dot(y[i], self.weights[i])\n                   activity = self.activity(activation)\n                   #add the bias for the next layer\n                   activity = numpy.concatenate((numpy.ones(1), \n                              numpy.array(activity)))\n                   y.append(activity)\n\n               #last layer \n               activation = numpy.dot(y[-1], self.weights[-1])\n               activity = self.activity(activation)\n               y.append(activity)\n```", "```py\n               #error for the output layer\n               error = labels[sample] - y[-1]\n               delta_vec = [error * self.activity_derivative(y[-1])] \n               #we need to begin from the back, \n               #from the next to last layer\n               for i in range(self.layers-2, 0, -1):  \n                   error = delta_vec[-1].dot(self.weights[i][1:].T) \n                   error = error*self.activity_derivative(y[i][1:])\n                   delta_vec.append(error)\n               #Now we need to set the values from back to front\n               delta_vec.reverse()\n\n               #Finally, we adjust the weights, \n               #using the backpropagation rules\n               for i in range(len(self.weights)):\n                   layer = y[i].reshape(1, nn.arch[i]+1)\n                   delta = delta_vec[i].reshape(1, nn.arch[i+1])\n                   self.weights[i] +=learning_rate*layer.T.dot(delta)\n```", "```py\n    def predict(self, x):          \n        val = numpy.concatenate((numpy.ones(1).T, numpy.array(x)))      \n        for i in range(0, len(self.weights)):             \n            val = self.activity(numpy.dot(val, self.weights[i]))             \n            val = numpy.concatenate((numpy.ones(1).T, \n                                     numpy.array(val)))         \n        return val[1]\n```", "```py\n    if __name__ == '__main__':  \n    numpy.random.seed(0)\n    #Initialize the NeuralNetwork with \n    #2 input neurons\n    #2 hidden neurons\n    #1 output neuron    \n    nn = NeuralNetwork([2,2,1])      \n    X = numpy.array([[0, 0],\n                    [0, 1],\n                    [1, 0],\n                    [1, 1]])\n\n    #Set the labels, the correct results for the xor operation    \n    y = numpy.array([0, 1, 1, 0])      \n\n    #Call the fit function and train the network \n    #for a chosen number of epochs\n    nn.fit(X, y, epochs=10)\n\n    print \"Final prediction\"     \n    for s in X:         \n       print(s, nn.predict(s))\n```"]