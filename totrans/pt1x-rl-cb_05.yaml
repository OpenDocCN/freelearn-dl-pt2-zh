- en: Solving Multi-armed Bandit Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-armed bandit algorithms are probably among the most popular algorithms
    in reinforcement learning. This chapter will start by creating a multi-armed bandit
    and experimenting with random policies. We will focus on how to solve the multi-armed
    bandit problem using four strategies, including epsilon-greedy, softmax exploration,
    upper confidence bound, and Thompson sampling. We will see how they deal with
    the exploration-exploitation dilemma in their own unique ways. We will also work
    on a billion-dollar problem, online advertising, and demonstrate how to solve
    it using a multi-armed bandit algorithm. Finally, we will solve the contextual
    advertising problem using contextual bandits to make more informed decisions in
    ad optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multi-armed bandit environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the epsilon-greedy policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with softmax exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with an upper confidence bound algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving internet advertising problems with the multi-armed bandit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the Thompson sampling algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving internet advertising problems with contextual bandits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a multi-armed bandit environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s get started with a simple project of estimating the value of π using the
    Monte Carlo method, which is the core of model-free reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**The** **multi-armed bandit** problem is one of the simplest reinforcement
    learning problems. It is best described as a slot machine with multiple levers
    (arms), and each lever has a different payout and payout probability. Our goal
    is to discover the best lever with the maximum return so that we can keep choosing
    it afterward. Let’s start with a simple multi-armed bandit problem in which the
    payout and payout probability is fixed for each arm. After creating the environment,
    we will solve it using the random policy algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s develop the multi-armed bandit environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The step method executes an action and returns the reward if it pays out, otherwise
    it returns 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use a multi-armed bandit as an example and solve it with random
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For example, there is a 10% chance of getting a reward of 4 by choosing arm
    0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the random policy, which randomly selects an arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the example we just worked on, there are three slot machines. Each machine
    has a different payout (reward) and payout probability. In each episode, we randomly
    chose one arm of the machine to pull (one action to execute) and get a payout
    at a certain probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the lines of code in *Step 5*; you will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b526f1a1-f16e-4d00-8f6a-c2e08fe8774e.png)'
  prefs: []
  type: TYPE_IMG
- en: Arm 1 is the best arm with the largest average reward. Also, the average rewards
    start to saturate round 10,000 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: This solution seems very naive as we only perform an exploration of all arms.
    We will come up with more intelligent strategies in the upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the epsilon-greedy policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of exploring solely with random policy, we can do better with a combination
    of exploration and exploitation. Here comes the well-known epsilon-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Epsilon-greedy for multi-armed bandits exploits the best action the majority
    of the time and also keeps exploring different actions from time to time. Given
    a parameter, ε, with a value from 0 to 1, the probabilities of performing exploration
    and exploitation are ε and 1 - ε, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Epsilon**: Each action is taken with a probability calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f07c030e-3e0d-42ba-a12c-11171c62525d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, |A| is the number of possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy**: The action with the highest state-action value is favored, and
    its probability of being chosen is increased by 1 - ε:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b76b5ffc-631a-46a9-867d-54c71bc2ab58.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the multi-armed bandit problem using the epsilon-greedy policy as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the PyTorch and the bandit environment we developed in the previous
    recipe, *Creating a multi-armed bandit environment* (assuming the `BanditEnv`
    class is in a file called `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the epsilon-greedy policy function, specify the value of epsilon, and
    create an epsilon-greedy policy instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `Q` function, which is the average reward obtained by individual
    arms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will update the `Q` function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to other MDP problems, the epsilon-greedy policy selects the best arm
    with a probability of 1 - ε and performs random exploration with a probability
    of ε. Epsilon manages the trade-off between exploration and exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 7*, you will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d2ef11b-8e61-4a96-9fbc-53652af32739.png)'
  prefs: []
  type: TYPE_IMG
- en: Arm 1 is the best arm, with the largest average reward at the end. Also, its
    average reward starts to saturate after around 1,000 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may wonder whether the epsilon-greedy policy actually outperforms the random
    policy. Besides the fact that the value for the optimal arm converges earlier
    with the epsilon-greedy policy, we can also prove that, on average, the reward
    we get during the course of training is higher with the epsilon-greedy policy
    than the random policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply average the reward over all episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Over 100,000 episodes, the average payout is `0.43718` with the epsilon-greedy
    policy. Repeating the same computation for the random policy solution, we get
    0.37902 as the average payout.
  prefs: []
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the softmax exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will solve the multi-armed bandit problem using the softmax
    exploration, algorithm. We will see how it differs from the epsilon-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen with epsilon-greedy, when performing exploration we randomly
    select one of the non-best arms with a probability of ε/|A|. Each non-best arm
    is treated equivalently regardless of its value in the Q function. Also, the best
    arm is chosen with a fixed probability regardless of its value. In **softmax exploration**,
    an arm is chosen based on a probability from the **softmax distribution** of the
    Q function values. The probability is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c1bf157-b0ab-4725-841d-7f8299aff6e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the τ parameter is the temperature factor, which specifies the randomness
    of the exploration. The higher the value of τ, the closer to equal exploration
    it becomes; the lower the value of τ, the more likely the best arm is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the multi-armed bandit problem using the softmax exploration algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the softmax exploration policy function, specify the value of τ, and
    create a softmax exploration policy instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the Q function, which is the average reward obtained by the individual
    arms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We will update the Q function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the softmax exploration strategy, the dilemma of exploitation and exploration
    is solved with a softmax function based on the Q values. Instead of using a fixed
    pair of probabilities for the best arm and non-best arms, it adjusts the probabilities
    according to the softmax distribution with the τ parameter as a temperature factor.
    The higher the value of τ, the more focus will be shifted to exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 7*, you will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85a2e1aa-29e4-4277-a3ca-811fcffeccec.png)'
  prefs: []
  type: TYPE_IMG
- en: Arm 1 is the best arm, with the largest average reward at the end. Also, its
    average reward starts to saturate after around 800 episodes in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the upper confidence bound algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two recipes, we explored random actions in the multi-armed bandit
    problem with probabilities that are either assigned as fixed values in the epsilon-greedy
    policy or computed based on the Q-function values in the softmax exploration algorithm.
    In either algorithm, the probabilities of taking random actions are not adjusted
    over time. Ideally, we want less exploration as learning progresses. In this recipe,
    we will use a new algorithm called **upper confidence bound** to achieve this
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **upper confidence bound** (**UCB**) algorithm stems from the idea of the
    confidence interval. In general, the confidence interval is a range of values
    where the true value lies. In the UCB algorithm, the confidence interval for an
    arm is a range where the mean reward obtained with this arm lies. The interval
    is in the form of [lower confidence bound, upper confidence bound] and we only
    use the upper bound, which is the UCB, to estimate the potential of the arm. The
    UCB is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dfdbc84-307b-4249-8a2b-0dd4f636dca4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, t is the number of episodes, and N(a) is the number of times arm a is
    chosen among t episodes. As learning progresses, the confidence interval shrinks
    and becomes more and more accurate. The arm to pull is the one with the highest
    UCB.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the multi-armed bandit problem using the UCB algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the Q function, which is the average reward obtained with individual
    arms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will update the Q function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes with our UCB policy. For each episode, we also
    update the statistics of each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solved the multi-armed bandit with the UCB algorithm. It
    adjusts the exploitation-exploration dilemma according to the number of episodes.
    For an action with a few data points, its confidence interval is relatively wide,
    hence, choosing this action is of relatively high uncertainty. With more episodes
    of the action being selected, the confidence interval becomes narrow and shrinks
    to its actual value. In this case, it is of high certainty to choose (or not)
    this action. Finally, the UCB algorithm pulls the arm with the highest UCB in
    each episode and gains more and more confidence over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the code in *Step 7*, you will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34c31cbd-e4b9-4415-aa1f-0a4313d1382d.png)'
  prefs: []
  type: TYPE_IMG
- en: Arm 1 is the best arm, with the largest average reward in the end.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may wonder whether UCB actually outperforms the epsilon-greedy policy. We
    can compute the average reward over the entire training process, and the policy
    with the highest average reward learns faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply average the reward over all episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Over 100,000 episodes, the average payout is 0.44605 with UCB, which is higher
    than 0.43718 with the epsilon-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For those who want to brush up on confidence intervals, feel free to check
    out the following: [http://www.stat.yale.edu/Courses/1997-98/101/confint.htm](http://www.stat.yale.edu/Courses/1997-98/101/confint.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: Solving internet advertising problems with a multi-armed bandit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine you are an advertiser working on ad optimization on a website:'
  prefs: []
  type: TYPE_NORMAL
- en: There are three different colors of ad background – red, green, and blue. Which
    one will achieve the best click-through rate (CTR)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are three types of wordings of the ad – *learn …*, *free ...*, and *try
    ...*. Which one will achieve the best CTR?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each visitor, we need to choose an ad in order to maximize the CTR over
    time. How can we solve this?
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you are thinking about A/B testing, where you randomly split the traffic
    into groups and assign each ad to a different group, and then choose the ad from
    the group with the highest CTR after a period of observation. However, this is
    basically a complete exploration, and we are usually unsure of how long the observation
    period should be and will end up losing a large portion of potential clicks. Besides,
    in A/B testing, the unknown CTR for an ad is assumed to not change over time.
    Otherwise, such A/B testing should be re-run periodically.
  prefs: []
  type: TYPE_NORMAL
- en: A multi-armed bandit can certainly do better than A/B testing. Each arm is an
    ad, and the reward for an arm is either 1 (click) or 0 (no click).
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to solve it with the UCB algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can solve the multi-armed bandit advertising problem using the UCB algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the three-armed bandit (three
    ad candidates, for example) and create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, the true CTR for ad 0 is 1%, for ad 1 1.5%, and for ad 2 3%.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the Q function, which is the average reward obtained by individual
    arms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We will update the Q function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes with the UCB policy. For each episode, we also
    update the statistics of each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solved the ad optimization problem in a multi-armed bandit
    manner. It overcomes the challenges confronting the A/B testing approach. We used
    the UCB algorithm to solve the multi-armed (multi-ad) bandit problem; the reward
    for each arm is either 1 or 0\. Instead of pure exploration and no interaction
    between action and reward, UCB (or other algorithms such as epsilon-greedy and
    softmax exploration) dynamically switches between exploitation and exploration
    where necessarly. For an ad with a few data points, the confidence interval is
    relatively wide, hence, choosing this action is of relatively high uncertainty.
    With more episodes of the ad being selected, the confidence interval becomes narrow
    and shrinks to its actual value.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the resulting plot in *Step 7* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/269802e0-7738-422f-aedb-183bdbfb4505.png)'
  prefs: []
  type: TYPE_IMG
- en: Ad 2 is the best ad with the highest predicted CTR (average reward) after the
    model converges.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we found that ad 2 is the optimal one to choose, which is true.
    Also, the sooner we figure this out the better, because we will lose fewer potential
    clicks. In this example, ad 2 outperformed the others after around 100 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Solving multi-armed bandit problems with the Thompson sampling algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will tackle the exploitation and exploration dilemma in the
    advertising bandits problem using another algorithm, Thompson sampling. We will
    see how it differs greatly from the previous three algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thompson sampling** (**TS**) is also called Bayesian bandits as it applies
    the Bayesian way of thinking from the following perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a probabilistic algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It computes the prior distribution for each arm and samples a value from each
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then selects the arm with the highest value and observes the reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it updates the prior distribution based on the observed reward. This
    process is called **Bayesian updating**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen that in our ad optimization case, the reward for each arm is
    either 1 or 0\. We can use **beta distribution** for our prior distribution because
    the value of the beta distribution is from 0 to 1\. The beta distribution is parameterized
    by two parameters, α and β. α represents the number of times we receive the reward
    of 1 and β, indicates the number of times we receive the reward of 0.
  prefs: []
  type: TYPE_NORMAL
- en: To help you understand the beta distribution better, we will start by looking
    at several beta distributions before we implement the TS algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s explore the beta distribution through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and matplotlib because we will visualize the shape of the distributions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by visualizing the shape of the beta distribution with the starting
    positions, α=1 and β=1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0960d6c4-036e-4345-a1c1-2851ef650ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, when α=1 and β=1, it doesn't provide any information about where
    the true value lies in the range of 0 to 1\. Hence, it becomes a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then visualize the shape of the beta distribution with α=5 and β=1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc882317-7591-47f4-a7d9-aec44de5bd29.png)'
  prefs: []
  type: TYPE_IMG
- en: When α=5 and β=1, this means that there are 4 consecutive rewards of 1 in 4
    experiments. The distribution shifts toward 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s experiment with α=1 and β=5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd081d9e-e4bb-4eeb-9605-46f95b53f71c.png)'
  prefs: []
  type: TYPE_IMG
- en: When α=1 and β=5, this means that there are 4 consecutive rewards of 0 in 4
    experiments. The distribution shifts toward 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we take a look at the situation when α=5 and β=5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa2fe6f2-4782-4c9a-aab7-d5a26db84f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: When α=5 and β=5, we observe the same numbers of clicks and no-clicks in 8 rounds.
    The distribution shifts toward the middle point, **0.5**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to solve the multi-armed bandit advertising problem using the
    Thompson sampling algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the bandit environment we developed in the first recipe, *Creating a
    multi-armed bandit environment* (assuming the `BanditEnv` class is in a file called
    `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the three-armed bandit (three
    ad candidates) and create an instance of the bandit environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the TS function, which samples a value from the beta distribution of
    each arm and selects the arm with the highest value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize α and β for each arm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that each beta distribution should start with α=β=1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes with the TS algorithm. For each episode, we also
    update α and β of each arm based on the observed reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solved the ad bandits problem with the TS algorithm. The
    biggest difference between TS and the three other approaches is the adoption of
    Bayesian optimization. It first computes the prior distribution for each possible
    arm, and then randomly draws a value from each distribution. It then picks the
    arm with the highest value and uses the observed outcome to update the prior distribution.
    The TS policy is both stochastic and greedy. If an ad is more likely to receive
    clicks, its beta distribution shifts toward 1 and, hence, the value of a random
    sample tends to be closer to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the lines of code in *Step 7*, you will see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9200fdd-cbcb-43cf-967e-ff9f581fb1e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Ad 2 is the best ad, with the highest predicted CTR (average reward).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For those who want to brush up on the beta distribution, feel free to check
    out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://varianceexplained.org/statistics/beta_distribution_and_baseball/](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving internet advertising problems with contextual bandits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may notice that in the ad optimization problem, we only care about the ad
    and ignore other information, such as user information and web page information,
    that might affect the ad being clicked on or not. In this recipe, we will talk
    about how we take more information into account beyond the ad itself and solve
    the problem with contextual bandits.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-armed bandit problems we have worked with so far do not involve the
    concept of state, which is very different from MDPs. We only have several actions,
    and a reward will be generated that is associated with the action selected. **Contextual
    bandits** extend multi-armed bandits by introducing the concept of state. State
    provides a description of the environment, which helps the agent take more informed
    actions. In the advertising example, the state could be the user's gender (two
    states, male and female), the user’s age group (four states, for example), or
    page category (such as sports, finance, or news). Intuitively, users of certain
    demographics are more likely to click on an ad on certain pages.
  prefs: []
  type: TYPE_NORMAL
- en: It is not difficult to understand contextual bandits. A multi-armed bandit is
    a single machine with multiple arms, while contextual bandits are a set of such
    machines (bandits). Each machine in contextual bandits is a state that has multiple
    arms. The learning goal is to find the best arm (action) for each machine (state).
  prefs: []
  type: TYPE_NORMAL
- en: We will work with an advertising example with two states for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the contextual bandits advertising problem using the UCB algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the payout probabilities and rewards for the two three-armed bandits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Here, the true CTR of ad 0 is 1%, of ad 1 is 1.5%, and of ad 2 is 3% for the
    first state, and [2.5%, 1%, and 1.5%] for the second state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of slot machines in our case is two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a list of bandits given the corresponding payout information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms in each state, the number of times
    individual arms are chosen in each state, and the average reward over time for
    each arm in each state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the Q function, which is the average reward obtained with individual
    arms for individual states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We will update the Q-function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run 100,000 episodes with the UCB policy. For each episode, we also
    update the statistics of each arm in each state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time for each state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solved the contextual advertising problem with contextual
    bandits using the UCB algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Running the lines of code in *Step 7*, you will see the following plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get this for the first state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9a9590f-38ca-4319-819f-58b8715f2fb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we get this for the second state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60b4910d-b5cf-41c8-be9e-1ef25c9c1e54.png)'
  prefs: []
  type: TYPE_IMG
- en: Given the first state, ad 2 is the best ad, with the highest predicted CTR.
    Given the second state, ad 0 is the optimal ad, with the highest average reward.
    And these are both true.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual bandits are a set of multi-armed bandits. Each bandit represents
    a unique state of the environment. The state provides a description of the environment,
    which helps the agent take more informed actions. In our advertising example,
    male users might be more likely to click an ad than female users. We simply used
    two slot machines to incorporate two states and searched for the best arm to pull
    given each state.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note is that contextual bandits are still different from MDPs,
    although they involve the concept of state. First, the states in contextual bandits
    are not determined by the previous actions or states, but are simply observations
    of the environment. Second, there is no delayed or discounted reward in contextual
    bandits because a bandit episode is one step. However, compared to multi-armed
    bandits, contextual bandits are closer to MDP as the actions are conditional to
    the states in the environment. It is safe to say that contextual bandits are in
    between multi-armed bandits and full MDP reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
