["```py\n    !pip install -q torch_snippets\n    from torch_snippets import *\n    from torchvision.datasets import MNIST\n    from torchvision import transforms\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    img_transform = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize([0.5], [0.5]),\n                        transforms.Lambda(lambda x: x.to(device))\n                    ]) \n    ```", "```py\n    trn_ds = MNIST('/content/', transform=img_transform, \\\n                   train=True, download=True)\n    val_ds = MNIST('/content/', transform=img_transform, \\\n                   train=False, download=True) \n    ```", "```py\n    batch_size = 256\n    trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True)\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False) \n    ```", "```py\n    class AutoEncoder(nn.Module):\n        def __init__(self, latent_dim):\n            super().__init__()\n            self.latend_dim = latent_dim\n            self.encoder = nn.Sequential(\n                                nn.Linear(28 * 28, 128), \n                                nn.ReLU(True),\n                                nn.Linear(128, 64), \n                                nn.ReLU(True), \n                                nn.Linear(64, latent_dim))\n            self.decoder = nn.Sequential(\n                                nn.Linear(latent_dim, 64), \n                                nn.ReLU(True),\n                                nn.Linear(64, 128), \n                                nn.ReLU(True), \n                                nn.Linear(128, 28 * 28), \n                                nn.Tanh()) \n    ```", "```py\n     def forward(self, x):\n            x = x.view(len(x), -1)\n            x = self.encoder(x)\n            x = self.decoder(x)\n            x = x.view(len(x), 1, 28, 28)\n            return x \n    ```", "```py\n    !pip install torch_summary\n    from torchsummary import summary\n    model = AutoEncoder(3).to(device)\n    summary(model, torch.zeros(2,1,28,28)) \n    ```", "```py\n    def train_batch(input, model, criterion, optimizer):\n        model.train()\n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, input)\n        loss.backward()\n        optimizer.step()\n        return loss \n    ```", "```py\n    @torch.no_grad()\n    def validate_batch(input, model, criterion):\n        model.eval()\n        output = model(input)\n        loss = criterion(output, input)\n        return loss \n    ```", "```py\n    model = AutoEncoder(3).to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.AdamW(model.parameters(), \\\n                                  lr=0.001, weight_decay=1e-5) \n    ```", "```py\n    num_epochs = 5\n    log = Report(num_epochs)\n    for epoch in range(num_epochs):\n        N = len(trn_dl)\n        for ix, (data, _) in enumerate(trn_dl):\n            loss = train_batch(data, model, criterion, optimizer)\n            log.record(pos=(epoch + (ix+1)/N), trn_loss=loss, end='\\r')\n\n        N = len(val_dl)\n        for ix, (data, _) in enumerate(val_dl):\n            loss = validate_batch(data, model, criterion)\n            log.record(pos=(epoch + (ix+1)/N), val_loss=loss, end='\\r')\n\n        log.report_avgs(epoch+1) \n    ```", "```py\n    log.plot_epochs(log=True) \n    ```", "```py\noutput:\n```", "```py\n    for _ in range(3):\n        ix = np.random.randint(len(val_ds))\n        im, _ = val_ds[ix]\n        _im = model(im[None])[0]\n        fig, ax = plt.subplots(1, 2, figsize=(3,3))\n        show(im[0], ax=ax[0], title='input')\n        show(_im[0], ax=ax[1], title='prediction')\n        plt.tight_layout()\n        plt.show() \n    ```", "```py\n    !pip install -q torch_snippets\n    from torch_snippets import *\n    from torchvision.datasets import MNIST\n    from torchvision import transforms\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    img_transform = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize([0.5], [0.5]),\n                        transforms.Lambda(lambda x: x.to(device))\n                                        ])\n    trn_ds = MNIST('/content/', transform=img_transform, \\\n                   train=True, download=True)\n    val_ds = MNIST('/content/', transform=img_transform, \\\n                   train=False, download=True)\n    batch_size = 128\n    trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True)\n    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False) \n    ```", "```py\n    class ConvAutoEncoder(nn.Module):\n        def __init__(self):\n            super().__init__() \n    ```", "```py\n     self.encoder = nn.Sequential(\n                                nn.Conv2d(1, 32, 3, stride=3, padding=1), \n                                nn.ReLU(True),\n                                nn.MaxPool2d(2, stride=2),\n                                nn.Conv2d(32, 64, 3, stride=2, padding=1), \n                                nn.ReLU(True),\n                                nn.MaxPool2d(2, stride=1)\n                            ) \n    ```", "```py\n     self.decoder = nn.Sequential(\n                            nn.ConvTranspose2d(64, 32, 3, stride=2), \n                            nn.ReLU(True),\n                            nn.ConvTranspose2d(32, 16, 5, stride=3,padding=1),\n                            nn.ReLU(True),\n                            nn.ConvTranspose2d(16, 1, 2, stride=2,padding=1), \n                            nn.Tanh()\n                        ) \n    ```", "```py\n     def forward(self, x):\n            x = self.encoder(x)\n            x = self.decoder(x)\n            return x \n    ```", "```py\n    model = ConvAutoEncoder().to(device)\n    !pip install torch_summary\n    from torchsummary import summary\n    summary(model, torch.zeros(2,1,28,28)); \n    ```", "```py\n    latent_vectors = []\n    classes = [] \n    ```", "```py\n    for im,clss in val_dl:\n        latent_vectors.append(model.encoder(im).view(len(im),-1))\n        classes.extend(clss) \n    ```", "```py\n    latent_vectors = torch.cat(latent_vectors).cpu().detach().numpy() \n    ```", "```py\n    from sklearn.manifold import TSNE\n    tsne = TSNE(2) \n    ```", "```py\n    clustered = tsne.fit_transform(latent_vectors) \n    ```", "```py\n    fig = plt.figure(figsize=(12,10))\n    cmap = plt.get_cmap('Spectral', 10)\n    plt.scatter(*zip(*clustered), c=classes, cmap=cmap)\n    plt.colorbar(drawedges=True) \n    ```", "```py\n    latent_vectors = []\n    classes = []\n    for im,clss in val_dl:\n        latent_vectors.append(model.encoder(im))\n        classes.extend(clss)\n    latent_vectors = torch.cat(latent_vectors).cpu()\\\n                          .detach().numpy().reshape(10000, -1) \n    ```", "```py\n    rand_vectors = []\n    for col in latent_vectors.transpose(1,0):\n        mu, sigma = col.mean(), col.std()\n        rand_vectors.append(sigma*torch.randn(1,100) + mu) \n    ```", "```py\n    rand_vectors=torch.cat(rand_vectors).transpose(1,0).to(device)\n    fig,ax = plt.subplots(10,10,figsize=(7,7)); ax = iter(ax.flat)\n    for p in rand_vectors:\n        img = model.decoder(p.reshape(1,64,2,2)).view(28,28)\n        show(img, ax=next(ax)) \n    ```", "```py\n    !pip install -q torch_snippets\n    from torch_snippets import *\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torchvision import datasets, transforms\n    from torchvision.utils import make_grid\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    train_dataset = datasets.MNIST(root='MNIST/', train=True, \\\n                                   transform=transforms.ToTensor(), \\\n                                   download=True)\n    test_dataset = datasets.MNIST(root='MNIST/', train=False, \\\n                                  transform=transforms.ToTensor(), \\\n                                  download=True)\n    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \\\n                                               batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(dataset= test_dataset, \\\n                                              batch_size=64, shuffle=False) \n    ```", "```py\n    class VAE(nn.Module):\n        def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n            super(VAE, self).__init__()\n            self.d1 = nn.Linear(x_dim, h_dim1)\n            self.d2 = nn.Linear(h_dim1, h_dim2)\n            self.d31 = nn.Linear(h_dim2, z_dim)\n            self.d32 = nn.Linear(h_dim2, z_dim)\n            self.d4 = nn.Linear(z_dim, h_dim2)\n            self.d5 = nn.Linear(h_dim2, h_dim1)\n            self.d6 = nn.Linear(h_dim1, x_dim) \n    ```", "```py\n     def encoder(self, x):\n            h = F.relu(self.d1(x))\n            h = F.relu(self.d2(h))\n            return self.d31(h), self.d32(h) \n    ```", "```py\n     def sampling(self, mean, log_var):\n            std = torch.exp(0.5*log_var)\n            eps = torch.randn_like(std)\n            return eps.mul(std).add_(mean) \n    ```", "```py\n     def decoder(self, z):\n            h = F.relu(self.d4(z))\n            h = F.relu(self.d5(h))\n            return F.sigmoid(self.d6(h)) \n    ```", "```py\n     def forward(self, x):\n            mean, log_var = self.encoder(x.view(-1, 784))\n            z = self.sampling(mean, log_var)\n            return self.decoder(z), mean, log_var \n    ```", "```py\n    def train_batch(data, model, optimizer, loss_function):\n        model.train()\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mean, log_var = model(data)\n        loss, mse, kld = loss_function(recon_batch, data, mean, log_var)\n        loss.backward()\n        optimizer.step()\n        return loss, mse, kld, log_var.mean(), mean.mean()\n    @torch.no_grad()\n    def validate_batch(data, model, loss_function):\n        model.eval()\n        data = data.to(device)\n        recon, mean, log_var = model(data)\n        loss, mse, kld = loss_function(recon, data, mean, log_var)\n        return loss, mse, kld, log_var.mean(), mean.mean() \n    ```", "```py\n    def loss_function(recon_x, x, mean, log_var):\n        RECON = F.mse_loss(recon_x, x.view(-1, 784), reduction='sum')\n        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n        return RECON + KLD, RECON, KLD \n    ```", "```py\n    vae = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=50).to(device)\n    optimizer = optim.AdamW(vae.parameters(), lr=1e-3) \n    ```", "```py\n    n_epochs = 10\n    log = Report(n_epochs)\n    for epoch in range(n_epochs):\n        N = len(train_loader)\n        for batch_idx, (data, _) in enumerate(train_loader):\n            loss, recon, kld, log_var, mean = train_batch(data, \\\n                                                          vae, optimizer, \\\n                                                          loss_function)\n            pos = epoch + (1+batch_idx)/N\n            log.record(pos, train_loss=loss, train_kld=kld, \\\n                       train_recon=recon,train_log_var=log_var, \\\n                       train_mean=mean, end='\\r')\n\n        N = len(test_loader)\n        for batch_idx, (data, _) in enumerate(test_loader):\n            loss, recon, kld,log_var,mean = validate_batch(data, \\\n                                               vae, loss_function)\n            pos = epoch + (1+batch_idx)/N\n            log.record(pos, val_loss=loss, val_kld=kld, \\\n                       val_recon=recon, val_log_var=log_var, \\\n                       val_mean=mean, end='\\r')\n\n        log.report_avgs(epoch+1)\n        with torch.no_grad():\n            z = torch.randn(64, 50).to(device)\n            sample = vae.decoder(z).to(device)\n            images = make_grid(sample.view(64, 1, 28, 28)).permute(1,2,0)\n            show(images)\n    log.plot_epochs(['train_loss','val_loss']) \n    ```", "```py\n    !pip install torch_snippets\n    from torch_snippets import inspect, show, np, torch, nn\n    from torchvision.models import resnet50\n    model = resnet50(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n    model = model.eval()\n    import requests\n    from PIL import Image\n    url = 'https://lionsvalley.co.za/wp-content/uploads/2015/11/african-elephant-square.jpg'\n    original_image = Image.open(requests.get(url, stream=True)\\\n                                .raw).convert('RGB')\n    original_image = np.array(original_image)\n    original_image = torch.Tensor(original_image) \n    ```", "```py\n    image_net_classes = 'https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt'\n    image_net_classes = requests.get(image_net_classes).text\n    image_net_ids = eval(image_net_classes)\n    image_net_classes = {i:j for j,i in image_net_ids.items()} \n    ```", "```py\n    from torchvision import transforms as T\n    from torch.nn import functional as F\n    normalize = T.Normalize([0.485, 0.456, 0.406], \n                            [0.229, 0.224, 0.225])\n    denormalize=T.Normalize( \\\n                    [-0.485/0.229,-0.456/0.224,-0.406/0.225], \n                    [1/0.229, 1/0.224, 1/0.225])\n    def image2tensor(input):\n        x = normalize(input.clone().permute(2,0,1)/255.)[None]\n        return x\n    def tensor2image(input):\n        x = (denormalize(input[0].clone()).permute(1,2,0)*255.)\\\n                                          .type(torch.uint8)\n        return x \n    ```", "```py\n    def predict_on_image(input):\n        model.eval()\n        show(input)\n        input = image2tensor(input)\n        pred = model(input)\n        pred = F.softmax(pred, dim=-1)[0]\n        prob, clss = torch.max(pred, 0)\n        clss = image_net_ids[clss.item()]\n        print(f'PREDICTION: `{clss}` @ {prob.item()}') \n    ```", "```py\n    from tqdm import trange\n    losses = []\n    def attack(image, model, target, epsilon=1e-6): \n    ```", "```py\n     input = image2tensor(image)\n        input.requires_grad = True \n    ```", "```py\n     pred = model(input)\n        loss = nn.CrossEntropyLoss()(pred, target) \n    ```", "```py\n     loss.backward()\n        losses.append(loss.mean().item()) \n    ```", "```py\n     output = input - epsilon * input.grad.sign() \n    ```", "```py\n     output = tensor2image(output)\n        del input\n        return output.detach() \n    ```", "```py\n    modified_images = []\n    desired_targets = ['lemon', 'comic book', 'sax, saxophone'] \n    ```", "```py\n    for target in desired_targets:\n        target = torch.tensor([image_net_classes[target]]) \n    ```", "```py\n     image_to_attack = original_image.clone()\n        for _ in trange(10):\n            image_to_attack = attack(image_to_attack,model,target)\n        modified_images.append(image_to_attack) \n    ```", "```py\n    for image in [original_image, *modified_images]:\n        predict_on_image(image)\n        inspect(image) \n    ```", "```py\n    !pip install torch_snippets\n    from torch_snippets import *\n    from torchvision import transforms as T\n    from torch.nn import functional as F\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    from torchvision.models import vgg19\n    preprocess = T.Compose([\n                    T.ToTensor(),\n                    T.Normalize(mean=[0.485, 0.456, 0.406], \n                                std=[0.229, 0.224, 0.225]),\n                    T.Lambda(lambda x: x.mul_(255))\n                ])\n    postprocess = T.Compose([\n                    T.Lambda(lambda x: x.mul_(1./255)),\n                    T.Normalize(\\\n                    mean=[-0.485/0.229,-0.456/0.224,-0.406/0.225], \n                                std=[1/0.229, 1/0.224, 1/0.225]),\n                ]) \n    ```", "```py\n    class GramMatrix(nn.Module):\n        def forward(self, input):\n            b,c,h,w = input.size()\n            feat = input.view(b, c, h*w)\n            G = feat@feat.transpose(1,2)\n            G.div_(h*w)\n            return G \n    ```", "```py\n    class GramMSELoss(nn.Module):\n        def forward(self, input, target):\n            out = F.mse_loss(GramMatrix()(input), target)\n            return(out) \n    ```", "```py\n    class vgg19_modified(nn.Module):\n        def __init__(self):\n            super().__init__() \n    ```", "```py\n     features = list(vgg19(pretrained = True).features)\n            self.features = nn.ModuleList(features).eval() \n    ```", "```py\n     def forward(self, x, layers=[]):\n            order = np.argsort(layers)\n            _results, results = [], []\n            for ix,model in enumerate(self.features):\n                x = model(x)\n                if ix in layers: _results.append(x)\n            for o in order: results.append(_results[o])\n            return results if layers is not [] else x \n    ```", "```py\n    vgg = vgg19_modified().to(device) \n    ```", "```py\n    !wget https://www.dropbox.com/s/z1y0fy2r6z6m6py/60.jpg\n    !wget https://www.dropbox.com/s/1svdliljyo0a98v/style_image.png \n    ```", "```py\n    imgs = [Image.open(path).resize((512,512)).convert('RGB') \\\n            for path in ['style_image.png', '60.jpg']]\n    style_image,content_image=[preprocess(img).to(device)[None] \\\n                                  for img in imgs] \n    ```", "```py\n    opt_img = content_image.data.clone()\n    opt_img.requires_grad = True \n    ```", "```py\n    style_layers = [0, 5, 10, 19, 28] \n    content_layers = [21]\n    loss_layers = style_layers + content_layers \n    ```", "```py\n    loss_fns = [GramMSELoss()] * len(style_layers) + \\\n                [nn.MSELoss()] * len(content_layers)\n    loss_fns = [loss_fn.to(device) for loss_fn in loss_fns] \n    ```", "```py\n    style_weights = [1000/n**2 for n in [64,128,256,512,512]] \n    content_weights = [1]\n    weights = style_weights + content_weights \n    ```", "```py\n    style_targets = [GramMatrix()(A).detach() for A in \\\n                     vgg(style_image, style_layers)]\n    content_targets = [A.detach() for A in \\\n                       vgg(content_image, content_layers)]\n    targets = style_targets + content_targets \n    ```", "```py\n    max_iters = 500\n    optimizer = optim.LBFGS([opt_img])\n    log = Report(max_iters) \n    ```", "```py\n    iters = 0\n    while iters < max_iters:\n        def closure():\n            global iters\n            iters += 1\n            optimizer.zero_grad()\n            out = vgg(opt_img, loss_layers)\n            layer_losses = [weights[a]*loss_fns[a](A,targets[a]) \\\n                            for a,A in enumerate(out)]\n            loss = sum(layer_losses)\n            loss.backward()\n            log.record(pos=iters, loss=loss, end='\\r')\n            return loss\n        optimizer.step(closure) \n    ```", "```py\n    log.plot(log=True) \n    ```", "```py\n    With torch.no_grad()\n        out_img = postprocess(opt_img[0]).permute(1,2,0)\n    show(out_img) \n    ```", "```py\n    import os\n    if not os.path.exists('Faceswap-Deepfake-Pytorch'):\n        !wget -q https://www.dropbox.com/s/5ji7jl7httso9ny/person_images.zip\n        !wget -q https://raw.githubusercontent.com/sizhky/deep-fake-util/main/random_warp.py\n        !unzip -q person_images.zip\n    !pip install -q torch_snippets torch_summary\n    from torch_snippets import *\n    from random_warp import get_training_data \n    ```", "```py\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \\\n                            'haarcascade_frontalface_default.xml') \n    ```", "```py\n    def crop_face(img):\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n        if(len(faces)>0):\n            for (x,y,w,h) in faces:\n                img2 = img[y:(y+h),x:(x+w),:]\n            img2 = cv2.resize(img2,(256,256))\n            return img2, True\n        else:\n            return img, False \n    ```", "```py\n    !mkdir cropped_faces_personA\n    !mkdir cropped_faces_personB\n    def crop_images(folder):\n        images = Glob(folder+'/*.jpg')\n        for i in range(len(images)):\n            img = read(images[i],1)\n            img2, face_detected = crop_face(img)\n            if(face_detected==False):\n                continue\n            else:\n                cv2.imwrite('cropped_faces_'+folder+'/'+str(i)+ \\\n                    '.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n    crop_images('personA')\n    crop_images('personB') \n    ```", "```py\n    class ImageDataset(Dataset):\n        def __init__(self, items_A, items_B):\n            self.items_A = np.concatenate([read(f,1)[None] \\\n                                           for f in items_A])/255.\n            self.items_B = np.concatenate([read(f,1)[None] \\\n                                           for f in items_B])/255.\n            self.items_A += self.items_B.mean(axis=(0, 1, 2)) \\\n                            - self.items_A.mean(axis=(0, 1, 2))\n        def __len__(self):\n            return min(len(self.items_A), len(self.items_B))\n        def __getitem__(self, ix):\n            a, b = choose(self.items_A), choose(self.items_B)\n            return a, b\n        def collate_fn(self, batch):\n            imsA, imsB = list(zip(*batch))\n            imsA, targetA = get_training_data(imsA, len(imsA))\n            imsB, targetB = get_training_data(imsB, len(imsB))\n            imsA, imsB, targetA, targetB = [torch.Tensor(i)\\\n                                            .permute(0,3,1,2)\\\n                                            .to(device) \\\n                                            for i in [imsA, imsB,\\\n                                            targetA, targetB]]\n            return imsA, imsB, targetA, targetB\n    a = ImageDataset(Glob('cropped_faces_personA'), \\\n                     Glob('cropped_faces_personB'))\n    x = DataLoader(a, batch_size=32, collate_fn=a.collate_fn) \n    ```", "```py\n    inspect(*next(iter(x)))\n    for i in next(iter(x)):\n        subplots(i[:8], nc=4, sz=(4,2)) \n    ```", "```py\n        def _ConvLayer(input_features, output_features):\n            return nn.Sequential(\n                nn.Conv2d(input_features, output_features, \n                          kernel_size=5, stride=2, padding=2),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n        def _UpScale(input_features, output_features):\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_features, output_features, \n                                 kernel_size=2, stride=2, padding=0),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n        class Reshape(nn.Module):\n            def forward(self, input):\n                output = input.view(-1, 1024, 4, 4) # channel * 4 * 4\n                return output \n        ```", "```py\n        class Autoencoder(nn.Module):\n            def __init__(self):\n                super(Autoencoder, self).__init__()\n                self.encoder = nn.Sequential(\n                                _ConvLayer(3, 128),\n                                _ConvLayer(128, 256),\n                                _ConvLayer(256, 512),\n                                _ConvLayer(512, 1024),\n                                nn.Flatten(),\n                                nn.Linear(1024 * 4 * 4, 1024),\n                                nn.Linear(1024, 1024 * 4 * 4),\n                                Reshape(),\n                                _UpScale(1024, 512),\n                            )\n                self.decoder_A = nn.Sequential(\n                                _UpScale(512, 256),\n                                _UpScale(256, 128),\n                                _UpScale(128, 64),\n                                nn.Conv2d(64, 3, kernel_size=3, padding=1),\n                                nn.Sigmoid(),\n                            )\n                self.decoder_B = nn.Sequential(\n                                _UpScale(512, 256),\n                                _UpScale(256, 128),\n                                _UpScale(128, 64),\n                                nn.Conv2d(64, 3, kernel_size=3, padding=1),\n                                nn.Sigmoid(),\n                            )\n            def forward(self, x, select='A'):\n                if select == 'A':\n                    out = self.encoder(x)\n                    out = self.decoder_A(out)\n                else:\n                    out = self.encoder(x)\n                    out = self.decoder_B(out)\n                return out \n        ```", "```py\n    from torchsummary import summary\n    model = Autoencoder()\n    summary(model, torch.zeros(32,3,64,64), 'A'); \n    ```", "```py\n    def train_batch(model, data, criterion, optimizes):\n        optA, optB = optimizers\n        optA.zero_grad()\n        optB.zero_grad()\n        imgA, imgB, targetA, targetB = data\n        _imgA, _imgB = model(imgA, 'A'), model(imgB, 'B')\n        lossA = criterion(_imgA, targetA)\n        lossB = criterion(_imgB, targetB)\n\n        lossA.backward()\n        lossB.backward()\n        optA.step()\n        optB.step()\n        return lossA.item(), lossB.item() \n    ```", "```py\n    model = Autoencoder().to(device)\n    dataset = ImageDataset(Glob('cropped_faces_personA'), \\\n                           Glob('cropped_faces_personB'))\n    dataloader = DataLoader(dataset, 32, collate_fn=dataset.collate_fn)\n    optimizers=optim.Adam( \\\n                    [{'params': model.encoder.parameters()}, \\\n                     {'params': model.decoder_A.parameters()}], \\\n                     lr=5e-5, betas=(0.5, 0.999)), \\\n            optim.Adam([{'params': model.encoder.parameters()}, \\\n                     {'params': model.decoder_B.parameters()}], \\\n                            lr=5e-5, betas=(0.5, 0.999))\n\n    criterion = nn.L1Loss() \n    ```", "```py\n    n_epochs = 1000\n    log = Report(n_epochs)\n    !mkdir checkpoint\n    for ex in range(n_epochs):\n        N = len(dataloader)\n        for bx,data in enumerate(dataloader):\n            lossA, lossB = train_batch(model, data, \n                                       criterion, optimizers)\n            log.record(ex+(1+bx)/N, lossA=lossA, \n                       lossB=lossB, end='\\r')\n        log.report_avgs(ex+1)\n        if (ex+1)%100 == 0:\n            state = {\n                    'state': model.state_dict(),\n                    'epoch': ex\n                }\n            torch.save(state, './checkpoint/autoencoder.pth')\n        if (ex+1)%100 == 0:\n            bs = 5\n            a,b,A,B = data\n            line('A to B')\n            _a = model(a[:bs], 'A')\n            _b = model(a[:bs], 'B')\n            x = torch.cat([A[:bs],_a,_b])\n            subplots(x, nc=bs, figsize=(bs*2, 5))\n            line('B to A')\n            _a = model(b[:bs], 'A')\n            _b = model(b[:bs], 'B')\n            x = torch.cat([B[:bs],_a,_b])\n            subplots(x, nc=bs, figsize=(bs*2, 5))\n    log.plot_epochs() \n    ```"]