["```py\nimport random\nimport numpy as np\n\nclass Bandit:\n    def __init__(self, K=2, probs=None):\n        self.K = K\n        if probs is None:\n            self.probs = [\n                random.random() for _ in range(self.K)\n            ]\n        else:\n            assert len(probs) == K\n            self.probs = probs\n\n        self.probs = list(np.array(probs) / np.sum(probs))\n        self.best_probs = max(self.probs)\n\n    def play(self, i):\n        if random.random() < self.probs[i]:\n            return 1\n        else:\n            return 0\n```", "```py\nclass Agent:\n    def __init__(self, env):        \n        self.env = env\n        self.listeners = {}\n        self.metrics = {}\n        self.reset()\n\n    def reset(self):\n        for k in self.metrics:\n            self.metrics[k] = []\n\n    def add_listener(self, name, fun):\n        self.listeners[name] = fun\n        self.metrics[name] = []\n\n    def run_metrics(self, i):\n        for key, fun in self.listeners.items():\n            fun(self, i, key)\n\n    def run_one_step(self):\n        raise NotImplementedError\n\n    def run(self, n_steps):\n        raise NotImplementedError\n```", "```py\n\nclass UCB1(Agent):\n    def __init__(self, env, alpha=2.):\n        self.alpha = alpha\n        super(UCB1, self).__init__(env)\n\n    def run_exploration(self):\n        for i in range(self.env.K):\n            self.estimates[i] = self.env.play(i)\n            self.counts[i] += 1\n            self.history.append(i)\n            self.run_metrics(i) \n            self.t += 1\n\n    def update_estimate(self, i, r):\n        self.estimates[i] += (r - self.estimates[i]) / (self.counts[i] + 1)\n\n    def reset(self):\n        self.history = []\n        self.t = 0\n        self.counts = [0] * self.env.K\n        self.estimates = [None] * self.env.K\n        super(UCB1, self).reset()\n\n    def run(self, n_steps):\n        assert self.env is not None\n        self.reset()\n        if self.estimates[0] is None:\n            self.run_exploration()\n        for _ in range(n_steps):\n            i = self.run_one_step()\n            self.counts[i] += 1\n            self.history.append(i)\n            self.run_metrics(i)\n\n    def upper_bound(self, i):\n        return np.sqrt(\n            self.alpha * np.log(self.t) / (1 + self.counts[i])\n        )\n\n    def run_one_step(self):\n        i = max(\n            range(self.env.K),\n            key=lambda i: self.estimates[i] + self.upper_bound(i)\n        )\n        r = self.env.play(i)\n        self.update_estimate(i, r)\n        self.t += 1\n        return i\n```", "```py\nfrom scipy import stats\n\ndef update_regret(agent, i, key):\n    regret = agent.env.best_probs - agent.env.probs[i]\n    if agent.metrics[key]:\n        agent.metrics[key].append(\n            agent.metrics[key][-1] + regret\n        )\n    else:\n        agent.metrics[key] = [regret]\n\ndef update_rank_corr(agent, i, key):\n    if agent.t < agent.env.K:\n        agent.metrics[key].append(0.0)\n    else:\n        agent.metrics[key].append(\n            stats.spearmanr(agent.env.probs, agent.estimates)[0]\n        )\n```", "```py\nrandom.seed(42.0)\nbandit = Bandit(20)\nagent = UCB1(bandit, alpha=2.0)\nagent.add_listener('regret', update_regret)\nagent.add_listener('corr', update_rank_corr)\nagent.run(5000)\n```", "```py\npip install gym\n```", "```py\nimport gym\n\nenv = gym.make('CartPole-v1')\nprint('observation space: {}'.format(\n    env.observation_space\n))\nprint('actions: {}'.format(\n    env.action_space.n\n))\n#observation space: Box(4,)\n#actions: 2\n```", "```py\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass PolicyNetwork(nn.Module):\n    def __init__(\n        self, lr, n_inputs,\n        n_hidden, n_actions\n    ):\n        super(PolicyNetwork, self).__init__()\n        self.lr = lr\n        self.fc1 = nn.Linear(n_inputs, n_hidden)\n        self.fc2 = nn.Linear(n_hidden, n_actions)\n        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n\n        self.device = T.device(\n            'cuda:0'\n            if T.cuda.is_available()\n            else 'cpu:0'\n        )\n        self.to(self.device)\n\n    def forward(self, observation):\n        x = T.Tensor(observation.reshape(-1).astype('float32'),\n        ).to(self.device)\n        x = F.relu(self.fc1(x))\n        x = F.softmax(self.fc2(x), dim=0)\n        return x\n```", "```py\nclass Agent:\n    eps = np.finfo(\n        np.float32\n    ).eps.item()\n\n    def __init__(self, env, lr, params, gamma=0.99):\n        self.env = env\n        self.gamma = gamma\n        self.actions = []\n        self.rewards = []\n        self.policy = PolicyNetwork(\n            lr=lr,\n            **params\n        )\n\n    def choose_action(self, observation):\n        output = self.policy.forward(observation)\n        action_probs = T.distributions.Categorical(\n            output\n        )\n        action = action_probs.sample()\n        log_probs = action_probs.log_prob(action)\n        action = action.item()\n        self.actions.append(log_probs)\n        return action, log_probs\n```", "```py\n    def run(self):\n        state = self.env.reset()\n        probs = []\n        rewards = []\n        done = False\n        observation = self.env.reset()\n        t = 0\n        while not done:\n            action, prob = self.choose_action(observation.reshape(-1))\n            probs.append(prob)\n            observation, reward, done, _ = self.env.step(action)\n            rewards.append(reward)\n            t += 1\n\n        policy_loss = []\n        returns = []\n        R = 0\n        for r in rewards[::-1]:\n            R = r + self.gamma * R\n            returns.insert(0, R)\n        returns = T.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n\n        for log_prob, R in zip(probs, returns):\n            policy_loss.append(-log_prob * R)\n\n        if(len(policy_loss)) > 0:\n            self.policy.optimizer.zero_grad()\n            policy_loss = T.stack(policy_loss, 0).sum()\n            policy_loss.backward()\n            self.policy.optimizer.step()\n        return t\n```", "```py\nenv._max_episode_steps = 10000\ninput_dims = env.observation_space.low.reshape(-1).shape[0]\nn_actions = env.action_space.n\n\nagent = Agent(\n    env=env,\n    lr=0.01,\n    params=dict(\n        n_inputs=input_dims,\n        n_hidden=10,\n        n_actions=n_actions\n    ),\n    gamma=0.99,\n)\nupdate_interval = 100\nscores = []\nscore = 0\nn_episodes = 25000\nstop_criterion = 1000\nfor i in range(n_episodes):\n    mean_score = np.mean(scores[-update_interval:])\n    if (i>0) and (i % update_interval) == 0:\n        print('Iteration {}, average score: {:.3f}'.format(\n            i, mean_score\n        ))\n        T.save(agent.policy.state_dict(), filename)\n\n    score = agent.run()\n    scores.append(score)\n    if score >= stop_criterion:\n        print('Stopping. Iteration {}, average score: {:.3f}'.format(\n            i, mean_score\n        ))\n        break\n```", "```py\nIteration 100, average score: 31.060\nIteration 200, average score: 132.340\nIteration 300, average score: 236.550\nStopping. Iteration 301, average score: 238.350\n```", "```py\nfrom IPython import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nobservation = env.reset()\nimg = plt.imshow(env.render(mode='rgb_array'))\nfor _ in range(100):\n    img.set_data(env.render(mode='rgb_array'))\n    display.display(plt.gcf())\n    display.clear_output(wait=True)\n    action, prob = agent.choose_action(observation)\n    observation, _, done, _ = agent.env.step(action)\n    if done:\n        break\n```", "```py\n!sudo apt-get install -y xvfb ffmpeg \n!pip install 'gym==0.10.11'\n!pip install 'imageio==2.4.0'\n!pip install PILLOW\n!pip install 'pyglet==1.3.2'\n!pip install pyvirtualdisplay\ndisplay = pyvirtualdisplay.Display(\n    visible=0, size=(1400, 900)\n).start()\n```", "```py\nimport ray\nfrom ray import tune\nfrom ray.rllib.agents.ppo import PPOTrainer\n\nray.init()\ntrainer = PPOTrainer\n\nanalysis = tune.run(\n    trainer,\n    stop={'episode_reward_mean': 100},\n    config={'env': 'CartPole-v0'},\n    checkpoint_freq=1,\n)\n```", "```py\npip install gym\n```", "```py\n#this is based on https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\nfrom collections import namedtuple\n\nTransition = namedtuple(\n    'Transition',\n    ('state', 'action', 'next_state', 'reward')\n)\n\nclass ReplayMemory:\n    def __init__(self, capacity=2000):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        batch = random.sample(self.memory, batch_size)\n        batch = Transition(\n            *(np.array(el).reshape(batch_size, -1) for el in zip(*batch))\n        )\n        return batch\n\n    def __len__(self):\n        return len(self.memory)\n```", "```py\nimport random\nimport numpy as np\nimport numpy.matlib\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import initializers\n\nclass DQNAgent():\n    def __init__(self, env, epsilon=1.0, lr=0.5, batch_size=128):\n        self.env = env\n        self.action_size = self.env.action_space.n\n        self.state_size = env.observation_space\n        self.memory = ReplayMemory()\n        self.epsilon = epsilon\n        self.lr = lr\n        self.batch_size = batch_size\n        self.model = self._build_model()\n\n    def encode(self, state, action=None):\n        if action is None:\n            action = np.reshape(\n                list(range(self.action_size)),\n                (self.action_size, 1)\n            )\n            return np.hstack([\n                np.matlib.repmat(state, self.action_size, 1),\n                action\n            ])\n        return np.hstack([state, action])\n\n    def play(self, state):\n        state = np.reshape(state, (1, 3)).astype(float)\n        if np.random.rand() <= self.epsilon:\n            action = np.random.randint(0, self.action_size)\n        else:\n            action_value = self.model.predict(self.encode(state)).squeeze()\n            action = np.argmax(action_value)\n\n        next_state1, reward, done, _ = self.env.step(action)\n        next_state = np.reshape(next_state1, (1, 3)).astype(float)\n        if done:\n            self.memory.push(state, action, next_state, reward)\n        return next_state1, reward, done\n\n    def learn(self):\n        if len(self.memory) < self.batch_size:\n            return\n        batch = self.memory.sample(\n            self.batch_size\n        )\n        result = self.model.fit(\n            self.encode(batch.state, batch.action),\n            batch.reward,\n            epochs=1,\n            verbose=0\n        )\n```", "```py\n    def _build_model(self):\n        model = tf.keras.Sequential([\n            layers.Dense(\n                100,\n                input_shape=(4,),\n                kernel_initializer=initializers.RandomNormal(stddev=5.0),\n                bias_initializer=initializers.Ones(),\n                activation='relu',\n                name='state'\n            ),\n            layers.Dense(\n                2,\n                activation='relu'\n            ),\n            layers.Dense(1, name='action', activation='tanh'),\n        ])\n        model.summary()\n        model.compile(\n            loss='hinge',\n            optimizer=optimizers.RMSprop(lr=self.lr)\n        )\n        return model\n```", "```py\nimport gym\nenv = gym.make('Blackjack-v0')\n\nagent = DQNAgent(\n    env=env, epsilon=0.01, lr=0.1, batch_size=100\n)\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nstate (Dense)                (None, 100)               500       \n_________________________________________________________________\ndense_4 (Dense)              (None, 2)                 202       \n_________________________________________________________________\naction (Dense)               (None, 1)                 3         \n=================================================================\nTotal params: 705\nTrainable params: 705\nNon-trainable params: 0\n```", "```py\nnum_rounds = 5000\nexploit_runs = num_rounds // 5\nbest_100 = -1.0\n\npayouts = []\nepsilons = np.hstack([\n np.linspace(0.5, 0.01, num=num_rounds - exploit_runs), \n np.zeros(exploit_runs)\n])\n```", "```py\nfrom tqdm.notebook import trange\n\nfor sample in trange(num_rounds):\n  epsilon = epsilons[sample]\n  agent.epsilon = epsilon\n  total_payout = 0\n  state = agent.env.reset()\n  for _ in range(10):\n    state, payout, done = agent.play(state)\n    total_payout += payout\n    if done:\n      break\n  if epsilon > 0:\n    agent.learn()\n\n  mean_100 = np.mean(payouts[-100:])\n    if mean_100 > best_100:\n      best_100 = mean_100\n\n  payouts.append(total_payout)\n  if (sample % 100) == 0 and sample >= 100:\n    print('average payout: {:.3f}'.format(\n      mean_100\n    ))\n    print(agent.losses[-1])\n\nprint('best 100 average: {:.3f}'.format(best_100))\n```"]