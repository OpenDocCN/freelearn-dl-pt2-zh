- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Detecting Customer Emotions to Make Predictions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测客户情绪以进行预测
- en: Sentiment analysis relies on the principle of compositionality. How can we understand
    a whole sentence if we cannot understand parts of a sentence? Is this tough task
    possible for NLP transformer models? We will try several transformer models in
    this chapter to find out.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析依赖于组合性原则。如果我们无法理解句子的部分，如何理解整个句子？这对于 NLP 变压器模型来说是一项困难的任务吗？我们将在本章中尝试几个变压器模型以找出答案。
- en: We will start with the **Stanford Sentiment Treebank** (**SST**). The SST provides
    datasets with complex sentences to analyze. It is easy to analyze sentences such
    as `The movie was great`. However, what happens if the task becomes very tough
    with complex sentences such as `Although the movie was a bit too long, I really
    enjoyed it.?` This sentence is segmented. It forces a transformer model to understand
    the structure of the sequence and its logical form.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 **斯坦福情感树库**（**SST**）开始。SST 提供了用于分析的复杂句子数据集。分析诸如 `电影很棒` 这样的句子很容易。但是，如果任务变得非常困难，如
    `虽然电影有点太长，但我真的很喜欢它。` 这个句子被分割了。它迫使一个变压器模型理解序列的结构和逻辑形式。
- en: We will then test several transformer models with complex sentences and simple
    sentences. We will find that no matter which model we try, it will not work if
    it isn’t trained enough. Transformer models are like us. They are students that
    need to work hard to learn and try to reach real-life human baselines.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将测试几个变压器模型，包括复杂句子和简单句子。我们会发现，无论我们尝试哪种模型，如果没有足够的训练，它都不会起作用。变压器模型就像我们一样。它们是需要努力学习并尝试达到现实生活中人类基准的学生。
- en: Running DistilBERT, RoBERTa-large, BERT-base, MiniLM-L12-H84-uncased, and BERT-base
    multilingual models is fun! However, we will discover that some of these students
    require more training, just like we would.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 DistilBERT、RoBERTa-large、BERT-base、MiniLM-L12-H84-uncased 和 BERT-base 多语言模型非常有趣！然而，我们会发现其中一些模型需要更多的训练，就像我们一样。
- en: Along the way, we will see how to use the output of the sentiment tasks to improve
    customer relationships and see a nice five-star interface you could implement
    on your website.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们将看到如何利用情感任务的输出来改善客户关系，并了解一个您可以在网站上实现的漂亮的五星级界面。
- en: Finally, we will use GPT-3’s online interface for sentiment analysis with an
    OpenAI account. No AI development or API is required!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用 GPT-3 的在线界面进行情感分析，无需 OpenAI 账户的 AI 开发或 API！
- en: 'This chapter covers the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: The SST for sentiment analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于情感分析的 SST
- en: Defining compositionality for long sequences
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为长序列定义组合性
- en: Sentiment analysis with AllenNLP (RoBERTa)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AllenNLP 进行情感分析（RoBERTa）
- en: Running complex sentences to explore the new frontier of transformers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行复杂句子，探索变压器的新领域
- en: Using Hugging Face sentiment analysis models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 情感分析模型
- en: DistilBERT for sentiment analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT 用于情感分析
- en: Experimenting with MiniLM-L12-H384-uncased
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试 MiniLM-L12-H384-uncased
- en: Exploring RoBERTa-large-mnli
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 RoBERTa-large-mnli
- en: Looking into a BERT-base multilingual model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探究 BERT-base 多语言模型
- en: Sentiment analysis with GPT-3
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT-3 进行情感分析
- en: Let’s begin by going through the SST.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 SST 开始。
- en: 'Getting started: Sentiment analysis transformers'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门：情感分析 transformers
- en: This section will first explore the SST that the transformers will use to train
    models on sentiment analysis.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将首先探索变压器将用于情感分析的 SST。
- en: We will then use AllenNLP to run a RoBERTa-large transformer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将使用 AllenNLP 运行一个 RoBERTa-large transformer。
- en: The Stanford Sentiment Treebank (SST)
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福情感树库（SST）
- en: '*Socher* et al. (2013) designed semantic word spaces over long phrases. They
    defined principles of *compositionality* applied to long sequences. The principle
    of compositionality means that an NLP model must examine the constituent expressions
    of a complex sentence and the rules that combine them to understand the meaning
    of a sequence.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*Socher* 等人（2013）设计了长短语的语义词空间。他们定义了应用于长序列的 *组合性* 原则。组合性原则意味着 NLP 模型必须检查复杂句子的组成表达式以及组合它们的规则，以理解序列的含义。'
- en: Let’s take a sample from the SST to grasp the meaning of the principle of compositionality.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 SST 中取样以掌握组合性原则的含义。
- en: This section and chapter are self-contained, so you can choose to perform the
    actions described or read the chapter and view the screenshots provided.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节和本章内容独立完整，您可以选择执行所述的操作，或者阅读章节并查看提供的屏幕截图。
- en: 'Go to the interactive sentiment treebank: [https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33](https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前往交互式情感树库：[https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33](https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33)。
- en: 'You can make the selections you wish. Graphs of sentiment trees will appear
    on the page. Click on an image to obtain a sentiment tree:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进行你想要的选择。情感树图将显示在页面上。点击图像以获取情感树：
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_12_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 自动生成描述](img/B17948_12_01.png)'
- en: 'Figure 12.1: Graphs of sentiment trees'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：情感树图
- en: 'For this example, I clicked on graph number 6, which contains a sentence mentioning
    `Jacques Derrida, a pioneer in deconstruction theories in linguistics`. A long,
    complex sentence appears:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我点击了包含提到`在语言学中解构理论的先驱雅克·德里达`的句子的第6图表。出现了一个又长又复杂的句子：
- en: '`Whether or not you''re enlightened by any of Derrida''s lectures on the other
    and the self, Derrida is an undeniably fascinating and playful fellow.`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是否受到德里达关于“他者”和“自我”的任何讲座的启示，德里达无疑是一个极具吸引力和富有趣味的人。
- en: '*Socher* et al. (2013) worked on compositionality in vector spaces and logic
    forms.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*Socher*等人（2013）致力于向量空间和逻辑形式中的组合性。'
- en: 'For example, defining the rule of logic that governs the Jacques Derrida sample
    implies an understanding of the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，定义统治雅克·德里达样本的逻辑规则意味着理解以下内容：
- en: How the words `Whether`, `or`, and `not` and the comma that separates the `Whether`
    phrase from the rest of the sentence can be interpreted
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解释单词`无论`、`还是`和`不`以及将`无论`短语与句子其他部分分开的逗号
- en: How to understand the second part of the sentence after the comma with yet another
    `and`!
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何理解逗号后句子的第二部分带有另一个`和`！
- en: Once the vector space was defined, *Socher* et al. (2013) could produce complex
    graphs representing the principle of compositionality.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦向量空间被定义，*Socher*等人（2013）可以生成代表组合原则的复杂图表。
- en: 'We can now view the graph section by section. The first section is the `Whether`
    segment of the sentence:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以逐段查看图表。第一部分是句子的`无论`部分：
- en: '![Diagram  Description automatically generated](img/B17948_12_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图表 自动生成描述](img/B17948_12_02.png)'
- en: 'Figure 12.2: The “Whether” segment of a complex sentence'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：复杂句子中的“无论”部分
- en: 'The sentence has been correctly split into two main parts. The second segment
    is also correct:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 句子已被正确地分成两个主要部分。第二部分也是正确的：
- en: '![Chart, radar chart, scatter chart  Description automatically generated](img/B17948_12_03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图表，雷达图，散点图 自动生成描述](img/B17948_12_03.png)'
- en: 'Figure 12.3: The main segment of a complex sentence'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：复杂句子的主要部分
- en: 'We can draw several conclusions from the method *Socher* et al. (2013) designed:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从*Socher*等人（2013）设计的方法中得出几个结论：
- en: Sentiment analysis cannot be reduced to counting positive and negative words
    in a sentence
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析不能简化为在句子中计算积极和消极的词语
- en: A transformer model or any NLP model must be able to learn the principle of
    compositionality to understand how the constituents of a complex sentence fit
    together with logical form rules
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个变压器模型或任何NLP模型必须能够学习组合原则，以理解复杂句子的构成成分如何与逻辑形式规则相互契合
- en: A transformer model must be able to build a vector space to interpret the subtilities
    of a complex sentence
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个变压器模型必须能够建立一个向量空间来解释复杂句子的微妙之处
- en: We will now put this theory into practice with a RoBERTa-large model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将用一个RoBERTa-large模型将这一理论付诸实践。
- en: Sentiment analysis with RoBERTa-large
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RoBERTa-large进行情感分析
- en: In this section, we will use the AllenNLP resources to run a RoBERTa-large transformer.
    *Liu* et al. (2019) analyzed the existing BERT models and found that they were
    not trained as well as expected. Considering the speed at which the models were
    produced, this was not surprising. They worked on improving the pretraining of
    BERT models to produce a **Robustly Optimized BERT Pretraining Approach** (**RoBERTa**).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用AllenNLP资源运行RoBERTa-large变压器。*Liu*等人（2019）分析了现有的BERT模型，并发现它们并没有训练得像预期的那样好。考虑到这些模型的生成速度，这并不令人惊讶。他们致力于改进BERT模型的预训练，以产生一个**强大优化的BERT预训练方法**（**RoBERTa**）。
- en: Let’s first run a RoBERTa-large model in `SentimentAnalysis.ipynb`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先在`SentimentAnalysis.ipynb`中运行一个RoBERTa-large模型。
- en: 'Run the first cell to install `allennlp-models`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 运行第一个单元格以安装`allennlp-models`：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let’s try to run our Jacques Derrida sample:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试运行我们的雅克·德里达样本：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output first displays the architecture of the RoBERTa-large model, which
    has `24` layers and `16` attention heads:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出首先显示 RoBERTa-large 模型的架构，该模型有`24`层和`16`个注意力头：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If necessary, you can take a few minutes to go through the description of a
    BERT architecture in the *BERT model configuration* section in *Chapter 3*, *Fine-Tuning
    BERT Models*, to take full advantage of this model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以花几分钟阅读 *第 3 章* *Fine-Tuning BERT Models* 中的 *BERT model configuration*
    部分的 BERT 架构描述，以充分利用该模型。
- en: Sentiment analysis produces values between `0` (negative) and `1` (positive).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析会产生介于`0`（负面）和`1`（正面）之间的数值。
- en: 'The output then produces the result of the sentiment analysis task, displaying
    the output logits and the final positive result:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出然后产生情感分析任务的结果，显示输出 logits 和最终的正面结果：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note: The algorithm is stochastic so the ouputs may vary from one run to another.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：算法是随机的，因此输出可能会在每次运行时变化。
- en: 'The output also contains the token IDs (which may vary from one run to another)
    and the final output label:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还包含令牌 ID（可能会在每次运行时变化）和最终的输出标签：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output also displays the tokens themselves:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还显示了令牌本身：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Take some time to enter some samples to explore the well-designed and pretrained
    RoBERTa model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 花些时间输入一些样本来探索设计良好且预训练的 RoBERTa 模型。
- en: Now let’s see how we can use sentiment analysis to predict customer behavior
    with other transformer models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用其他 transformer 模型来使用情感分析来预测客户行为。
- en: Predicting customer behavior with sentiment analysis
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用情感分析预测客户行为
- en: This section will run a sentiment analysis task on several Hugging Face transformer
    models to see which ones produce the best results and which ones we simply like
    the best.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分将在几个 Hugging Face transformer 模型上运行情感分析任务，以查看哪些模型产生了最佳结果，哪些模型我们简单地喜欢。
- en: We will begin this by using a Hugging Face DistilBERT model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Hugging Face 的 DistilBERT 模型开始此过程。
- en: Sentiment analysis with DistilBERT
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DistilBERT 进行情感分析
- en: Let’s run a sentiment analysis task with DistilBERT and see how we can use the
    result to predict customer behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 DistilBERT 运行情感分析任务，并看看我们如何使用结果来预测客户行为。
- en: 'Open `SentimentAnalysis.ipynb` and the transformer installation and import
    cells:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `SentimentAnalysis.ipynb` 和变压器安装和导入单元格：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will now create a function named `classify`, which will run the model with
    the sequences we send to it:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个名为 `classify` 的函数，它将使用我们发送给它的序列运行模型：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that if you send `M=1` to the function, it will display the configuration
    of the DistilBERT 6-layer, 12-head model we are using:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您将 `M=1` 发送给该函数，它将显示我们正在使用的 DistilBERT 6 层、12 个头模型的配置：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The specific parameters of this DistilBERT model are the label definitions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此 DistilBERT 模型的具体参数是标签定义。
- en: 'We now create a list of sequences (you can add more) that we can send to the
    `classify` function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个序列列表（您可以添加更多），我们可以将其发送给 `classify` 函数：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case, `seq=3` is activated to simulate a customer issue we need to
    take into account. The output is negative, which is the example we are looking
    for:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`seq=3` 被激活以模拟我们需要考虑的客户问题。 输出为负面，这正是我们要寻找的示例：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can draw several conclusions from this result to predict customer behavior
    by writing a function that would:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个结果中得出几个结论，以预测客户行为，编写一个函数来：
- en: Store the predictions in the customer management database.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预测存储在客户管理数据库中。
- en: Count the number of times a customer complains about a service or product in
    a period (week, month, year). A customer that complains often might switch to
    a competitor to get a better product or service.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计客户在某段时间内（周、月、年）投诉服务或产品的次数。 经常投诉的客户可能会转向竞争对手以获得更好的产品或服务。
- en: Detect the products and services that keep occurring in negative feedback messages.
    The product or service might be faulty and require quality control and improvements.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测在负面反馈信息中不断出现的产品和服务。 产品或服务可能存在缺陷，需要质量控制和改进。
- en: You can take a few minutes to run other sequences or create some sequences to
    explore the DistilBERT model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以花几分钟运行其他序列或创建一些序列来探索 DistilBERT 模型。
- en: We will now explore other Hugging Face transformers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探索其他 Hugging Face transformers。
- en: Sentiment analysis with Hugging Face’s models’ list
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 模型列表进行情感分析
- en: This section will explore Hugging Face’s transformer models list and enter some
    samples to evaluate their results. The idea is to test several models, not only
    one, and see which model fits your need the best for a given project.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探索 Hugging Face 的变压器模型列表，并输入一些样本以评估它们的结果。想法是测试多个模型，而不仅仅是一个，并查看哪个模型最适合您的给定项目需求。
- en: 'We will be running Hugging Face models: [https://huggingface.co/models](https://huggingface.co/models).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行 Hugging Face 模型：[https://huggingface.co/models](https://huggingface.co/models)。
- en: 'For each model we use, you can find the description of the model in the documentation
    provided by Hugging Face: [https://huggingface.co/transformers/](https://huggingface.co/transformers/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们使用的每个模型，您可以在 Hugging Face 提供的文档中找到模型的描述：[https://huggingface.co/transformers/](https://huggingface.co/transformers/)。
- en: 'We will test several models. If you implement them, you might find that they
    require fine-tuning or even pretraining for the NLP tasks you wish to perform.
    In that case, for Hugging Face transformers, you can do the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试几个模型。如果您实现它们，您可能会发现它们需要微调甚至是对您希望执行的 NLP 任务进行预训练。在这种情况下，对于 Hugging Face
    变压器，您可以执行以下操作：
- en: For fine-tuning, you can refer to *Chapter 3*, *Fine-Tuning BERT Models*
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于微调，您可以参考*第 3 章*，*对 BERT 模型进行微调*
- en: For pretraining, you can refer to *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于预训练，您可以参考*第 4 章*，*从头开始预训练 RoBERTa 模型*
- en: 'Let’s first go through the list of Hugging Face models: [https://huggingface.co/models](https://huggingface.co/models).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先浏览 Hugging Face 模型列表：[https://huggingface.co/models](https://huggingface.co/models)。
- en: 'Then, select **Text Classification** in the **Tasks** pane:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在**任务**窗格中选择**文本分类**：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_04.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本说明的图像](img/B17948_12_04.png)'
- en: 'Figure 12.4: Selecting text classification models'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：选择文本分类模型
- en: 'A list of transformer models trained for text classification will appear:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将显示一系列用于文本分类的变压器模型：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本说明的图像](img/B17948_12_05.png)'
- en: 'Figure 12.5: Hugging Face pretrained text-classification models'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：Hugging Face 预训练的文本分类模型
- en: 'The default sort mode is **Sort: Most downloads**.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 默认排序模式为**排序：最多下载**。
- en: We will now search for some exciting transformer models we can test online.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将搜索一些有趣的变压器模型，我们可以在线测试。
- en: We will begin with DistilBERT.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 DistilBERT 开始。
- en: DistilBERT for SST
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DistilBERT 用于 SST
- en: The `distilbert-base-uncased-finetuned-sst-2-english` model was fine-tuned on
    the SST.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`distilbert-base-uncased-finetuned-sst-2-english` 模型在 SST 上进行了微调。'
- en: 'Let’s try an example that requires a good understanding of the principles of
    compositionality:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试一个需要对组成原理有良好理解的例子：
- en: '`"Though the customer seemed unhappy, she was, in fact satisfied but thinking
    of something else at the time, which gave a false impression."`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`"尽管客户似乎不满意，但事实上她是满意的，只是当时在想其他事情，这给了一个错误的印象。"`'
- en: This sentence is tough for a transformer to analyze and requires logical rule
    training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子对于变压器来说很难分析，需要逻辑规则训练。
- en: 'The output is a false negative:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个假阴性：
- en: '![](img/B17948_12_06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_12_06.png)'
- en: 'Figure 12.6: The output of a complex sequence classification task'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：复杂序列分类任务的输出
- en: A false negative does not mean that the model is not working correctly. We could
    choose another model. However, it could mean that we must download and train it
    longer and better!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性并不意味着模型工作不正确。我们可以选择另一个模型。然而，这可能意味着我们必须下载并训练它更长时间更好！
- en: At the time of writing this book, BERT-like models have good rankings on both
    the GLUE and SuperGLUE leaderboards. The rankings will continuously change but
    not the fundamental concepts of transformers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，类似 BERT 的模型在 GLUE 和 SuperGLUE 排行榜上排名靠前。排名将不断变化，但变压器的基本概念不会改变。
- en: We will now try a difficult but less complicated example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试一个困难但不那么复杂的示例。
- en: This example is a crucial lesson for real-life projects. When we try to estimate
    how many times a customer complained, we will get both false negatives and false
    positives. *Therefore, regular human intervention will still be mandatory for
    several more years*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子是真实项目的一个关键教训。当我们尝试估计客户投诉的次数时，我们将得到假阴性和假阳性。*因此，对于未来几年，定期的人工干预仍将是必要的*。
- en: Let’s give a MiniLM model a try.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试一个 MiniLM 模型。
- en: MiniLM-L12-H384-uncased
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MiniLM-L12-H384-uncased
- en: Microsoft/MiniLM-L12-H384-uncased optimizes the size of the last self-attention
    layer of the teacher, among other tweakings of a BERT model, to obtain better
    performances. It has 12 layers, 12 heads, and 33 million parameters, and is 2.7
    times faster than BERT-base.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft/MiniLM-L12-H384-uncased 优化了老师的最后一个自注意力层的大小，以及对BERT模型的其他调整，以获得更好的性能。它有12层，12个头部和3300万参数，并且比BERT-base快2.7倍。
- en: 'Let’s test it for its capacity to understand the principles of compositionality:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试它对组成原则的理解能力：
- en: '`Though the customer seemed unhappy, she was, in fact satisfied but thinking
    of something else at the time, which gave a false impression.`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`尽管客户似乎不高兴，但事实上她很满意，只是在那时在想其他事情，这造成了误解。`'
- en: 'The output is interesting because it produces a careful split (undecided) score:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出很有趣，因为它产生了一个谨慎的分数（未决定）：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_07.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本描述的图像](img/B17948_12_07.png)'
- en: 'Figure 12.7: Complex sentence sentiment analysis'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.7: 复杂句子情感分析'
- en: We can see that this output is not conclusive since it is around `0.5`. It should
    be positive.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个输出不是决定性的，因为它在`0.5`左右。应该是积极的。
- en: Let’s try a model involving entailment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个涉及含意的模型。
- en: RoBERTa-large-mnli
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RoBERTa-large-mnli
- en: A **Multi-Genre Natural Language Inference** (**MultiNLI**) task, [https://cims.nyu.edu/~sbowman/multinli/](https://cims.nyu.edu/~sbowman/multinli/),
    can help solve the interpretation of a complex sentence when we are trying to
    determine what a customer means. Inference tasks must determine whether a sequence
    entails the following one or not.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**多题材自然语言推理**（**MultiNLI**）任务，[https://cims.nyu.edu/~sbowman/multinli/](https://cims.nyu.edu/~sbowman/multinli/)，可以帮助解决复杂句子的解释，当我们试图确定客户的意思时。推理任务必须确定一个序列是否导致下一个序列。
- en: 'We need to format our input and split the sequence with sequence splitting
    tokens:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对输入进行格式化，并使用序列分割标记分割序列：
- en: '`Though the customer seemed unhappy</s></s> she was, in fact satisfied but
    thinking of something else at the time, which gave a false impression`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`尽管客户似乎不高兴</s></s> 事实上她很满意，只是当时在想其他事情，这造成了误解`'
- en: 'The result is interesting, although it remains neutral:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很有趣，尽管保持中性：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本描述的图像](img/B17948_12_08.png)'
- en: 'Figure 12.8: The neutral result obtained for a slightly positive sentence'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.8: 针对稍微积极的句子获得的中性结果'
- en: However, there is no mistake in this result. The second sequence is not inferred
    from the first sequence. The result is carefully correct.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这个结果没有错误。第二个序列不是从第一个序列推断出的。结果非常正确。
- en: Let’s finish our experiments on a “positive sentiment” multilingual BERT-base
    model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以“积极情感”多语言BERT-base模型结束我们的实验。
- en: BERT-base multilingual model
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT-base多语言模型
- en: 'Let’s run our final experiment on a super cool BERT-base model: `nlptown/bert-base-multilingual-uncased-sentiment`.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个超级酷的BERT-base模型上运行我们的最终实验：`nlptown/bert-base-multilingual-uncased-sentiment`。
- en: It is very well-designed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它设计得非常好。
- en: 'Let’s run it with a friendly and positive sentence in English:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用友好和积极的英文句子来运行它：
- en: '![](img/B17948_12_09.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_12_09.png)'
- en: 'Figure 12.9: Sentiment analysis in English'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.9: 英文情感分析'
- en: 'Let’s try it in French with `"Ce modèle est super bien!"` (`"this model is
    super good,"` meaning `"cool"`):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用法语尝试一下，“Ce modèle est super bien!”（“这个模型超级好！”，意思是“酷”）：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_10.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本描述的图像](img/B17948_12_10.png)'
- en: 'Figure 12.10: Sentiment analysis in French'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.10: 法语情感分析'
- en: The path of this model for Hugging Face is `nlptown/bert-base-multilingual-uncased-sentiment`.
    You can find it in the search form on the Hugging Face website. Its present link
    is [https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face上这个模型的路径是`nlptown/bert-base-multilingual-uncased-sentiment`。您可以在Hugging
    Face网站的搜索表单中找到它。其当前链接为[https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21)。
- en: 'You can implement it on your website with the following initialization code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下初始化代码在您的网站上实现它：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It will take some time and patience, but the result could be super cool!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一些时间和耐心，但结果可能会非常好！
- en: You could implement this transformer on your website to average out the global
    satisfaction of your customers! You could also use it as continuous feedback to
    improve your customer service and anticipate customer reactions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在您的网站上实施此转换器，以平均全球客户的满意度！您还可以将其用作持续反馈，以改善您的客户服务并预测客户的反应。
- en: Before we leave, we will see how GPT-3 performs sentiment analysis.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开之前，我们将看看 GPT-3 如何执行情感分析。
- en: Sentiment analysis with GPT-3
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 进行情感分析
- en: 'You will need an OpenAI account to run the examples in this section. The educational
    interface requires no API, no development, or training. You can simply enter some
    tweets, for example, and ask for sentiment analysis:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个 OpenAI 账户来运行本节的示例。教育界面不需要 API、开发或培训。您可以简单地输入一些推特，然后请求进行情感分析：
- en: '**Tweet**: `I didn''t find the movie exciting, but somehow I really enjoyed
    watching it!`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**推特**: `我觉得这部电影不那么令人兴奋，但不知怎么的我真的很享受看它！`'
- en: '**Sentiment**: `Positive`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感**: `积极`'
- en: '**Tweet**: `I never ate spicy food like this before but find it super good!`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**推特**: `我以前从未吃过这样的辛辣食物，但觉得它超级好吃！`'
- en: '**Sentiment**: `Positive`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感**: `积极`'
- en: The outputs are satisfactory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是令人满意的。
- en: 'We will now submit a difficult sequence to the GPT-3 engine:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将向 GPT-3 引擎提交一个困难的序列：
- en: '**Tweet**: `It''s difficult to find what we really enjoy in life because of
    all of the parameters we have to take into account.`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**推特**: `因为我们不得不考虑所有参数，所以很难找到我们在生活中真正喜欢的东西。`'
- en: '**Sentiment**: `Positive`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感**: `积极`'
- en: The output is false! The sentiment is not positive at all. The sentence shows
    the difficulty of life. However, the word `enjoy` introduced bias for GPT-3.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是错误的！这个情感根本不是积极的。这句话显示了生活的困难。然而，“享受”这个词为 GPT-3 引入了偏见。
- en: 'If we take enjoy out of the sequence and replace it with the verb `are`, the
    output is negative:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从序列中去除“享受”并用动词 `是` 替换，输出将是负面的：
- en: '**Tweet**: `It''s difficult to find what we really are in life because of all
    of the parameters we have to take into account.`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**推特**: `因为我们不得不考虑所有参数，所以很难找到我们在生活中真正喜欢的东西。`'
- en: '**Sentiment**: `Negative`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感**: `消极`'
- en: The output is false also! It’s not because life is difficult to figure out that
    we can conclude that the sentence is negative. The correct output should have
    been neutral. Then we could ask GPT-3 to perform another task to explain why it
    is difficult in a pipeline, for example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出也是错误的！并不是因为生活难以理解，我们就可以得出这句话是负面的结论。正确的输出应该是中性的。然后我们可以要求 GPT-3 在流水线中执行另一个任务，例如解释为什么它很困难。
- en: 'Running NLP tasks as a user with nothing to do shows where Industry 4.0 (I4.0)
    is going: less human intervention and more automatic functionality. *However,
    we know that some situations require our new skills, such as designing preprocessing
    functions when the transformer doesn’t produce the expected result. Humans are
    still useful!*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个没有任务的用户运行 NLP 任务显示了工业 4.0（I4.0）的发展方向：更少的人为干预，更多的自动功能。 *然而，我们知道某些情况下我们的新技能是必需的，比如在转换器产生意外结果时设计预处理功能。人类仍然是有用的！*
- en: An example of tweet classification with ready-to-use code is described in the
    *Running OpenAI GPT-3 Tasks* section of *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*. You can implement the examples of this section in that code
    if you wish.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第七章*，*GPT-3 引擎崛起的超人类转换器* 的 *运行 OpenAI GPT-3 任务* 部分中描述了一种具有现成代码的推特分类示例。如果您愿意，您可以在该代码中实现本节的示例。
- en: Let’s now see how we can still prove ourselves valuable assets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们如何仍然证明自己是有价值的资产。
- en: Some Pragmatic I4.0 thinking before we leave
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在我们离开之前进行一些务实的 I4.0 思考
- en: The sentiment analysis with Hugging Face transformers contained a sentence that
    came out as “neutral.”
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face transformers 进行情感分析时包含了一句被判定为“中性”的句子。
- en: But is that true?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 但那是真的吗？
- en: Labeling this sentence “neutral” bothered me. I was curious to see if OpenAI
    GPT-3 could do better. After all, GPT-3 is a foundation model that can theoretically
    do many things it wasn’t trained for.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将这句话标记为“中性”让我感到不安。我很好奇 OpenAI GPT-3 能否做得更好。毕竟，GPT-3 是一个理论上可以做许多其未经过训练的事情的基础模型。
- en: 'I examined the sentence again:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我再次检查了这句话：
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but thinking
    of something else at the time, which gave a false impression.`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`虽然客户看起来不高兴，但实际上她却很满意，只是在那时在想其他事情，这给了一个错误的印象。`'
- en: When I read the sentence closely, I could see that the customer is `she`. When
    I looked deeper, I understood that `she` is `in fact satisfied`. I decided not
    to try models blindly until I reached one that works. Trying one model after the
    other is not productive.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我仔细阅读句子时，我能看到顾客是 `she`。当我更深入地观察时，我理解到 `she` 实际上是 `satisfied`。我决定不是盲目地尝试各种模型，直到找到一个有效的为止。一直尝试一个接一个的模型是没有生产力的。
- en: I needed to get to the root of the problem using logic and experimentation.
    I didn’t want to rely on an algorithm that would find the reason automatically.
    Sometimes we need to use *our* neurons!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要通过逻辑和实验来找到问题的根源。我不想依赖自动找到原因的算法。有时候我们需要使用*我们的*神经元！
- en: Could the problem be that it is difficult to identify `she` as the `customer`
    for a machine? As we did in *Chapter 10*, *Semantic Role Labeling with BERT-Based
    Transformers*, let’s ask SRL BERT.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能是很难为机器识别 `she` 作为 `customer` 吗？就像我们在*第10章*中所做的那样，*基于BERT的语义角色标注*，让我们问问SRL
    BERT。
- en: Investigating with SRL
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用SRL进行调查
- en: '*Chapter 10* ended with my recommendation to use SRL with other tools, which
    we are doing now.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*第10章*以我建议使用SRL与其他工具结束，我们现在正在这样做。'
- en: I first ran `She was satisfied` using the **Semantic Role Labeling** interface
    on [https://demo.allennlp.org/](https://demo.allennlp.org/).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先在[https://demo.allennlp.org/](https://demo.allennlp.org/)上使用**语义角色标注**接口运行了
    `She was satisfied`。
- en: 'The result was correct:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是正确的：
- en: '![Graphical user interface  Description automatically generated](img/B17948_12_11.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成描述](img/B17948_12_11.png)'
- en: 'Figure 12.11: SRL of a simple sentence'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：简单句子的SRL
- en: 'The analysis is clear in the frame of this predicate: `was` is the verb, `She`
    is **ARG1**, and `satisfied` is **ARG2**.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个谓词框架中的分析很清晰：`was` 是动词，`She` 是 **ARG1**，而 `satisfied` 是 **ARG2**。
- en: 'We should find the same analysis in a complex sentence, and we do:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在一个复杂的句子中找到相同的分析结果，而我们做到了：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_12.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动以中等置信度生成的描述](img/B17948_12_12.png)'
- en: 'Figure 12.12: The verb “satisfied” is merged with other words, causing confusion'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：动词“satisfied”与其他单词合并，导致混淆
- en: '`Satisfied` is still **ARG2**, so the problem might not be there.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`Satisfied` 仍然是 **ARG2**，所以问题可能不在这里。'
- en: Now, the focus is on **ARGM-ADV**, which modifies `was` as well. The word `false`
    is quite misleading because **ARGM-ADV** is relative to **ARG2**, which contains
    `thinking`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，焦点在**ARGM-ADV**上，它也修改了 `was`。这个字 `false` 是相当具有误导性的，因为**ARGM-ADV**是与包含 `thinking`
    的**ARG2**相关的。
- en: The `thinking` predicate gave a `false impression`, but thinking is not identified
    as a predicate in this complex sentence. Could it be that `she was` is an unidentified
    ellipsis, as we saw in the *Questioning the scope of SRL* section of *Chapter
    10*?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`thinking` 谓词给出了一个 `false impression`，但在这个复杂的句子中却没有被识别为谓词。难道 `she was` 是一个未知的省略，正如*第10章*中的*SRL范围的质疑*部分所示？'
- en: 'We can quickly verify that by entering the full sentence without an ellipsis:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过输入完整句子而不使用省略号，我们可以快速验证：
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but she was
    thinking of something else at the time, which gave a false impression.`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`Though the customer seemed unhappy, she was, in fact, satisfied but she was
    thinking of something else at the time, which gave a false impression.`'
- en: The problem with SRL was the ellipsis again, as we saw in *Chapter 10*. We now
    have five correct predicates with five accurate frames.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: SRL 的问题再次出现，正如我们在*第10章*中所看到的那样。我们现在有五个正确的谓词和五个准确的框架。
- en: '*Frame 1* shows that `unhappy` is correctly related to `seemed`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*Frame 1* 显示 `unhappy` 与 `seemed` 正确相关：'
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_13.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动以中等置信度生成的描述](img/B17948_12_13.png)'
- en: 'Figure 12.13: “Unhappy” is correctly related to “seemed”'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：“Unhappy” 与 “seemed” 正确相关
- en: '*Frame 2* shows that `satisfied` is now separated from the sentence and individually
    identified as an argument of `was` in a complex sentence:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*Frame 2* 显示 `satisfied` 现在从句子中分离出来，并在一个复杂句子中单独识别为 `was` 的参数：'
- en: '![Graphical user interface  Description automatically generated](img/B17948_12_14.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成描述](img/B17948_12_14.png)'
- en: 'Figure 12.14: “satisfied” is now a separate word in ARG2'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：“satisfied” 现在是ARG2中的一个独立单词
- en: 'Now, let’s go straight to the predicate containing `thinking`, which is the
    verb we wanted BERT SRL to analyze correctly. Now that we suppressed the ellipsis
    and repeated “she was” in the sentence, the output is correct:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们直接到包含`thinking`的谓语，这是我们希望BERT SRL能够正确分析的动词。现在我们压制了省略号并重复了“she was”在句子中，输出是正确的：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_15.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述由中等置信度自动生成](img/B17948_12_15.png)'
- en: 'Figure 12.15: Accurate output without an ellipsis'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15：没有省略号的准确输出
- en: 'Now, we can leave our SRL investigation with two clues:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以用两个线索结束我们的SRL调查：
- en: The word `false` is a confusing argument for an algorithm to relate to other
    words in a complex sentence
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词`false`对算法来说是一个让其难以理解复杂句子中其他单词的令人困惑的论点
- en: The ellipsis of the repetition of `she was`
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`she was`的重复省略号'
- en: Before we go to GPT-3, let’s go back to Hugging Face with our clues.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向GPT-3之前，让我们回到Hugging Face，带着我们的线索。
- en: Investigating with Hugging Face
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用Hugging Face进行调查
- en: Let’s go back to the DistilBERT base uncased fine-tuned SST-2 model we used
    in this chapter’s *DistilBERT for SST* section.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到本章 *DistilBERT用于SST* 部分使用的DistilBERT基础uncased fine-tuned SST-2模型。
- en: 'We will investigate our two clues:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调查我们的两个线索：
- en: The ellipsis of `she was`
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`she was`的省略号'
- en: 'We will first submit a full sentence with no ellipsis:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将首先提交一个没有省略号的完整句子：
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but she was
    thinking of something else at the time, which gave a false impression`'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “尽管顾客似乎不快乐，但事实上，她是满意的，只是当时在想其他事情，给人以错误的印象”
- en: 'The output remains negative:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出仍然是消极的：
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_12_16.png)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图形用户界面，应用 描述自动生成](img/B17948_12_16.png)'
- en: 'Figure 12.16: A false negative'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.16：一个假阴性
- en: The presence of `false` in an otherwise positive sentence.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个原本积极的句子中出现了`false`。
- en: 'We will now take `false` out of the sentence but leave the ellipsis:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们将`false`从句子中移除，但留下省略号：
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but thinking
    of something else at the time, which gave an impression`'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “尽管顾客似乎不满意，但事实上，她很满意，只是当时在想其他事情，留下了这样的印象”
- en: 'Bingo! The output is positive:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中了！输出为积极：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_12_17.png)'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图形用户界面，文本 应用 描述自动生成](img/B17948_12_17.png)'
- en: 'Figure 12.17: A true positive'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.17：一个真阳性
- en: We know that the word `false` creates confusion for SRL if there is an ellipsis
    of `was thinking`. We also know that `false` creates confusion for the sentiment
    analysis Hugging Face transformer model we used.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道单词`false`对SRL造成混淆，如果有`was thinking`的省略号。我们也知道`false`对我们使用的情感分析Hugging Face
    transformer模型造成困惑。
- en: Can GPT-3 do better? Let’s see.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3能做得更好吗？让我们看看。
- en: Investigating with the GPT-3 playground
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用GPT-3游��场进行调查
- en: 'Let’s use the OpenAI’s example of an **Advanced tweet classifier** and modify
    it to satisfy our investigation in three steps:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用OpenAI的**高级推文分类器**的例子，并通过三个步骤修改它来满足我们的调查需求：
- en: '*Step 1*: Showing GPT-3 what we expect:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤1*：向GPT-3展示我们期望的内容：'
- en: '**Sentence**: `"The customer was satisfied"`'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子**：“顾客很满意”'
- en: '**Sentiment**: `Positive`'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**情感**：`积极`'
- en: '**Sentence**: `"The customer was not satisfied"`'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子**：“顾客并不满意”'
- en: '**Sentiment**: `Negative`'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**情感**：`消极`'
- en: '**Sentence**: `"The service was ![](img/Icon_01.png)"`'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子**：“服务很 ![](img/Icon_01.png)`好`”'
- en: '**Sentiment**: `Positive`'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**情感**：`积极`'
- en: '**Sentence**: `"``This is the link to the review"`'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子**：“`这是评论的链接`”'
- en: '**Sentiment**: `Neutral`'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**情感**：`中性`'
- en: '*Step 2*: Showing it a few examples of the output format we expect:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤2*：展示少量输出格式示例：'
- en: '`1\. "I loved the new Batman movie!"`'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1\. "我喜欢新的蝙蝠侠电影！"`'
- en: '`2\. "I hate it when my phone battery dies"`'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2\. "我讨厌我的手机电量耗尽时的情况"`'
- en: '`3\. "My day has been ![](img/Icon_01.png)"`'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3\. "我的一天过得如此 ![](img/Icon_01.png)"`'
- en: '`4\. "This is the link to the article"`'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4\. "这是文章的链接"`'
- en: '`5\. "This new music video blew my mind"`'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5\. "这个新的音乐视频让我大吃一惊"`'
- en: 'Sentence sentiment ratings:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子情感评分：
- en: '`1: Positive`'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1: 积极`'
- en: '`2: Negative`'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2: 消极`'
- en: '`3: Positive`'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3: 积极`'
- en: '`4: Neutral`'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4: 中性`'
- en: '`5: Positive`'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5: 积极`'
- en: '*Step 3*: Entering our sentence among others (number 3):'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤3*：将我们的句子与其他句子一起输入（编号3）：'
- en: '`1\. "I can''t stand this product"`'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1\. "我受不了这个产品"`'
- en: '`2\. "The service was bad! ![](img/Icon_02.png)"`'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2\. "服务很差！ ![](img/Icon_02.png)"`'
- en: '`3\. "Though the customer seemed unhappy she was in fact satisfied but thinking
    of something else at the time, which gave a false impression"`'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3\. "尽管客户看起来不满意，但事实上她当时是满意的，只是想着其他事情，这给人以错误的印象"`'
- en: '`4\. "The support team was ![](img/Icon_03.png)![](img/Icon_03.png)"`'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4\. "支持团队很棒！"`'
- en: '`5\. "Here is the link to the product."`'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5\. "这是产品的链接。"`'
- en: 'Sentence sentiment ratings:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子情感评分：
- en: '`1: Negative`'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1: 消极`'
- en: '`2: Positive`'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2: 积极`'
- en: '`3: Positive`'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3: 积极`'
- en: '`4: Positive`'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4: 积极`'
- en: '`5: Neutral`'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5: 中性`'
- en: The output seems satisfactory since our sentence is positive (number 3). Is
    this result reliable? We could run the example here several times. But let’s go
    down to code level to find out.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出似乎令人满意，因为我们的句子是积极的（编号 3）。这个结果可靠吗？我们可以在这里多次运行示例。但让我们深入到代码层面去找出答案。
- en: GPT-3 code
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-3 代码
- en: 'We just click on **View code** in the playground, copy it, and paste it into
    our `SentimentAnalysis.ipynb` chapter notebook. We add a line to only print what
    we want to see:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需在游乐场中点击**查看代码**，复制它，并将其粘贴到我们的 `SentimentAnalysis.ipynb` 章节笔记本中。我们添加一行只打印我们想要看到的内容：
- en: '[PRE12]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is not stable, as we can see in the following responses:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输出不稳定，正如我们在以下回应中所看到的：
- en: '**Run 1**: Our sentence (number 3) is neutral:'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行 1**：我们的句子（编号 3）是中性的：'
- en: '`1: Negative`'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1: 消极`'
- en: '`2: Negative`'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2: 消极`'
- en: '`3: Neutral`'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3: 中性`'
- en: '`4: Positive`'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4: 积极`'
- en: '`5: Positive`'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5: 积极`'
- en: '**Run 2**: Our sentence (number 3) is positive:'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行 2**：我们的句子（编号 3）是积极的：'
- en: '`1: Negative`'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1: 消极`'
- en: '`2: Negative`'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2: 消极`'
- en: '`3: Positive`'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`3: 积极`'
- en: '`4: Positive`'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`4: 积极`'
- en: '`5: Neutral`'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5: 中性`'
- en: '**Run 3**: Our sentence (number 3) is positive'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行 3**：我们的句子（编号 3）是积极的'
- en: '**Run 4**: Our sentence (number 3) is negative'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行 4**：我们的句子（编号 3）是消极的'
- en: 'This leads us to the conclusions of our investigation:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们得出调查的结论：
- en: SRL shows that if a sentence is simple and complete (no ellipsis, no missing
    words), we will get a reliable sentiment analysis output.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRL 表明，如果句子是简单而完整的（没有省略号，没有遗漏的单词），我们将得到可靠的情感分析输出。
- en: SRL shows that if the sentence is moderately difficult, the output might, or
    might not, be reliable.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRL 表明，如果句子是中等难度的，输出可能可靠，也可能不可靠。
- en: SRL shows that if the sentence is complex (ellipsis, several propositions, many
    ambiguous phrases to solve, and so on), the result is not stable, and therefore
    not reliable.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRL 表明，如果句子是复杂的（省略号、多个命题、许多含糊不清的短语等），结果是不稳定的，因此不可靠。
- en: 'The conclusions of the job positions of developers in the present and future
    are:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者的现在和未来的工作岗位的结论是：
- en: Less AI development will be required with Cloud AI and ready-to-use modules.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云 AI 和即用模块将需要更少的 AI 开发。
- en: More design skills will be required.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将需要更多的设计技能。
- en: Classical development of pipelines to feed AI algorithms, control them, and
    analyze their outputs will require thinking and targeted development.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发展用于供给 AI 算法、控制它们并分析其输出的经典流程管道将需要思考和有针对性的发展。
- en: This chapter shows a huge future for developers as thinkers, designers, and
    pipeline development!
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了开发者作为思想家、设计师和管道开发的巨大前景！
- en: It’s now time to sum up our journey and explore new transformer horizons.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是总结我们的旅程并探索新的变压器地平线的时候了。
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went through some advanced theories. The principle of compositionality
    is not an intuitive concept. The principle of compositionality means that the
    transformer model must understand every part of the sentence to understand the
    whole sentence. This involves logical form rules that will provide links between
    the sentence segments.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些高级理论。组合性原则不是一个直观的概念。组合性原则意味着变压器模型必须理解句子的每个部分才能理解整个句子。这涉及到将提供句子部分之间联系的逻辑形式规则。
- en: The theoretical difficulty of sentiment analysis requires a large amount of
    transformer model training, powerful machines, and human resources. Although many
    transformer models are trained for many tasks, they often require more training
    for specific tasks.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析的理论难度需要大量的变压器模型训练、强大的机器和人力资源。虽然许多变压器模型为许多任务进行了训练，但它们通常需要针对特定任务进行更多的训练。
- en: We tested RoBERTa-large, DistilBERT, MiniLM-L12-H384-uncased, and the excellent
    BERT-base multilingualmodel. We found that some provided interesting answers but
    required more training to solve the SST sample we ran on several models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了 RoBERTa-large、DistilBERT、MiniLM-L12-H384-uncased 和出色的 BERT-base 多语言模型。我们发现其中一些提供了有趣的答案，但需要更多的训练来解决我们在多个模型上运行的
    SST 样本。
- en: Sentiment analysis requires a deep understanding of a sentence and extraordinarily
    complex sequences. So, it made sense to try RoBERTa-large-mnli to see what an
    interference task would produce. The lesson here is not to be conventional with
    something as unconventional as transformer models! Try everything. Try different
    models on various tasks. Transformers’ flexibility allows us to try many different
    tasks on the same model or the same task on many different models.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析需要对句子有深入的理解和非常复杂的序列。所以，尝试RoBERTa-large-mnli是有意义的，以了解干扰任务会产生什么。这里的教训是，在像transformer模型这样非常不传统的东西上，不要墨守成规！尝试一切。在各种任务上尝试不同的模型。transformer的灵活性使我们能够在同一模型上尝试许多不同的任务，或者在许多不同的模型上尝试相同的任务。
- en: We gathered some ideas along the way to improve customer relations. If we detect
    that a customer is unsatisfied too often, that customer might just seek out our
    competition. If several customers complain about a product or service, we must
    anticipate future problems and improve our services. We can also display our quality
    of service with online real-time representations of a transformer’s feedback.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一路上收集了一些改善客户关系的想法。如果我们发现一个客户经常不满意，那么这个客户可能只是在寻找我们的竞争对手。如果有几个客户抱怨某个产品或服务，我们必须预见未来的问题并改善我们的服务。我们还可以通过transformer反馈的在线实时表现来展示我们的服务质量。
- en: Finally, we ran sentiment analysis with GPT-3 directly online with nothing to
    do but use the interface! It’s surprisingly effective, but we see that humans
    are still required to solve the more difficult sequences. We saw how SRL could
    help identify the issues in complex sequences.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们直接在线使用GPT-3进行情感分析，除了使用界面之外什么也不用做！这是令人惊讶的有效，但我们看到人类仍然需要解决更困难的序列。我们看到SRL如何帮助识别复杂序列中的问题。
- en: We can conclude that developers have a huge future as thinkers, designers, and
    pipeline development.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，开发人员在思考者、设计者和管道开发方面有着巨大的未来。
- en: In the next chapter, *Analyzing Fake News with Transformers*, we’ll use sentiment
    analysis to analyze emotional reactions to fake news.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*使用transformer分析假新闻*，我们将使用情感分析来分析对假新闻的情感反应。
- en: Questions
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: It is not necessary to pretrain transformers for sentiment analysis. (True/False)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于情感分析，没有必要为transformer进行预训练。(是/否)
- en: A sentence is always positive or negative. It cannot be neutral. (True/False)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个句子总是积极的或消极的。它不能是中性的。(是/否)
- en: The principle of compositionality signifies that a transformer must grasp every
    part of a sentence to understand it. (True/False)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合成性原理表示transformer必须理解句子的每一部分才能理解它。(是/否)
- en: RoBERTa-large was designed to improve the pretraining process of transformer
    models. (True/False)
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa-large旨在改进transformer模型的预训练过程。(是/否)
- en: A transformer can provide feedback that informs us whether a customer is satisfied
    or not. (True/False)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个transformer可以提供反馈，告诉我们客户是否满意。(是/否)
- en: If the sentiment analysis of a product or service is consistently negative,
    it helps us make the proper decisions to improve our offer. (True/False)
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果产品或服务的情感分析一直是负面的，这有助于我们做出适当的决策，以改善我们的服务。(是/否)
- en: If a model fails to provide a good result on a task, it requires more training
    before changing models. (True/False)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个模型在一个任务上不能提供良好的结果，那么在更改模型之前，它需要更多的训练。(是/否)
- en: References
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Richard Socher*, *Alex Perelygin*, *Jean Wu*, *Jason Chuang*, *Christopher
    Manning*, *Andrew Ng*, and *Christopher Potts*, *Recursive Deep Models for Semantic
    Compositionality over a Sentiment Treebank*: [https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Richard Socher*, *Alex Perelygin*, *Jean Wu*, *Jason Chuang*, *Christopher
    Manning*, *Andrew Ng*, 和 *Christopher Potts*, *递归深度模型用于情感树库的语义组成性*：[https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
- en: 'Hugging Face pipelines, models, and documentation:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face管道、模型和文档：
- en: '[https://huggingface.co/transformers/main_classes/pipelines.html](https://huggingface.co/transformers/main_classes/pipelines.html)'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/transformers/main_classes/pipelines.html](https://huggingface.co/transformers/main_classes/pipelines.html)'
- en: '[https://huggingface.co/models](https://huggingface.co/models)'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/models](https://huggingface.co/models)'
- en: '[https://huggingface.co/transformers/](https://huggingface.co/transformers/)'
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/transformers/](https://huggingface.co/transformers/)'
- en: '*Yinhan Liu*, *Danqi Chen*, *Omer Levy*, *Mike Lewis*, *Luke Zettlemoyer*,
    and *Veselin Stoyanov*, 2019, *RoBERTa: A Robustly Optimized BERT Pretraining
    Approach*: [https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*刘银涵*, *陈丹琪*, *奥默·莱维*, *迈克·刘易斯*, *卢克·泽特莫耶*, 和 *维塞林·斯托扬诺夫*, 2019, *RoBERTa:
    一个经过强化优化的 BERT 预训练方法*: [https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf)'
- en: '*The Allen Institute for AI*: [https://allennlp.org/](https://allennlp.org/)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*艾伦人工智能研究所*: [https://allennlp.org/](https://allennlp.org/)'
- en: 'The Allen Institute for reading comprehension resources: [https://demo.allennlp.org/sentiment-analysis](https://demo.allennlp.org/sentiment-analysis)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艾伦人工智能阅读理解资源：[https://demo.allennlp.org/sentiment-analysis](https://demo.allennlp.org/sentiment-analysis)
- en: 'RoBERTa-large contribution, *Zhaofeng Wu*: [https://zhaofengwu.github.io/](https://zhaofengwu.github.io/)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RoBERTa-large 贡献者，*吴照峰*: [https://zhaofengwu.github.io/](https://zhaofengwu.github.io/)'
- en: '*The Stanford Sentiment Treebank*: [https://nlp.stanford.edu/sentiment/treebank.html](https://nlp.stanford.edu/sentiment/treebank.html)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*斯坦福情感树库*: [https://nlp.stanford.edu/sentiment/treebank.html](https://nlp.stanford.edu/sentiment/treebank.html)'
- en: Join our book’s Discord space
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的 *问我任何事* 会议：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
