- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Customer Emotions to Make Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis relies on the principle of compositionality. How can we understand
    a whole sentence if we cannot understand parts of a sentence? Is this tough task
    possible for NLP transformer models? We will try several transformer models in
    this chapter to find out.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the **Stanford Sentiment Treebank** (**SST**). The SST provides
    datasets with complex sentences to analyze. It is easy to analyze sentences such
    as `The movie was great`. However, what happens if the task becomes very tough
    with complex sentences such as `Although the movie was a bit too long, I really
    enjoyed it.?` This sentence is segmented. It forces a transformer model to understand
    the structure of the sequence and its logical form.
  prefs: []
  type: TYPE_NORMAL
- en: We will then test several transformer models with complex sentences and simple
    sentences. We will find that no matter which model we try, it will not work if
    it isn’t trained enough. Transformer models are like us. They are students that
    need to work hard to learn and try to reach real-life human baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Running DistilBERT, RoBERTa-large, BERT-base, MiniLM-L12-H84-uncased, and BERT-base
    multilingual models is fun! However, we will discover that some of these students
    require more training, just like we would.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, we will see how to use the output of the sentiment tasks to improve
    customer relationships and see a nice five-star interface you could implement
    on your website.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use GPT-3’s online interface for sentiment analysis with an
    OpenAI account. No AI development or API is required!
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The SST for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining compositionality for long sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis with AllenNLP (RoBERTa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running complex sentences to explore the new frontier of transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hugging Face sentiment analysis models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilBERT for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with MiniLM-L12-H384-uncased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring RoBERTa-large-mnli
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking into a BERT-base multilingual model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis with GPT-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by going through the SST.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting started: Sentiment analysis transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will first explore the SST that the transformers will use to train
    models on sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We will then use AllenNLP to run a RoBERTa-large transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The Stanford Sentiment Treebank (SST)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Socher* et al. (2013) designed semantic word spaces over long phrases. They
    defined principles of *compositionality* applied to long sequences. The principle
    of compositionality means that an NLP model must examine the constituent expressions
    of a complex sentence and the rules that combine them to understand the meaning
    of a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a sample from the SST to grasp the meaning of the principle of compositionality.
  prefs: []
  type: TYPE_NORMAL
- en: This section and chapter are self-contained, so you can choose to perform the
    actions described or read the chapter and view the screenshots provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the interactive sentiment treebank: [https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33](https://nlp.stanford.edu/sentiment/treebank.html?na=3&nb=33).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can make the selections you wish. Graphs of sentiment trees will appear
    on the page. Click on an image to obtain a sentiment tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Graphs of sentiment trees'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, I clicked on graph number 6, which contains a sentence mentioning
    `Jacques Derrida, a pioneer in deconstruction theories in linguistics`. A long,
    complex sentence appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Whether or not you''re enlightened by any of Derrida''s lectures on the other
    and the self, Derrida is an undeniably fascinating and playful fellow.`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Socher* et al. (2013) worked on compositionality in vector spaces and logic
    forms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, defining the rule of logic that governs the Jacques Derrida sample
    implies an understanding of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How the words `Whether`, `or`, and `not` and the comma that separates the `Whether`
    phrase from the rest of the sentence can be interpreted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to understand the second part of the sentence after the comma with yet another
    `and`!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the vector space was defined, *Socher* et al. (2013) could produce complex
    graphs representing the principle of compositionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now view the graph section by section. The first section is the `Whether`
    segment of the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: The “Whether” segment of a complex sentence'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence has been correctly split into two main parts. The second segment
    is also correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, radar chart, scatter chart  Description automatically generated](img/B17948_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: The main segment of a complex sentence'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can draw several conclusions from the method *Socher* et al. (2013) designed:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis cannot be reduced to counting positive and negative words
    in a sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A transformer model or any NLP model must be able to learn the principle of
    compositionality to understand how the constituents of a complex sentence fit
    together with logical form rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A transformer model must be able to build a vector space to interpret the subtilities
    of a complex sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now put this theory into practice with a RoBERTa-large model.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with RoBERTa-large
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use the AllenNLP resources to run a RoBERTa-large transformer.
    *Liu* et al. (2019) analyzed the existing BERT models and found that they were
    not trained as well as expected. Considering the speed at which the models were
    produced, this was not surprising. They worked on improving the pretraining of
    BERT models to produce a **Robustly Optimized BERT Pretraining Approach** (**RoBERTa**).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first run a RoBERTa-large model in `SentimentAnalysis.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the first cell to install `allennlp-models`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s try to run our Jacques Derrida sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output first displays the architecture of the RoBERTa-large model, which
    has `24` layers and `16` attention heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If necessary, you can take a few minutes to go through the description of a
    BERT architecture in the *BERT model configuration* section in *Chapter 3*, *Fine-Tuning
    BERT Models*, to take full advantage of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis produces values between `0` (negative) and `1` (positive).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output then produces the result of the sentiment analysis task, displaying
    the output logits and the final positive result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: The algorithm is stochastic so the ouputs may vary from one run to another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output also contains the token IDs (which may vary from one run to another)
    and the final output label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also displays the tokens themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Take some time to enter some samples to explore the well-designed and pretrained
    RoBERTa model.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see how we can use sentiment analysis to predict customer behavior
    with other transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting customer behavior with sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will run a sentiment analysis task on several Hugging Face transformer
    models to see which ones produce the best results and which ones we simply like
    the best.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this by using a Hugging Face DistilBERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with DistilBERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s run a sentiment analysis task with DistilBERT and see how we can use the
    result to predict customer behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `SentimentAnalysis.ipynb` and the transformer installation and import
    cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create a function named `classify`, which will run the model with
    the sequences we send to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if you send `M=1` to the function, it will display the configuration
    of the DistilBERT 6-layer, 12-head model we are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The specific parameters of this DistilBERT model are the label definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now create a list of sequences (you can add more) that we can send to the
    `classify` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, `seq=3` is activated to simulate a customer issue we need to
    take into account. The output is negative, which is the example we are looking
    for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can draw several conclusions from this result to predict customer behavior
    by writing a function that would:'
  prefs: []
  type: TYPE_NORMAL
- en: Store the predictions in the customer management database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the number of times a customer complains about a service or product in
    a period (week, month, year). A customer that complains often might switch to
    a competitor to get a better product or service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect the products and services that keep occurring in negative feedback messages.
    The product or service might be faulty and require quality control and improvements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can take a few minutes to run other sequences or create some sequences to
    explore the DistilBERT model.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore other Hugging Face transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with Hugging Face’s models’ list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will explore Hugging Face’s transformer models list and enter some
    samples to evaluate their results. The idea is to test several models, not only
    one, and see which model fits your need the best for a given project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be running Hugging Face models: [https://huggingface.co/models](https://huggingface.co/models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each model we use, you can find the description of the model in the documentation
    provided by Hugging Face: [https://huggingface.co/transformers/](https://huggingface.co/transformers/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will test several models. If you implement them, you might find that they
    require fine-tuning or even pretraining for the NLP tasks you wish to perform.
    In that case, for Hugging Face transformers, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For fine-tuning, you can refer to *Chapter 3*, *Fine-Tuning BERT Models*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For pretraining, you can refer to *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s first go through the list of Hugging Face models: [https://huggingface.co/models](https://huggingface.co/models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, select **Text Classification** in the **Tasks** pane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Selecting text classification models'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of transformer models trained for text classification will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Hugging Face pretrained text-classification models'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default sort mode is **Sort: Most downloads**.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now search for some exciting transformer models we can test online.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with DistilBERT.
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT for SST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `distilbert-base-uncased-finetuned-sst-2-english` model was fine-tuned on
    the SST.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try an example that requires a good understanding of the principles of
    compositionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Though the customer seemed unhappy, she was, in fact satisfied but thinking
    of something else at the time, which gave a false impression."`'
  prefs: []
  type: TYPE_NORMAL
- en: This sentence is tough for a transformer to analyze and requires logical rule
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a false negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: The output of a complex sequence classification task'
  prefs: []
  type: TYPE_NORMAL
- en: A false negative does not mean that the model is not working correctly. We could
    choose another model. However, it could mean that we must download and train it
    longer and better!
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, BERT-like models have good rankings on both
    the GLUE and SuperGLUE leaderboards. The rankings will continuously change but
    not the fundamental concepts of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: We will now try a difficult but less complicated example.
  prefs: []
  type: TYPE_NORMAL
- en: This example is a crucial lesson for real-life projects. When we try to estimate
    how many times a customer complained, we will get both false negatives and false
    positives. *Therefore, regular human intervention will still be mandatory for
    several more years*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s give a MiniLM model a try.
  prefs: []
  type: TYPE_NORMAL
- en: MiniLM-L12-H384-uncased
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft/MiniLM-L12-H384-uncased optimizes the size of the last self-attention
    layer of the teacher, among other tweakings of a BERT model, to obtain better
    performances. It has 12 layers, 12 heads, and 33 million parameters, and is 2.7
    times faster than BERT-base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test it for its capacity to understand the principles of compositionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy, she was, in fact satisfied but thinking
    of something else at the time, which gave a false impression.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is interesting because it produces a careful split (undecided) score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Complex sentence sentiment analysis'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this output is not conclusive since it is around `0.5`. It should
    be positive.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try a model involving entailment.
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa-large-mnli
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **Multi-Genre Natural Language Inference** (**MultiNLI**) task, [https://cims.nyu.edu/~sbowman/multinli/](https://cims.nyu.edu/~sbowman/multinli/),
    can help solve the interpretation of a complex sentence when we are trying to
    determine what a customer means. Inference tasks must determine whether a sequence
    entails the following one or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to format our input and split the sequence with sequence splitting
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy</s></s> she was, in fact satisfied but
    thinking of something else at the time, which gave a false impression`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is interesting, although it remains neutral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: The neutral result obtained for a slightly positive sentence'
  prefs: []
  type: TYPE_NORMAL
- en: However, there is no mistake in this result. The second sequence is not inferred
    from the first sequence. The result is carefully correct.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s finish our experiments on a “positive sentiment” multilingual BERT-base
    model.
  prefs: []
  type: TYPE_NORMAL
- en: BERT-base multilingual model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s run our final experiment on a super cool BERT-base model: `nlptown/bert-base-multilingual-uncased-sentiment`.'
  prefs: []
  type: TYPE_NORMAL
- en: It is very well-designed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run it with a friendly and positive sentence in English:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Sentiment analysis in English'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it in French with `"Ce modèle est super bien!"` (`"this model is
    super good,"` meaning `"cool"`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Sentiment analysis in French'
  prefs: []
  type: TYPE_NORMAL
- en: The path of this model for Hugging Face is `nlptown/bert-base-multilingual-uncased-sentiment`.
    You can find it in the search form on the Hugging Face website. Its present link
    is [https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=Ce+mod%C3%A8le+est+super+bien%21).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can implement it on your website with the following initialization code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It will take some time and patience, but the result could be super cool!
  prefs: []
  type: TYPE_NORMAL
- en: You could implement this transformer on your website to average out the global
    satisfaction of your customers! You could also use it as continuous feedback to
    improve your customer service and anticipate customer reactions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we leave, we will see how GPT-3 performs sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need an OpenAI account to run the examples in this section. The educational
    interface requires no API, no development, or training. You can simply enter some
    tweets, for example, and ask for sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tweet**: `I didn''t find the movie exciting, but somehow I really enjoyed
    watching it!`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment**: `Positive`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tweet**: `I never ate spicy food like this before but find it super good!`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment**: `Positive`'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs are satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now submit a difficult sequence to the GPT-3 engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tweet**: `It''s difficult to find what we really enjoy in life because of
    all of the parameters we have to take into account.`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment**: `Positive`'
  prefs: []
  type: TYPE_NORMAL
- en: The output is false! The sentiment is not positive at all. The sentence shows
    the difficulty of life. However, the word `enjoy` introduced bias for GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take enjoy out of the sequence and replace it with the verb `are`, the
    output is negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tweet**: `It''s difficult to find what we really are in life because of all
    of the parameters we have to take into account.`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment**: `Negative`'
  prefs: []
  type: TYPE_NORMAL
- en: The output is false also! It’s not because life is difficult to figure out that
    we can conclude that the sentence is negative. The correct output should have
    been neutral. Then we could ask GPT-3 to perform another task to explain why it
    is difficult in a pipeline, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running NLP tasks as a user with nothing to do shows where Industry 4.0 (I4.0)
    is going: less human intervention and more automatic functionality. *However,
    we know that some situations require our new skills, such as designing preprocessing
    functions when the transformer doesn’t produce the expected result. Humans are
    still useful!*'
  prefs: []
  type: TYPE_NORMAL
- en: An example of tweet classification with ready-to-use code is described in the
    *Running OpenAI GPT-3 Tasks* section of *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*. You can implement the examples of this section in that code
    if you wish.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how we can still prove ourselves valuable assets.
  prefs: []
  type: TYPE_NORMAL
- en: Some Pragmatic I4.0 thinking before we leave
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sentiment analysis with Hugging Face transformers contained a sentence that
    came out as “neutral.”
  prefs: []
  type: TYPE_NORMAL
- en: But is that true?
  prefs: []
  type: TYPE_NORMAL
- en: Labeling this sentence “neutral” bothered me. I was curious to see if OpenAI
    GPT-3 could do better. After all, GPT-3 is a foundation model that can theoretically
    do many things it wasn’t trained for.
  prefs: []
  type: TYPE_NORMAL
- en: 'I examined the sentence again:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but thinking
    of something else at the time, which gave a false impression.`'
  prefs: []
  type: TYPE_NORMAL
- en: When I read the sentence closely, I could see that the customer is `she`. When
    I looked deeper, I understood that `she` is `in fact satisfied`. I decided not
    to try models blindly until I reached one that works. Trying one model after the
    other is not productive.
  prefs: []
  type: TYPE_NORMAL
- en: I needed to get to the root of the problem using logic and experimentation.
    I didn’t want to rely on an algorithm that would find the reason automatically.
    Sometimes we need to use *our* neurons!
  prefs: []
  type: TYPE_NORMAL
- en: Could the problem be that it is difficult to identify `she` as the `customer`
    for a machine? As we did in *Chapter 10*, *Semantic Role Labeling with BERT-Based
    Transformers*, let’s ask SRL BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating with SRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chapter 10* ended with my recommendation to use SRL with other tools, which
    we are doing now.'
  prefs: []
  type: TYPE_NORMAL
- en: I first ran `She was satisfied` using the **Semantic Role Labeling** interface
    on [https://demo.allennlp.org/](https://demo.allennlp.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The result was correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B17948_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: SRL of a simple sentence'
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis is clear in the frame of this predicate: `was` is the verb, `She`
    is **ARG1**, and `satisfied` is **ARG2**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should find the same analysis in a complex sentence, and we do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: The verb “satisfied” is merged with other words, causing confusion'
  prefs: []
  type: TYPE_NORMAL
- en: '`Satisfied` is still **ARG2**, so the problem might not be there.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the focus is on **ARGM-ADV**, which modifies `was` as well. The word `false`
    is quite misleading because **ARGM-ADV** is relative to **ARG2**, which contains
    `thinking`.
  prefs: []
  type: TYPE_NORMAL
- en: The `thinking` predicate gave a `false impression`, but thinking is not identified
    as a predicate in this complex sentence. Could it be that `she was` is an unidentified
    ellipsis, as we saw in the *Questioning the scope of SRL* section of *Chapter
    10*?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly verify that by entering the full sentence without an ellipsis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but she was
    thinking of something else at the time, which gave a false impression.`'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with SRL was the ellipsis again, as we saw in *Chapter 10*. We now
    have five correct predicates with five accurate frames.
  prefs: []
  type: TYPE_NORMAL
- en: '*Frame 1* shows that `unhappy` is correctly related to `seemed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: “Unhappy” is correctly related to “seemed”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Frame 2* shows that `satisfied` is now separated from the sentence and individually
    identified as an argument of `was` in a complex sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B17948_12_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: “satisfied” is now a separate word in ARG2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go straight to the predicate containing `thinking`, which is the
    verb we wanted BERT SRL to analyze correctly. Now that we suppressed the ellipsis
    and repeated “she was” in the sentence, the output is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_12_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Accurate output without an ellipsis'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can leave our SRL investigation with two clues:'
  prefs: []
  type: TYPE_NORMAL
- en: The word `false` is a confusing argument for an algorithm to relate to other
    words in a complex sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ellipsis of the repetition of `she was`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we go to GPT-3, let’s go back to Hugging Face with our clues.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating with Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s go back to the DistilBERT base uncased fine-tuned SST-2 model we used
    in this chapter’s *DistilBERT for SST* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will investigate our two clues:'
  prefs: []
  type: TYPE_NORMAL
- en: The ellipsis of `she was`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will first submit a full sentence with no ellipsis:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but she was
    thinking of something else at the time, which gave a false impression`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output remains negative:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_12_16.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 12.16: A false negative'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The presence of `false` in an otherwise positive sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now take `false` out of the sentence but leave the ellipsis:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Though the customer seemed unhappy, she was, in fact, satisfied but thinking
    of something else at the time, which gave an impression`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Bingo! The output is positive:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_12_17.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 12.17: A true positive'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We know that the word `false` creates confusion for SRL if there is an ellipsis
    of `was thinking`. We also know that `false` creates confusion for the sentiment
    analysis Hugging Face transformer model we used.
  prefs: []
  type: TYPE_NORMAL
- en: Can GPT-3 do better? Let’s see.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating with the GPT-3 playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the OpenAI’s example of an **Advanced tweet classifier** and modify
    it to satisfy our investigation in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1*: Showing GPT-3 what we expect:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence**: `"The customer was satisfied"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentiment**: `Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence**: `"The customer was not satisfied"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentiment**: `Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence**: `"The service was ![](img/Icon_01.png)"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentiment**: `Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence**: `"``This is the link to the review"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentiment**: `Neutral`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Step 2*: Showing it a few examples of the output format we expect:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1\. "I loved the new Batman movie!"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2\. "I hate it when my phone battery dies"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3\. "My day has been ![](img/Icon_01.png)"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4\. "This is the link to the article"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5\. "This new music video blew my mind"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sentence sentiment ratings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`1: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4: Neutral`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Step 3*: Entering our sentence among others (number 3):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1\. "I can''t stand this product"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2\. "The service was bad! ![](img/Icon_02.png)"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3\. "Though the customer seemed unhappy she was in fact satisfied but thinking
    of something else at the time, which gave a false impression"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4\. "The support team was ![](img/Icon_03.png)![](img/Icon_03.png)"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5\. "Here is the link to the product."`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sentence sentiment ratings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`1: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5: Neutral`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output seems satisfactory since our sentence is positive (number 3). Is
    this result reliable? We could run the example here several times. But let’s go
    down to code level to find out.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We just click on **View code** in the playground, copy it, and paste it into
    our `SentimentAnalysis.ipynb` chapter notebook. We add a line to only print what
    we want to see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is not stable, as we can see in the following responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Run 1**: Our sentence (number 3) is neutral:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3: Neutral`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Run 2**: Our sentence (number 3) is positive:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`2: Negative`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`3: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`4: Positive`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5: Neutral`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Run 3**: Our sentence (number 3) is positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run 4**: Our sentence (number 3) is negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This leads us to the conclusions of our investigation:'
  prefs: []
  type: TYPE_NORMAL
- en: SRL shows that if a sentence is simple and complete (no ellipsis, no missing
    words), we will get a reliable sentiment analysis output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SRL shows that if the sentence is moderately difficult, the output might, or
    might not, be reliable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SRL shows that if the sentence is complex (ellipsis, several propositions, many
    ambiguous phrases to solve, and so on), the result is not stable, and therefore
    not reliable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The conclusions of the job positions of developers in the present and future
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Less AI development will be required with Cloud AI and ready-to-use modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More design skills will be required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical development of pipelines to feed AI algorithms, control them, and
    analyze their outputs will require thinking and targeted development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter shows a huge future for developers as thinkers, designers, and
    pipeline development!
  prefs: []
  type: TYPE_NORMAL
- en: It’s now time to sum up our journey and explore new transformer horizons.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through some advanced theories. The principle of compositionality
    is not an intuitive concept. The principle of compositionality means that the
    transformer model must understand every part of the sentence to understand the
    whole sentence. This involves logical form rules that will provide links between
    the sentence segments.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical difficulty of sentiment analysis requires a large amount of
    transformer model training, powerful machines, and human resources. Although many
    transformer models are trained for many tasks, they often require more training
    for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We tested RoBERTa-large, DistilBERT, MiniLM-L12-H384-uncased, and the excellent
    BERT-base multilingualmodel. We found that some provided interesting answers but
    required more training to solve the SST sample we ran on several models.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis requires a deep understanding of a sentence and extraordinarily
    complex sequences. So, it made sense to try RoBERTa-large-mnli to see what an
    interference task would produce. The lesson here is not to be conventional with
    something as unconventional as transformer models! Try everything. Try different
    models on various tasks. Transformers’ flexibility allows us to try many different
    tasks on the same model or the same task on many different models.
  prefs: []
  type: TYPE_NORMAL
- en: We gathered some ideas along the way to improve customer relations. If we detect
    that a customer is unsatisfied too often, that customer might just seek out our
    competition. If several customers complain about a product or service, we must
    anticipate future problems and improve our services. We can also display our quality
    of service with online real-time representations of a transformer’s feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we ran sentiment analysis with GPT-3 directly online with nothing to
    do but use the interface! It’s surprisingly effective, but we see that humans
    are still required to solve the more difficult sequences. We saw how SRL could
    help identify the issues in complex sequences.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that developers have a huge future as thinkers, designers, and
    pipeline development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Analyzing Fake News with Transformers*, we’ll use sentiment
    analysis to analyze emotional reactions to fake news.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not necessary to pretrain transformers for sentiment analysis. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A sentence is always positive or negative. It cannot be neutral. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The principle of compositionality signifies that a transformer must grasp every
    part of a sentence to understand it. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RoBERTa-large was designed to improve the pretraining process of transformer
    models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer can provide feedback that informs us whether a customer is satisfied
    or not. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the sentiment analysis of a product or service is consistently negative,
    it helps us make the proper decisions to improve our offer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a model fails to provide a good result on a task, it requires more training
    before changing models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Richard Socher*, *Alex Perelygin*, *Jean Wu*, *Jason Chuang*, *Christopher
    Manning*, *Andrew Ng*, and *Christopher Potts*, *Recursive Deep Models for Semantic
    Compositionality over a Sentiment Treebank*: [https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face pipelines, models, and documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/main_classes/pipelines.html](https://huggingface.co/transformers/main_classes/pipelines.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/models](https://huggingface.co/models)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/](https://huggingface.co/transformers/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yinhan Liu*, *Danqi Chen*, *Omer Levy*, *Mike Lewis*, *Luke Zettlemoyer*,
    and *Veselin Stoyanov*, 2019, *RoBERTa: A Robustly Optimized BERT Pretraining
    Approach*: [https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Allen Institute for AI*: [https://allennlp.org/](https://allennlp.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Allen Institute for reading comprehension resources: [https://demo.allennlp.org/sentiment-analysis](https://demo.allennlp.org/sentiment-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RoBERTa-large contribution, *Zhaofeng Wu*: [https://zhaofengwu.github.io/](https://zhaofengwu.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Stanford Sentiment Treebank*: [https://nlp.stanford.edu/sentiment/treebank.html](https://nlp.stanford.edu/sentiment/treebank.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
