- en: Patterns, Outliers, and Recommendations
  prefs: []
  type: TYPE_NORMAL
- en: In order to gain knowledge from data, it's important to understand the structure
    behind a dataset. The way we represent a dataset can make it more intuitive to
    work with in a certain way, and consequently, easier to draw insights from it.
    The law of the instrument states that when holding a hammer, everything seems
    like a nail (based on Andrew Maslow's *The Psychology of Science*, 1966) and is
    about the tendency to adapt jobs to the available tools. There is no silver bullet,
    however, as all methods have their drawbacks given the problem at hand. It's therefore
    important to know the basic methods in the arsenal of available tools in order
    to recognize the situations where we should use a hammer as supposed to a screwdriver.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at different ways of representing data, be it visualizing
    customer groups for marketing purposes and finding unusual patterns, or projecting
    data to emphasize differences, recommending products to customers based on their
    own previous choices, along with those of other customers, and identifying fraudsters
    by their similarities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we''ll present the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering market segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing for similarity search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotting fraudster communities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering market segments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll apply clustering methods in order to find groups of customers
    for marketing purposes. We'll look at the German Credit Risk dataset, and we'll
    try to identify different segments of customers; ideally, we'd want to find the
    groups that are most profitable and different at the same time, so we can target
    them with advertising.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we'll be using a dataset of credit risk, usually referred to
    in full as the German Credit Risk dataset. Each row describes a person who took
    a loan, gives us a few attributes about the person, and tells us whether the person
    paid the loan back (that is, whether the credit was a good or bad risk).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need to download and load up the German credit data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For visualizations, we''ll use the `dython` library. The `dython` library works
    directly on categorical and numeric variables, and makes adjustments for numeric-categorical
    or categorical-categorical comparisons. Please see the documentation for details,
    at [http://shakedzy.xyz/dython/](http://shakedzy.xyz/dython/). Let''s install
    the library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can now play with the German credit dataset, visualize it with `dython`,
    and see how the people represented inside can be clustered together in different
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll first visualize the dataset, do some preprocessing, and apply a clustering
    algorithm. We'll try to make sense out of the clusters, and – with the new insights
    – cluster again.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by visualizing the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualizing correlations**: In this recipe, we''ll use the `dython` library.
    We can calculate the correlations with dython''s `associations` function, which
    calls categorical, numerical (Pearson correlation), and mixed categorical-numerical
    correlation functions depending on the variable types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This call not only calculates correlations, but also cleans up the correlation
    matrix by clustering variables together that are correlated. The data is visualized
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf1b680e-510c-471b-a063-c0036295dbde.png)'
  prefs: []
  type: TYPE_IMG
- en: We can't really see clear cluster demarcations; however, there seem to be a
    few groups if you look along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Also, a few variables such as telephone and job stand out a bit from the rest.
    In the notebook on GitHub, we've tried dimensionality reduction methods to see
    if this would help our clustering. However, dimensionality reduction didn't work
    that well, while clustering directly worked better: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: As the first step for clustering, we'll convert some variables into dummy variables;
    this means we will do a one-hot-encoding of the categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing variables**: We one-hot encode (also called *dummy-transform*)
    the categorical variables as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, when we visualize this dataset to highlight customer differences,
    the result is not appealing. You can see the notebook online for some attempts
    at this.
  prefs: []
  type: TYPE_NORMAL
- en: '**First attempt at clustering**: A typical method for clustering is `kmeans`.
    Let''s try it out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The inertia is the sum of distances to the closest cluster center over all the
    data points. A visual criterion for choosing the best number of clusters (the
    hyperparameter `k` in the *k*-means clustering algorithm) is called the **elbow
    criterion**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize the inertia over a number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02754259-7e11-4d76-8cfc-6a19784573c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic idea of the elbow criterion is to choose the number of clusters where
    the error or inertia flattens off. According to the elbow criterion, we could
    be taking `4` clusters. Let''s get the clusters again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Summarizing clusters**: Now we can summarize the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the summary table for the clusters. We''ve included marketing characteristics,
    such as age, and others that give us an idea about how much money the customers
    make us. We are showing standard deviations over some of these in order to understand
    how consistent the groups are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6dde32-3fb8-4b7c-9f1d-c7060cc6d7bf.png)'
  prefs: []
  type: TYPE_IMG
- en: We see in this little excerpt that differences are largely due to differences
    in credit amount. This brings us back to where we started out, namely that we
    largely get out of the clustering what we put in. There's no trivial way of resolving
    this problem, but we can select the variables we want to focus on in our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**New attempt at clustering**: We can go back to the drawing board, simplify
    our aims, and start again with the question of what we actually want to find:
    groups of customers that fulfill two characteristics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The clusters should distinguish customers by who makes us money: this leads
    us to variables such as credit amount, duration of their loan, and whether they
    paid it back.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters should highlight different characteristics with respect to marketing,
    such as age, gender, or some other characteristic.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this in mind, we''ll make a new attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now produce the overview table again in order to view the cluster stats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And here comes the new summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7097097c-6dea-412c-a491-5a535dfc88a5.png)'
  prefs: []
  type: TYPE_IMG
- en: I would argue this is more useful than the previous clustering, because it clearly
    shows us which customers can make us money, and highlights other differences between
    them that are relevant to marketing.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is a very common visualization technique in business intelligence.
    In marketing, you'll target people differently, say teens, versus pensioners,
    and some groups are more valuable than others. Often, as a first step, dimensionality
    is reduced using a dimensionality reduction method or by feature selection, then
    groups are separated by applying a clustering algorithm. For example, you could
    first apply **Principal Component Analysis** (**PCA**) to reduce dimensionality
    (the number of features), and then *k*-means for finding groups of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since visualizations are difficult to judge objectively, in the previous section,
    what we did was to take a step back and look at the actual purpose, the business
    goal, that we want to achieve. We took the following steps to achieve this goal:'
  prefs: []
  type: TYPE_NORMAL
- en: We concentrated on the variables that are important for our goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We created a function that would help us determine the quality of the clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From this premise, we then tried different methods and evaluated them against
    our business goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''ve paid attention when looking at the recipe, you might have noticed
    that we don''t standardize our output (z-scores). In standardization with the
    z-score, a raw score *x* is converted into a standard score by subtracting the
    mean and dividing by the standard deviation, so every standardized variable has
    a mean of 0 and a standard deviation of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2ad8f8e-a93b-4c82-b215-08c42d9df455.png)'
  prefs: []
  type: TYPE_IMG
- en: We don't apply standardization because variables that have been dummy-transformed
    would have higher importance proportional to their number of factors. To put it
    simply, z-scores mean that every variable would have the same importance. One-hot
    encoding gives us a separate variable for each value that it can take. If we calculate
    and use z-scores after dummy-transforming, a variable that was converted to many
    new (dummy) variables, because it has many values, would be less important than
    another variable that has fewer values and consequently fewer dummy columns. This
    situation is something we want to avoid, so we don't apply z-scores.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to take away, however, is that we have to focus on differences
    that we can understand and describe. Otherwise, we might end up with clusters
    that are of limited use.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll go more into detail with the *k*-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA was proposed in 1901 (by Karl Pearson, in *On Lines and Planes of Closest
    Fit to Systems of Points in Space*) and *k*-means in 1967 (by James MacQueen,
    in *Some Methods for Classification and Analysis of Multivariate Observations*).
    While both methods had their place when data and computing resources were hard
    to come by, today many alternatives exist that can work with more complex relationships
    between data points and features. On a personal note, as the authors of this book,
    we often find it frustrating to see methods that rely on normality or a very limited
    kind of relationship between variables, such as classic methods like PCA or *K*-means,
    especially when there are so many better methods.
  prefs: []
  type: TYPE_NORMAL
- en: Both PCA and *k*-means have serious shortcomings that affect their usefulness
    in practice. Since PCA operates over the correlation matrix, it can only find
    linear correlations between data points. This means that if variables were related,
    but not linearly (as you would see in a scatter plot), then PCA would fail. Furthermore,
    PCA is based on mean and variance, which are parameters for Gaussian distribution.
    *K*-means, being a centroid-based clustering algorithm, can only find spherical
    groups in Euclidean space – that is, it fails to uncover any more complicated
    structures. More information on this can be found at [https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).
  prefs: []
  type: TYPE_NORMAL
- en: 'Other robust, nonlinear methods are available, for example, affinity propagation,
    fuzzy *c*-means, agglomerative clustering, and others. However, it''s important
    to remember that, although these methods separate data points into groups, the
    following statements are also true:'
  prefs: []
  type: TYPE_NORMAL
- en: This is done according to a heuristic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's based on the differences apparent in the dataset and based on the distance
    metric applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of clustering is to visualize and simplify the output for human
    observers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the *k*-means algorithm in more detail. It''s actually really
    simple and can be written down from scratch in `numpy` or `jax`. This implementation
    is based on the one in NapkinML ([https://github.com/eriklindernoren/NapkinML](https://github.com/eriklindernoren/NapkinML)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The main logic – as should be expected – is in the `fit()` method. It comes
    in three steps that are iterated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the distances between each point and the centers of the clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each point gets assigned to the cluster of its closest cluster center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster centers are recalculated as the arithmetic mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It''s surprising that such a simple idea can result in something that looks
    meaningful to human observers. Here''s an example of it being used. Let''s try
    it out with the Iris dataset that we already know from the *Classifying in scikit-learn,
    Keras, and PyTorch* recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting
    Started with Artificial Intelligence in Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We end up with clusters that we can visualize or inspect, similar to before.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to get an overview of different clustering methods, please refer to
    a survey or review paper. Saxena et al. cover most of the important terminology
    in their article, *Review of Clustering Techniques and Developments* (2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'We would recommend looking at the following methods relevant to clustering
    and dimensionality reduction (we link to implementations):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Affinity propagation ([https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)):
    A clustering method that finds a number of clusters as part of its heuristics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuzzy c-means ([https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html](https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html)):
    A fuzzy version of the *k*-means algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Local linear embedding ([https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html)):
    A lower-dimensional embedding, where local similarities are maintained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'T-distributed stochastic neighbor embedding ([https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)):
    A nonlinear dimensionality reduction method suited for visualization in two- or
    three-dimensional space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixture-modeling ([https://mixem.readthedocs.io/en/latest/examples/old_faithful.html](https://mixem.readthedocs.io/en/latest/examples/old_faithful.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea of taking advantage of a pre-trained random forest in order to provide
    a custom-tailored kernel is discussed in *The Random Forest Kernel and other kernels
    for big data from random partitions* (2014) by Alex Davies and Zoubin Ghahramani,
    available at [https://arxiv.org/abs/1402.4293](https://arxiv.org/abs/1402.4293).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering anomalies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An anomaly is anything that deviates from the expected or normal outcomes. Detecting
    anomalies can be important in **I****ndustrial Process Monitoring** (**IPM**),
    where data-driven fault detection and diagnosis can help achieve achieve higher
    levels of safety, efficiency, and quality.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll look at methods for outlier detection. We'll go through
    an example of outlier detection in a time series with **Python Outlier Detection** (**pyOD**),
    a toolbox for outlier detection that implements many state-of-the-art methods
    and visualizations. PyOD's documentation can be found at [https://pyod.readthedocs.io/en/latest/](https://pyod.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: We'll apply an autoencoder for a similarity-based approach, and then an online
    learning approach suitable for finding events in streams of data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will focus on finding outliers. We'll demonstrate how to do this
    with the pyOD library including an autoencoder approach. We'll also outline the
    upsides and downsides to the different methods.
  prefs: []
  type: TYPE_NORMAL
- en: The streams of data are time series of **key performance indicators** (**KPIs**) of
    website performance. This dataset is provided in the DONUT outlier detector repository,
    available at [https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download and load it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use methods from pyOD, so let''s install that as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that some pyOD methods have dependencies such as TensorFlow and
    Keras, so you might have to make sure that these are also installed. If you get
    a message reading `No Module named Keras` you can install Keras separately as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Please note that it's usually better to use the Keras version that ships with
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at our dataset, and then apply different outlier detection
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll cover different steps and methods in this section. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running an isolation forest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running an autoencoder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start by exploring and visualizing our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize our dataset over time as a time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s have a look at our dataset with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3651c14d-4b9b-4ac8-9293-a2b8101f54cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This time series of KPIs is geared toward monitoring the operation and maintenance
    of web services. They come with a label that indicates an abnormality – in other
    words, an outlier – if a problem has occurred with the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the resulting plot, where the dots represent outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c45a89a-1996-4c4f-a869-4dc7369bf9a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can see where outliers are located in the spectrum of the
    KPIs, and how clearly they are distinguishable from normal data, with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding code, we plot two histograms against each other using line
    plots. Alternatively, we could be using `hist()` with opacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot is the outlier distribution density, where the values of
    the time series are on the *x* axis, and the two lines show what''s recognized
    as normal and what''s recognized as an outlier, respectively – 0 indicates normal
    data points, and 1 indicates outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49581a8-fb11-4b4a-902d-e2f23e88ac35.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll be using the same visualization for all subsequent methods so we can compare
    them graphically.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers (shown with the dotted line) are hardly distinguishable from normal
    data points (the squares), so we won't be expecting perfect performance.
  prefs: []
  type: TYPE_NORMAL
- en: We'll now implement benchmarking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we go on and test methods for outlier detection, let's set down a process
    for comparing them, so we'll have a benchmark of the relative performances of
    the tested methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split our data into training and test sets as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write a testing function that we can use with different outlier
    detection methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This function tests an outlier detection method on the dataset. It trains a
    model, gets performance metrics from the model, and plots a visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train`: Training features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train`: Training labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_test`: Test features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test`: Test labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`only_neg`: Whether to use only normal points (that is, not outliers) for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`basemethod`: The model to test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can choose to only train on normal points (all points excluding outliers)
    in order to learn the distribution or general characteristics of these points,
    and the outlier detection method can then decide whether the new points do or
    don't fit these characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this is done, let''s test two methods for outlier detection: the isolation
    forest and an autoencoder.'
  prefs: []
  type: TYPE_NORMAL
- en: The isolation forest comes first.
  prefs: []
  type: TYPE_NORMAL
- en: Here we'll look at the isolation forest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We run the benchmarking method and hand over an isolation forest detection
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The **Receiver Operating Characteristic** (**ROC**) performance of the isolation
    forest predictions against the test data is about 0.86, so it does reasonably
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from the following graph, however, that there are no 1s (predicted
    outliers) in the lower range of the KPI spectrum. The model misses out on outliers
    in the lower range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9af65a20-b46d-4b87-9a99-b3f01a2a5474.png)'
  prefs: []
  type: TYPE_IMG
- en: It only recognizes points as outliers that have higher values (*>=1.5)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s try running an autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the Keras network structure and the output from the test function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The performance of the autoencoder is very similar to the isolation forest;
    however, the autoencoder finds outliers both in the lower and upper ranges of
    the KPI spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we don''t get an appreciable difference when providing either
    only normal data or both normal data and outliers. We can see how the autoencoder
    works in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/274696f4-58a3-40a2-952e-959b46161e5a.png)'
  prefs: []
  type: TYPE_IMG
- en: This doesn't look too bad, actually – values in the mid-range are classified
    as normal, while values on the outside of the spectrum are classified as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Please remember that these methods are unsupervised; of course, we could get
    better results if we used a supervised method. As a practical consideration, if
    we use supervised methods with our own datasets, this would require us to do additional
    work by annotating anomalies, which we don't have to do with unsupervised methods.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers are extreme values that deviate from other observations on the data.
    Outlier detection is important in many domains, including network security, finance, traffic, social
    media, machine learning, the monitoring of machine model performance, and surveillance. A
    host of algorithms have been proposed for outlier detection in these domains.
    The most prominent algorithms include ***k*-Nearest Neighbors** (**kNN**), **L****ocal
    Outlier Factors** (**LOF**), and the isolation forest, and more recently, autoencoders, **L****ong
    Short-Term Memory** (**LSTM**), and **G****enerative Adversarial Networks** (**GANs**).
    We'll discover some of these methods in later recipes. In this recipe, we've used
    kNN, an autoencoder, and the isolation forest algorithm. Let's talk about these
    three methods briefly.
  prefs: []
  type: TYPE_NORMAL
- en: k-nearest neighbors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The kNN classifier is a non-parametric classifier introduced by Thomas Cover
    and Peter Hart (in *Nearest neighbor pattern classification*, 1967). The main
    idea is that a new point is likely to belong to the same class as its neighbors.
    The hyperparameter `k` is the number of neighbors to compare. There are weighted
    versions based on the relative distance of a new point from its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea of the isolation forest is relatively simple: create random decision
    trees (this means each leaf uses a randomly chosen feature and a randomly chosen
    split value) until only one point is left. The length of the path across the trees
    to get to a terminal node indicates whether a point is an outlier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find out more details about the isolation forest in its original publication
    by Liu et al., *Isolation Forest*. ICDM 2008: 413–422: [https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autoencoder is a neural network architecture, where we learn a representation
    based on a set of data. This is usually done by a smaller hidden layer (**bottleneck**)
    from which the original is to be restored. In that way, it's similar to many other
    dimensionality reduction methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'An autoencoder consists of two parts: the encoder and the decoder. What we
    are really trying to learn is the transformation of the encoder, which gives us
    a code or the representation of the data that we look for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, we can define the encoder as the function ![](img/cceac08e-9c97-44f0-bc14-82340bf370d3.png),
    and the decoder as the function ![](img/f2ab91f4-652d-49b7-880a-58a35110e5e8.png).
    We try to find ![](img/4ff556c5-447c-4eeb-a8dd-ef0e76f6265a.png) and ![](img/339c4a30-f3ad-4d7e-a334-66ffee521b9f.png) so
    that the reconstruction error is minimized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f012cf6c-7842-446d-9f3a-46c9acb63767.png)'
  prefs: []
  type: TYPE_IMG
- en: The autoencoder represents data in an intermediate network layer, and the closer
    they can be reconstructed based on the intermediate representation, the less of
    an outlier they are.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many implementations of outlier detection are publicly available for Python:'
  prefs: []
  type: TYPE_NORMAL
- en: As part of Numenta's Cortical Learning Algorithm: [http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model](http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banpei, for singular spectrum transformation: [https://github.com/tsurubee/banpei](https://github.com/tsurubee/banpei)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep anomaly detection methods for time series: [https://github.com/KDD-OpenSource/DeepADoTS](https://github.com/KDD-OpenSource/DeepADoTS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telemanom – LSTMs for multivariate time-series data: [https://github.com/khundman/telemanom](https://github.com/khundman/telemanom)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DONUT – a variational auto-encoder for seasonal KPIs: [https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fantastic resource for material about outlier detection is the PyOD author's
    dedicated repository, available at [https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources).
  prefs: []
  type: TYPE_NORMAL
- en: Representing for similarity search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we want to find a way to decide whether two strings are similar
    given a representation of those two strings. We'll try to improve the way strings
    are represented in order to make more meaningful comparisons between strings. But
    first, we'll get a baseline using more traditional string comparison algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll do the following: given a dataset of paired string matches, we''ll try
    out different functions for measuring string similarity, then a bag-of-characters
    representation, and finally a **Siamese neural network** (also called a **twin
    neural network**) dimensionality reduction of the string representation. We''ll
    set up a twin network approach for learning a latent similarity space of strings
    based on character n-gram frequencies.'
  prefs: []
  type: TYPE_NORMAL
- en: A **Siamese neural network**, also sometimes called **twin neural network**,
    is named as such using the analogy of conjoined twins. It is a way to train a
    projection or a metric space. Two models are trained at the same time, and the
    output of the two models is compared. The training takes the comparison output
    rather than the models' outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, we need to download or load a dataset and install the necessary dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use a dataset of paired strings, where they are either matched or not
    based on whether they are similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then read it in as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset includes pairs of strings that either correspond to each other
    or don''t correspond. It starts like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b428a0db-d2f2-437f-9d15-0b9fb09da16e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There''s also a test dataset available from the same GitHub repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read it into a pandas DataFrame the same way as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll use a few libraries in this recipe that we can install like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The **Levenshtein distance** (also sometimes referred to as **edit distance**)
    measures the number of insertions, deletions, and substitutions that are necessary
    to transform one string into another string. It performs a search in order to
    come up with the shortest way to do this transformation. The library used here
    is a very fast implementation of this algorithm. You can find more about the `python-Levenshtein`
    library at [https://github.com/ztane/python-Levenshtein](https://github.com/ztane/python-Levenshtein).
  prefs: []
  type: TYPE_NORMAL
- en: The `annoy` library provides a highly optimized implementation of the nearest-neighbor
    search. Given a set of points and a distance, we can index all the points using
    a tree representation, and then for any point, we can traverse the tree to find
    similar data points. You can find out more about annoy at [https://github.com/spotify/annoy](https://github.com/spotify/annoy).
  prefs: []
  type: TYPE_NORMAL
- en: Let's get to it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned before, we'll first calculate the baseline using standard string
    comparison functions, then we'll use a bag-of-characters approach, and then we'll
    learn a projection using a Siamese neural network approach. You can find the corresponding
    notebook on the book's GitHub repo, at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Baseline – string comparison functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's first implement a few standard string comparison functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to make sure we clean up our strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We'll use this cleaning function in each of the string comparison functions
    that we'll see in the following code. We will use this function to remove special
    characters before any string distance calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can implement simple string comparison functions. Let''s do the `Levenshtein`
    distance first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s do the `Jaro-Winkler` distance, which is the minimum number of single-character
    transpositions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also use the longest common substring between the compared pair. We
    can use `SequenceMatcher` for this, which is part of the Python standard library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run over all string pairs and calculate the string distances based
    on each of the three methods. For each of the three algorithms, we can calculate
    the **area under the curve** (**AUC**) score to see how well it does at separating
    matching strings from non-matching strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The AUC scores for all algorithms are around 95%, which seems good. All three
    distances perform quite well already. Let's try to beat this.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-characters approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll now implement a bag-of-characters method for string similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A bag of characters means that we will create a histogram of characters, or
    in other words, we will count the occurrences of the characters in each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We've set the range of `ngrams` to just `1`, which means we want only single
    characters. This parameter can be interesting, however, if you want to include
    longer-range dependencies between characters, rather than just the character frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what performance we can get out of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the AUC score of about 93%, this approach doesn't yet perform
    quite as well overall, although the performance is not completely bad. So let's
    try to tweak this.
  prefs: []
  type: TYPE_NORMAL
- en: Siamese neural network approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we'll implement a Siamese network to learn a projection that represents
    the similarities (or differences) between strings.
  prefs: []
  type: TYPE_NORMAL
- en: The Siamese network approach may seem a little daunting if you are not familiar
    with it. We'll discuss it further in the *How it works...* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the string featurization function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `create_string_featurization_model` function returns a model for string
    featurization. The featurization model is a non-linear projection of the bag-of-characters
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function has the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_dimensionality`: The number of features coming from the vectorizer
    (that is, the dimensionality of the bag-of-characters output)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_dim`: The dimensions of the embedding/projection that we are trying
    to create'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we need to create the **conjoined twins** of the two models. For this,
    we need a comparison function. We take the normalized Euclidean distance. This
    is the Euclidean distance between the two *L2-*normalized projected vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *L2* norm of a vector *x* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91661c56-aaae-4e42-8cc6-1c08744738cf.png)'
  prefs: []
  type: TYPE_IMG
- en: The *L2* normalization is dividing the vector *x* by its norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the distance function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the Siamese network can use the function by wrapping it as a Lambda layer.
    Let''s define how to conjoin the twins or, in other words, how we can wrap it
    into a bigger model so we can train with pairs of strings and the label (that
    is, similar and dissimilar):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a verbose way of saying: take the two networks, calculate the normalized
    Euclidean distance, and take the distance as the output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the twin network and train:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This creates a model with an output of 10 dimensions; given 41 dimensions from
    the n-gram featurizer, this means we have a total of 420 parameters (*41 * 10
    + 10*).
  prefs: []
  type: TYPE_NORMAL
- en: As we've mentioned before, the output of our combined network is the Euclidean
    distance between the two outputs. This means we have to invert our target (matched)
    column in order to change the meaning from similar to distant, so that 1 corresponds
    to different and 0 to the same. We can do this easily by subtracting from 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now get the performance of this new projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We've managed to beat the other methods. And that's before we've even tried
    to tune any hyperparameters. Our projection clearly works in highlighting differences
    between strings that are important for string similarity comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-learn `CountVectorizer` counts the occurrences of features in strings.
    A common use case is to count words in sentences – this representation is called
    a **bag of words**, and in that case, the features would be words. In our case,
    we are interested in character-based features, so we just count how many times
    an **a **occurred, a **b **occurred, and so on. We could make this representation
    cleverer by representing tuples of successive characters such as **ab** or **ba**;
    however, that's beyond our scope here.
  prefs: []
  type: TYPE_NORMAL
- en: A Siamese network training is the situation where two (or more) neural networks
    are trained against each other by comparing the output of the networks given a
    pair (or tuple) of inputs and the knowledge of the difference between these inputs.
    Often the Siamese network consists of the same network (that is, the same weights).
    The comparison function between the two network outputs can be metrics such as
    the Euclidean distance or the cosine similarity. Since we know whether the two
    inputs are similar, and even how similar they are, we can train against this knowledge
    as the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the information flow and the different building
    blocks that we''ll be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/782734e6-0391-439c-bd33-4ba62c6f599a.png)'
  prefs: []
  type: TYPE_IMG
- en: Given the two strings that we want to compare, we'll use the same model to create
    features from each one, resulting in two representations. We can then compare
    these representations, and we hope that the comparison correlates with the outcome,
    so that if our comparison shows a big difference, the strings will be dissimilar,
    and if the comparison shows little difference, then the strings will be similar.
  prefs: []
  type: TYPE_NORMAL
- en: We can actually directly train this complete model, given a string comparison
    model and a dataset consisting of a pair of strings and a target. This training
    will tune the string featurization model so the representation will be more useful.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll be building a recommendation system. A recommender is
    an information-filtering system that predicts rankings or similarities by bringing
    content and social connections together.
  prefs: []
  type: TYPE_NORMAL
- en: We'll download a dataset of book ratings that have been collected from the Goodreads
    website, where users rank and review books that they've read. We'll build different
    recommender models, and we'll suggest new books based on known ratings.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To prepare for our recipe, we'll download the dataset and install the required
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the dataset and install the two libraries we''ll use here – `spotlight`
    and `lightfm` are recommender libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to get the dataset of book ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset comes in the shape of an interaction object. According to spotlight''s
    documentation, an interaction object can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[It] contains (at a minimum) a pair of user-item interactions, but can also
    be enriched with ratings, timestamps, and interaction weights.'
  prefs: []
  type: TYPE_NORMAL
- en: For **implicit feedback** scenarios, user IDs and item IDs should only be provided
    for user-item pairs where an interaction was observed. All pairs that are not
    provided are treated as missing observations, and often interpreted as (implicit)
    negative signals.
  prefs: []
  type: TYPE_NORMAL
- en: For **explicit feedback **scenarios, user IDs, item IDs, and ratings should
    be provided for all user-item-rating triplets that were observed in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to know which books the item numbers refer to, we''ll download the
    following CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement a function to get the book titles by `id`. This will
    be useful for showing our recommendations later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've got the dataset and the libraries installed, we can start our
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll first use a matrix factorization model, then a deep learning model. You
    can find more examples in the Jupyter notebook available at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to set a lot of parameters, including the number of latent dimensions
    and the number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b57e6718-d537-4cae-b6ce-138017220b1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We get the following recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8257d8a3-4713-4b5e-b469-dc9b3aa82a38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we''ll use the `lightfm` recommendation algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also look at the recommendations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53b50f3c-aad6-453e-95c9-3d9e07787b1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Both recommenders have their applications. On the basis of the precision at
    *k (k=5)* for both recommenders, we can conclude that the second recommender,
    `lightfm`, performs better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recommenders recommend products to users.
  prefs: []
  type: TYPE_NORMAL
- en: 'They can produce recommendations based on different principles, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: They can predict based on the assumption that customers who have shown similar
    tastes in previous purchases will buy similar items in the future (**collaborative
    filtering**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions based on the idea that customers will have an interest in items
    similar to the ones they've bought in the past (**content-based filtering**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions based on a combination of collaborative filtering, content-based
    filtering, or other approaches (a **hybrid recommender**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid models can combine approaches in different ways, such as making content-based
    and collaborative-based predictions separately and then adding up the scores,
    or by unifying the approaches into a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both models we''ve tried are based on the idea that we can separate the influences
    of users and items. We''ll explain each model in turn, and how they combine approaches,
    but first let''s explain the metric we are using: precision at *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision at k
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The metric we are extracting here is **precision at *k***. For example, precision
    at 10 calculates the number of relevant results among the top *k* documents, with
    typically *k=5* or *k=10*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision at *k* doesn''t take into account the ordering within the top *k*
    results, nor does it include how many of the really good results that we absolutely
    should have captured are actually returned: that would be recall. That said, precision
    at *k* is a sensible metric, and it''s intuitive.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The explicit model in `spotlight` is based on the matrix factorization technique
    presented by Yehuda Koren and others (in *Matrix Factorization Techniques for
    Recommender Systems*, 2009). The basic idea is that a user-item (interaction)
    matrix can be decomposed into two components, ![](img/30cd346a-e2c6-4fab-b244-3ae3d317cb46.png) and ![](img/d3edb87b-cb64-441d-8da4-7d503d3d42f0.png),
    representing user latent factors and item latent factors respectively, so that
    recommendations given an item ![](img/b8cafe93-d461-4120-8df1-26c77ed61a27.png)
    and a user ![](img/6c7fa830-ca0a-42fe-8c15-35445af4e31c.png) can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a4d3d97-fbf1-454d-a708-f387edd25f54.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Matrix decomposition** or **matrix factorization** is the factorization of
    a matrix into a product of matrices. Many different such decompositions exist,
    serving a variety of purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: A relatively simple decomposition is the **singular value decomposition** (**SVD**)
    however, modern recommenders use other decompositions. Both the `spotlight` matrix
    factorization and the `lightfm` model use linear integrations.
  prefs: []
  type: TYPE_NORMAL
- en: The lightfm model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `lightfm` model was introduced in a paper by Kula (*Metadata Embeddings
    for User and Item Cold-start Recommendations,* 2015). More specifically, we use
    the WARP loss, which is explained in the paper *WSABIE: Scaling Up To Large Vocabulary
    Image Annotation* by Jason Weston et al., from 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lightfm` algorithm, the following function is used for predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5434beaa-96ea-4332-8ea8-fed1f65ab755.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding function, we have bias terms for users and items and ![](img/748792ad-b5cf-4b86-99e4-a2d9a7885a5a.png)
    is the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model training maximizes the likelihood of the data conditional on the
    parameters expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/715a6d4a-09ac-4fb0-9ebd-ecbc4ef2a389.png)'
  prefs: []
  type: TYPE_IMG
- en: There are different ways to measure how well recommenders are performing and,
    as always, which one we choose to use depends on the goal we are trying to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, there are a lot of libraries around that make it easy to get up and
    running. First of all, I''d like to highlight these two, which we''ve already
    used in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: lightfm: [https://github.com/lyst/lightfm](https://github.com/lyst/lightfm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotlight: [https://maciejkula.github.io/spotlight](https://maciejkula.github.io/spotlight)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But some others are very promising, too:'
  prefs: []
  type: TYPE_NORMAL
- en: Polara, which includes an algorithm called HybridSVD that seems to be very strong: [https://github.com/evfro/polara](https://github.com/evfro/polara)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepRec, which provides deep learning models for recommendations (based on TensorFlow): [https://github.com/cheungdaven/DeepRec](https://github.com/cheungdaven/DeepRec)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find a demonstration of library functionality for item ranking with
    a dataset at the following repo: [https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py](https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py).
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft has been writing about recommender best practices: [https://github.com/Microsoft/Recommenders](https://github.com/Microsoft/Recommenders).
  prefs: []
  type: TYPE_NORMAL
- en: Last, but not least, you might find the following reading list about recommender
    systems useful: [https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md](https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md).
  prefs: []
  type: TYPE_NORMAL
- en: Spotting fraudster communities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll try to detect fraud communities using methods from network
    analysis. This is a use case that often seems to come up in graph analyses and
    intuitively appeals because, when carrying out fraud detection, we are interested
    in relationships between people, such as whether they live close together, are
    connected over social media, or have the same job.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to get everything in place for the recipe, we'll install the required
    libraries and we'll download a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`networkx` - is a graph analysis library: [https://networkx.github.io/documentation](https://networkx.github.io/documentation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`annoy` - is a very efficient nearest-neighbors implementation: [https://github.com/spotify/annoy](https://github.com/spotify/annoy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tqdm` - to provide us with progress bars: [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, we''ll use SciPy, but this comes with the Anaconda distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the following dataset of fraudulent credit card transactions: [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Credit Card Fraud dataset contains transactions made by credit cards in September 2013 by
    European  cardholders. This dataset presents transactions that occurred over two days, with
    492 fraudulent transactions out of 284,807 transactions. The dataset is highly unbalanced:
    the positive class (fraud) accounts for 0.172% of all transactions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the dataset, and then split it into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We are ready! Let's do the recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll first create an adjacency matrix, then we can apply the community detection
    methods to it, and lastly, we'll evaluate the quality of the generated communities.
    The whole process has the added difficulty associated with a large dataset, which
    means we can only apply certain algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an adjacency matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we need to calculate the distances of all points. This is a real problem
    with a large dataset such as this. You can find several approaches online.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `annoy` library from Spotify for this purpose, which is very fast
    and memory-efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then initialize our adjacency matrix with the distances as given by
    our index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We can now apply some community detection algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Community detection algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The size of our matrix leaves us with limited choice. We''ll apply the two
    following algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strongly Connected Components** (**SCC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Louvain algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can apply the SCC algorithm directly onto the adjacency matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'For the second algorithm, we first need to convert the adjacency matrix to
    a graph; this means that we treat each point in the matrix as an edge between
    nodes. In order to save space, we use a simplified graph class for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can apply the Louvain algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now we have two different partitions of our dataset. Let's find out if they
    are worth anything!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the communities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the ideal case, we'd expect that some communities have only fraudsters in
    them, while others (most) have none at all. This purity is what we would be looking
    for in a perfect community. However, since we also possibly want some suggestions
    of who else might be a fraudster, we would anticipate some points to be labeled
    as fraudsters in a majority-nonfraudster group and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by looking at the histograms of fraud frequency per community.
    The Louvain fraudster distribution looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c6a7b40-e85a-4ab3-950c-a58c34b5ac75.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows that communities have a very high frequency of people who are not
    fraudsters, and very few other values. But can we quantify how good this is?
  prefs: []
  type: TYPE_NORMAL
- en: We can describe the fraudster distribution by calculating the **class entropy**
    in each cluster. We'll explain entropy in the *How it works...* section.
  prefs: []
  type: TYPE_NORMAL
- en: We can then create appropriately chosen random experiments to see if any other
    community assignments would have resulted in a better class entropy. If we randomly
    shuffle the fraudsters and then calculate the entropy across communities, we get
    an entropy distribution. This will give us a **p-value**, the **statistical significance**,
    for the entropy of the Louvain communities.
  prefs: []
  type: TYPE_NORMAL
- en: The **p-value** is the probability that we get a distribution like this (or
    better) purely by chance.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the implementation for the sampling in the notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: We get a very low significance, meaning that it is highly unlikely to have gotten
    anything like this by chance, which leads us to conclude that we have found meaningful
    clusters in terms of identifying fraudsters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hardest part of network analysis with a big dataset is constructing the
    adjacency matrix. You can find different approaches in the notebook available
    online. The two problems are runtime and memory. Both can increase exponentially
    with the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Our dataset contains 284,807 points. This means a full connectivity matrix between
    all points would take a few hundred gigabytes (at 4 bytes per point), ![](img/c2ffa319-8c64-46e9-922a-6b272cff3262.png).
  prefs: []
  type: TYPE_NORMAL
- en: We are using a sparse matrix where most adjacencies are 0s if they don't exceed
    the given threshold. We represent each connection between the points as a Boolean
    (1 bit) and we take a sample of 33%, 93,986 points, rather than the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Graph community algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's go through two graph community algorithms to get an idea of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Louvain algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We've used the Louvain algorithm in this recipe. The algorithm was published
    in 2008 by Blondel *et al.* ([https://arxiv.org/abs/0803.0476](https://arxiv.org/abs/0803.0476)). Since
    its time complexity is ![](img/8f50fad5-5b38-4055-b54e-b0494c083389.png), the
    Louvain algorithm can and has been used with big datasets, including data from
    Twitter containing 2.4 million nodes and 38 million links.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea of the Louvain algorithm is to proceed in an agglomerative manner
    by successively merging communities together so as to increase their connectedness. The
    connectedness is measured by edge modularity, ![](img/cd651623-1eea-4d47-a0c3-6510d7c7d524.png),
    which is the density of edges within a community connected to other vertices of
    the same community versus the vertices of other communities. Any switch of community
    for a vertex ![](img/aae57f62-676c-4db5-905a-7e4064c902e3.png) has an associated ![](img/766320a0-c764-47a3-9ef4-0a65818930db.png). After
    the initial assignment of each of the vertices to their own communities, the heuristic
    operates in two steps: a greedy assignment of vertices to communities, and a coarse
    graining.'
  prefs: []
  type: TYPE_NORMAL
- en: For all vertices ![](img/60211e6a-f320-4dc8-9bb5-16b631454862.png), assign them
    to the community so that ![](img/e7248b3a-8ff8-4182-bd06-6cccdf1cf6ca.png) is
    the highest it can be. This step can be repeated a few times until no improvement
    in modularity occurs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All communities are treated as vertices. This means that edges are also grouped
    together so that all edges that are part of the vertices that were grouped together
    are now edges of the newly created vertex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two steps are iterated until no further improvement in modularity occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Girvan–Newman algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an example of another algorithm, let's look at the Girvan–Newman algorithm.
    The Girvan–Newman algorithm (by Girvan and Newman, 2002, with the paper available
    at [https://www.pnas.org/content/99/12/7821](https://www.pnas.org/content/99/12/7821))
    is based on the concept of the shortest path between nodes. The **edge betweenness **of
    an edge is the number of shortest paths between nodes that run along the edge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the edge betweenness of all edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the edge of the highest edge betweenness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recalculate the edge betweenness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate *steps 2* and *3* until no edges remain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result is a dendrogram that shows the arrangement of clusters by the steps
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The whole algorithm has a time complexity of ![](img/83121cdb-be4a-4a35-8e79-a30d97d7b1f0.png),
    with edges *m* and vertices *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Information entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a discrete random variable ![](img/de6d5b7f-f53f-4b22-87f4-dc2b89398466.png) with
    possible values (or outcomes) ![](img/d7ba7d07-5891-4791-bafa-23b331a70c2b.png) that
    occur with probability ![](img/bb4b527e-1e5e-4877-875b-03259d6a76e0.png), the
    entropy of ![](img/491e6796-8028-4b71-8efb-1f7ce3b9881a.png) is formally defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4362bfae-04db-43bf-9c60-ed3134dcc2ce.png)'
  prefs: []
  type: TYPE_IMG
- en: This is generally taken as the level of surprise, uncertainty, or chaos in a
    random variable.
  prefs: []
  type: TYPE_NORMAL
- en: If a variable is not discrete, we can apply binning (for example, via a histogram)
    or use non-discrete versions of the formula.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could have applied other algorithms, such as SCC, published by David Pearce
    in 2005 (in *An Improved Algorithm for Finding the Strongly Connected Components
    of a Directed Graph*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try this method out as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The SCC community fraudster distribution looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7663ef5e-972f-430e-b909-efc2c2175b99.png)'
  prefs: []
  type: TYPE_IMG
- en: And again we get a p-value that shows very high statistical significance. This
    means that this is unlikely to have occurred by pure chance, and indicates that
    our method is indeed a good classifier for fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have also applied more traditional clustering algorithms. For example,
    the affinity propagation algorithm takes an adjacency matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: There are a host of other methods that we could apply. For some of them, we'd
    have to convert the adjacency matrix to a distance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find reading materials about graph classification and graph algorithms
    on GitHub, collected by Benedek Rozemberczki, at [https://github.com/benedekrozemberczki/awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification).
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in graph convolution networks or graph attention networks,
    there's also a useful list for you at [https://github.com/Jiakui/awesome-gcn](https://github.com/Jiakui/awesome-gcn).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some very nice graph libraries around for Python with many implementations
    for community detection or graph analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Cdlib: [https://cdlib.readthedocs.io/en/latest/](https://cdlib.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karateclub: [https://karateclub.readthedocs.io/en/latest/](https://karateclub.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networkx: [https://networkx.github.io/](https://networkx.github.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation: [https://github.com/yamaguchiyuto/label_propagation](https://github.com/yamaguchiyuto/label_propagation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most Python libraries work with small- to medium-sized adjacency matrices (perhaps
    up to around 1,000 edges). Libraries suited for bigger data sizes include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Snap.py: [https://snap.stanford.edu/snappy/index.html](https://snap.stanford.edu/snappy/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python-louvain[:](https://github.com/taynaud/python-louvain) [https://github.com/taynaud/python-louvain](https://github.com/taynaud/python-louvain)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph-tool: [https://graph-tool.skewed.de/](https://graph-tool.skewed.de/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cdlib also contains the BigClam algorithm, which works with big graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Some graph databases such as neo4j, which comes with a Python interface, implement
    community detection algorithms: [https://neo4j.com/docs/graph-algorithms/current/](https://neo4j.com/docs/graph-algorithms/current/).
  prefs: []
  type: TYPE_NORMAL
