["```py\n    import pandas as pd\n    imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\n    reviews = imdb_df.review.to_string(index=None)\n    with open(\"corpus.txt\", \"w\") as f:\n          f.writelines(reviews)\n    ```", "```py\n    >>> from tokenizers import BertWordPieceTokenizer\n    >>> bert_wordpiece_tokenizer =BertWordPieceTokenizer()\n    >>> bert_wordpiece_tokenizer.train(\"corpus.txt\")\n    ```", "```py\n    >>> bert_wordpiece_tokenizer.get_vocab()\n    ```", "```py\n    {'almod': 9111, 'events': 3710, 'bogart': 7647, 'slapstick': 9541, 'terrorist': 16811, 'patter': 9269, '183': 16482, '##cul': 14292, 'sophie': 13109, 'thinki': 10265, 'tarnish': 16310, '##outh': 14729, 'peckinpah': 17156, 'gw': 6157, '##cat': 14290, '##eing': 14256, 'successfully': 12747, 'roomm': 7363, 'stalwart': 13347,...}\n    ```", "```py\n    >>> bert_wordpiece_tokenizer.save_model(\"tokenizer\")\n    ```", "```py\n    >>> tokenizer = \\ BertWordPieceTokenizer.from_file(\"tokenizer/vocab.txt\")\n    ```", "```py\n    >>> tokenized_sentence = \\\n    tokenizer.encode(\"Oh it works just fine\")\n    >>> tokenized_sentence.tokens\n    ['[CLS]', 'oh', 'it', 'works', 'just', 'fine','[SEP]']\n    ```", "```py\n    >>> tokenized_sentence = \\\n    tokenizer.encode(\"ohoh i thougt it might be workingg well\")\n    ['[CLS]', 'oh', '##o', '##h', 'i', 'thoug', '##t', 'it', 'might', 'be', 'working', '##g', 'well', '[SEP]']\n    ```", "```py\n    >>> from Transformers import BertTokenizerFast\n    >>> tokenizer = \\ BertTokenizerFast.from_pretrained(\"tokenizer\")\n    ```", "```py\n    >>> from Transformers import LineByLineTextDataset\n    >>> dataset = \\ \n    LineByLineTextDataset(tokenizer=tokenizer,\n                          file_path=\"corpus.txt\", \n                          block_size=128)\n    ```", "```py\n    >>> from Transformers import DataCollatorForLanguageModeling\n    >>> data_collator = DataCollatorForLanguageModeling(\n                          tokenizer=tokenizer, \n                          mlm=True, \n                          mlm_probability=0.15)\n    ```", "```py\n    >>> from Transformers import TrainingArguments\n    >>> training_args = TrainingArguments(\n                          output_dir=\"BERT\",\n                          overwrite_output_dir=True,\n                          num_train_epochs=1,\n                          per_device_train_batch_size=128)\n    ```", "```py\n    >>> from Transformers import BertConfig, BertForMaskedLM\n    >>> bert = BertForMaskedLM(BertConfig())\n    ```", "```py\n    >>> from Transformers import Trainer\n    >>> trainer = Trainer(model=bert, \n                          args=training_args,\n                          data_collator=data_collator,\n                          train_dataset=dataset)\n    ```", "```py\n    >>> trainer.train()\n    ```", "```py\n    >>> trainer.save_model(\"MyBERT\")\n    ```", "```py\n    >>> from Transformers import BertConfig\n    >>> BertConfig()\n    ```", "```py\n    >>> tiny_bert_config = \\ BertConfig(max_position_embeddings=512, hidden_size=128, \n               num_attention_heads=2, \n               num_hidden_layers=2, \n               intermediate_size=512)\n    >>> tiny_bert_config\n    ```", "```py\n    >>> tiny_bert = BertForMaskedLM(tiny_bert_config)\n    ```", "```py\n    >>> trainer = Trainer(model=tiny_bert, args=training_args,\n                         data_collator=data_collator,\n                         train_dataset=dataset)\n    >>> trainer.train()\n    ```", "```py\n    >>>  from Transformers import\\\n    TFBertModel, BertTokenizerFast\n    >>>  bert = TFBertModel.from_pretrained(\n    \"bert-base-uncased\")\n    >>> tokenizer = BertTokenizerFast.from_pretrained(\n    \"bert-base-uncased\")\n    ```", "```py\n    >>> bert.layers\n    [<Transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7f72459b1110>]\n    ```", "```py\n    >>> tokenized_text = tokenizer.batch_encode_plus(\n                       [\"hello how is it going with you\",\n                       \"lets test it\"], \n                        return_tensors=\"tf\", \n                        max_length=256, \n                        truncation=True, \n                        pad_to_max_length=True)\n    >>> bert(tokenized_text)\n    ```", "```py\n    from tensorflow import keras\n    import tensorflow as tf\n    max_length = 256\n    tokens = keras.layers.Input(shape=(max_length,),\n                               dtype=tf.dtypes.int32)\n    masks = keras.layers.Input(shape=(max_length,),\n                              dtype=tf.dtypes.int32)\n    embedding_layer = bert.layers[0]([tokens,masks])[0][:,0,:]\n    dense = tf.keras.layers.Dense(units=2, \n            activation=\"softmax\")(embedding_layer)\n    model = keras.Model([tokens,masks],dense)\n    ```", "```py\n    >>> tokenized = tokenizer.batch_encode_plus(\n    [\"hello how is it going with you\",\n    \"hello how is it going with you\"], \n    return_tensors=\"tf\", \n    max_length= max_length, \n    truncation=True, \n    pad_to_max_length=True)\n    ```", "```py\n    >>>model([tokenized[\"input_ids\"],tokenized[\"attention_mask\"]])\n    ```", "```py\n    >>> model.compile(optimizer=\"Adam\",\n    loss=\"categorical_crossentropy\", \n    metrics=[\"accuracy\"])\n    >>> model.summary()\n    ```", "```py\n    >>> model.layers[2].trainable = False\n    ```", "```py\n    import pandas as pd\n    imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\n    reviews = list(imdb_df.review)\n    tokenized_reviews = \\\n    tokenizer.batch_encode_plus(reviews, return_tensors=\"tf\",\n                               max_length=max_length,\n                               truncation=True,\n                               pad_to_max_length=True)\n    import numpy as np\n    train_split = int(0.8 * \\ len(tokenized_reviews[\"attention_mask\"]))\n    train_tokens = tokenized_reviews[\"input_ids\"]\\\n    [:train_split]\n    test_tokens = tokenized_reviews[\"input_ids\"][train_split:]\n    train_masks = tokenized_reviews[\"attention_mask\"]\\\n    [:train_split]\n    test_masks = tokenized_reviews[\"attention_mask\"]\\\n    [train_split:]\n    sentiments = list(imdb_df.sentiment)\n    labels = np.array([[0,1] if sentiment == \"positive\" else\\\n    [1,0] for sentiment in sentiments])\n    train_labels = labels[:train_split]\n    test_labels = labels[train_split:]\n    ```", "```py\n    >>> model.fit([train_tokens,train_masks],train_labels, \n                 epochs=5)\n    ```", "```py\n    Transformers-cli login\n    ```", "```py\n    Transformers-cli repo create a-fancy-model-name\n    ```", "```py\n    git lfs install\n    ```", "```py\n    git clone https://huggingface.co/username/a-fancy-model-name\n    ```", "```py\n    git add . && git commit -m \"Update from $USER\"\n    git push\n    ```", "```py\n    #BERT-BASE (L=12, H=768, A=12, Total Parameters=110M) \n    >> from Transformers import BertConfig, BertModel\n    >> bert_base= BertConfig()\n    >> model = BertModel(bert_base)\n    >> print(f\"{model.num_parameters() /(10**6)}\\\n     million parameters\")\n    109.48224 million parameters\n    ```", "```py\n    # Albert-base Configuration\n    >>> from Transformers import AlbertConfig, AlbertModel\n    >>> albert_base = AlbertConfig(hidden_size=768,\n                                  num_attention_heads=12,\n                                  intermediate_size=3072,)\n    >>> model = AlbertModel(albert_base)\n    >>> print(f\"{model.num_parameters() /(10**6)}\\\n    million parameters\")\n    11.683584 million parameters\n    ```", "```py\n    from Transformers import AlbertTokenizer, AlbertModel\n    tokenizer = \\\n    AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n    model = AlbertModel.from_pretrained(\"albert-base-v2\")\n    text = \"The cat is so sad .\"\n    encoded_input = tokenizer(text, return_tensors='pt')\n    output = model(**encoded_input)\n    ```", "```py\n    from Transformers import pipeline\n    fillmask= pipeline('fill-mask', model='albert-base-v2')\n    pd.DataFrame(fillmask(\"The cat is so [MASK] .\"))\n    ```", "```py\n>>> from Transformers import RobertaConfig, RobertaModel\n>>> conf= RobertaConfig()\n>>> model = RobertaModel(conf)\n>>> print(f\"{model.num_parameters() /(10**6)}\\\nmillion parameters\")\n109.48224 million parameters\n```", "```py\nfrom Transformers import RobertaTokenizer, RobertaModel\ntokenizer = \\\nRobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"The cat is so sad .\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```", "```py\n>>> from Transformers import pipeline\n>>> fillmask= pipeline(\"fill-mask \",model=\"roberta-base\",\n                       tokenizer=tokenizer)\n>>> pd.DataFrame(fillmask(\"The cat is so <mask> .\"))\n```", "```py\n>>> tokenizer = \\ \n AlbertTokenizer.from_pretrained('albert-base-v2')\n>>> print(tokenizer.mask_token)\n[MASK] \n>>> tokenizer = \\\nRobertaTokenizer.from_pretrained('roberta-base')\n>>> print(tokenizer.mask_token)\n<mask>\n```", "```py\nfillmask(f\"The cat is very\\\n{fillmask.tokenizer.mask_token}.\")\n```", "```py\nfillmask = \\\npipeline(\"fill-mask\", model=\"google/electra-small-generator\")\nfillmask(f\"The cat is very \\{fillmask.tokenizer.mask_token} .\")\n```", "```py\n>>> from Transformers import AutoModel, AutoTokenizer\n>>> tokenizerTUR = AutoTokenizer.from_pretrained(\n                   \"dbmdz/bert-base-turkish-uncased\")\n>>> print(f\"VOC size is: {tokenizerTUR.vocab_size}\")\n>>> print(f\"The model is: {type(tokenizerTUR)}\")\nVOC size is: 32000 \nThe model is: Transformers.models.bert.tokenization_bert_fast.BertTokenizerFast\n```", "```py\n>>> from Transformers import AutoModel, AutoTokenizer\n>>> tokenizerEN = \\\n AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> print(f\"VOC size is: {tokenizerEN.vocab_size}\")\n>>> print(f\"The model is {type(tokenizerEN)}\")\nVOC size is: 30522\nThe model is ... BertTokenizerFast\n```", "```py\n>>> word_en=\"telecommunication\"\n>>> print(f\"is in Turkish Model ? \\\n{word_en in tokenizerTUR.vocab}\")\n>>> print(f\"is in English Model ? \\\n{word_en in tokenizerEN.vocab}\")\nis in Turkish Model ? False\nis in English Model ? True\n```", "```py\n>>> tokens=tokenizerTUR.tokenize(word_en)\n>>> tokens\n['tel', '##eco', '##mm', '##un', '##ica', '##tion']\n```", "```py\n>>> [t in tokenizerTUR.vocab for t in tokens]\n[True, True, True, True, True, True]\n```", "```py\n>>> tokenizerEN.tokenize(word_en)\n['telecommunication']\n```", "```py\n>>> print(tokenizerTUR.tokenize(long_word_tur))\n['muvaffak', '##iyet', '##siz', '##les', '##tir', '##ici', '##les', '##tir', '##iver', '##emeye', '##bilecekleri', '##mi', '##z', '##den', '##mis', '##siniz', '##cesine']\n```", "```py\n$ pip install tokenizers\n```", "```py\n    import nltk \n    from nltk.corpus import gutenberg \n    nltk.download('gutenberg') \n    nltk.download('punkt') \n    plays=['shakespeare-macbeth.txt','shakespeare-hamlet.txt',\n          'shakespeare-caesar.txt']\n    shakespeare=[\" \".join(s) for ply in plays \\\n    for s in gutenberg.sents(ply)]\n    ```", "```py\n    from tokenizers.processors import TemplateProcessing\n    special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"]\n    temp_proc= TemplateProcessing(\n        single=\"[CLS] $A [SEP]\",\n        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n        special_tokens=[\n            (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n            (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n        ],\n    )\n    ```", "```py\n    from tokenizers import Tokenizer\n    from tokenizers.normalizers import \\\n    (Sequence,Lowercase, NFD, StripAccents)\n    from tokenizers.pre_tokenizers import Whitespace\n    from tokenizers.models import BPE\n    from tokenizers.decoders import BPEDecoder\n    ```", "```py\n    tokenizer = Tokenizer(BPE())\n    ```", "```py\n    tokenizer.normalizer = Sequence(\n    [NFD(),Lowercase(),StripAccents()])\n    tokenizer.pre_tokenizer = Whitespace()\n    tokenizer.decoder = BPEDecoder()\n    tokenizer.post_processor=temp_proc\n    ```", "```py\n    >>> from tokenizers.trainers import BpeTrainer\n    >>> trainer = BpeTrainer(vocab_size=5000, \n                            special_tokens= special_tokens)\n    >>> tokenizer.train_from_iterator(shakespeare,\n                                      trainer=trainer)\n    >>> print(f\"Trained vocab size:\\\n    {tokenizer.get_vocab_size()}\" )\n    Trained vocab size: 5000\n    ```", "```py\n    >>> sen= \"Is this a dagger which I see before me,\\\n     the handle toward my hand?\"\n    >>> sen_enc=tokenizer.encode(sen)\n    >>> print(f\"Output: {format(sen_enc.tokens)}\")\n    Output: ['[CLS]', 'is', 'this', 'a', 'dagger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'hand', 'le', 'toward', 'my', 'hand', '?', '[SEP]']\n    ```", "```py\n    >>> sen_enc2=tokenizer.encode(\"Macbeth and Hugging Face\") \n    >>> print(f\"Output: {format(sen_enc2.tokens)}\")\n    Output: ['[CLS]', 'macbeth', 'and', 'hu', 'gg', 'ing', 'face', '[SEP]']\n    ```", "```py\n    >>> two_enc=tokenizer.encode(\"I like Hugging Face!\",\n    \"He likes Macbeth!\")\n    >>> print(f\"Output: {format(two_enc.tokens)}\")\n    Output: ['[CLS]', 'i', 'like', 'hu', 'gg', 'ing', 'face', '!', '[SEP]', 'he', 'li', 'kes', 'macbeth', '!', '[SEP]']\n    ```", "```py\n    >>> tokenizer.model.save('.')\n    ['./vocab.json', './merges.txt']\n    ```", "```py\n    $ wc -l ./merges.txt\n    4948 ./merges.txt\n    ```", "```py\n    $ head -3 ./merges.txt\n    t h\n    o u\n    a n\n    th e\n    r e\n    ```", "```py\n    >>> tokenizer.save(\"MyBPETokenizer.json\")\n    >>> tokenizerFromFile = \\\n    Tokenizer.from_file(\"MyBPETokenizer.json\")\n    >>> sen_enc3 = \\\n    tokenizerFromFile.encode(\"I like Hugging Face and Macbeth\")\n    >>> print(f\"Output: {format(sen_enc3.tokens)}\")\n    Output: ['[CLS]', 'i', 'like', 'hu', 'gg', 'ing', 'face', 'and', 'macbeth', '[SEP]']\n    ```", "```py\n    from tokenizers.models import WordPiece\n    from tokenizers.decoders import WordPiece \\\n    as WordPieceDecoder\n    from tokenizers.normalizers import BertNormalizer \n    ```", "```py\n    tokenizer = Tokenizer(WordPiece())\n    tokenizer.normalizer=BertNormalizer()\n    tokenizer.pre_tokenizer = Whitespace()\n    tokenizer.decoder= WordPieceDecoder()\n    ```", "```py\n    >>> from tokenizers.trainers import WordPieceTrainer\n    >>> trainer = WordPieceTrainer(vocab_size=5000,\\\n                 special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\",\\\n                 \"[PAD]\", \"[MASK]\"])\n    >>> tokenizer.train_from_iterator(shakespeare,\n    trainer=trainer)\n    >>> output = tokenizer.encode(sen)\n    >>> print(output.tokens)\n    ['is', 'this', 'a', 'dagger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'hand', '##le', 'toward', 'my', 'hand', '?']\n    ```", "```py\n    >>> tokenizer.decode(output.ids)\n    'is this a dagger which i see before me, the handle toward my hand?'\n    ```", "```py\n    >>> tokenizer.encode(\"Kralsın aslansın Macbeth!\").tokens\n    '[UNK]', '[UNK]', 'macbeth', '!']\n    ```", "```py\n>>> from tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\n```"]