- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying Deep Learning Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **deep learning** (**DL**) models that are deployed in production environments
    are often different from the models that are fresh out of the training process.
    They are usually augmented to handle incoming requests with the highest performance.
    However, the target environments are often too broad, so a lot of customization
    is necessary to cover vastly different deployment settings. To overcome this difficulty,
    you can make use of **open neural network exchange** (**ONNX**), a standard file
    format for ML models. In this chapter, we will introduce how you can utilize ONNX
    to convert DL models between DL frameworks and how it separates the model development
    process from deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion between TensorFlow and ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion between PyTorch and ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the supplemental material for this chapter from the following
    GitHub link: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_8](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_8).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a variety of DL frameworks you can use to train a DL model. However,
    *one of the major difficulties in DL model deployment arises from the lack of
    interoperability among these frameworks*. For example, conversion between PyTorch
    and **TensorFlow** (**TF**) introduces many difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, DL models are augmented further for the deployment environment
    to increase accuracy and reduce inference latency, utilizing the acceleration
    provided by the underlying hardware. Unfortunately, this requires a broad knowledge
    of software as well as hardware because each type of hardware provides different
    accelerations for the running application. Hardware that is commonly used for
    DL includes the **Central Processing Unit** (**CPU**), **Graphical Processing
    Unit** (**GPU**), **Associative Processing Unit** (**APU**), **Tensor Processing
    Unit** (**TPU**), **Field Programmable Gate Array** (**FPGA**), **Vision Processing
    Unit** (**VPU**), **Neural Processing Unit** (**NPU**), and **JetsonBoard**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is not a one-time operation; once the model has been updated in
    any way, this process may need to be repeated. To reduce the engineering effort
    in this domain, a group of engineers have worked together to come up with a mediator
    that standardizes the model components: `.onnx` file that keeps track of how the
    model is designed and how each operation within a network is linked to other components.
    `.onnx` file ([https://github.com/lutzroeder/netron](https://github.com/lutzroeder/netron)).
    The following is a sample visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Netron visualization for an ONNX file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Netron visualization for an ONNX file
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, ONNX is a layer between training frameworks and deployment
    environments. While the ONNX file defines an exchange format, there also exists
    **ONNX Runtime** (**ORT**), which supports hardware-agnostic acceleration for
    ONNX models. In other words, the ONNX ecosystem allows you to choose any DL framework
    for training and makes hardware-specific optimization for deployment easily achievable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The position of ONNX in a DL project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – The position of ONNX in a DL project
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, ONNX helps with the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the model conversion among various DL frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing hardware-agnostic optimizations for DL models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will take a closer look at ORT.
  prefs: []
  type: TYPE_NORMAL
- en: Running inference using ONNX Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ORT is designed to support training and inferencing using ONNX models directly
    without converting them into a particular framework. However, training is not
    the main use case of ORT, so we will focus on the latter aspect, inferencing,
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: ORT leverages different hardware acceleration libraries, so-called **Execution
    Providers** (**EPs**), to improve the latency and accuracy of various hardware
    architectures. The ORT inference code will stay the same regardless of the DL
    framework used during model training and the underlying hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet is a sample ONNX inference code. The complete details
    can be found at [https://onnxruntime.ai/docs/get-started/with-python.html](https://onnxruntime.ai/docs/get-started/with-python.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `InferenceSession` class takes in a filename, a serialized ONNX model,
    or an ORT model in a byte string. In the preceding example, we specified the name
    of an ONNX file (`"model.onnx"`). The `providers` parameter and a list of execution
    providers ordered by precedence (such as `CPUExecutionProvider`, `TvmExecutionProvider`,
    `CUDAExecutionProvider`, and many more) are optional but important as they define
    the type of hardware acceleration that will be applied. In the last line, the
    `run` function triggers the model prediction. There are two main parameters for
    the `run` function: `output_names` (the names of the model’s output) and `input_feed`
    (the input dictionary with input names and values that you want to run model prediction
    with).'
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. ONNX provides a standardized and cross-platform representation for ML models.
  prefs: []
  type: TYPE_NORMAL
- en: b. ONNX can be used to convert a DL model implemented in one DL framework into
    another with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: c. ORT provides hardware-agnostic acceleration for deployed models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next two sections, we will look at the process of creating ONNX models
    using TF and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion between TensorFlow and ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will look at the conversion between TF and ONNX. We will break down
    the process into two: converting a TF model into an ONNX model and converting
    an ONNX model back into a TF model.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting a TensorFlow model into an ONNX model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`tf2onnx` is used to convert a TF model into an ONNX model ([https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)).
    This library supports both versions of TF (version 1 as well as version 2). Furthermore,
    conversions to deployment-specific TF formats such as TensorFlow.js and TensorFlow
    Lite are also available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert a TF model generated using the `saved_model` module into an ONNX
    model, you can use the `tf2onnx.convert` module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, `tensorflow-model-path` points to a TF model saved
    on disk, `--output` defines where the generated ONNX model will be saved, and
    `--opset` sets ONNX to `opset`, which defines the ONNX version and operators ([https://github.com/onnx/onnx/releases](https://github.com/onnx/onnx/releases)).
    If your TF model wasn’t saved using the `tf.saved_model.save` function, you need
    to specify the input and output format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands describe the conversion for models in Checkpoint ([https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint))
    and GraphDef ([https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef](https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef))
    formats. The key arguments are `--checkpoint` and `--graphdef`, which indicate
    the model format as well as the location of the source model.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf2onnx` also provides a Python API that you can find at [https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how to convert an ONNX model into a TF model.
  prefs: []
  type: TYPE_NORMAL
- en: Converting an ONNX model into a TensorFlow model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While `tf2onnx` is used for conversion from TF into ONNX, `onnx-tensorflow`
    ([https://github.com/onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow))
    is used for converting an ONNX model into a TF model. It is based on terminal
    commands as in the case of `tf2onnx`. The following line shows a simple `onnx-tf`
    command use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, the `-i` parameter is used to specify the source `.onnx`
    file, and the `-o` parameter is used to specify the output location for the new
    TF model. Other use cases of the `onnx-tf` command are well-documented at [https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md](https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, you can perform the same conversion using a Python API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding Python code, the ONNX model is loaded using the `onnx.load`
    function and then adjusted for conversion using `prepare`, which was imported
    from `onnx_tf.backend`. Finally, the TF model gets exported and saved to the specified
    location (`tensorflow_model_file_path`) using the `export_graph` function.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Conversions from TF into ONNX and from ONNX into TF are performed via `onnx-tensorflow`
    and `tf2onnx`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: b. Both `onnx-tensorflow` and `tf2onnx` support command-line interfaces as well
    as providing a Python API.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will describe how the conversions from and to ONNX are performed in
    PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion between PyTorch and ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explain how to convert a PyTorch model into an ONNX
    model and back again. With the conversion between TF and ONNX covered in the previous
    section, you should be able to convert your model between TF and PyTorch as well
    by the end of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a PyTorch model into an ONNX model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interestingly, PyTorch has built-in support for exporting its model as an ONNX
    model ([https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)).
    Given a model, all you need is the `torch.onnx.export` function as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter of `torch.onnx.export` is a PyTorch model that you want
    to convert. As the second parameter, you must provide a tensor that represents
    a dummy input. In other words, this tensor must be the size that the model is
    expecting as an input. The last parameter is the local path for the ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: After triggering the `torch.onnx.export` function, you should see an `.onnx`
    file generated at the path you provide (`onnx_model_path`).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at how to load an ONNX model as a PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: Converting an ONNX model into a PyTorch model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, PyTorch does not have built-in support for loading an ONNX model.
    However, there is a popular library for this conversion called `onnx2pytorch`
    ([https://github.com/ToriML/onnx2pytorch](https://github.com/ToriML/onnx2pytorch)).
    Given that this library is installed with a `pip` command, the following code
    snippet demonstrates the conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The key class we need from the `onnx2pytorch` module is `ConverModel`. As shown
    in the preceding code snippet, we pass an ONNX model into this class to generate
    a PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. PyTorch has built-in support for exporting a PyTorch model as an ONNX model.
    This process involves the `torch.onnx.export` function.
  prefs: []
  type: TYPE_NORMAL
- en: b. Importing an ONNX model into a PyTorch environment requires the `onnx2pytorch`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we described the conversion between ONNX and PyTorch. Since
    we already know how to convert a model between ONNX and TF, the conversion between
    TF and PyTorch comes naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced ONNX, a universal representation of ML models.
    The benefit of ONNX mostly comes from its model deployment, as it handles environment-specific
    optimization and conversions for us behind the scenes through ORT. Another advantage
    of ONNX comes from its interoperability; it can be used to convert a DL model
    generated with a framework for the other frameworks. In this chapter, we covered
    conversion for TensorFlow and PyTorch specifically, as they are the two most standard
    DL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Taking another step toward efficient DL model deployment, in the next chapter,
    we will learn how to use **Elastic Kubernetes Service** (**EKS**) and SageMaker
    to set up a model inference endpoint.
  prefs: []
  type: TYPE_NORMAL
