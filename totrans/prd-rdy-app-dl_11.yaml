- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying Deep Learning Model Deployment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **deep learning** (**DL**) models that are deployed in production environments
    are often different from the models that are fresh out of the training process.
    They are usually augmented to handle incoming requests with the highest performance.
    However, the target environments are often too broad, so a lot of customization
    is necessary to cover vastly different deployment settings. To overcome this difficulty,
    you can make use of **open neural network exchange** (**ONNX**), a standard file
    format for ML models. In this chapter, we will introduce how you can utilize ONNX
    to convert DL models between DL frameworks and how it separates the model development
    process from deployment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ONNX
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion between TensorFlow and ONNX
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion between PyTorch and ONNX
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the supplemental material for this chapter from the following
    GitHub link: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_8](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_8).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ONNX
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a variety of DL frameworks you can use to train a DL model. However,
    *one of the major difficulties in DL model deployment arises from the lack of
    interoperability among these frameworks*. For example, conversion between PyTorch
    and **TensorFlow** (**TF**) introduces many difficulties.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, DL models are augmented further for the deployment environment
    to increase accuracy and reduce inference latency, utilizing the acceleration
    provided by the underlying hardware. Unfortunately, this requires a broad knowledge
    of software as well as hardware because each type of hardware provides different
    accelerations for the running application. Hardware that is commonly used for
    DL includes the **Central Processing Unit** (**CPU**), **Graphical Processing
    Unit** (**GPU**), **Associative Processing Unit** (**APU**), **Tensor Processing
    Unit** (**TPU**), **Field Programmable Gate Array** (**FPGA**), **Vision Processing
    Unit** (**VPU**), **Neural Processing Unit** (**NPU**), and **JetsonBoard**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is not a one-time operation; once the model has been updated in
    any way, this process may need to be repeated. To reduce the engineering effort
    in this domain, a group of engineers have worked together to come up with a mediator
    that standardizes the model components: `.onnx` file that keeps track of how the
    model is designed and how each operation within a network is linked to other components.
    `.onnx` file ([https://github.com/lutzroeder/netron](https://github.com/lutzroeder/netron)).
    The following is a sample visualization:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Netron visualization for an ONNX file'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_08_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Netron visualization for an ONNX file
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, ONNX is a layer between training frameworks and deployment
    environments. While the ONNX file defines an exchange format, there also exists
    **ONNX Runtime** (**ORT**), which supports hardware-agnostic acceleration for
    ONNX models. In other words, the ONNX ecosystem allows you to choose any DL framework
    for training and makes hardware-specific optimization for deployment easily achievable:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，ONNX 是训练框架和部署环境之间的一层。虽然 ONNX 文件定义了一种交换格式，但也存在支持 ONNX 模型的 **ONNX Runtime**
    (**ORT**)，后者支持对 ONNX 模型进行硬件无关的加速优化。换句话说，ONNX 生态系统允许您选择任何 DL 框架进行训练，并使得部署时的硬件特定优化变得轻而易举。
- en: '![Figure 8.2 – The position of ONNX in a DL project'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – ONNX 在深度学习项目中的位置'
- en: '](img/B18522_08_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18522_08_02.jpg](img/B18522_08_02.jpg)'
- en: Figure 8.2 – The position of ONNX in a DL project
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – ONNX 在深度学习项目中的位置
- en: 'To summarize, ONNX helps with the following tasks:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，ONNX 有助于以下任务：
- en: Simplifying the model conversion among various DL frameworks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化不同深度学习框架之间的模型转换
- en: Providing hardware-agnostic optimizations for DL models
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为深度学习模型提供与硬件无关的优化
- en: In the following section, we will take a closer look at ORT.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将更详细地了解 ORT。
- en: Running inference using ONNX Runtime
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ONNX Runtime 运行推理
- en: ORT is designed to support training and inferencing using ONNX models directly
    without converting them into a particular framework. However, training is not
    the main use case of ORT, so we will focus on the latter aspect, inferencing,
    in this section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ORT 旨在直接支持使用 ONNX 模型进行训练和推理，无需将其转换为特定框架。然而，训练并不是 ORT 的主要用例，因此我们将专注于推理这一方面，在本节中进行讨论。
- en: ORT leverages different hardware acceleration libraries, so-called **Execution
    Providers** (**EPs**), to improve the latency and accuracy of various hardware
    architectures. The ORT inference code will stay the same regardless of the DL
    framework used during model training and the underlying hardware.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ORT 利用不同的硬件加速库，称为 **Execution Providers** (**EPs**)，以提高各种硬件架构的延迟和准确性。无论模型训练期间使用的
    DL 框架和底层硬件如何，ORT 推理代码保持不变。
- en: 'The following code snippet is a sample ONNX inference code. The complete details
    can be found at [https://onnxruntime.ai/docs/get-started/with-python.html](https://onnxruntime.ai/docs/get-started/with-python.html):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段是一个 ONNX 推理代码示例。完整详情请查阅 [https://onnxruntime.ai/docs/get-started/with-python.html](https://onnxruntime.ai/docs/get-started/with-python.html)。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `InferenceSession` class takes in a filename, a serialized ONNX model,
    or an ORT model in a byte string. In the preceding example, we specified the name
    of an ONNX file (`"model.onnx"`). The `providers` parameter and a list of execution
    providers ordered by precedence (such as `CPUExecutionProvider`, `TvmExecutionProvider`,
    `CUDAExecutionProvider`, and many more) are optional but important as they define
    the type of hardware acceleration that will be applied. In the last line, the
    `run` function triggers the model prediction. There are two main parameters for
    the `run` function: `output_names` (the names of the model’s output) and `input_feed`
    (the input dictionary with input names and values that you want to run model prediction
    with).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`InferenceSession` 类接受文件名、序列化的 ONNX 模型或 ORT 模型的字节字符串作为输入。在上述示例中，我们指定了一个 ONNX
    文件的名称 (`"model.onnx"`)。`providers` 参数和按优先顺序排列的执行提供者列表（如 `CPUExecutionProvider`、`TvmExecutionProvider`、`CUDAExecutionProvider`
    等）是可选的，但非常重要，因为它们定义了将应用的硬件加速类型。在最后一行，`run` 函数触发模型预测。`run` 函数有两个主要参数：`output_names`（模型输出的名称）和
    `input_feed`（输入字典，包含您希望使用模型进行预测的输入名称和值）。'
- en: Things to remember
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事项
- en: a. ONNX provides a standardized and cross-platform representation for ML models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: a. ONNX 提供了用于 ML 模型的标准化和跨平台的表示。
- en: b. ONNX can be used to convert a DL model implemented in one DL framework into
    another with minimal effort.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: b. ONNX 可以用于将一个 DL 框架中实现的模型转换为另一个框架，转换过程需要很少的工作量。
- en: c. ORT provides hardware-agnostic acceleration for deployed models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: c. ORT 为已部署的模型提供与硬件无关的加速。
- en: In the next two sections, we will look at the process of creating ONNX models
    using TF and PyTorch.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将看看使用 TF 和 PyTorch 创建 ONNX 模型的过程。
- en: Conversion between TensorFlow and ONNX
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 和 ONNX 之间的转换
- en: 'First, we will look at the conversion between TF and ONNX. We will break down
    the process into two: converting a TF model into an ONNX model and converting
    an ONNX model back into a TF model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将研究 TF 到 ONNX 的转换。我们将这个过程分解为两步：将 TF 模型转换为 ONNX 模型，以及将 ONNX 模型转换回 TF 模型。
- en: Converting a TensorFlow model into an ONNX model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 TensorFlow 模型转换为 ONNX 模型
- en: '`tf2onnx` is used to convert a TF model into an ONNX model ([https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)).
    This library supports both versions of TF (version 1 as well as version 2). Furthermore,
    conversions to deployment-specific TF formats such as TensorFlow.js and TensorFlow
    Lite are also available.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf2onnx` 用于将 TF 模型转换为 ONNX 模型 ([https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx))。此库支持
    TF 的两个版本（版本 1 和版本 2）。此外，还支持将模型转换为特定部署的 TF 格式，如 TensorFlow.js 和 TensorFlow Lite。'
- en: 'To convert a TF model generated using the `saved_model` module into an ONNX
    model, you can use the `tf2onnx.convert` module, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要将使用 `saved_model` 模块生成的 TF 模型转换为 ONNX 模型，可以使用 `tf2onnx.convert` 模块，如下所示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding command, `tensorflow-model-path` points to a TF model saved
    on disk, `--output` defines where the generated ONNX model will be saved, and
    `--opset` sets ONNX to `opset`, which defines the ONNX version and operators ([https://github.com/onnx/onnx/releases](https://github.com/onnx/onnx/releases)).
    If your TF model wasn’t saved using the `tf.saved_model.save` function, you need
    to specify the input and output format as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`tensorflow-model-path` 指向磁盘上保存的 TF 模型，`--output` 定义了生成的 ONNX 模型保存的位置，`--opset`
    设置了 ONNX 的 `opset`，它定义了 ONNX 的版本和操作符 ([https://github.com/onnx/onnx/releases](https://github.com/onnx/onnx/releases))。如果您的
    TF 模型未使用 `tf.saved_model.save` 函数保存，需要按照以下格式指定输入和输出格式：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding commands describe the conversion for models in Checkpoint ([https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint))
    and GraphDef ([https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef](https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef))
    formats. The key arguments are `--checkpoint` and `--graphdef`, which indicate
    the model format as well as the location of the source model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令描述了 Checkpoint ([https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint))
    和 GraphDef ([https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef](https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef))
    格式模型的转换。关键参数是 `--checkpoint` 和 `--graphdef`，它们指示了模型格式以及源模型的位置。
- en: '`tf2onnx` also provides a Python API that you can find at [https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf2onnx` 还提供了一个 Python API，您可以在 [https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)
    找到它。'
- en: Next, we will look at how to convert an ONNX model into a TF model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下如何将 ONNX 模型转换为 TF 模型。
- en: Converting an ONNX model into a TensorFlow model
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 ONNX 模型转换为 TensorFlow 模型
- en: 'While `tf2onnx` is used for conversion from TF into ONNX, `onnx-tensorflow`
    ([https://github.com/onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow))
    is used for converting an ONNX model into a TF model. It is based on terminal
    commands as in the case of `tf2onnx`. The following line shows a simple `onnx-tf`
    command use case:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `tf2onnx` 用于从 TF 到 ONNX 的转换，但 `onnx-tensorflow` ([https://github.com/onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow))
    用于将 ONNX 模型转换为 TF 模型。它与 `tf2onnx` 一样，基于终端命令。以下是一个简单的 `onnx-tf` 命令示例：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding command, the `-i` parameter is used to specify the source `.onnx`
    file, and the `-o` parameter is used to specify the output location for the new
    TF model. Other use cases of the `onnx-tf` command are well-documented at [https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md](https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`-i` 参数用于指定源 `.onnx` 文件，而 `-o` 参数用于指定新 TF 模型的输出位置。`onnx-tf` 命令的其他用例在
    [https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md](https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md)
    中有详细说明。
- en: 'In addition, you can perform the same conversion using a Python API:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您也可以使用 Python API 执行相同的转换：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding Python code, the ONNX model is loaded using the `onnx.load`
    function and then adjusted for conversion using `prepare`, which was imported
    from `onnx_tf.backend`. Finally, the TF model gets exported and saved to the specified
    location (`tensorflow_model_file_path`) using the `export_graph` function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述 Python 代码中，使用 `onnx.load` 函数加载 ONNX 模型，然后使用从 `onnx_tf.backend` 导入的 `prepare`
    进行调整，最后使用 `export_graph` 函数将 TF 模型导出并保存到指定位置 (`tensorflow_model_file_path`)。
- en: Things to remember
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. Conversions from TF into ONNX and from ONNX into TF are performed via `onnx-tensorflow`
    and `tf2onnx`, respectively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: a. 从 TF 到 ONNX 的转换和从 ONNX 到 TF 的转换分别通过 `onnx-tensorflow` 和 `tf2onnx` 完成。
- en: b. Both `onnx-tensorflow` and `tf2onnx` support command-line interfaces as well
    as providing a Python API.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: b. `onnx-tensorflow` 和 `tf2onnx` 都支持命令行界面和提供 Python API。
- en: Next, we will describe how the conversions from and to ONNX are performed in
    PyTorch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述在 PyTorch 中如何执行从 ONNX 到 ONNX 的转换。
- en: Conversion between PyTorch and ONNX
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 和 ONNX 之间的转换
- en: In this section, we will explain how to convert a PyTorch model into an ONNX
    model and back again. With the conversion between TF and ONNX covered in the previous
    section, you should be able to convert your model between TF and PyTorch as well
    by the end of this section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何将 PyTorch 模型转换为 ONNX 模型，然后再转换回来。在前一节已经覆盖了 TF 和 ONNX 之间的转换，所以通过本节结束时，你应该能够完成
    TF 和 PyTorch 之间模型的转换。
- en: Converting a PyTorch model into an ONNX model
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 PyTorch 模型转换为 ONNX 模型
- en: 'Interestingly, PyTorch has built-in support for exporting its model as an ONNX
    model ([https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)).
    Given a model, all you need is the `torch.onnx.export` function as shown in the
    following code snippet:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，PyTorch 内置支持将其模型导出为 ONNX 模型 ([https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html))。给定一个模型，你只需要使用
    `torch.onnx.export` 函数，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first parameter of `torch.onnx.export` is a PyTorch model that you want
    to convert. As the second parameter, you must provide a tensor that represents
    a dummy input. In other words, this tensor must be the size that the model is
    expecting as an input. The last parameter is the local path for the ONNX model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.onnx.export` 的第一个参数是你想要转换的 PyTorch 模型。第二个参数必须是一个表示虚拟输入的张量。换句话说，这个张量必须是模型期望输入的大小。最后一个参数是
    ONNX 模型的本地路径。'
- en: After triggering the `torch.onnx.export` function, you should see an `.onnx`
    file generated at the path you provide (`onnx_model_path`).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 触发 `torch.onnx.export` 函数后，你应该能够看到一个 `.onnx` 文件生成在你提供的路径下 (`onnx_model_path`)。
- en: Now, let’s look at how to load an ONNX model as a PyTorch model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将一个 ONNX 模型加载为 PyTorch 模型。
- en: Converting an ONNX model into a PyTorch model
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 ONNX 模型转换为 PyTorch 模型。
- en: 'Unfortunately, PyTorch does not have built-in support for loading an ONNX model.
    However, there is a popular library for this conversion called `onnx2pytorch`
    ([https://github.com/ToriML/onnx2pytorch](https://github.com/ToriML/onnx2pytorch)).
    Given that this library is installed with a `pip` command, the following code
    snippet demonstrates the conversion:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，PyTorch 没有内置支持加载 ONNX 模型的功能。但是，有一个名为 `onnx2pytorch` 的流行库可用于此转换 ([https://github.com/ToriML/onnx2pytorch](https://github.com/ToriML/onnx2pytorch))。假设这个库通过
    `pip` 命令安装，下面的代码片段展示了这个转换过程：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The key class we need from the `onnx2pytorch` module is `ConverModel`. As shown
    in the preceding code snippet, we pass an ONNX model into this class to generate
    a PyTorch model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `onnx2pytorch` 模块中需要的关键类是 `ConverModel`。如前面的代码片段所示，我们将一个 ONNX 模型传递给这个类来生成一个
    PyTorch 模型。
- en: Things to remember
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需记住的事项
- en: a. PyTorch has built-in support for exporting a PyTorch model as an ONNX model.
    This process involves the `torch.onnx.export` function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: a. PyTorch 内置支持将 PyTorch 模型导出为 ONNX 模型。这个过程涉及到 `torch.onnx.export` 函数。
- en: b. Importing an ONNX model into a PyTorch environment requires the `onnx2pytorch`
    library.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将 ONNX 模型导入 PyTorch 环境需要使用 `onnx2pytorch` 库。
- en: In this section, we described the conversion between ONNX and PyTorch. Since
    we already know how to convert a model between ONNX and TF, the conversion between
    TF and PyTorch comes naturally.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了在 ONNX 和 PyTorch 之间的转换过程。由于我们已经知道如何在 ONNX 和 TF 之间转换模型，所以 TF 和 PyTorch
    之间的转换自然而然地进行了。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced ONNX, a universal representation of ML models.
    The benefit of ONNX mostly comes from its model deployment, as it handles environment-specific
    optimization and conversions for us behind the scenes through ORT. Another advantage
    of ONNX comes from its interoperability; it can be used to convert a DL model
    generated with a framework for the other frameworks. In this chapter, we covered
    conversion for TensorFlow and PyTorch specifically, as they are the two most standard
    DL frameworks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 ONNX，这是一个通用的 ML 模型表示方式。ONNX 的好处主要来自于其模型部署能力，因为它可以通过 ORT 在幕后处理环境特定的优化和转换。ONNX
    的另一个优势来自于其互操作性；它可以用来将一个使用某一框架生成的 DL 模型转换为其他框架的模型。在本章中，我们特别介绍了 TensorFlow 和 PyTorch
    的转换，因为它们是两个最常见的 DL 框架。
- en: Taking another step toward efficient DL model deployment, in the next chapter,
    we will learn how to use **Elastic Kubernetes Service** (**EKS**) and SageMaker
    to set up a model inference endpoint.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在迈向高效的深度学习模型部署的又一步中，在下一章中，我们将学习如何使用**弹性 Kubernetes 服务**（**EKS**）和 SageMaker
    来建立一个模型推理端点。
