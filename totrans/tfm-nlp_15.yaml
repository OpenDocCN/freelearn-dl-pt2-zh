- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From NLP to Task-Agnostic Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to now, we have examined variations of the original Transformer model with
    encoder and decoder layers, and we explored other models with encoder-only or
    decoder-only stacks of layers. Also, the size of the layers and parameters has
    increased. However, the fundamental architecture of the transformer retains its
    original structure with identical layers and the parallelization of the computing
    of the attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore innovative transformer models that respect
    the basic structure of the original Transformer but make some significant changes.
    Scores of transformer models will appear, like the many possibilities a box of
    LEGO^© pieces gives. You can assemble those pieces in hundreds of ways! Transformer
    model sublayers and layers are the LEGO^© pieces of advanced AI.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by asking which transformer model to choose among the many offers
    and the ecosystem we will implement them in.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will discover **Locality Sensitivity Hashing** (**LSH**) buckets and
    chunking in Reformer models. We will then learn what disentanglement is in DeBERTa
    models. DeBERTa also introduces an alternative way of managing positions in the
    decoder. DeBERTA’s high-powered transformer model exceeds human baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Our last step stops will be to discover powerful computer vision transformers
    such as Vit, CLIP, and DALL-E. We can add CLIP and DALL-E to OpenAI GPT-3 and
    Google BERT (trained by Google) to the very small group of **foundation** **models**.
  prefs: []
  type: TYPE_NORMAL
- en: These powerful foundation models prove that transformers are *task-agnostic*.
    A transformer learns sequences. These sequences include vision, sound, and any
    type of data represented as a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Images contain sequences of data-like language. We will run ViT, CLIP, and DALL-E
    models to learn. We will take vision models to innovative levels.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will see that the world of *task-agnostic* transformers
    has evolved into a universe of imagination and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a transformer model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Reformer transformer model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locality Sensitivity Hashing** (**LSH**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucket and chunking techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DeBERTA transformer model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disentangled attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Absolute positions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-image vision transformers with CLIP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DALL-E, a creative text-image vision transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to see how to choose a model and an ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a model and an ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We thought that testing transformer models by downloading them would require
    machine and human resources. Also, you might have thought that if a platform doesn’t
    have an online sandbox by this time, it will be a risk to go further because of
    the work to test a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: However, sites such as Hugging Face download pretrained models automatically
    in real time, as we will see in *The Reformer* and *DeBERTa* sections! So, what
    should we do? Thanks to that, we can run Hugging Face models in Google Colab without
    installing anything on the machine ourselves. We can also test Hugging Face models
    online.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to analyze without having anything to “install.” “Nothing to Install”
    in 2022 can mean:'
  prefs: []
  type: TYPE_NORMAL
- en: Running a transformer task online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a transformer on a preinstalled Google Colaboratory VM that seamlessly
    downloads a pretrained model for a task, which we can run in a few lines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a transformer through an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of “install” has expanded over the past few years. The definition
    of “online” has widened. We can consider using a few lines of code to run an API
    as a meta-online test.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will refer to “without installing,” and “online” in a broad sense in this
    section. *Figure 15.1* shows how we should test models “online”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with low confidence](img/B17948_15_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Testing transformer models online'
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing in this decade has become flexible and productive, as the following
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face hosts API models such as DeBERTa and some other models. In addition,
    Hugging Face offers an AutoML service to train and deploy transformer models in
    their ecosystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s GPT-3 engine runs on the online playground and provides an API. OpenAI
    offers models that cover many NLP tasks. The models require no training. GPT-3’s
    billion-parameter zero-shot engine is impressive. It shows that transformer models
    with many parameters produce better results overall. Microsoft Azure, Google Cloud
    AI, AllenNLP, and other platforms offer interesting services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An online model analysis can be done by reading a paper if it is worthwhile.
    A good example is Google’s publication by *Fedus* et al., (2021), on *Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. Google
    increased the size of the T5-based models we studied in *Chapter 8*, *Applying
    Transformers to Legal and Financial Documents for AI Text Summarization*. This
    paper confirms the strategy of large online models such as GTP-3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, in the end, *you are the one taking the risk* of choosing one solution
    over another. The time you spend on exploring platforms and models will help you
    optimize the implementation of your project once you have made your choice.
  prefs: []
  type: TYPE_NORMAL
- en: You can host your choice in three different ways, as shown in *Figure 15.2:*
  prefs: []
  type: TYPE_NORMAL
- en: On a local machine using an API. OpenAI, Google Cloud AI, Microsoft Azure AI,
    Hugging Face, and others provide good APIs. An application can be on a local machine
    and not on a cloud platform but can go through a cloud service with an API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On a cloud platform such as **Amazon Web Services** (**AWS**) or Google Cloud.
    You can train, fine-tune, test, and run the models on these platforms. In this
    case, there is no application on a local machine. Everything is on the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From anywhere using an API! On a local machine, a data center VM, or from anywhere.
    This means that the API would be integrated in a physical system such as a windmill,
    an airplane, a rocket, or an autonomous vehicle. The system could thus permanently
    connect with another system through an API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_15_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Implementing options for your models'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, it is up to you to make the decision. Take your time. Test, analyze,
    compute the costs, and work as a team to listen to different perspectives. *The
    more you understand how transformers work, the better the choices you make will
    be*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the Reformer, a variation of the original Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Kitaev* et al. (2020) designed the Reformer to solve the attention and memory
    issues, adding functionality to the original Transformer model.'
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer first solves the attention issue with **Locality Sensitivity Hashing**
    (**LSH**) buckets and chunking.
  prefs: []
  type: TYPE_NORMAL
- en: LSH searches for nearest neighbors in datasets. The hash function determines
    that if datapoint *q* is close to *p*, then *hash*(*q*) == *hash*(*p*). In this
    case, the data points are the keys of the transformer model’s heads.
  prefs: []
  type: TYPE_NORMAL
- en: The LSH function converts the keys into LSH *buckets* (*B1* to *B4* in *Figure
    15.3*) in a process called LSH bucketing, just like how we would take objects
    similar to each other and put them in the same sorted buckets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sorted buckets are split into *chunks* (*C1* to *C4* in *Figure 15.3*)
    to parallelize. Finally, attention will only be applied within the same bucket
    in its chunk and the previous chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: LSH attention heads'
  prefs: []
  type: TYPE_NORMAL
- en: LSH bucketing and chunking considerably reduce the complexity from *O*(*L*²),
    attending to all the word pairs, to *O*(*L*log*L*), only attending to the content
    of each bucket.
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer also solves the memory issue of recomputing each layer’s input
    instead of storing the information for multi-layer models. The recomputing is
    achieved on-demand instead of consuming terabytes of memory for some large multi-layer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: We will now use a Reformer model trained on the English translation of *Crime
    and Punishment* by *Fyodor Dostoevsky*.
  prefs: []
  type: TYPE_NORMAL
- en: Running an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s run it directly online with the hosted inference API. The input sentence
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The student``was impoverished and did not know what to do.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The link to the online interface contains the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do](https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hosted inference API appears with the input sentence. Click on `compute`
    to obtain an inference, and the result will appear right under the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: The Reformer’s hosted inference API'
  prefs: []
  type: TYPE_NORMAL
- en: You might get a different response since the algorithm is stochastic. The Reformer
    was reasonably trained, though not with a supercomputer with billions of bits
    of information like OpenAI’s GPT-3\. The result of the Reformer is not very impressive.
    It would take more training and fine-tuning to obtain better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s GPT-3 engine produces the following result for text completion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The student was impoverished and did not know what to do. He did not have any
    one to turn to and could not find a place to stay. He took out a pad from his
    bag and started writing. He wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"My name is XXXXXXXXXX. I am a student at XXXXXXXXXX. I have no family, no
    friends, no money."`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is more convincing. You can access OpenAI’s playground after having
    signed up: [https://openai.com/](https://openai.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: OpenAI GPT-3, as with other transformer models and most deep learning
    models, is based on stochastic algorithms. The results might vary from one to
    another.'
  prefs: []
  type: TYPE_NORMAL
- en: This shows that a highly well-trained transformer model containing billions
    of parameters can outperform an innovative transformer model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Will supercomputer-driven cloud AI platforms progressively outperform local
    attempts or even less powerful cloud platforms? You need to address these issues
    through prototypes before investing in one solution over another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: The stochastic nature of transformer models may produce different
    results when running them. Also, online platforms continually change their interfaces.
    We need to accept that and adapt.'
  prefs: []
  type: TYPE_NORMAL
- en: DeBERTa introduces another innovative architecture, which we will now explore.
  prefs: []
  type: TYPE_NORMAL
- en: DeBERTa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another new approach to transformers can be found through *disentanglement*.
    *Disentanglement* in AI allows you to separate the representation features to
    make the training process more flexible. *Pengcheng He*, *Xiaodong Liu*, *Jianfeng
    Gao*, and *Weizhu Chen* designed DeBERTa, a disentangled version of a transformer,
    and described the model in an interesting article: *DeBERTa: Decoding-enhanced
    BERT with Disentangled Attention*: [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main ideas implemented in DeBERTa are:'
  prefs: []
  type: TYPE_NORMAL
- en: Disentangle the content and position in the transformer model to train the two
    vectors separately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an absolute position in the decoder to predict masked tokens in the pretraining
    process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The authors provide the code on GitHub: [https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeBERTa exceeds the human baseline on the SuperGLUE leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: DeBERTa on the SuperGLUE leaderboard'
  prefs: []
  type: TYPE_NORMAL
- en: Remove any space before Let’s run an example on Hugging Face’s cloud platform.
  prefs: []
  type: TYPE_NORMAL
- en: Running an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run an example on Hugging Face’s cloud platform, click on the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/cross-encoder/nli-deberta-base](https://huggingface.co/cross-encoder/nli-deberta-base)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hosted inference API will appear with an example and output of possible
    class names:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: DeBERTa’s hosted inference API'
  prefs: []
  type: TYPE_NORMAL
- en: The possible class names are `mobile`, `website`, `billing`, and `account access`.
  prefs: []
  type: TYPE_NORMAL
- en: The result is interesting. Let’s compare it to a GPT-3 keyword task. First,
    sign up on [https://openai.com/](https://openai.com/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter `Text` as the input and `Keywords` to ask the engine to find keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text**: `Last week I upgraded my iOS version and ever since then my phone
    has been overheating whenever I use your app.`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keywords**: `app, overheating, phone`'
  prefs: []
  type: TYPE_NORMAL
- en: The possible keywords are `app`, `overheating`, and `phone`.
  prefs: []
  type: TYPE_NORMAL
- en: We have gone through the DeBERTa and GPT-3 transformers. We will now extend
    transformers to vision models.
  prefs: []
  type: TYPE_NORMAL
- en: From Task-Agnostic Models to Vision Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Foundation models, as we saw in *Chapter 1*, *What Are Transformers?*, have
    two distinct and unique properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Emergence** – Transformer models that qualify as foundation models can perform
    tasks they were not trained for. They are large models trained on supercomputers.
    They are not trained to learn specific tasks like many other models. Foundation
    models learn how to understand sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homogenization** – The same model can be used across many domains with the
    same fundamental architecture. Foundation models can learn new skills through
    data faster and better than any other model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 and Google BERT (only the BERT models trained by Google) are task-agnostic
    foundation models. These task-agnostic models lead directly to ViT, CLIP, and
    DALL-E models. Transformers have uncanny sequence analysis abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The level of abstraction of transformer models leads to multi-modal neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-modal neurons** can processimages that can be tokenized as pixels or
    image patches. Then they can be processed as *words* in vision transformers. Once
    an image has been encoded, transformer models see the tokens as any *word* token,
    as shown in *Figure 15.7*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_15_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Images can be encoded into word-like tokens'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will go through:'
  prefs: []
  type: TYPE_NORMAL
- en: ViT, vision transformers that process images as patches of *words*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLIP, vision transformers that encode text and image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DALL-E, vision transformers that construct images with text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by exploring ViT, a vision transformer that processes images as
    patches of *words*.
  prefs: []
  type: TYPE_NORMAL
- en: ViT – Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dosovitskiy* et al. (2021) summed up the essence of the vision transformer
    architecture they designed in the title of their paper: *An Image is Worth 16x16
    Words: Transformers for Image Recognition at Scale*.'
  prefs: []
  type: TYPE_NORMAL
- en: An image can be converted into patches of 16x16 words.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first see the architecture of a ViT before looking into the code.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Architecture of ViT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A vision transformer can process an image as patches of *words*. In this section,
    we will go through the process in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the image into patches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A linear projection of the patches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hybrid input embedding sublayer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step is to SPLIT the image into equal-sized patches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Splitting the image into patches'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The image is split into *n* patches, as shown in *Figure 15.8*. There is no
    rule saying how many patches as long as all the patches have the same dimensions,
    such as 16x16:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, treemap chart  Description automatically generated](img/B17948_15_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: Splitting an image into patches'
  prefs: []
  type: TYPE_NORMAL
- en: The patches of equal dimensions now represent the *words* of our sequence. The
    problem of what to do with these patches remains. We will see that each type of
    vision transformer has its own method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image citation: The image of cats used in this section and the subsequent sections
    was taken by *DocChewbacca*: [https://www.flickr.com/photos/st3f4n/](https://www.flickr.com/photos/st3f4n/),
    in 2006\. It is under a Flickr free license, [https://creativecommons.org/licenses/by-sa/2.0/](https://creativecommons.org/licenses/by-sa/2.0/).
    For more details, see *DocChewbacca*’s image on Flickr: [https://www.flickr.com/photos/st3f4n/210383891](https://www.flickr.com/photos/st3f4n/210383891)'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, for the ViT, *Step 2* will be to make a linear projection of flattened
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Linear projection of flattened images'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Step 1* converted an image to equal-sized patches. The motivation of the patches
    is to avoid processing the image pixel by pixel. However, the problem remains
    to find a way to process the patches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The team at Google Research decided to design a linear projection of flattened
    images with the patches obtained by splitting the image, as shown in *Figure 15.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Linear projection of flattened images'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to obtain a sequence of work-like patches. The remaining problem
    is to embed the sequence of flattened images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: The hybrid input embedding sublayer'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Word-like image sequences can fit into a transformer. The problem is that they
    still are images!
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Research decided that a hybrid input model would do the job, as shown
    in *Figure 15.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a convolutional network to embed the linear projection of the patches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add positional encoding to retain the structure of the original image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then process the embedded input with a standard original BERT-like encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_15_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: A hybrid input sublayer and a standard encoder'
  prefs: []
  type: TYPE_NORMAL
- en: Google Research found a clever way to convert an NLP transformer model into
    a vision transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s implement a Hugging Face example of a vision transformer in code.
  prefs: []
  type: TYPE_NORMAL
- en: Vision transformers in code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will focus on the main areas of code that relate to the
    specific architecture of vision transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Open `Vision_Transformers.ipynb`, which is in the GitHub repository for this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Colab VM’s contain many pre-installed packages such as `torch` and `torchvision`.
    You can display them by uncommenting the command in the first cell of the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then go to the **Vision Transformer** (**ViT**) cell of the notebook. The notebook
    first installs Hugging Face transformers and imports the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**: At the time of writing this book, Hugging Face warns us that the
    code can be unstable due to constant evolutions. This should not stop us from
    exploring ViT models. Testing new territory is what the cutting edge is all about!'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then download an image from the COCO dataset. You can find a comprehensive
    corpus of datasets on their website if you wish to experiment further: [https://cocodataset.org/](https://cocodataset.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s download from the VAL2017 dataset. Follow the COCO dataset website instructions
    to obtain these images through programs or download the datasets locally.
  prefs: []
  type: TYPE_NORMAL
- en: The VAL2017 contains 5,000 images we can choose from to test this ViT model.
    You can run any of the 5,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test the notebook with the image of the cats. We first retrieve the image
    of the cats through their URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We next download Google’s feature extractor and classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The model was trained on 224 x 244 resolution images but was presented with
    16 x 16 patches for feature extraction and classification. The notebook runs the
    model and makes a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Explore the code that follows the prediction, which gives us information at
    a low level, among which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.config.id2label`, which will list the labels of the classes. The 1000
    label classes explain why we obtain a class and not a detailed text description:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`model`, which will display the architecture of the model that begins with
    the hybrid usage of a convolutional input sublayer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the convolutional input embedding sublayer, the model is a BERT-like encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Take your time to explore this innovative move from NLP transformers to transformers
    for images, leading to transformers for everything quite rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go through CLIP, another computer vision model.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Contrastive Language-Image Pre-Training** (**CLIP**) follows the philosophy
    of transformers. It plugs sequences of data in its transformer-type layers. Instead
    of sending text pairs, this time, the model sends text-image pairs. Once the data
    is tokenized, encoded, and embedded, CLIP, a task-agnostic model, learns text-image
    pairs as with any other sequence of data.'
  prefs: []
  type: TYPE_NORMAL
- en: The method is contrastive because it looks for the contrasts in the features
    of the image. It is the method we use in some magazine games in which we have
    to find the differences, the contrasts, between two images.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first see the architecture of CLIP before looking into the code.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Architecture of CLIP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Contrastive: the images are trained to learn how they fit together through
    their differences and similarities. The image and captions find their way toward
    each other through (joint text, image) pretraining. After pretraining, CLIP learns
    new tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: CLIPs are transferable because they can learn new visual concepts, like GPT
    models, such as action recognition in video sequences. The captions lead to endless
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'ViT splits images into word-like patches. CLIP jointly trains *text and image*
    encoders for (caption, image) pairs to maximize cosine similarity, as shown in
    *Figure 15.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_15_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Jointly training text and images'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.11* shows how the transformer will run a standard transformer encoder
    for the text input. It will run a ResNet 50-layer CNN for the images in a transformer
    structure. ResNet 50 was modified to run an average pooling layer in an attention
    pooling mechanism with a multi-head QKV attention head.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how CLIP learns text-image sequences to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP in code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open `Vision_Transformers.ipynb`, which is in the repository for this chapter
    on GitHub. Then go to the `CLIP` cell of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program begins by installing PyTorch and CLIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The program also imports the modules and CIFAR-100 to access the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 10,000 images available with an index between 0 and 9,999\. The next
    step is to select an image we want to run a prediction on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_15_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: Selecting an image index'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program then loads the model on the device that is available (GPU or CPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The images are downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The input is prepared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s visualize the input we selected before running the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that `index 15` is a lion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_15_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: Image of Index 15'
  prefs: []
  type: TYPE_NORMAL
- en: 'The images in this section are from *Learning Multiple Layers of Features from
    Tiny Images*, *Alex Krizhevsky*, 2009: [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
    They are part of the `CIFAR-10` and `CIFAR-100` datasets (`toronto.edu`): [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We know this is a lion because we are humans. A transformer initially designed
    for NLP has to learn what an image is. We will now see how well it can recognize
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program shows that it is running a joint transformer model by separating
    the image input from the text input when calculating the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now CLIP makes a prediction and displays the top five predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can modify `topk(5)` if you want to obtain more or fewer predictions. The
    top five predictions are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: CLIP found the lion, which shows the flexibility of transformer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next cell displays the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can go through the classes to see that with only one label per class, which
    is restrictive, CLIP did a good job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The notebook contains several other cells describing the architecture and configuration
    of CLIP that you can explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model` cell is particularly interesting because you can see the visual
    encoder that begins with a convolutional embedding like for the ViT model and
    then continues as a “standard” size-768 transformer with multi-head attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Another interesting aspect of the `model` cell is to look into the size-512
    text encoder that runs jointly with the image encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Go through the cells that describe the architecture, configuration, and parameters
    to see how CLIP represents data.
  prefs: []
  type: TYPE_NORMAL
- en: We showed that task-agnostic transformer models process image-text pairs as
    text-text pairs. We could apply task-agnostic models to music-text, sound-text,
    music-images, and any type of data pairs.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore DALL-E, another task-agnostic transformer model that can
    process images and text.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DALL-E, as with CLIP, is a task-agnostic model. CLIP processed text-image pairs.
    DALL-E processes the text and image tokens differently. DALL-E’s input is a single
    stream of text and image of 1,280 tokens. 256 tokens are for the text, and 1,024
    tokens are used for the image. DALL-E is a foundation model like CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E was named after *Salvador Dali* and Pixar’s WALL-E. The usage of DALL-E
    is to enter a text prompt and produce an image. However, DALL-E must first learn
    how to generate images with text.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E is a 12-billion-parameter version of GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: This transformer generates images from text descriptions using a dataset of
    text-image pairs.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Architecture of DALL-E
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike CLIP, DALL-E concatenates up to 256 BPE-encoded text tokens with 32×32
    = 1,024 image tokens, as shown in *Figure 15.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a video game  Description automatically generated with medium
    confidence](img/B17948_15_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: DALL-E concatenates text and image input'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.14* shows that this time, our cat image is concatenated with the
    input text.'
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E has an encoder and a decoder stack, which is built with a hybrid architecture
    of infusing convolutional functions in a transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s peek into the code to see how the model works.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E in code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will see how DALL-E reconstructs images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Vision_Transformers.ipynb`. Then go to the `DALL-E` cell of the notebook.
    The notebook first installs OpenAI DALL-E:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The notebook downloads the images and processes an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The program now loads the OpenAI DALL-E encoder and decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'I added the `enc` and `dec` cells so that you can look into the encoder and
    decoder blocks to see how this hybrid model works: the convolutional functionality
    in a transformer model and the concatenation of text and image input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image processed in this section is `mycat.jpg` (creator: Denis Rothman,
    all rights reserved, written permission required to reproduce it). The image is
    in the `Chapter15` directory of this book’s repository. It is downloaded and processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we display the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant chat, intérieur, mammifère, pose  Description générée
    automatiquement](img/B17948_15_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: An image of a cat'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the program processes and displays the *reconstructed* image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The reconstructed image looks extremely similar to the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant intérieur, chat, pose, lit  Description générée automatiquement](img/B17948_15_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: DALL-E reconstructs the image of the cat'
  prefs: []
  type: TYPE_NORMAL
- en: The result is impressive. DALL-E learned how to generate images on its own.
  prefs: []
  type: TYPE_NORMAL
- en: The full DALL-E source code is not available at the time of the book’s writing
    and might never be. An OpenAI API to generate images from text prompts is not
    online yet. But keep your eyes open!
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, we can continue discovering DALL-E on OpenAI at [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have reached the page, scroll down to the examples provided. For example,
    I chose a photo of Alamo Square in San Francisco as a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_15_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: Prompt for Alamo Square, SF'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then I modified “at night” to “in the morning”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_15_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: Modifying the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'DALL-E then generated a multitude of `text2image` images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, sign, different  Description automatically generated](img/B17948_15_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.19: Generating images from text prompts'
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented ViT, CLIP, and DALL-E, three vision transformers. Let’s
    go through some final thoughts before we finish.
  prefs: []
  type: TYPE_NORMAL
- en: An expanding universe of models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'New transformer models, like new smartphones, emerge nearly every week. Some
    of these models are both mind-blowing and challenging for a project manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ERNIE** is a continual pretraining framework that produces impressive results
    for language understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://arxiv.org/abs/1907.12412](https://arxiv.org/abs/1907.12412)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: Hugging Face provides a model. Is it a full-blown model? Is
    it the one Baidu trained to exceed human baselines on the SuperGLUE Leaderboard
    (December 2021): [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)?
    Do we have access to the best one or just a toy model? What is the purpose of
    running AutoML on such small versions of models? Will we gain access to it on
    the Baidu platform or a similar one? How much will it cost?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SWITCH**: A trillion-parameter model optimized with sparse modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: The paper is fantastic. Where is the model? Will we ever have
    access to the real fully trained model? How much will it cost?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Megatron-Turing**: A 500 billion parameter transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blog**: [https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: One of the best models on the market. Will we have access through
    an API? Will it be a full-blown model? How much will it cost?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**XLNET** is pretrained like BERT, but the authors contend it exceeds BERT
    model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: Does XLNET really exceed the performances of Google BERT, the
    version Google uses for their activities? Do we have access to the best versions
    of Google BERT or XLNET models?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The list has become endless and it is growing!
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing them all remains a challenge beyond the issues mentioned previously.
    Only a few transformer models qualify as foundation models. A foundation model
    must be:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully trained for a wide range of tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to perform tasks it was not trained for because of the unique level of
    NLU it has attained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficiently large to guarantee reasonably accurate results, such as OpenAI
    GPT-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many sites offer transformers that prove useful for educational purposes but
    cannot be considered sufficiently trained and large to qualify for benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach is to deepen your understanding of transformer models as much
    as possible. At one point, you will become an expert, and finding your way through
    the jungle of big tech innovations will be as easy as choosing a smartphone!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New transformer models keep appearing on the market. Therefore, it is good practice
    to keep up with cutting-edge research by reading publications and books and testing
    some systems.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to assess which transformer models to choose and how to implement
    them. We cannot spend months exploring every model that appears on the market.
    We cannot change models every month if a project is in production. Industry 4.0
    is moving to seamless API ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: Learning all the models is impossible. However, understanding a new model quickly
    can be achieved by deepening our knowledge of transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: The basic structure of transformer models remains unchanged. The layers of the
    encoder and/or decoder stacks remain identical. The attention head can be parallelized
    to optimize computation speeds.
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer model applies **LSH** buckets and chunking. It also recomputes
    each layer’s input instead of storing the information, thus optimizing memory
    issues. However, a billion-parameter model such as GPT-3 produces acceptable results
    for the same examples.
  prefs: []
  type: TYPE_NORMAL
- en: The DeBERTa model disentangles content and positions, making the training process
    more flexible. The results are impressive. However, billion-parameter models such
    as GPT-3 can equal the outputs of a DeBERTa.
  prefs: []
  type: TYPE_NORMAL
- en: ViT, CLIP, and DALL-E took us into the fascinating world of task-agnostic text-image
    vision transformer models. Combining language and images produces new and productive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: The question remains to see how far ready-to-use AI and automated systems will
    go. We will attempt to visualize the future of transformer-based AI in the next
    chapter on the rise of metahumans.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain encoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain decoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inputs are stored layer by layer in Reformer models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeBERTa transformer models disentangle content and positions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is necessary to test the hundreds of pretrained transformer models before
    choosing one for a project. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latest transformer model is always the best. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is better to have one transformer model per NLP task than one multi-task
    transformer model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer model always needs to be fine-tuned. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI GPT-3 engines can perform a wide range of NLP tasks without fine-tuning.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is always better to implement an AI algorithm on a local server. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hugging Face Reformer: [https://huggingface.co/transformers/model_doc/reformer.html?highlight=reformer](https://huggingface.co/transformers/model_doc/reformer.html?highlight=reformer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face DeBERTa: [https://huggingface.co/transformers/model_doc/deberta.html](https://huggingface.co/transformers/model_doc/deberta.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pengcheng He*, *Xiaodong Liu*, *Jianfeng Gao*, *Weizhu Chen*, 2020, *Decoding-enhanced
    BERT with Disentangled Attention*: [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alexey Dosovitskiy*, *Lucas Beyer*, *Alexander Kolesnikov*, *Dirk Weissenborn*,
    *Xiaohua Zhai*, *Thomas Unterthiner*, *Mostafa Dehghani*, *Matthias Minderer*,
    *Georg Heigold*, *Sylvain Gelly*, *Jakob Uszkoreit*, *Neil Houlsby*, *2020, An
    Image is Worth 16x16 Words:* *Transformers for Image Recognition at Scale*: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI: [https://openai.com/](https://openai.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*William Fedus*, *Barret Zoph*, *Noam Shazeer*, 2021, *Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alec Radford*, *Jong Wook Kim*, *Chris Hallacy*, *Aditya Ramesh*, *Gabriel
    Goh*, *Sandhini Agarwal*, *Girish Sastry*, *Amanda Askell*, *Pamela Mishkin*,
    *Jack Clark*, *Gretchen Krueger*, *Ilya Sutskever*, 2021, *Learning Transferable
    Visual Models From Natural Language Supervision*: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C7LIP: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Aditya Ramesh*, *Mikhail Pavlov*, *Gabriel Goh*, *Scott Gray*, *Chelsea Voss*,
    *Alec Radford*, *Mark Chen*, *Ilya Sutskever*, 2021, *Zero-Shot Text-to-Image
    Generation*: [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DALL-E: [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
