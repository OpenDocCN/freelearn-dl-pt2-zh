- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: From NLP to Task-Agnostic Transformer Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 NLP 到任务不可知的变压器模型
- en: Up to now, we have examined variations of the original Transformer model with
    encoder and decoder layers, and we explored other models with encoder-only or
    decoder-only stacks of layers. Also, the size of the layers and parameters has
    increased. However, the fundamental architecture of the transformer retains its
    original structure with identical layers and the parallelization of the computing
    of the attention heads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经检查了原始变压器模型的变体，具有编码器和解码器层，以及具有仅编码器或仅解码器层的其他模型。此外，层和参数的大小已经增加。然而，变压器的基本结构保留了其原始结构，具有相同的层和注意头的计算并行化。
- en: In this chapter, we will explore innovative transformer models that respect
    the basic structure of the original Transformer but make some significant changes.
    Scores of transformer models will appear, like the many possibilities a box of
    LEGO^© pieces gives. You can assemble those pieces in hundreds of ways! Transformer
    model sublayers and layers are the LEGO^© pieces of advanced AI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索尊重原始变压器基本结构但进行了一些重大变化的创新变压器模型。许多变压器模型会出现，就像一个乐高^© 件盒给出的许多可能性一样。你可以以数百种方式组装这些部件！变压器模型子层和层是先进
    AI 的乐高^© 部件。
- en: We will begin by asking which transformer model to choose among the many offers
    and the ecosystem we will implement them in.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从众多提供中选择哪个变压器模型，并选择我们将实施它们的生态系统开始。
- en: Then we will discover **Locality Sensitivity Hashing** (**LSH**) buckets and
    chunking in Reformer models. We will then learn what disentanglement is in DeBERTa
    models. DeBERTa also introduces an alternative way of managing positions in the
    decoder. DeBERTA’s high-powered transformer model exceeds human baselines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将发现 Reformer 模型中的**局部敏感哈希**（**LSH**）桶和分块技术。然后我们将了解 DeBERTa 模型中解耦关注的含义。DeBERTa
    还介绍了一种在解码器中管理位置的替代方法。DeBERTA 的高功率变压器模型超越了人类基线。
- en: Our last step stops will be to discover powerful computer vision transformers
    such as Vit, CLIP, and DALL-E. We can add CLIP and DALL-E to OpenAI GPT-3 and
    Google BERT (trained by Google) to the very small group of **foundation** **models**.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步将是发现像 Vit、CLIP 和 DALL-E 这样的强大的计算机视觉变压器。我们可以将 CLIP 和 DALL-E 添加到 OpenAI
    GPT-3 和 Google BERT（由 Google 训练）这个非常小的**基础模型**组中。
- en: These powerful foundation models prove that transformers are *task-agnostic*.
    A transformer learns sequences. These sequences include vision, sound, and any
    type of data represented as a sequence.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些强大的基础模型证明了变压器是*任务不可知*的。变压器学习序列。这些序列包括视觉、声音以及任何以序列表示的数据类型。
- en: Images contain sequences of data-like language. We will run ViT, CLIP, and DALL-E
    models to learn. We will take vision models to innovative levels.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像包含类似语言的数据序列。我们将运行 ViT、CLIP 和 DALL-E 模型进行学习。我们将把视觉模型提升到创新水平。
- en: By the end of the chapter, you will see that the world of *task-agnostic* transformers
    has evolved into a universe of imagination and creativity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将看到*任务不可知*变压器的世界已经演变成了一个充满想象力和创造力的宇宙。
- en: 'This chapter covers the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Choosing a transformer model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择变压器模型
- en: The Reformer transformer model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reforme 变压器模型
- en: '**Locality Sensitivity Hashing** (**LSH**)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部敏感哈希**（**LSH**）'
- en: Bucket and chunking techniques
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桶和分块技术
- en: The DeBERTA transformer model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTA 变压器模型
- en: Disentangled attention
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解耦关注
- en: Absolute positions
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对位置
- en: Text-image vision transformers with CLIP
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 CLIP 的文本图像视觉变压器
- en: DALL-E, a creative text-image vision transformer
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E，一个创意文本图像视觉变压器
- en: Our first step will be to see how to choose a model and an ecosystem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是看看如何选择一个模型和一个生态系统。
- en: Choosing a model and an ecosystem
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个模型和一个生态系统
- en: We thought that testing transformer models by downloading them would require
    machine and human resources. Also, you might have thought that if a platform doesn’t
    have an online sandbox by this time, it will be a risk to go further because of
    the work to test a few examples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为通过下载转换器模型进行测试将需要机器和人力资源。而且，你可能会认为如果一个平台到这个时候还没有在线沙盒，那么进一步进行测试就会有风险，因为测试几个示例的工作量。
- en: However, sites such as Hugging Face download pretrained models automatically
    in real time, as we will see in *The Reformer* and *DeBERTa* sections! So, what
    should we do? Thanks to that, we can run Hugging Face models in Google Colab without
    installing anything on the machine ourselves. We can also test Hugging Face models
    online.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像Hugging Face这样的网站会在实时自动下载预训练模型，正如我们将在*The Reformer*和*DeBERTa*部分看到的那样！那么我们该怎么办？由于这个，我们可以在Google
    Colab上运行Hugging Face模型，而不需要在本机上安装任何东西。我们还可以在线测试Hugging Face模型。
- en: 'The idea is to analyze without having anything to “install.” “Nothing to Install”
    in 2022 can mean:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是分析而无需任何“安装”。在2022年，“无需安装”可能意味着：
- en: Running a transformer task online
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线运行一个变压器任务
- en: Running a transformer on a preinstalled Google Colaboratory VM that seamlessly
    downloads a pretrained model for a task, which we can run in a few lines
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预安装的Google Colaboratory虚拟机上运行一个变压器，它可以无缝地下载一个预训练模型以完成任务，我们可以用几行代码来运行。
- en: Running a transformer through an API
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过API运行一个变压器
- en: The definition of “install” has expanded over the past few years. The definition
    of “online” has widened. We can consider using a few lines of code to run an API
    as a meta-online test.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: “安装”的定义在过去几年已经扩展了。 “在线”的定义也已经扩大。我们可以考虑使用几行代码来运行一个API作为元在线测试。
- en: 'We will refer to “without installing,” and “online” in a broad sense in this
    section. *Figure 15.1* shows how we should test models “online”:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将广义上谈到“无需安装”和“在线”。*图15.1*显示了我们应该如何“在线”测试模型：
- en: '![Diagram  Description automatically generated with low confidence](img/B17948_15_01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用低置信度自动生成的图表说明](img/B17948_15_01.png)'
- en: 'Figure 15.1: Testing transformer models online'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：在线测试变压器模型
- en: 'Testing in this decade has become flexible and productive, as the following
    shows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如下面展示的，这十年的测试变得灵活和高效：
- en: Hugging Face hosts API models such as DeBERTa and some other models. In addition,
    Hugging Face offers an AutoML service to train and deploy transformer models in
    their ecosystem.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face托管诸如DeBERTa和其他一些模型等API模型。此外，Hugging Face还提供AutoML服务，用于在其生态系统中训练和部署变压器模型。
- en: OpenAI’s GPT-3 engine runs on the online playground and provides an API. OpenAI
    offers models that cover many NLP tasks. The models require no training. GPT-3’s
    billion-parameter zero-shot engine is impressive. It shows that transformer models
    with many parameters produce better results overall. Microsoft Azure, Google Cloud
    AI, AllenNLP, and other platforms offer interesting services.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3引擎在在线游乐场上运行并提供API。OpenAI提供涵盖许多NLP任务的模型。这些模型无需训练。 GPT-3的十亿参数零射引擎令人印象深刻。它显示出参数多的变压器模型整体产生更好的结果。Microsoft
    Azure、Google Cloud AI、AllenNLP和其他平台提供有趣的服务。
- en: 'An online model analysis can be done by reading a paper if it is worthwhile.
    A good example is Google’s publication by *Fedus* et al., (2021), on *Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. Google
    increased the size of the T5-based models we studied in *Chapter 8*, *Applying
    Transformers to Legal and Financial Documents for AI Text Summarization*. This
    paper confirms the strategy of large online models such as GTP-3.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果值得的话，可以通过阅读一篇论文来进行在线模型分析。Google的*Fedus*等人在2021年的一篇关于*Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*的论文是一个很好的例子。Google增加了我们在*第8章*中研究的基于T5的模型的规模，即*应用于法律和金融文件的AI文本摘要*。这篇论文证实了像GTP-3这样的大型在线模型的策略。'
- en: However, in the end, *you are the one taking the risk* of choosing one solution
    over another. The time you spend on exploring platforms and models will help you
    optimize the implementation of your project once you have made your choice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最终，*选择一种解决方案的风险还是在你自己*。在选择一种解决方案后，你花在探索平台和模型上的时间将有助于你优化项目的实施。
- en: You can host your choice in three different ways, as shown in *Figure 15.2:*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过三种不同的方式托管你的选择，如*图15.2*所示：
- en: On a local machine using an API. OpenAI, Google Cloud AI, Microsoft Azure AI,
    Hugging Face, and others provide good APIs. An application can be on a local machine
    and not on a cloud platform but can go through a cloud service with an API.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用API在本地机器上。OpenAI、Google Cloud AI、Microsoft Azure AI、Hugging Face和其他提供了良好的API。一个应用程序可以在本地机器上而不是在云平台上，但可以通过API经过云服务。
- en: On a cloud platform such as **Amazon Web Services** (**AWS**) or Google Cloud.
    You can train, fine-tune, test, and run the models on these platforms. In this
    case, there is no application on a local machine. Everything is on the cloud.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在像**亚马逊网络服务**(**AWS**)或 Google Cloud 这样的云平台上。您可以在这些平台上训练、微调、测试和运行模型。在这种情况下，没有应用在本地机器上。一切都在云上。
- en: From anywhere using an API! On a local machine, a data center VM, or from anywhere.
    This means that the API would be integrated in a physical system such as a windmill,
    an airplane, a rocket, or an autonomous vehicle. The system could thus permanently
    connect with another system through an API.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从任何地方使用 API！在本地机器上，数据中心虚拟机上，或从任何地方。这意味着 API 将集成在一个物理系统中，比如风车、飞机、火箭或自动驾驶车辆。因此，系统可以通过
    API 与另一个系统永久连接。
- en: '![Diagram  Description automatically generated](img/B17948_15_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17948_15_02.png)'
- en: 'Figure 15.2: Implementing options for your models'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15.2: 为您的模型实现选项'
- en: In the end, it is up to you to make the decision. Take your time. Test, analyze,
    compute the costs, and work as a team to listen to different perspectives. *The
    more you understand how transformers work, the better the choices you make will
    be*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，决定权在你手中。花点时间。测试、分析、计算成本，并团队合作听取不同的观点。*你对转换器的工作方式了解得越多，你做出的选择就会越好*。
- en: Let’s now explore the Reformer, a variation of the original Transformer model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探索改革者，这是原始转换器模型的一个变种。
- en: The Reformer
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改革者
- en: '*Kitaev* et al. (2020) designed the Reformer to solve the attention and memory
    issues, adding functionality to the original Transformer model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kitaev*等人（2020年）设计改革者来解决注意力和内存问题，为原始转换器模型添加功能。'
- en: The Reformer first solves the attention issue with **Locality Sensitivity Hashing**
    (**LSH**) buckets and chunking.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 改革者首先用**局部敏感哈希**(**LSH**)桶和分块来解决注意力问题。
- en: LSH searches for nearest neighbors in datasets. The hash function determines
    that if datapoint *q* is close to *p*, then *hash*(*q*) == *hash*(*p*). In this
    case, the data points are the keys of the transformer model’s heads.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LSH 在数据集中搜索最近的邻居。哈希函数确定如果数据点*q*接近*p*，那么*hash*(*q*) == *hash*(*p*)。在这种情况下，数据点是变压器模型头的键。
- en: The LSH function converts the keys into LSH *buckets* (*B1* to *B4* in *Figure
    15.3*) in a process called LSH bucketing, just like how we would take objects
    similar to each other and put them in the same sorted buckets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: LSH 函数将键转换为 LSH *桶*（*图 15.3* 中的*B1*到*B4*），这个过程称为 LSH 分桶，就像我们将彼此相似的对象放在同一个排序的桶中一样。
- en: 'The sorted buckets are split into *chunks* (*C1* to *C4* in *Figure 15.3*)
    to parallelize. Finally, attention will only be applied within the same bucket
    in its chunk and the previous chunk:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 排序的桶被分成*块*（*图 15.3* 中的*C1*到*C4*）进行并行化。最后，注意力仅在其块内和前一个块中的相同桶中应用：
- en: '![](img/B17948_15_03.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_03.png)'
- en: 'Figure 15.3: LSH attention heads'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15.3: LSH 注意力头'
- en: LSH bucketing and chunking considerably reduce the complexity from *O*(*L*²),
    attending to all the word pairs, to *O*(*L*log*L*), only attending to the content
    of each bucket.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LSH 分桶和分块将复杂度从*O*(*L*²)，即关注所有词对，减少到*O*(*L*log*L*)，即仅关注每个桶的内容。
- en: The Reformer also solves the memory issue of recomputing each layer’s input
    instead of storing the information for multi-layer models. The recomputing is
    achieved on-demand instead of consuming terabytes of memory for some large multi-layer
    models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 改革者还解决了重新计算每一层输入而不是存储多层模型信息的内存问题。重新计算是按需实现的，而不是为一些大型多层模型消耗的千字节内存。
- en: We will now use a Reformer model trained on the English translation of *Crime
    and Punishment* by *Fyodor Dostoevsky*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用在*Fyodor Dostoevsky*的*《罪与罚》*英文翻译上训练的一个改革者模型。
- en: Running an example
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行一个例子
- en: 'Let’s run it directly online with the hosted inference API. The input sentence
    is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接在托管的推理 API 上在线运行它。输入句子是：
- en: '`The student``was impoverished and did not know what to do.`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`这个学生贫困潦倒，不知所措。`'
- en: 'The link to the online interface contains the input:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线接口的链接中包含输入：
- en: '[https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do](https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do](https://huggingface.co/google/reformer-crime-and-punishment?text=The+student+was+impoverished+and+did+not+know+what+to+do)'
- en: 'The hosted inference API appears with the input sentence. Click on `compute`
    to obtain an inference, and the result will appear right under the input:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 托管推理API与输入句子一起显示。点击`计算`以获得推理，结果将直接显示在输入下方：
- en: '![](img/B17948_15_04.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_04.png)'
- en: 'Figure 15.4: The Reformer’s hosted inference API'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：Reformer的托管推理API
- en: You might get a different response since the algorithm is stochastic. The Reformer
    was reasonably trained, though not with a supercomputer with billions of bits
    of information like OpenAI’s GPT-3\. The result of the Reformer is not very impressive.
    It would take more training and fine-tuning to obtain better results.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法是随机的，您可能会得到不同的响应。Reformer经过了合理的训练，尽管没有像OpenAI的GPT-3那样拥有数十亿位信息的超级计算机。Reformer的结果并不令人印象深刻。它需要更多的训练和微调才能获得更好的结果。
- en: 'OpenAI’s GPT-3 engine produces the following result for text completion:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3引擎对文本完成产生了以下结果：
- en: 'The student was impoverished and did not know what to do. He did not have any
    one to turn to and could not find a place to stay. He took out a pad from his
    bag and started writing. He wrote:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 学生生活贫困，不知所措。他找不到可以求助的人，也找不到住的地方。他从包里拿出一个记事本开始写。他写道：
- en: '`"My name is XXXXXXXXXX. I am a student at XXXXXXXXXX. I have no family, no
    friends, no money."`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: “我的名字叫XXXXXXXXXX。我是XXXXXXX的学生。我没有家人，没有朋友，没有钱。”
- en: 'The result is more convincing. You can access OpenAI’s playground after having
    signed up: [https://openai.com/](https://openai.com/)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果更具说服力。注册后，您可以访问OpenAI的游乐场：[https://openai.com/](https://openai.com/)
- en: 'Note: OpenAI GPT-3, as with other transformer models and most deep learning
    models, is based on stochastic algorithms. The results might vary from one to
    another.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：OpenAI GPT-3，与其他变压器模型和大多数深度学习模型一样，基于随机算法。结果可能会有所不同。
- en: This shows that a highly well-trained transformer model containing billions
    of parameters can outperform an innovative transformer model architecture.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，包含数十亿参数的高度训练有素的变压器模型可以胜过一种创新的变压器模型架构。
- en: Will supercomputer-driven cloud AI platforms progressively outperform local
    attempts or even less powerful cloud platforms? You need to address these issues
    through prototypes before investing in one solution over another.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算机驱动的云AI平台是否会逐渐胜过本地尝试甚至功能较弱的云平台？在投资于一种解决方案之前，您需要通过原型解决这些问题。
- en: '**Note**: The stochastic nature of transformer models may produce different
    results when running them. Also, online platforms continually change their interfaces.
    We need to accept that and adapt.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：变压器模型的随机性可能导致在运行这些模型时产生不同的结果。此外，在线平台不断变化其界面。我们需要接受并适应这一点。'
- en: DeBERTa introduces another innovative architecture, which we will now explore.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa引入了另一种创新的架构，我们现在将对其进行探讨。
- en: DeBERTa
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeBERTa
- en: 'Another new approach to transformers can be found through *disentanglement*.
    *Disentanglement* in AI allows you to separate the representation features to
    make the training process more flexible. *Pengcheng He*, *Xiaodong Liu*, *Jianfeng
    Gao*, and *Weizhu Chen* designed DeBERTa, a disentangled version of a transformer,
    and described the model in an interesting article: *DeBERTa: Decoding-enhanced
    BERT with Disentangled Attention*: [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '通过*分解*还可以找到变压器的另一种新方法。在AI中，*分解*允许您分离表示特征，使训练过程更加灵活。*Pengcheng He*，*Xiaodong
    Liu*，*Jianfeng Gao*和*Weizhu Chen*设计了DeBERTa，这是变压器的分解版本，并在一篇有趣的文章中描述了该模型：*DeBERTa:
    Decoding-enhanced BERT with Disentangled Attention*: [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)'
- en: 'The two main ideas implemented in DeBERTa are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa实现的两个主要思想是：
- en: Disentangle the content and position in the transformer model to train the two
    vectors separately
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在变压器模型中将内容和位置分离开来，分别训练这两个向量
- en: Use an absolute position in the decoder to predict masked tokens in the pretraining
    process
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预训练过程中使用解码器中的绝对位置来预测屏蔽的标记
- en: 'The authors provide the code on GitHub: [https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在GitHub上提供了代码：[https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)
- en: 'DeBERTa exceeds the human baseline on the SuperGLUE leaderboard:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa超越了SuperGLUE排行榜上的人类基准：
- en: '![](img/B17948_15_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_05.png)'
- en: 'Figure 15.5: DeBERTa on the SuperGLUE leaderboard'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：DeBERTa在SuperGLUE排行榜上的表现
- en: Remove any space before Let’s run an example on Hugging Face’s cloud platform.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在“让我们在Hugging Face的云平台上运行一个示例”之前，去除任何空格。
- en: Running an example
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行一个示例
- en: 'To run an example on Hugging Face’s cloud platform, click on the following
    link:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Hugging Face的云平台上运行示例，请单击以下链接：
- en: '[https://huggingface.co/cross-encoder/nli-deberta-base](https://huggingface.co/cross-encoder/nli-deberta-base)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/cross-encoder/nli-deberta-base](https://huggingface.co/cross-encoder/nli-deberta-base)'
- en: 'The hosted inference API will appear with an example and output of possible
    class names:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 托管推理API将显示一个示例和可能的类名称输出：
- en: '![](img/B17948_15_06.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_06.png)'
- en: 'Figure 15.6: DeBERTa’s hosted inference API'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6：DeBERTa的托管推理API
- en: The possible class names are `mobile`, `website`, `billing`, and `account access`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的类名称是`mobile`，`website`，`billing`和`account access`。
- en: The result is interesting. Let’s compare it to a GPT-3 keyword task. First,
    sign up on [https://openai.com/](https://openai.com/)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很有趣。让我们将其与GPT-3关键词任务进行比较。首先，在[https://openai.com/](https://openai.com/)上注册一下
- en: 'Enter `Text` as the input and `Keywords` to ask the engine to find keywords:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`Text`作为输入，并输入`Keywords`以要求引擎找到关键词：
- en: '**Text**: `Last week I upgraded my iOS version and ever since then my phone
    has been overheating whenever I use your app.`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本**：`上周我升级了我的iOS版本，自那时起，每当我使用你们的应用时，我的手机就会过热。`'
- en: '**Keywords**: `app, overheating, phone`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词**: `app, overheating, phone`'
- en: The possible keywords are `app`, `overheating`, and `phone`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的关键词是`app`，`overheating`和`phone`。
- en: We have gone through the DeBERTa and GPT-3 transformers. We will now extend
    transformers to vision models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了DeBERTa和GPT-3变压器。现在我们将将变压器扩展到视觉模型。
- en: From Task-Agnostic Models to Vision Transformers
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从任务无关模型到视觉变压器
- en: 'Foundation models, as we saw in *Chapter 1*, *What Are Transformers?*, have
    two distinct and unique properties:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型，正如我们在第1章“什么是变压器？”中所看到的，具有两个独特而独特的属性：
- en: '**Emergence** – Transformer models that qualify as foundation models can perform
    tasks they were not trained for. They are large models trained on supercomputers.
    They are not trained to learn specific tasks like many other models. Foundation
    models learn how to understand sequences.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**涌现** - 符合基础模型标准的变压器模型可以执行未经训练的任务。它们是在超级计算机上训练的大型模型。它们没有被训练来学习像许多其他模型那样的特定任务。基础模型学会了如何理解序列。'
- en: '**Homogenization** – The same model can be used across many domains with the
    same fundamental architecture. Foundation models can learn new skills through
    data faster and better than any other model.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同质化** - 相同的模型可以在许多领域中使用相同的基本架构。基础模型可以通过数据比其他任何模型更快更好地学习新技能。'
- en: GPT-3 and Google BERT (only the BERT models trained by Google) are task-agnostic
    foundation models. These task-agnostic models lead directly to ViT, CLIP, and
    DALL-E models. Transformers have uncanny sequence analysis abilities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3和Google BERT（仅Google训练的BERT模型）是任务不可知的基础模型。这些任务不可知的模型直接引导到ViT、CLIP和DALL-E模型。变压器有着奇特的序列分析能力。
- en: 'The level of abstraction of transformer models leads to multi-modal neurons:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型的抽象程度导致多模态神经元：
- en: '**Multi-modal neurons** can processimages that can be tokenized as pixels or
    image patches. Then they can be processed as *words* in vision transformers. Once
    an image has been encoded, transformer models see the tokens as any *word* token,
    as shown in *Figure 15.7*:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态神经元**可以处理可被标记为像素或图像片段的图像。然后它们可以在视觉变压器中被处理为*单词*。一旦图像被编码，变压器模型将标记视为任何*单词*标记，如图15.7所示：'
- en: '![](img/B17948_15_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_07.png)'
- en: 'Figure 15.7: Images can be encoded into word-like tokens'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7：图像可以编码为类似单词的标记
- en: 'In this section, we will go through:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍以下内容：
- en: ViT, vision transformers that process images as patches of *words*
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT，将图像处理为*单词*片段的视觉变压器
- en: CLIP, vision transformers that encode text and image
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP，将文本和图像编码的视觉变压器
- en: DALL-E, vision transformers that construct images with text
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E，用文本构造图像的视觉变压器
- en: Let’s begin by exploring ViT, a vision transformer that processes images as
    patches of *words*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始探索ViT，一个将图像处理为*单词*片段的视觉变压器。
- en: ViT – Vision Transformers
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViT - 视觉变压器
- en: '*Dosovitskiy* et al. (2021) summed up the essence of the vision transformer
    architecture they designed in the title of their paper: *An Image is Worth 16x16
    Words: Transformers for Image Recognition at Scale*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dosovitskiy*等人（2021）在文章标题中总结了他们设计的视觉变压器架构的精华：*一张图片价值16x16个单词：大规模图像识别的变压器*。'
- en: An image can be converted into patches of 16x16 words.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可以转换为16x16单词的片段。
- en: Let’s first see the architecture of a ViT before looking into the code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看代码之前，让我们先看一下ViT的架构。
- en: The Basic Architecture of ViT
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ViT的基本架构
- en: 'A vision transformer can process an image as patches of *words*. In this section,
    we will go through the process in three steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器可以将图像作为“单词”补丁来处理。在本节中，我们将按照三个步骤进行处理：
- en: Splitting the image into patches
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像拆分成补丁
- en: A linear projection of the patches
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 补丁的线性投影
- en: The hybrid input embedding sublayer
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合输入嵌入子层
- en: The first step is to SPLIT the image into equal-sized patches.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将图像拆分成相同尺寸的补丁。
- en: 'Step 1: Splitting the image into patches'
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：将图像拆分成补丁
- en: 'The image is split into *n* patches, as shown in *Figure 15.8*. There is no
    rule saying how many patches as long as all the patches have the same dimensions,
    such as 16x16:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被分割成*n*个补丁，如图15.8所示。只要所有补丁的尺寸相同（例如16x16），就没有规定补丁数量的规定：
- en: '![Chart, treemap chart  Description automatically generated](img/B17948_15_08.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图表，树状图描述自动生成](img/B17948_15_08.png)'
- en: 'Figure 15.8: Splitting an image into patches'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：将图像拆分成补丁
- en: The patches of equal dimensions now represent the *words* of our sequence. The
    problem of what to do with these patches remains. We will see that each type of
    vision transformer has its own method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 相等尺寸的补丁现在代表我们序列的“单词”。如何处理这些补丁仍然是个问题。我们将看到每种视觉变换器都有自己的方法。
- en: 'Image citation: The image of cats used in this section and the subsequent sections
    was taken by *DocChewbacca*: [https://www.flickr.com/photos/st3f4n/](https://www.flickr.com/photos/st3f4n/),
    in 2006\. It is under a Flickr free license, [https://creativecommons.org/licenses/by-sa/2.0/](https://creativecommons.org/licenses/by-sa/2.0/).
    For more details, see *DocChewbacca*’s image on Flickr: [https://www.flickr.com/photos/st3f4n/210383891](https://www.flickr.com/photos/st3f4n/210383891)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图像引用：本节和随后的几节中使用的猫的图像由*DocChewbacca*拍摄：[https://www.flickr.com/photos/st3f4n/](https://www.flickr.com/photos/st3f4n/)，于2006年。它属于Flickr免费许可证，[https://creativecommons.org/licenses/by-sa/2.0/](https://creativecommons.org/licenses/by-sa/2.0/)。有关更多详情，请查看Flickr上*DocChewbacca*的图片：[https://www.flickr.com/photos/st3f4n/210383891](https://www.flickr.com/photos/st3f4n/210383891)
- en: In this case, for the ViT, *Step 2* will be to make a linear projection of flattened
    images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对于ViT，*第2步*是对展平的图像进行线性投影。
- en: 'Step 2: Linear projection of flattened images'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：对展平的图像进行线性投影
- en: '*Step 1* converted an image to equal-sized patches. The motivation of the patches
    is to avoid processing the image pixel by pixel. However, the problem remains
    to find a way to process the patches.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1步*将图像转换为尺寸相等的补丁。补丁的动机是避免逐像素处理图像，但问题在于找到处理补丁的方法。'
- en: 'The team at Google Research decided to design a linear projection of flattened
    images with the patches obtained by splitting the image, as shown in *Figure 15.9*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Google Research团队决定设计一个由图像的展平图像的线性投影和通过分割图像获得的补丁组成的线性投影，如图15.9所示：
- en: '![](img/B17948_15_09.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_09.png)'
- en: 'Figure 15.9: Linear projection of flattened images'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：图像的线性投影
- en: The idea is to obtain a sequence of work-like patches. The remaining problem
    is to embed the sequence of flattened images.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是获得一个类似单词的补丁序列。剩下的问题是嵌入展平图像序列。
- en: 'Step 3: The hybrid input embedding sublayer'
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：混合输入嵌入子层
- en: Word-like image sequences can fit into a transformer. The problem is that they
    still are images!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 类似单词的图像序列可以适应变换器。问题在于它们仍然是图像！
- en: 'Google Research decided that a hybrid input model would do the job, as shown
    in *Figure 15.10*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Google Research决定使用混合输入模型，如图15.10所示：
- en: Add a convolutional network to embed the linear projection of the patches
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个卷积网络以嵌入补丁的线性投影
- en: Add positional encoding to retain the structure of the original image
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加位置编码以保留原始图像的结构
- en: Then process the embedded input with a standard original BERT-like encoder
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用标准的原始BERT样式编码器处理嵌入的输入
- en: '![](img/B17948_15_10.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_10.png)'
- en: 'Figure 15.10: A hybrid input sublayer and a standard encoder'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10：混合输入子层和标准编码器
- en: Google Research found a clever way to convert an NLP transformer model into
    a vision transformer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Google Research找到了一种巧妙的方法将NLP变换器模型转换成视觉变换器。
- en: Now, let’s implement a Hugging Face example of a vision transformer in code.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在代码中实现一个Hugging Face视觉变换器示例。
- en: Vision transformers in code
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的视觉变换器
- en: In this section, we will focus on the main areas of code that relate to the
    specific architecture of vision transformers.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注与视觉变换器特定体系结构相关的主要代码区域。
- en: Open `Vision_Transformers.ipynb`, which is in the GitHub repository for this
    chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Vision_Transformers.ipynb`，它位于本章的GitHub存储库中。
- en: 'Google Colab VM’s contain many pre-installed packages such as `torch` and `torchvision`.
    You can display them by uncommenting the command in the first cell of the notebook:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab VM中包含许多预安装的软件包，如`torch`和`torchvision`。可以通过在笔记本的第一个单元格中取消注释该命令来显示它们：
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then go to the **Vision Transformer** (**ViT**) cell of the notebook. The notebook
    first installs Hugging Face transformers and imports the necessary modules:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后前往笔记本的**Vision Transformer**（**ViT**）单元。该笔记本先安装了 Hugging Face transformers
    并导入了必要的模块：
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Note**: At the time of writing this book, Hugging Face warns us that the
    code can be unstable due to constant evolutions. This should not stop us from
    exploring ViT models. Testing new territory is what the cutting edge is all about!'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：在撰写本书时，Hugging Face警告我们，由于不断的演变，代码可能不稳定。但这不应阻止我们探索ViT模型。探索新领域正是前沿的所在！'
- en: 'We then download an image from the COCO dataset. You can find a comprehensive
    corpus of datasets on their website if you wish to experiment further: [https://cocodataset.org/](https://cocodataset.org/)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从 COCO 数据集下载了一幅图像。如果你想进一步进行实验，可以在他们的网站上找到大量数据集：[https://cocodataset.org/](https://cocodataset.org/)
- en: Let’s download from the VAL2017 dataset. Follow the COCO dataset website instructions
    to obtain these images through programs or download the datasets locally.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 VAL2017 数据集中下载。按照 COCO 数据集网站的说明通过程序获取这些图像或在本地下载数据集。
- en: The VAL2017 contains 5,000 images we can choose from to test this ViT model.
    You can run any of the 5,000 images.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: VAL2017 包含了 5,000 张我们可以选择的图像，以测试这个 ViT 模型。你可以运行其中任何一张图像。
- en: 'Let’s test the notebook with the image of the cats. We first retrieve the image
    of the cats through their URL:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用猫的图片测试笔记本。我们首先通过它们的 URL 检索猫的图片：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We next download Google’s feature extractor and classification model:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要下载 Google 的特征提取器和分类模型：
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The model was trained on 224 x 244 resolution images but was presented with
    16 x 16 patches for feature extraction and classification. The notebook runs the
    model and makes a prediction:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在 224 x 244 分辨率图像上进行训练，但在特征提取和分类时采用了 16 x 16 的块。笔记本运行模型并进行预测：
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Explore the code that follows the prediction, which gives us information at
    a low level, among which are:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 探索在预测后的代码，它提供了一些低层次的信息，其中包括：
- en: '`model.config.id2label`, which will list the labels of the classes. The 1000
    label classes explain why we obtain a class and not a detailed text description:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.config.id2label`，会列出类的标签。这 1000 个标签类解释了为什么我们得到一个类而不是详细的文本描述：'
- en: '[PRE6]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`model`, which will display the architecture of the model that begins with
    the hybrid usage of a convolutional input sublayer:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型`，该模型将显示从卷积输入子层开始的模型架构：'
- en: '[PRE7]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After the convolutional input embedding sublayer, the model is a BERT-like encoder.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积输入嵌入子层之后，模型是一个类似 BERT 的编码器。
- en: Take your time to explore this innovative move from NLP transformers to transformers
    for images, leading to transformers for everything quite rapidly.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请花些时间探索这个创新的从 NLP 变换器到图像变换器的转变，迅速导向为一切都使用变换器。
- en: Now, let’s go through CLIP, another computer vision model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解另一个计算机视觉模型CLIP。
- en: CLIP
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP
- en: '**Contrastive Language-Image Pre-Training** (**CLIP**) follows the philosophy
    of transformers. It plugs sequences of data in its transformer-type layers. Instead
    of sending text pairs, this time, the model sends text-image pairs. Once the data
    is tokenized, encoded, and embedded, CLIP, a task-agnostic model, learns text-image
    pairs as with any other sequence of data.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**对比语言-图像预训练**（**CLIP**）遵循了变换器的理念。它在其变换器类型的层中插入数据序列。这次，模型发送的是文本-图像对，而不是文本对。一旦数据被分词、编码和嵌入，CLIP，一个无关任务的模型，就像处理任何其他数据序列一样学习文本-图像对。'
- en: The method is contrastive because it looks for the contrasts in the features
    of the image. It is the method we use in some magazine games in which we have
    to find the differences, the contrasts, between two images.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是对比的，因为它寻找图像特征中的对比。这是我们在一些杂志游戏中使用的方法，在这些游戏中，我们必须找到两幅图像之间的差异，对比。
- en: Let’s first see the architecture of CLIP before looking into the code.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看一下 CLIP 的架构，然后再看代码。
- en: The Basic Architecture of CLIP
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP 的基本架构
- en: 'Contrastive: the images are trained to learn how they fit together through
    their differences and similarities. The image and captions find their way toward
    each other through (joint text, image) pretraining. After pretraining, CLIP learns
    new tasks.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对比：图像通过它们的差异和相似之处学习如何相互配合。通过（联合文本，图像）预训练，图像和字幕找到彼此的路径。预训练后，CLIP学习新任务。
- en: CLIPs are transferable because they can learn new visual concepts, like GPT
    models, such as action recognition in video sequences. The captions lead to endless
    applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是可转移的，因为它们可以学习新的视觉概念，就像GPT模型一样，比如视频序列中的动作识别。字幕带来了无尽的应用。
- en: 'ViT splits images into word-like patches. CLIP jointly trains *text and image*
    encoders for (caption, image) pairs to maximize cosine similarity, as shown in
    *Figure 15.11*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ViT将图像分割成类似单词的补丁。CLIP联合训练*文本和图像*编码器以最大化余弦相似度，如*图15.11*所示：
- en: '![Diagram  Description automatically generated](img/B17948_15_11.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17948_15_11.png)'
- en: 'Figure 15.11: Jointly training text and images'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11：联合训练文本和图像
- en: '*Figure 15.11* shows how the transformer will run a standard transformer encoder
    for the text input. It will run a ResNet 50-layer CNN for the images in a transformer
    structure. ResNet 50 was modified to run an average pooling layer in an attention
    pooling mechanism with a multi-head QKV attention head.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.11*显示了变压器将为文本输入运行标准变压器编码器。它将在变压器结构中为图像运行一个ResNet 50层CNN。 ResNet 50被修改为在具有多头QKV注意头的注意力池化机制中运行平均池化层。'
- en: Let’s see how CLIP learns text-image sequences to make predictions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看CLIP是如何学习文本-图像序列以进行预测的。
- en: CLIP in code
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的CLIP
- en: Open `Vision_Transformers.ipynb`, which is in the repository for this chapter
    on GitHub. Then go to the `CLIP` cell of the notebook.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 打开GitHub上本章的存储库中的`Vision_Transformers.ipynb`。然后转到笔记本的`CLIP`单元格。
- en: 'The program begins by installing PyTorch and CLIP:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序开始安装PyTorch和CLIP：
- en: '[PRE8]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The program also imports the modules and CIFAR-100 to access the images:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序还导入模块和CIFAR-100来访问图像：
- en: '[PRE9]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are 10,000 images available with an index between 0 and 9,999\. The next
    step is to select an image we want to run a prediction on:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有10,000张图像可用，索引介于0和9,999之间。下一步是选择我们要进行预测的图像：
- en: '![](img/B17948_15_12.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_15_12.png)'
- en: 'Figure 15.12: Selecting an image index'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12：选择一个图像索引
- en: 'The program then loads the model on the device that is available (GPU or CPU):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 程序然后将模型加载到可用的设备上（GPU或CPU）：
- en: '[PRE10]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The images are downloaded:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已下载：
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The input is prepared:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输入已准备好：
- en: '[PRE12]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s visualize the input we selected before running the prediction:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行预测之前，让我们可视化所选的输入：
- en: '[PRE13]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output shows that `index 15` is a lion:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`索引15`是一只狮子：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_15_13.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![带有中等置信度自动生成的图形用户界面描述](img/B17948_15_13.png)'
- en: 'Figure 15.13: Image of Index 15'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13：索引15的图像
- en: 'The images in this section are from *Learning Multiple Layers of Features from
    Tiny Images*, *Alex Krizhevsky*, 2009: [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
    They are part of the `CIFAR-10` and `CIFAR-100` datasets (`toronto.edu`): [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的图像来自于*从小图像中学习多层特征*，*Alex Krizhevsky*，2009年：[https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)。它们是`CIFAR-10`和`CIFAR-100`数据集的一部分（`toronto.edu`）：[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
- en: We know this is a lion because we are humans. A transformer initially designed
    for NLP has to learn what an image is. We will now see how well it can recognize
    images.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这是一只狮子，因为我们是人类。一个最初设计用于自然语言处理的变压器必须学习图像是什么。现在我们将看到它能多好地识别图像。
- en: 'The program shows that it is running a joint transformer model by separating
    the image input from the text input when calculating the features:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算特征时，程序表明正在运行一个联合变压器模型，将图像输入与文本输入分开：
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now CLIP makes a prediction and displays the top five predictions:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在CLIP进行预测并显示前五个预测：
- en: '[PRE15]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can modify `topk(5)` if you want to obtain more or fewer predictions. The
    top five predictions are displayed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想获得更多或更少的预测结果，可以修改`topk(5)`。显示前五个预测：
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: CLIP found the lion, which shows the flexibility of transformer architectures.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP找到了狮子，这显示了变压器架构的灵活性。
- en: 'The next cell displays the classes:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格显示类别：
- en: '[PRE17]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can go through the classes to see that with only one label per class, which
    is restrictive, CLIP did a good job:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The notebook contains several other cells describing the architecture and configuration
    of CLIP that you can explore.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model` cell is particularly interesting because you can see the visual
    encoder that begins with a convolutional embedding like for the ViT model and
    then continues as a “standard” size-768 transformer with multi-head attention:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Another interesting aspect of the `model` cell is to look into the size-512
    text encoder that runs jointly with the image encoder:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Go through the cells that describe the architecture, configuration, and parameters
    to see how CLIP represents data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: We showed that task-agnostic transformer models process image-text pairs as
    text-text pairs. We could apply task-agnostic models to music-text, sound-text,
    music-images, and any type of data pairs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore DALL-E, another task-agnostic transformer model that can
    process images and text.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DALL-E, as with CLIP, is a task-agnostic model. CLIP processed text-image pairs.
    DALL-E processes the text and image tokens differently. DALL-E’s input is a single
    stream of text and image of 1,280 tokens. 256 tokens are for the text, and 1,024
    tokens are used for the image. DALL-E is a foundation model like CLIP.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E was named after *Salvador Dali* and Pixar’s WALL-E. The usage of DALL-E
    is to enter a text prompt and produce an image. However, DALL-E must first learn
    how to generate images with text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E is a 12-billion-parameter version of GPT-3.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: This transformer generates images from text descriptions using a dataset of
    text-image pairs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Architecture of DALL-E
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike CLIP, DALL-E concatenates up to 256 BPE-encoded text tokens with 32×32
    = 1,024 image tokens, as shown in *Figure 15.14*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a video game  Description automatically generated with medium
    confidence](img/B17948_15_14.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: DALL-E concatenates text and image input'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.14* shows that this time, our cat image is concatenated with the
    input text.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E has an encoder and a decoder stack, which is built with a hybrid architecture
    of infusing convolutional functions in a transformer model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Let’s peek into the code to see how the model works.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E in code
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will see how DALL-E reconstructs images.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Vision_Transformers.ipynb`. Then go to the `DALL-E` cell of the notebook.
    The notebook first installs OpenAI DALL-E:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The notebook downloads the images and processes an image:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The program now loads the OpenAI DALL-E encoder and decoder:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I added the `enc` and `dec` cells so that you can look into the encoder and
    decoder blocks to see how this hybrid model works: the convolutional functionality
    in a transformer model and the concatenation of text and image input.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'The image processed in this section is `mycat.jpg` (creator: Denis Rothman,
    all rights reserved, written permission required to reproduce it). The image is
    in the `Chapter15` directory of this book’s repository. It is downloaded and processed:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we display the original image:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output displays the image:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant chat, intérieur, mammifère, pose  Description générée
    automatiquement](img/B17948_15_15.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: An image of a cat'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the program processes and displays the *reconstructed* image:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The reconstructed image looks extremely similar to the original:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant intérieur, chat, pose, lit  Description générée automatiquement](img/B17948_15_16.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: DALL-E reconstructs the image of the cat'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The result is impressive. DALL-E learned how to generate images on its own.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The full DALL-E source code is not available at the time of the book’s writing
    and might never be. An OpenAI API to generate images from text prompts is not
    online yet. But keep your eyes open!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, we can continue discovering DALL-E on OpenAI at [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have reached the page, scroll down to the examples provided. For example,
    I chose a photo of Alamo Square in San Francisco as a prompt:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_15_17.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: Prompt for Alamo Square, SF'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Then I modified “at night” to “in the morning”:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_15_18.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: Modifying the prompt'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'DALL-E then generated a multitude of `text2image` images:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, sign, different  Description automatically generated](img/B17948_15_19.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.19: Generating images from text prompts'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented ViT, CLIP, and DALL-E, three vision transformers. Let’s
    go through some final thoughts before we finish.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: An expanding universe of models
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'New transformer models, like new smartphones, emerge nearly every week. Some
    of these models are both mind-blowing and challenging for a project manager:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**ERNIE** is a continual pretraining framework that produces impressive results
    for language understanding.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://arxiv.org/abs/1907.12412](https://arxiv.org/abs/1907.12412)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: Hugging Face provides a model. Is it a full-blown model? Is
    it the one Baidu trained to exceed human baselines on the SuperGLUE Leaderboard
    (December 2021): [https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)?
    Do we have access to the best one or just a toy model? What is the purpose of
    running AutoML on such small versions of models? Will we gain access to it on
    the Baidu platform or a similar one? How much will it cost?'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SWITCH**: A trillion-parameter model optimized with sparse modeling.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: The paper is fantastic. Where is the model? Will we ever have
    access to the real fully trained model? How much will it cost?'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Megatron-Turing**: A 500 billion parameter transformer model.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blog**: [https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: One of the best models on the market. Will we have access through
    an API? Will it be a full-blown model? How much will it cost?'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**XLNET** is pretrained like BERT, but the authors contend it exceeds BERT
    model performance.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paper**: [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Challenges**: Does XLNET really exceed the performances of Google BERT, the
    version Google uses for their activities? Do we have access to the best versions
    of Google BERT or XLNET models?'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The list has become endless and it is growing!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing them all remains a challenge beyond the issues mentioned previously.
    Only a few transformer models qualify as foundation models. A foundation model
    must be:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Fully trained for a wide range of tasks
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to perform tasks it was not trained for because of the unique level of
    NLU it has attained
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficiently large to guarantee reasonably accurate results, such as OpenAI
    GPT-3
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many sites offer transformers that prove useful for educational purposes but
    cannot be considered sufficiently trained and large to qualify for benchmarking.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The best approach is to deepen your understanding of transformer models as much
    as possible. At one point, you will become an expert, and finding your way through
    the jungle of big tech innovations will be as easy as choosing a smartphone!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New transformer models keep appearing on the market. Therefore, it is good practice
    to keep up with cutting-edge research by reading publications and books and testing
    some systems.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to assess which transformer models to choose and how to implement
    them. We cannot spend months exploring every model that appears on the market.
    We cannot change models every month if a project is in production. Industry 4.0
    is moving to seamless API ecosystems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Learning all the models is impossible. However, understanding a new model quickly
    can be achieved by deepening our knowledge of transformer models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The basic structure of transformer models remains unchanged. The layers of the
    encoder and/or decoder stacks remain identical. The attention head can be parallelized
    to optimize computation speeds.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer model applies **LSH** buckets and chunking. It also recomputes
    each layer’s input instead of storing the information, thus optimizing memory
    issues. However, a billion-parameter model such as GPT-3 produces acceptable results
    for the same examples.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The DeBERTa model disentangles content and positions, making the training process
    more flexible. The results are impressive. However, billion-parameter models such
    as GPT-3 can equal the outputs of a DeBERTa.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: ViT, CLIP, and DALL-E took us into the fascinating world of task-agnostic text-image
    vision transformer models. Combining language and images produces new and productive
    information.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The question remains to see how far ready-to-use AI and automated systems will
    go. We will attempt to visualize the future of transformer-based AI in the next
    chapter on the rise of metahumans.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain encoders. (True/False)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain decoders. (True/False)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inputs are stored layer by layer in Reformer models. (True/False)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeBERTa transformer models disentangle content and positions. (True/False)
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is necessary to test the hundreds of pretrained transformer models before
    choosing one for a project. (True/False)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latest transformer model is always the best. (True/False)
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is better to have one transformer model per NLP task than one multi-task
    transformer model. (True/False)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer model always needs to be fine-tuned. (True/False)
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI GPT-3 engines can perform a wide range of NLP tasks without fine-tuning.
    (True/False)
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is always better to implement an AI algorithm on a local server. (True/False)
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hugging Face Reformer: [https://huggingface.co/transformers/model_doc/reformer.html?highlight=reformer](https://huggingface.co/transformers/model_doc/reformer.html?highlight=reformer)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face DeBERTa: [https://huggingface.co/transformers/model_doc/deberta.html](https://huggingface.co/transformers/model_doc/deberta.html)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pengcheng He*, *Xiaodong Liu*, *Jianfeng Gao*, *Weizhu Chen*, 2020, *Decoding-enhanced
    BERT with Disentangled Attention*: [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alexey Dosovitskiy*, *Lucas Beyer*, *Alexander Kolesnikov*, *Dirk Weissenborn*,
    *Xiaohua Zhai*, *Thomas Unterthiner*, *Mostafa Dehghani*, *Matthias Minderer*,
    *Georg Heigold*, *Sylvain Gelly*, *Jakob Uszkoreit*, *Neil Houlsby*, *2020, An
    Image is Worth 16x16 Words:* *Transformers for Image Recognition at Scale*: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI: [https://openai.com/](https://openai.com/)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*William Fedus*, *Barret Zoph*, *Noam Shazeer*, 2021, *Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961
    )'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alec Radford*, *Jong Wook Kim*, *Chris Hallacy*, *Aditya Ramesh*, *Gabriel
    Goh*, *Sandhini Agarwal*, *Girish Sastry*, *Amanda Askell*, *Pamela Mishkin*,
    *Jack Clark*, *Gretchen Krueger*, *Ilya Sutskever*, 2021, *Learning Transferable
    Visual Models From Natural Language Supervision*: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alec Radford*, *Jong Wook Kim*, *Chris Hallacy*, *Aditya Ramesh*, *Gabriel
    Goh*, *Sandhini Agarwal*, *Girish Sastry*, *Amanda Askell*, *Pamela Mishkin*,
    *Jack Clark*, *Gretchen Krueger*, *Ilya Sutskever*, 2021, *Learning Transferable
    Visual Models From Natural Language Supervision*: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
- en: 'C7LIP: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'C7LIP: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)'
- en: '*Aditya Ramesh*, *Mikhail Pavlov*, *Gabriel Goh*, *Scott Gray*, *Chelsea Voss*,
    *Alec Radford*, *Mark Chen*, *Ilya Sutskever*, 2021, *Zero-Shot Text-to-Image
    Generation*: [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aditya Ramesh*, *Mikhail Pavlov*, *Gabriel Goh*, *Scott Gray*, *Chelsea Voss*,
    *Alec Radford*, *Mark Chen*, *Ilya Sutskever*, 2021, *Zero-Shot Text-to-Image
    Generation*: [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)'
- en: 'DALL-E: [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DALL-E: [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)'
- en: Join our book’s Discord space
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的*问我任何事*会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
