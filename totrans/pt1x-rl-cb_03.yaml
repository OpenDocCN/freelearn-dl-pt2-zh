- en: Monte Carlo Methods for Making Numerical Estimations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于进行数值估计的蒙特卡洛方法
- en: In the previous chapter, we evaluated and solved a **Markov Decision Process**
    (**MDP**) using **dynamic programming** (**DP**). Model-based methods such as
    DP have some drawbacks. They require the environment to be fully known, including
    the transition matrix and reward matrix. They also have limited scalability, especially
    for environments with plenty of states.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用动态规划评估和解决了**马尔可夫决策过程** (**MDP**)。像DP这样的模型基方法有一些缺点。它们需要完全了解环境，包括转移矩阵和奖励矩阵。它们的可扩展性也有限，特别是对于具有大量状态的环境。
- en: In this chapter, we will continue our learning journey with a model-free approach,
    the **Monte Carlo** (**MC**) methods, which have no requirement of prior knowledge
    of the environment and are much more scalable than DP. We will start by estimating
    the value of Pi with the Monte Carlo method. Moving on, we will talk about how
    to use the MC method to predict state values and state-action values in a first-visit
    and every-visit manner. We will demonstrate training an agent to play the Blackjack
    card game using Monte Carlo. Also, we will implement on-policy and off-policy
    MC control to find the optimal policy for Blackjack. Advanced MC control with
    epsilon-greedy policy and weighted importance sampling will also be covered.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续我们的学习之旅，采用无模型方法，即**蒙特卡洛** (**MC**) 方法，它不需要环境的先前知识，并且比DP具有更高的可扩展性。我们将从使用蒙特卡洛方法估计π的值开始。接下来，我们将讨论如何使用MC方法以首次访问和每次访问的方式来预测状态值和状态-动作值。我们将演示如何使用蒙特卡洛训练代理玩二十一点游戏。此外，我们还将实现基于策略和离策略的MC控制，以找到二十一点的最优策略。还将介绍具有ε-贪心策略和加权重要性采样的高级MC控制。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: Calculating Pi using the Monte Carlo method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法计算π
- en: Performing Monte Carlo policy evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行蒙特卡洛策略评估
- en: Playing Blackjack with Monte Carlo prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛预测玩二十一点
- en: Performing on-policy Monte Carlo control
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行基于策略的蒙特卡洛控制
- en: Developing Monte Carlo control with epsilon-greedy policy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发具有ε-贪心策略的蒙特卡洛控制
- en: Performing off-policy Monte Carlo control
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行离策略蒙特卡洛控制
- en: Developing MC control with weighted importance sampling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发具有加权重要性采样的MC控制
- en: Calculating Pi using the Monte Carlo method
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法计算π
- en: 'Let''s get started with a simple project: estimating the value of π using the
    Monte Carlo method, which is the core of model-free reinforcement learning algorithms.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一个简单的项目：使用蒙特卡洛方法估算π的值，这是无模型强化学习算法的核心。
- en: A **Monte Carlo method** is any method that uses randomness to solve problems.
    The algorithm repeats suitable **random** **sampling** and observes the fraction
    of samples that obey particular properties in order to make numerical estimations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**蒙特卡洛方法**是使用随机性解决问题的任何方法。该算法重复适当的**随机抽样**，并观察满足特定属性的样本分数，以便进行数值估计。'
- en: 'Let''s do a fun exercise where we approximate the value of π using the MC method.
    We''ll place a large number of random points in a square whose width = 2 (-1<x<1,
    -1<y<1), and count how many points fall within the circle of unit radius. We all
    know that the area of the square is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个有趣的练习，使用MC方法近似计算π的值。我们在一个边长为2的正方形内随机放置大量点（-1<x<1, -1<y<1），并计算落入单位半径圆内的点数。我们都知道正方形的面积为：
- en: '![](img/22232c40-3aa2-4836-b2ef-b51b2373eb64.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22232c40-3aa2-4836-b2ef-b51b2373eb64.png)'
- en: 'And the area of the circle is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 圆的面积为：
- en: '![](img/5d3f9d93-05b8-4de2-8244-cb7d0d2327dd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d3f9d93-05b8-4de2-8244-cb7d0d2327dd.png)'
- en: 'If we divide the area of the circle by the area of the square, we have the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将圆的面积除以正方形的面积，我们有以下结果：
- en: '![](img/ec0955f0-26b9-444b-ac7e-d89e809dc3f2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec0955f0-26b9-444b-ac7e-d89e809dc3f2.png)'
- en: '*S/C* can be measured by the fraction of points falling within the circle.
    As a result, the value of π can be estimated as four times *S/C*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*S/C*可以用落入圆内的点的比例来衡量。因此，π的值可以估计为*S/C*的四倍。'
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'We use the MC method to estimate the value of π as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用MC方法来估算π的值如下：
- en: 'Import the necessary modules, including PyTorch, `math` for the true value
    of π, and `matplotlib` to plot the random points in the square:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块，包括PyTorch，π的真实值的`math`，以及用于在正方形内绘制随机点的`matplotlib`：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We randomly generate 1,000 points within the square, with the range of -1<x<1
    and -1<y<1:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随机生成1,000个点在正方形内，范围为-1<x<1和-1<y<1：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize the number of points falling within the unit circle, and a list
    storing those points:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化单位圆内的点数，并存储这些点的列表：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For each random point, calculate the distance to the origin. A point falls
    within the circle if the distance is less than 1:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个随机点，计算到原点的距离。如果距离小于1，则点落在圆内：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Count the number of points in the circle and keep track of those points:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计圆内点的数量，并跟踪这些点：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plot all random points and use a different color for those in the circle:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制所有随机点，并对圆内的点使用不同的颜色：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Draw the circle for better visualization:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制圆以获得更好的可视化效果：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, calculate the value of π:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算π的值：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理是这样的...
- en: 'In *Step 5*, you will see the following plot, where dots are randomly placed
    inside the circle:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5步*，您将看到以下图，其中的点是随机放置在圆内：
- en: '![](img/15f7952d-dfb0-48d7-b405-32f202678cee.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15f7952d-dfb0-48d7-b405-32f202678cee.png)'
- en: The Monte Carlo method is so powerful thanks to the **Law of Large Numbers**
    (**LLN**). Under LLN, the average performance of a large number of repeated events
    or actions will eventually converge to the expected value. In our case, with a
    large number of random points, `4 * (n_point_circle / n_point)` will eventually
    converge to the true value of π.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法之所以如此强大，要归功于**大数定律**（**LLN**）。根据大数定律，大量重复事件或动作的平均表现最终会收敛于期望值。在我们的情况下，大量随机点，`4
    * (n_point_circle / n_point)` 最终会收敛于π的真实值。
- en: 'Finally, in *Step 8*, we print the estimated value of pi and get the following
    result:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*第8步*，我们打印π的估计值，得到以下结果：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The value of π approximated using the Monte Carlo method is quite close to its
    true value (3.14159...).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法近似计算π的值非常接近其真实值（3.14159...）。
- en: There's more...
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: We can further improve our estimation with more iterations than 1,000\. Here,
    we'll experiment with 10,000 iterations. In each iteration, we randomly generate
    a point in the square and see whether it is in the circle; we estimate the value
    of π on the fly based on the fraction of existing points falling within the circle.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过比1,000次更多的迭代进一步改进我们的估计。在这里，我们将尝试10,000次迭代。在每次迭代中，我们在正方形内随机生成一个点，并检查它是否在圆内；根据落入圆内的点的比例，我们即时估算π的值。
- en: 'We then plot the estimation history along with the true value of π. Put these
    into the following function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将估计历史与π的真实值一起绘制。将它们放入以下函数中：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And we call this function with 10,000 iterations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用10,000次迭代调用这个函数：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Refer to the following plot for the resulting estimation history:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表查看估计历史的结果：
- en: '![](img/5a4865b9-bf24-483e-8945-c77c549bca97.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a4865b9-bf24-483e-8945-c77c549bca97.png)'
- en: We can see that with more iterations, the estimation of π is getting closer
    to the true value. There is always some variation in an event or action. More
    repetitions can help smooth it out.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着更多的迭代次数，π的估计值越来越接近真实值。事件或行动总是存在一些变化。增加重复次数可以帮助平滑这种变化。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'If you are interested in seeing more applications of the Monte Carlo method,
    here are some interesting ones:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对蒙特卡洛方法的更多应用感兴趣，这里有一些有趣的应用：
- en: '*Playing games such as Go*, *Havannah*, *and Battleship*, *with MC tree search*,
    *which searches for the best move*: [https://en.wikipedia.org/wiki/Monte_Carlo_tree_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过MC树搜索玩游戏，如围棋*，*哈瓦纳*，*战舰*，*寻找最佳移动*：[https://en.wikipedia.org/wiki/Monte_Carlo_tree_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)'
- en: '*Assessing investments and portfolio*: [https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance](https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估投资和投资组合*：[https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance](https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance)'
- en: '*Studying biological systems with MC simulations*: [https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny](https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用MC模拟研究生物系统*：[https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny](https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny)'
- en: Performing Monte Carlo policy evaluation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行蒙特卡洛策略评估
- en: In [Chapter 2](4e7c9eb4-8260-4d8c-890c-5c7d2d046d0b.xhtml), *Markov Decision
    Process and Dynamic Programming*, we applied DP to perform policy evaluation,
    which is the value (or state-value) function of a policy. It works really well,
    but has some limitations. Fundamentally, it requires a fully known environment,
    including the transition matrix and reward matrix. However, the transition matrix
    in most real-life situations is not known beforehand. A reinforcement learning
    algorithm that needs a known MDP is categorized as a **model-based** algorithm.
    On the other hand, one with no requirement of prior knowledge of transitions and
    rewards is called a **model-free** algorithm. Monte Carlo-based reinforcement
    learning is a model-free approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](4e7c9eb4-8260-4d8c-890c-5c7d2d046d0b.xhtml)，*马尔可夫决策过程与动态规划*中，我们应用DP进行策略评估，即策略的值（或状态值）函数。这确实效果很好，但也有一些限制。基本上，它需要完全了解环境，包括转移矩阵和奖励矩阵。然而，在大多数实际情况下，转移矩阵事先是未知的。需要已知MDP的强化学习算法被归类为**基于模型**的算法。另一方面，不需要先验知识的转移和奖励的算法被称为**无模型**算法。基于蒙特卡洛的强化学习是一种无模型方法。
- en: 'In this recipe, we will evaluate the value function using the Monte Carlo method.
    We will use the FrozenLake environment again as an example, assuming we don''t
    have access to both of its transition and reward matrices. You will recall that
    the **returns** of a process, which are the total rewards over the long run, are
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用蒙特卡洛方法评估值函数。我们再次使用FrozenLake环境作为示例，假设我们无法访问其转移和奖励矩阵。你会记得过程的**返回**，即长期内的总奖励，如下所示：
- en: '![](img/fc629134-5984-43ee-9080-ec6adfd8f2e7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc629134-5984-43ee-9080-ec6adfd8f2e7.png)'
- en: MC policy evaluation uses **empirical mean** **return** instead of **expected
    return** (as in DP) to estimate the value function. There are two ways to perform
    MC policy evaluation. One is **first-visit MC prediction**, which averages the
    returns **only** for the **first occurrence** of a state, s, in an episode. Another
    one is **every-visit MC prediction**, which averages the returns for **every occurrence**
    of a state, *s*, in an episode. Obviously, first-visit MC prediction has a lot
    fewer calculations than the every-visit version, hence it is more frequently used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MC策略评估使用**经验均值返回**而不是**期望返回**（如DP中）来估计值函数。有两种方法可以进行MC策略评估。一种是**首次访问MC预测**，它仅对状态s在一个episode中的**第一次出现**进行返回平均。另一种是**每次访问MC预测**，它对状态s在一个episode中的**每次出现**进行返回平均。显然，首次访问MC预测比每次访问版本计算要少得多，因此更频繁地使用。
- en: How to do it...
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We perform first-visit MC prediction for the optimal policy of FrozenLake as
    follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对FrozenLake的最优策略执行首次访问MC预测如下：
- en: 'Import the PyTorch and Gym libraries, and create an instance of the FrozenLake
    environment:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch和Gym库，并创建FrozenLake环境的实例：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To evaluate a policy using the Monte Carlo method, we first need to define
    a function that simulates a FrozenLake episode given a policy and returns the
    reward and state for each step:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用蒙特卡洛方法评估策略，我们首先需要定义一个函数，该函数模拟给定策略的FrozenLake episode，并返回每个步骤的奖励和状态：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, in the Monte Carlo setting, we need to keep track of the states and rewards
    for all steps, since we don't have access to the full environment, including the
    transition probabilities and reward matrix.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在蒙特卡洛设置中，我们需要跟踪所有步骤的状态和奖励，因为我们无法访问完整的环境，包括转移概率和奖励矩阵。
- en: 'Now, define the function that evaluates the given policy with first-visit MC:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个使用首次访问MC评估给定策略的函数：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We specify the discount rate as 1 for easier computation, and simulate 10,000
    episodes:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折现率设定为1以便计算更加简便，并模拟了10,000个episode：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We use the optimal policy calculated in the previous chapter, *Markov* *Decision
    Process and Dynamic Programming*, and feed it to the first-visit MC function,
    along with other parameters:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用前一章节中计算的最优策略，*马尔可夫决策过程与动态规划*，将其输入到首次访问MC函数中，同时还包括其他参数：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have just solved the value function of the optimal policy using first-visit
    MC prediction.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚使用首次访问MC预测解决了最优策略的值函数。
- en: How it works...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: 'In *Step 3*, we perform the following tasks in MC prediction:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们在MC预测中执行以下任务：
- en: We run `n_episode` episodes
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运行`n_episode`个episode
- en: For each episode, we compute the returns for the first visit of each state
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个episode，我们计算每个状态的首次访问的返回
- en: For each state, we obtain the value by averaging its first returns from all
    episodes
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个状态，我们通过平均所有集的首次回报来获取值
- en: As you can see, in MC-based prediction, it is not necessary to know about the
    full model of the environment. In fact, in most real-world cases, the transition
    matrix and reward matrix are not known beforehand, or are extremely difficult
    to obtain. Imagine how many possible states there are playing chess or Go and
    the number of possible actions; it is almost impossible to work out the transition
    matrix and reward matrix. Model-free reinforcement learning is about learning
    from experience by interacting with the environment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在基于MC的预测中，并不需要了解环境的完整模型。事实上，在大多数真实情况下，过渡矩阵和奖励矩阵事先是未知的，或者极其难以获得。想象一下下棋或围棋中可能的状态数量以及可能的动作数量；几乎不可能计算出过渡矩阵和奖励矩阵。无模型强化学习是通过与环境交互从经验中学习的过程。
- en: In our case, we only considered what could be observed, which included the new
    state and reward in each step, and made predictions using the Monte Carlo method.
    Note that the more episodes we simulate, the more accurate predictions we can
    obtain. If you plot the value updated after each episode, you will see how it
    converges over time, which is similar to what we saw when estimating the value
    of π.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们只考虑了可以观察到的内容，这包括每一步中的新状态和奖励，并使用Monte Carlo方法进行预测。请注意，我们模拟的集数越多，我们可以获得的预测越精确。如果您绘制每个集后更新的值，您将看到它如何随时间收敛，这与我们估计π值时看到的情况类似。
- en: There's more...
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We decide to also perform every-visit MC prediction for the optimal policy
    of FrozenLake:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定为冰湖的最优策略也执行每次访问的MC预测：
- en: 'We define the function that evaluates the given policy with every-visit MC:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了使用每次访问MC评估给定策略的函数：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Similar to first-visit MC, the every-visit function does the following tasks:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与首次访问MC类似，每次访问函数执行以下任务：
- en: It runs `n_episode` episodes
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`n_episode`集
- en: For each episode, it computes the returns for each visit of a state
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一集，它计算每次访问状态的回报
- en: For each state, it obtains the value by averaging all of its returns from all
    episodes
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个状态，通过平均所有集的所有回报来获取值
- en: 'Compute the value by feeding the policy and other parameters in the function:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在函数中输入策略和其他参数来计算值：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Display the resulting value:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果值：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Playing Blackjack with Monte Carlo prediction
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Monte Carlo预测玩21点
- en: In this recipe, we will play Blackjack (also called 21) and evaluate a policy
    we think might work well. You will get more familiar with Monte Carlo prediction
    with the Blackjack example, and get ready to search for the optimal policy using
    Monte Carlo control in the upcoming recipes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将玩21点（也称为21），并评估我们认为可能有效的策略。您将通过21点的示例更加熟悉使用Monte Carlo预测，并准备在即将到来的示例中使用Monte
    Carlo控制搜索最优策略。
- en: Blackjack is a popular card game where the goal is to have the sum of cards
    as close to 21 as possible without exceeding it. The J, K, and Q cards have a
    points value of 10, and cards from 2 to 10 have values from 2 to 10\. The ace
    card can be either 1 or 11 points; when the latter value is chosen, it is called
    a **usable** ace. The player competes against a dealer. At the beginning, both
    parties are given two random cards, but only one of the dealer's cards is revealed
    to the player. The player can request additional cards (called **hit**) or stop
    receiving any more cards (called **stick**). After the player sticks, the dealer
    keeps drawing cards until the sum of cards is greater than or equal to 17\. Before
    the player calls **stick**, if the sum of their cards exceeds 21 (called going
    **bust**), the player loses. Otherwise, if the sum of the dealer's cards exceeds
    21, the player wins.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 21点是一种流行的纸牌游戏，目标是使牌的总和尽可能接近21点而不超过它。J、K和Q牌的点数为10，2到10的牌的点数为2到10。A牌可以是1点或11点；选择后者称为**可用**A。玩家与庄家竞争。一开始，双方都会得到两张随机牌，但只有一张庄家的牌对玩家可见。玩家可以要求额外的牌（称为**要牌**）或停止接收更多的牌（称为**停牌**）。在玩家停牌之前，如果他们的牌的总和超过21（称为爆牌），则玩家输。否则，如果庄家的牌总和超过21，玩家赢。
- en: 'If neither of the two parties goes bust, the one with the highest score will
    win or it may be a draw. The Blackjack environment in Gym is formulated as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两方都没有爆牌，得分最高的一方将获胜，或者可能是平局。Gym中的21点环境如下所述：
- en: An episode of the Blackjack finite MDP starts with two cards for each party,
    and only one of the dealer's cards is observed.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blackjack 有限 MDP 的一轮情节以每方两张牌开始，只有一张庄家的牌是可见的。
- en: An episode ends with either party winning or the parties drawing. The final
    reward of an episode is +1 if the player wins, -1 if they lose, or 0 if a draw
    occurs.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个情节以任一方获胜或双方平局结束。情节的最终奖励如下：如果玩家获胜，则为 `+1`；如果玩家输，则为 `-1`；如果平局，则为 `0`。
- en: In each round, two actions can be taken by the player, hit (1) and stick (0),
    meaning requesting another card and requesting to not receive any further cards.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每轮中，玩家可以执行两个动作，要牌（1）和停牌（0），表示请求另一张牌和请求不再接收任何更多的牌。
- en: We'll first experiment with a simple policy where we keep adding new cards as
    long as the total number of points is less than 18 (or 19 or 20 if you prefer).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先尝试一个简单的策略，即只要总点数小于 18（或者您更喜欢的 19 或 20），就继续添加新的牌。
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Let''s start by simulating the Blackjack environment and exploring its states
    and actions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从模拟 Blackjack 环境开始，并探索其状态和行动：
- en: 'Import PyTorch and Gym, and create a `Blackjack` instance:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和 Gym，并创建一个 `Blackjack` 实例：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Reset the environment:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It returns three state variables:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回三个状态变量：
- en: The player's points (`20` in this example)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家的点数（本例中为 `20`）
- en: The dealer's points (`5` in this example)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄家的点数（本例中为 `5`）
- en: Whether the player has a usable ace (`False` in this example)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家是否拥有可重复使用的 Ace（本例中为 `False`）
- en: A usable ace means the player has an ace that can be counted as 11 without them
    going bust. If the player doesn't have an ace, or has an ace but it makes them
    bust, the state parameter will become `False`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复使用的 Ace 意味着玩家拥有一个 Ace，可以将其计为 11 而不会爆牌。如果玩家没有 Ace，或者有 Ace 但使他们爆牌，状态参数将变为 `False`。
- en: 'Take a look at the following episode :'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下情节：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The 18 points and `True` means that the player has an ace and a 7, and the ace
    is counted as 11.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 18 点和 `True` 表示玩家有一个 Ace 和一个 7，Ace 被计为 11。
- en: 'Let''s take some actions and see how the Blackjack environment works. First,
    we take a hit (requesting an additional card) since we have a usable ace, which
    offers some flexibility:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们执行一些动作，看看 Blackjack 环境的工作原理。首先，我们要牌（请求额外的一张牌），因为我们有可重复使用的 Ace，这提供了一些灵活性：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This returns three state variables `(20, 6, True)`, a reward (`0` for now),
    and whether the episode ends or not (`False` for now).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回三个状态变量 `(20, 6, True)`，一个奖励（目前为`0`），以及该情节是否结束（目前为`False`）。
- en: 'We then stop drawing cards:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们停止抽牌：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We just won in this episode, hence the reward is `1`, and now the episode ends.
    Again, once the player calls **stick**, the dealer will take their actions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一情节中我们刚刚赢得了比赛，因此奖励为 `1`，现在情节结束了。再次提醒，一旦玩家选择 **停牌**，庄家将采取他们的行动。
- en: 'Sometimes we lose; for example:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时我们会输掉；例如：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we''ll predict the value for a simple policy where we stop adding new
    cards when the score reaches 18:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预测一个简单策略的值，在该策略中，当分数达到 18 时停止添加新牌：
- en: 'As always, we first need to define a function that simulates a Blackjack episode
    under a simple policy:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先需要定义一个函数，模拟一个简单策略下的 Blackjack 情节：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we define a function that evaluates a simple Blackjack policy with a first-visit
    MC:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个评估简单的 Blackjack 策略的函数，使用首次访问 MC 方法：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We specify `hold_score` as 18, the discount rate as 1, and simulate 500,000
    episodes:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 `hold_score` 设置为 18，折扣率设置为 1，并模拟 500,000 个情节：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let''s perform MC prediction by plugging in all the variables:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过插入所有变量来执行 MC 预测：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We try to print the resulting value function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试打印结果值函数：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We have just computed the values for all possible states:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚计算了所有可能状态的值：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: And there are 280 states in total.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有 280 个状态。
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: As you can see, in *Step 4*, our points exceed 21, so we lose. Again, the state
    in Blackjack is actually a three-element tuple. The first element is the player's
    score; the second element is the revealed card from the dealer's deck, whose value
    can be from 1 to 10; and the third element is about having a reusable ace or not.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，在 *第 4 步*，我们的点数超过了 21，所以我们输了。再次提醒，Blackjack 的状态实际上是一个三元组。第一个元素是玩家的分数；第二个元素是庄家牌堆中的明牌，其值可以是
    1 到 10；第三个元素是是否拥有可重复使用的 Ace。
- en: It is worth noting that in *Step 5*, in each round of an episode, the agent
    takes a hit or sticks based on the current score, sticking if it is less than
    `hold_score`, and taking a hit otherwise. Again, in the Monte Carlo setting, we
    keep track of the states and rewards for all steps.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在*步骤5*中，在每一轮的一个回合中，代理根据当前分数是否停止，如果分数小于`hold_score`则停止，否则继续。同样，在蒙特卡罗设置中，我们跟踪所有步骤的状态和奖励。
- en: 'Executing the lines of code in *Step 8*, you will see the following result:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*步骤8*中的代码行，您将看到以下结果：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We have just experienced how effective it is to compute the value function
    of 280 states in the Blackjack environment using MC prediction. In the MC prediction
    function in *Step 2*, we performed the following tasks:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚体验了使用MC预测计算21点环境中280个状态的值函数的效果。在*步骤2*中的MC预测函数中，我们执行了以下任务：
- en: We ran `n_episode` episodes under the simple Blackjack policy
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在简单的21点策略下运行了`n_episode`轮次。
- en: For each episode, we computed the returns for the first visit of each state
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一轮次，我们计算了每个状态的首次访问的回报。
- en: For each state, we obtained the value by averaging its first returns from all
    episodes
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个状态，我们通过所有轮次的首次回报的平均值来获取价值。
- en: Note that we ignore states where the player's sum is greater than 21 since we
    know they are all going to be -1\.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们忽略了玩家总分大于21分的状态，因为我们知道它们都会是-1。
- en: The model of the Blackjack environment, including the transition matrix and
    reward matrix, is not known beforehand. Moreover, obtaining the transition probabilities
    between two states is extremely costly. In fact, the transition matrix would be
    of size 280 * 280 * 2, which would require a lot of computation. In the MC-based
    solution, we just need to simulate sufficient episodes, and, for each episode,
    compute the returns and update the value function accordingly.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 21点环境的模型，包括转移矩阵和奖励矩阵，在先验上是不知道的。此外，获取两个状态之间的转移概率是非常昂贵的。事实上，转移矩阵的大小将是280 * 280
    * 2，这将需要大量的计算。在基于MC的解决方案中，我们只需模拟足够的轮次，并且对于每一轮次，计算回报并相应地更新值函数即可。
- en: Next time you play Blackjack with the simple policy (stick if the sum reaches
    a certain level), it would be interesting to use the predicted values to decide
    how much to bid in each game.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下次你使用简单策略玩21点时（如果总分达到某一水平则停止），使用预测的值来决定每局游戏下注金额将会很有趣。
- en: There's more...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Because there are so many states in this case, it is difficult to read their
    values one by one. We can actually visualize the value function by making three-dimensional
    surface plots. The state is three-dimensional, and the third dimension comes with
    two possible options (having a usable ace or not). We can split our plots into
    two parts: one for states with a usable ace, and the other for states without
    a usable ace. In each plot, the *x* axis is the player''s sum, the *y* axis is
    the dealer''s revealed card, and the *z* axis is the value.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在这种情况下有很多状态，逐一读取它们的值是困难的。我们实际上可以通过制作三维表面图来可视化值函数。状态是三维的，第三个维度具有两个可能的选项（有可用有效王牌或无）。我们可以将我们的图分为两部分：一部分用于具有可用有效王牌的状态，另一部分用于没有可用有效王牌的状态。在每个图中，*x*轴是玩家的总和，*y*轴是庄家的明牌，*z*轴是值。
- en: 'Let''s follow these steps to create the visualization:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照这些步骤来创建可视化：
- en: 'Import all the necessary modules in matplotlib for visualization:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于可视化的matplotlib中的所有必要模块：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define a utility function that creates a 3D surface plot:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个创建三维表面图的实用函数：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we define a function that constructs the arrays to be plotted along three
    dimensions and calls the `plot_surface` to visualize the value with and without
    a usable ace, respectively:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，构建要在三个维度上绘制的数组，并调用`plot_surface`来可视化具有和不具有可用有效王牌的值：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We are only interested in looking at states where the player's score is more
    than 11 and we create a `values_to_plot` tensor to store those values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对玩家得分超过11分的状态感兴趣，并创建一个`values_to_plot`张量来存储这些数值。
- en: 'Finally, we call the `plot_blackjack_value` function:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们调用`plot_blackjack_value`函数：
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting value plot for states without a usable ace is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 无法使用有效王牌的状态的结果值图如下所示：
- en: '![](img/b03f4268-f314-4c77-a9f6-25b3e78e846d.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b03f4268-f314-4c77-a9f6-25b3e78e846d.png)'
- en: 'And the value function for states with a usable ace is visualized as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 而对于有可用有效王牌的状态的值函数如下图所示：
- en: '![](img/7ba4cb6d-4fe6-4248-ba3d-393cd29c5660.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ba4cb6d-4fe6-4248-ba3d-393cd29c5660.png)'
- en: Feel free to play around with the value of `hold_score` and see how it affects
    the value function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 随时调整`hold_score`的值并查看它如何影响值函数。
- en: See also
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: If the Blackjack environment is new to you, you can learn more about it from
    the source code at [https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对**Blackjack**环境还不熟悉，可以从源代码了解更多信息，位于[https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py)。
- en: Sometimes, it is easier to read code than to read a plain English description.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，阅读代码比阅读简单的英文描述更容易。
- en: Performing on-policy Monte Carlo control
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行基于策略的Monte Carlo控制
- en: In the previous recipe, we predicted the value of a policy where the agent holds
    if the score gets to 18\. This is a simple policy that everyone can easily come
    up with, although obviously not the optimal one. In this recipe, we will search
    for the optimal policy to play Blackjack, using on-policy Monte Carlo control.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们预测了一种策略的值，其中代理如果得分达到18则持有。这是每个人都可以轻松提出的一种简单策略，尽管显然不是最优策略。在本示例中，我们将使用基于策略的Monte
    Carlo控制来寻找最优的Blackjack玩法策略。
- en: 'Monte Carlo prediction is used to evaluate the value for a given policy, while
    **Monte Carlo control** (**MC control**) is for finding the optimal policy when
    such a policy is not given. There are basically categories of MC control: on-policy
    and off-policy. **On-policy** methods learn about the optimal policy by executing
    the policy and evaluating and improving it, while **off-policy** methods learn
    about the optimal policy using data generated by another policy. The way on-policy
    MC control works is quite similar to policy iteration in dynamic programming,
    which has two phases, evaluation and improvement:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Monte Carlo预测用于评估给定策略的值，而**Monte Carlo控制**（**MC控制**）用于在没有给定策略时寻找最优策略。基本上有两种类型的MC控制：基于策略和脱离策略。**基于策略**方法通过执行策略并评估和改进来学习最优策略，而**脱离策略**方法使用由另一个策略生成的数据来学习最优策略。基于策略MC控制的工作方式与动态规划中的策略迭代非常相似，有两个阶段，评估和改进：
- en: In the evaluation phase, instead of evaluating the value function (also called
    the **state value**, or **utility**), it evaluates the action-value. The **action-value**
    is more frequently called the **Q-function**, which is the utility of a state-action
    pair *(s, a)* by taking action a in state *s* under a given policy. Again, the
    evaluation can be conducted in a first-visit manner or an every-visit manner.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估阶段，它评估动作值函数（也称为**行动值**或**效用**），而不是评估值函数（也称为**状态值**或**效用**）。**行动值**更常被称为**Q函数**，它是在给定策略下，通过在状态*s*中采取动作*a*来获得的状态-动作对*(s,
    a)*的效用。再次强调，评估可以以首次访问方式或每次访问方式进行。
- en: 'In the improvement phase, the policy is updated by assigning the optimal action
    to each state:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在改进阶段，通过为每个状态分配最优动作来更新策略：
- en: '![](img/16d64c6b-4fb4-42bc-974c-09ccb60bc49e.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16d64c6b-4fb4-42bc-974c-09ccb60bc49e.png)'
- en: The optimal policy will be obtained by alternating two phases for a large number
    of iterations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略将通过在大量迭代中交替两个阶段来获取。
- en: How to do it...
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s search for the optimal Blackjack policy with on-policy MC control by
    taking the following steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤使用基于策略的MC控制来寻找最优的Blackjack策略：
- en: 'Import the necessary modules and create a Blackjack instance:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个Blackjack实例：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, let''s develop a function that runs an episode and takes actions under
    a Q-function. This is the improvement phase:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们开发一个函数，运行一个剧集并根据Q函数采取行动。这是改进阶段：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we develop the on-policy MC control algorithm:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开发了基于策略的MC控制算法：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折现率设为1，并将使用500,000个剧集：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Perform on-policy MC control to obtain the optimal Q-function and policy:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行基于策略的MC控制，以获取最优的Q函数和策略：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also calculate the value function of the optimal policy and print out
    the optimal value as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以计算最优策略的值函数，并打印出最优值如下：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Visualize the value using `plot_blackjack_value` and the `plot_surface` function
    we developed in the previous recipe, *Playing Blackjack with Monte Carlo prediction*:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot_blackjack_value`和我们在上一个示例*使用Monte Carlo预测玩Blackjack*中开发的`plot_surface`函数来可视化值：
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works...
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we solve the Blackjack game with on-policy MC control by exploring
    starts. This accomplishes our policy optimization goal by alternating between
    evaluation and improvement with each episode we simulate.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们通过探索启动的 on-policy MC 控制解决了二十一点游戏。通过模拟每个剧集交替进行评估和改进，达到了我们的策略优化目标。
- en: 'In *Step 2*, we run an episode and take actions under a Q-function by performing
    the following tasks:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 2*，我们运行一个剧集，并根据 Q 函数执行动作，具体任务如下：
- en: We initialize an episode.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们初始化一个剧集。
- en: We take a random action as an exploring start.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以探索启动的方式随机选择一个动作。
- en: After the first action, we take actions based on the current Q-function, that
    is, [![](img/9a88e66d-d415-43f7-b28d-1d37c5a07c39.png)].
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个动作后，我们根据当前 Q 函数采取动作，即 [![](img/9a88e66d-d415-43f7-b28d-1d37c5a07c39.png)]。
- en: We record the states, actions, and rewards for all steps in the episode, which
    will be used in the evaluation phase.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们记录剧集中所有步骤的状态、动作和奖励，这将在评估阶段中使用。
- en: It is important to note that the first action is picked randomly because the
    MC control algorithm will converge to the optimal solution only in such cases.
    Starting an episode with a random action in the MC algorithm is called **exploring
    starts**.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，第一个动作是随机选择的，因为只有在这种情况下，MC 控制算法才会收敛到最优解。在 MC 算法中以随机动作开始一个剧集称为 **探索启动**。
- en: In the exploring starts setting, the first action in an episode is chosen randomly,
    in order to ensure the policy converges to the optimal solution. Otherwise, some
    states are never visited, so their state-action values are never optimized, and
    the policy will become suboptimal in the end.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索启动设置中，为了确保策略收敛到最优解，一个剧集中的第一个动作是随机选择的。否则，一些状态将永远不会被访问，因此它们的状态-动作值永远不会被优化，最终策略将变得次优。
- en: '*Step 2* is the improvement phase, and *Step 3* is for MC control, where we
    perform the following tasks:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 2* 是改进阶段，*Step 3* 是用于 MC 控制的阶段，在这个阶段我们执行以下任务：'
- en: Initialize the Q-function with arbitrary small values.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用任意小的值初始化 Q 函数。
- en: Run `n_episode` episodes.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 `n_episode` 个剧集。
- en: For each episode, perform policy improvement and obtain the states, actions,
    and rewards; and perform policy evaluation using first-visit MC prediction based
    on the resulting states, actions, and rewards, which updates the Q-function.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个剧集，执行策略改进，并获得状态、动作和奖励；并使用基于结果状态、动作和奖励的首次访问 MC 预测执行策略评估，更新 Q 函数。
- en: In the end, the optimal Q-function is finalized and the optimal policy is obtained
    by taking the best action for each state in the optimal Q-function.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，最优 Q 函数完成并且通过在最优 Q 函数中为每个状态选择最佳动作获得最优策略。
- en: In each iteration, we make the policy greedy by taking the optimal action with
    respect to the current action-value function Q (that is, [![](img/009695d3-ffb3-4839-b30a-abcad4bffb2b.png)]).
    As a result, we will be able to obtain an optimal policy, even though we started
    with an arbitrary policy.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们通过采取相对于当前动作值函数 Q 的最优动作来使策略贪婪化（即 [![](img/009695d3-ffb3-4839-b30a-abcad4bffb2b.png)]）。因此，我们将能够获得一个最优策略，即使我们从任意策略开始。
- en: 'In *Step 5*, you can see the resulting optimal policy, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 5*，您可以看到最优策略的结果如下：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In *Step 6*, you can see the resulting values for the optimal policy, as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 6*，您可以看到最优策略的结果值如下：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In *Step 7*, you will see the resulting value plot for states without a usable
    ace, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 7*，您将看到没有可用 Ace 的状态的结果值图如下：
- en: '![](img/4dec7a3a-0249-47d3-aabc-885ad9a6112e.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dec7a3a-0249-47d3-aabc-885ad9a6112e.png)'
- en: 'And the value function for states with a usable ace is visualized as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 并且对于有可用 Ace 的状态，其值函数如下所示：
- en: '![](img/ea892c26-215b-451c-b219-8ab9d39c9dc9.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea892c26-215b-451c-b219-8ab9d39c9dc9.png)'
- en: There's more...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You may want to know whether the optimal policy really works better than the
    simple policy. Let''s now simulate 100,000 Blackjack episodes under the optimal
    policy and the simple policy, respectively. We will compare the chances of winning
    and losing for both policies:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道最优策略是否比简单策略表现更好。现在让我们在最优策略和简单策略下模拟 100,000 个二十一点剧集。我们将比较两种策略的获胜和失败的机会：
- en: 'First, we define the simple policy where the **stick** action is taken when
    the score reaches 18:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义一个简单的策略，当分数达到 18 时采取 **stick** 动作：
- en: '[PRE45]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we define a wrapper function that runs one episode under a given policy
    and returns the final reward:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个包装函数，根据给定的策略运行一个剧集，并返回最终的奖励：
- en: '[PRE46]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we specify the number of episodes (100,000), and start the count of wins
    and losses:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定回合数（100,000），并开始计算胜利和失败的次数：
- en: '[PRE47]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we run 100,000 episodes and keep track of the wins and losses:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们运行了100,000个回合并跟踪了胜利和失败的情况：
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we print out the results we get:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出我们得到的结果：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Playing under the optimal policy has a 41.28% chance of winning, while playing
    under the simple policy has a 39.92% chance. Then, we have the probability of
    losing:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在最优策略下玩牌有41.28%的赢的可能性，而在简单策略下玩牌有39.92%的可能性。然后，我们有输的概率：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: On the other hand, playing under the optimal policy has a 49.3% chance of losing,
    while playing under the simple policy has a 51.02% chance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在最优策略下玩牌有49.3%的输的可能性，而在简单策略下玩牌有51.02%的可能性。
- en: Our optimal policy is clearly the winner!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最优策略显然是赢家！
- en: Developing MC control with epsilon-greedy policy
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发MC控制和**ε-贪心策略**：
- en: In the previous recipe, we searched for the optimal policy using MC control
    with greedy search where the action with the highest state-action value was selected.
    However, the best choice available in early episodes does not guarantee an optimal
    solution. If we just focus on what is temporarily the best option and ignore the
    overall problem, we will be stuck in local optima instead of reaching the global
    optima. The workaround is epsilon-greedy policy.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步骤中，我们使用MC控制和贪心搜索搜索最优策略，选择具有最高状态-动作值的动作。然而，在早期回合中的最佳选择并不保证最优解。如果我们只关注临时的最佳选项并忽略整体问题，我们将陷入局部最优解而无法达到全局最优解。解决方法是**ε-贪心策略**。
- en: 'In MC control with **epsilon-greedy policy**, we no longer exploit the best
    action all the time, but choose an action randomly under certain probabilities.
    As the name implies, the algorithm has two folds:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在**ε-贪心策略**的MC控制中，我们不再始终利用最佳动作，而是在一定概率下随机选择动作。顾名思义，该算法有两个方面：
- en: 'Epsilon: given a parameter, ε, with a value from *0* to *1*, each action is
    taken with a probability calculated as follows:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ε：给定参数ε，其值从*0*到*1*，每个动作的选择概率如下计算：
- en: '![](img/854959b9-922c-4630-b928-3715ae33f9a7.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/854959b9-922c-4630-b928-3715ae33f9a7.png)'
- en: Here, |A| is the number of possible actions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，|A| 是可能的动作数。
- en: 'Greedy: the action with the highest state-action value is favored, and its
    probability of being chosen is increased by *1-ε*:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪心：偏爱具有最高状态-动作值的动作，并且它被选择的概率增加了*1-ε*：
- en: '![](img/9dae4a2b-4b54-4a48-a4f8-ca0fb3d63cc7.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9dae4a2b-4b54-4a48-a4f8-ca0fb3d63cc7.png)'
- en: Epsilon-greedy policy exploits the best action most of the time and also keeps
    exploring different actions from time to time.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**ε-贪心策略**大部分时间都会利用最佳动作，同时也会时不时地探索不同的动作。'
- en: How to do it...
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s solve the Blackjack environment using epsilon-greedy policy:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用**ε-贪心策略**解决Blackjack环境：
- en: 'Import the necessary modules and create a Blackjack instance:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个Blackjack实例：
- en: '[PRE51]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, let''s develop a function that runs an episode and performs epsilon-greedy:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们开发一个运行回合并执行**ε-贪心**的函数：
- en: '[PRE52]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, develop the on-policy MC control with epsilon-greedy:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开发on-policy MC控制和**ε-贪心策略**：
- en: '[PRE53]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We specify the discount rate as 1, ε as 0.1, and will use 500,000 episodes:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折扣率指定为1，ε指定为0.1，并将使用500,000个回合：
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Perform MC control with epsilon-greedy policy to obtain the optimal Q-function
    and policy:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行MC控制和**ε-贪心策略**以获取最优的Q函数和策略：
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Feel free to print out the optimal value and visualize it using `plot_blackjack_value`
    and the `plot_surface` function we developed. We do not repeat the process herein.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随意打印出最优值，并使用我们开发的`plot_blackjack_value`和`plot_surface`函数进行可视化。我们此处不再重复该过程。
- en: 'Finally, we want to know whether the epsilon-greedy method really works better.
    Again, we simulate 100,000 Blackjack episodes under the optimal policy generated
    by epsilon-greedy and compute the chances of winning and losing:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们想知道**ε-贪心方法**是否真的效果更好。再次，我们模拟了100,000个Blackjack的回合，在**ε-贪心**生成的最优策略下计算赢和输的概率：
- en: '[PRE56]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, we reuse the `simulate_episode` function from the previous recipe.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们重新使用了上一个示例中的`simulate_episode`函数。
- en: How it works...
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we solve the Blackjack game with on-policy MC control with epsilon-greedy.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用**ε-贪心**的on-policy MC控制解决了Blackjack游戏。
- en: 'In *Step 2*, we run an episode and perform epsilon-greedy with the following
    tasks:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2步*中，我们运行一个回合，并执行**ε-贪心**，完成以下任务：
- en: We initialize an episode.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们初始化一个回合。
- en: 'We compute the probabilities of choosing individual actions: for the best action
    based on the current Q-function, the probability is [![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png),]
    otherwise, the probability is [![](img/04c14eba-02a1-4015-884c-8544e51c7101.png)].'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算选择各个动作的概率：基于当前Q函数的最佳动作的概率为[![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png)]，否则的概率为[![](img/04c14eba-02a1-4015-884c-8544e51c7101.png)]。
- en: We record the states, actions, and rewards for all steps in the episode, which
    will be used in the evaluation phase.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们记录了每一集中所有步骤的状态、动作和奖励，这将在评估阶段中使用。
- en: Epsilon-greedy outperforms the greedy search method by exploiting the best action
    with a probability of [![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png)] and
    at the same time, allows the exploration of other actions randomly with a probability
    of [![](img/5d0c99f6-6729-49ea-8cf2-245222bfe187.png)]. The hyperparameter, ε,
    is a trade-off between exploitation and exploration. If its value is 0, the algorithm
    becomes entirely greedy; if the value is *1*, each action is chosen evenly, so
    the algorithm just does random exploration.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ε贪心方法通过以[![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png)]的概率利用最佳动作，并同时允许以[![](img/5d0c99f6-6729-49ea-8cf2-245222bfe187.png)]的概率随机探索其他动作。超参数ε是利用与探索之间的权衡。如果其值为0，则算法完全贪婪；如果值为1，则每个动作均匀选择，因此算法只进行随机探索。
- en: The value of ε needs to be tuned based on the experiment, and there is no universal
    value that works best for all experiments. With that being said, in general, we
    can choose 0.1, 0.2, or 0.3 to begin with. Another approach is to start with a
    slightly larger value (such as 0.5 or 0.7) and gradually reduce it over time (for
    example, decaying by 0.999 for each episode). In this way, the policy will focus
    on exploring different actions at the beginning and, over time, it will tend to
    exploit good actions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ε的值需要根据实验进行调整，没有一个适用于所有实验的通用值。话虽如此，一般来说，我们可以选择0.1、0.2或0.3作为起点。另一种方法是从稍大的值（如0.5或0.7）开始，并逐渐减少（例如每集减少0.999）。通过这种方式，策略将在开始时专注于探索不同的动作，并随着时间的推移，趋向于利用好的动作。
- en: 'Finally, after performing *Step 6*, averaging the results from 100,000 episodes
    and printing the winning probability, we now have the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在执行*步骤6*、对来自10万集的结果进行平均并打印获胜概率后，我们现在有以下结果：
- en: '[PRE57]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The optimal policy obtained by the epsilon-greedy method has a 42.44% chance
    of winning, which is higher than the chance of winning (41.28%) without epsilon-greedy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通过ε贪心方法得到的最优策略具有42.44%的获胜机率，比没有ε贪心的获胜机率（41.28%）要高。
- en: 'Then, we also print the losing probability:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还打印出了失利概率：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As you can see, the epsilon-greedy method has a lower chance of losing (48.05%
    versus 49.3% without epsilon-greedy).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，ε贪心方法具有较低的失利机率（48.05%与没有ε贪心的49.3%相比）。
- en: Performing off-policy Monte Carlo control
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行离策略蒙特卡洛控制
- en: Another MC-based approach to solve an MDP is with **off-policy** control, which
    we will discuss in this recipe.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于MC的方法来解决MDP是**离策略**控制，我们将在这个章节中讨论。
- en: The **off-policy** method optimizes the **target policy****,** π, using data
    generated by another policy, called the **behavior policy**, b. The target policy
    performs exploitation all the time while the behavior policy is for exploration
    purposes. This means that the target policy is greedy with respect to its current
    Q-function, and the behavior policy generates behavior so that the target policy
    has data to learn from. The behavior policy can be anything as long as all actions
    in all states can be chosen with non-zero probabilities, which guarantees that
    the behavior policy can explore all possibilities.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**离策略**方法通过由另一个称为**行为策略**b生成的数据来优化**目标策略**π。目标策略始终进行利用，而行为策略则用于探索目的。这意味着目标策略在当前Q函数的贪婪方面是贪婪的，而行为策略生成行为以便目标策略有数据可学习。行为策略可以是任何东西，只要所有状态的所有动作都能以非零概率选择，这保证了行为策略可以探索所有可能性。'
- en: 'Since we are dealing with two different policies in the off-policy method,
    we can only use the **common** steps in episodes that take place in both policies.
    This means that we start with the latest step whose action taken under the behavior
    policy is different from the action taken under the greedy policy. And to learn
    about the target policy with another policy, we use a technique called **importance
    sampling**, which is commonly used to estimate the expected value under a distribution,
    given samples generated from a different distribution. The weighted importance
    for a state-action pair is calculated as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在离策略方法中处理两种不同的策略，我们只能在两种策略中发生的剧集中使用**共同**步骤。这意味着我们从行为策略下执行的最新步骤开始，其行动与贪婪策略下执行的行动不同。为了了解另一个策略的目标策略，并使用一种称为**重要性抽样**的技术，这种技术通常用于估计在给定从不同分布生成的样本下的预期值。状态-动作对的加权重要性计算如下：
- en: '![](img/dfb06509-08d5-4657-8233-a1497c2dd69c.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfb06509-08d5-4657-8233-a1497c2dd69c.png)'
- en: Here, π (*ak* | *sk*) is the probability of taking action *ak* in state *sk*
    under the target policy; *b*(*ak* | *sk*) is the probability under the behavior
    policy; and the weight, *wt*, is the multiplication of ratios between those two
    probabilities from step *t* to the end of the episode. The weight, *wt*, is applied
    to the return at step *t*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，π（*ak* | *sk*）是在目标策略下在状态*sk*中采取动作*ak*的概率；*b*（*ak* | *sk*）是在行为策略下的概率；权重*wt*是从步骤*t*到剧集结束时那两个概率的比率的乘积。权重*wt*应用于步骤*t*的回报。
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s search for the optimal Blackjack policy with off-policy MC control by
    using the following steps:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下步骤来搜索使用离策略蒙特卡洛控制的最优21点策略：
- en: 'Import the necessary modules and create a Blackjack instance:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个21点实例：
- en: '[PRE59]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We start by defining the behavior policy, which randomly chooses an action
    with the same probability in our case:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义行为策略，它在我们的情况下随机选择一个动作：
- en: '[PRE60]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The behavior policy can be anything as long as it selects all actions in all
    states with non-zero probability.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 行为策略可以是任何东西，只要它以非零概率选择所有状态中的所有动作。
- en: 'Next, let''s develop a function that runs an episode and takes actions under
    the behavior policy:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们开发一个函数，运行一个剧集，并在行为策略下执行动作：
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This records the states, actions, and rewards for all of the steps in the episode,
    which will be used as learning data for the target policy.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这记录了剧集中所有步骤的状态、动作和奖励，这将作为目标策略的学习数据使用。
- en: 'Now, we''ll develop the off-policy MC control algorithm:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将开发离策略蒙特卡洛控制算法：
- en: '[PRE62]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折现率设为1，并将使用500,000个剧集：
- en: '[PRE63]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Perform off-policy MC control with the `random_policy` behavior policy to obtain
    the optimal Q-function and policy:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`random_policy`行为策略执行离策略蒙特卡洛控制以获取最优Q函数和策略：
- en: '[PRE64]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How it works...
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we solve the Blackjack game with off-policy MC.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用离策略蒙特卡洛解决21点游戏。
- en: 'In *Step 4*, the off-policy MC control algorithm does the following tasks:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，离策略蒙特卡洛控制算法执行以下任务：
- en: It initializes the Q-function with arbitrary small values.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它用任意小的值初始化Q函数。
- en: It runs `n_episode` episodes.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行`n_episode`个剧集。
- en: For each episode, it performs the behavior policy to generate the states, actions,
    and rewards; it performs policy evaluation on the target policy using first-visit
    MC prediction based on the **common** steps; and it updates the Q-function based
    on the weighted return.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个剧集，它执行行为策略以生成状态、动作和奖励；它使用基于**共同**步骤的首次访问蒙特卡洛预测对目标策略进行策略评估；并根据加权回报更新Q函数。
- en: In the end, the optimal Q-function is finalized, and the optimal policy is obtained
    by taking the best action for each state in the optimal Q-function.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，最优Q函数完成，并且通过在最优Q函数中为每个状态选择最佳动作来获取最优策略。
- en: It learns about the target policy by observing another agent and reusing the
    experience generated from another policy. The target policy is optimized in a
    greedy way, while the behavior policy keeps exploring different options. It averages
    the returns from the behavior policy with the importance ratios of their probabilities
    in the target policy. You may wonder why π (*ak* | *sk*) is always equal to 1
    in the computation of the importance ratio, *wt*. Recall that we only consider
    the common steps taken under the behavior policy and presumably the target policy,
    and the target policy is always greedy. Hence, π (*a* | *s*) = 1 is always true.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过观察另一个代理并重复使用从另一个策略生成的经验来学习目标策略。目标策略以贪婪方式优化，而行为策略则继续探索不同的选项。它将行为策略的回报与目标策略中其概率的重要性比率平均起来。你可能会想知道为什么在重要性比率
    *wt* 的计算中，π (*ak* | *sk*) 总是等于 1。回想一下，我们只考虑在行为策略和目标策略下采取的共同步骤，并且目标策略总是贪婪的。因此，π
    (*a* | *s*) = 1 总是成立。
- en: There's more...
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'We can actually implement the MC method in an incremental way. In an episode,
    instead of storing the return and importance ratio for each first-occurring, state-action
    pair, we can calculate the Q-function on the fly. In a non-incremental way, the
    Q-function is computed in the end with all stored returns in n episodes:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实际上以增量方式实现蒙特卡洛方法。在一个 episode 中，我们可以即时计算 Q 函数，而不是为每个首次出现的状态-动作对存储回报和重要性比率。在非增量方式中，Q
    函数在 n 个 episode 中的所有存储回报最终计算出来：
- en: '![](img/e12f8275-764f-4780-b714-1aac61b0c50f.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e12f8275-764f-4780-b714-1aac61b0c50f.png)'
- en: 'While in the incremental approach, the Q-function is updated in each step of
    an episode as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 而在增量方法中，Q 函数在每个 episode 的每个步骤中更新如下：
- en: '![](img/55912852-d91f-4f99-ab0d-db395b02c49e.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55912852-d91f-4f99-ab0d-db395b02c49e.png)'
- en: 'The incremental equivalent is more efficient as it reduces memory consumption
    and is more scalable. Let''s go ahead and implement it:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 增量等价版本更高效，因为它减少了内存消耗并且更具可扩展性。让我们继续实施它：
- en: '[PRE65]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can call this incremental version to obtain the optimal policy:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用此增量版本来获得最优策略：
- en: '[PRE66]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: See also
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For a detailed explanation of importance sampling, the following is a perfect
    resource:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解重要性抽样的详细解释，请参考以下完美资源：
- en: '[https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf](https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf](https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf)'
- en: Developing MC control with weighted importance sampling
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用加权重要性抽样开发蒙特卡洛控制
- en: In the previous recipe, we simply averaged the returns from the behavior policy
    with importance ratios of their probabilities in the target policy. This technique
    is formally called **ordinary importance sampling**. It is known to have high
    variance and, therefore, we usually prefer the weighted version of importance
    sampling, which we will talk about in this recipe.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们简单地使用了行为策略的回报与目标策略中其概率的重要性比率的平均值。这种技术在形式上称为**普通重要性抽样**。众所周知，它具有很高的方差，因此我们通常更喜欢重要性抽样的加权版本，在本示例中我们将讨论这一点。
- en: 'Weighted importance sampling differs from ordinary importance sampling in the
    way it averages returns. Instead of simply averaging, it takes the weighted average
    of the returns:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 加权重要性抽样与普通重要性抽样的不同之处在于它在平均回报方面采用了加权平均值：
- en: '![](img/5de98fd1-216b-4c49-a723-60f3b3d4640b.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5de98fd1-216b-4c49-a723-60f3b3d4640b.png)'
- en: It often has a much lower variance compared to the ordinary version. If you
    have experimented with ordinary importance sampling for Blackjack, you will find
    the results vary a lot in each experiment.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常与普通版本相比具有更低的方差。如果您对二十一点游戏尝试过普通重要性抽样，您会发现每次实验结果都不同。
- en: How to do it...
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s solve Blackjack with off-policy MC control with weighted importance
    sampling by using the following steps:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤使用加权重要性抽样来解决二十一点游戏的离策略蒙特卡洛控制问题：
- en: 'Import the necessary modules and create a Blackjack instance:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个二十一点实例：
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We start by defining the behavior policy, which randomly chooses an action
    with the same probability in our case:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义行为策略，该策略在我们的情况下随机选择一个动作：
- en: '[PRE68]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Next, we reuse the `run_episode` function, which runs an episode and takes actions
    under the behavior policy.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们重复使用`run_episode`函数，该函数在行为策略下运行一个 episode 并采取行动。
- en: 'Now, we develop the off-policy MC control algorithm with weighted importance
    sampling:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用加权重要性抽样来开发离策略蒙特卡洛控制算法：
- en: '[PRE69]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Note that this is an incremental version of MC control.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这是离策略蒙特卡洛控制的增量版本。
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折扣率设定为1，并将使用500,000个情节：
- en: '[PRE70]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Perform off-policy MC control with the `random_policy` behavior policy to obtain
    the optimal Q-function and policy:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`random_policy`行为策略执行离策略蒙特卡洛控制，以获取最优的Q函数和策略：
- en: '[PRE71]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works...
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: We have solved the Blackjack problem using off-policy MC control with weighted
    importance sampling in this recipe. It is quite similar to ordinary importance
    sampling, but instead of scaling the returns by the ratios and averaging the results,
    it scales the returns using the weighted average. And, in practice, weighted importance
    sampling is of much lower variance than ordinary importance sampling and is therefore
    strongly preferred.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个示例中使用了带有加权重要性抽样的离策略蒙特卡洛控制来解决二十一点问题。这与普通重要性抽样非常相似，但不是通过比率来缩放回报和平均结果，而是使用加权平均值来缩放回报。实际上，加权重要性抽样的方差比普通重要性抽样低得多，因此被强烈推荐使用。
- en: There's more...
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Finally, why don't we simulate some episodes and see what the chances of winning
    and losing will be under the resulting optimal policy?
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为什么不模拟一些情节，看看在生成的最优策略下获胜和失败的机会如何？
- en: 'We reuse the `simulate_episode` function we developed in the *Performing on-policy
    Monte Carlo control* recipe and simulate 100,000 episodes:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复使用了我们在*执行策略蒙特卡洛控制*食谱中开发的`simulate_episode`函数，并模拟了100,000个情节：
- en: '[PRE72]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Then, we print out the results we get:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打印出我们得到的结果：
- en: '[PRE73]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: See also
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For proof of the fact that weighted importance sampling outperforms ordinary
    importance sampling, feel free to check out the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 有关加权重要性抽样优于普通重要性抽样的证明，请随时查看以下内容：
- en: '*Hesterberg*, *T*. *C*., *Advances in importance sampling*, *Ph*.*D*. *Dissertation*,
    *Statistics Department*, *Stanford University*, 1988'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hesterberg*, *T*. *C*., *进展中的重要性抽样*, *统计系*, *斯坦福大学*, 1988'
- en: '*Casella*, *G*., *Robert*, *C*. *P.,* *Post*-*processing accept*-*reject samples*:
    *recycling and rescaling*. *Journal of Computational and Graphical Statistics*,
    *7*(*2*):*139–157*, *1988*'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Casella*, *G*., *Robert*, *C*. *P.*, *Post*-*processing accept*-*reject samples*:
    *recycling and rescaling*. *计算与图形统计杂志*, *7*(*2*):*139–157*, *1988*'
- en: '*Precup*, *D*., *Sutton*, *R. S*., *Singh*, *S., Eligibility traces for off*-*policy
    policy evaluation*. *In Proceedings of the 17th International Conference on Machine
    Learning*, *pp. 759*–*766*, *2000*'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Precup*, *D*., *Sutton*, *R. S*., *Singh*, *S*., 离策略策略评估的资格痕迹。*在第17届国际机器学习大会上的论文集*,
    *pp. 759*–*766*, *2000*'
