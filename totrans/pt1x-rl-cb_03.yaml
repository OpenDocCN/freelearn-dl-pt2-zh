- en: Monte Carlo Methods for Making Numerical Estimations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we evaluated and solved a **Markov Decision Process**
    (**MDP**) using **dynamic programming** (**DP**). Model-based methods such as
    DP have some drawbacks. They require the environment to be fully known, including
    the transition matrix and reward matrix. They also have limited scalability, especially
    for environments with plenty of states.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will continue our learning journey with a model-free approach,
    the **Monte Carlo** (**MC**) methods, which have no requirement of prior knowledge
    of the environment and are much more scalable than DP. We will start by estimating
    the value of Pi with the Monte Carlo method. Moving on, we will talk about how
    to use the MC method to predict state values and state-action values in a first-visit
    and every-visit manner. We will demonstrate training an agent to play the Blackjack
    card game using Monte Carlo. Also, we will implement on-policy and off-policy
    MC control to find the optimal policy for Blackjack. Advanced MC control with
    epsilon-greedy policy and weighted importance sampling will also be covered.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Pi using the Monte Carlo method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing Monte Carlo policy evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing Blackjack with Monte Carlo prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing on-policy Monte Carlo control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing Monte Carlo control with epsilon-greedy policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing off-policy Monte Carlo control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing MC control with weighted importance sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating Pi using the Monte Carlo method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started with a simple project: estimating the value of π using the
    Monte Carlo method, which is the core of model-free reinforcement learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: A **Monte Carlo method** is any method that uses randomness to solve problems.
    The algorithm repeats suitable **random** **sampling** and observes the fraction
    of samples that obey particular properties in order to make numerical estimations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a fun exercise where we approximate the value of π using the MC method.
    We''ll place a large number of random points in a square whose width = 2 (-1<x<1,
    -1<y<1), and count how many points fall within the circle of unit radius. We all
    know that the area of the square is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22232c40-3aa2-4836-b2ef-b51b2373eb64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the area of the circle is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d3f9d93-05b8-4de2-8244-cb7d0d2327dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we divide the area of the circle by the area of the square, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec0955f0-26b9-444b-ac7e-d89e809dc3f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*S/C* can be measured by the fraction of points falling within the circle.
    As a result, the value of π can be estimated as four times *S/C*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the MC method to estimate the value of π as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules, including PyTorch, `math` for the true value
    of π, and `matplotlib` to plot the random points in the square:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We randomly generate 1,000 points within the square, with the range of -1<x<1
    and -1<y<1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the number of points falling within the unit circle, and a list
    storing those points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For each random point, calculate the distance to the origin. A point falls
    within the circle if the distance is less than 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of points in the circle and keep track of those points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot all random points and use a different color for those in the circle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw the circle for better visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, calculate the value of π:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 5*, you will see the following plot, where dots are randomly placed
    inside the circle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15f7952d-dfb0-48d7-b405-32f202678cee.png)'
  prefs: []
  type: TYPE_IMG
- en: The Monte Carlo method is so powerful thanks to the **Law of Large Numbers**
    (**LLN**). Under LLN, the average performance of a large number of repeated events
    or actions will eventually converge to the expected value. In our case, with a
    large number of random points, `4 * (n_point_circle / n_point)` will eventually
    converge to the true value of π.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in *Step 8*, we print the estimated value of pi and get the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The value of π approximated using the Monte Carlo method is quite close to its
    true value (3.14159...).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can further improve our estimation with more iterations than 1,000\. Here,
    we'll experiment with 10,000 iterations. In each iteration, we randomly generate
    a point in the square and see whether it is in the circle; we estimate the value
    of π on the fly based on the fraction of existing points falling within the circle.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then plot the estimation history along with the true value of π. Put these
    into the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And we call this function with 10,000 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following plot for the resulting estimation history:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a4865b9-bf24-483e-8945-c77c549bca97.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that with more iterations, the estimation of π is getting closer
    to the true value. There is always some variation in an event or action. More
    repetitions can help smooth it out.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are interested in seeing more applications of the Monte Carlo method,
    here are some interesting ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Playing games such as Go*, *Havannah*, *and Battleship*, *with MC tree search*,
    *which searches for the best move*: [https://en.wikipedia.org/wiki/Monte_Carlo_tree_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assessing investments and portfolio*: [https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance](https://en.wikipedia.org/wiki/Monte_Carlo_methods_in_finance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Studying biological systems with MC simulations*: [https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny](https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing Monte Carlo policy evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](4e7c9eb4-8260-4d8c-890c-5c7d2d046d0b.xhtml), *Markov Decision
    Process and Dynamic Programming*, we applied DP to perform policy evaluation,
    which is the value (or state-value) function of a policy. It works really well,
    but has some limitations. Fundamentally, it requires a fully known environment,
    including the transition matrix and reward matrix. However, the transition matrix
    in most real-life situations is not known beforehand. A reinforcement learning
    algorithm that needs a known MDP is categorized as a **model-based** algorithm.
    On the other hand, one with no requirement of prior knowledge of transitions and
    rewards is called a **model-free** algorithm. Monte Carlo-based reinforcement
    learning is a model-free approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will evaluate the value function using the Monte Carlo method.
    We will use the FrozenLake environment again as an example, assuming we don''t
    have access to both of its transition and reward matrices. You will recall that
    the **returns** of a process, which are the total rewards over the long run, are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc629134-5984-43ee-9080-ec6adfd8f2e7.png)'
  prefs: []
  type: TYPE_IMG
- en: MC policy evaluation uses **empirical mean** **return** instead of **expected
    return** (as in DP) to estimate the value function. There are two ways to perform
    MC policy evaluation. One is **first-visit MC prediction**, which averages the
    returns **only** for the **first occurrence** of a state, s, in an episode. Another
    one is **every-visit MC prediction**, which averages the returns for **every occurrence**
    of a state, *s*, in an episode. Obviously, first-visit MC prediction has a lot
    fewer calculations than the every-visit version, hence it is more frequently used.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We perform first-visit MC prediction for the optimal policy of FrozenLake as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the PyTorch and Gym libraries, and create an instance of the FrozenLake
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate a policy using the Monte Carlo method, we first need to define
    a function that simulates a FrozenLake episode given a policy and returns the
    reward and state for each step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, in the Monte Carlo setting, we need to keep track of the states and rewards
    for all steps, since we don't have access to the full environment, including the
    transition probabilities and reward matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, define the function that evaluates the given policy with first-visit MC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the discount rate as 1 for easier computation, and simulate 10,000
    episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the optimal policy calculated in the previous chapter, *Markov* *Decision
    Process and Dynamic Programming*, and feed it to the first-visit MC function,
    along with other parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We have just solved the value function of the optimal policy using first-visit
    MC prediction.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 3*, we perform the following tasks in MC prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: We run `n_episode` episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, we compute the returns for the first visit of each state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each state, we obtain the value by averaging its first returns from all
    episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, in MC-based prediction, it is not necessary to know about the
    full model of the environment. In fact, in most real-world cases, the transition
    matrix and reward matrix are not known beforehand, or are extremely difficult
    to obtain. Imagine how many possible states there are playing chess or Go and
    the number of possible actions; it is almost impossible to work out the transition
    matrix and reward matrix. Model-free reinforcement learning is about learning
    from experience by interacting with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we only considered what could be observed, which included the new
    state and reward in each step, and made predictions using the Monte Carlo method.
    Note that the more episodes we simulate, the more accurate predictions we can
    obtain. If you plot the value updated after each episode, you will see how it
    converges over time, which is similar to what we saw when estimating the value
    of π.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We decide to also perform every-visit MC prediction for the optimal policy
    of FrozenLake:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the function that evaluates the given policy with every-visit MC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to first-visit MC, the every-visit function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It runs `n_episode` episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, it computes the returns for each visit of a state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each state, it obtains the value by averaging all of its returns from all
    episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute the value by feeding the policy and other parameters in the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the resulting value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Playing Blackjack with Monte Carlo prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will play Blackjack (also called 21) and evaluate a policy
    we think might work well. You will get more familiar with Monte Carlo prediction
    with the Blackjack example, and get ready to search for the optimal policy using
    Monte Carlo control in the upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Blackjack is a popular card game where the goal is to have the sum of cards
    as close to 21 as possible without exceeding it. The J, K, and Q cards have a
    points value of 10, and cards from 2 to 10 have values from 2 to 10\. The ace
    card can be either 1 or 11 points; when the latter value is chosen, it is called
    a **usable** ace. The player competes against a dealer. At the beginning, both
    parties are given two random cards, but only one of the dealer's cards is revealed
    to the player. The player can request additional cards (called **hit**) or stop
    receiving any more cards (called **stick**). After the player sticks, the dealer
    keeps drawing cards until the sum of cards is greater than or equal to 17\. Before
    the player calls **stick**, if the sum of their cards exceeds 21 (called going
    **bust**), the player loses. Otherwise, if the sum of the dealer's cards exceeds
    21, the player wins.
  prefs: []
  type: TYPE_NORMAL
- en: 'If neither of the two parties goes bust, the one with the highest score will
    win or it may be a draw. The Blackjack environment in Gym is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An episode of the Blackjack finite MDP starts with two cards for each party,
    and only one of the dealer's cards is observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An episode ends with either party winning or the parties drawing. The final
    reward of an episode is +1 if the player wins, -1 if they lose, or 0 if a draw
    occurs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each round, two actions can be taken by the player, hit (1) and stick (0),
    meaning requesting another card and requesting to not receive any further cards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll first experiment with a simple policy where we keep adding new cards as
    long as the total number of points is less than 18 (or 19 or 20 if you prefer).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by simulating the Blackjack environment and exploring its states
    and actions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and Gym, and create a `Blackjack` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns three state variables:'
  prefs: []
  type: TYPE_NORMAL
- en: The player's points (`20` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dealer's points (`5` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the player has a usable ace (`False` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A usable ace means the player has an ace that can be counted as 11 without them
    going bust. If the player doesn't have an ace, or has an ace but it makes them
    bust, the state parameter will become `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following episode :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The 18 points and `True` means that the player has an ace and a 7, and the ace
    is counted as 11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take some actions and see how the Blackjack environment works. First,
    we take a hit (requesting an additional card) since we have a usable ace, which
    offers some flexibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This returns three state variables `(20, 6, True)`, a reward (`0` for now),
    and whether the episode ends or not (`False` for now).
  prefs: []
  type: TYPE_NORMAL
- en: 'We then stop drawing cards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We just won in this episode, hence the reward is `1`, and now the episode ends.
    Again, once the player calls **stick**, the dealer will take their actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes we lose; for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll predict the value for a simple policy where we stop adding new
    cards when the score reaches 18:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we first need to define a function that simulates a Blackjack episode
    under a simple policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define a function that evaluates a simple Blackjack policy with a first-visit
    MC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify `hold_score` as 18, the discount rate as 1, and simulate 500,000
    episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform MC prediction by plugging in all the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We try to print the resulting value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We have just computed the values for all possible states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: And there are 280 states in total.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, in *Step 4*, our points exceed 21, so we lose. Again, the state
    in Blackjack is actually a three-element tuple. The first element is the player's
    score; the second element is the revealed card from the dealer's deck, whose value
    can be from 1 to 10; and the third element is about having a reusable ace or not.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that in *Step 5*, in each round of an episode, the agent
    takes a hit or sticks based on the current score, sticking if it is less than
    `hold_score`, and taking a hit otherwise. Again, in the Monte Carlo setting, we
    keep track of the states and rewards for all steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the lines of code in *Step 8*, you will see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We have just experienced how effective it is to compute the value function
    of 280 states in the Blackjack environment using MC prediction. In the MC prediction
    function in *Step 2*, we performed the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: We ran `n_episode` episodes under the simple Blackjack policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, we computed the returns for the first visit of each state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each state, we obtained the value by averaging its first returns from all
    episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we ignore states where the player's sum is greater than 21 since we
    know they are all going to be -1\.
  prefs: []
  type: TYPE_NORMAL
- en: The model of the Blackjack environment, including the transition matrix and
    reward matrix, is not known beforehand. Moreover, obtaining the transition probabilities
    between two states is extremely costly. In fact, the transition matrix would be
    of size 280 * 280 * 2, which would require a lot of computation. In the MC-based
    solution, we just need to simulate sufficient episodes, and, for each episode,
    compute the returns and update the value function accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Next time you play Blackjack with the simple policy (stick if the sum reaches
    a certain level), it would be interesting to use the predicted values to decide
    how much to bid in each game.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because there are so many states in this case, it is difficult to read their
    values one by one. We can actually visualize the value function by making three-dimensional
    surface plots. The state is three-dimensional, and the third dimension comes with
    two possible options (having a usable ace or not). We can split our plots into
    two parts: one for states with a usable ace, and the other for states without
    a usable ace. In each plot, the *x* axis is the player''s sum, the *y* axis is
    the dealer''s revealed card, and the *z* axis is the value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow these steps to create the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary modules in matplotlib for visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a utility function that creates a 3D surface plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a function that constructs the arrays to be plotted along three
    dimensions and calls the `plot_surface` to visualize the value with and without
    a usable ace, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We are only interested in looking at states where the player's score is more
    than 11 and we create a `values_to_plot` tensor to store those values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the `plot_blackjack_value` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting value plot for states without a usable ace is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b03f4268-f314-4c77-a9f6-25b3e78e846d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the value function for states with a usable ace is visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ba4cb6d-4fe6-4248-ba3d-393cd29c5660.png)'
  prefs: []
  type: TYPE_IMG
- en: Feel free to play around with the value of `hold_score` and see how it affects
    the value function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the Blackjack environment is new to you, you can learn more about it from
    the source code at [https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is easier to read code than to read a plain English description.
  prefs: []
  type: TYPE_NORMAL
- en: Performing on-policy Monte Carlo control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we predicted the value of a policy where the agent holds
    if the score gets to 18\. This is a simple policy that everyone can easily come
    up with, although obviously not the optimal one. In this recipe, we will search
    for the optimal policy to play Blackjack, using on-policy Monte Carlo control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Monte Carlo prediction is used to evaluate the value for a given policy, while
    **Monte Carlo control** (**MC control**) is for finding the optimal policy when
    such a policy is not given. There are basically categories of MC control: on-policy
    and off-policy. **On-policy** methods learn about the optimal policy by executing
    the policy and evaluating and improving it, while **off-policy** methods learn
    about the optimal policy using data generated by another policy. The way on-policy
    MC control works is quite similar to policy iteration in dynamic programming,
    which has two phases, evaluation and improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: In the evaluation phase, instead of evaluating the value function (also called
    the **state value**, or **utility**), it evaluates the action-value. The **action-value**
    is more frequently called the **Q-function**, which is the utility of a state-action
    pair *(s, a)* by taking action a in state *s* under a given policy. Again, the
    evaluation can be conducted in a first-visit manner or an every-visit manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the improvement phase, the policy is updated by assigning the optimal action
    to each state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/16d64c6b-4fb4-42bc-974c-09ccb60bc49e.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimal policy will be obtained by alternating two phases for a large number
    of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s search for the optimal Blackjack policy with on-policy MC control by
    taking the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Blackjack instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s develop a function that runs an episode and takes actions under
    a Q-function. This is the improvement phase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we develop the on-policy MC control algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform on-policy MC control to obtain the optimal Q-function and policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate the value function of the optimal policy and print out
    the optimal value as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the value using `plot_blackjack_value` and the `plot_surface` function
    we developed in the previous recipe, *Playing Blackjack with Monte Carlo prediction*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solve the Blackjack game with on-policy MC control by exploring
    starts. This accomplishes our policy optimization goal by alternating between
    evaluation and improvement with each episode we simulate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 2*, we run an episode and take actions under a Q-function by performing
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: We initialize an episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We take a random action as an exploring start.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the first action, we take actions based on the current Q-function, that
    is, [![](img/9a88e66d-d415-43f7-b28d-1d37c5a07c39.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We record the states, actions, and rewards for all steps in the episode, which
    will be used in the evaluation phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that the first action is picked randomly because the
    MC control algorithm will converge to the optimal solution only in such cases.
    Starting an episode with a random action in the MC algorithm is called **exploring
    starts**.
  prefs: []
  type: TYPE_NORMAL
- en: In the exploring starts setting, the first action in an episode is chosen randomly,
    in order to ensure the policy converges to the optimal solution. Otherwise, some
    states are never visited, so their state-action values are never optimized, and
    the policy will become suboptimal in the end.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* is the improvement phase, and *Step 3* is for MC control, where we
    perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q-function with arbitrary small values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `n_episode` episodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, perform policy improvement and obtain the states, actions,
    and rewards; and perform policy evaluation using first-visit MC prediction based
    on the resulting states, actions, and rewards, which updates the Q-function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, the optimal Q-function is finalized and the optimal policy is obtained
    by taking the best action for each state in the optimal Q-function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each iteration, we make the policy greedy by taking the optimal action with
    respect to the current action-value function Q (that is, [![](img/009695d3-ffb3-4839-b30a-abcad4bffb2b.png)]).
    As a result, we will be able to obtain an optimal policy, even though we started
    with an arbitrary policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 5*, you can see the resulting optimal policy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Step 6*, you can see the resulting values for the optimal policy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Step 7*, you will see the resulting value plot for states without a usable
    ace, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dec7a3a-0249-47d3-aabc-885ad9a6112e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the value function for states with a usable ace is visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea892c26-215b-451c-b219-8ab9d39c9dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may want to know whether the optimal policy really works better than the
    simple policy. Let''s now simulate 100,000 Blackjack episodes under the optimal
    policy and the simple policy, respectively. We will compare the chances of winning
    and losing for both policies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the simple policy where the **stick** action is taken when
    the score reaches 18:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a wrapper function that runs one episode under a given policy
    and returns the final reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we specify the number of episodes (100,000), and start the count of wins
    and losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run 100,000 episodes and keep track of the wins and losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the results we get:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Playing under the optimal policy has a 41.28% chance of winning, while playing
    under the simple policy has a 39.92% chance. Then, we have the probability of
    losing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: On the other hand, playing under the optimal policy has a 49.3% chance of losing,
    while playing under the simple policy has a 51.02% chance.
  prefs: []
  type: TYPE_NORMAL
- en: Our optimal policy is clearly the winner!
  prefs: []
  type: TYPE_NORMAL
- en: Developing MC control with epsilon-greedy policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we searched for the optimal policy using MC control
    with greedy search where the action with the highest state-action value was selected.
    However, the best choice available in early episodes does not guarantee an optimal
    solution. If we just focus on what is temporarily the best option and ignore the
    overall problem, we will be stuck in local optima instead of reaching the global
    optima. The workaround is epsilon-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MC control with **epsilon-greedy policy**, we no longer exploit the best
    action all the time, but choose an action randomly under certain probabilities.
    As the name implies, the algorithm has two folds:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Epsilon: given a parameter, ε, with a value from *0* to *1*, each action is
    taken with a probability calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/854959b9-922c-4630-b928-3715ae33f9a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, |A| is the number of possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Greedy: the action with the highest state-action value is favored, and its
    probability of being chosen is increased by *1-ε*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9dae4a2b-4b54-4a48-a4f8-ca0fb3d63cc7.png)'
  prefs: []
  type: TYPE_IMG
- en: Epsilon-greedy policy exploits the best action most of the time and also keeps
    exploring different actions from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s solve the Blackjack environment using epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Blackjack instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s develop a function that runs an episode and performs epsilon-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, develop the on-policy MC control with epsilon-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the discount rate as 1, ε as 0.1, and will use 500,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform MC control with epsilon-greedy policy to obtain the optimal Q-function
    and policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to print out the optimal value and visualize it using `plot_blackjack_value`
    and the `plot_surface` function we developed. We do not repeat the process herein.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we want to know whether the epsilon-greedy method really works better.
    Again, we simulate 100,000 Blackjack episodes under the optimal policy generated
    by epsilon-greedy and compute the chances of winning and losing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, we reuse the `simulate_episode` function from the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solve the Blackjack game with on-policy MC control with epsilon-greedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 2*, we run an episode and perform epsilon-greedy with the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: We initialize an episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We compute the probabilities of choosing individual actions: for the best action
    based on the current Q-function, the probability is [![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png),]
    otherwise, the probability is [![](img/04c14eba-02a1-4015-884c-8544e51c7101.png)].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We record the states, actions, and rewards for all steps in the episode, which
    will be used in the evaluation phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epsilon-greedy outperforms the greedy search method by exploiting the best action
    with a probability of [![](img/f9c9d30b-76c1-4c72-ba9c-8aaa87ee4c28.png)] and
    at the same time, allows the exploration of other actions randomly with a probability
    of [![](img/5d0c99f6-6729-49ea-8cf2-245222bfe187.png)]. The hyperparameter, ε,
    is a trade-off between exploitation and exploration. If its value is 0, the algorithm
    becomes entirely greedy; if the value is *1*, each action is chosen evenly, so
    the algorithm just does random exploration.
  prefs: []
  type: TYPE_NORMAL
- en: The value of ε needs to be tuned based on the experiment, and there is no universal
    value that works best for all experiments. With that being said, in general, we
    can choose 0.1, 0.2, or 0.3 to begin with. Another approach is to start with a
    slightly larger value (such as 0.5 or 0.7) and gradually reduce it over time (for
    example, decaying by 0.999 for each episode). In this way, the policy will focus
    on exploring different actions at the beginning and, over time, it will tend to
    exploit good actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after performing *Step 6*, averaging the results from 100,000 episodes
    and printing the winning probability, we now have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The optimal policy obtained by the epsilon-greedy method has a 42.44% chance
    of winning, which is higher than the chance of winning (41.28%) without epsilon-greedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we also print the losing probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the epsilon-greedy method has a lower chance of losing (48.05%
    versus 49.3% without epsilon-greedy).
  prefs: []
  type: TYPE_NORMAL
- en: Performing off-policy Monte Carlo control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another MC-based approach to solve an MDP is with **off-policy** control, which
    we will discuss in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The **off-policy** method optimizes the **target policy****,** π, using data
    generated by another policy, called the **behavior policy**, b. The target policy
    performs exploitation all the time while the behavior policy is for exploration
    purposes. This means that the target policy is greedy with respect to its current
    Q-function, and the behavior policy generates behavior so that the target policy
    has data to learn from. The behavior policy can be anything as long as all actions
    in all states can be chosen with non-zero probabilities, which guarantees that
    the behavior policy can explore all possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with two different policies in the off-policy method,
    we can only use the **common** steps in episodes that take place in both policies.
    This means that we start with the latest step whose action taken under the behavior
    policy is different from the action taken under the greedy policy. And to learn
    about the target policy with another policy, we use a technique called **importance
    sampling**, which is commonly used to estimate the expected value under a distribution,
    given samples generated from a different distribution. The weighted importance
    for a state-action pair is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfb06509-08d5-4657-8233-a1497c2dd69c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, π (*ak* | *sk*) is the probability of taking action *ak* in state *sk*
    under the target policy; *b*(*ak* | *sk*) is the probability under the behavior
    policy; and the weight, *wt*, is the multiplication of ratios between those two
    probabilities from step *t* to the end of the episode. The weight, *wt*, is applied
    to the return at step *t*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s search for the optimal Blackjack policy with off-policy MC control by
    using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Blackjack instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by defining the behavior policy, which randomly chooses an action
    with the same probability in our case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The behavior policy can be anything as long as it selects all actions in all
    states with non-zero probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s develop a function that runs an episode and takes actions under
    the behavior policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This records the states, actions, and rewards for all of the steps in the episode,
    which will be used as learning data for the target policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll develop the off-policy MC control algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform off-policy MC control with the `random_policy` behavior policy to obtain
    the optimal Q-function and policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we solve the Blackjack game with off-policy MC.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, the off-policy MC control algorithm does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes the Q-function with arbitrary small values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It runs `n_episode` episodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, it performs the behavior policy to generate the states, actions,
    and rewards; it performs policy evaluation on the target policy using first-visit
    MC prediction based on the **common** steps; and it updates the Q-function based
    on the weighted return.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, the optimal Q-function is finalized, and the optimal policy is obtained
    by taking the best action for each state in the optimal Q-function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It learns about the target policy by observing another agent and reusing the
    experience generated from another policy. The target policy is optimized in a
    greedy way, while the behavior policy keeps exploring different options. It averages
    the returns from the behavior policy with the importance ratios of their probabilities
    in the target policy. You may wonder why π (*ak* | *sk*) is always equal to 1
    in the computation of the importance ratio, *wt*. Recall that we only consider
    the common steps taken under the behavior policy and presumably the target policy,
    and the target policy is always greedy. Hence, π (*a* | *s*) = 1 is always true.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can actually implement the MC method in an incremental way. In an episode,
    instead of storing the return and importance ratio for each first-occurring, state-action
    pair, we can calculate the Q-function on the fly. In a non-incremental way, the
    Q-function is computed in the end with all stored returns in n episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e12f8275-764f-4780-b714-1aac61b0c50f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While in the incremental approach, the Q-function is updated in each step of
    an episode as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55912852-d91f-4f99-ab0d-db395b02c49e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The incremental equivalent is more efficient as it reduces memory consumption
    and is more scalable. Let''s go ahead and implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call this incremental version to obtain the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a detailed explanation of importance sampling, the following is a perfect
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf](https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Developing MC control with weighted importance sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we simply averaged the returns from the behavior policy
    with importance ratios of their probabilities in the target policy. This technique
    is formally called **ordinary importance sampling**. It is known to have high
    variance and, therefore, we usually prefer the weighted version of importance
    sampling, which we will talk about in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted importance sampling differs from ordinary importance sampling in the
    way it averages returns. Instead of simply averaging, it takes the weighted average
    of the returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5de98fd1-216b-4c49-a723-60f3b3d4640b.png)'
  prefs: []
  type: TYPE_IMG
- en: It often has a much lower variance compared to the ordinary version. If you
    have experimented with ordinary importance sampling for Blackjack, you will find
    the results vary a lot in each experiment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s solve Blackjack with off-policy MC control with weighted importance
    sampling by using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Blackjack instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by defining the behavior policy, which randomly chooses an action
    with the same probability in our case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Next, we reuse the `run_episode` function, which runs an episode and takes actions
    under the behavior policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we develop the off-policy MC control algorithm with weighted importance
    sampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is an incremental version of MC control.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the discount rate as 1, and will use 500,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform off-policy MC control with the `random_policy` behavior policy to obtain
    the optimal Q-function and policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have solved the Blackjack problem using off-policy MC control with weighted
    importance sampling in this recipe. It is quite similar to ordinary importance
    sampling, but instead of scaling the returns by the ratios and averaging the results,
    it scales the returns using the weighted average. And, in practice, weighted importance
    sampling is of much lower variance than ordinary importance sampling and is therefore
    strongly preferred.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, why don't we simulate some episodes and see what the chances of winning
    and losing will be under the resulting optimal policy?
  prefs: []
  type: TYPE_NORMAL
- en: 'We reuse the `simulate_episode` function we developed in the *Performing on-policy
    Monte Carlo control* recipe and simulate 100,000 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we print out the results we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For proof of the fact that weighted importance sampling outperforms ordinary
    importance sampling, feel free to check out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hesterberg*, *T*. *C*., *Advances in importance sampling*, *Ph*.*D*. *Dissertation*,
    *Statistics Department*, *Stanford University*, 1988'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Casella*, *G*., *Robert*, *C*. *P.,* *Post*-*processing accept*-*reject samples*:
    *recycling and rescaling*. *Journal of Computational and Graphical Statistics*,
    *7*(*2*):*139–157*, *1988*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precup*, *D*., *Sutton*, *R. S*., *Singh*, *S., Eligibility traces for off*-*policy
    policy evaluation*. *In Proceedings of the 17th International Conference on Machine
    Learning*, *pp. 759*–*766*, *2000*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
