["```py\n>>> import gym\n>>> env = gym.envs.make(\"MountainCar-v0\")\n>>> n_action = env.action_space.n\n>>> print(n_action)\n3\n```", "```py\n>>> env.reset()\narray([-0.52354759,  0\\. ])\n```", "```py\n>>> is_done = False\n >>> while not is_done:\n ...     next_state, reward, is_done, info = env.step(2)\n ...     print(next_state, reward, is_done)\n ...     env.render()\n >>> env.render()\n [-0.49286453  0.00077561] -1.0 False\n [-0.4913191   0.00154543] -1.0 False\n [-0.48901538  0.00230371] -1.0 False\n [-0.48597058  0.0030448 ] -1.0 False\n ......\n ......\n [-0.29239555 -0.0046231 ] -1.0 False\n [-0.29761694 -0.00522139] -1.0 False\n [-0.30340632 -0.00578938] -1.0 True\n```", "```py\nenv.close()\n```", "```py\n>>> import torch\n>>> from torch.autograd import Variable\n>>> import math\n```", "```py\n>>> class Estimator():\n ...     def __init__(self, n_feat, n_state, n_action, lr=0.05):\n ...         self.w, self.b = self.get_gaussian_wb(n_feat, n_state)\n ...         self.n_feat = n_feat\n ...         self.models = []\n ...         self.optimizers = []\n ...         self.criterion = torch.nn.MSELoss()\n ...         for _ in range(n_action):\n ...             model = torch.nn.Linear(n_feat, 1)\n ...             self.models.append(model)\n ...             optimizer = torch.optim.SGD(model.parameters(), lr)\n ...             self.optimizers.append(optimizer)\n```", "```py\n>>>     def get_gaussian_wb(self, n_feat, n_state, sigma=.2):\n ...         \"\"\"\n ...         Generate the coefficients of the feature set from \n             Gaussian distribution\n ...         @param n_feat: number of features\n ...         @param n_state: number of states\n ...         @param sigma: kernel parameter\n ...         @return: coefficients of the features\n ...         \"\"\"\n ...         torch.manual_seed(0)\n ...         w = torch.randn((n_state, n_feat)) * 1.0 / sigma\n ...         b = torch.rand(n_feat) * 2.0 * math.pi\n ...         return w, b\n```", "```py\n>>>     def get_feature(self, s):\n ...         \"\"\"\n ...         Generate features based on the input state\n ...         @param s: input state\n ...         @return: features\n ...         \"\"\"\n ...         features = (2.0 / self.n_feat) ** .5 * torch.cos(\n                  torch.matmul(torch.tensor(s).float(), self.w) \n                  + self.b)\n ...         return features\n```", "```py\n>>>     def update(self, s, a, y):\n ...         \"\"\"\n ...         Update the weights for the linear estimator with \n             the given training sample\n ...         @param s: state\n ...         @param a: action\n ...         @param y: target value\n ...         \"\"\"\n ...         features = Variable(self.get_feature(s))\n ...         y_pred = self.models[a](features)\n ...         loss = self.criterion(y_pred, \n                     Variable(torch.Tensor([y])))\n ...         self.optimizers[a].zero_grad()\n ...         loss.backward()\n ...         self.optimizers[a].step()\n```", "```py\n>>>     def predict(self, s):\n ...         \"\"\"\n ...         Compute the Q values of the state using \n                 the learning model\n ...         @param s: input state\n ...         @return: Q values of the state\n ...         \"\"\"\n ...         features = self.get_feature(s)\n ...         with torch.no_grad():\n ...             return torch.tensor([model(features) \n                                     for model in self.models])\n```", "```py\n>>> estimator = Estimator(10, 2, 1)\n```", "```py\n>>> s1 = [0.5, 0.1]\n>>> print(estimator.get_feature(s1))\ntensor([ 0.3163, -0.4467, -0.0450, -0.1490,  0.2393, -0.4181, -0.4426, 0.3074,\n         -0.4451,  0.1808])\n```", "```py\n>>> s_list = [[1, 2], [2, 2], [3, 4], [2, 3], [2, 1]]\n>>> target_list = [1, 1.5, 2, 2, 1.5]\n>>> for s, target in zip(s_list, target_list):\n...     feature = estimator.get_feature(s)\n...     estimator.update(s, 0, target)\n```", "```py\n>>> print(estimator.predict([0.5, 0.1]))\n tensor([0.6172])\n>>> print(estimator.predict([2, 3]))\n tensor([0.8733])\n```", "```py\n>>> import gym\n>>> import torch\n>>> from linear_estimator import Estimator >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n ...     def policy_function(state):\n ...         probs = torch.ones(n_action) * epsilon / n_action\n ...         q_values = estimator.predict(state)\n ...         best_action = torch.argmax(q_values).item()\n ...         probs[best_action] += 1.0 - epsilon\n ...         action = torch.multinomial(probs, 1).item()\n ...         return action\n ...     return policy_function\n```", "```py\n>>> def q_learning(env, estimator, n_episode, gamma=1.0, \n                    epsilon=0.1, epsilon_decay=.99):\n ...     \"\"\"\n ...     Q-Learning algorithm using Function Approximation\n ...     @param env: Gym environment\n ...     @param estimator: Estimator object\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         policy = gen_epsilon_greedy_policy(estimator, \n                   epsilon * epsilon_decay ** episode, n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             q_values_next = estimator.predict(next_state)\n ...             td_target = reward + \n                             gamma * torch.max(q_values_next)\n ...             estimator.update(state, action, td_target)\n ...             total_reward_episode[episode] += reward\n ...\n ...             if is_done:\n ...                 break\n ...             state = next_state\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_feature = 200\n>>> lr = 0.03 >>> estimator = Estimator(n_feature, n_state, n_action, lr)\n```", "```py\n>>> n_episode = 300\n>>> total_reward_episode = [0] * n_episode\n>>> q_learning(env, estimator, n_episode, epsilon=0.1)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import gym\n>>> import torch\n>>> from linear_estimator import Estimator >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> def sarsa(env, estimator, n_episode, gamma=1.0, \n                 epsilon=0.1, epsilon_decay=.99):\n ...     \"\"\"\n ...     SARSA algorithm using Function Approximation\n ...     @param env: Gym environment\n ...     @param estimator: Estimator object\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         policy = gen_epsilon_greedy_policy(estimator, \n                             epsilon * epsilon_decay ** episode,\n                             env.action_space.n)\n ...         state = env.reset()\n ...         action = policy(state)\n ...         is_done = False\n ...\n ...         while not is_done:\n ...             next_state, reward, done, _ = env.step(action)\n ...             q_values_next = estimator.predict(next_state)\n ...             next_action = policy(next_state)\n ...             td_target = reward + \n                             gamma * q_values_next[next_action]\n ...             estimator.update(state, action, td_target)\n ...             total_reward_episode[episode] += reward\n ...\n ...             if done:\n ...                 break\n ...             state = next_state\n ...             action = next_action\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_feature = 200\n>>> lr = 0.03\n>>> estimator = Estimator(n_feature, n_state, n_action, lr)\n```", "```py\n>>> n_episode = 300\n>>> total_reward_episode = [0] * n_episode\n>>> sarsa(env, estimator, n_episode, epsilon=0.1)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import gym\n >>> import torch\n >>> from linear_estimator import Estimator\n >>> from collections import deque\n >>> import random >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_feature = 200\n >>> lr = 0.03\n >>> estimator = Estimator(n_feature, n_state, n_action, lr)\n```", "```py\n>>> memory = deque(maxlen=400)\n```", "```py\n>>> def q_learning(env, estimator, n_episode, replay_size, \n                 gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n ...     \"\"\"\n ...     Q-Learning algorithm using Function Approximation, \n             with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: Estimator object\n ...     @param replay_size: number of samples we use to \n                             update the model each time\n ...     @param n_episode: number of episode\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         policy = gen_epsilon_greedy_policy(estimator, \n                             epsilon * epsilon_decay ** episode,\n                             n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             if is_done:\n ...                 break\n ...\n ...             q_values_next = estimator.predict(next_state)\n ...             td_target = reward + \n                             gamma * torch.max(q_values_next)\n ...             memory.append((state, action, td_target))\n ...             state = next_state\n ...\n ...         replay_data = random.sample(memory, \n                              min(replay_size, len(memory)))\n ...         for state, action, td_target in replay_data:\n ...             estimator.update(state, action, td_target)\n```", "```py\n>>> n_episode = 1000\n```", "```py\n>>> replay_size = 190\n```", "```py\n>>> total_reward_episode = [0] * n_episode\n>>> q_learning(env, estimator, n_episode, replay_size, epsilon=0.1)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> class Estimator():\n ...     def __init__(self, n_feat, n_state, n_action, lr=0.05):\n ...         self.w, self.b = self.get_gaussian_wb(n_feat, n_state)\n ...         self.n_feat = n_feat\n ...         self.models = []\n ...         self.optimizers = []\n ...         self.criterion = torch.nn.MSELoss()\n ...         for _ in range(n_action):\n ...             model = torch.nn.Sequential(\n ...                              torch.nn.Linear(n_feat, n_hidden),\n ...                              torch.nn.ReLU(),\n ...                              torch.nn.Linear(n_hidden, 1)\n ...             )\n ...             self.models.append(model)\n ...             optimizer = torch.optim.Adam(model.parameters(), lr)\n ...             self.optimizers.append(optimizer)\n```", "```py\n>>> import gym\n>>> import torch\n>>> from nn_estimator import Estimator\n>>> from collections import deque\n>>> import random >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_feature = 200\n>>> n_hidden = 50\n>>> lr = 0.001 \n>>> estimator = Estimator(n_feature, n_state, n_action, n_hidden, lr)\n```", "```py\n>>> memory = deque(maxlen=300)\n```", "```py\n>>> n_episode = 1000\n>>> replay_size = 200\n```", "```py\n>>> total_reward_episode = [0] * n_episode\n>>> q_learning(env, estimator, n_episode, replay_size, epsilon=0.1)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import gym\n>>> import torch\n>>> from nn_estimator import Estimator >>> env = gym.envs.make(\"CartPole-v0\")\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_feature = 400\n >>> n_hidden = 100\n >>> lr = 0.01 \n >>> estimator = Estimator(n_feature, n_state, n_action, n_hidden, lr)\n```", "```py\n>>> n_episode = 1000\n >>> total_reward_episode = [0] * n_episode\n >>> q_learning(env, estimator, n_episode, epsilon=0.1)\n```", "```py\n>>> import matplotlib.pyplot as plt\n >>> plt.plot(total_reward_episode)\n >>> plt.title('Episode reward over time')\n >>> plt.xlabel('Episode')\n >>> plt.ylabel('Total reward')\n >>> plt.show()\n```"]