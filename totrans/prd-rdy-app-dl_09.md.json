["```py\nfrom ray import tune\ndef tr_function(conf):\n    num_iterations = conf[\"num_it\"]\n    for i in range(num_iterations):\n        … // training logic\n        tune.report(mean_accuracy=acc)\ntune.run(\n    run_or_experiment=tr_function\n    conf={\"num_it\": tune.grid_search([10, 20, 30, 40])})\n```", "```py\nfrom ray import tune\nfrom ray.tune.suggest.bayesopt import BayesOptSearch\nconf = {\"num_it\": tune.randint(100, 200)}\nbayesopt = BayesOptSearch(metric=\"mean_accuracy\", mode=\"max\")\ntune.run(\n    run_or_experiment=tr_function\n    config = conf,\n    search_alg = bayesopt)\n```", "```py\n# dictionary-based stop\ntune.run(tr_function,\n        stop={\"training_iteration\": 20, \n              \"mean_accuracy\": 0.96})\n# function-based stop\ndef stp_function(trial_id, result):\n    return result[\"training_iteration\"] > 20 or\n           result[\"mean_accuracy\"] > 0.96\ntune.run(tr_function, stop=stp_function)\n```", "```py\n# last reported results\ndf = analysis.results_df\n# list of trials\ntrs = analysis.trials\n# max accuracy \nmax_acc_df = analysis.dataframe(metric=\"mean_accuracy\", mode=\"max\")\n# dict mapping for all trials in the experiment\nall_dfs = analysis.trial_dataframes\n```", "```py\nimport eli5\nfrom eli5.sklearn import PermutationImportance\ndef score(self, x, y_true):\n    y_pred = model.predict(x)\n    return tf.math.sqrt( tf.math.reduce_mean( tf.math.square(y_pred-y_true), axis=-1)) \nperm = PermutationImportance(model, random_state=1, scoring=score).fit(features, labels)\nfi_perm=perm.feature_importances_\nfi_std=perm.feature_importances_std_\n```", "```py\nplt.figure()\nfor index, row in enumerate(fi_perm):\n    plt.bar(index, \n            fi_perm[index], \n            color=\"b\", \n            yerr=fi_std[index], \n            align=\"center\")\nplt.show()\n```", "```py\nimport numpy as np\nfrom eli5.permutation_importance import get_score_importances\n# A trained PyTorch model\nblack_box_model = ...\ndef score(X, y):\n    y_pred = black_box_model.predict(X)\n    return accuracy_score(y, y_pred)\nbase_score, score_decreases = get_score_importances(score, X, y)\nfeature_importances = np.mean(score_decreases, axis=0)\n```", "```py\nimport shap\n# initialize visualization\nshap.initjs()\nmodel = … # tf.keras model or PyTorch model (nn.Module) \nexplainer = shap.KernelExplainer(model, sampled_data)\nshap_values = explainer.shap_values(data, nsamples=300)\nshap.force_plot(explainer.expected_value, shap_values, data)\nshap.summary_plot(shap_values, sampled_data, feature_names=names, plot_type=\"bar\")\n```", "```py\nfrom lime.lime_tabular import LimeTabularExplainer as Lime\nfrom matplotlib import pyplot as plt\nexpl = Lime(features, mode='classification', class_names=[0, 1])\n# explain first sample\nexp = expl.explain_instance(x[0], model.predict, num_features=5, top_labels=1)\n# show plot\nexp.show_in_notebook(show_table=True, show_all=False) \n```"]