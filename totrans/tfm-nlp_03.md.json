["```py\nThe cat sat on it because it was a nice rug. \n```", "```py\nThe cat sat on it<masked sequence> \n```", "```py\nThe cat sat on it because it was a nice rug. \n```", "```py\nThe cat sat on it <masked sequence>. \n```", "```py\nThe cat sat on it [MASK] it was a nice rug. \n```", "```py\n    The cat sat on it [because] it was a nice rug. \n    ```", "```py\n    The cat sat on it [often] it was a nice rug. \n    ```", "```py\n    The cat sat on it [MASK] it was a nice rug. \n    ```", "```py\nThe cat slept on the rug. It likes sleeping all day. \n```", "```py\n[CLS] the cat slept on the rug [SEP] it likes sleep ##ing all day[SEP] \n```", "```py\n#@title Installing the Hugging Face PyTorch Interface for Bert\n!pip install -q transformers \n```", "```py\n#@title Importing the modules\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertConfig\nfrom transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup \n```", "```py\nfrom tqdm import tqdm, trange \n```", "```py\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n% matplotlib inline \n```", "```py\n#@title Harware verification and device attribution\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n!nvidia-smi \n```", "```py\nimport os\n!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/in_domain_train.tsv --output \"in_domain_train.tsv\"\n!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/out_of_domain_dev.tsv --output \"out_of_domain_dev.tsv\" \n```", "```py\n#@title Loading the Dataset\n#source of dataset : https://nyu-mll.github.io/CoLA/\ndf = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\ndf.shape \n```", "```py\n(8551, 4) \n```", "```py\ndf.sample(10) \n```", "```py\n1742 \n```", "```py\nr-67 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\nthey said that tom would n't pay up , but pay… \n```", "```py\n937 \n```", "```py\nbc01 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\nalthough he likes cabbage too , fred likes egg… \n```", "```py\n5655 \n```", "```py\nc_13 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\nwendy 's mother country is iceland . \n```", "```py\n500 \n```", "```py\nbc01 \n```", "```py\n0 \n```", "```py\n* \n```", "```py\njohn is wanted to win . \n```", "```py\n4596 \n```", "```py\nks08 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\ni did n't find any bugs in my bed . \n```", "```py\n7412 \n```", "```py\nsks13 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\nthe girl he met at the departmental party will... \n```", "```py\n8456 \n```", "```py\nad03 \n```", "```py\n0 \n```", "```py\n* \n```", "```py\npeter is the old pigs . \n```", "```py\n744 \n```", "```py\nbc01 \n```", "```py\n0 \n```", "```py\n* \n```", "```py\nfrank promised the men all to leave . \n```", "```py\n5420 \n```", "```py\nb_73 \n```", "```py\n0 \n```", "```py\n* \n```", "```py\ni 've seen as much of a coward as frank . \n```", "```py\n5749 \n```", "```py\nc_13 \n```", "```py\n1 \n```", "```py\nNaN \n```", "```py\nwe drove all the way to buenos aires . \n```", "```py\n#@ Creating sentence, label lists and adding Bert tokens\nsentences = df.sentence.values\n# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values \n```", "```py\n#@title Activating the BERT Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0]) \n```", "```py\nTokenize the first sentence:\n['[CLS]', 'our', 'friends', 'wo', 'n', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]'] \n```", "```py\n#@title Processing the data\n# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 128\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") \n```", "```py\n#@title Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n```", "```py\n#@title Splitting data into train and validation sets\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2018, test_size=0.1) \n```", "```py\n#@title Converting all the data into torch tensors\n# Torch tensors are the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks) \n```", "```py\n#@title Selecting a Batch Size and Creating and Iterator\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size) \n```", "```py\n#@title BERT Model Configuration\n# Initializing a BERT bert-base-uncased style configuration\n#@title Transformer Installation\ntry:\n  import transformers\nexcept:\n  print(\"Installing transformers\")\n  !pip -qq install transformers\n\nfrom transformers import BertModel, BertConfig\nconfiguration = BertConfig()\n# Initializing a model from the bert-base-uncased style configuration\nmodel = BertModel(configuration)\n# Accessing the model configuration\nconfiguration = model.config\nprint(configuration) \n```", "```py\nBertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n} \n```", "```py\n#@title Loading the Hugging Face Bert uncased base model \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel = nn.DataParallel(model)\nmodel.to(device) \n```", "```py\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        ) \n```", "```py\n##@title Optimizer Grouped Parameters\n#This code is taken from:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n# Don't apply weight decay to any parameters whose names include these tokens.\n# (Here, the BERT doesn't have 'gamma' or 'beta' parameters, only 'bias' terms)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.weight']\n# Separate the 'weight' parameters from the 'bias' parameters. \n# - For the 'weight' parameters, this specifies a 'weight_decay_rate' of 0.01\\. \n# - For the 'bias' parameters, the 'weight_decay_rate' is 0.0\\. \noptimizer_grouped_parameters = [\n    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.1},\n\n    # Filter for parameters which *do* include those.\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n# Note - 'optimizer_grouped_parameters' only includes the parameter values, not the names. \n```", "```py\n#@title The Hyperparameters for the Training Loop \noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=2e-5,\n                     warmup=.1) \n```", "```py\n#Creating the Accuracy Measurement Function\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat) \n```", "```py\n#@title The Training Loop\nt = [] \n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 4\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n…./…\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps)) \n```", "```py\n***output***\nEpoch:   0%|          | 0/4 [00:00<?, ?it/s]\nTrain loss: 0.5381132976395461\nEpoch:  25%|██▌       | 1/4 [07:54<23:43, 474.47s/it]\nValidation Accuracy: 0.788966049382716\nTrain loss: 0.315329696132929\nEpoch:  50%|█████     | 2/4 [15:49<15:49, 474.55s/it]\nValidation Accuracy: 0.836033950617284\nTrain loss: 0.1474070605354314\nEpoch:  75%|███████▌  | 3/4 [23:43<07:54, 474.53s/it]\nValidation Accuracy: 0.814429012345679\nTrain loss: 0.07655430570461196\nEpoch: 100%|██████████| 4/4 [31:38<00:00, 474.58s/it]\nValidation Accuracy: 0.810570987654321 \n```", "```py\n#@title Training Evaluation\nplt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show() \n```", "```py\n#@title Predicting and Evaluating Using the Holdout Dataset \ndf = pd.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n# Create sentence and label lists\nsentences = df.sentence.values\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n.../... \n```", "```py\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) \n```", "```py\n # Move logits and labels to CPU\n  logits =  logits['logits'].detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy() \n```", "```py\n # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids) \n```", "```py\n#@title Evaluating Using Matthew's Correlation Coefficient\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import matthews_corrcoef \n```", "```py\nmatthews_set = [] \n```", "```py\nfor i in range(len(true_labels)):\n  matthews = matthews_corrcoef(true_labels[i],\n                 np.argmax(predictions[i], axis=1).flatten())\n  matthews_set.append(matthews) \n```", "```py\n#@title Score of Individual Batches\nmatthews_set \n```", "```py\n[0.049286405809014416,\n -0.2548235957188128,\n 0.4732058754737091,\n 0.30508307783296046,\n 0.3567530340063379,\n 0.8050112948805689,\n 0.23329882422520506,\n 0.47519096331149147,\n 0.4364357804719848,\n 0.4700159919404217,\n 0.7679476477883045,\n 0.8320502943378436,\n 0.5807564950208268,\n 0.5897435897435898,\n 0.38461538461538464,\n 0.5716350506349809,\n 0.0] \n```", "```py\n#@title Matthew's Evaluation on the Whole Dataset\n# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nflat_true_labels = [item for sublist in true_labels for item in sublist]\nmatthews_corrcoef(flat_true_labels, flat_predictions) \n```", "```py\n0.45439842471680725 \n```"]