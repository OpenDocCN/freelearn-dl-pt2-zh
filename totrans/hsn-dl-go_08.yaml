- en: Object Recognition with Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to get to some computer vision or image classification problems
    that are a little more general than our earlier MNIST handwriting example. A lot
    of the same principles apply, but we will be using some new types of operations
    to build **Convolutional Neural** **Networks** (**CNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an example CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the results and making improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are a class of deep neural networks—they are well suited to data with several
    channels and are sensitive to the locality of the information contained within
    the inputs fed into the network. This makes CNNs well suited for tasks associated
    with computer vision such as facial recognition, image classification, scene labeling,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: What is a CNN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs, also known as **ConvNets**, are a class or a category of neural networks
    that are generally accepted to be very good at image classification, that is to
    say, they are very good at distinguishing cats from dogs, cars from planes, and
    many other common classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN typically consists of convolution layers, activation layers, and pooling
    layers. However, it has been structured specifically to take advantage of the
    fact that the inputs are typically images, and take advantage of the fact that
    some parts of the image are very likely to be next to each other.
  prefs: []
  type: TYPE_NORMAL
- en: They are actually fairly similar implementation wise to the feedforward networks
    that we have covered in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Normal feedforward versus ConvNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, a neural network receives a single vector as input (such as our
    MNIST example in [Chapter 3](200c9784-4718-47d4-84ce-95e41854a151.xhtml), *Beyond
    Basic Neural Networks – Autoencoders and RBMs*) and then goes through several
    hidden layers, before arriving at the end with our inference for the result. This
    is fine for images that aren't that big; when our images become larger, however,
    as they usually are in most real-life applications, we want to ensure that we
    aren't building immensely large hidden layers to process them correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, one of the convenient features that is present in our ideas with
    tensors is the fact that we don''t actually have to feed a vector into the model;
    we can feed something a little more complicated and with more dimensions. Basically,
    what we want to do with a CNN is that we want to have neurons arranged in three
    dimensions: height, width, and depth—what we mean by depth here is the number
    of colors in our color system, in our case being red, green, and blue.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of trying to connect every neuron in a layer together, we will try to
    reduce it so that it is more manageable and less likely to be overfitted for our
    sample size, as we won't be trying to train every single pixel of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, CNNs use layers, and we will need to talk about some of these layers
    in more detail, because we haven''t discussed them yet; in general, there are
    three main layers in a CNN: convolutional layers, pooling layers, and fully connected
    layers (these are the ones you''ve already seen).'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional layers are part of the name of this neural network and form a
    very important part of the neural network architecture. It can be broadly explained
    as scanning across the image to find certain features. We create a small filter,
    which we then slide across the entire image according to our desired stride.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, the first cell of the output would be calculated by finding
    the **Dot Product** of our 3 x 3 filter with the top-left corner of our **Image**,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91d32a0-1c57-4c34-b655-ec010737ac00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if your stride was one, it would shift one column right and continue, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc423070-20d8-4603-859f-3818d9aacb32.png)'
  prefs: []
  type: TYPE_IMG
- en: This would then continue until we had our entire output.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling layers are commonly put in between convolutional layers; they are meant
    to reduce the volume of data being passed around, therefore reducing the number
    of parameters, as well as reducing the amount of computation required by the network.
    In this case, we are *pooling* numbers together by taking the maximum over a given
    region of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: These layers also work similarly to the convolutional layers; they apply on
    a predetermined grid and perform the pooling operation. In this case, it is the
    maximum operation, so it will take the highest value within the grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in a max pooling operation on a 2 x 2 grid, the first cell of
    output will come from the top left, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f819e869-6747-45e1-91de-27b878f7ef45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And with a stride of two, the second will come from the grid shifted right
    two rows, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/911e2c7d-0498-4763-9c0a-828918037283.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you understand the layers, let''s talk about the basic structure of
    a CNN. A CNN consists broadly of the following: an input layer, and then several
    layers of convolutional layers, activation layers, and pooling layers, before
    ending in a fully connected layer at the end to get to our final results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic structure looks a little like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a840475f-4055-40b9-b2ef-34e9126359a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Building an example CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate how a CNN works in practice, we will be building a model to recognize
    whether an object in a photo is a cat or not. The dataset we are using has more
    depth than this, but it would take a rather long time to train it to correctly
    classify everything. It is fairly trivial to extend the example to classify everything,
    but we would rather not be sitting there for a week waiting for the model to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will be using the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1773604f-ee5a-4e45-a98e-1931d0252d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: CIFAR-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using CIFAR-10 for our example this time instead of MNIST. As such, we
    do not have the convenience of using the already convenient MNIST loader. Let's
    quickly go through what it takes to load this new dataset!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the binary format for CIFAR-10, which you can download here:
    [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset was put together by Alex Krizhevsky, Vinod Nair, and Geoffrey
    Hinton. It consists of 60,000 tiny images 32 pixels high by 32 pixels wide. The
    binary format of CIFAR-10 is laid out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It should be noted that it is not delimited or does not have any other information
    for validation of the file; as such, you should ensure that the MD5 checksum for
    the file that you have downloaded matches that on the website. As the structure
    is relatively simple, we can just pull the binary file straight into Go and parse
    it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The 3,072 pixels are actually three layers of red, green, and blue values from
    0 to 255, over a 32 x 32 grid in row-major order, so this gives us our image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The label is a number from **0** to **9**, representing one of the following
    categories respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dbee2fd-efdf-4ec2-815f-7db63d1ab4eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'CIFAR-10 comes in six files, five training set files of 10,000 images each
    and one test set file of 10,000 images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Importing this in Go is easy—open the file and read the raw bytes. As every
    single underlying value is an 8-bit integer within a single byte, we can just
    cast it to whatever we want. If you wanted the single integer values, you could
    just convert them all into unsigned 8-bit integers; this is useful for when you
    want to convert the data into an image. You''ll find, however, we''ve made some
    slightly different decisions in the code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are interested in using this data for our deep learning algorithm, it
    is prudent to not stray too far from our happy medium between `0` and `1`. We''re
    reusing pixel weight from the MNIST example, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will convert all our pixel values from 0 to 255 to a range between `0.1`
    and `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for our labels, we will be using one-hot encoding again, encoding
    the desired label at `0.9` and everything else at `0.1`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve packaged this into a convenient `Load` function so we can call it from
    our code. It''ll return two conveniently shaped tensors for us to work with. This
    gives us a function that can import both the train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to load the data in my `main` by calling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Epochs and batch size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll choose `10` epochs for this example so that the code can be trained
    in less than an hour. It should be noted that 10 epochs will only get us to around
    20% accuracy, so do not be alarmed if you find the resulting model does not appear
    accurate; you will need to train it for much longer, maybe even around 1,000 epochs.
    On a modern computer, an epoch takes around three minutes to complete; for the
    sake of not requiring three days to complete this example, we''ve chosen to abbreviate
    the training process and will leave it as an exercise to assess the results of
    more epochs, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that this model will consume a fairly large amount of memory; a `batchsize`
    of `100` can still mean you will need around 4 GB of memory. If you don't have
    this amount available without resorting to swapping memory, you may want to lower
    the batch size to make the code perform better on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As this model takes much longer to converge, we should also add a rudimentary
    metric to track our accuracy. In order to do this, we must first extract our labels
    from the data - which we can do as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We must then get our prediction from the output data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can then use this to update our accuracy metric. The amount by which it is
    updated is scaled by the number of examples - so that our output will be a percentage
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This give us a broad *accuracy* metric that we can use to gauge our training
    progress.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can think of our layer structure having four parts. We are going to have
    three convolutional layers and one fully connected layer. Our first two layers
    are extremely similar - they follow the convolution-ReLU-MaxPool-dropout structure
    that we''ve described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our following layer is similar - we just need to join it to the output of our
    previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following layer is essentially the same, but there is a slight change to
    prepare it for the change to the fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Layer 3` is something we''re already very familiar with—the fully connected
    layer—here, we have a fairly simple structure. We can certainly add more tiers
    to this layer (and this has been done by many different architectures before as
    well, with differing levels of success). This layer is demonstrated in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Loss function and solver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the ordinary cross-entropy loss function here, which can be
    implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Together with that, we will be using the Gorgonia tape machine and the RMSprop
    solver, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Test set output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the end of our training, we should pit our model against the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we should import our test data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need recalculate our batches as the test set is sized differently
    from the train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to just add a quick way to track our results and output our results
    for later inspection by inserting the following code into the accuracy metric
    calculation code described earlier in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, at the end of our run through the entire test set - write the
    data out to text files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let's now assess the results.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, the example trained over 10 epochs is not particularly
    accurate. You will need to train it over many epochs to get better results. If
    you have been watching the cost and accuracy of the model, you''ll find that cost
    will stay relatively flat as accuracy increased over the number of epochs, as
    shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56e90fa4-1342-4b15-8d55-a1287eb2cba9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is still useful to explore the results to see how the model is performing;
    we''ll specifically look at cats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/235068fc-b8a5-4209-aa8c-ebef1b32bb49.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, it currently appears to do much better with cats in very specific
    positions. Obviously, we need to find a solution to train it faster.
  prefs: []
  type: TYPE_NORMAL
- en: GPU acceleration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolution and its associated operations tend to do very well on GPU acceleration.
    You saw earlier that our GPU acceleration had minimal impact, but it is extremely
    useful for building CNNs. All we need to do is add the magical `''cuda''` build
    tag, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As we tend to be more memory constrained on GPUs, be aware that the same batch
    size may not work on your GPU. The model as mentioned previously uses around 4
    GB of memory, so you will probably want to reduce the batch size if you have less
    than 6 GB of GPU memory (because presumably, you will be using about 1 GB for
    your normal desktop). If your model is running very slowly, or the CUDA version
    of your executable just fails, it would be prudent to check if being out of memory
    is the issue. You can do this using the NVIDIA SMI utility and getting it to check
    your memory every second, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will tend to produce the following report every second; watching it while
    your code runs will tell you broadly how much GPU memory your code is consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c24d016-134d-4fe8-b69c-397847f0af81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s quickly compare the performance between CPU and GPU versions of our
    code. The CPU version takes broadly around three minutes per epoch, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The GPU version takes around two minutes thirty seconds per epoch, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A future version of Gorgonia will also include support for better operations;
    this is currently in testing, and you can use it by importing `gorgonia.org/gorgonia/ops/nn`
    and replacing your `Conv2d`, `Rectify`, `MaxPool2D`, and `Dropout` calls from
    their Gorgonia versions with their `nnops` version, An example of a slightly different
    `Layer 0` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, replace all the necessary operations and run it to see how it
    is different.
  prefs: []
  type: TYPE_NORMAL
- en: CNN weaknesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs actually have a fairly major weakness: they are not orientation invariant,
    which means that if you were to feed the same image in, but upside down, the network
    is likely to not recognize it at all. One of the ways we can ensure this is not
    the case is to train the model with different rotations; however, there are better
    architectures that can solve this problem, which we will discuss later in this
    book.'
  prefs: []
  type: TYPE_NORMAL
- en: They are also not scale invariant. Feeding it the same image much smaller or
    much larger makes it likely to fail. If you think back to why this is the case,
    it's because we are building the model based on a filter of a very specific size
    on a very specific group of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: You have also seen that the model is very slow to train in general, especially
    on the CPU. We can get around this somewhat by using the GPU instead, but overall,
    it is an expensive process and can take several days to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have now learned how to build a CNN and how to tune some of the hyperparameters
    (such as the number of epochs and batch sizes) in order to get the desired result
    and get it running smoothly on different computers.
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, you should try training this model to recognize MNIST digits,
    and even change around the structure of the convolutional layers; try Batch Normalization,
    and perhaps even more weights in the fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will give an introduction to reinforcement learning and Q-learning
    and how to build a DQN and solve a maze.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Character-Level Convolutional Networks for Text Classification* by *Xiang
    Zhang, Junbo Zhao* and *Yann LeCun*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U-Net: Convolutional Networks for Biomedical Image Segmentation* by *Olaf
    Ronneberger*, *Philipp Fischer*, and *Thomas Brox*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*
    by *Shaoqing Ren*, *Kaiming He*, *Ross Girshick*, and *Jian Sun*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Long-term Recurrent Convolutional Networks for Visual Recognition and Description*
    by *Jeff Donahue*, *Lisa Anne Hendricks*, *Marcus Rohrbach*, *Subhashini Venugopalan*,
    *Sergio Guadarrama*, *Kate Saenko*, and *Trevor Darrell*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
