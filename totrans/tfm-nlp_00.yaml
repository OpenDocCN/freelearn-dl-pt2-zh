- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Transformers are a game-changer for **Natural Language Understanding** (**NLU**),
    a subset of **Natural Language Processing** (**NLP**), which has become one of
    the pillars of artificial intelligence in a global digital economy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器对于**自然语言理解**（**NLU**）的重要性不言而喻，NLU是**自然语言处理**（**NLP**）的一个子集，在全球数字经济中已经成为人工智能的支柱之一。
- en: Transformer models mark the beginning of a new era in artificial intelligence.
    Language understanding has become the pillar of language modeling, chatbots, personal
    assistants, question answering, text summarizing, speech-to-text, sentiment analysis,
    machine translation, and more. We are witnessing the expansion of social networks
    versus physical encounters, e-commerce versus physical shopping, digital newspapers,
    streaming versus physical theaters, remote doctor consultations versus physical
    visits, remote work instead of on-site tasks, and similar trends in hundreds of
    more domains. It would be incredibly difficult for society to use web browsers,
    streaming services, and any digital activity involving language without AI language
    understanding. The paradigm shift of our societies from physical to massive digital
    information forced artificial intelligence into a new era. Artificial intelligence
    has evolved to billion-parameter models to face the challenge of trillion-word
    datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型标志着人工智能的新时代的开始。语言理解已成为语言建模、聊天机器人、个人助手、问答、文本摘要、语音转文本、情感分析、机器翻译等领域的支柱。我们正在见证社交网络与面对面的社交相比的扩张，电子商务与实体购物的竞争，数字新闻与传统媒体的竞争，流媒体与实体剧院的竞争，远程医生咨询与实体就诊的竞争，远程工作与现场任务的竞争以及数百个领域中类似的趋势。如果没有AI语言理解，社会将难以使用网络浏览器、流媒体服务以及任何涉及语言的数字活动。我们社会从物理信息到大规模数字信息的范式转变迫使人工智能进入了一个新时代。人工智能已经演化到了亿级参数模型来应对万亿字数据集的挑战。
- en: The Transformer architecture is both revolutionary and disruptive. It breaks
    with the past, leaving the dominance of RNNs and CNNs behind. BERT and GPT models
    abandoned recurrent network layers and replaced them with self-attention. Transformer
    models outperform RNNs and CNNs. The 2020s are experiencing a major change in
    AI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构既具有革命性又具有颠覆性。它打破了过去，摆脱了RNN和CNN的主导地位。 BERT和GPT模型放弃了循环网络层，并用自注意力替换它们。变压器模型胜过了RNN和CNN。
    2020年代正在经历人工智能的重大变革。
- en: Transformer encoders and decoders contain attention heads that train separately,
    parallelizing cutting-edge hardware. Attention heads can run on separate GPUs
    opening the door to billion-parameter models and soon-to-come trillion-parameter
    models. OpenAI trained a 175 billion parameter GPT-3 Transformer model on a supercomputer
    with 10,000 GPUs and 285,000 CPU cores.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器编码器和解码器包含可以单独训练的注意力头，可以并行化最先进的硬件。注意力头可以在单独的GPU上运行，为亿级参数模型和即将推出的万亿级参数模型敞开大门。OpenAI在拥有10000个GPU和285000个CPU核心的超级计算机上训练了一个有1750亿参数的GPT-3变压器模型。
- en: The increasing amount of data requires training AI models at scale. As such,
    transformers pave the way to a new era of parameter-driven AI. Learning to understand
    how hundreds of millions of words fit together in sentences requires a tremendous
    amount of parameters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量的增加要求以规模训练AI模型。因此，变压器为参数驱动AI开辟了新的时代。学会理解数亿个单词在句子中如何组合需要大量参数。
- en: Transformer models such as Google BERT and OpenAI GPT-3 have taken emergence
    to another level. Transformers can perform hundreds of NLP tasks they were not
    trained for.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌BERT和OpenAI GPT-3等变压器模型已经将新兴技术推上了新的高度。变压器可以执行它们未经过训练的数百种NLP任务。
- en: Transformers can also learn image classification and reconstruction by embedding
    images as sequences of words. This book will introduce you to cutting-edge computer
    vision transformers such as **Vision Transformers** (**ViT**), CLIP, and DALL-E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器还可以通过将图像嵌入为单词序列来学习图像分类和重建。本书将向您介绍最前沿的计算机视觉变压器，如**视觉变压器**（**ViT**），CLIP和DALL-E。
- en: Foundation models are fully trained transformer models that can carry out hundreds
    of tasks without fine-tuning. Foundation models at this scale offer the tools
    we need in this massive information era.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是完全训练的变压器模型，可以在不进行微调的情况下执行数百种任务。在这个大规模信息时代，基础模型提供了我们所需要的工具。
- en: Think of how many humans it would take to control the content of the billions
    of messages posted on social networks per day to decide if they are legal and
    ethical before extracting the information they contain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Think of how many humans would be required to translate the millions of pages
    published each day on the web. Or imagine how many people it would take to manually
    control the millions of messages made per minute!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Finally, think of how many humans it would take to write the transcripts of
    all of the vast amount of hours of streaming published per day on the web. Finally,
    think about the human resources required to replace AI image captioning for the
    billions of images that continuously appear online.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: This book will take you from developing code to prompt design, a new “programming”
    skill that controls the behavior of a transformer model. Each chapter will take
    you through the key aspects of language understanding from scratch in Python,
    PyTorch, and TensorFlow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the architecture of the original Transformer, Google BERT, OpenAI
    GPT-3, T5, and several other models. You will fine-tune transformers, train models
    from scratch, and learn to use powerful APIs. Facebook, Google, Microsoft, and
    other big tech corporations share large datasets for us to explore.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: You will keep close to the market and its demand for language understanding
    in many fields such as media, social media, and research papers, for example.
    Among hundreds of AI tasks, we need to summarize the vast amounts of data for
    research, translate documents for every area of our economy, and scan all social
    media posts for ethical and legal reasons.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, you will work hands-on with Python, PyTorch, and TensorFlow.
    You will be introduced to the key AI language understanding neural network models.
    You will then learn how to explore and implement transformers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the new skills required to become an Industry 4.0 AI Specialist
    in this disruptive AI era. The book aims to give readers the knowledge and tools
    for Python deep learning needed for effectively developing the key aspects of
    language understanding.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is not an introduction to Python programming or machine learning concepts.
    Instead, it focuses on deep learning for machine translations, speech-to-text,
    text-to-speech, language modeling, question answering, and many more NLP domains.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Readers who can benefit the most from this book are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and NLP practitioners who are familiar with Python programming.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysts and data scientists who want an introduction to AI language understanding
    to process the increasing amounts of language-driven functions.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What this book covers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Part I: Introduction to Transformer Architectures**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 1*, *What are Transformers?*, explains, at a high level, what transformers
    are. We’ll look at the transformer ecosystem and the properties of foundation
    models. The chapter highlights many of the platforms available and the evolution
    of Industry 4.0 AI specialists.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 1 章*，*什么是 Transformer？*，在高层次上解释了 Transformer 是什么。我们将看看 Transformer 生态系统以及基础模型的特性。本章突出了许多可用的平台以及工业
    4.0 AI 专家的演变。'
- en: '*Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    goes through the background of NLP to understand how RNN, LSTM, and CNN deep learning
    architectures evolved into the Transformer architecture that opened a new era.
    We will go through the Transformer’s architecture through the unique *Attention
    Is All You Need* approach invented by the Google Research and Google Brain authors.
    We will describe the theory of transformers. We will get our hands dirty in Python
    to see how the multi-attention head sub-layers work. By the end of this chapter,
    you will have understood the original architecture of the Transformer. You will
    be ready to explore the multiple variants and usages of the Transformer in the
    following chapters.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 2 章*，*了解 Transformer 模型的架构初步*，通过 NLP 的背景来理解 RNN、LSTM 和 CNN 深度学习架构是如何演变成打开新时代的
    Transformer 架构的。我们将通过 Google Research 和 Google Brain 的作者们提出的独特 *注意力就是一切* 方法来详细了解
    Transformer 的架构。我们将描述变压器的理论。我们将用 Python 亲自动手来看看多头注意力子层是如何工作的。在本章结束时，你将了解到 Transformer
    的原始架构。你将准备好在接下来的章节里探索 Transformer 的多种变体和用法。'
- en: '*Chapter 3*, *Fine-Tuning BERT Models*, builds on the architecture of the original
    Transformer. **Bidirectional Encoder Representations from Transformers** (**BERT**)
    shows you a new way of perceiving the world of NLP. Instead of analyzing a past
    sequence to predict a future sequence, BERT attends to the whole sequence! We
    will first go through the key innovations of BERT’s architecture and then fine-tune
    a BERT model by going through each step in a Google Colaboratory notebook. Like
    humans, BERT can learn tasks and perform other new ones without having to learn
    the topic from scratch.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 3 章*，*微调 BERT 模型*，在原始 Transformer 的架构基础上构建。**来自变压器的双向编码表示**（**BERT**）向我们展示了一个感知
    NLP 世界的新方式。BERT 不是分析过去的序列以预测未来的序列，而是关注整个序列！我们将首先了解 BERT 架构的关键创新，然后通过在 Google Colaboratory
    笔记本中逐步微调 BERT 模型。像人类一样，BERT 可以学习任务，并且执行其他新任务而无需从头学习话题。'
- en: '*Chapter 4*, *Pretraining a RoBERTa Model from Scratch*, builds a RoBERTa transformer
    model from scratch using the Hugging Face PyTorch modules. The transformer will
    be both BERT-like and DistilBERT-like. First, we will train a tokenizer from scratch
    on a customized dataset. The trained transformer will then run on a downstream
    masked language modeling task.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 4 章*，*从头开始预训练 RoBERTa 模型*，使用 Hugging Face PyTorch 模块从头构建一个 RoBERTa 变压器模型。
    这个变压器模型将类似于 BERT 和 DistilBERT。首先，我们将在一个定制的数据集上从头开始训练一个分词器。然后，训练好的变压器模型将在一个下游掩码语言建模任务中运行。'
- en: '**Part II: Applying Transformers for Natural Language Understanding and Generation**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 II 部分：将 Transformer 应用于自然语言理解和生成**'
- en: '*Chapter 5*, *Downstream NLP Tasks with Transformers*, reveals the magic of
    transformer models with downstream NLP tasks. A pretrained transformer model can
    be fine-tuned to solve a range of NLP tasks such as BoolQ, CB, MultiRC, RTE, WiC,
    and more, dominating the GLUE and SuperGLUE leaderboards. We will go through the
    evaluation process of transformers, the tasks, datasets, and metrics. We will
    then run some of the downstream tasks with Hugging Face’s pipeline of transformers.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 5 章*，*使用 Transformer 进行下游 NLP 任务*，展示了使用下游 NLP 任务的魔力。一个预训练的变压器模型可以被微调以解决一系列
    NLP 任务，如 BoolQ、CB、MultiRC、RTE、WiC 等，在 GLUE 和 SuperGLUE 排行榜上占据主导地位。我们将介绍变压器的评估过程、任务、数据集和指标。然后我们将使用
    Hugging Face 的变压器管道运行一些下游任务。'
- en: '*Chapter 6*, *Machine Translation with the Transformer*, defines machine translation
    to understand how to go from human baselines to machine transduction methods.
    We will then preprocess a WMT French-English dataset from the European Parliament.
    Machine translation requires precise evaluation methods, and in this chapter,
    we explore the BLEU scoring method. Finally, we will implement a Transformer machine
    translation model with Trax.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 6 章*，*使用 Transformer 进行机器翻译*，定义机器翻译以了解如何从人类标准到机器传导方法。然后，我们将预处理来自欧洲议会的 WMT
    法英数据集。机器翻译需要精确的评估方法，在本章中，我们将探讨 BLEU 评分方法。最后，我们将使用 Trax 实现一个 Transformer 机器翻译模型。'
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, explores
    many aspects of OpenAI’s GPT-2 and GPT-3 transformers. We will first examine the
    architecture of OpenAI’s GPT models before explaining the different GPT-3 engines.
    Then we will run a GPT-2 345M parameter model and interact with it to generate
    text. Next, we’ll see the GPT-3 playground in action before coding a GPT-3 model
    for NLP tasks and comparing the results to GPT-2.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Applying Transformers to Legal and Financial Documents for AI
    Text Summarization*, goes through the concepts and architecture of the T5 transformer
    model. We will initialize a T5 model from Hugging Face to summarize documents.
    We will task the T5 model to summarize various documents, including a sample from
    the *Bill of Rights*, exploring the successes and limitations of transfer learning
    approaches applied to transformers. Finally, we will use GPT-3 to summarize some
    corporation law text to a second-grader.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9*, *Matching Tokenizers and Datasets*, analyzes the limits of tokenizers
    and looks at some of the methods applied to improve the data encoding process’s
    quality. We will first build a Python program to investigate why some words are
    omitted or misinterpreted by word2vector tokenizers. Following this, we find the
    limits of pretrained tokenizers with a tokenizer-agonistic method.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: We will improve a T5 summary by applying some of the ideas that show that there
    is still much room left to improve the methodology of the tokenization process.
    Finally, we will test the limits of GPT-3’s language understanding.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Semantic Role Labeling with BERT-Based Transformers*, explores
    how transformers learn to understand a text’s content. **Semantic Role Labeling**
    (**SRL**) is a challenging exercise for a human. Transformers can produce surprising
    results. We will implement a BERT-based transformer model designed by the Allen
    Institute for AI in a Google Colab notebook. We will also use their online resources
    to visualize SRL outputs. Finally, we will question the scope of SRL and understand
    the reasons behind its limitations.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**Part III: Advanced Language Understanding Techniques**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 11*, *Let Your Data Do the Talking: Story, Questions, and Answers*,
    shows how a transformer can learn how to reason. A transformer must be able to
    understand a text, a story, and also display reasoning skills. We will see how
    question answering can be enhanced by adding NER and SRL to the process. We will
    build the blueprint for a question generator that can be used to train transformers
    or as a stand-alone solution.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 12*, *Detecting Customer Emotions to Make Predictions*, shows how
    transformers have improved sentiment analysis. We will analyze complex sentences
    using the Stanford Sentiment Treebank, challenging several transformer models
    to understand not only the structure of a sequence but also its logical form.
    We will see how to use transformers to make predictions that trigger different
    actions depending on the sentiment analysis output. The chapter finishes with
    some edge cases using GPT-3.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 13*, *Analyzing Fake News with Transformers*, delves into the hot
    topic of fake news and how transformers can help us understand the different perspectives
    of the online content we see each day. Every day, billions of messages, posts,
    and articles are published on the web through social media, websites, and every
    form of real-time communication available. Using several techniques from the previous
    chapters, we will analyze debates on climate change and gun control and the Tweets
    from a former president. We will go through the moral and ethical problem of determining
    what can be considered fake news beyond reasonable doubt and what news remains
    subjective.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 14*, *Interpreting Black Box Transformer Models*, lifts the lid on
    the black box that is transformer models by visualizing their activity. We will
    use BertViz to visualize attention heads and **Language Interpretability Tool**
    (**LIT**) to carry out a **principal component analysis** (**PCA**). Finally,
    we will use LIME to visualize transformers via dictionary learning.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 15*, *From NLP to Task-Agnostic Transformer Models*, delves into the
    advanced models, Reformer and DeBERTa, running examples using Hugging Face. Transformers
    can process images as sequences of words. We will also look at different vision
    transformers such as ViT, CLIP, and DALL-E. We will test them on computer vision
    tasks, including generating computer images.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 16*, *The Emergence of Transformer-Driven Copilots*, explores the
    maturity of Industry 4.0\. The chapter begins with prompt engineering examples
    using informal/casual English. Next, we will use GitHub Copilot and OpenAI Codex
    to create code from a few lines of instructions. We will see that vision transformers
    can help NLP transformers visualize the world around them. We will create a transformer-based
    recommendation system, which can be used by digital humans in whatever metaverse
    you may end up in!'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix I*, *Terminology of Transformer Models*, examines the high-level
    structure of a transformer, from stacks and sublayers to attention heads.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix II*, *Hardware Constraints for Transformer Models*, looks at CPU
    and GPU performance running transformers. We will see why transformers and GPUs
    and transformers are a perfect match, concluding with a test using Google Colab
    CPU, Google Colab Free GPU, and Google Colab Pro GPU.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix III*, *Generic Text Completion with GPT-2*, provides a detailed explanation
    of generic text completion using GPT-2 from *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix IV*, *Custom Text Completion with GPT-2*, supplements *Chapter 7*,
    *The Rise of Suprahuman Transformers with GPT-3 Engines* by building and training
    a GPT-2 model and making it interact with custom text.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix V*, *Answers to the Questions*, provides answers to the questions
    at the end of each chapter.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the programs in the book are Colaboratory notebooks. All you will need
    is a free Google Gmail account, and you will be able to run the notebooks on Google
    Colaboratory’s free VM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: You will need Python installed on your machine for some of the educational programs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Take the necessary time to read *Chapter 2*, *Getting Started with the Architecture
    of the Transformer Model* and *Appendix I*, *Terminology of Transformer Models*.
    *Chapter 2* contains the description of the original Transformer, which is built
    from building blocks explained in *Appendix I*, *Terminology of Transformer Models*,
    that will be implemented throughout the book. If you find it difficult, then pick
    up the general intuitive ideas out of the chapter. You can then go back to these
    chapters when you feel more comfortable with transformers after a few chapters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: After reading each chapter, consider how you could implement transformers for
    your customers or use them to move up in your career with novel ideas.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we use OpenAI Codex later on in the book, which currently has
    a waiting list. Sign up now to avoid a long wait time at [https://openai.com/blog/openai-codex/](https://openai.com/blog/openai-codex/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code bundle for the book is hosted on GitHub at [https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that contains color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several text conventions used throughout this book.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates sentences and words run through the models in the book,
    code words in text, database table names, folder names, filenames, file extensions,
    pathnames, dummy URLs, user input, and Twitter handles. For example, “However,
    if you wish to explore the code, you will find it in the Google Colaboratory `positional_encoding.ipynb`
    notebook and the `text.txt` file in this chapter’s GitHub repository.”'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: “In our case, we are looking for **t5-large**, a t5-large model
    we can smoothly run in Google Colaboratory.”'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: Email `feedback@packtpub.com` and mention the book’s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit, [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Transformers for Natural Language Processing - Second Edition*,
    we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1803247339) for this book and share your feedback.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
