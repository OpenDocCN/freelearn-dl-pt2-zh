- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers are a game-changer for **Natural Language Understanding** (**NLU**),
    a subset of **Natural Language Processing** (**NLP**), which has become one of
    the pillars of artificial intelligence in a global digital economy.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models mark the beginning of a new era in artificial intelligence.
    Language understanding has become the pillar of language modeling, chatbots, personal
    assistants, question answering, text summarizing, speech-to-text, sentiment analysis,
    machine translation, and more. We are witnessing the expansion of social networks
    versus physical encounters, e-commerce versus physical shopping, digital newspapers,
    streaming versus physical theaters, remote doctor consultations versus physical
    visits, remote work instead of on-site tasks, and similar trends in hundreds of
    more domains. It would be incredibly difficult for society to use web browsers,
    streaming services, and any digital activity involving language without AI language
    understanding. The paradigm shift of our societies from physical to massive digital
    information forced artificial intelligence into a new era. Artificial intelligence
    has evolved to billion-parameter models to face the challenge of trillion-word
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture is both revolutionary and disruptive. It breaks
    with the past, leaving the dominance of RNNs and CNNs behind. BERT and GPT models
    abandoned recurrent network layers and replaced them with self-attention. Transformer
    models outperform RNNs and CNNs. The 2020s are experiencing a major change in
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer encoders and decoders contain attention heads that train separately,
    parallelizing cutting-edge hardware. Attention heads can run on separate GPUs
    opening the door to billion-parameter models and soon-to-come trillion-parameter
    models. OpenAI trained a 175 billion parameter GPT-3 Transformer model on a supercomputer
    with 10,000 GPUs and 285,000 CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: The increasing amount of data requires training AI models at scale. As such,
    transformers pave the way to a new era of parameter-driven AI. Learning to understand
    how hundreds of millions of words fit together in sentences requires a tremendous
    amount of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models such as Google BERT and OpenAI GPT-3 have taken emergence
    to another level. Transformers can perform hundreds of NLP tasks they were not
    trained for.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers can also learn image classification and reconstruction by embedding
    images as sequences of words. This book will introduce you to cutting-edge computer
    vision transformers such as **Vision Transformers** (**ViT**), CLIP, and DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models are fully trained transformer models that can carry out hundreds
    of tasks without fine-tuning. Foundation models at this scale offer the tools
    we need in this massive information era.
  prefs: []
  type: TYPE_NORMAL
- en: Think of how many humans it would take to control the content of the billions
    of messages posted on social networks per day to decide if they are legal and
    ethical before extracting the information they contain.
  prefs: []
  type: TYPE_NORMAL
- en: Think of how many humans would be required to translate the millions of pages
    published each day on the web. Or imagine how many people it would take to manually
    control the millions of messages made per minute!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, think of how many humans it would take to write the transcripts of
    all of the vast amount of hours of streaming published per day on the web. Finally,
    think about the human resources required to replace AI image captioning for the
    billions of images that continuously appear online.
  prefs: []
  type: TYPE_NORMAL
- en: This book will take you from developing code to prompt design, a new “programming”
    skill that controls the behavior of a transformer model. Each chapter will take
    you through the key aspects of language understanding from scratch in Python,
    PyTorch, and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the architecture of the original Transformer, Google BERT, OpenAI
    GPT-3, T5, and several other models. You will fine-tune transformers, train models
    from scratch, and learn to use powerful APIs. Facebook, Google, Microsoft, and
    other big tech corporations share large datasets for us to explore.
  prefs: []
  type: TYPE_NORMAL
- en: You will keep close to the market and its demand for language understanding
    in many fields such as media, social media, and research papers, for example.
    Among hundreds of AI tasks, we need to summarize the vast amounts of data for
    research, translate documents for every area of our economy, and scan all social
    media posts for ethical and legal reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, you will work hands-on with Python, PyTorch, and TensorFlow.
    You will be introduced to the key AI language understanding neural network models.
    You will then learn how to explore and implement transformers.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the new skills required to become an Industry 4.0 AI Specialist
    in this disruptive AI era. The book aims to give readers the knowledge and tools
    for Python deep learning needed for effectively developing the key aspects of
    language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is not an introduction to Python programming or machine learning concepts.
    Instead, it focuses on deep learning for machine translations, speech-to-text,
    text-to-speech, language modeling, question answering, and many more NLP domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Readers who can benefit the most from this book are:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and NLP practitioners who are familiar with Python programming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysts and data scientists who want an introduction to AI language understanding
    to process the increasing amounts of language-driven functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Part I: Introduction to Transformer Architectures**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 1*, *What are Transformers?*, explains, at a high level, what transformers
    are. We’ll look at the transformer ecosystem and the properties of foundation
    models. The chapter highlights many of the platforms available and the evolution
    of Industry 4.0 AI specialists.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    goes through the background of NLP to understand how RNN, LSTM, and CNN deep learning
    architectures evolved into the Transformer architecture that opened a new era.
    We will go through the Transformer’s architecture through the unique *Attention
    Is All You Need* approach invented by the Google Research and Google Brain authors.
    We will describe the theory of transformers. We will get our hands dirty in Python
    to see how the multi-attention head sub-layers work. By the end of this chapter,
    you will have understood the original architecture of the Transformer. You will
    be ready to explore the multiple variants and usages of the Transformer in the
    following chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Fine-Tuning BERT Models*, builds on the architecture of the original
    Transformer. **Bidirectional Encoder Representations from Transformers** (**BERT**)
    shows you a new way of perceiving the world of NLP. Instead of analyzing a past
    sequence to predict a future sequence, BERT attends to the whole sequence! We
    will first go through the key innovations of BERT’s architecture and then fine-tune
    a BERT model by going through each step in a Google Colaboratory notebook. Like
    humans, BERT can learn tasks and perform other new ones without having to learn
    the topic from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 4*, *Pretraining a RoBERTa Model from Scratch*, builds a RoBERTa transformer
    model from scratch using the Hugging Face PyTorch modules. The transformer will
    be both BERT-like and DistilBERT-like. First, we will train a tokenizer from scratch
    on a customized dataset. The trained transformer will then run on a downstream
    masked language modeling task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part II: Applying Transformers for Natural Language Understanding and Generation**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 5*, *Downstream NLP Tasks with Transformers*, reveals the magic of
    transformer models with downstream NLP tasks. A pretrained transformer model can
    be fine-tuned to solve a range of NLP tasks such as BoolQ, CB, MultiRC, RTE, WiC,
    and more, dominating the GLUE and SuperGLUE leaderboards. We will go through the
    evaluation process of transformers, the tasks, datasets, and metrics. We will
    then run some of the downstream tasks with Hugging Face’s pipeline of transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6*, *Machine Translation with the Transformer*, defines machine translation
    to understand how to go from human baselines to machine transduction methods.
    We will then preprocess a WMT French-English dataset from the European Parliament.
    Machine translation requires precise evaluation methods, and in this chapter,
    we explore the BLEU scoring method. Finally, we will implement a Transformer machine
    translation model with Trax.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, explores
    many aspects of OpenAI’s GPT-2 and GPT-3 transformers. We will first examine the
    architecture of OpenAI’s GPT models before explaining the different GPT-3 engines.
    Then we will run a GPT-2 345M parameter model and interact with it to generate
    text. Next, we’ll see the GPT-3 playground in action before coding a GPT-3 model
    for NLP tasks and comparing the results to GPT-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Applying Transformers to Legal and Financial Documents for AI
    Text Summarization*, goes through the concepts and architecture of the T5 transformer
    model. We will initialize a T5 model from Hugging Face to summarize documents.
    We will task the T5 model to summarize various documents, including a sample from
    the *Bill of Rights*, exploring the successes and limitations of transfer learning
    approaches applied to transformers. Finally, we will use GPT-3 to summarize some
    corporation law text to a second-grader.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9*, *Matching Tokenizers and Datasets*, analyzes the limits of tokenizers
    and looks at some of the methods applied to improve the data encoding process’s
    quality. We will first build a Python program to investigate why some words are
    omitted or misinterpreted by word2vector tokenizers. Following this, we find the
    limits of pretrained tokenizers with a tokenizer-agonistic method.'
  prefs: []
  type: TYPE_NORMAL
- en: We will improve a T5 summary by applying some of the ideas that show that there
    is still much room left to improve the methodology of the tokenization process.
    Finally, we will test the limits of GPT-3’s language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Semantic Role Labeling with BERT-Based Transformers*, explores
    how transformers learn to understand a text’s content. **Semantic Role Labeling**
    (**SRL**) is a challenging exercise for a human. Transformers can produce surprising
    results. We will implement a BERT-based transformer model designed by the Allen
    Institute for AI in a Google Colab notebook. We will also use their online resources
    to visualize SRL outputs. Finally, we will question the scope of SRL and understand
    the reasons behind its limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part III: Advanced Language Understanding Techniques**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 11*, *Let Your Data Do the Talking: Story, Questions, and Answers*,
    shows how a transformer can learn how to reason. A transformer must be able to
    understand a text, a story, and also display reasoning skills. We will see how
    question answering can be enhanced by adding NER and SRL to the process. We will
    build the blueprint for a question generator that can be used to train transformers
    or as a stand-alone solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 12*, *Detecting Customer Emotions to Make Predictions*, shows how
    transformers have improved sentiment analysis. We will analyze complex sentences
    using the Stanford Sentiment Treebank, challenging several transformer models
    to understand not only the structure of a sequence but also its logical form.
    We will see how to use transformers to make predictions that trigger different
    actions depending on the sentiment analysis output. The chapter finishes with
    some edge cases using GPT-3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 13*, *Analyzing Fake News with Transformers*, delves into the hot
    topic of fake news and how transformers can help us understand the different perspectives
    of the online content we see each day. Every day, billions of messages, posts,
    and articles are published on the web through social media, websites, and every
    form of real-time communication available. Using several techniques from the previous
    chapters, we will analyze debates on climate change and gun control and the Tweets
    from a former president. We will go through the moral and ethical problem of determining
    what can be considered fake news beyond reasonable doubt and what news remains
    subjective.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 14*, *Interpreting Black Box Transformer Models*, lifts the lid on
    the black box that is transformer models by visualizing their activity. We will
    use BertViz to visualize attention heads and **Language Interpretability Tool**
    (**LIT**) to carry out a **principal component analysis** (**PCA**). Finally,
    we will use LIME to visualize transformers via dictionary learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 15*, *From NLP to Task-Agnostic Transformer Models*, delves into the
    advanced models, Reformer and DeBERTa, running examples using Hugging Face. Transformers
    can process images as sequences of words. We will also look at different vision
    transformers such as ViT, CLIP, and DALL-E. We will test them on computer vision
    tasks, including generating computer images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 16*, *The Emergence of Transformer-Driven Copilots*, explores the
    maturity of Industry 4.0\. The chapter begins with prompt engineering examples
    using informal/casual English. Next, we will use GitHub Copilot and OpenAI Codex
    to create code from a few lines of instructions. We will see that vision transformers
    can help NLP transformers visualize the world around them. We will create a transformer-based
    recommendation system, which can be used by digital humans in whatever metaverse
    you may end up in!'
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix I*, *Terminology of Transformer Models*, examines the high-level
    structure of a transformer, from stacks and sublayers to attention heads.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix II*, *Hardware Constraints for Transformer Models*, looks at CPU
    and GPU performance running transformers. We will see why transformers and GPUs
    and transformers are a perfect match, concluding with a test using Google Colab
    CPU, Google Colab Free GPU, and Google Colab Pro GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix III*, *Generic Text Completion with GPT-2*, provides a detailed explanation
    of generic text completion using GPT-2 from *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix IV*, *Custom Text Completion with GPT-2*, supplements *Chapter 7*,
    *The Rise of Suprahuman Transformers with GPT-3 Engines* by building and training
    a GPT-2 model and making it interact with custom text.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Appendix V*, *Answers to the Questions*, provides answers to the questions
    at the end of each chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the programs in the book are Colaboratory notebooks. All you will need
    is a free Google Gmail account, and you will be able to run the notebooks on Google
    Colaboratory’s free VM.
  prefs: []
  type: TYPE_NORMAL
- en: You will need Python installed on your machine for some of the educational programs.
  prefs: []
  type: TYPE_NORMAL
- en: Take the necessary time to read *Chapter 2*, *Getting Started with the Architecture
    of the Transformer Model* and *Appendix I*, *Terminology of Transformer Models*.
    *Chapter 2* contains the description of the original Transformer, which is built
    from building blocks explained in *Appendix I*, *Terminology of Transformer Models*,
    that will be implemented throughout the book. If you find it difficult, then pick
    up the general intuitive ideas out of the chapter. You can then go back to these
    chapters when you feel more comfortable with transformers after a few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: After reading each chapter, consider how you could implement transformers for
    your customers or use them to move up in your career with novel ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we use OpenAI Codex later on in the book, which currently has
    a waiting list. Sign up now to avoid a long wait time at [https://openai.com/blog/openai-codex/](https://openai.com/blog/openai-codex/).
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code bundle for the book is hosted on GitHub at [https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that contains color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates sentences and words run through the models in the book,
    code words in text, database table names, folder names, filenames, file extensions,
    pathnames, dummy URLs, user input, and Twitter handles. For example, “However,
    if you wish to explore the code, you will find it in the Google Colaboratory `positional_encoding.ipynb`
    notebook and the `text.txt` file in this chapter’s GitHub repository.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: “In our case, we are looking for **t5-large**, a t5-large model
    we can smoothly run in Google Colaboratory.”'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: Email `feedback@packtpub.com` and mention the book’s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit, [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Transformers for Natural Language Processing - Second Edition*,
    we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1803247339) for this book and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
