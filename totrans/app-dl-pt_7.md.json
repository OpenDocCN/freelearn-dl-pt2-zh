["```py\n    import torch\n    import torch.nn as nn\n    import matplotlib.pyplot as plt\n    ```", "```py\n    x = torch.randn(100,5)\n    y = torch.randint(0, 2, (100, 1)).type(torch.FloatTensor)\n    ```", "```py\n    model = nn.Sequential(nn.Linear(5, 1),\n                          nn.Sigmoid())\n    ```", "```py\n    loss_function = torch.nn.MSELoss()\n    ```", "```py\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    ```", "```py\n    losses = []\n    for i in range(100):\n        y_pred = model(x)\n        loss = loss_function(y_pred, y)\n        print(loss.item())\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    model.state_dict()\n    ```", "```py\n    plt.plot(range(0,100), losses)\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    data = pd.read_csv(\"YearPredictionMSD.txt\", header=None, nrows=50000)\n    data.head()\n    ```", "```py\n    data.iloc[0,:]\n    ```", "```py\n    data.isnull().sum().sum() \n    ```", "```py\n    outliers = {}\n    for i in range(data.shape[1]):\n        min_t = data[data.columns[i]].mean() - (            3 * data[data.columns[i]].std())\n        max_t = data[data.columns[i]].mean() + (            3 * data[data.columns[i]].std())\n        count = 0\n        for j in data[data.columns[i]]:\n            if j < min_t or j > max_t:\n                count += 1\n        percentage = count/data.shape[0]\n        outliers[data.columns[i]] = \"%.3f\" % percentage\n    print(outliers)\n    ```", "```py\n    X = data.iloc[:, 1:]\n    Y = data.iloc[:, 0]\n    ```", "```py\n    X = (X - X.mean())/X.std()\n    X.head()\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_shuffle = X.sample(frac=1)\n    Y_shuffle = Y.sample(frac=1)\n    x_new, x_test, y_new, y_test = train_test_split(X_shuffle,                                                 Y_shuffle,                                                 test_size=0.2,                                                 random_state=0)\n    dev_per = x_test.shape[0]/x_new.shape[0]\n    x_train, x_dev, y_train, y_dev = train_test_split(x_new,                                                   y_new,                                                   test_size=dev_per,                                                   random_state=0)\n    ```", "```py\n(30000, 90) (30000, )\n(10000, 90) (10000, )\n(10000, 90) (10000, )\n```", "```py\n    import torch\n    import torch.nn as nn\n    ```", "```py\n    x_train = torch.tensor(x_train.values).float()\n    y_train = torch.tensor(y_train.values).float()\n    x_dev = torch.tensor(x_dev.values).float()\n    y_dev = torch.tensor(y_dev.values).float()\n    x_test = torch.tensor(x_test.values).float()\n    y_test = torch.tensor(y_test.values).float()\n    ```", "```py\n    model = nn.Sequential(nn.Linear(x_train.shape[1], 10),\n                            nn.ReLU(),\n                            nn.Linear(10, 7),\n                            nn.ReLU(),\n                            nn.Linear(7, 5),\n                            nn.ReLU(),\n                            nn.Linear(5, 1))\n    ```", "```py\n    loss_function = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    ```", "```py\n    for i in range(100):\n        y_pred = model(x_train)\n        loss = loss_function(y_pred, y_train)\n        print(i, loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    pred = model(x_test[0])\n    print(y_test[0], pred)\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import shuffle\n    from sklearn.metrics import accuracy_score\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import matplotlib.pyplot as plt\n    ```", "```py\n    data = pd.read_csv(\"dccc_prepared.csv\")\n    ```", "```py\n    X = data.iloc[:,:-1]\n    y = data[\"default payment next month\"]\n    ```", "```py\n    X_new, X_test, y_new, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    dev_per = X_test.shape[0]/X_new.shape[0]\n    X_train, X_dev, y_train, y_dev = train_test_split(X_new, y_new, test_size=dev_per, random_state=0)\n    ```", "```py\n    Training sets: (28036, 22) (28036,)\n    Validation sets: (9346, 22) (9346,)\n    Testing sets: (9346, 22) (9346,)\n    ```", "```py\n    X_dev_torch = torch.tensor(X_dev.values).float()\n    y_dev_torch = torch.tensor(y_dev.values)\n    X_test_torch = torch.tensor(X_test.values).float()\n    y_test_torch = torch.tensor(y_test.values)\n    ```", "```py\n    class Classifier(nn.Module):\n        def __init__(self, input_size):\n            super().__init__()\n            self.hidden_1 = nn.Linear(input_size, 10)\n            self.hidden_2 = nn.Linear(10, 10)\n            self.hidden_3 = nn.Linear(10, 10)\n            self.output = nn.Linear(10, 2)\n        def forward(self, x):\n            z = F.relu(self.hidden_1(x))\n            z = F.relu(self.hidden_2(z))\n            z = F.relu(self.hidden_3(z))\n            out = F.log_softmax(self.output(z), dim=1)\n            return out\n    ```", "```py\n    model = Classifier(X_train.shape[1])\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 50\n    batch_size = 128\n    ```", "```py\n    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []\n    for e in range(epochs):\n        X_, y_ = shuffle(X_train, y_train)\n        running_loss = 0\n        running_acc = 0\n        iterations = 0\n        for i in range(0, len(X_), batch_size):\n            iterations += 1\n            b = i + batch_size\n            X_batch = torch.tensor(X_.iloc[i:b,:].values).float()\n            y_batch = torch.tensor(y_.iloc[i:b].values)\n            log_ps = model(X_batch)\n            loss = criterion(log_ps, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            ps = torch.exp(log_ps)\n            top_p, top_class = ps.topk(1, dim=1)\n            running_acc += accuracy_score(y_batch, top_class)\n            dev_loss = 0\n            acc = 0\n            with torch.no_grad():\n                log_dev = model(X_dev_torch)\n                dev_loss = criterion(log_dev, y_dev_torch)\n                ps_dev = torch.exp(log_dev)\n                top_p, top_class_dev = ps_dev.topk(1, dim=1)\n                acc = accuracy_score(y_dev_torch, top_class_dev)\n                train_losses.append(running_loss/iterations)\n                dev_losses.append(dev_loss)\n                train_acc.append(running_acc/iterations)\n                dev_acc.append(acc)\n                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n                \"Training Loss: {:.3f}.. \".format(running_loss/iterations),\n                \"Validation Loss: {:.3f}.. \".format(dev_loss),\n                \"Training Accuracy: {:.3f}.. \".format(running_acc/                                                  iterations),\n                \"Validation Accuracy: {:.3f}\".format(acc))\n    ```", "```py\n    plt.plot(train_losses, label='Training loss')\n    plt.plot(dev_losses, label='Validation loss')\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    plt.plot(train_acc, label=\"Training accuracy\")\n    plt.plot(dev_acc, label=\"Validation accuracy\")\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import shuffle\n    from sklearn.metrics import accuracy_score\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import matplotlib.pyplot as plt\n    torch.manual_seed(0)\n    ```", "```py\n    data = pd.read_csv(\"dccc_prepared.csv\")\n    X = data.iloc[:,:-1]\n    y = data[\"default payment next month\"]\n    X_new, X_test, y_new, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    dev_per = X_test.shape[0]/X_new.shape[0]\n    X_train, X_dev, y_train, y_dev = train_test_split(X_new, y_new, test_size=dev_per, random_state=0)\n    X_dev_torch = torch.tensor(X_dev.values).float()\n    y_dev_torch = torch.tensor(y_dev.values)\n    X_test_torch = torch.tensor(X_test.values).float()\n    y_test_torch = torch.tensor(y_test.values)\n    ```", "```py\n    # class defining model's architecture and operations between layers\n    class Classifier(nn.Module):\n        def __init__(self, input_size):\n            super().__init__()\n            self.hidden_1 = nn.Linear(input_size, 100)\n            self.hidden_2 = nn.Linear(100, 100)\n            self.hidden_3 = nn.Linear(100, 50)\n            self.hidden_4 = nn.Linear(50,50)\n            self.output = nn.Linear(50, 2)\n            self.dropout = nn.Dropout(p=0.1)\n            #self.dropout_2 = nn.Dropout(p=0.1)\n        def forward(self, x):\n            z = self.dropout(F.relu(self.hidden_1(x)))\n            z = self.dropout(F.relu(self.hidden_2(z)))\n            z = self.dropout(F.relu(self.hidden_3(z)))\n            z = self.dropout(F.relu(self.hidden_4(z)))\n            out = F.log_softmax(self.output(z), dim=1)\n            return out\n    # parameters definition\n    model = Classifier(X_train.shape[1])\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 3000\n    batch_size = 128\n    # training process\n    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []\n    x_axis = []\n    for e in range(1, epochs + 1):\n        X_, y_ = shuffle(X_train, y_train)\n        running_loss = 0\n        running_acc = 0\n        iterations = 0\n\n        for i in range(0, len(X_), batch_size):\n            iterations += 1\n            b = i + batch_size\n            X_batch = torch.tensor(X_.iloc[i:b,:].values).float()\n            y_batch = torch.tensor(y_.iloc[i:b].values)\n\n            log_ps = model(X_batch)\n            loss = criterion(log_ps, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            ps = torch.exp(log_ps)\n            top_p, top_class = ps.topk(1, dim=1)\n            running_acc += accuracy_score(y_batch, top_class)\n\n        dev_loss = 0\n        acc = 0\n        # Turn off gradients for validation, saves memory and computations       with torch.no_grad():\n            log_dev = model(X_dev_torch)\n            dev_loss = criterion(log_dev, y_dev_torch)\n            ps_dev = torch.exp(log_dev)\n            top_p, top_class_dev = ps_dev.topk(1, dim=1)\n            acc = accuracy_score(y_dev_torch, top_class_dev)\n        if e%50 == 0 or e == 1:\n            x_axis.append(e)\n\n            train_losses.append(running_loss/iterations)\n            dev_losses.append(dev_loss)\n            train_acc.append(running_acc/iterations)\n            dev_acc.append(acc)\n\n            print(\"Epoch: {}/{}.. \".format(e, epochs),\n                  \"Training Loss: {:.3f}.. \".format(running_loss/                                                iterations),\n                  \"Validation Loss: {:.3f}.. \".format(dev_loss),\n                  \"Training Accuracy: {:.3f}.. \".format(running_acc/                                                    iterations),\n                  \"Validation Accuracy: {:.3f}\".format(acc))\n    ```", "```py\n    plt.plot(x_axis,train_losses, label='Training loss')\n    plt.plot(x_axis, dev_losses, label='Validation loss')\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    plt.plot(x_axis, train_acc, label=\"Training accuracy\")\n    plt.plot(x_axis, dev_acc, label=\"Validation accuracy\")\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    model.eval()\n    test_pred = model(X_test_torch)\n    test_pred = torch.exp(test_pred)\n    top_p, top_class_test = test_pred.topk(1, dim=1)\n    acc_test = accuracy_score(y_test_torch, top_class_test)\n    ```", "```py\n    checkpoint = {\"input\": X_train.shape[1],\n                  \"state_dict\": model.state_dict()}\n    torch.save(checkpoint, \"checkpoint.pth\")\n    ```", "```py\n    import torch\n    import final_model\n    ```", "```py\n    def load_model_checkpoint(path):\n        checkpoint = torch.load(path)\n        model = final_model.Classifier(checkpoint[\"input\"])\n        model.load_state_dict(checkpoint[\"state_dict\"])\n        return model\n    model = load_model_checkpoint(\"checkpoint.pth\")\n    ```", "```py\n    example = torch.tensor([[0.0606, 0.5000, 0.3333, 0.4828, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.1651, 0.0869, 0.0980, 0.1825, 0.1054, 0.2807, 0.0016, 0.0000, 0.0033, 0.0027, 0.0031, 0.0021]]).float()\n    pred = model(example)\n    pred = torch.exp(pred)\n    top_p, top_class_test = pred.topk(1, dim=1)\n    ```", "```py\n    traced_script = torch.jit.trace(model, example, check_trace=False)\n    ```", "```py\n    prediction = traced_script(example)\n    prediction = torch.exp(prediction)\n    top_p_2, top_class_test_2 = prediction.topk(1, dim=1)\n    ```", "```py\n    import numpy as np\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    from torchvision import datasets\n    import torchvision.transforms as transforms\n    from torch.utils.data.sampler import SubsetRandomSampler\n    from sklearn.metrics import accuracy_score\n    import matplotlib.pyplot as plt\n    ```", "```py\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    ```", "```py\n    batch_size = 100\n    train_data = datasets.CIFAR10('data', train=True, download=True, transform=transform)\n    test_data = datasets.CIFAR10('data', train=False, download=True, transform=transform)\n    ```", "```py\n    dev_size = 0.2\n    idx = list(range(len(train_data)))\n    np.random.shuffle(idx)\n    split_size = int(np.floor(dev_size * len(train_data)))\n    train_idx, dev_idx = idx[split_size:], idx[:split_size]\n    train_sampler = SubsetRandomSampler(train_idx)\n    dev_sampler = SubsetRandomSampler(dev_idx)\n    ```", "```py\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n    dev_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=dev_sampler)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n    ```", "```py\n        class CNN(nn.Module):\n            def __init__(self):\n                super(CNN, self).__init__()\n                self.conv1 = nn.Conv2d(3, 10, 3, 1, 1)\n                self.conv2 = nn.Conv2d(10, 20, 3, 1, 1)\n                self.conv3 = nn.Conv2d(20, 40, 3, 1, 1)\n                self.pool = nn.MaxPool2d(2, 2)\n\n                self.linear1 = nn.Linear(40 * 4 * 4, 100)\n                self.linear2 = nn.Linear(100, 10)\n                self.dropout = nn.Dropout(0.2)\n            def forward(self, x):\n                x = self.pool(F.relu(self.conv1(x)))\n                x = self.pool(F.relu(self.conv2(x)))\n                x = self.pool(F.relu(self.conv3(x)))\n\n                x = x.view(-1, 40 * 4 * 4)\n                x = self.dropout(x)\n                x = F.relu(self.linear1(x))\n                x = self.dropout(x)\n                x = F.log_softmax(self.linear2(x), dim=1)\n\n                return x\n        ```", "```py\n    model = CNN()\n    loss_function = nn.NLLLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 50\n    ```", "```py\n    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []\n    x_axis = []\n    for e in range(1, epochs+1):\n        losses = 0\n        acc = 0\n        iterations = 0\n\n        model.train()\n        for data, target in train_loader:\n            iterations += 1\n            pred = model(data)\n            loss = loss_function(pred, target)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            losses += loss.item()\n            p = torch.exp(pred)\n            top_p, top_class = p.topk(1, dim=1)\n            acc += accuracy_score(target, top_class)\n\n        dev_losss = 0\n        dev_accs = 0\n        iter_2 = 0\n\n        if e%5 == 0 or e == 1:\n            x_axis.append(e)\n\n            with torch.no_grad():\n                model.eval()\n\n                for data_dev, target_dev in dev_loader:\n                    iter_2 += 1\n\n                    dev_pred = model(data_dev)\n                    dev_loss = loss_function(dev_pred, target_dev)\n                    dev_losss += dev_loss.item()\n                    dev_p = torch.exp(dev_pred)\n                    top_p, dev_top_class = dev_p.topk(1, dim=1)\n                    dev_accs += accuracy_score(target_dev,                 dev_top_class)\n\n            train_losses.append(losses/iterations)\n            dev_losses.append(dev_losss/iter_2)\n            train_acc.append(acc/iterations)\n            dev_acc.append(dev_accs/iter_2)\n\n            print(\"Epoch: {}/{}.. \".format(e, epochs),\n                  \"Training Loss: {:.3f}.. \".format(losses/iterations),\n                  \"Validation Loss: {:.3f}.. \".format(dev_losss/iter_2),\n                  \"Training Accuracy: {:.3f}.. \".format(acc/iterations),\n                  \"Validation Accuracy: {:.3f}\".format(dev_accs/iter_2))\n    ```", "```py\n    plt.plot(x_axis,train_losses, label='Training loss')\n    plt.plot(x_axis, dev_losses, label='Validation loss')\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    plt.plot(x_axis, train_acc, label=\"Training accuracy\")\n    plt.plot(x_axis, dev_acc, label=\"Validation accuracy\")\n    plt.legend(frameon=False)\n    plt.show()\n    ```", "```py\n    model.eval()\n    iter_3 = 0\n    acc_test = 0\n    for data_test, target_test in test_loader:\n        iter_3 += 1\n        test_pred = model(data_test)\n        test_pred = torch.exp(test_pred)\n        top_p, top_class_test = test_pred.topk(1, dim=1)\n        acc_test += accuracy_score(target_test, top_class_test)\n    print(acc_test/iter_3)\n    ```", "```py\n        transform = {\n            \"train\": transforms.Compose([\n            transforms.RandomHorizontalFlip(0.5), \n            transforms.RandomGrayscale(0.1),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n            \"test\": transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n        ```", "```py\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(3, 10, 3, 1, 1)\n            self.norm1 = nn.BatchNorm2d(10)\n            self.conv2 = nn.Conv2d(10, 20, 3, 1, 1)\n            self.norm2 = nn.BatchNorm2d(20)\n            self.conv3 = nn.Conv2d(20, 40, 3, 1, 1)\n            self.norm3 = nn.BatchNorm2d(40)\n            self.pool = nn.MaxPool2d(2, 2)\n            self.linear1 = nn.Linear(40 * 4 * 4, 100)\n            self.norm4 = nn.BatchNorm1d(100)\n            self.linear2 = nn.Linear(100, 10)\n            self.dropout = nn.Dropout(0.2)\n        def forward(self, x):\n            x = self.pool(self.norm1(F.relu(self.conv1(x))))\n            x = self.pool(self.norm2(F.relu(self.conv2(x))))\n            x = self.pool(self.norm3(F.relu(self.conv3(x))))\n            x = x.view(-1, 40 * 4 * 4)\n            x = self.dropout(x)\n            x = self.norm4(F.relu(self.linear1(x)))\n            x = self.dropout(x)\n            x = F.log_softmax(self.linear2(x), dim=1)\n            return x\n    ```", "```py\n    import numpy as np\n    import torch\n    from torch import nn, optim\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    from torchvision import transforms, models\n    ```", "```py\n    imsize = 224\n    loader = transforms.Compose([\n    transforms.Resize(imsize), \n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n    ```", "```py\n    def image_loader(image_name):\n        image = Image.open(image_name)\n        image = loader(image).unsqueeze(0)\n        return image\n    content_img = image_loader(\"images/landscape.jpg\")\n    style_img = image_loader(\"images/monet.jpg\")\n    ```", "```py\n    unloader = transforms.Compose([\n    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225)),\n    transforms.ToPILImage()])\n    ```", "```py\n    def tensor2image(tensor):\n        image = tensor.clone()\n        image = image.squeeze(0)\n        image = unloader(image)\n        return image\n    plt.figure()\n    plt.imshow(tensor2image(content_img))\n    plt.title(\"Content Image\")\n    plt.show()\n    plt.figure()\n    plt.imshow(tensor2image(style_img))\n    plt.title(\"Style Image\")\n    plt.show()\n    ```", "```py\n    model = models.vgg19(pretrained=True).features\n    for param in model.parameters():\n        param.requires_grad_(False)\n    ```", "```py\n    relevant_layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1', '21': 'conv4_2', '28': 'conv5_1'}\n    def features_extractor(x, model, layers):\n        features = {}\n        for index, layer in model._modules.items():\n            if index in layers:\n                x = layer(x)\n                features[layers[index]] = x\n        return features\n    content_features = features_extractor(content_img, model, relevant_layers)\n    style_features = features_extractor(style_img, model, relevant_layers)\n    ```", "```py\n    style_grams = {}\n    for i in style_features:\n        layer = style_features[i]\n        _, d1, d2, d3 = layer.shape\n        features = layer.view(d1, d2 * d3)\n        gram = torch.mm(features, features.t())\n    style_grams[i] = gram\n    target_img = content_img.clone().requires_grad_(True)\n    ```", "```py\n    style_weights = {'conv1_1': 1., 'conv2_1': 0.8, 'conv3_1': 0.6, 'conv4_1': 0.4, 'conv5_1': 0.2}\n    alpha = 1\n    beta = 1e6\n    ```", "```py\n    for i in range(1, iterations+1):\n\n        target_features = features_extractor(target_img, model,                                          relevant_layers)\n        content_loss = torch.mean((target_features['conv4_2'] -                                content_features['conv4_2'])**2)\n\n        style_losses = 0\n        for layer in style_weights:\n\n            target_feature = target_features[layer]\n            _, d1, d2, d3 = target_feature.shape\n\n            target_reshaped = target_feature.view(d1, d2 * d3)\n            target_gram = torch.mm(target_reshaped, target_reshaped.t())\n            style_gram = style_grams[layer]\n\n            style_loss = style_weights[layer] * torch.mean((target_gram -                                                         style_gram)**2)\n            style_losses += style_loss / (d1 * d2 * d3)\n\n        total_loss = alpha * content_loss + beta * style_loss\n\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        if  i % print_statement == 0 or i == 1:\n            print('Total loss: ', total_loss.item())\n            plt.imshow(tensor2image(target_img))\n            plt.show()\n    ```", "```py\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n    ax1.imshow(tensor2image(content_img))\n    ax2.imshow(tensor2image(target_img))\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import torch\n    from torch import nn, optim\n    ```", "```py\n    torch.manual_seed(10)\n    ```", "```py\n    data = pd.read_csv(\"Sales_Transactions_Dataset_Weekly.csv\")\n    data = data.iloc[:,1:53]\n    data.head()\n    ```", "```py\n    plot_data = data.sample(5, random_state=0)\n    x = range(1,53)\n    plt.figure(figsize=(10,5))\n    for i,row in plot_data.iterrows():\n        plt.plot(x,row)\n        plt.legend(plot_data.index)\n        plt.xlabel(\"Weeks\")\n        plt.ylabel(\"Sales transactions per product\")\n    plt.show()\n    ```", "```py\n    data_train = data.iloc[:,:-1]\n    inputs = torch.Tensor(data_train.values).unsqueeze(1)\n    targets = data_train.shift(-1, axis=\"columns\", fill_value=data.iloc[:,-1]).astype(dtype = \"float32\")\n    targets = torch.Tensor(targets.values)\n    ```", "```py\n    class RNN(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers):\n            super().__init__()\n            self.hidden_size = hidden_size\n            self.rnn = nn.RNN(input_size, hidden_size, num_layers,                           batch_first=True)\n            self.output = nn.Linear(hidden_size, 1)\n\n        def forward(self, x, hidden):\n            out, hidden = self.rnn(x, hidden)\n            out = out.view(-1, self.hidden_size)\n            out = self.output(out)\n\n            return out, hidden\n    ```", "```py\n    model = RNN(data_train.shape[1], 10, 1) \n    ```", "```py\n    loss_function = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 10000\n    ```", "```py\n    losses = []\n    for i in range(1, epochs+1):\n        hidden = None\n        pred, hidden = model(inputs, hidden)\n        loss = loss_function(targets, pred)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        if i%1000 == 0:\n            print(\"epoch: \", i, \"=... Loss function: \", losses[-1])\n    ```", "```py\n    x_range = range(len(losses))\n    plt.plot(x_range, losses)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"Loss function\")\n    plt.show()\n    ```", "```py\n    x_range = range(len(data))\n    target = data.iloc[:,-1].values.reshape(len(data),1)\n    plt.figure(figsize=(15,5))\n    plt.scatter(x_range[:20], target[:20])\n    plt.scatter(x_range[:20], pred.detach().numpy()[:20])\n    plt.legend([\"Ground truth\", \"Prediction\"])\n    plt.xlabel(\"Product\")\n    plt.ylabel(\"Sales Transactions\")\n    plt.xticks(range(0, 20))\n    plt.show()\n    ```", "```py\n    import math\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    ```", "```py\n    with open('alice.txt', 'r', encoding='latin1') as f:\n        data = f.read()\n        print(\"Extract: \", data[:50])\n        print(\"Length: \", len(data))\n    ```", "```py\n    chars = list(set(data))\n    indexer = {char: index for (index, char) in enumerate(chars)}\n    ```", "```py\n    indexed_data = []\n    for c in data:\n        indexed_data.append(indexer[c])\n\n    print(\"Indexed extract: \", indexed_data[:50])\n    print(\"Length: \", len(indexed_data))\n    ```", "```py\n    def index2onehot(batch):\n        batch_flatten = batch.flatten()\n        onehot_flat = np.zeros((batch.shape[0] * batch.shape[1],len(indexer)))\n        onehot_flat[range(len(batch_flatten)), batch_flatten] = 1\n        onehot = onehot_flat.reshape((batch.shape[0],\n        batch.shape[1], -1))\n        return onehot\n    ```", "```py\n    class LSTM(nn.Module):\n        def __init__(self, char_length, hidden_size, n_layers):\n            super().__init__()\n            self.hidden_size = hidden_size\n            self.n_layers = n_layers\n            self.lstm = nn.LSTM(char_length, hidden_size,\n            n_layers, batch_first=True)\n            self.output = nn.Linear(hidden_size, char_length)\n        def forward(self, x, states):\n            out, states = self.lstm(x, states)\n            out = out.contiguous().view(-1, self.hidden_size)\n            out = self.output(out)\n            return out, states\n        def init_states(self, batch_size):\n            hidden = next(self.parameters()).data.new(\n            self.n_layers, batch_size,\n            self.hidden_size).zero_()\n            cell = next(self.parameters()).data.new(self.n_layers,                                                batch_size,                                                 self.hidden_size).                                                zero_()\n            states = (hidden, cell)\n            return states\n    ```", "```py\n    n_seq = 100 ## Number of sequences per batch\n    seq_length =  50\n    n_batches = math.floor(len(indexed_data) / n_seq / seq_length)\n    total_length = n_seq * seq_length * n_batches\n    x = indexed_data[:total_length]\n    x = np.array(x).reshape((n_seq,-1))\n    ```", "```py\n    model = LSTM(len(chars), 256, 2)\n    ```", "```py\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 20\n    ```", "```py\n    losses = []\n    for e in range(1, epochs+1):\n        states = model.init_states(n_seq)\n        batch_loss = []\n\n        for b in range(0, x.shape[1], seq_length):\n            x_batch = x[:,b:b+seq_length]\n\n            if b == x.shape[1] - seq_length:\n                y_batch = x[:,b+1:b+seq_length]\n                y_batch = np.hstack((y_batch, indexer[\".\"] *                       np.ones((y_batch.shape[0],1))))\n            else:\n                y_batch = x[:,b+1:b+seq_length+1]\n\n            x_onehot = torch.Tensor(index2onehot(x_batch))\n            y = torch.Tensor(y_batch).view(n_seq * seq_length)\n\n            pred, states = model(x_onehot, states)\n            loss = loss_function(pred, y.long())\n            optimizer.zero_grad()\n            loss.backward(retain_graph=True)\n            optimizer.step()\n\n            batch_loss.append(loss.item())\n\n        losses.append(np.mean(batch_loss))\n\n        if e%1 == 0:\n            print(\"epoch: \", e, \"... Loss function: \", losses[-1])\n    ```", "```py\n    x_range = range(len(losses))\n    plt.plot(x_range, losses)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"Loss function\")\n    plt.show()\n    ```", "```py\n    starter = \"So she was considering in her own mind \"\n    states = None\n    for ch in starter:\n        x = np.array([[indexer[ch]]])\n        x = index2onehot(x)\n        x = torch.Tensor(x)\n\n        pred, states = model(x, states)\n    counter = 0\n    while starter[-1] != \".\" and counter < 50:\n        counter += 1\n        x = np.array([[indexer[starter[-1]]]])\n        x = index2onehot(x)\n        x = torch.Tensor(x)\n\n        pred, states = model(x, states)\n        pred = F.softmax(pred, dim=1)\n        p, top = pred.topk(10)\n        p = p.detach().numpy()[0]\n        top = top.numpy()[0]\n        index = np.random.choice(top, p=p/p.sum())\n\n        starter += chars[index]\n        print(starter)\n    ```", "```py\n    So she was considering in her own mind of would the cace to she tount ang to ges seokn.\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from string import punctuation\n    from sklearn.metrics import accuracy_score\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    ```", "```py\n    data = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\",        header=None)\n    reviews = data.iloc[:,0].str.lower()\n    sentiment = data.iloc[:,1].values\n    ```", "```py\n    for i in punctuation:\n        reviews = reviews.str.replace(i,\"\")\n    ```", "```py\n    words = ' '.join(reviews)\n    words = words.split()\n    vocabulary = set(words)\n    indexer = {word: index for (index, word) in enumerate(vocabulary)}\n    ```", "```py\n    indexed_reviews = []\n    for review in reviews:\n        indexed_reviews.append([indexer[word] for word in     review.split()])\n    ```", "```py\n    class LSTM(nn.Module):\n        def __init__(self, vocab_size, embed_dim, hidden_size,     n_layers):\n            super().__init__()\n            self.hidden_size = hidden_size\n            self.embedding = nn.Embedding(vocab_size, embed_dim)\n            self.lstm = nn.LSTM(embed_dim, hidden_size, n_layers,                     batch_first=True)\n            self.output = nn.Linear(hidden_size, 1)\n\n        def forward(self, x):\n            out = self.embedding(x)\n            out, _ = self.lstm(out)\n            out = out.contiguous().view(-1, self.hidden_size)\n            out = self.output(out)\n            out = out[-1,0]\n            out = torch.sigmoid(out)\n\n            return out\n    ```", "```py\n    model = LSTM(len(vocabulary), 64, 128, 3)\n    ```", "```py\n    loss_function = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    epochs = 10\n    ```", "```py\n    losses = []\n    acc = []\n    for e in range(1, epochs+1):\n        single_loss = []\n        preds = []\n        targets = []\n        for i, r in enumerate(indexed_reviews):\n            if len(r) <= 1:\n                continue\n            x = torch.Tensor([r]).long()\n            y = torch.Tensor([sentiment[i]])\n\n            pred = model(x)\n            loss = loss_function(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            final_pred = np.round(pred.detach().numpy())\n            preds.append(final_pred)\n            targets.append(y)\n            single_loss.append(loss.item())\n\n        losses.append(np.mean(single_loss))\n        accuracy = accuracy_score(targets,preds)\n        acc.append(accuracy)\n        if e%1 == 0:\n            print(\"Epoch: \", e, \"... Loss function: \", losses[-1],         \"... Accuracy: \", acc[-1])\n    ```", "```py\n    x_range = range(len(losses))\n    plt.plot(x_range, losses)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"Loss function\")\n    plt.show()\n    ```", "```py\n    x_range = range(len(acc))\n    plt.plot(x_range, acc)\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"Accuracy score\")\n    plt.show()\n    ```"]