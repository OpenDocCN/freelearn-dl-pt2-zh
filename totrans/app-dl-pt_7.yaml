- en: '*Appendix*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is included to assist the students to perform the activities in
    the book. It includes detailed steps that are to be performed by the students
    to achieve the objectives of the activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 1: Introduction to Deep Learning and PyTorch'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 1: Creating a Single-Layer Neural Network'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create dummy input data (`x`) of random values and dummy target data (`y`)
    that only contains 0s and 1s. Store the data in PyTorch tensors. Tensor `x` should
    have a size of (100,5), while the size of `y` should be (100,1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the architecture of the model and store it in a variable named `model`.
    Remember to create a single-layer model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function to be used. Use the Mean Square Error loss function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the optimizer of your model. Use the Adam optimizer and a learning rate
    of 0.01:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the optimization for 100 iterations. In each iteration, print and save
    the loss value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The final loss should be approximately 0.238.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the values of the final weights and bias. There should be a total of
    five weights (one for each feature of the input data) and one bias value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a line plot to display the loss value for each iteration step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.8: Loss function throughout the training process](img/C11865_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Loss function throughout the training process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 2: Building Blocks of Neural Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 2: Performing Data Preparation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using pandas, load the text file. Considering that the previously downloaded
    text file has the same formatting as a CSV file, you can read it using the `read_csv()`
    function. Make sure to change the header argument to `None`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: To avoid memory limitations, use the `nrows` argument when reading the text
    file in order to read a smaller section of the entire dataset. In the preceding
    example, we are reading the first 50,000 rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify whether any qualitative data is present in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check for missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you add an additional `sum()` function to the line of code previously used
    for this purpose, you will get the sum of missing values in the entire dataset,
    without discriminating by column:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the features from the target data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale the features data using the standardization methodology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into three sets: training, validation, and testing. Use the
    approach of your preference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting shapes should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 3: Performing Data Preparation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the features from the targets for all three sets of data created in the
    previous activity. Convert the DataFrames into tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the architecture of the network. Feel free to try different combinations
    for the number of layers and the number of units per layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function and the optimizer algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use a `for` loop to train the network for 100 iteration steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test your model by performing a prediction on the first instance of the testing
    set and comparing it to the ground truth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29: Output of the activity](img/C11865_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.29: Output of the activity'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 3: A Classification Problem Using DNNs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 4: Building an ANN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the previously prepared dataset, which should have been named `dccc_prepared.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the features from the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using scikit-learn''s `train_test_split` function, split the dataset into training,
    validation, and testing sets. Use a 60/20/20% split ratio. Set `random_state`
    as 0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final shapes of each of the sets is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Convert the validation and testing sets to tensors, considering that the features
    matrices should be of type float, while the target matrices should not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the training sets unconverted for the moment as they will undergo further
    transformations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a custom module class defining the layers of the network. Include a forward
    function that specifies the activation functions that will be applied to the output
    of each layer. Use ReLU for all layers, except for the output, where you should
    use `log_softmax`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define all the variables required for the training of the model. Set the number
    of epochs to 50 and the batch size to 128\. Use a learning rate of 0.001:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the network using the training sets data. Use the validation sets to
    measure performance. To do so, save the loss and the accuracy for both the training
    and validation sets in each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the loss of both sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting plot should look similar to the one here, albeit with some differences,
    considering that the shuffling of the training data may derive slightly different
    results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10: A plot displaying the training and validation losses'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C11865_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: A plot displaying the training and validation losses'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the accuracy of both sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot derived from this code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: A plot displaying the accuracy of the sets](img/C11865_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: A plot displaying the accuracy of the sets'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 5: Improving a Model''s Performance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the same libraries as those in the previous activity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data and split the features from the target. Next, split the data
    into the three subsets (training, validation, and testing), using a 60:20:20 split
    ratio. Finally, convert the validation and testing sets into PyTorch tensors,
    just as you did in the previous activity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Considering that the model is suffering from high bias, the focus should be
    on increasing the number of epochs or increasing the size of the network by adding
    additional layers or units to each layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The aim should be to approximate the accuracy over the validation set to 80%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code snippet is from the best-performing model, which was achieved
    after several fine-tuning attempts:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The accompanying Jupyter notebook for this activity can be found over in the
    GitHub repository that was previously shared. There, you will find the different
    attempts at fine-tuning the model, with their results. The best-performing model
    is found at the end of the notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the loss and accuracy for both sets of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.12: A plot displaying the loss of the sets](img/C11865_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.12: A plot displaying the loss of the sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.13: A plot displaying the accuracy of the sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C11865_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.13: A plot displaying the accuracy of the sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the best-performing model, perform prediction over the testing set (which
    should not have been used during the fine-tuning process). Compare the prediction
    to the ground truth by calculating the accuracy of the model over this set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy obtained through the model architecture and the parameters defined
    above should be around 80%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 6: Making Use of Your Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Jupyter notebook that you used for the previous activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save a Python file containing the class where you define the architecture of
    your best-performing module. Make sure to import PyTorch's required libraries
    and modules. Name it `final_model.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The file should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14: A screenshot of final_model.py](img/C11865_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.14: A screenshot of final_model.py'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Save the best-performing model. Make sure to save the information of the input
    units, along with the parameters of the model. Name it `checkpoint.pth`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import PyTorch, as well as the Python file previously created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that loads the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a prediction by inputting the following tensor into your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By printing `top_class_test`, we obtain the prediction of the model, which in
    this case is equal to 1 (yes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert the model using the JIT module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a prediction by inputting the following information to the traced script
    of your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By printing `top_class_test_2`, we get the prediction from the traced script
    representation of your model, which again is equal to 1 (yes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Chapter 4: Convolutional Neural Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 7: Building a CNN for an Image Classification Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the transformations to be performed on the data, which will be the conversion
    of the data to tensors and the normalization of the pixel values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a batch size of 100 images and download both the training and testing data
    from the `CIFAR10` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using a validation size of 20%, define the training and validation sampler
    that will be used to divide the dataset into those 2 sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `DataLoader()` function to define the batches of each set of data to
    be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the architecture of your network. Use the following information to do
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conv1: A convolutional layer that takes as input the colored image and passes
    it through 10 filters of size 3\. Both the padding and the stride should be set
    to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv2: A convolutional layer that passes the input data through 20 filters
    of size 3\. Both the padding and the stride should be set to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv3: A convolutional layer that passes the input data through 40 filters
    of size 3\. Both the padding and the stride should be set to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the ReLU activation function after each convolutional layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A pooling layer after each convolutional layer, with a filter size and stride
    of 2.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A dropout term set to 20%, after flattening the image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear1: A fully-connected layer that receives as input the flattened matrix
    from the previous layer and generates an output of 100 units. Use the ReLU activation
    function for this layer. A dropout term here is set to 20%.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear2: A fully-connected layer that generates 10 outputs, one for each class
    label. Use the `log_softmax` activation function for the output layer:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Define all of the parameters required to train your model. Train it for 100
    epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train your network and be sure to save the values for the loss and accuracy
    of both the training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the loss and accuracy of both sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19: Resulting plot showing loss of sets](img/C11865_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.19: Resulting plot showing loss of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy should look similar to the next graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: Resulting plot showing accuracy of sets](img/C11865_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.20: Resulting plot showing accuracy of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As can be seen, after the fifteenth epoch, overfitting starts to affect the
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the model''s accuracy on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy on the testing set is very similar to the accuracy achieved on
    the other 2 sets, which means that the model has the capability to perform equally
    well on unseen data. It should be around 72%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 8: Implementing Data Augmentation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate the notebook from the previous activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To solve this activity, no code will be altered besides the definition of the
    variable mentioned in the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the definition of the `transform` variable to include, in addition to
    normalizing and converting the data into tensors, the following transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the training/validation sets, a `RandomHorizontalFlip` function with a probability
    of 50% (0.5) and a `RandomGrayscale` function with a probability of 10% (0.1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the testing set, do not add any other transformation:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Train the model for 100 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting plots for loss and accuracy on the training and validation sets
    should be similar to the ones displayed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: Resulting plot showing loss of sets](img/C11865_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.21: Resulting plot showing loss of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 4.22: Resulting plot showing accuracy of sets](img/C11865_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.22: Resulting plot showing accuracy of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: By adding data augmentation, it is possible to improve the performance of the
    model as well as to reduce the overfitting that was occurring.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the accuracy of the resulting model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The performance of the model on the testing set has gone up to around 76%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 9: Implementing Batch Normalization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate the notebook from the previous activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add batch normalization to each convolutional layer, as well as to the first
    fully-connected layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting architecture of the network should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model for 100 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting plots of the loss and accuracy on the training and validation
    sets should be similar to the ones displayed next:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23: Resulting plot showing loss of sets](img/C11865_04_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.23: Resulting plot showing loss of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 4.24: Resulting plot showing loss of sets](img/C11865_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.24: Resulting plot showing loss of sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Although overfitting was introduced to the model again, it is possible to see
    that the performance on both sets has gone up.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Although it is not explored in this chapter, an ideal step would be to add dropout
    to the architecture of the network in order to reduce high variance. Feel free
    to try it to see if you are able to improve the performance even more.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the accuracy of the resulting model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The accuracy of the model on the testing set should be around 78%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Chapter 5: Style Transfer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 10: Performing Style Transfer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To be able to run this activity for many iterations (30,000) a GPU was used.
    According to this, a copy adapting the following code to work using GPUs can be
    found in the GitHub's repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the transformations to be performed over the input images. Be sure
    to resize them to the same size, convert them to tensor, and normalize them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an image loader function. It should open the image and load it. Call
    the image loader function to load both input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To be able to display the images, set the transformations to revert the normalization
    of the images and to convert the tensors into PIL images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function capable of performing the previous transformation over tensors.
    Call the function for both images and plot the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the VGG-19 model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dictionary mapping the index of the relevant layers (keys) to a name
    (values). Then, create a function to extract the feature maps of the relevant
    layers. Use them to extract the features of both input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the gram matrix for the style features. Also, create the initial
    target image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the weights for different style layers, as well as the weights for the
    content and style losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the model for 500 iterations. Define the Adam optimization algorithm before
    starting to train the model, using 0.001 as the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot both content and target images to compare the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the plot derived from this code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10: Plot of the content and target images](img/C11865_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Plot of the content and target images'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 6: Analyzing the Sequence of Data with RNNs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 11: Using a Simple RNN for a Time Series Prediction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed equal to 0 to reproduce the results in this book, using the following
    line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and then slice it so that it contains all the rows but only
    the columns from index 1 to 52:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the sales transactions by week of five randomly-chosen products from the
    entire dataset. Use a random seed of 0 when doing the random sampling in order
    to achieve the same results as in the current activity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot should look as follows:![Figure 6.21: Plot of the output](img/C11865_06_21.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.21: Plot of the output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Create the `inputs` and `targets` variables that will be fed to the network
    to create the model. These variables should be of the same shape and be converted
    to PyTorch Tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `inputs` variable should contain the data for all products for all weeks,
    except the last week – as the idea of the model is to predict this final week.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `targets` variable should be one step ahead of the `inputs` variable –
    that is, the first value of the `targets` variable should be the second one of
    the inputs variable, and so on, until the last value of the targets variable (which
    should be the last week that was left outside of the `inputs` variable):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a class containing the architecture of the network; note that the output
    size of the fully-connected layer should be 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `class` function containing the model; then, feed the input
    size, the number of neurons in each recurrent layer (10), and the number of recurrent
    layers (1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a loss function, an optimization algorithm and the number of epochs
    to train the network; for example, you can use the Mean Squared Error loss function,
    the Adam optimizer, and 10,000 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use a `for` loop to perform the training process by going through all the epochs.
    In each epoch, a prediction must be made, along with the subsequent calculation
    of the loss function and the optimization of the parameters of the network. Save
    the loss of each of the epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the losses of all epochs, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot should look as follows:![Figure 6.22: Plot displaying the
    losses of all epochs](img/C11865_06_22.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.22: Plot displaying the losses of all epochs'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using a scatter plot, display the predictions that were obtained in the last
    epoch of the training process against the ground truth values (that is, the sales
    transactions of the last week):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final plot should be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.23: Scatter plot displaying predictions](img/C11865_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Scatter plot displaying predictions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 12: Text Generation with LSTM Networks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open and read the text from *Alice in Wonderland* into the notebook. Print
    an extract of the first 100 characters and the total length of the text file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable containing a list of the unduplicated characters in your
    dataset. Then, create a dictionary that maps each character to an integer, where
    the characters will be the keys and the integers will be the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode each letter of your dataset to their paired integer. Print the first
    100 encoded characters and the total length of the encoded version of your dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that takes in a batch and encodes it as a one-hot matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the class that defines the architecture of the network. The class should
    contain an additional function that initializes the states of the LSTM layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Determine the number of batches to be created out of your dataset, bearing
    in mind that each batch should contain 100 sequences, and each with a length of
    50\. Next, split the encoded data into 100 sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize your model, using 256 as the number of hidden units for a total
    of 2 recurrent layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function and the optimization algorithms. Use the Adam optimizer
    and the cross-entropy loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the network for 20 epochs, bearing in mind that, in each epoch, the data
    must be divided into batches with a sequence length of 50\. This means, that each
    epoch with have 100 epochs, each with a sequence of 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the progress of the `loss` function over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The chart should look as follows:![Figure 6.24: Chart displaying progress of
    the loss function'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C11865_06_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.24: Chart displaying progress of the loss function'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Feed the following sentence starter into the trained model and complete the
    sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"So she was considering in her own mind"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final sentence will vary considering that there is a random factor when
    it comes to choosing each character, however it should look something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding sentence does not have a meaning considering that the network
    selects each character at a time, without a long-term memory of the previously-created
    words. Nevertheless, we can see that after only 20 epochs, the network is already
    capable of forming some words that have a meaning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Activity 13: Performing NLP for Sentiment Analysis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset containing a set of 1,000 product reviews from Amazon, which
    is paired with a label of 0 (for negative reviews) or 1 (for positive reviews).
    Separate the data into two variables – one containing the reviews and the other
    containing the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the punctuation from the reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable containing the vocabulary of the entire set of reviews. Additionally,
    create a dictionary that maps each word to an integer, where the words will be
    the keys and the integers will be the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode the reviews data by replacing each word in a review for its paired integer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a class containing the architecture of the network. Make sure that you
    include an embedding layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the model using 64 embedding dimensions and 128 neurons for 3 LSTM
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function, an optimization algorithm, and the number of epochs
    to train for. For example, you can use the binary cross-entropy loss as the loss
    function, the Adam optimizer, and train for 10 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `for` loop that goes through the different epochs and through every
    single review individually. For each review, perform a prediction, calculate the
    loss function, and update the parameters of the network. Additionally, calculate
    the accuracy of the network over that training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the progress of the loss function and accuracy over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output plot should look as follows:![Figure 6.25: Plot displaying the progress
    of the loss function](img/C11865_06_25.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.25: Plot displaying the progress of the loss function'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.26: Plot displaying progress of accuracy](img/C11865_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Plot displaying progress of accuracy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
