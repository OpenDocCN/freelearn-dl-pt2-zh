["```py\n    import numpy as np\n    def feed_forward(inputs, outputs, weights): \n    ```", "```py\n     pre_hidden = np.dot(inputs,weights[0])+ weights[1] \n    ```", "```py\n     hidden = 1/(1+np.exp(-pre_hidden)) \n    ```", "```py\n     pred_out = np.dot(hidden, weights[2]) + weights[3] \n    ```", "```py\n     mean_squared_error = np.mean(np.square(pred_out - outputs))\n        return mean_squared_error \n    ```", "```py\n    def tanh(x):\n        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) \n    ```", "```py\n    def relu(x):      \n        return np.where(x>0,x,0) \n    ```", "```py\n    def linear(x):      \n        return x \n    ```", "```py\n    def softmax(x):      \n        return np.exp(x)/np.sum(np.exp(x)) \n    ```", "```py\ndef mse(p, y):  \n    return np.mean(np.square(p - y)) \n```", "```py\ndef mae(p, y):      \n    return np.mean(np.abs(p-y)) \n```", "```py\ndef binary_cross_entropy(p, y):     \n    return -np.mean((y*np.log(p)+(1-y)*np.log(1-p))) \n```", "```py\ndef categorical_cross_entropy(p, y):        \n    return -np.mean(np.log(p[np.arange(len(y)),y])) \n```", "```py\n    from copy import deepcopy\n    import numpy as np\n    def feed_forward(inputs, outputs, weights):\n        pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n        hidden = 1/(1+np.exp(-pre_hidden))\n        pred_out = np.dot(hidden, weights[2]) + weights[3]\n        mean_squared_error = np.mean(np.square(pred_out - outputs))\n        retur mean_squared_error \n    ```", "```py\n    def update_weights(inputs, outputs, weights, lr): \n    ```", "```py\n     original_weights = deepcopy(weights)\n        temp_weights = deepcopy(weights)\n        updated_weights = deepcopy(weights) \n    ```", "```py\n     original_loss = feed_forward(inputs, outputs, original_weights) \n    ```", "```py\n     for i, layer in enumerate(original_weights): \n    ```", "```py\n     for index, weight in np.ndenumerate(layer): \n    ```", "```py\n     temp_weights = deepcopy(weights)\n                temp_weights[i][index] += 0.0001\n                _loss_plus = feed_forward(inputs, outputs, temp_weights) \n    ```", "```py\n     grad = (_loss_plus - original_loss)/(0.0001) \n    ```", "```py\n     updated_weights[i][index] -= grad*lr \n    ```", "```py\n     return updated_weights, original_loss \n    ```", "```py\n    from copy import deepcopy\n    import numpy as np\n    x = np.array([[1,1]])\n    y = np.array([[0]]) \n    ```", "```py\n    W = [\n        np.array([[-0.0053, 0.3793],\n                  [-0.5820, -0.5204],\n                  [-0.2723, 0.1896]], dtype=np.float32).T,\n        np.array([-0.0140, 0.5607, -0.0628], dtype=np.float32),\n        np.array([[ 0.1528,-0.1745,-0.1135]],dtype=np.float32).T,\n        np.array([-0.5516], dtype=np.float32)\n    ] \n    ```", "```py\n    def feed_forward(inputs, outputs, weights):\n        pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n        hidden = 1/(1+np.exp(-pre_hidden))\n        pred_out = np.dot(hidden, weights[2]) + weights[3]\n        mean_squared_error = np.mean(np.square(pred_out - outputs))\n        return mean_squared_error \n    ```", "```py\n    def update_weights(inputs, outputs, weights, lr):\n        original_weights = deepcopy(weights)\n        temp_weights = deepcopy(weights)\n        updated_weights = deepcopy(weights)\n        original_loss = feed_forward(inputs, outputs, original_weights)\n        for i, layer in enumerate(original_weights):\n            for index, weight in np.ndenumerate(layer):\n                temp_weights = deepcopy(weights)\n                temp_weights[i][index] += 0.0001\n                _loss_plus = feed_forward(inputs, outputs, temp_weights)\n                grad = (_loss_plus - original_loss)/(0.0001)\n                updated_weights[i][index] -= grad*lr\n        return updated_weights, original_loss \n    ```", "```py\n    losses = []\n    for epoch in range(100):\n        W, loss = update_weights(x,y,W,0.01)\n        losses.append(loss) \n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    plt.plot(losses)\n    plt.title('Loss over increasing number of epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss value') \n    ```", "```py\n    [array([[ 0.01424004, -0.5907864 , -0.27549535],\n            [ 0.39883757, -0.52918637, 0.18640439]], dtype=float32),\n     array([ 0.00554004, 0.5519136 , -0.06599568], dtype=float32),\n     array([[ 0.3475135 ],\n            [-0.05529078],\n            [ 0.03760847]], dtype=float32),\n     array([-0.22443289], dtype=float32)] \n    ```", "```py\n    pre_hidden = np.dot(x,W[0]) + W[1]\n    hidden = 1/(1+np.exp(-pre_hidden))\n    pred_out = np.dot(hidden, W[2]) + W[3]\n    # -0.017 \n    ```", "```py\n    x = [[1],[2],[3],[4]]\n    y = [[3],[6],[9],[12]] \n    ```", "```py\n    from copy import deepcopy\n    import numpy as np\n    def feed_forward(inputs, outputs, weights):\n        pred_out = np.dot(inputs,weights[0])+ weights[1]\n        mean_squared_error = np.mean(np.square(pred_out - outputs))\n        return mean_squared_error \n    ```", "```py\n    def update_weights(inputs, outputs, weights, lr):\n        original_weights = deepcopy(weights)\n        org_loss = feed_forward(inputs, outputs,original_weights)\n        updated_weights = deepcopy(weights)\n        for i, layer in enumerate(original_weights):\n            for index, weight in np.ndenumerate(layer):\n                temp_weights = deepcopy(weights)\n                temp_weights[i][index] += 0.0001\n                _loss_plus = feed_forward(inputs, outputs, temp_weights)\n                grad = (_loss_plus - org_loss)/(0.0001)\n                updated_weights[i][index] -= grad*lr\n        return updated_weights \n    ```", "```py\n    W = [np.array([[0]], dtype=np.float32),\n         np.array([[0]], dtype=np.float32)] \n    ```", "```py\n    weight_value = []\n    for epx in range(1000):\n        W = update_weights(x,y,W,0.01)\n        weight_value.append(W[0][0][0]) \n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    epochs = range(1, 1001)\n    plt.plot(epochs,weight_value)\n    plt.title('Weight value over increasing \\\n    epochs when learning rate is 0.01')\n    plt.xlabel('Epochs')\n    plt.ylabel('Weight value') \n    ```", "```py\ndef update_weights(inputs, outputs, weights, lr):\n    original_weights = deepcopy(weights)\n    org_loss = feed_forward(inputs, outputs, original_weights)\n    updated_weights = deepcopy(weights)\n    for i, layer in enumerate(original_weights):\n        for index, weight in np.ndenumerate(layer):\n            temp_weights = deepcopy(weights)\n            temp_weights[i][index] += 0.0001\n            _loss_plus = feed_forward(inputs, outputs, temp_weights)\n            grad = (_loss_plus - org_loss)/(0.0001)\n            updated_weights[i][index] -= grad*lr\n            **if****(i %** **2** **==** **0****):**\n**print****(****'weight value:'****, \\**\n **np.****round****(original_weights[i][index],****2****), \\**\n**'original loss:'****, np.****round****(org_loss,****2****), \\**\n**'loss_plus:'****, np.****round****(_loss_plus,****2****), \\**\n**'gradient:'****, np.****round****(grad,****2****), \\**\n**'updated_weights:'****, \\**\n **np.****round****(updated_weights[i][index],****2****))**\n    return updated_weights \n```", "```py\nW = [np.array([[0]], dtype=np.float32),\n     np.array([[0]], dtype=np.float32)]\nweight_value = []\nfor epx in range(10):\n    W = update_weights(x,y,W,0.01)\n    weight_value.append(W[0][0][0])\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nepochs = np.arange(1,11)\nplt.plot(epochs, weight_value)\nplt.title('Weight value over increasing epochs \\n when learning rate is 0.01')\nplt.xlabel('Epochs')\nplt.ylabel('Weight value')\nplt.subplot(122)\nplt.plot(epochs, loss_value)\nplt.title('Loss value over increasing epochs \\n when learning rate is 0.01')\nplt.xlabel('Epochs')\nplt.ylabel('Loss value') \n```"]