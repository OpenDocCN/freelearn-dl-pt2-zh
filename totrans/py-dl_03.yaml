- en: Chapter 3. Deep Learning Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](part0016_split_000.html#F8901-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 1. Machine Learning – An Introduction"), *Machine Learning – An Introduction*,
    we introduced machine learning and some of its applications, and we briefly talked
    about a few different algorithms and techniques that can be used to implement
    machine learning. In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we concentrated on neural networks;
    we have shown that 1-layer networks are too simple and can only work on linear
    problems, and we have introduced the Universal Approximation Theorem, showing
    how 2-layer neural networks with just one hidden layer are able to approximate
    to any degree any continuous function on a compact subset of *R* *[n]*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce deep learning and deep neural networks, that
    is, neural networks with at least two or more hidden layers. The reader may wonder
    what is the point of using more than one hidden layer, given the Universal Approximation
    Theorem, and this is in no way a naïve question, since for a long period the neural
    networks used were very shallow, with just one hidden layer. The answer is that
    it is true that 2-layer neural networks can approximate any continuous function
    to any degree, however, it is also true that adding layers adds levels of complexity
    that may be much harder and may require many more neurons to simulate with shallow
    networks. There is also another, more important, reason behind the term *deep*
    of deep learning that refers not just to the depth of the network, or how many
    layers the neural net has, but to the level of "learning". In deep learning, the
    network does not simply learn to predict an output *Y* given an input *X*, but
    it also understands basic features of the input. In deep learning, the neural
    network is able to make abstractions of the features that comprise the input examples,
    to understand the basic characteristics of the examples, and to make predictions
    based on those characteristics. In deep learning, there is a level of abstraction
    that is missing in other basic machine learning algorithms or in shallow neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamental concepts of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU versus CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular open source libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton published an article
    titled *ImageNet Classification with Deep Convolutional Neural Networks* in Proceedings
    of Neural Information Processing Systems (NIPS) (2012) and, at the end of their
    paper, they wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"It is notable that our network''s performance degrades if a single convolutional
    layer is removed. For example, removing any of the middle layers results in a
    loss of about 2% for the top-1 performance of the network. So the depth really
    is important for achieving our results."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this milestone paper, they clearly mention the importance of the number
    of hidden layers present in deep networks. Krizheysky, Sutskever, and Hilton talk
    about convolutional layers, and we will not discuss them until [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*, but the basic question remains:
    *What do those hidden layers do?*'
  prefs: []
  type: TYPE_NORMAL
- en: A typical English saying is *a picture is worth a thousand words*. Let's use
    this approach to understand what Deep Learning is. In H. Lee, R. Grosse, R. Ranganath,
    and A. Ng, *Convolutional deep belief networks for scalable unsupervised learning
    of hierarchical representations* in Proceedings of International Conference on
    Machine Learning (ICML) (2009) (Refer to [http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf))
    the authors use a few images, which we copy here.
  prefs: []
  type: TYPE_NORMAL
- en: '![What is deep learning?](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In their example, they showed neural network pictures of different categories
    of objects and/or animals, and the network learned some basic features for each
    category. For instance, the network can learn some very basic shapes, like lines
    or edges, which are common to every category. In the next layer, however, the
    network can learn how those lines and edges fit together for each category to
    make images that feature the eyes of a face, or the wheels of a car. This is similar
    to how the visual cortex in humans works, where our brain recognizes features
    more and more complex starting from simple lines and edges.
  prefs: []
  type: TYPE_NORMAL
- en: '![What is deep learning?](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layers in a deep neural network work similarly by understanding
    more and more complex features in each hidden layer. If we want to define what
    makes a face, we need to define its parts: the eyes, the nose, the mouth, and
    then we need to go a level up and define their position with respect to each other:
    the two eyes are on the top middle part, at the same height, the nose in the middle,
    the mouth in the lower middle part, below the nose. Deep neural networks catch
    these features by themselves, first learning the components of the image, then
    its relative position and so on, similarly to how, in Images 1 and 2, we see the
    level of deeper abstraction working in each layer. Some deep learning networks
    can, in fact, be considered generative algorithms, as in the case of **Restricted
    Boltzmann Machines** (**RBMs**), rather than simply a predictive algorithm, as
    they learn to generate a signal, and then they make the prediction based on the
    generation assumptions they have learned. As we will progress through this chapter,
    we will make this concept clearer.'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 1801, Joseph Marie Charles invented the Jacquard loom. Charles named the
    Jacquard, hence the name of its invention, was not a scientist, but simply a merchant.
    The Jacquard loom used a set of punched cards, and each punched card represented
    a pattern to be reproduced on the loom. Each punched card represented an abstraction
    of a design, a pattern, and each punched card was an abstract representation of
    that pattern. Punched cards have been used afterwards, for example in the tabulating
    machine invented by Herman Hollerith in 1890, or in the first computers where
    they were used to feed code to the machine. However, in the tabulating machine,
    for example, punched cards were simply abstractions of samples to be fed into
    the machine to calculate statistics on a population. In the Jacquard loom, the
    use of punched cards was subtler; in it, each card represented the abstraction
    of a pattern that could then be combined together with others to create more complex
    patterns. The punched card is an abstract representation of a feature of a reality,
    the final weaved design.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a way, the Jacquard loom had the seed of what makes deep learning today:
    the definition of a reality through the representations of its features. In deep
    learning, the neural network does not simply recognize what makes a cat a cat,
    or a squirrel a squirrel, but it understands what features are present in a cat
    and what features are present in a squirrel, and it learns to *design* a cat or
    a squirrel using those features. If we were to design a weaving pattern in the
    shape of a cat using a Jacquard loom, we would need to use punched cards that
    have *moustaches* on the nose, like those of a cat, and an elegant and slender
    body. Instead, if we were to design a squirrel, we would need to use the punched
    card that makes a furry tail, for example. A deep network that learns basic representations
    of its output can make classifications using the assumptions it has made; therefore,
    if there is no furry tail it will probably not be a squirrel, but rather a cat.
    This has many implications, as we will see, not least that the amount of information
    that the network learns is much more complete and robust. By learning to *generate*
    the model (in technical parlance by learning the joint probability *p(x,y)* rather
    than simply *p(y|x)*, the network is much less sensitive to *noise*, and it learns
    to recognize images even when there are other objects present in the scene or
    the object is partially obscured. The most exciting part is that deep neural networks
    learn to do this automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ising model was invented by the physicist Wilhelm Lenz in1920, and he gave
    it as a problem to his student Ernst Ising. The model consists of discrete variables
    that can be in two states (positive or negative) and that represent magnetic dipoles.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning*,
    we will introduce Restricted Boltzmann machines and auto-encoders, and we will
    start going deeper into how to build multi-layer neural networks. The type of
    neural networks that we have seen so far all have a feed-forward architecture,
    but we will see that we can define networks with a feedback loop to help tune
    the weights that define the neural network. Ising models, though not directly
    used in deep learning, are a good physical example that helps us understand the
    basic inner workings of tuning deep neural architectures, including Restricted
    Boltzmann machines, and in particular help us understand the concept of representation.
  prefs: []
  type: TYPE_NORMAL
- en: What we are going to discuss in this section is a simple adaption (and simplification)
    of the Ising model to deep learning. In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we discussed how important it
    is to tune the weights of the connections between neurons. In fact, it is the
    weights in a neural network that make the network learn. Given an input (fixed),
    this input propagates to the next layer and sets the internal state of the neurons
    in the next layer based on the weights of their connections. Then, these neurons
    will fire and move the information over to the following layer through new connections
    defined by new weights, and so on. The weights are the only variables of the network,
    and they are what make the network learn. In general, if our activity function
    were a simple threshold function, a large positive weight would tend to make two
    neurons fire together. By firing together, we mean that, if one neuron fires,
    and the connecting weight is high, then the other neuron will also fire (since
    the input times the large connecting weight will likely make it over the chosen
    threshold). In fact, in 1949, in his *The organization of behavior*, Donald Hebb
    ([http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf](http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf))
    proposed that the opposite should also be true. Donald Hebb was a Canadian psychologist,
    who lived during the 20th century, who proposed the rule that goes by his name,
    the Hebb rule, which says that when neurons fire together their connection strengthens;
    when they do not fire together, their connection weakens.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we think of an Ising model as a network of neurons
    that acts in a binary way, that is, where they can only activate (fire) or not,
    and that, the stronger their relative connection, the more likely they are to
    fire together. We assume that the network is stochastic, and therefore if two
    neurons are strongly connected they are only very likely to fire together.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stochastic means probabilistic. In a stochastic network, we define the probability
    of a neuron to fire: the higher the probability, the more likely the neuron is
    to fire. When two neurons are strongly connected, that is, they are connected
    by a large weight, the probability that one firing will induce the other one to
    fire as well is very high (and vice versa, a weak connection will give a low probability).
    However, the neuron will only fire according to a probability, and therefore we
    cannot know with certainty whether it will fire.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if they are inversely correlated (a large negative weight),
    they are very likely not to fire together. Let''s show some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large and positive, so the third neuron will also be
    active. In the second figure, the first two neurons are off, and their connection
    with the third neuron is positive, so the third neuron will also be off.
  prefs: []
  type: TYPE_NORMAL
- en: In the second figure, the first two neurons are off, and their connection with
    the third neuron is positive, so the third neuron will also be off.
  prefs: []
  type: TYPE_NORMAL
- en: There are several combinations that may be present; we will show only a few
    of them. The idea is that the state of the neurons in the first layer will probabilistically
    determine the state of the neurons in the following layer, depending on the sign
    and strength of the connection. If the connections are weak, the connected neurons
    in the following layer may have equal or almost equal probability to be in any
    state. But if the connections are very strong, then the sign of the weight will
    make the connected neurons act in a similar or opposite way. Of course, if the
    neuron on the second layer has more than one neuron as its input, we will weigh
    all the input connections as usual. And if the input neurons are not all on or
    off, and their connections are equally strong, then again, the connected neuron
    may have equal or an almost equal chance of being on or off.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large and negative, so the third neuron will also be
    off. In the second figure, the first two neurons are off, and their connection
    with the third neuron is large and negative, so the third neuron will likely be
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is then clear that, to most likely determine the state of the neurons in
    the following layers, the neurons in the first layer should all be in similar
    states (on or off) and all be connected with strong (that is, large weights) connections.
    Let''s see more examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large but opposite, so the third neuron could be equally
    likely to be on or off. In the second figure, the first two neurons are, one on
    and one off, and their connections with the third neuron are both large and positive,
    so the third neuron will also be equally likely to be on or off. In the last figure,
    the first two neurons are active, but their connection with the third neuron is
    small, so the third neuron is slightly more likely to be on but it has a relatively
    high chance to be off as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point of introducing this adaptation of the Ising model is to understand
    how representation learning works in deep neural networks. We have seen that setting
    the correct weights can make a neural network turn on or off certain neurons,
    or in general, affect their output. Picturing neurons in just two states, however,
    helps our intuition and our visual description of what goes on in a neural network.
    It also helps our visual intuition to represent our network layers in 2-dimensions,
    rather than as a 1-dimensional layer. Let''s picture our neural network layer
    as in a 2-dimensional plane. We could then imagine that each neuron represents
    the pixel on a 2-dimensional image, and that an "on" neuron represents a (visible)
    dark dot on a white plane, while an "off" neuron blends in (invisibly) with the
    white background. Our input layer of on/off neurons can then be seen as a simple
    2-dimensional black and white image. For example, let''s suppose we want to represent
    a smiley face, or a sad face—we would just turn on (activate) the correct neurons
    to get the following figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A happy and a sad face: the difference lies in a few neurons on the side of
    the mouth that can be on or off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s suppose that this corresponds to the input layer, so this layer
    would be connected to another layer, one of the hidden layers. There would then
    be connections between each pixel in this image (both black and white) and each
    neuron in the following layer. In particular, each black (on) pixel would be connected
    to each neuron in the following layer. Let''s now assume that the connections
    from each neuron making the left eye has a strong (large positive weight) connection
    to a particular pixel in the hidden layer, but it has a large negative connection
    to any other neuron in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On the left a smiley face, on the right, the same smiley face and the connections
    between its left eye and a hidden neuron.
  prefs: []
  type: TYPE_NORMAL
- en: What this means is that if we set large positive weights between the hidden
    layer and the left eye, and large negative connections between the left eye and
    any other hidden neuron, whenever we show the network a face that contains a left
    eye (which means those neurons are on) this particular hidden neuron will activate,
    while all the other neurons will tend to stay off. This means that this particular
    neuron will be able to detect when a left eye is present or not. We can similarly
    create connections between the right eye, the nose and the main part of the mouth,
    so that we can start detecting all those face features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Each face feature, the eyes, the nose and the mouth, has large positive connections
    with certain hidden neurons and large but negative connections with the others.
  prefs: []
  type: TYPE_NORMAL
- en: This shows how we can select weights for our connections, to have the hidden
    neurons start recognizing features of our input.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an important reminder, we want to point out to the reader that, in fact,
    we do not select the weights for our connections to start recognizing features
    of the input. Instead, those weights are automatically selected by the network
    using back-propagation or other tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can have more hidden layers that can recognize features of the
    features (*is the mouth in our face smiling or is it sad?*) and therefore get
    more precise results.
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages to deep learning. The first is, as we have seen,
    that it can recognize features. Another, even more important, is that it will
    recognize features automatically. In this example, we have set the weights ourselves
    to recognize the features we chose. This is one of the disadvantages of many machine
    learning algorithms, that the user must use his/her own experience to select what
    he/she thinks are the best features. A lot of time goes therefore, into feature
    selection that must still be performed by a human being. Deep Learning algorithms,
    instead, automatically select the best features. This can be done, as we have
    seen in the previous chapter, using back-propagation, but in fact, other techniques
    also exist to select those weights, and those will be the important points that
    will be treated in the next chapter, such as auto-encoders and restricted Boltzmann
    machines (or *Harmoniums*, as Paul Smolensky, who invented them in 1986, called
    them). We should, however, also caution the reader that the advantage we get from
    the automatic feature selection has to pay the price of the fact that we need
    to choose the correct architecture for the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In some deep learning systems (for example in Restricted Boltzmann Machines,
    as we will see in the next chapter), the neural network can also learn to "repair"
    itself. As we mentioned in the previous example, we could produce a general face
    by activating the four neurons we have associated to the right/left eye, the nose,
    and the mouth respectively. Because of the large positive weights between them
    and the neurons in the previous layer, those neurons will turn on and we will
    have the neurons corresponding to those features activated, generating a general
    image of a face. At the same time, if the neurons corresponding to the face are
    turned on, the four corresponding neurons to the eyes, nose, and mouth will also
    activate. What this means is that, even if not all the neurons defining the face
    are on, if the connections are strong enough, they may still turn on the four
    corresponding neurons, which, in turn, will activate the missing neurons for the
    face.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has one more extra advantage: robustness. Human vision can recognize objects
    even when the view is partly obscured. We can recognize people even when they
    wear a hat, or a scarf that covers their mouth; we are not sensitive to noise
    in the image. Similarly, when we create this correspondence, if we alter the face
    slightly, for example by modifying the mouth by one or two pixels, the signal
    would still be strong enough to turn the "mouth" neuron on, which in turn would
    turn on the correct pixel and off the wrong pixel making up the modified eye.
    This system is not sensitive to noise and can make auto-corrections.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say, for example, that the mouth has a couple of pixels off (in the figure
    those are the pixels that have an **x**).
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This image has a couple of pixels making the mouth that are not turned on.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the mouth may still have enough neurons in the right place to be able
    to turn on the corresponding neuron representing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Even though a couple of neurons are off, the connections with the other neurons
    are strong enough that the neuron representing the mouth in the next layer will
    turn on anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we could now travel the connections backwards and whenever
    the neuron representing the mouth is on, this would turn on all the neurons comprising
    the mouth, including the two neurons that were previously off:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The two neurons have been activated by the neuron on top
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, deep learning advantages with respect to many other machine learning
    algorithms and shallow neural networks in particular are:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning can learn representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is less sensitive to noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning can be a generative algorithm (more on this in the next chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To further understand why many hidden layers may be necessary, let's consider
    the task of recognizing a simple geometric figure, a cube. Say that each possible
    line in 3D is associated with a neuron (let's forget for a moment that this will
    require an infinite number of neurons).
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Each line on the same visual field is associated to a different neuron.
  prefs: []
  type: TYPE_NORMAL
- en: If we restrict ourselves to a single eye, lines at different angles in our vision
    will project to the same line on a 2-dimensional plane. Each line we see could
    therefore be given by any corresponding 3D-line that projects to the same line
    onto the retina. Assume that any possible 3D-line is associated to a neuron. Two
    distinct lines that make up the cube are therefore associated to a family of neurons
    each. However, the fact that these two lines intersect, allows us to create a
    connection between two neurons each belonging to a different family. We have many
    neurons for the line making up one of the edges of the cube, and many other neurons
    for the line making up another of the edges of the cube, but since those two lines
    intersect, there are two neurons that will be connected. Similarly, each of these
    lines connects to other lines that make up the cube, allowing us to further redefine
    our representation. At a higher level, our neural net can also start to identify
    that these lines are not connected at just any angle, but they are connected at
    exactly 90 degree angles. This way we can make increasingly more abstract representations
    that allow us to identify the set of lines drawn on a piece of paper as a cube.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons in different layers, organized hierarchically, represent different levels
    of abstraction of basic elements in the image and how they are structured. This
    toy example shows that each layer can, in an abstract system, link together what
    different neurons at a lower level are seeing, making connections between them,
    similar to how we can make connections between abstract lines. It can, using those
    connections, realize that those abstract lines are connected at a point, and,
    in a further up layer, are in fact connected at 90 degrees and make up a cube,
    the same way we described how we can learn to recognize a face by recognizing
    the eyes, the nose, and the mouth, and their relative position.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature learning](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Each line is associated with a neuron, and we can create basic representations
    by associating neurons that represent lines that intersect, and more complex representations
    by associating neurons that represent lines at specific angles.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous paragraph, we have given an intuitive introduction to deep
    learning. In this section, we will give a more precise definition of key concepts
    that will be thoroughly introduced in the next chapters. Deep Neural Networks
    with many layers also have a biological reason to exist: through our study of
    how humans understand speech, it has in fact become clear that we are endowed
    with a layered hierarchical structure that transforms the information from the
    audible sound input into the linguistic level. Similarly, the visual system and
    the visual cortex have a similar layered structure, from the V1 (or striate cortex),
    to the V2, V3 and V4 visual area in the brain. Deep neural networks mimic the
    nature of our brains, though in a very primitive way. We should warn the reader,
    however, that while understanding our brain can help us create better artificial
    neural networks, in the end, we may be creating a completely different architecture,
    the same way we may have created airplanes by trying to mimic birds, but ended
    up with a very different model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we introduced the back-propagation
    algorithm as a popular training algorithm. In practice, when we have many layers,
    back-propagation may be a slow and difficult algorithm to use. Back-propagation,
    in fact, is mainly based on the gradient of the function, and often the existence
    of local minima may prevent convergence of the method. However, the term deep
    learning applies to a class of deep neural networks algorithms that may use different
    training algorithms and weight tuning, and they are not limited to back-propagation
    and classical feed-forward neural networks. We should then more generally define
    deep learning as a class of machine learning techniques where information is processed
    in hierarchical layers, to understand representations and features from the data
    in increasing levels of complexity. In this class of algorithms, we can generally
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-Layer Perceptrons (MLP)**: A neural network with many hidden layers,
    with feed-forward propagation. As discussed, this is one of the first examples
    of deep learning network but not the only possible one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boltzmann Machines (BM)**: A stochastic symmetric network with a well-defined
    energy function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Restricted Boltzmann Machines (RBM)**: Similar to the Ising model example
    above, restricted Boltzmann machines are comprised of symmetric connections between
    two layers, one visible and one hidden layer, but unlike general Boltzmann machines,
    neurons have no intra-layers connections. They can be stacked together to form
    DBNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Belief Networks (DBN)**: a stochastic generative model where the top
    layers have symmetric connections between them (undirected, unlike feed-forward
    networks), while the bottom layers receive the processed information from directed
    connections from the layers above them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders**: A class of unsupervised learning algorithms in which the
    output shape is the same as the input, that allows the network to better learn
    basic representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks (CNN)**: Convolutional layers apply filters
    to the input image (or sound) by sliding this filter all across the incoming signal
    to produce a bi-dimensional activation map. CNNs allow the enhancement of features
    hidden in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these deep learning implementations has its own advantages and disadvantages,
    and they can be easier or harder to train depending on the number of layers and
    neurons in each layer. While simple feed-forward Deep Neural Networks can generally
    be trained using the back-propagation algorithm discussed in the second chapter,
    different techniques exist for the other types of networks, as will be discussed
    further in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next couple of paragraphs, we will discuss how deep neural networks have
    applications in the field of speech recognition and computer vision, and how their
    application in recent years has vastly improved accuracy in these two fields by
    completely outperforming many other machine learning algorithms not based on deep
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has started to be used in speech recognition starting in this
    decade (2010 and later, see for example the 2012 article titled *Deep Neural Networks
    for Acoustic Modeling in Speech Recognition* by Hinton et al., available online
    at [http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf));
    until then, speech recognition methods were dominated by algorithms called GMM-HMM
    methods (Hidden Markov Models with Gaussian Mixture Emission). Understanding speech
    is a complex task, since speech is not, as is naively thought, made up of separate
    words with clear boundaries between them. In reality, in speech there are no really
    distinguishable parts, and there are no clear boundaries between spoken words.
    In studying sounds when composing words, we often look at so-called triphones,
    which are comprised of three regions where the first part depends on the previous
    sound, the middle part is generally stable, and the next one depends on the following
    sound. In addition, typically it is better to detect only parts of a triphone,
    and those detectors are called senones.
  prefs: []
  type: TYPE_NORMAL
- en: In *Deep Neural Networks for Acoustic Modeling in Speech Recognition*, several
    comparisons were made between the then state-of-the art models and the model used
    by the authors that comprised five hidden layers with 2048 units per layer. The
    first comparison was using the Bing voice search application, achieving a 69.6%
    accuracy vs. a 63.8% accuracy with a classical method, named the GMM-HMM model,
    on 24 hours of training data. The same model was also tested on the Switchboard
    speech recognition task, a public speech-to-text transcription benchmark (similar
    to the MNIST dataset used for digit recognition) that includes about 2500 conversations
    by 500 speakers from around the US. In addition, tests and comparisons were performed
    using Google Voice input speech, YouTube data, and English Broadcast News speech
    data. In the next table, we summarize the results from the article, showing the
    error rate for DNN versus GMM-HMM.
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Total number of hours of training data | DNN(error rate) | GMM-HMM
    with same training(error rate) | GMM-HMM with longer training(error rate) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Switchboard (test1) | 309 | 18.5 | 27.4 | 18.6 (2000hrs) |'
  prefs: []
  type: TYPE_TB
- en: '| Switchboard (test2) | 309 | 16.1 | 23.6 | 17.1 (2000hrs) |'
  prefs: []
  type: TYPE_TB
- en: '| English Broadcast News | 50 | 17.5 | 18.8 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Bing Voice Search | 24 | 30.4 | 36.2 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Google Voice | 5870 | 12.3 |   | 16.0 (>>5870hrs) |'
  prefs: []
  type: TYPE_TB
- en: '| YouTube | 1400 | 47.6 | 52.3 |   |'
  prefs: []
  type: TYPE_TB
- en: 'In another article, *New types of Deep Neural Network Learning for Speech Recognition
    and Related Applications: An overview*, by Deng, Hinton, and Kingsbury ([https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/](https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/)),
    the authors also notice how DNNs work particularly well for noisy speech.'
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of DNNs is that before DNNs, people had to create transformations
    of speech spectrograms. A spectrogram is a visual representation of the frequencies
    in a signal. By using DNNs, these neural networks can autonomously and automatically
    pick primitive features, in this case represented by primitive spectral features.
    Use of techniques such as convolution and pooling operations, can be applied on
    this primitive spectral feature to cope with typical speech variations between
    speakers. In recent years, more sophisticated neural networks that have recurrent
    connections (RNN) have been employed with great success (A. Graves, A. Mohamed
    and G. Hinton , *Speech Recognition with Deep Recurrent Neural Networks* in Proceedings
    of International Conference on Acoustic Speech and Signal Processing(ICASSP) (2013);
    refer to [http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf))
    for example, a particular type of deep neural network called **LSTM** (**long
    short-term memory** neural network) that will be described in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we have discussed different
    activity functions, and although the logistic sigmoid and the hyperbolic tangent
    are often the best known, they are also often slow to train. Recently, the ReLU
    activity function has been used successfully in speech recognition, for example
    in an article by G. Dahl, T. Sainath, and G. Hinton in *Improving Deep Neural
    Networks for LVCSR Using Rectified Linear Units and Dropout* in Proceeding of
    International Conference on Acoustics Speech and Signal Processing (ICASSP) (2013)
    ([http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf](http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf)).
    In [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*, we will also mention the
    meaning of "Dropout", as discussed in this paper (and also mentioned in its title).
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition and classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is perhaps the area where deep neural networks, success is best documented
    and understood. As in speech recognition, DNNs can discover basic representations
    and features automatically. In addition, handpicked features were often able to
    capture only low-level edge information, while DNNs can capture higher-level representations
    such as edge intersections. In 2012, results from the ImageNet Large Scale Visual
    Recognition Competition (the results are available online at [http://image-net.org/challenges/LSVRC/2012/results.html](http://image-net.org/challenges/LSVRC/2012/results.html))
    showed the winning team, composed of Alex Krizhevsky, Ilya Sutskever, and Geoff
    Hinton, using a large network with 60 million parameters and 650,000 neurons with
    five convolutional layers and followed by max-pooling layers, beat the second
    placed team with an error rate of 16.4% versus an error rate of 26.2%. Convolutional
    layers and max-pooling layers will be the focus of [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*. It was a huge and impressive
    result, and that breakthrough result sparked the current renaissance in neural
    networks. The authors used many novel ways to help the learning process by bringing
    together convolutional networks, use of the GPU, and some tricks like dropout
    methods and the use of the ReLU activity function instead of the sigmoid.
  prefs: []
  type: TYPE_NORMAL
- en: The network was trained using GPUs (we will talk about GPU advantages in the
    next section) and showed how a large amount of labeled data can greatly improve
    the performance of deep learning neural nets, greatly outperforming more conventional
    approaches to image recognition and computer vision. Given the success of convolutional
    layers in deep learning, Zeiler and Fergus in two articles (M. Zeiler and R. Fergus,
    *Stochastic pooling for regularization of deep convolutional neural networks*,
    in Proceeding of International Conference on Learning Representations (ICLR),
    2013 ([http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf](http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf))
    and M. Zeiler and R. Fergus, *Visualizing and Understanding Convolutional Networks*,
    arXiv:1311.2901, pages 1-11, 2013, ([http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf)))
    tried to understand why using convolutional networks in deep learning worked so
    well, and what representations were being learned by the network. Zeiler and Fergus
    set to visualize what the intermediate layers captured by mapping back their neural
    activities. They created a de-convolutional network attached to each layer, providing
    a loop back to the image pixels of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Object recognition and classification](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Image taken from M. Zeiler and R. Fergus, Visualizing and Understanding Convolutional
    Networks
  prefs: []
  type: TYPE_NORMAL
- en: The article shows what features are being revealed, where **layer 2** shows
    corner and edges, **layer 3**, different mesh patterns, **layer 4**, dog faces
    and bird legs, and **layer 5** shows entire objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![Object recognition and classification](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Image taken from M. Zeiler and R. Fergus, Visualizing and Understanding Convolutional
    Networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning can also be used in unsupervised learning by using networks that
    incorporate RBMs and autoencoders. In an article by Q. Le, M. Ranzato, M. Devin,
    G. Corrado, K. Chen, J. Dean, and A. Ng, *Building high-level features using large
    scale unsupervised learning*, in Proceedings of International Conference on Machine
    Learning (ICML) ([http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf))
    the authors used a 9-layer network with autoencoders, with one billion connections
    trained on 10 million images downloaded from the Internet. The unsupervised feature
    learning allows the system to be trained to recognize faces without being told
    whether the image contains a face or not. In the article the authors state:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"It is possible to train neurons to be selective for high-level concepts using
    entirely unlabeled data … neurons functions as detectors for faces, human bodies,
    and cat faces by training on random frames of YouTube videos … starting from these
    representations we obtain 15.8% accuracy for object recognition on ImageNet with
    20,000 categories, a significant leap of 70% relative improvement over the state-of-the-art."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: GPU versus CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons for the popularity of deep learning today is the drastically
    increased processing capacity of **GPUs** (**Graphical Processing Units**). Architecturally,
    the **CPU** (**Central Processing Unit**) is composed of a few cores that can
    handle a few threads at a time, while GPUs are composed of hundreds of cores that
    can handle thousands of threads at the same time. A GPU is a highly parallelizable
    unit, compared to the CPU that is mainly a serial unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'DNNs are composed of several layers, and each layer has neurons that behave
    in the same manner. Moreover, we have discussed how the activity value for each
    neuron is , or, if we express it in matrix form, we have *a = wx*, where *a* and
    *x* are vectors and w a matrix. All activation values are calculated in the same
    way across the network. CPUs and GPUs have a different architecture, in particular
    they are optimized differently: CPUs are latency optimized and GPUs are bandwidth
    optimized. In a deep neural network with many layers and a large number of neurons,
    bandwidth becomes the bottleneck, not latency, and this is the reason why GPUs
    perform so much better. In addition, the L1 cache of the GPU is much faster than
    the L1 cache for the CPU and is also larger.'
  prefs: []
  type: TYPE_NORMAL
- en: The L1 cache represents memory of information that the program is likely to
    use next, and storing this data can speed up the process. Much of the memory gets
    re-used in deep neural networks, which is why L1 cache memory is important. Using
    GPUs, you can get your program, go up to one order of magnitude faster than simply
    using CPUs, and use of this speed-up is also the reason behind much of the recent
    progress in speech and image processing using deep neural networks, an increase
    in computing power that was not available a decade ago.
  prefs: []
  type: TYPE_NORMAL
- en: '![GPU versus CPU](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to be faster for DNN *training*, GPUs are also more efficient to
    run the DNN *inference*. Inference is the post-training phase where we deploy
    our trained DNN. In a whitepaper published by GPU vendor Nvidia titled *GPU-Based
    Deep Learning Inference: A Performance and Power Analysis*, available online at
    [http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf](http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf),
    an efficiency comparison is made between the use of GPUs and CPUs on the AlexNet
    network (a DNN with several convolutional layers) and the results are summarized
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Network: AlexNet | Batch Size | Tegra X1 (FP32) | Tegra X1 (FP16) | Core
    i7 6700K (FP32) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Inference performance | 1 | 47 img/sec | 67 img/sec | 62 img/sec |'
  prefs: []
  type: TYPE_TB
- en: '| Power | 5.5 W | 5.1 W | 49.7 W |'
  prefs: []
  type: TYPE_TB
- en: '| Performance/Watt | 8.6 img/sec/W | 13.1 img/sec/W | 1.3 img/sec/W |'
  prefs: []
  type: TYPE_TB
- en: '| Inference performance | 128 (Tegra X1)48 (Core i7) | 155 img/sec | 258 img/sec
    | 242 img/sec |'
  prefs: []
  type: TYPE_TB
- en: '| Power | 6.0 W | 5.7 W | 62.5 W |'
  prefs: []
  type: TYPE_TB
- en: '| Performance/Watt | 25.8 img/sec/W | 45 img/sec/W | 3.9 img/sec/W |'
  prefs: []
  type: TYPE_TB
- en: The results show that inference on Tegra X1 can be up to an order of magnitude
    more energy-efficient that CPU-based inference while achieving comparable performance
    levels
  prefs: []
  type: TYPE_NORMAL
- en: Writing code to access the GPU directly instead of the CPU is not easy, but
    that is why most popular open source libraries like Theano or TensorFlow allow
    you to simply turn on a simple *switch* in your code to use the GPU rather than
    the CPU. Use of these libraries does not require writing specialized code, but
    the same code can run on both the CPU and the GPU, if available. The *switch*
    depends on the open source library, but typically it can be through setting up
    determined environment variables or by creating a specialized resource (`.rc`)
    file that is used by the particular open source library chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Popular open source libraries – an introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several Open Source libraries available that allow the creation of
    deep neural nets in Python without having to explicitly write the code from scratch.
    The most commonly used are: Keras, Theano, TensorFlow, Caffe, and Torch. In this
    book we will provide examples using the first three libraries, which can all be
    used in Python. The reason for this is that Torch is not based on Python, but
    on a different language, called Lua, while Caffe is mainly used for Image recognition
    only. For these libraries, we will quickly describe how to turn on the GPU *switch
    we discussed in the previous paragraph. Much of the code in this book can then
    be run on a CPU or a GPU, depending on the hardware available to the reader.*'
  prefs: []
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theano ([http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))
    is an open source library written in Python that implements many features that
    make it easy to write code for neural networks. In addition, Theano makes it very
    easy to take advantage of GPU acceleration and performance. Without going into
    the details of how Theano works, Theano uses symbolic variables and functions.
    Among many features that are really appealing, Theano allows us to use back-propagation
    very easily by calculating all the derivatives for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Theano also makes it very easily to utilize the GPU on
    your machine. There are many ways to do this, but the simplest is to create a
    resource file called `.theanorc` with the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy to check whether Theano is configured to use your GPU by simply
    typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We refer to the Theano documentation for learning the first steps on how to
    use Theano, and we will implement some test code examples using Theano for deep
    learning in this book.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow ([https://www.tensorflow.org](https://www.tensorflow.org)) works
    very similarly to Theano, and in TensorFlow, computations are also represented
    as graphs. A TensorFlow graph is therefore a description of computations. In TensorFlow,
    you do not need to explicitly require the use of your GPU, rather, TensorFlow
    will automatically try to use your GPU if you have one, however if you have more
    than one GPU you must assign operations to each GPU explicitly, or only the first
    one will be used. To do this, you simply need to type the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the following devices can be defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"/cpu:0"`: main CPU of your machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"/gpu:0"`: first GPU of your machine, if one exists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"/gpu:1"`: second GPU of your machine, if it exists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"/gpu:2"`: third GPU of your machine, if it exists, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, we refer to the TensorFlow documentation for learning the first
    steps on how to use TensorFlow and test code examples using TensorFlow will be
    implemented in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras ([http://keras.io](http://keras.io)) is a neural net Python library that
    can run on top of either Theano or TensorFlow, even though it will run by default
    using TensorFlow. Instructions online are provided at [http://keras.io/backend/](http://keras.io/backend/).
    Keras can run on a CPU or GPU and to do that, if you run it on top of Theano,
    you will need to set up a `.theanorc` file as described before. Keras allows different
    ways to create deep neural networks, and it makes it easy by using a *model* for
    the neural network. The main type of *model* is the `Sequential` model which creates
    a linear stack of layers. You can then add new layers by simply calling the `add`
    function. In the coming section, we will create a few examples using Keras. Keras
    can be easily installed with the following, simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It can also be installed by forking from its Git repository and then running
    setup on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, we refer the reader to the online documentation for further information.
  prefs: []
  type: TYPE_NORMAL
- en: Sample deep neural net code using Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will introduce some simple code to use Keras for the correct
    classification of digits using the popular dataset MNIST. MNIST is a dataset comprised
    of 70,000 examples of handwritten digits by many different people. The first 60,000
    are typically used for training and the remaining 10,000 for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample deep neural net code using Keras](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample of digits taken from the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of Keras is that it can import this dataset for you without
    the need to explicitly download it from the web (Keras will download it for you).
    This can be achieved by one simple line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few classes we need to import from Keras to use a classical deep
    neural net, and these are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to start writing our code to import the data, and we can do
    this with just one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This imports the training data and the testing data; in addition, both datasets
    are divided into two subsets: one that contains the actual images and the other
    that contains the labels. We need to slightly modify the data to be able to use
    it. The `X_train` and `X_test` data in fact is comprised of 60000 small (28,28)-pixels
    images, but we want to reshape each sample to be a 784-pixels long vector, rather
    than a (28,28) 2-dimensional matrix. This can be easily accomplished with these
    two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the labels are indicating the value of the digit depicted by the
    images, and we want to convert this into a 10-entry vector with all zeroes and
    just one 1 in the entry corresponding to the digit, so for example 4 is mapped
    to [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, before calling our main function, we just need to set the size of
    our input (the size of the `mnist` images), how many hidden neurons our hidden
    layer has, for how many epochs we want to try our network, and the batch size
    for our training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to write the code for our main function. Keras works by defining
    a model, and we will use the `Sequential` model, then add layers (in this case
    we will use regular *dense*, not sparse layers) specifying the number of input
    and output neurons. For each layer, we specify the activity function of its neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras now provides a simple way to specify the cost function (the `loss`) and
    its optimization (training rate, momentum, and so on). We are not going to modify
    the default values, so we can simply pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the optimizer is `sgd`, which stands for stochastic gradient
    descent. At this point, we need to train the network, and this, similarly to scikit-learn,
    is done calling a `fit` function. We will use the verbose parameter so that we
    can follow the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All that is left to do is to add code to evaluate our network on the test data
    and print the accuracy result, which is done simply by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. It is now enough to run. The test accuracy will be about 94%,
    which is not a great result, but this example runs in less than 30 seconds on
    a CPU and is an extremely simple implementation. There are simple improvements
    that could be made, for example selecting a larger number of hidden neurons or
    selecting a larger number of epochs, and we leave those simple changes to the
    reader to familiarize himself or herself with the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras allows us to also look at the weight matrix it creates. To do that, it
    is enough to type the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'By adding the following lines to our previous code, we can look at what the
    hidden neurons have learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a clearer image, we have increased the number of epochs to 100 to get
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample deep neural net code using Keras](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Composite figure with what was learned by all the hidden neurons
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we have aggregated all the images for each neuron in a single
    figure that represents a composite for all the neurons. Clearly, since the initial
    images are very small and do not have lots of details (they are just digits),
    the features learned by the hidden neurons are not all that interesting, but it
    is already clear that each neuron is learning a different "shape".
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for drawing above should be immediately clear; we just notice that
    the following line is importing `cm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This simply allows for a grayscale representation of the neurons, and it is
    used inside the `imshow()` call by passing in the option `cmap = cm.Greys_r`.
    This is because the `mnist` images are not color images but gray scale images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The beauty of Keras is that it is easy to create neural nets, but it is also
    easy to download test datasets. Let''s try to use the `cifar10` dataset instead
    of the `mnist` dataset. Instead of digits, the `cifar10` dataset is comprised
    of 10 classes of objects: airplanes, automobiles, birds, cats, deers, dogs, frogs,
    horses, ships, and trucks. To use the `cifar10` dataset, it is enough to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In place of the preceding code line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to make these changes to the code we wrote above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This is because there are only 50,000 training images (instead of 60,000) and
    because the images are colored (RGB) 32 x 32 pixel images, therefore their size
    is 3 x 32 x 32\. We can keep everything else as before for now, however, if we
    run this example, we can see that our performance is now very bad, just around
    20%. This is due to the fact that the data is much more complex and it requires
    a more complex neural network. In fact, most neural networks implemented for classification
    of images use some basic convolutional layers, that will be discussed only in
    [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 5. Image
    Recognition"), *Image Recognition*, however, for now we could try raising the
    number of hidden neurons to 3,000, and adding a second hidden layer with 2,000
    neurons. We are also going to use the ReLU activity function in the first hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this we simply need to write the following lines defining the model,
    instead of what we had before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, we will see that it will take much longer to train, however,
    at the end, we will have about a 60% accuracy rate for the training set, but only
    about a 50% accuracy for the test data. The much poorer accuracy rate, with respect
    to the much simpler `mnist` dataset, despite the larger network and the much longer
    training time, is due to the higher complexity of the data. In addition, by substituting
    the line where we fit the network with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can also output during the process how the accuracy improves on a split 90/10
    of the training data. This also shows that while the accuracy of the training
    keeps increasing during training, the accuracy of the validation set plateaus
    at some point, showing that the network starts to overfit and to saturate some
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: While this may seem like a failure of deep networks to deliver good accuracy
    on richer datasets, we will see that in fact, there are ways around this problem
    that will allow us to get better performances on even much more complex and larger
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have walked the reader toward an understanding of what deep
    learning is and how it is related to deep neural networks. We have also discussed
    how many different implementations of deep neural networks exist, besides the
    classical feed-forward implementation, and have discussed the recent successes
    deep learning has had on many standard classification tasks. This chapter has
    been rich with concepts and ideas, developed through examples and historical remarks
    from the Jacquard loom to the Ising model. This is just the beginning, and we
    will work out many examples in which the ideas introduced in this chapter will
    be explained and developed more precisely.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to start this process in the coming chapter, where we will finally
    introduce the readers to many of the concepts we have touched on in this one,
    like RBMs and auto-encoders, and it will be clear how we can create more powerful
    deep neural networks than simple feed-forward DNNs. In addition, it will also
    be clear how the concept of representations and features arise naturally in these
    particular neural networks. From the last example, using cifar10, it is clear
    that classical feed-forward DNNs are difficult to train on more complex datasets,
    and we need a better way to set the weight parameters. X. Glorot and Y. Bengio,
    *Understanding the difficulty of training deep feed-forward neural* networks,
    in Proceedings of the International Conference on Artificial Intelligence and
    Statistics (AISTATS'10), (2010) ([http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)),
    treats the issue of poor performance of deep neural networks trained with gradient
    descent on random weight initialization. New algorithms that can be used to train
    deep neural networks successfully will be introduced and discussed in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
