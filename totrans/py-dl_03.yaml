- en: Chapter 3. Deep Learning Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章深度学习基础
- en: In [Chapter 1](part0016_split_000.html#F8901-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 1. Machine Learning – An Introduction"), *Machine Learning – An Introduction*,
    we introduced machine learning and some of its applications, and we briefly talked
    about a few different algorithms and techniques that can be used to implement
    machine learning. In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we concentrated on neural networks;
    we have shown that 1-layer networks are too simple and can only work on linear
    problems, and we have introduced the Universal Approximation Theorem, showing
    how 2-layer neural networks with just one hidden layer are able to approximate
    to any degree any continuous function on a compact subset of *R* *[n]*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0016_split_000.html#F8901-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 1. Machine
    Learning – An Introduction")*机器学习-简介*中，我们介绍了机器学习及其一些应用，并简要讨论了一些可用于实现机器学习的算法和技术。在[第2章](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks")*神经网络*中，我们专注于神经网络；我们已经表明1层网络太简单，只能处理线性问题，而且我们介绍了通用逼近定理，展示了只有一个隐层的2层神经网络能以任意程度逼近R*[n]*的紧致子集上的任何连续函数。
- en: In this chapter, we will introduce deep learning and deep neural networks, that
    is, neural networks with at least two or more hidden layers. The reader may wonder
    what is the point of using more than one hidden layer, given the Universal Approximation
    Theorem, and this is in no way a naïve question, since for a long period the neural
    networks used were very shallow, with just one hidden layer. The answer is that
    it is true that 2-layer neural networks can approximate any continuous function
    to any degree, however, it is also true that adding layers adds levels of complexity
    that may be much harder and may require many more neurons to simulate with shallow
    networks. There is also another, more important, reason behind the term *deep*
    of deep learning that refers not just to the depth of the network, or how many
    layers the neural net has, but to the level of "learning". In deep learning, the
    network does not simply learn to predict an output *Y* given an input *X*, but
    it also understands basic features of the input. In deep learning, the neural
    network is able to make abstractions of the features that comprise the input examples,
    to understand the basic characteristics of the examples, and to make predictions
    based on those characteristics. In deep learning, there is a level of abstraction
    that is missing in other basic machine learning algorithms or in shallow neural
    networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍深度学习和深度神经网络，也就是至少有两个或更多个隐层的神经网络。读者可能会想知道为什么要使用多个隐层，考虑到通用逼近定理，并无不合理之处，因为在很长一段时间内使用的神经网络非常浅，只有一个隐层。答案是2层神经网络确实可以以任意程度逼近任何连续函数，然而，增加层次也增加了可能更加难以模拟的复杂性，并且可能需要更多的神经元来模拟浅层网络。还有另一个更重要的原因是深度学习的术语“深度”并不仅仅指网络的深度或神经网络的层数，而是指“学习”的水平。在深度学习中，神经网络不仅仅是学习在给定输入*X*的情况下预测输出*Y*，而且还能理解输入的基本特征。在深度学习中，神经网络能够对构成输入示例的特征进行抽象化，理解示例的基本特征，并根据这些特征进行预测。在深度学习中，存在其他基本机器学习算法或浅层神经网络中缺乏的抽象层面。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is deep learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: Fundamental concepts of deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: Applications of deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: GPU versus CPU
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU与CPU对比
- en: Popular open source libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的开源库
- en: What is deep learning?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: 'In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton published an article
    titled *ImageNet Classification with Deep Convolutional Neural Networks* in Proceedings
    of Neural Information Processing Systems (NIPS) (2012) and, at the end of their
    paper, they wrote:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，Alex Krizhevsky，Ilya Sutskever和Geoff Hinton在《神经信息处理系统》（NIPS）（2012）的《ImageNet分类与深度卷积神经网络》一文中，写道：
- en: '*"It is notable that our network''s performance degrades if a single convolutional
    layer is removed. For example, removing any of the middle layers results in a
    loss of about 2% for the top-1 performance of the network. So the depth really
    is important for achieving our results."*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"值得注意的是，如果移除一个卷积层，我们网络的性能会下降。例如，移除任何一个中间层都会导致网络的 top-1 性能损失约为 2%。所以深度确实对于实现我们的结果至关重要。"*'
- en: 'In this milestone paper, they clearly mention the importance of the number
    of hidden layers present in deep networks. Krizheysky, Sutskever, and Hilton talk
    about convolutional layers, and we will not discuss them until [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*, but the basic question remains:
    *What do those hidden layers do?*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个重要的论文中，他们明确提到了深度网络中隐藏层的数量的重要性。Krizheysky、Sutskever 和 Hilton 谈到了卷积层，我们将在[第
    5 章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "第 5 章。图像识别")，*图像识别*中讨论它们，但基本问题仍然存在：*这些隐藏层到底做什么？*
- en: A typical English saying is *a picture is worth a thousand words*. Let's use
    this approach to understand what Deep Learning is. In H. Lee, R. Grosse, R. Ranganath,
    and A. Ng, *Convolutional deep belief networks for scalable unsupervised learning
    of hierarchical representations* in Proceedings of International Conference on
    Machine Learning (ICML) (2009) (Refer to [http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf))
    the authors use a few images, which we copy here.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的英语谚语是*一张图片胜过千言万语*。让我们使用这种方法来理解深度学习是什么。在 H. Lee、R. Grosse、R. Ranganath 和
    A. Ng，*用于可扩展无监督学习分层表示的卷积深度置信网络*，发表于 2009 年国际机器学习会议(ICML)的论文中（参见[http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)），作者使用了一些图片，我们在这里复制了一些。
- en: '![What is deep learning?](img/00100.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![什么是深度学习？](img/00100.jpeg)'
- en: In their example, they showed neural network pictures of different categories
    of objects and/or animals, and the network learned some basic features for each
    category. For instance, the network can learn some very basic shapes, like lines
    or edges, which are common to every category. In the next layer, however, the
    network can learn how those lines and edges fit together for each category to
    make images that feature the eyes of a face, or the wheels of a car. This is similar
    to how the visual cortex in humans works, where our brain recognizes features
    more and more complex starting from simple lines and edges.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的例子中，他们展示了不同类别的对象和/或动物的神经网络图片，并且网络学习了每个类别的一些基本特征。例如，网络可以学习一些非常基本的形状，如线条或边缘，这些是每个类别都共有的。然而，在下一层，网络可以学习这些线条和边缘如何组合在一起，以使每个类别的图像具有眼睛或车轮等特征。这类似于人类视觉皮层的工作方式，我们的大脑从简单的线条和边缘开始越来越复杂地识别特征。
- en: '![What is deep learning?](img/00101.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![什么是深度学习？](img/00101.jpeg)'
- en: 'The hidden layers in a deep neural network work similarly by understanding
    more and more complex features in each hidden layer. If we want to define what
    makes a face, we need to define its parts: the eyes, the nose, the mouth, and
    then we need to go a level up and define their position with respect to each other:
    the two eyes are on the top middle part, at the same height, the nose in the middle,
    the mouth in the lower middle part, below the nose. Deep neural networks catch
    these features by themselves, first learning the components of the image, then
    its relative position and so on, similarly to how, in Images 1 and 2, we see the
    level of deeper abstraction working in each layer. Some deep learning networks
    can, in fact, be considered generative algorithms, as in the case of **Restricted
    Boltzmann Machines** (**RBMs**), rather than simply a predictive algorithm, as
    they learn to generate a signal, and then they make the prediction based on the
    generation assumptions they have learned. As we will progress through this chapter,
    we will make this concept clearer.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络中的隐藏层也是通过逐层理解更加复杂的特征来工作的。如果我们想要定义什么是一个人脸，我们需要定义它的部分：眼睛、鼻子、嘴巴，然后我们需要上升到更高的层次，并定义它们相对于彼此的位置：两只眼睛在顶部的中间位置，处于同样的高度，鼻子在中间，嘴巴在下方中间位置，位于鼻子下方。深度神经网络通过自我学习来捕捉这些特征，首先学习图像的组成部分，然后学习它们的相对位置等等，就像在图像1和图像2中，我们可以看到深层抽象在每一层中的作用。一些深度学习网络实际上可以被看作是生成算法，例如**受限玻尔兹曼机**（**RBMs**），而不仅仅是一个预测算法，因为它们学会生成一个信号，然后根据已学习的生成假设进行预测。随着我们在本章中逐步深入，我们将使这个概念更加清晰。
- en: Fundamental concepts
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本概念
- en: In 1801, Joseph Marie Charles invented the Jacquard loom. Charles named the
    Jacquard, hence the name of its invention, was not a scientist, but simply a merchant.
    The Jacquard loom used a set of punched cards, and each punched card represented
    a pattern to be reproduced on the loom. Each punched card represented an abstraction
    of a design, a pattern, and each punched card was an abstract representation of
    that pattern. Punched cards have been used afterwards, for example in the tabulating
    machine invented by Herman Hollerith in 1890, or in the first computers where
    they were used to feed code to the machine. However, in the tabulating machine,
    for example, punched cards were simply abstractions of samples to be fed into
    the machine to calculate statistics on a population. In the Jacquard loom, the
    use of punched cards was subtler; in it, each card represented the abstraction
    of a pattern that could then be combined together with others to create more complex
    patterns. The punched card is an abstract representation of a feature of a reality,
    the final weaved design.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 1801年，约瑟夫·玛丽·查尔斯发明了雅卡尔织布机。查尔斯（Charles）并非科学家，而是一个简简单单的商人，雅卡尔织布机以他的名字命名。雅卡尔织布机使用一组打孔卡片，每个打孔卡片代表了需要在织布机上复制的图案。每个打孔卡片都代表了一个设计的抽象，一个图案的抽象，每个打孔卡片都是该图案的一个抽象表示。打孔卡片之后被用在其他地方，例如赫尔曼·荷里歇在1890年发明的制表机中，或者在最早的计算机中，它们被用来输入机器的代码。然而，在制表机中，打孔卡片只是对样本的抽象，用于计算人口的统计数据。而在雅卡尔织布机中，打孔卡片的使用更加微妙；在其中，每个卡片都代表一个图案的抽象，然后与其他卡片组合在一起，形成更复杂的图案。打孔卡片是对现实特征的一个抽象表示，最终成为织物的设计。
- en: 'In a way, the Jacquard loom had the seed of what makes deep learning today:
    the definition of a reality through the representations of its features. In deep
    learning, the neural network does not simply recognize what makes a cat a cat,
    or a squirrel a squirrel, but it understands what features are present in a cat
    and what features are present in a squirrel, and it learns to *design* a cat or
    a squirrel using those features. If we were to design a weaving pattern in the
    shape of a cat using a Jacquard loom, we would need to use punched cards that
    have *moustaches* on the nose, like those of a cat, and an elegant and slender
    body. Instead, if we were to design a squirrel, we would need to use the punched
    card that makes a furry tail, for example. A deep network that learns basic representations
    of its output can make classifications using the assumptions it has made; therefore,
    if there is no furry tail it will probably not be a squirrel, but rather a cat.
    This has many implications, as we will see, not least that the amount of information
    that the network learns is much more complete and robust. By learning to *generate*
    the model (in technical parlance by learning the joint probability *p(x,y)* rather
    than simply *p(y|x)*, the network is much less sensitive to *noise*, and it learns
    to recognize images even when there are other objects present in the scene or
    the object is partially obscured. The most exciting part is that deep neural networks
    learn to do this automatically.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，Jacquard 织机具有使得深度学习成为今天的样子的种子：通过其特征的表示来定义现实。在深度学习中，神经网络不仅仅是识别使猫成为猫，松鼠成为松鼠的因素，而是了解猫中存在哪些特征，松鼠中存在哪些特征，并且学会使用这些特征设计猫或松鼠。如果我们要使用
    Jacquard 织机设计成猫形状的编织图案，我们需要使用带有猫鼻子上胡子的打孔卡，以及优雅修长的身体。相反，如果我们要设计松鼠，我们需要使用制作毛茸茸尾巴的打孔卡，例如。一个学会基本输出表示的深度网络可以使用它所做出的假设进行分类；因此，如果没有毛茸茸的尾巴，它可能不会是松鼠，而更可能是猫。这有许多含义，正如我们将看到的那样，最重要的是，网络学到的信息量更加完整和稳健。通过学习*生成*模型（在技术术语中通过学习联合概率*p(x,y)*而不仅仅是*p(y|x)*，网络对*噪声*的敏感性大大降低，并且它学会了识别即使场景中存在其他物体或物体部分被遮挡的图像。最激动人心的部分是，深度神经网络自动学会做到这一点。
- en: Feature learning
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征学习
- en: The Ising model was invented by the physicist Wilhelm Lenz in1920, and he gave
    it as a problem to his student Ernst Ising. The model consists of discrete variables
    that can be in two states (positive or negative) and that represent magnetic dipoles.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Ising 模型是物理学家威廉·伦兹在 1920 年发明的，他把它作为问题交给了他的学生恩斯特·伊辛。该模型由可以处于两种状态（正或负）的离散变量组成，代表磁偶极子。
- en: In [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning*,
    we will introduce Restricted Boltzmann machines and auto-encoders, and we will
    start going deeper into how to build multi-layer neural networks. The type of
    neural networks that we have seen so far all have a feed-forward architecture,
    but we will see that we can define networks with a feedback loop to help tune
    the weights that define the neural network. Ising models, though not directly
    used in deep learning, are a good physical example that helps us understand the
    basic inner workings of tuning deep neural architectures, including Restricted
    Boltzmann machines, and in particular help us understand the concept of representation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 4 章](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 4. Unsupervised
    Feature Learning")《无监督特征学习》中，我们将介绍限制玻尔兹曼机和自动编码器，并且我们将开始更深入地了解如何构建多层神经网络。迄今为止我们看到的神经网络类型都具有前馈架构，但我们将看到我们可以定义具有反馈环路的网络以帮助调整定义神经网络的权重。虽然
    Ising 模型并不直接用于深度学习，但它是一个很好的物理示例，帮助我们理解调整深度神经结构的基本内部工作方式，包括限制玻尔兹曼机，特别是帮助我们理解表示的概念。
- en: What we are going to discuss in this section is a simple adaption (and simplification)
    of the Ising model to deep learning. In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we discussed how important it
    is to tune the weights of the connections between neurons. In fact, it is the
    weights in a neural network that make the network learn. Given an input (fixed),
    this input propagates to the next layer and sets the internal state of the neurons
    in the next layer based on the weights of their connections. Then, these neurons
    will fire and move the information over to the following layer through new connections
    defined by new weights, and so on. The weights are the only variables of the network,
    and they are what make the network learn. In general, if our activity function
    were a simple threshold function, a large positive weight would tend to make two
    neurons fire together. By firing together, we mean that, if one neuron fires,
    and the connecting weight is high, then the other neuron will also fire (since
    the input times the large connecting weight will likely make it over the chosen
    threshold). In fact, in 1949, in his *The organization of behavior*, Donald Hebb
    ([http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf](http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf))
    proposed that the opposite should also be true. Donald Hebb was a Canadian psychologist,
    who lived during the 20th century, who proposed the rule that goes by his name,
    the Hebb rule, which says that when neurons fire together their connection strengthens;
    when they do not fire together, their connection weakens.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节我们将讨论的是对 Ising 模型进行的简单调整（和简化），以适应深度学习。在[第二章](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks") *神经网络*中，我们已经讨论了调整神经元之间连接权重的重要性。实际上，神经网络中的权重使网络学习。给定一个输入（固定），这个输入传播到下一层，并根据它们之间连接的权重设置下一层神经元的内部状态。然后，这些神经元会发射信号，并通过新的连接将信息传递到下一层，这些连接由新的权重定义，依此类推。权重是网络的唯一变量，它们使网络学习。通常情况下，如果我们的激活函数是一个简单的阈值函数，一个较大的正权重会倾向于使两个神经元一起激活。所谓一起激活，我们指的是，如果一个神经元激活，且连接权重很高，则另一个神经元也会激活（因为输入乘以较大的连接权重很可能会超过选择的阈值）。事实上，1949
    年，在他的*行为的组织*中，唐纳德·赫布（[http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf](http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf)）提出了相反的观点也是真的。唐纳德·赫布是一位加拿大心理学家，生活在
    20 世纪，他提出了以他的名字命名的规则，赫布规则，该规则指出当神经元一起激活时，它们的连接加强；当它们不一起激活时，它们的连接减弱。
- en: In the following example, we think of an Ising model as a network of neurons
    that acts in a binary way, that is, where they can only activate (fire) or not,
    and that, the stronger their relative connection, the more likely they are to
    fire together. We assume that the network is stochastic, and therefore if two
    neurons are strongly connected they are only very likely to fire together.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将 Ising 模型看作是一种以二进制方式运作的神经元网络，即它们只能激活（发射）或不激活，并且，它们的相对连接越强，它们一起激活的可能性就越大。我们假设网络是随机的，因此如果两个神经元之间连接很强，它们只有很大的可能性一起激活。
- en: Tip
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Stochastic means probabilistic. In a stochastic network, we define the probability
    of a neuron to fire: the higher the probability, the more likely the neuron is
    to fire. When two neurons are strongly connected, that is, they are connected
    by a large weight, the probability that one firing will induce the other one to
    fire as well is very high (and vice versa, a weak connection will give a low probability).
    However, the neuron will only fire according to a probability, and therefore we
    cannot know with certainty whether it will fire.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随机意味着概率性。在随机网络中，我们定义神经元激活的概率：概率越高，神经元激活的可能性就越大。当两个神经元之间的连接很强时，即它们之间连接的权重很大时，一个神经元激活将引起另一个神经元也激活的概率非常高（反之亦然，弱连接会导致低概率）。然而，神经元只会根据概率激活，因此我们无法确定它是否会激活。
- en: 'On the other hand, if they are inversely correlated (a large negative weight),
    they are very likely not to fire together. Let''s show some examples:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果它们呈反相关（具有较大的负权重），它们非常可能不会一起激活。让我们举些例子：
- en: '![Feature learning](img/00102.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00102.jpeg)'
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large and positive, so the third neuron will also be
    active. In the second figure, the first two neurons are off, and their connection
    with the third neuron is positive, so the third neuron will also be off.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张图中，前两个神经元处于活跃状态，并且它们与第三个神经元的连接很大且为正，因此第三个神经元也将处于活跃状态。在第二张图中，前两个神经元处于关闭状态，并且它们与第三个神经元的连接为正，因此第三个神经元也将处于关闭状态。
- en: In the second figure, the first two neurons are off, and their connection with
    the third neuron is positive, so the third neuron will also be off.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二张图中，前两个神经元处于关闭状态，并且它们与第三个神经元的连接为正，因此第三个神经元也将处于关闭状态。
- en: There are several combinations that may be present; we will show only a few
    of them. The idea is that the state of the neurons in the first layer will probabilistically
    determine the state of the neurons in the following layer, depending on the sign
    and strength of the connection. If the connections are weak, the connected neurons
    in the following layer may have equal or almost equal probability to be in any
    state. But if the connections are very strong, then the sign of the weight will
    make the connected neurons act in a similar or opposite way. Of course, if the
    neuron on the second layer has more than one neuron as its input, we will weigh
    all the input connections as usual. And if the input neurons are not all on or
    off, and their connections are equally strong, then again, the connected neuron
    may have equal or an almost equal chance of being on or off.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现几种组合；我们将只展示其中几种。想法是第一层神经元的状态将以概率方式确定后续层神经元的状态，取决于连接的符号和强度。如果连接较弱，则后续层中连接的神经元可能以任何状态相等或几乎相等的概率存在。但如果连接非常强，则权重的符号将使连接的神经元以类似或相反的方式运作。当然，如果第二层的神经元具有超过一个神经元作为其输入，我们将像往常一样加权所有输入连接。如果输入神经元并非全部处于开启或关闭状态，并且它们的连接同样强，则连接的神经元可能以相等或几乎相等的概率处于开启或关闭状态。
- en: '![Feature learning](img/00103.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00103.jpeg)'
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large and negative, so the third neuron will also be
    off. In the second figure, the first two neurons are off, and their connection
    with the third neuron is large and negative, so the third neuron will likely be
    on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张图中，前两个神经元处于活跃状态，并且它们与第三个神经元的连接很大且为负，因此第三个神经元也将处于关闭状态。在第二张图中，前两个神经元处于关闭状态，并且它们与第三个神经元的连接很大且为负，因此第三个神经元很可能处于打开状态。
- en: 'It is then clear that, to most likely determine the state of the neurons in
    the following layers, the neurons in the first layer should all be in similar
    states (on or off) and all be connected with strong (that is, large weights) connections.
    Let''s see more examples:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来很明显，要最有可能地确定下一层神经元的状态，第一层神经元应该都处于相似的状态（开或关）并且都与强连接（即，较大的权重）连接。让我们看更多的例子：
- en: '![Feature learning](img/00104.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00104.jpeg)'
- en: In the first figure, the first two neurons are active, and their connection
    with the third neuron is large but opposite, so the third neuron could be equally
    likely to be on or off. In the second figure, the first two neurons are, one on
    and one off, and their connections with the third neuron are both large and positive,
    so the third neuron will also be equally likely to be on or off. In the last figure,
    the first two neurons are active, but their connection with the third neuron is
    small, so the third neuron is slightly more likely to be on but it has a relatively
    high chance to be off as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张图中，前两个神经元处于活跃状态，并且它们与第三个神经元的连接很大但方向相反，因此第三个神经元可能同样有可能处于开启或关闭状态。在第二张图中，前两个神经元一个处于开启状态，一个处于关闭状态，并且它们与第三个神经元的连接都很大且为正，因此第三个神经元同样有可能处于开启或关闭状态。在最后一张图中，前两个神经元处于活跃状态，但它们与第三个神经元的连接很小，因此第三个神经元更有可能处于开启状态，但它也有相当高的几率处于关闭状态。
- en: 'The point of introducing this adaptation of the Ising model is to understand
    how representation learning works in deep neural networks. We have seen that setting
    the correct weights can make a neural network turn on or off certain neurons,
    or in general, affect their output. Picturing neurons in just two states, however,
    helps our intuition and our visual description of what goes on in a neural network.
    It also helps our visual intuition to represent our network layers in 2-dimensions,
    rather than as a 1-dimensional layer. Let''s picture our neural network layer
    as in a 2-dimensional plane. We could then imagine that each neuron represents
    the pixel on a 2-dimensional image, and that an "on" neuron represents a (visible)
    dark dot on a white plane, while an "off" neuron blends in (invisibly) with the
    white background. Our input layer of on/off neurons can then be seen as a simple
    2-dimensional black and white image. For example, let''s suppose we want to represent
    a smiley face, or a sad face—we would just turn on (activate) the correct neurons
    to get the following figures:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 引入这种Ising模型的改编的目的是理解深度神经网络中的表示学习是如何工作的。我们已经看到，设置正确的权重可以使神经网络打开或关闭某些神经元，或者一般地影响它们的输出。然而，将神经元描绘成只有两种状态，有助于我们直观地理解神经网络中发生的事情。在二维平面中表示我们的网络层对我们的直观和视觉描述也有所帮助，而不是表示为一维层。让我们把我们的神经网络层想象成二维平面。然后我们可以想象每个神经元代表了二维图像上的像素，而“开”状态的神经元代表了白色平面上的黑点，而“关”状态的神经元则与白色背景融为一体（不可见）。我们的开/关状态的输入层可以被看作一个简单的二维黑白图像。比如，假设我们想要表示一个笑脸，或者一个悲伤的脸——我们只需激活正确的神经元，就可以得到以下图形：
- en: '![Feature learning](img/00105.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00105.jpeg)'
- en: 'A happy and a sad face: the difference lies in a few neurons on the side of
    the mouth that can be on or off.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快乐的脸和一个悲伤的脸：区别在于嘴角的几个神经元，可能是开或关状态。
- en: 'Now let''s suppose that this corresponds to the input layer, so this layer
    would be connected to another layer, one of the hidden layers. There would then
    be connections between each pixel in this image (both black and white) and each
    neuron in the following layer. In particular, each black (on) pixel would be connected
    to each neuron in the following layer. Let''s now assume that the connections
    from each neuron making the left eye has a strong (large positive weight) connection
    to a particular pixel in the hidden layer, but it has a large negative connection
    to any other neuron in the hidden layer:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设这对应于输入层，因此此层将连接到另一层，即隐藏层之一。然后，这幅图像中的每个像素（无论是黑色还是白色）与下一层的每个神经元之间都会有连接。特别是，每个黑色（开）像素将连接到下一层的每个神经元。现在假设每个使左眼的神经元的连接具有强（大正权重）连接到隐藏层中的特定像素，但与隐藏层中的其他任何神经元都有大的负连接：
- en: '![Feature learning](img/00106.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00106.jpeg)'
- en: On the left a smiley face, on the right, the same smiley face and the connections
    between its left eye and a hidden neuron.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是一个笑脸，右边是相同的笑脸和其左眼与隐藏神经元之间的连接。
- en: What this means is that if we set large positive weights between the hidden
    layer and the left eye, and large negative connections between the left eye and
    any other hidden neuron, whenever we show the network a face that contains a left
    eye (which means those neurons are on) this particular hidden neuron will activate,
    while all the other neurons will tend to stay off. This means that this particular
    neuron will be able to detect when a left eye is present or not. We can similarly
    create connections between the right eye, the nose and the main part of the mouth,
    so that we can start detecting all those face features.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们在隐藏层和左眼之间设置大的正权重，以及左眼与任何其他隐藏神经元之间的大的负连接，每当我们向网络展示一个包含左眼的脸时（这意味着那些神经元处于开状态），这个特定的隐藏神经元将激活，而所有其他神经元往往会保持关闭。这意味着这个特定的神经元将能够检测左眼是否存在。我们也可以类似地创建右眼、鼻子和嘴巴主要部分之间的连接，这样我们就可以开始检测所有这些面部特征。
- en: '![Feature learning](img/00107.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00107.jpeg)'
- en: Each face feature, the eyes, the nose and the mouth, has large positive connections
    with certain hidden neurons and large but negative connections with the others.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每个脸部特征, 眼睛、鼻子和嘴巴, 都与某些隐藏神经元具有大的正连接, 但与其他神经元具有大的负连接。
- en: This shows how we can select weights for our connections, to have the hidden
    neurons start recognizing features of our input.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了我们如何为我们的连接选择权重，让隐藏神经元开始识别输入的特征。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As an important reminder, we want to point out to the reader that, in fact,
    we do not select the weights for our connections to start recognizing features
    of the input. Instead, those weights are automatically selected by the network
    using back-propagation or other tuning methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个重要的提醒，我们想向读者指出，事实上，我们并没有选择权重来开始识别输入的特征。相反，这些权重是由网络使用反向传播或其他调整方法自动选择的。
- en: In addition, we can have more hidden layers that can recognize features of the
    features (*is the mouth in our face smiling or is it sad?*) and therefore get
    more precise results.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以拥有更多的隐藏层，它们可以识别特征的特征（*我们脸上的嘴是笑着的还是悲伤的？*），因此可以得到更精确的结果。
- en: There are several advantages to deep learning. The first is, as we have seen,
    that it can recognize features. Another, even more important, is that it will
    recognize features automatically. In this example, we have set the weights ourselves
    to recognize the features we chose. This is one of the disadvantages of many machine
    learning algorithms, that the user must use his/her own experience to select what
    he/she thinks are the best features. A lot of time goes therefore, into feature
    selection that must still be performed by a human being. Deep Learning algorithms,
    instead, automatically select the best features. This can be done, as we have
    seen in the previous chapter, using back-propagation, but in fact, other techniques
    also exist to select those weights, and those will be the important points that
    will be treated in the next chapter, such as auto-encoders and restricted Boltzmann
    machines (or *Harmoniums*, as Paul Smolensky, who invented them in 1986, called
    them). We should, however, also caution the reader that the advantage we get from
    the automatic feature selection has to pay the price of the fact that we need
    to choose the correct architecture for the neural network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有几个优点。第一个优点就像我们所见，它可以识别特征。另一个更重要的优点是，它会自动识别特征。在这个例子中，我们自己设置了权重以识别我们选择的特征。这是许多机器学习算法的缺点之一，用户必须使用自己的经验来选择他/她认为最好的特征。因此，需要大量的时间来进行特征选择，这仍然需要人类来执行。相反，深度学习算法会自动选择最佳特征。正如我们在前一章中所见，这可以通过反向传播来完成，但事实上，还存在其他技术来选择这些权重，这些将是下一章将要讨论的重要点，如自动编码器和受限玻尔兹曼机（或*哈蒙尼姆*，1986年由保罗·斯莫伦斯基发明）。然而，我们还要提醒读者，我们从自动特征选择中获得的优势必须付出这样的代价，即我们需要选择正确的神经网络结构。
- en: In some deep learning systems (for example in Restricted Boltzmann Machines,
    as we will see in the next chapter), the neural network can also learn to "repair"
    itself. As we mentioned in the previous example, we could produce a general face
    by activating the four neurons we have associated to the right/left eye, the nose,
    and the mouth respectively. Because of the large positive weights between them
    and the neurons in the previous layer, those neurons will turn on and we will
    have the neurons corresponding to those features activated, generating a general
    image of a face. At the same time, if the neurons corresponding to the face are
    turned on, the four corresponding neurons to the eyes, nose, and mouth will also
    activate. What this means is that, even if not all the neurons defining the face
    are on, if the connections are strong enough, they may still turn on the four
    corresponding neurons, which, in turn, will activate the missing neurons for the
    face.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些深度学习系统中（例如在受限玻尔兹曼机中，正如我们将在下一章中看到的那样），神经网络还可以学会“修复”自己。正如我们在前面的例子中提到的，我们可以通过激活我们分别与右/左眼、鼻子和嘴相联系的四个神经元来产生一个通用的面孔。由于它们与前一层之间的连接权重较大，这些神经元将被激活，我们将激活与这些特征对应的神经元，从而生成一个通用的面孔图像。同时，如果与面部对应的神经元被激活，那么眼睛、鼻子和嘴的四个对应神经元也将被激活。这意味着，即使没有所有定义面部的神经元都处于开启状态，如果连接足够强大，它们仍然可能激活四个对应的神经元，进而激活面部缺失的神经元。
- en: 'This has one more extra advantage: robustness. Human vision can recognize objects
    even when the view is partly obscured. We can recognize people even when they
    wear a hat, or a scarf that covers their mouth; we are not sensitive to noise
    in the image. Similarly, when we create this correspondence, if we alter the face
    slightly, for example by modifying the mouth by one or two pixels, the signal
    would still be strong enough to turn the "mouth" neuron on, which in turn would
    turn on the correct pixel and off the wrong pixel making up the modified eye.
    This system is not sensitive to noise and can make auto-corrections.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这还有一个额外的优势：鲁棒性。人类视觉在视图部分被遮挡时也能识别物体。我们甚至可以在对方戴帽子或遮住嘴巴的围巾时认出人；我们对图像中的噪声不敏感。同样地，当我们创建这种对应关系时，如果我们稍微改变面部，比如通过微调嘴巴一两个像素，信号仍然足够强大，可以打开“嘴巴”神经元，这将打开正确的像素并关闭组成修改后眼睛的错误像素。这个系统对噪声不敏感，并且可以进行自动修正。
- en: Let's say, for example, that the mouth has a couple of pixels off (in the figure
    those are the pixels that have an **x**).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，嘴巴有一对像素关闭（在图中那些带有**x**的像素）。
- en: '![Feature learning](img/00108.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00108.jpeg)'
- en: This image has a couple of pixels making the mouth that are not turned on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图有一对构成嘴巴的像素没有打开。
- en: 'However, the mouth may still have enough neurons in the right place to be able
    to turn on the corresponding neuron representing it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，嘴巴可能仍然有足够数量的神经元在正确的位置上，可以打开代表它的对应神经元：
- en: '![Feature learning](img/00109.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00109.jpeg)'
- en: Even though a couple of neurons are off, the connections with the other neurons
    are strong enough that the neuron representing the mouth in the next layer will
    turn on anyway.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一对神经元关闭，与其他神经元的连接足够强大，下一层代表嘴巴的神经元仍然会打开。
- en: 'On the other hand, we could now travel the connections backwards and whenever
    the neuron representing the mouth is on, this would turn on all the neurons comprising
    the mouth, including the two neurons that were previously off:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们现在可以沿着连接逆向传播，每当代表嘴巴的神经元打开时，这将打开组成嘴巴的所有神经元，包括以前关闭的两个神经元：
- en: '![Feature learning](img/00110.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00110.jpeg)'
- en: The two neurons have been activated by the neuron on top
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个神经元被顶部的神经元激活。
- en: 'In summary, deep learning advantages with respect to many other machine learning
    algorithms and shallow neural networks in particular are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，深度学习相对于许多其他机器学习算法，特别是浅层神经网络的优势有：
- en: Deep learning can learn representations
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以学习表示
- en: Deep learning is less sensitive to noise
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习对噪声不太敏感。
- en: Deep learning can be a generative algorithm (more on this in the next chapter)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以是一种生成算法（在下一章中会更详细介绍）
- en: To further understand why many hidden layers may be necessary, let's consider
    the task of recognizing a simple geometric figure, a cube. Say that each possible
    line in 3D is associated with a neuron (let's forget for a moment that this will
    require an infinite number of neurons).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解为什么许多隐藏层可能是必要的，让我们考虑识别一个简单几何图形，一个立方体的任务。假设3D中的每条可能的线与一个神经元相关联（让我们暂时忘记这将需要无限多的神经元）。
- en: '![Feature learning](img/00111.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00111.jpeg)'
- en: Each line on the same visual field is associated to a different neuron.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 同一视野上的每一条线都与不同的神经元相关联。
- en: If we restrict ourselves to a single eye, lines at different angles in our vision
    will project to the same line on a 2-dimensional plane. Each line we see could
    therefore be given by any corresponding 3D-line that projects to the same line
    onto the retina. Assume that any possible 3D-line is associated to a neuron. Two
    distinct lines that make up the cube are therefore associated to a family of neurons
    each. However, the fact that these two lines intersect, allows us to create a
    connection between two neurons each belonging to a different family. We have many
    neurons for the line making up one of the edges of the cube, and many other neurons
    for the line making up another of the edges of the cube, but since those two lines
    intersect, there are two neurons that will be connected. Similarly, each of these
    lines connects to other lines that make up the cube, allowing us to further redefine
    our representation. At a higher level, our neural net can also start to identify
    that these lines are not connected at just any angle, but they are connected at
    exactly 90 degree angles. This way we can make increasingly more abstract representations
    that allow us to identify the set of lines drawn on a piece of paper as a cube.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们限制自己只看一个眼睛，我们视野中不同角度的线条将投影到二维平面上的同一条线上。因此，我们看到的每条线都可以由任何对应的三维线条给出，这些三维线条投影到视网膜上的同一条线上。假设任何可能的三维线条都与一个神经元相关联。因此，构成立方体的两条不同线条各自与一个神经元族相关联。然而，这两条线相交的事实允许我们连接属于不同族的两个神经元。我们对于构成立方体一条边的线有许多神经元，对于构成立方体另一条边的线也有许多神经元，但因为这两条线相交，有两个神经元会被连接。同样，每条线也连接到构成立方体的其他线条，使我们能够进一步重新定义我们的表示。在更高的层次上，我们的神经网络还可以开始识别这些线不是以任意角度相连，而是以确切的90度角相连。通过这种方式，我们可以制作越来越抽象的表示，从而使我们能够将在一张纸上画出的线条组识别为一个立方体。
- en: Neurons in different layers, organized hierarchically, represent different levels
    of abstraction of basic elements in the image and how they are structured. This
    toy example shows that each layer can, in an abstract system, link together what
    different neurons at a lower level are seeing, making connections between them,
    similar to how we can make connections between abstract lines. It can, using those
    connections, realize that those abstract lines are connected at a point, and,
    in a further up layer, are in fact connected at 90 degrees and make up a cube,
    the same way we described how we can learn to recognize a face by recognizing
    the eyes, the nose, and the mouth, and their relative position.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不同层中的神经元按层次结构组织，并表示图像中基本元素及其结构的不同抽象水平。这个玩具例子显示，每个层次在抽象系统中都可以将下层的不同神经元联系在一起，建立它们之间的连接，类似于我们如何在抽象线条之间建立连接。它可以利用这些连接意识到这些抽象线条在一个点上相连，在更高层次上，实际上以90度相连并组成一个立方体，就像我们描述如何通过识别眼睛、鼻子和嘴巴及其相对位置来学习识别脸部的方式一样。
- en: '![Feature learning](img/00112.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![特征学习](img/00112.jpeg)'
- en: Each line is associated with a neuron, and we can create basic representations
    by associating neurons that represent lines that intersect, and more complex representations
    by associating neurons that represent lines at specific angles.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每条线都与一个神经元相关联，通过关联表示相交的线条来创建基本表示，通过关联表示特定角度的线条来创建更复杂的表示。
- en: Deep learning algorithms
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习算法
- en: 'In the previous paragraph, we have given an intuitive introduction to deep
    learning. In this section, we will give a more precise definition of key concepts
    that will be thoroughly introduced in the next chapters. Deep Neural Networks
    with many layers also have a biological reason to exist: through our study of
    how humans understand speech, it has in fact become clear that we are endowed
    with a layered hierarchical structure that transforms the information from the
    audible sound input into the linguistic level. Similarly, the visual system and
    the visual cortex have a similar layered structure, from the V1 (or striate cortex),
    to the V2, V3 and V4 visual area in the brain. Deep neural networks mimic the
    nature of our brains, though in a very primitive way. We should warn the reader,
    however, that while understanding our brain can help us create better artificial
    neural networks, in the end, we may be creating a completely different architecture,
    the same way we may have created airplanes by trying to mimic birds, but ended
    up with a very different model.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的段落中，我们对深度学习进行了直观的介绍。在本节中，我们将对下一章节中将彻底介绍的关键概念给出更精确的定义。具有许多层的深度神经网络也有存在的生物学原因：通过我们对人类理解语音的研究，实际上已经清楚地表明，我们天生具有一种分层的层次结构，它将听到的声音输入转化为语言水平。类似地，视觉系统和视觉皮层具有类似的分层结构，从
    V1（或条纹皮层）到大脑中的 V2、V3 和 V4 视觉区域。深度神经网络模仿了我们大脑的本质，尽管以非常原始的方式。然而，我们应该警告读者，尽管理解我们的大脑可以帮助我们创建更好的人工神经网络，但最终，我们可能正在创建一种完全不同的架构，就像我们通过模仿鸟类创建了飞机，但最终得到了一个非常不同的模型。
- en: 'In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we introduced the back-propagation
    algorithm as a popular training algorithm. In practice, when we have many layers,
    back-propagation may be a slow and difficult algorithm to use. Back-propagation,
    in fact, is mainly based on the gradient of the function, and often the existence
    of local minima may prevent convergence of the method. However, the term deep
    learning applies to a class of deep neural networks algorithms that may use different
    training algorithms and weight tuning, and they are not limited to back-propagation
    and classical feed-forward neural networks. We should then more generally define
    deep learning as a class of machine learning techniques where information is processed
    in hierarchical layers, to understand representations and features from the data
    in increasing levels of complexity. In this class of algorithms, we can generally
    include:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 2. Neural
    Networks") *神经网络* 中，我们介绍了反向传播算法作为一种流行的训练算法。在实践中，当我们有许多层时，反向传播可能是一种缓慢且难以使用的算法。事实上，反向传播主要是基于函数的梯度，而局部最小值的存在往往会阻止该方法的收敛。然而，深度学习这个术语适用于一类可能使用不同训练算法和权重调整的深度神经网络算法，它们不限于反向传播和经典的前馈神经网络。因此，我们应更加普遍地将深度学习定义为一类机器学习技术，其中信息在分层层次中进行处理，以便在逐渐增加的复杂性水平上理解数据的表示和特征。在这类算法中，我们通常可以包括：
- en: '**Multi-Layer Perceptrons (MLP)**: A neural network with many hidden layers,
    with feed-forward propagation. As discussed, this is one of the first examples
    of deep learning network but not the only possible one.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器（MLP）**：具有许多隐藏层的神经网络，采用前馈传播。正如讨论的那样，这是深度学习网络的第一个示例，但不是唯一可能的示例。'
- en: '**Boltzmann Machines (BM)**: A stochastic symmetric network with a well-defined
    energy function.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玻尔兹曼机（BM）**：具有明确定义的能量函数的随机对称网络。'
- en: '**Restricted Boltzmann Machines (RBM)**: Similar to the Ising model example
    above, restricted Boltzmann machines are comprised of symmetric connections between
    two layers, one visible and one hidden layer, but unlike general Boltzmann machines,
    neurons have no intra-layers connections. They can be stacked together to form
    DBNs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**受限玻尔兹曼机（RBM）**：与上面的伊辛模型示例类似，受限玻尔兹曼机由两层之间的对称连接组成，一个是可见层，一个是隐藏层，但与一般玻尔兹曼机不同，神经元之间没有层内连接。它们可以堆叠在一起形成
    DBN。'
- en: '**Deep Belief Networks (DBN)**: a stochastic generative model where the top
    layers have symmetric connections between them (undirected, unlike feed-forward
    networks), while the bottom layers receive the processed information from directed
    connections from the layers above them.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度信念网络（DBN）**：一种随机生成模型，其中顶层之间具有对称连接（与前馈网络不同，是无向的），而底层通过来自上面层的定向连接接收来自处理后的信息。'
- en: '**Autoencoders**: A class of unsupervised learning algorithms in which the
    output shape is the same as the input, that allows the network to better learn
    basic representations.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**：一类无监督学习算法，其输出形状与输入相同，这使得网络能够更好地学习基本表示。'
- en: '**Convolutional Neural Networks (CNN)**: Convolutional layers apply filters
    to the input image (or sound) by sliding this filter all across the incoming signal
    to produce a bi-dimensional activation map. CNNs allow the enhancement of features
    hidden in the input.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络（CNN）**：卷积层通过将滤波器应用于输入图像（或声音），通过在传入信号上滑动此滤波器来生成二维激活图。CNN允许增强输入中隐藏的特征。'
- en: Each of these deep learning implementations has its own advantages and disadvantages,
    and they can be easier or harder to train depending on the number of layers and
    neurons in each layer. While simple feed-forward Deep Neural Networks can generally
    be trained using the back-propagation algorithm discussed in the second chapter,
    different techniques exist for the other types of networks, as will be discussed
    further in the next chapter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每种深度学习实现都有其优缺点，它们的训练难度取决于每层的层数和神经元数量。虽然简单的前馈深度神经网络通常可以使用第二章讨论的反向传播算法进行训练，但对于其他类型的网络存在不同的技术，这将在下一章中进一步讨论。
- en: Deep learning applications
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习应用
- en: In the next couple of paragraphs, we will discuss how deep neural networks have
    applications in the field of speech recognition and computer vision, and how their
    application in recent years has vastly improved accuracy in these two fields by
    completely outperforming many other machine learning algorithms not based on deep
    neural networks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几段中，我们将讨论深度神经网络在语音识别和计算机视觉领域的应用，以及近年来它们在这两个领域的应用如何通过完全超越许多其他不基于深度神经网络的机器学习算法而大大提高了准确性。
- en: Speech recognition
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音识别
- en: Deep learning has started to be used in speech recognition starting in this
    decade (2010 and later, see for example the 2012 article titled *Deep Neural Networks
    for Acoustic Modeling in Speech Recognition* by Hinton et al., available online
    at [http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf));
    until then, speech recognition methods were dominated by algorithms called GMM-HMM
    methods (Hidden Markov Models with Gaussian Mixture Emission). Understanding speech
    is a complex task, since speech is not, as is naively thought, made up of separate
    words with clear boundaries between them. In reality, in speech there are no really
    distinguishable parts, and there are no clear boundaries between spoken words.
    In studying sounds when composing words, we often look at so-called triphones,
    which are comprised of three regions where the first part depends on the previous
    sound, the middle part is generally stable, and the next one depends on the following
    sound. In addition, typically it is better to detect only parts of a triphone,
    and those detectors are called senones.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习开始在本年代（2010年及以后，例如2012年一篇标题为*Deep Neural Networks for Acoustic Modeling
    in Speech Recognition*的文章，由Hinton等人撰写，可在[http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf)在线获取）中用于语音识别；在此之前，语音识别方法主要由称为GMM-HMM方法（具有高斯混合发射的隐马尔可夫模型）的算法主导。理解语音是一个复杂的任务，因为语音并不像天真地认为的那样，由清晰分隔开的单词组成，它们之间有明确的边界。实际上，语音中没有真正可辨识的部分，也没有清晰的单词边界。在组合单词时研究声音时，我们经常看到所谓的三音素，它们由三个区域组成，其中第一部分取决于前一个声音，中间部分通常是稳定的，下一个声音取决于后一个声音。此外，通常最好只检测三音素的部分，这些检测器称为senones。
- en: In *Deep Neural Networks for Acoustic Modeling in Speech Recognition*, several
    comparisons were made between the then state-of-the art models and the model used
    by the authors that comprised five hidden layers with 2048 units per layer. The
    first comparison was using the Bing voice search application, achieving a 69.6%
    accuracy vs. a 63.8% accuracy with a classical method, named the GMM-HMM model,
    on 24 hours of training data. The same model was also tested on the Switchboard
    speech recognition task, a public speech-to-text transcription benchmark (similar
    to the MNIST dataset used for digit recognition) that includes about 2500 conversations
    by 500 speakers from around the US. In addition, tests and comparisons were performed
    using Google Voice input speech, YouTube data, and English Broadcast News speech
    data. In the next table, we summarize the results from the article, showing the
    error rate for DNN versus GMM-HMM.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《语音识别中的深度神经网络用于声学建模》*中，对当时最先进的模型和作者采用的模型进行了几次比较，该模型由五个隐藏层组成，每层2048个单元。第一次比较是使用必应语音搜索应用程序，在24小时的训练数据上实现了69.6％的准确率，而使用传统方法，名为GMM-HMM模型，在相同的训练数据上实现了63.8％的准确率。该模型还在Switchboard语音识别任务上进行了测试，这是一个公共语音转文本转录基准（类似于用于数字识别的MNIST数据集），包括来自美国各地约500位发言者的大约2500次对话。此外，还使用了Google语音输入语音、YouTube数据和英语广播新闻语音数据进行了测试和比较。在下一个表格中，我们总结了该文章的结果，显示了DNN与GMM-HMM的错误率对比。
- en: '| Task | Total number of hours of training data | DNN(error rate) | GMM-HMM
    with same training(error rate) | GMM-HMM with longer training(error rate) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 训练数据总小时数 | DNN（错误率） | 具有相同训练的GMM-HMM（错误率） | 具有更长训练的GMM-HMM（错误率） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Switchboard (test1) | 309 | 18.5 | 27.4 | 18.6 (2000hrs) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Switchboard（测试1） | 309 | 18.5 | 27.4 | 18.6（2000小时） |'
- en: '| Switchboard (test2) | 309 | 16.1 | 23.6 | 17.1 (2000hrs) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Switchboard（测试2） | 309 | 16.1 | 23.6 | 17.1（2000小时） |'
- en: '| English Broadcast News | 50 | 17.5 | 18.8 |   |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 英语广播新闻 | 50 | 17.5 | 18.8 |   |'
- en: '| Bing Voice Search | 24 | 30.4 | 36.2 |   |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 必应语音搜索 | 24 | 30.4 | 36.2 |   |'
- en: '| Google Voice | 5870 | 12.3 |   | 16.0 (>>5870hrs) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Google语音 | 5870 | 12.3 |   | 16.0（>>5870小时） |'
- en: '| YouTube | 1400 | 47.6 | 52.3 |   |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| YouTube | 1400 | 47.6 | 52.3 |   |'
- en: 'In another article, *New types of Deep Neural Network Learning for Speech Recognition
    and Related Applications: An overview*, by Deng, Hinton, and Kingsbury ([https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/](https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/)),
    the authors also notice how DNNs work particularly well for noisy speech.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一篇文章，*《语音识别和相关应用的新型深度神经网络学习方法概述》*，由邓、欣顿和金斯伯里（[https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/](https://www.microsoft.com/en-us/research/publication/new-types-of-deep-neural-network-learning-for-speech-recognition-and-related-applications-an-overview/)）撰写，作者们也注意到深度神经网络在嘈杂语音方面表现出色。
- en: Another advantage of DNNs is that before DNNs, people had to create transformations
    of speech spectrograms. A spectrogram is a visual representation of the frequencies
    in a signal. By using DNNs, these neural networks can autonomously and automatically
    pick primitive features, in this case represented by primitive spectral features.
    Use of techniques such as convolution and pooling operations, can be applied on
    this primitive spectral feature to cope with typical speech variations between
    speakers. In recent years, more sophisticated neural networks that have recurrent
    connections (RNN) have been employed with great success (A. Graves, A. Mohamed
    and G. Hinton , *Speech Recognition with Deep Recurrent Neural Networks* in Proceedings
    of International Conference on Acoustic Speech and Signal Processing(ICASSP) (2013);
    refer to [http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf))
    for example, a particular type of deep neural network called **LSTM** (**long
    short-term memory** neural network) that will be described in a later chapter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 的另一个优点是在 DNN 出现之前，人们必须创建语音声谱图的变换。声谱图是信号中频率的视觉表示。通过使用 DNN，这些神经网络可以自主自动地选择原始特征，本例中以原始谱特征表示。使用卷积和池化等技术，可以应用于这种原始谱特征，以应对说话者之间的典型语音变化。近年来，更加复杂的具有循环连接的神经网络（RNN）取得了巨大成功（A.
    Graves、A. Mohamed 和 G. Hinton，《Speech Recognition with Deep Recurrent Neural Networks》发表于国际会议
    *Acoustic Speech and Signal Processing(ICASSP)* (2013)；参见 [http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf)），例如，一种特定类型的深度神经网络称为
    **LSTM**（**长短期记忆**神经网络），将在后面的章节中描述。
- en: In [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we have discussed different
    activity functions, and although the logistic sigmoid and the hyperbolic tangent
    are often the best known, they are also often slow to train. Recently, the ReLU
    activity function has been used successfully in speech recognition, for example
    in an article by G. Dahl, T. Sainath, and G. Hinton in *Improving Deep Neural
    Networks for LVCSR Using Rectified Linear Units and Dropout* in Proceeding of
    International Conference on Acoustics Speech and Signal Processing (ICASSP) (2013)
    ([http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf](http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf)).
    In [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*, we will also mention the
    meaning of "Dropout", as discussed in this paper (and also mentioned in its title).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "第2章. 神经网络")
    *神经网络* 中，我们讨论了不同的活动函数，尽管逻辑 S 形函数和双曲正切函数通常是最为人知的，但它们往往训练速度较慢。最近，ReLU 活动函数在语音识别中取得了成功应用，例如
    G. Dahl、T. Sainath 和 G. Hinton 在 *Improving Deep Neural Networks for LVCSR Using
    Rectified Linear Units and Dropout* 中提到的文章，发表于国际会议 *Acoustics Speech and Signal
    Processing (ICASSP)* (2013) ([http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf](http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf))。在[第5章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第5章. 图像识别") *图像识别* 中，我们还将提及“Dropout”的含义，正如这篇论文中所讨论的（也在其标题中提到）。
- en: Object recognition and classification
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对象识别和分类
- en: This is perhaps the area where deep neural networks, success is best documented
    and understood. As in speech recognition, DNNs can discover basic representations
    and features automatically. In addition, handpicked features were often able to
    capture only low-level edge information, while DNNs can capture higher-level representations
    such as edge intersections. In 2012, results from the ImageNet Large Scale Visual
    Recognition Competition (the results are available online at [http://image-net.org/challenges/LSVRC/2012/results.html](http://image-net.org/challenges/LSVRC/2012/results.html))
    showed the winning team, composed of Alex Krizhevsky, Ilya Sutskever, and Geoff
    Hinton, using a large network with 60 million parameters and 650,000 neurons with
    five convolutional layers and followed by max-pooling layers, beat the second
    placed team with an error rate of 16.4% versus an error rate of 26.2%. Convolutional
    layers and max-pooling layers will be the focus of [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*. It was a huge and impressive
    result, and that breakthrough result sparked the current renaissance in neural
    networks. The authors used many novel ways to help the learning process by bringing
    together convolutional networks, use of the GPU, and some tricks like dropout
    methods and the use of the ReLU activity function instead of the sigmoid.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是深度神经网络取得成功并得到最好记录和理解的领域。就像语音识别一样，DNN能够自动发现基本表示和特征。此外，手工选择的特征通常只能捕捉低级边缘信息，而DNN能够捕捉到更高级的表示，比如边缘交叉点。在2012年，来自ImageNet大规模视觉识别比赛的结果（结果可在[http://image-net.org/challenges/LSVRC/2012/results.html](http://image-net.org/challenges/LSVRC/2012/results.html)上找到）显示，由Alex
    Krizhevsky，Ilya Sutskever和Geoff Hinton组成的获胜团队使用了一个拥有6000万参数和650,000个神经元的大型网络，其中包括五个卷积层和紧随其后的最大池化层，以16.4%的错误率击败了第二名团队的26.2%的错误率。卷积层和最大池化层将是[第5章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第5章。图像识别")，*图像识别*的焦点。这是一个巨大而令人印象深刻的成果，这一突破性的结果引发了当前神经网络的复兴。作者们使用了许多新颖的方法来通过结合卷积网络、GPU的使用以及一些技巧，比如放弃方法和使用ReLU活性函数代替Sigmoid来帮助学习过程。
- en: The network was trained using GPUs (we will talk about GPU advantages in the
    next section) and showed how a large amount of labeled data can greatly improve
    the performance of deep learning neural nets, greatly outperforming more conventional
    approaches to image recognition and computer vision. Given the success of convolutional
    layers in deep learning, Zeiler and Fergus in two articles (M. Zeiler and R. Fergus,
    *Stochastic pooling for regularization of deep convolutional neural networks*,
    in Proceeding of International Conference on Learning Representations (ICLR),
    2013 ([http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf](http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf))
    and M. Zeiler and R. Fergus, *Visualizing and Understanding Convolutional Networks*,
    arXiv:1311.2901, pages 1-11, 2013, ([http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf)))
    tried to understand why using convolutional networks in deep learning worked so
    well, and what representations were being learned by the network. Zeiler and Fergus
    set to visualize what the intermediate layers captured by mapping back their neural
    activities. They created a de-convolutional network attached to each layer, providing
    a loop back to the image pixels of the input.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络是使用GPU进行训练的（我们将在下一节讨论GPU的优势），并展示了大量标记数据可以极大地提高深度学习神经网络的性能，大大超越了图像识别和计算机视觉的更传统方法。鉴于深度学习中卷积层的成功，Zeiler和Fergus在两篇文章中（M.
    Zeiler和R. Fergus，*用于深度卷积神经网络的随机池化正则化*，国际学习代表大会(ICLR) , 2013年（[http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf](http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf))和M.
    Zeiler和R. Fergus，*视觉化和理解卷积网络*，arXiv:1311.2901, 页面1-11, 2013年，([http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf)）试图了解为什么在深度学习中使用卷积网络效果如此好，以及网络学到了哪些表示。Zeiler和Fergus试图通过映射回他们的神经活动来可视化中间层捕捉到的内容。他们为每个层创建了一个反卷积网络，将其环绕回输入的图像像素。
- en: '![Object recognition and classification](img/00113.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![对象识别和分类](img/00113.jpeg)'
- en: Image taken from M. Zeiler and R. Fergus, Visualizing and Understanding Convolutional
    Networks
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于M. Zeiler和R. Fergus，《视觉化和理解卷积网络》。
- en: The article shows what features are being revealed, where **layer 2** shows
    corner and edges, **layer 3**, different mesh patterns, **layer 4**, dog faces
    and bird legs, and **layer 5** shows entire objects.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 文章展示了正在揭示的特征，其中**第二层**显示了角落和边缘，**第三层**显示了不同的网格图案，**第四层**显示了狗脸和鸟腿，而**第五层**显示了整个对象。
- en: '![Object recognition and classification](img/00114.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![对象识别和分类](img/00114.jpeg)'
- en: Image taken from M. Zeiler and R. Fergus, Visualizing and Understanding Convolutional
    Networks
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图像取自M. Zeiler和R. Fergus，《可视化和理解卷积网络》
- en: 'Deep learning can also be used in unsupervised learning by using networks that
    incorporate RBMs and autoencoders. In an article by Q. Le, M. Ranzato, M. Devin,
    G. Corrado, K. Chen, J. Dean, and A. Ng, *Building high-level features using large
    scale unsupervised learning*, in Proceedings of International Conference on Machine
    Learning (ICML) ([http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf))
    the authors used a 9-layer network with autoencoders, with one billion connections
    trained on 10 million images downloaded from the Internet. The unsupervised feature
    learning allows the system to be trained to recognize faces without being told
    whether the image contains a face or not. In the article the authors state:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习也可以通过使用包含RBM和自编码器的网络进行无监督学习。在Q. Le、M. Ranzato、M. Devin、G. Corrado、K. Chen、J.
    Dean和A. Ng的一篇文章中，*使用大规模无监督学习构建高级特征*，在国际机器学习大会（ICML）论文集中，作者使用了一个9层的自编码器网络，拥有十亿个连接，训练了来自互联网的1000万张图像。无监督特征学习使系统能够被训练以识别是否包含人脸的图像，而不需要告知。在文章中，作者表明：
- en: '*"It is possible to train neurons to be selective for high-level concepts using
    entirely unlabeled data … neurons functions as detectors for faces, human bodies,
    and cat faces by training on random frames of YouTube videos … starting from these
    representations we obtain 15.8% accuracy for object recognition on ImageNet with
    20,000 categories, a significant leap of 70% relative improvement over the state-of-the-art."*'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"通过完全未标记的数据，可以训练神经元对高级概念进行选择性训练……通过对YouTube视频的随机帧进行训练，神经元可以成为面部、人体和猫脸的检测器……从这些表示开始，我们在ImageNet上的对象识别准确率达到了15.8%，其中包括20,000个类别，相对于最先进技术的70%的显著提高。"*'
- en: GPU versus CPU
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU与CPU
- en: One of the reasons for the popularity of deep learning today is the drastically
    increased processing capacity of **GPUs** (**Graphical Processing Units**). Architecturally,
    the **CPU** (**Central Processing Unit**) is composed of a few cores that can
    handle a few threads at a time, while GPUs are composed of hundreds of cores that
    can handle thousands of threads at the same time. A GPU is a highly parallelizable
    unit, compared to the CPU that is mainly a serial unit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如今深度学习受欢迎的一个原因是**GPU**（图形处理单元）的处理能力大幅增加。从架构上看，**CPU**（中央处理单元）由几个核心组成，每次只能处理几个线程，而GPU由数百个核心组成，可以同时处理数千个线程。与主要是串行单元的CPU相比，GPU是高度可并行化的单元。
- en: 'DNNs are composed of several layers, and each layer has neurons that behave
    in the same manner. Moreover, we have discussed how the activity value for each
    neuron is , or, if we express it in matrix form, we have *a = wx*, where *a* and
    *x* are vectors and w a matrix. All activation values are calculated in the same
    way across the network. CPUs and GPUs have a different architecture, in particular
    they are optimized differently: CPUs are latency optimized and GPUs are bandwidth
    optimized. In a deep neural network with many layers and a large number of neurons,
    bandwidth becomes the bottleneck, not latency, and this is the reason why GPUs
    perform so much better. In addition, the L1 cache of the GPU is much faster than
    the L1 cache for the CPU and is also larger.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: DNN由几层组成，每一层的神经元的行为方式相同。此外，我们已经讨论了每个神经元的活动值是如何计算的，或者，如果用矩阵形式表示，我们有*a = wx*，其中*a*和*x*是向量，w是矩阵。在整个网络中，所有激活值都是以相同的方式计算的。CPU和GPU具有不同的架构，特别是它们的优化方式不同：CPU是延迟优化的，而GPU是带宽优化的。在具有许多层和大量神经元的深度神经网络中，带宽成为瓶颈，而不是延迟，这就是为什么GPU性能如此出色的原因。此外，GPU的L1缓存比CPU的L1缓存速度快得多，而且也更大。
- en: The L1 cache represents memory of information that the program is likely to
    use next, and storing this data can speed up the process. Much of the memory gets
    re-used in deep neural networks, which is why L1 cache memory is important. Using
    GPUs, you can get your program, go up to one order of magnitude faster than simply
    using CPUs, and use of this speed-up is also the reason behind much of the recent
    progress in speech and image processing using deep neural networks, an increase
    in computing power that was not available a decade ago.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: L1 缓存表示程序下一步可能要使用的信息的内存，并存储这些数据可以加快处理速度。在深度神经网络中，许多内存会被重复使用，这就是为什么 L1 缓存内存很重要的原因。使用
    GPU，你可以让你的程序的速度比单纯使用 CPU 快上一个数量级，并且这种加速也是近年来在使用深度神经网络进行语音和图像处理方面取得的许多进展背后的原因，这种计算能力的增加在十年前是不可用的。
- en: '![GPU versus CPU](img/00115.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 对比 CPU](img/00115.jpeg)'
- en: 'In addition to be faster for DNN *training*, GPUs are also more efficient to
    run the DNN *inference*. Inference is the post-training phase where we deploy
    our trained DNN. In a whitepaper published by GPU vendor Nvidia titled *GPU-Based
    Deep Learning Inference: A Performance and Power Analysis*, available online at
    [http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf](http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf),
    an efficiency comparison is made between the use of GPUs and CPUs on the AlexNet
    network (a DNN with several convolutional layers) and the results are summarized
    in the following table:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在 DNN *训练* 方面更快外，GPU 在运行 DNN *推理* 时也更有效率。推理是我们部署经过训练的 DNN 的后训练阶段。在 GPU 供应商
    Nvidia 发布的一份名为*基于 GPU 的深度学习推理：性能和功耗分析*的白皮书中，对 AlexNet 网络（具有多个卷积层的 DNN）使用 GPU 和
    CPU 的效率进行了比较，并在以下表格中总结了结果，该白皮书可在线获取：[http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf](http://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf)。
- en: '| Network: AlexNet | Batch Size | Tegra X1 (FP32) | Tegra X1 (FP16) | Core
    i7 6700K (FP32) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 网络：AlexNet | 批大小 | Tegra X1（FP32） | Tegra X1（FP16） | Core i7 6700K（FP32）
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Inference performance | 1 | 47 img/sec | 67 img/sec | 62 img/sec |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 推理性能 | 1 | 47 img/sec | 67 img/sec | 62 img/sec |'
- en: '| Power | 5.5 W | 5.1 W | 49.7 W |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 功耗 | 5.5 W | 5.1 W | 49.7 W |'
- en: '| Performance/Watt | 8.6 img/sec/W | 13.1 img/sec/W | 1.3 img/sec/W |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 性能/瓦特 | 8.6 img/sec/W | 13.1 img/sec/W | 1.3 img/sec/W |'
- en: '| Inference performance | 128 (Tegra X1)48 (Core i7) | 155 img/sec | 258 img/sec
    | 242 img/sec |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 推理性能 | 128 (Tegra X1)48 (Core i7) | 155 img/sec | 258 img/sec | 242 img/sec
    |'
- en: '| Power | 6.0 W | 5.7 W | 62.5 W |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 功耗 | 6.0 W | 5.7 W | 62.5 W |'
- en: '| Performance/Watt | 25.8 img/sec/W | 45 img/sec/W | 3.9 img/sec/W |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 性能/瓦特 | 25.8 img/sec/W | 45 img/sec/W | 3.9 img/sec/W |'
- en: The results show that inference on Tegra X1 can be up to an order of magnitude
    more energy-efficient that CPU-based inference while achieving comparable performance
    levels
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在 Tegra X1 上的推理可以比基于 CPU 的推理节能一个数量级，同时实现可比较的性能水平。
- en: Writing code to access the GPU directly instead of the CPU is not easy, but
    that is why most popular open source libraries like Theano or TensorFlow allow
    you to simply turn on a simple *switch* in your code to use the GPU rather than
    the CPU. Use of these libraries does not require writing specialized code, but
    the same code can run on both the CPU and the GPU, if available. The *switch*
    depends on the open source library, but typically it can be through setting up
    determined environment variables or by creating a specialized resource (`.rc`)
    file that is used by the particular open source library chosen.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 直接编写访问 GPU 而不是 CPU 的代码并不容易，但这就是为什么大多数流行的开源库（如 Theano 或 TensorFlow）允许你简单地在代码中打开一个简单的*开关*来使用
    GPU 而不是 CPU。使用这些库不需要编写专门的代码，但如果可用，同样的代码可以在 CPU 和 GPU 上运行。*开关*取决于开源库，但通常可以通过设置确定的环境变量或创建一个特定的资源（`.rc`）文件来完成，该文件由所选择的特定开源库使用。
- en: Popular open source libraries – an introduction
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的开源库——简介
- en: 'There are several Open Source libraries available that allow the creation of
    deep neural nets in Python without having to explicitly write the code from scratch.
    The most commonly used are: Keras, Theano, TensorFlow, Caffe, and Torch. In this
    book we will provide examples using the first three libraries, which can all be
    used in Python. The reason for this is that Torch is not based on Python, but
    on a different language, called Lua, while Caffe is mainly used for Image recognition
    only. For these libraries, we will quickly describe how to turn on the GPU *switch
    we discussed in the previous paragraph. Much of the code in this book can then
    be run on a CPU or a GPU, depending on the hardware available to the reader.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源库可用，允许在 Python 中创建深度神经网络，而无需显式地从头编写代码。最常用的是：Keras、Theano、TensorFlow、Caffe
    和 Torch。在本书中，我们将提供使用前三个库的示例，这些库都可以在 Python 中使用。这样做的原因是 Torch 不基于 Python，而是基于一种称为
    Lua 的不同语言，而 Caffe 主要用于图像识别。对于这些库，我们将快速描述如何打开*我们在前一段讨论中讨论的 GPU 开关。然后，本书中的大部分代码都可以在
    CPU 或 GPU 上运行，这取决于读者可用的硬件。*
- en: Theano
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Theano
- en: Theano ([http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))
    is an open source library written in Python that implements many features that
    make it easy to write code for neural networks. In addition, Theano makes it very
    easy to take advantage of GPU acceleration and performance. Without going into
    the details of how Theano works, Theano uses symbolic variables and functions.
    Among many features that are really appealing, Theano allows us to use back-propagation
    very easily by calculating all the derivatives for us.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Theano（[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)）是一个用
    Python 编写的开源库，实现了许多使编写神经网络代码变得容易的功能。此外，Theano 也可以很容易地利用 GPU 加速和性能。不深入讨论 Theano
    如何工作的细节，Theano 使用符号变量和函数。在许多真正吸引人的功能中，Theano 允许我们通过为我们计算所有导数来很容易地使用反向传播。
- en: 'As mentioned earlier, Theano also makes it very easily to utilize the GPU on
    your machine. There are many ways to do this, but the simplest is to create a
    resource file called `.theanorc` with the following lines:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，Theano 也可以很容易地利用您计算机上的 GPU。有很多方法可以做到这一点，但最简单的方法是创建一个名为`.theanorc`的资源文件，并包含以下几行：
- en: '[PRE0]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is easy to check whether Theano is configured to use your GPU by simply
    typing:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过简单地输入以下命令来检查 Theano 是否配置为使用您的 GPU：
- en: '[PRE1]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We refer to the Theano documentation for learning the first steps on how to
    use Theano, and we will implement some test code examples using Theano for deep
    learning in this book.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们参考 Theano 文档来学习如何使用 Theano 的第一步，并且我们将在本书中使用 Theano 实现一些深度学习的测试代码示例。
- en: TensorFlow
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'TensorFlow ([https://www.tensorflow.org](https://www.tensorflow.org)) works
    very similarly to Theano, and in TensorFlow, computations are also represented
    as graphs. A TensorFlow graph is therefore a description of computations. In TensorFlow,
    you do not need to explicitly require the use of your GPU, rather, TensorFlow
    will automatically try to use your GPU if you have one, however if you have more
    than one GPU you must assign operations to each GPU explicitly, or only the first
    one will be used. To do this, you simply need to type the line:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow（[https://www.tensorflow.org](https://www.tensorflow.org)）与 Theano
    非常相似，在 TensorFlow 中，计算也表示为图。因此，TensorFlow 图就是对计算的描述。在 TensorFlow 中，您不需要显式地要求使用
    GPU，而是 TensorFlow 将自动尝试使用您的 GPU（如果有的话），但是如果您有多个 GPU，则必须显式地将操作分配给每个 GPU，否则只会使用第一个。要做到这一点，您只需键入以下行：
- en: '[PRE2]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, the following devices can be defined:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以定义以下设备：
- en: '`"/cpu:0"`: main CPU of your machine'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"/cpu:0"`：您的计算机的主 CPU'
- en: '`"/gpu:0"`: first GPU of your machine, if one exists'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"/gpu:0"`：如果存在的话，您计算机的第一个 GPU'
- en: '`"/gpu:1"`: second GPU of your machine, if it exists'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"/gpu:1"`：如果存在的话，您计算机的第二个 GPU'
- en: '`"/gpu:2"`: third GPU of your machine, if it exists, and so on'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"/gpu:2"`：如果存在的话，您计算机的第三个 GPU，以此类推'
- en: Once again, we refer to the TensorFlow documentation for learning the first
    steps on how to use TensorFlow and test code examples using TensorFlow will be
    implemented in the book.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们参考 TensorFlow 文档来学习如何使用 TensorFlow 的第一步，并且测试使用 TensorFlow 的代码示例将在本书中实现。
- en: Keras
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: 'Keras ([http://keras.io](http://keras.io)) is a neural net Python library that
    can run on top of either Theano or TensorFlow, even though it will run by default
    using TensorFlow. Instructions online are provided at [http://keras.io/backend/](http://keras.io/backend/).
    Keras can run on a CPU or GPU and to do that, if you run it on top of Theano,
    you will need to set up a `.theanorc` file as described before. Keras allows different
    ways to create deep neural networks, and it makes it easy by using a *model* for
    the neural network. The main type of *model* is the `Sequential` model which creates
    a linear stack of layers. You can then add new layers by simply calling the `add`
    function. In the coming section, we will create a few examples using Keras. Keras
    can be easily installed with the following, simple command:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Keras ([http://keras.io](http://keras.io)) 是一个可以在 Theano 或 TensorFlow 上运行的神经网络
    Python 库，尽管默认情况下会使用 TensorFlow 运行。在线提供了 [http://keras.io/backend/](http://keras.io/backend/)
    的说明。Keras 可以在 CPU 或 GPU 上运行，如果你在 Theano 上运行它，你将需要像之前描述的那样设置一个 `.theanorc` 文件。Keras
    允许不同的方式创建深度神经网络，它通过使用 *model* 来使其变得简单。主要类型的 *model* 是 `Sequential` model，它创建了一个线性堆叠的层。然后你可以通过简单调用
    `add` 函数来新增层。在接下来的部分中，我们将使用 Keras 创建一些示例。Keras 可以通过以下简单命令轻松安装：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It can also be installed by forking from its Git repository and then running
    setup on it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过从其 Git 存储库派生然后在上面运行设置来安装它：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, we refer the reader to the online documentation for further information.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们建议读者查阅在线文档以获取更多信息。
- en: Sample deep neural net code using Keras
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Keras 的深度神经网络示例代码
- en: In this section, we will introduce some simple code to use Keras for the correct
    classification of digits using the popular dataset MNIST. MNIST is a dataset comprised
    of 70,000 examples of handwritten digits by many different people. The first 60,000
    are typically used for training and the remaining 10,000 for testing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些简单的代码，使用 Keras 对使用流行数据集 MNIST 正确分类数字。MNIST 是一个包含许多不同人手写数字的数据集，共有
    70,000 个示例。通常，前 60,000 个用于训练，剩下的 10,000 个用于测试。
- en: '![Sample deep neural net code using Keras](img/00116.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 的深度神经网络示例代码](img/00116.jpeg)'
- en: Sample of digits taken from the MNIST dataset
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MNIST 数据集中获取的数字示例
- en: 'One of the advantages of Keras is that it can import this dataset for you without
    the need to explicitly download it from the web (Keras will download it for you).
    This can be achieved by one simple line of code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一个优点是它可以为你导入这个数据集，而无需显式从网络上下载它（Keras 会为你下载）。这可以通过一行简单的代码实现：
- en: '[PRE5]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There are a few classes we need to import from Keras to use a classical deep
    neural net, and these are:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从 Keras 导入一些类来使用经典的深度神经网络，它们是：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are now ready to start writing our code to import the data, and we can do
    this with just one line:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始编写导入数据的代码，我们可以用一行代码完成：
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This imports the training data and the testing data; in addition, both datasets
    are divided into two subsets: one that contains the actual images and the other
    that contains the labels. We need to slightly modify the data to be able to use
    it. The `X_train` and `X_test` data in fact is comprised of 60000 small (28,28)-pixels
    images, but we want to reshape each sample to be a 784-pixels long vector, rather
    than a (28,28) 2-dimensional matrix. This can be easily accomplished with these
    two lines:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这导入了训练数据和测试数据；此外，这两个数据集被分为两个子集：一个包含实际图像，另一个包含标签。我们需要稍微修改数据以便使用它。事实上，`X_train`
    和 `X_test` 数据包括了 60000 个小的 (28,28) 像素图像，但我们想将每个样本重新塑造为一个 784 像素长的向量，而不是一个 (28,28)
    的二维矩阵。这可以通过以下两行轻松实现：
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Similarly, the labels are indicating the value of the digit depicted by the
    images, and we want to convert this into a 10-entry vector with all zeroes and
    just one 1 in the entry corresponding to the digit, so for example 4 is mapped
    to [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，标签指示了图像所描述的数字的值，我们希望将其转换为一个包含全部零值和仅在对应于该数字的条目中有一个 1 的 10-entry 向量，因此例如 4
    被映射为 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]。
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, before calling our main function, we just need to set the size of
    our input (the size of the `mnist` images), how many hidden neurons our hidden
    layer has, for how many epochs we want to try our network, and the batch size
    for our training:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在调用我们的主函数之前，我们只需设置我们的输入大小（`mnist` 图像的大小）、隐藏层有多少个隐藏神经元、我们想要尝试我们网络的时期数量以及训练的批次大小：
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are now ready to write the code for our main function. Keras works by defining
    a model, and we will use the `Sequential` model, then add layers (in this case
    we will use regular *dense*, not sparse layers) specifying the number of input
    and output neurons. For each layer, we specify the activity function of its neurons:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好为我们的主函数编写代码了。Keras通过定义一个模型来工作，我们将使用`Sequential`模型，然后添加层（在这种情况下，我们将使用常规的*
    dense *，而不是稀疏层）指定输入和输出神经元的数量。对于每一层，我们指定其神经元的活动函数：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Keras now provides a simple way to specify the cost function (the `loss`) and
    its optimization (training rate, momentum, and so on). We are not going to modify
    the default values, so we can simply pass:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Keras提供了一种简单的方法来指定成本函数（`loss`）及其优化（训练速率、动量等）。我们不打算修改默认值，因此我们可以简单地传递：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, the optimizer is `sgd`, which stands for stochastic gradient
    descent. At this point, we need to train the network, and this, similarly to scikit-learn,
    is done calling a `fit` function. We will use the verbose parameter so that we
    can follow the process:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，优化器是`sgd`，代表随机梯度下降。在这一点上，我们需要训练网络，这与scikit-learn类似，通过调用`fit`函数完成。我们将使用verbose参数，以便可以跟踪这个过程：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All that is left to do is to add code to evaluate our network on the test data
    and print the accuracy result, which is done simply by:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的事情是添加代码来评估我们的网络在测试数据上的表现并打印准确率结果，这很简单：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And that's it. It is now enough to run. The test accuracy will be about 94%,
    which is not a great result, but this example runs in less than 30 seconds on
    a CPU and is an extremely simple implementation. There are simple improvements
    that could be made, for example selecting a larger number of hidden neurons or
    selecting a larger number of epochs, and we leave those simple changes to the
    reader to familiarize himself or herself with the code.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部。现在可以运行了。测试准确率大约为94%，这不是一个很好的结果，但这个例子在CPU上运行时间不到30秒，是一个非常简单的实现。有一些简单的改进可以做，比如选择更多的隐藏神经元或选择更多的epochs，我们把这些简单的改变留给读者自己去熟悉代码。
- en: 'Keras allows us to also look at the weight matrix it creates. To do that, it
    is enough to type the following line:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Keras还允许我们查看它创建的权重矩阵。要做到这一点，只需键入以下行：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'By adding the following lines to our previous code, we can look at what the
    hidden neurons have learned:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在我们之前的代码中添加以下行，我们可以看看隐藏神经元学到了什么：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To get a clearer image, we have increased the number of epochs to 100 to get
    the following figure:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到更清晰的图像，我们将epochs的数量增加到100，得到以下的图形：
- en: '![Sample deep neural net code using Keras](img/00117.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras的示例深度神经网络代码](img/00117.jpeg)'
- en: Composite figure with what was learned by all the hidden neurons
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所有隐藏神经元学到的内容组成的复合图形
- en: For simplicity, we have aggregated all the images for each neuron in a single
    figure that represents a composite for all the neurons. Clearly, since the initial
    images are very small and do not have lots of details (they are just digits),
    the features learned by the hidden neurons are not all that interesting, but it
    is already clear that each neuron is learning a different "shape".
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将每个神经元的所有图像聚合到一个单独的图形中，表示所有神经元的复合图形。显然，由于初始图像非常小且没有很多细节（它们只是数字），隐藏神经元学到的特征并不是很有趣，但已经清楚每个神经元都学到了不同的“形状”。
- en: 'The code for drawing above should be immediately clear; we just notice that
    the following line is importing `cm`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的绘图代码应该立即清晰明了；我们只注意到以下行正在导入`cm`：
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This simply allows for a grayscale representation of the neurons, and it is
    used inside the `imshow()` call by passing in the option `cmap = cm.Greys_r`.
    This is because the `mnist` images are not color images but gray scale images.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是允许对神经元进行灰度表示，它在`imshow()`调用中使用，通过传递选项`cmap = cm.Greys_r`。这是因为`mnist`图像不是彩色图像，而是灰度图像。
- en: 'The beauty of Keras is that it is easy to create neural nets, but it is also
    easy to download test datasets. Let''s try to use the `cifar10` dataset instead
    of the `mnist` dataset. Instead of digits, the `cifar10` dataset is comprised
    of 10 classes of objects: airplanes, automobiles, birds, cats, deers, dogs, frogs,
    horses, ships, and trucks. To use the `cifar10` dataset, it is enough to write:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的美妙之处在于它很容易创建神经网络，但也很容易下载测试数据集。让我们尝试使用`cifar10`数据集而不是`mnist`数据集。`cifar10`数据集不是数字，而是由10类对象组成：飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。要使用`cifar10`数据集，只需写：
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In place of the preceding code line:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行的位置：
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we need to make these changes to the code we wrote above:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要对上面编写的代码进行以下更改：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is because there are only 50,000 training images (instead of 60,000) and
    because the images are colored (RGB) 32 x 32 pixel images, therefore their size
    is 3 x 32 x 32\. We can keep everything else as before for now, however, if we
    run this example, we can see that our performance is now very bad, just around
    20%. This is due to the fact that the data is much more complex and it requires
    a more complex neural network. In fact, most neural networks implemented for classification
    of images use some basic convolutional layers, that will be discussed only in
    [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 5. Image
    Recognition"), *Image Recognition*, however, for now we could try raising the
    number of hidden neurons to 3,000, and adding a second hidden layer with 2,000
    neurons. We are also going to use the ReLU activity function in the first hidden
    layer.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为训练图像仅有50,000张（而不是60,000张），并且图像是彩色（RGB）32 x 32像素图像，因此它们的大小是3 x 32 x 32。就目前而言，我们可以保持其他一切不变，但是，如果我们运行这个示例，我们会发现我们的性能现在非常差，只有大约20%。这是因为数据更加复杂，需要更复杂的神经网络。事实上，大多数用于图像分类的神经网络都使用一些基本的卷积层，这将在[第5章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第五章。图像识别")中讨论，*图像识别*，然而，现在我们可以尝试将隐藏神经元数提高到3,000，并添加一个包含2,000个神经元的第二个隐藏层。我们还将在第一个隐藏层中使用ReLU激活函数。
- en: 'To do this we simply need to write the following lines defining the model,
    instead of what we had before:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们只需要写下以下定义模型的行，而不是之前的内容：
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we run this code, we will see that it will take much longer to train, however,
    at the end, we will have about a 60% accuracy rate for the training set, but only
    about a 50% accuracy for the test data. The much poorer accuracy rate, with respect
    to the much simpler `mnist` dataset, despite the larger network and the much longer
    training time, is due to the higher complexity of the data. In addition, by substituting
    the line where we fit the network with the following line:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这段代码，我们会发现训练时间要长得多，但是最后，我们的训练集准确率约为60%，而测试数据的准确率只有约50%。与较简单的`mnist`数据集相比，尽管网络更大，训练时间更长，但准确率要低得多，这是由于数据的复杂性更高。此外，通过将适配网络的行替换为以下行：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can also output during the process how the accuracy improves on a split 90/10
    of the training data. This also shows that while the accuracy of the training
    keeps increasing during training, the accuracy of the validation set plateaus
    at some point, showing that the network starts to overfit and to saturate some
    parameters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在过程中输出训练数据分割为90/10的准确性如何提高。这也表明，尽管训练的准确性在训练过程中不断提高，但验证集的准确性在某一点上会达到饱和，表明网络开始过拟合并饱和一些参数。
- en: While this may seem like a failure of deep networks to deliver good accuracy
    on richer datasets, we will see that in fact, there are ways around this problem
    that will allow us to get better performances on even much more complex and larger
    datasets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能看起来是深度网络在更丰富的数据集上无法提供良好准确性的失败，但事实上，我们将会看到，实际上有一些方法可以解决这个问题，让我们能够在更复杂更大的数据集上获得更好的性能。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have walked the reader toward an understanding of what deep
    learning is and how it is related to deep neural networks. We have also discussed
    how many different implementations of deep neural networks exist, besides the
    classical feed-forward implementation, and have discussed the recent successes
    deep learning has had on many standard classification tasks. This chapter has
    been rich with concepts and ideas, developed through examples and historical remarks
    from the Jacquard loom to the Ising model. This is just the beginning, and we
    will work out many examples in which the ideas introduced in this chapter will
    be explained and developed more precisely.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们引导读者理解了深度学习的概念以及它与深度神经网络的关系。我们还讨论了除了经典的前馈实现之外，还存在许多不同的深度神经网络实现，并讨论了深度学习在许多标准分类任务上取得的最新成功。本章充满了从Jacquard织布机到伊辛模型的概念和想法，通过示例和历史评论进行了发展。这只是一个开始，我们将在许多示例中解释并更准确地发展本章介绍的思想。
- en: We are going to start this process in the coming chapter, where we will finally
    introduce the readers to many of the concepts we have touched on in this one,
    like RBMs and auto-encoders, and it will be clear how we can create more powerful
    deep neural networks than simple feed-forward DNNs. In addition, it will also
    be clear how the concept of representations and features arise naturally in these
    particular neural networks. From the last example, using cifar10, it is clear
    that classical feed-forward DNNs are difficult to train on more complex datasets,
    and we need a better way to set the weight parameters. X. Glorot and Y. Bengio,
    *Understanding the difficulty of training deep feed-forward neural* networks,
    in Proceedings of the International Conference on Artificial Intelligence and
    Statistics (AISTATS'10), (2010) ([http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)),
    treats the issue of poor performance of deep neural networks trained with gradient
    descent on random weight initialization. New algorithms that can be used to train
    deep neural networks successfully will be introduced and discussed in the next
    chapter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节开始这个过程，并向读者介绍许多我们在本章中涉及的概念，比如 RBM 和自编码器，以及清楚地展示我们如何创建比简单的前馈 DNN 更强大的深度神经网络。此外，在这些特定的神经网络中，表示和特征的概念如何自然地产生也将变得清晰。从上一个例子，使用
    cifar10 数据集，可以清楚地看出经典的前馈 DNN 很难在更复杂的数据集上进行训练，我们需要更好的方法来设置权重参数。X. Glorot 和 Y. Bengio
    在其论文《*Understanding the difficulty of training deep feed-forward neural* networks》中探讨了使用梯度下降来训练具有随机权重初始化的深度神经网络的性能不佳的问题。下一章将介绍并讨论可以成功训练深度神经网络的新算法。
