["```py\nsudo apt update\napt-cache show python3-scipy\nsudo apt install -y python3-scipy\n```", "```py\nfrom scipy import stats\nimport numpy as np\nfrom sense_hat import SenseHat\nimport json\nfrom kafka import KafkaProducer\nimport time\n```", "```py\ntime.sleep(60)\n```", "```py\ndevice= \"Pi1\"\nserver = \"[the address of the kafka server]:9092\"\nproducer = KafkaProducer(bootstrap_servers=server)\nsense = SenseHat()\nsense.set_imu_config(False, True, True) \ngyro = []\naccel = [] \n```", "```py\ndef zscore(data):\n    return np.abs(stats.zscore(np.array(data)))[0]\n```", "```py\ndef sendAlert(lastestGyro,latestAccel):\n    alert = {'Gyro':lastestGyro, 'Accel':latestAccel}\n    message = json.dumps(alert)\n    producer.send(device+'alerts', key=bytes(\"alert\", \n                                             encoding='utf-8'),\n                  value=bytes(message, encoding='utf-8'))\n\n```", "```py\ndef combined_value(data):\n    return float(data['x'])+float(data['y'])+float(data['z'])\n```", "```py\nif __name__ == '__main__': \n    x = 0\n    while True:\n        gyro.insert(0,sense.gyro_raw)\n        accel.insert(0,sense.accel_raw)\n        if x > 1000: \n            gyro.pop() \n            accel.pop() \n        time.sleep(1)\n        x = x + 1\n        if x > 120:\n            if zscore(gyro) > 4 or zscore(accel) > 4:\n                sendAlert(gyro[0],accel[0]) \n```", "```py\nfrom pyod.models.auto_encoder import AutoEncoder\nfrom pyod.utils.data import generate_data\nfrom pyod.utils.data import evaluate_print\nimport numpy as np\nimport pickle\n```", "```py\nX_train = np.loadtxt('X_train.txt', dtype=float)\ny_train = np.loadtxt('y_train.txt', dtype=float)\nX_test = np.loadtxt('X_test.txt', dtype=float)\ny_test = np.loadtxt('y_test.txt', dtype=float)\n```", "```py\nclf = AutoEncoder(epochs=30)\nclf.fit(X_train)\n```", "```py\ny_test_pred = clf.predict(X_test) # outlier labels (0 or 1)\ny_test_scores = clf.decision_function(X_test) # outlier scores\nevaluate_print('AutoEncoder', y_test, y_test_scores)\n```", "```py\npickle.dump( clf, open( \"autoencoder.p\", \"wb\" ) )\n```", "```py\nAutoEncoder(epochs=30, contamination=0.2)\n```", "```py\nAutoEncoder(epochs=30, l2_regularizer=0.2)\n```", "```py\nfrom pyod.models.iforest import IForest\nfrom pyod.utils.data import generate_data\nfrom pyod.utils.data import evaluate_print\nimport numpy as np\nimport pickle\n```", "```py\nX_train = np.loadtxt('X_train.txt', dtype=float)\ny_train = np.loadtxt('y_train.txt', dtype=float)\nX_test = np.loadtxt('X_test.txt', dtype=float)\ny_test = np.loadtxt('y_test.txt', dtype=float)\n```", "```py\nclf = IForest()\nclf.fit(X_train)\n```", "```py\ny_test_pred = clf.predict(X_test) # outlier labels (0 or 1)\ny_test_scores = clf.decision_function(X_test) \nprint(y_test_pred)\n\n# evaluate and print the results\nprint(\"\\nOn Test Data:\")\nevaluate_print('IForest', y_test, y_test_scores)\n```", "```py\npickle.dump( clf, open( \"IForest.p\", \"wb\" ) )\n```", "```py\npip install luminol\n```", "```py\nimport pandas as pd \n\ndf = pd.read_csv('Beach_Water_Quality_-_Automated_Sensors.csv', \n                  header=0)\n\ndf = df[df['Beach Name'] == 'Rainbow Beach']\ndf = df[df['Water Temperature'] > -100]\ndf = df[df['Wave Period'] > -100]\ndf['Measurement Timestamp'] = pd.to_datetime(df['Measurement\n                                                 Timestamp'])\n\nTurbidity = df[['Measurement Timestamp', 'Turbidity']]\nTurbidity.to_csv('Turbidity.csv', index=False, header=False)\n```", "```py\nfrom luminol.anomaly_detector import AnomalyDetector\nimport time\n```", "```py\nmy_detector = AnomalyDetector('Turbidity.csv')\nscore = my_detector.get_all_scores()\n```", "```py\nfor (timestamp, value) in score.iteritems():\n    t_str = time.strftime('%y-%m-%d %H:%M:%S', \n                          time.localtime(timestamp))\n    if value > 0:\n        print(f'{t_str}, {value}')\n```", "```py\npip install sesd\n```", "```py\nimport pandas as pd \nimport sesd\nimport numpy as np\n```", "```py\ndf = pd.read_csv('Beach_Water_Quality_-_Automated_Sensors.csv',\n                  header=0)\ndf = df[df['Beach Name'] == 'Rainbow Beach']\ndf = df[df['Water Temperature'] > -100]\ndf = df[df['Wave Period'] > -100]\nwaveheight = df[['Wave Height']].to_numpy()\n```", "```py\noutliers_indices = sesd.seasonal_esd(waveheight, hybrid=True,\n                                     max_anomalies=2)\n```", "```py\nfor idx in outliers_indices:\n    print(\"Anomaly index: {}, anomaly value: {}\"\\\n           .format(idx, waveheight[idx]))\n```", "```py\n#device.py\n\nimport time\nfrom azure.iot.device import IoTHubDeviceClient, Message\nfrom sense_hat import SenseHat\nimport json\n```", "```py\nclient = IoTHubDeviceClient.create_from_connection_string(\"your device key here\")\nsense = SenseHat()\nsense.set_imu_config(True, True, True) \n```", "```py\ndef combined_value(data):\n    return float(data['x'])+float(data['y'])+float(data['z'])\n```", "```py\nwhile True:\n    gyro = combined_value(sense.gyro_raw)\n    accel = combined_value(sense.accel_raw)\n\n    msg_txt_formatted = msg.format(gyro=gyro, accel=accel)\n    message = Message(msg_txt_formatted)\n    client.send_message(message)\n\n    time.sleep(1)\n```", "```py\n    SELECT\n        EVENTENQUEUEDUTCTIME AS time,\n        CAST(gyro AS float) AS gyro,\n        AnomalyDetection_SpikeAndDip(CAST(gyro AS float), 95, 120, 'spikesanddips')\n            OVER(LIMIT DURATION(second, 120)) AS SpikeAndDipScores\n    INTO output\n    FROM tempin\n```", "```py\n#Gather.py\n\nimport numpy as np\nfrom sense_hat import SenseHat\nimport json\nimport time\n```", "```py\nsense = SenseHat()\nsense.set_imu_config(True, True, True) \nreadings = 1000\ngyro,accel = sense.gyro_raw, sense.accel_raw\nactions = ['normal', 'anomolous']\ndat = np.array([gyro['x'], gyro['y'], gyro['z'], accel['x'],\n                accel['y'], accel['z']])\nx = 1\n```", "```py\nfor user_input in actions:\n     activity = input('Hit enter to record '+user_input + \\\n                      ' activity')\n```", "```py\n    x = 1\n    while x < readings:\n        x = x + 1\n        time.sleep(0.1)\n        gyro,accel = sense.gyro_raw, sense.accel_raw\n        dat = np.vstack([dat, [[gyro['x'], gyro['y'], gyro['z'],\n                         accel['x'], accel['y'], accel['z']]]])\n        print(readings - x)\n```", "```py\nX_test = np.concatenate((np.full(800,0), np.full(800,1)), axis=0) \ny_test = np.concatenate((np.full(200,0), np.full(200,1)), axis=0) \nX_train = np.concatenate((dat[0:800,:],dat[1000:1800]))\ny_train = np.concatenate((dat[800:1000],dat[1800:2000]))\n\nnp.savetxt('y_train.txt', y_train,delimiter=' ', fmt=\"%10.8f\")\nnp.savetxt('y_test.txt',y_test, delimiter=' ',fmt=\"%10.8f\") \nnp.savetxt('X_train.txt', X_train,delimiter=' ', fmt=\"%10.8f\")\nnp.savetxt('X_test.txt',X_test, delimiter=' ',fmt=\"%10.8f\") \n```", "```py\n#AnomalyDetection.py\n\nimport numpy as np\nfrom sense_hat import SenseHat\nfrom pyod.models.iforest import IForest\nfrom pyod.utils.data import generate_data\nfrom pyod.utils.data import evaluate_print\nimport pickle\nsense = SenseHat()\n```", "```py\nclf = pickle.load( open( \"IForrest.p\", \"rb\" ) )\n```", "```py\ndef transform(arr):\n    ret = []\n    for z in arr:\n        for a in z:\n            ret.append(a)\n    return ret\n\nO = (10, 10, 10) # Black\nX = (255, 0 ,0) # red\n\nalert = transform([\n        [X, X, O, O, O, O, X, X],\n        [X, X, X, O, O, X, X, X],\n        [O, X, X, X, X, X, X, O],\n        [O, O, X, X, X, X, O, O],\n        [O, O, X, X, X, X, O, O],\n        [O, X, X, X, X, X, X, O],\n        [X, X, X, O, O, X, X, X],\n        [X, X, O, O, O, O, X, X]\n        ])\n\nclear = transform([\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O],\n        [O, O, O, O, O, O, O, O]\n        ])\n```", "```py\nwhile True:\n    dat = np.array([gyro['x'], gyro['y'], gyro['z'], accel['x'],\n                    accel['y'], accel['z']])\n    pred = clf.predict(dat)\n    if pred[0] == 1:\n        sense.set_pixels(alert)\n    else:\n        sense.set_pixels(clear)\n\n    time.sleep(0.1)\n```"]