- en: Exploring Few-Shot Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! We have made it to the final chapter. We have come a long way.
    We started off by learning what neural networks are and how they are used to recognize
    handwritten digits. Then we explored how to train neural networks with gradient
    descent algorithms. We also learned how recurrent neural networks i used for sequential
    tasks and how convolutional neural networks are used for image recognition. Following
    this, we investigated how the semantics of a text can be understood using word
    embedding algorithms. Then we got familiar with several different types of generative
    adversarial networks and autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned that deep learning algorithms perform exceptionally
    well when we have a substantially large dataset. But how can we handle the situation
    when we don't have a large number of data points to learn from? For most use cases,
    we might not get a large dataset. In such cases, we can use few-shot learning
    algorithms, which do not require huge datasets to learn from. In this chapter,
    we will understand how exactly few-shot learning algorithms learn from a smaller
    number of data points and we explore different types of few-shot learning algorithms.
    First, we will study a popular few-shot learning algorithm called a **siamese
    network**. Following this, we will learn some other few-shot learning algorithms
    such as the prototypical, relation, and matching networks intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will study the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is few-shot learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siamese networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of siamese networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prototypical networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relation networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is few-shot learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning from a few data points is called **few-shot** **learning** or **k-shot
    learning**, where k specifies the number of data points in each of the class in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Consider we are performing an image classification task. Say we have two classes
    – apple and orange – and we try to classify the given image as an apple or orange.
    When we have exactly one apple and one orange image in our training set, it is
    called one-shot learning; that is, we are learning from just one data point per
    each of the class. If we have, say, 11 images of an apple and 11 images of an
    orange, then that is called 11-shot learning. So, k in k-shot learning implies
    the number of data points we have per class.
  prefs: []
  type: TYPE_NORMAL
- en: There is also **zero-shot learning**, where we don't have any data points per
    class. Wait. What? How can we learn when there are no data points at all? In this
    case, we will not have data points, but we will have meta information about each
    of the class and we will learn from the meta information.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have two classes in our dataset, that is, apple and orange, we can
    call it **two-way k-shot learning**. So, in n-way k-shot learning, n-way implies
    the number of classes we have in our dataset and k-shot implies a number of data
    points we have in each class.
  prefs: []
  type: TYPE_NORMAL
- en: We need our models to learn from just a few data points. In order to attain
    this, we train them in the same way; that is, we train the model on a very few
    data points. Say we have a dataset, ![](img/8ea8bd84-c391-4118-99ea-2b799e44ae1c.png).
    We sample a few data points from each of the classes present in our dataset and
    we call it **support set**. Similarly, we sample some different data points from
    each of the classes and call it **query set**.
  prefs: []
  type: TYPE_NORMAL
- en: We train the model with a support set and test it with a query set. We train
    the model in an episodic fashion—that is, in each episode, we sample a few data
    points from our dataset, ![](img/191373ea-439a-4898-b792-99920d642af2.png), prepare
    our support set and query set, and train on the support set and test on the query
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Siamese networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Siamese networks are special types of neural networks and are among the simplest
    and most popularly used one-shot learning algorithms. As we have learned in the
    previous section, one-shot learning is a technique where we learn from only one
    training example per each class. So, siamese networks are predominantly used in
    applications where we don't have many data points for each of the class.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's say we want to build a face recognition model for our organization
    and say about 500 people are working in our organization. If we want to build
    our face recognition model using a **convolutional neural network** (**CNN**)
    from scratch then we need many images of all these 500 people, to train the network
    and attain good accuracy. But, apparently, we will not have many images for all
    these 500 people and therefore it is not feasible to build a model using a CNN
    or any deep learning algorithm unless we have sufficient data points. So, in these
    kinds of scenarios, we can resort to a sophisticated one-shot learning algorithm
    such as a siamese network, which can learn from fewer data points.
  prefs: []
  type: TYPE_NORMAL
- en: But how do siamese networks work? Siamese networks basically consist of two
    symmetrical neural networks both sharing the same weights and architecture and
    both joined together at the end using an energy function, ![](img/4526cd01-c2d0-42c7-9bf7-d3b4d66ef67a.png).
    The objective of our siamese network is to learn whether the two inputs are similar
    or dissimilar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have two images, ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)and
    ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png), and we want to learn whether
    the two images are similar or dissimilar. As shown in the following diagram, we
    feed **Image ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)** to **Network
    ![](img/834d0bf7-d38d-42a0-b532-aa3ce2d0fb73.png)** and **Image ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png)**
    to **Network ![](img/17ea0a48-207f-43ce-a8f0-4dad08a04873.png)**. The role of
    both of these networks is to generate embeddings (feature vectors) for the input
    image. So, we can use any network that will give us embeddings. Since our input
    is an image, we can use a convolutional network to generate the embeddings: that
    is, for extracting features. Remember that the role of the CNN here is only to
    extract features and not to classify.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know that these networks should have same weights and architecture, if
    **Network** ![](img/c6c1992c-2bb2-4c35-9192-a57f24bbb3b9.png) is a three-layer
    CNN then **Network** ![](img/f605751c-593e-4a12-888b-d0f0e26d3c83.png) should
    also be a three-layer CNN, and we have to use the same set of weights for both
    of these networks. So, **Network** ![](img/6deb49ca-2fd4-4b61-9f72-407d8308db55.png)
    and **Network** ![](img/f8153a01-1897-4a05-b7a5-38cb2f6927a0.png) will give us
    the embeddings for input images ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)
    and ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png) respectively. Then, we will
    feed these embeddings to the energy function, which tells us how similar the two
    input images are. Energy functions are basically any similarity measure, such
    as Euclidean distance and cosine similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a023ef9e-46ed-4111-b409-65a54e6af39b.png)'
  prefs: []
  type: TYPE_IMG
- en: Siamese networks are not only used for face recognition, but are is also used
    extensively in applications where we don't have many data points and tasks where
    we need to learn the similarity between two inputs. The applications of siamese
    networks include signature verification, similar question retrieval, and object
    tracking. We will study siamese networks in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of siamese networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a basic understanding of siamese networks, we will explore
    them in detail. The architecture of a siamese network is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fba47a65-a1b0-4603-a152-fb501e006d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding figure, a siamese network consists of two identical
    networks, both sharing the same weights and architecture. Let's say we have two
    inputs, ![](img/9077854b-355a-4606-ae06-2e521038b578.png) and ![](img/36a7ea70-62f4-495a-a246-b07e25a85d07.png).
    We feed **Input** ![](img/e2aa271e-c7fc-474a-94e7-d7c5cfb8b422.png) to **Network**
    ![](img/259f6957-4b31-438d-87a3-6d8947fc147d.png), that is, ![](img/313a447d-8f4f-459a-a019-24da4ab85aa9.png),
    and we feed **Input** ![](img/4fb3738e-00c5-48ba-bab4-0c50b08a1091.png) to **Network**
    ![](img/45a57887-05f2-445f-a95b-36f97e73443d.png), that is, ![](img/f5975395-4aee-4902-8149-dd528e1c6954.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, both of these networks have the same weights, ![](img/84870513-e640-4667-b014-56bd8b5f4cd8.png),
    and they will generate embeddings for our input, ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png)
    and ![](img/64a84803-b209-4c29-968a-40722def7dab.png). Then, we feed these embeddings
    to the energy function, ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png), which
    will give us similarity between the two inputs. It can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60c5a706-06ce-4d7e-b0fc-925a5990908d.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say we use Euclidean distance as our energy function; then the value of
    ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png) will be low if ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png)
    and ![](img/64a84803-b209-4c29-968a-40722def7dab.png) are similar. The value of
    ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png) will be large if the input values
    are dissimilar.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that you have two sentences, sentence 1 and sentence 2\. We feed sentence
    1 to Network ![](img/d473d10d-c2d9-4418-aade-1cf3e4a9f88e.png) and sentence 2
    to Network ![](img/9861dfc9-0ae1-4404-b665-77f3142dbb69.png). Let's say both our
    Network ![](img/a1bc9ed3-da8e-433e-863d-cc78eafdb98b.png) and Network ![](img/6e4329a8-431d-4a05-b1e6-5b54b056f4e3.png)
    are **long short-term memory** (**LSTM**) networks and they share the same weights.
    So, Network ![](img/9f2f26fc-4e1a-4708-ba2a-045709cd84da.png) and Network ![](img/af618eac-a43c-43f6-9d9a-e282ccd75ee8.png)
    will generate the embeddings for sentence 1 and sentence 2 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we feed these embeddings to the energy function, which gives us the similarity
    score between the two sentences. But how can we train our siamese networks? How
    should the data be? What are the features and labels? What is our objective function?
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to the siamese networks should be in pairs, ![](img/95ed580d-79e1-4be1-89ea-d5f1e297aaff.png),
    along with their binary label, ![](img/55b589b4-d02c-435f-9325-5599a12cb16d.png),
    stating whether the input pairs are a genuine pair the(same) or an imposite pair
    (different). As you can see in the following table, we have sentences as pairs
    and the label implies whether the sentence pairs are genuine (1) or imposite (0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99490a38-0e57-4305-a8e0-d9f2a1439581.png)'
  prefs: []
  type: TYPE_IMG
- en: So, what is the loss function of our siamese network?
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the goal of the siamese network is not to perform a classification task
    but to understand the similarity between the two input values, we use the contrastive
    loss function. It can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4feefc7f-35a0-48a2-9be7-ca2afa4d4568.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the value of ![](img/b9498bd7-3711-489f-b50f-79c68bb299d5.png)
    is the true label, which will be 1 if the two input values are similar and 0 if
    the two input values are dissimilar, and ![](img/3c536afe-2aa2-4305-ba0f-8c9bc7667612.png)
    is our energy function, which can be any distance measure. The term **margin**
    is used to hold the constraint, that is, when two input values are dissimilar,
    and if their distance is greater than a margin, then they do not incur a loss.
  prefs: []
  type: TYPE_NORMAL
- en: Prototypical networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prototypical networks are yet another simple, efficient, and popular learning
    algorithm. Like siamese networks, they try to learn the metric space to perform
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea of the prototypical network is to create a prototypical representation
    of each class and classify a query point (new point) based on the distance between
    the class prototype and the query point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have a support set comprising images of lions, elephants, and
    dogs, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a44e2b6a-2aaf-491f-82d4-d5c39b8cae7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have three classes (lion, elephant, and dog). Now we need to create a prototypical
    representation for each of these three classes. How can we build the prototype
    of these three classes? First, we will learn the embeddings of each data point
    using some embedding function. The embedding function, ![](img/5345c9dc-00e0-49f3-9e70-189f55e5f7b0.png)
    ,can be any function that can be used to extract features. Since our input is
    an image, we can use the convolutional network as our embedding function, which
    will extract features from the input images, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfab0e2b-76b0-4aa9-ac1d-96f005d59402.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we learn the embeddings of each data point, we take the mean embeddings
    of data points in each class and form the class prototype, shown as follows. So,
    a class prototype is basically the mean embeddings of data points in a class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8abebe0-665b-4307-b3cf-c2c9d17dd17d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, when a new data point comes in, that is, a query point for which
    we want to predict the label, we will generate the embeddings for this new data
    point using the same embedding function that we used to create the class prototype:
    that is, we generate the embeddings for our query point using the convolutional
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8db1780e-51b0-4d69-a173-be149de5370a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the embedding for our query point, we compare the distance between
    class prototypes and query point embeddings to find which class the query point
    belongs to. We can use Euclidean distance as a distance measure for finding the
    distance between the class prototypes and query points embeddings, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea975337-7c70-454f-bd48-70a4f612fac4.png)'
  prefs: []
  type: TYPE_IMG
- en: After finding the distance between the class prototype and query point embeddings,
    we apply softmax to this distance and get the probabilities. Since we have three
    classes, that is, lion, elephant, and dog, we will get three probabilities. The
    class that has high probability will be the class of our query point.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want our network to learn from just a few data points, that is, since
    we want to perform few-shot learning, we train our network in the same way. We
    use **episodic training**; for each episode, we randomly sample a few data points
    from each of the classes in our dataset, and we call that a support set, and we
    train the network using only the support set, instead of the whole dataset. Similarly,
    we randomly sample a point from the dataset as a query point and try to predict
    its class. In this way, our network learns how to learn from data points.
  prefs: []
  type: TYPE_NORMAL
- en: The overall flow of the prototypical network is shown in the following figure.
    As you can see, first, we will generate the embeddings for all the data points
    in our support set and build the class prototype by taking the mean embeddings
    of data points in a class. We also generate the embeddings for our query point.
    Then we compute the distance between the class prototype and the query point embeddings.
    We use Euclidean distance as the distance measure. Then we apply softmax to this
    distance and get the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following diagram, since our query point is a lion, the
    probability for lion is the highest, which is 0.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88bd003b-102f-4f92-974e-2ef36f599819.png)'
  prefs: []
  type: TYPE_IMG
- en: Prototypical networks are not only used for one-shot/few-shot learning, but
    are also used in zero-shot learning. Consider a case where we have no data points
    for each class but we have the meta-information containing a high-level description
    of each class.
  prefs: []
  type: TYPE_NORMAL
- en: In those cases, we learn the embeddings of meta- information of each class to
    form the class prototype and then perform classification with the class prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Relation networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Relation networks consist of two important functions: an embedding function,
    denoted by ![](img/e427d14c-2308-4c66-8ecc-9666f64c75cc.png) and the relation
    function, denoted by ![](img/c290ae58-090f-4a8e-b18a-7ac65be6776a.png). The embedding
    function is used for extracting the features from the input. If our input is an
    image, then we can use a convolutional network as our embedding function, which
    will give us the feature vectors/embeddings of an image. If our input is text,
    then we can use LSTM networks to get the embeddings of the text. Let us say, we
    have a support set containing three classes, {lion, elephant, dog} as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/319b659d-b9dd-4d2e-a83a-f28726d45f6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And let''s say we have a query image ![](img/7abb45ef-26fe-462f-b7eb-e095ea763f9c.png),
    as shown in the following diagram, and we want to predict the class of this query
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b34f9f63-5729-4203-a409-32051ab93fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we take each image, ![](img/e840f44c-6502-4cfb-aa78-1b27b42472b9.png),
    from the support set and pass it to the embedding function ![](img/5d2fbbe4-47c9-4941-b668-703c75ab2f90.png)
    for extract the features. Since our support set has images, we can use a convolutional
    network as our embedding function for learning the embeddings. The embedding function
    will give us the feature vector of each of the data points in the support set.
    Similarly, we will learn the embeddings of our query image ![](img/0180e52f-4fcc-4ce5-8b1d-fcefecc78f61.png)
    by passing it to the embedding function ![](img/22e9a6f8-6081-4e64-af6f-2c16a31bb5bc.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the feature vectors of the support set ![](img/327568ce-89fd-4121-99ee-6fe6b2d73d76.png)
    and query set ![](img/255c007b-bc4b-4adf-9005-39e9c8b836b5.png), we combine them
    using some operator ![](img/355f516e-7436-4050-b37a-c7d8e349df7c.png). Here, ![](img/794e0b65-28bc-49ad-a9b7-4dcd9f8992b4.png)
    can be any combination operator. We use concatenation as an operator to combining
    the feature vectors of the support and query set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6084602c-eed1-4dd6-9b6c-543b0f494679.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the following diagram, we will combine the feature vectors of the
    support set, ![](img/215d02a3-99bf-4967-a169-a3ae455f1bc5.png), and, query set,
    ![](img/d0a4b420-75dd-4d5a-97be-6344a359a5c9.png). But what is the use of combining
    like this? Well, it will help us to understand how the feature vector of an image
    in the support set is related to the feature vector of a query image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, it will help us to understand how the feature vector of a lion
    is related to the feature vector of a query image, how the feature vector of an
    elephant is related to the feature vector of a query image, and how the feature
    vector of dog is related to the feature vector of a query image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/763d3d50-8c77-42e0-8de9-2687b67962b2.png)'
  prefs: []
  type: TYPE_IMG
- en: But how can we measure this relatedness? Well, that is why we use a relation
    function ![](img/1e678152-02f9-4375-90a2-93ec5c438844.png). We pass these combined
    feature vectors to the relation function, which will generate the relation score
    ranging from 0 to 1, representing the similarity between samples in the support
    set ![](img/6eeaa3e6-88e7-4569-a8a8-2f2c319a0982.png) and samples in the query
    set ![](img/a29860ce-c399-457e-8e77-a84ef7da2b68.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation shows how we compute relation score ![](img/b8037d2f-42cd-4d2f-9f9e-ca58ab4b8702.png)
    in the relation network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb7811a-bc8e-4edf-a2ae-93a358281be6.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/f08b57bb-b307-4404-b888-1ae0cc33a679.png) denotes the relation
    score representing the similarity between each of the classes in the support set
    and the query image. Since we have three classes in the support set and one image
    in the query set, we will have three scores indicating how all the three classes
    in the support set are similar to the query image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall representation of the relation network in a one-shot learning setting
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/476c6b34-8305-41a1-ae56-aeaf9c4cbdca.png)'
  prefs: []
  type: TYPE_IMG
- en: Matching networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matching networks are yet another simple and efficient one-shot learning algorithm
    published by Google's DeepMind. It can even produce labels for the unobserved
    class in the dataset. Let's say we have a support set, ![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png)
    ,containing ![](img/7743d648-4b0e-4d96-8cde-e98e6790428c.png) examples as ![](img/a916403a-fec3-4114-9523-2d6a8a2dacb8.png).
    When given a query point (new unseen example), ![](img/877e3333-4958-4163-8504-2536ca1cb638.png),
    the matching network predicts the class of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    by comparing it with the support set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define this as ![](img/4a143914-5a02-4c57-b9fa-3ed8b9625ab7.png), where
    ![](img/78643764-3c92-4f94-b70f-dd465c6b31a5.png) is the parameterized neural
    network, ![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png) is the predicted class
    for query point ![](img/419cb580-0426-4c86-9553-1e0497079f52.png), and ![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png)
    is the support set. ![](img/dd9dc097-0ccc-4001-bca8-080b21fc7cb3.png) will return
    the probability of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) belonging
    to each class in the support set. Then we select the class of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    as the one that has the highest probability. But how does this work, exactly?
    How is this probability computed? Let''s see that now. The class, ![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png)
    ,of the query point, ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) ,can be
    predicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e560d60-e3fd-4fda-8606-333a046c21a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s decipher this equation. Here ![](img/42f20a66-29b6-47df-b1bd-1d8f3b3fbac5.png)
    and ![](img/93e299a7-1012-43e9-95b8-840fba0bcfe2.png) are the input and labels
    of the support set. ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) is the query
    input, that is, the input to which we want to predict the label. Also ![](img/ec63616e-db01-4cff-b4cb-285dbfa10120.png)
    is the attention mechanism between ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    and ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png). But how do we perform attention?
    Here, we use a simple attention mechanism, which is softmax over the cosine distance
    between ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12ae20db-cc46-4cb0-8022-99858213d603.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can''t calculate cosine distance between the raw inputs ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png) directly. So, first, we
    will learn their embeddings and calculate the cosine distance between the embeddings.
    We use two different embeddings, ![](img/a7796cc4-7eb6-4e44-96f7-78477d99f9a1.png)
    and ![](img/a3c5f2d6-ab38-41b4-9155-851a406fb2e9.png), for learning the embeddings
    of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png)
    respectively. We will learn how exactly these two embedding functions ![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png)
    and ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png) learn the embeddings in
    the upcoming section. So, we can rewrite our attention equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfc3af57-80b2-44e3-95a3-819355badafa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite the preceding equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58e2d8ec-2b3d-4d84-8ab4-79d9c1cfc48e.png)'
  prefs: []
  type: TYPE_IMG
- en: After calculating the attention matrix, ![](img/ec836138-3d93-430b-98bd-68553f00583a.png),
    we multiply our attention matrix with support set labels ![](img/5b35cf4f-0010-4965-9e7a-094e96381d55.png).
    But how can we multiply support set labels with our attention matrix? First, we
    convert our support set labels to the one hot encoded values and then multiply
    them with our attention matrix and, as a result, we get the probability of our
    query point ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) belonging to each
    of the classes in the support set. Then we apply *argmax* and select ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png)
    as the one that has a maximum probability value.
  prefs: []
  type: TYPE_NORMAL
- en: Still not clear about matching networks? Look at the following diagram; you
    can see we have three classes in our support set (lion, elephant, and dog) and
    we have a new query image ![](img/419cb580-0426-4c86-9553-1e0497079f52.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we feed the support set to embedding function ![](img/51ffa424-d10a-4d4e-8b51-a05216f68d7c.png)
    and the query image to the embedding function ![](img/e354a211-1d1d-4776-b33e-43a9fb66a11b.png)
    and learn their embeddings and calculate the cosine distance between them, and
    then we apply softmax attention over this cosine distance. Then we multiply our
    attention matrix with the one-hot encoded support set labels and get the probabilities.
    Next, we select ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png) as the one that
    has the highest probability. As you can see in the following diagram, the query
    set image is an elephant, and we have a high probability at the index 1, so we
    predict the class of ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png) as 1 (elephant):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bd1eedc-c537-4218-ae99-4b411f7ce247.png)'
  prefs: []
  type: TYPE_IMG
- en: We have learned that we use two embedding functions, ![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png)
    and ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png), for learning the embeddings
    of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png)
    respectively. Now we will see exactly how these two functions learn the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Support set embedding function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png)
    for learning the embeddings of the support set. We use bidirectional LSTM as our
    embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png). We can define
    our embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Query set embedding function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the embedding function ![](img/8d4d5a84-14c0-4b6d-bc9f-eb0d49e3e80d.png)
    for learning the embedding of our query point ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    . We use LSTM as our encoding function. Along with ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    as the input, we will also pass the embedding of our support set embeddings, which
    is *g(x)*, and we will pass one more parameter called *K*, which defines the number
    of processing steps. Let''s see how we compute query set embeddings step-by-step.
    First, we will initialize our LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for the number of processing steps, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate embeddings of the query set, ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png),
    by feeding it to the LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we perform softmax attention over the support set embeddings: that is,
    `g_embedings`. It helps us to avoid elements that are not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We update `previous_state` and repeat these steps for a number of processing
    steps, `K`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for computing `f_embeddings` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The architecture of matching networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The overall flow of matching network is shown in the following diagram and it
    is different from the image we saw already. You can see how the support set ![](img/8e559ad8-2621-4773-bc81-a1767a7b5a7c.png)
    and query set ![](img/b5c528a1-c1e1-4e80-ab81-9ab50e7076c8.png) are calculated
    through the embedding functions ![](img/cb68a610-6d09-4529-a1de-985a5abb612e.png)
    and ![](img/7017a935-4dcd-40d7-a4bc-d8162fcefe1f.png) respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the embedding function ![](img/fc7229de-25ee-464e-b14f-fc645b767ee3.png)
    takes the query set along with the support set embeddings as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2de06008-7882-4735-91d6-3beacb5eca89.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations again for learning all of the important and popular deep learning
    algorithms! Deep learning is an interesting and very popular field of AI that
    has revolutionized the world. Now that you've finished reading the book, you can
    start exploring various advancements in deep learning and start experimenting
    with various projects. Learn and deep learn!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding what k-shot learning is. We learned
    that in n-way k-shot learning, n-way implies the number of classes we have in
    our dataset and k-shot implies the number of data points we have in each class;
    and support set and the query set are equivalent to the train and test sets. Then
    we explored siamese networks. We learned how siamese networks use an identical
    network to learn the similarity of two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Followed by this, we learned about prototypical networks, which create a prototypical
    representation of each class and classify a query point (a new point) based on
    the distance between the class prototype and the query point. We also learned
    how relation networks use two different functions embedding and relation function
    to classify an image.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about matching networks and how it uses
    different embedding functions for the support set and the query set to classify
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is one of the most interesting branches in the field of AI. Now
    that you've understood various deep learning algorithms, you can start building
    deep learning models and create interesting applications and also contribute to
    deep learning research.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess the knowledge acquired from this chapter by answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is few-shot learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the support and query sets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define siamese networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define energy functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the loss function of siamese networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the prototypical network work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of functions used in relation networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about how to learn from a small number of data points, check out
    *Hands-On Meta Learning with Python* by Sudharsan Ravichandiran, published by
    Packt publishing available at, [https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python).
  prefs: []
  type: TYPE_NORMAL
