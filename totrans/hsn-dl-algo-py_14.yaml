- en: Exploring Few-Shot Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Few-Shot Learning算法
- en: Congratulations! We have made it to the final chapter. We have come a long way.
    We started off by learning what neural networks are and how they are used to recognize
    handwritten digits. Then we explored how to train neural networks with gradient
    descent algorithms. We also learned how recurrent neural networks i used for sequential
    tasks and how convolutional neural networks are used for image recognition. Following
    this, we investigated how the semantics of a text can be understood using word
    embedding algorithms. Then we got familiar with several different types of generative
    adversarial networks and autoencoders.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们终于来到了最后一章。我们已经走过了很长的路。我们首先学习了神经网络是什么，以及它们如何用于识别手写数字。然后我们探索了如何使用梯度下降算法训练神经网络。我们还学习了递归神经网络用于序列任务，以及卷积神经网络用于图像识别。接着，我们研究了如何使用单词嵌入算法理解文本的语义。然后我们熟悉了几种不同类型的生成对抗网络和自编码器。
- en: So far, we have learned that deep learning algorithms perform exceptionally
    well when we have a substantially large dataset. But how can we handle the situation
    when we don't have a large number of data points to learn from? For most use cases,
    we might not get a large dataset. In such cases, we can use few-shot learning
    algorithms, which do not require huge datasets to learn from. In this chapter,
    we will understand how exactly few-shot learning algorithms learn from a smaller
    number of data points and we explore different types of few-shot learning algorithms.
    First, we will study a popular few-shot learning algorithm called a **siamese
    network**. Following this, we will learn some other few-shot learning algorithms
    such as the prototypical, relation, and matching networks intuitively.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到，当我们有一个相当大的数据集时，深度学习算法表现得非常出色。但是当我们没有大量数据点可以学习时，我们该如何处理？对于大多数使用情况，我们可能得不到一个大型数据集。在这种情况下，我们可以使用few-shot
    learning算法，它不需要大量数据集进行学习。在本章中，我们将理解few-shot learning算法如何从较少数量的数据点中学习，并探索不同类型的few-shot
    learning算法。首先，我们将学习一个名为**siamese network**的流行few-shot learning算法。接下来，我们将直观地学习其他几种few-shot
    learning算法，如prototypical、relation和matching networks。
- en: 'In this chapter, we will study the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: What is few-shot learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是few-shot learning？
- en: Siamese networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siamese网络
- en: Architecture of siamese networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siamese网络的架构
- en: Prototypical networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prototypical networks
- en: Relation networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Relation网络
- en: Matching networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matching networks
- en: What is few-shot learning?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是few-shot learning？
- en: Learning from a few data points is called **few-shot** **learning** or **k-shot
    learning**, where k specifies the number of data points in each of the class in
    the dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从少量数据点中学习被称为**few-shot**学习或**k-shot learning**，其中k指定数据集中每个类别中的数据点数量。
- en: Consider we are performing an image classification task. Say we have two classes
    – apple and orange – and we try to classify the given image as an apple or orange.
    When we have exactly one apple and one orange image in our training set, it is
    called one-shot learning; that is, we are learning from just one data point per
    each of the class. If we have, say, 11 images of an apple and 11 images of an
    orange, then that is called 11-shot learning. So, k in k-shot learning implies
    the number of data points we have per class.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在执行图像分类任务。假设我们有两个类别 - 苹果和橙子 - 我们试图将给定的图像分类为苹果或橙子。当我们的训练集中恰好有一个苹果图像和一个橙子图像时，这被称为one-shot
    learning；也就是说，我们仅从每个类别中的一个数据点中学习。如果我们有，比如，11张苹果图像和11张橙子图像，那就称为11-shot learning。因此，k-shot
    learning中的k指的是每个类别中我们拥有的数据点数。
- en: There is also **zero-shot learning**, where we don't have any data points per
    class. Wait. What? How can we learn when there are no data points at all? In this
    case, we will not have data points, but we will have meta information about each
    of the class and we will learn from the meta information.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 还有**zero-shot learning**，其中我们没有任何类别的数据点。等等，什么？没有任何数据点怎么学习？在这种情况下，我们不会有数据点，但我们会有关于每个类别的元信息，我们将从这些元信息中学习。
- en: Since we have two classes in our dataset, that is, apple and orange, we can
    call it **two-way k-shot learning**. So, in n-way k-shot learning, n-way implies
    the number of classes we have in our dataset and k-shot implies a number of data
    points we have in each class.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集中有两个类别，即苹果和橙子，我们可以称其为**two-way k-shot learning**。所以，在n-way k-shot learning中，n-way表示数据集中类别的数量，k-shot表示每个类别中的数据点数。
- en: We need our models to learn from just a few data points. In order to attain
    this, we train them in the same way; that is, we train the model on a very few
    data points. Say we have a dataset, ![](img/8ea8bd84-c391-4118-99ea-2b799e44ae1c.png).
    We sample a few data points from each of the classes present in our dataset and
    we call it **support set**. Similarly, we sample some different data points from
    each of the classes and call it **query set**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要我们的模型仅从少数数据点中学习。为了达到这个目标，我们以同样的方式训练它们；也就是说，我们在非常少的数据点上训练模型。假设我们有一个数据集，![](img/8ea8bd84-c391-4118-99ea-2b799e44ae1c.png)。我们从数据集中每个类别中抽取少量数据点，并称之为**支持集**。同样地，我们从每个类别中抽取一些不同的数据点，并称之为**查询集**。
- en: We train the model with a support set and test it with a query set. We train
    the model in an episodic fashion—that is, in each episode, we sample a few data
    points from our dataset, ![](img/191373ea-439a-4898-b792-99920d642af2.png), prepare
    our support set and query set, and train on the support set and test on the query
    set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用支持集训练模型，并使用查询集进行测试。我们以每集方式训练模型，也就是说，在每一集中，我们从数据集中抽取几个数据点，![](img/191373ea-439a-4898-b792-99920d642af2.png)，准备支持集和查询集，并在支持集上训练，查询集上测试。
- en: Siamese networks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[孪生网络](https://wiki.example.org/siamese_networks)'
- en: Siamese networks are special types of neural networks and are among the simplest
    and most popularly used one-shot learning algorithms. As we have learned in the
    previous section, one-shot learning is a technique where we learn from only one
    training example per each class. So, siamese networks are predominantly used in
    applications where we don't have many data points for each of the class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络是一种特殊类型的神经网络，是最简单和最流行的一次学习算法之一。正如我们在前面的章节中所学到的，一次学习是一种技术，我们仅从每个类别中学习一个训练示例。因此，孪生网络主要用于那些每个类别数据点不多的应用场景。
- en: For instance, let's say we want to build a face recognition model for our organization
    and say about 500 people are working in our organization. If we want to build
    our face recognition model using a **convolutional neural network** (**CNN**)
    from scratch then we need many images of all these 500 people, to train the network
    and attain good accuracy. But, apparently, we will not have many images for all
    these 500 people and therefore it is not feasible to build a model using a CNN
    or any deep learning algorithm unless we have sufficient data points. So, in these
    kinds of scenarios, we can resort to a sophisticated one-shot learning algorithm
    such as a siamese network, which can learn from fewer data points.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想为我们的组织构建一个人脸识别模型，并且说我们的组织有大约500人。如果我们想要从头开始使用**卷积神经网络**（**CNN**）构建我们的人脸识别模型，那么我们需要这500人的许多图像来训练网络并获得良好的准确性。但是，显然，我们不会为这500人拥有足够多的图像，因此，除非我们有足够的数据点，否则使用CNN或任何深度学习算法构建模型是不可行的。因此，在这些情况下，我们可以借助一种复杂的一次学习算法，如孪生网络，它可以从较少的数据点中学习。
- en: But how do siamese networks work? Siamese networks basically consist of two
    symmetrical neural networks both sharing the same weights and architecture and
    both joined together at the end using an energy function, ![](img/4526cd01-c2d0-42c7-9bf7-d3b4d66ef67a.png).
    The objective of our siamese network is to learn whether the two inputs are similar
    or dissimilar.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但是孪生网络是如何工作的呢？孪生网络基本上由两个对称的神经网络组成，它们共享相同的权重和结构，并且在末端使用能量函数连接在一起，![](img/4526cd01-c2d0-42c7-9bf7-d3b4d66ef67a.png)。我们的孪生网络的目标是学习两个输入是相似还是不相似。
- en: 'Let''s say we have two images, ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)and
    ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png), and we want to learn whether
    the two images are similar or dissimilar. As shown in the following diagram, we
    feed **Image ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)** to **Network
    ![](img/834d0bf7-d38d-42a0-b532-aa3ce2d0fb73.png)** and **Image ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png)**
    to **Network ![](img/17ea0a48-207f-43ce-a8f0-4dad08a04873.png)**. The role of
    both of these networks is to generate embeddings (feature vectors) for the input
    image. So, we can use any network that will give us embeddings. Since our input
    is an image, we can use a convolutional network to generate the embeddings: that
    is, for extracting features. Remember that the role of the CNN here is only to
    extract features and not to classify.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两幅图像，![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png) 和 ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png)，我们想要学习这两幅图像是否相似或不相似。如下图所示，我们将**图像
    ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)** 输入**网络 ![](img/834d0bf7-d38d-42a0-b532-aa3ce2d0fb73.png)**，将**图像
    ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png)** 输入**网络 ![](img/17ea0a48-207f-43ce-a8f0-4dad08a04873.png)**。这两个网络的作用是为输入图像生成嵌入（特征向量）。因此，我们可以使用任何能够给我们嵌入的网络。由于我们的输入是图像，我们可以使用卷积网络来生成这些嵌入：也就是用于提取特征。请记住，在这里CNN的作用仅仅是提取特征而不是分类。
- en: 'As we know that these networks should have same weights and architecture, if
    **Network** ![](img/c6c1992c-2bb2-4c35-9192-a57f24bbb3b9.png) is a three-layer
    CNN then **Network** ![](img/f605751c-593e-4a12-888b-d0f0e26d3c83.png) should
    also be a three-layer CNN, and we have to use the same set of weights for both
    of these networks. So, **Network** ![](img/6deb49ca-2fd4-4b61-9f72-407d8308db55.png)
    and **Network** ![](img/f8153a01-1897-4a05-b7a5-38cb2f6927a0.png) will give us
    the embeddings for input images ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png)
    and ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png) respectively. Then, we will
    feed these embeddings to the energy function, which tells us how similar the two
    input images are. Energy functions are basically any similarity measure, such
    as Euclidean distance and cosine similarity:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这些网络应该具有相同的权重和架构，如果**网络** ![](img/c6c1992c-2bb2-4c35-9192-a57f24bbb3b9.png)
    是一个三层CNN，那么**网络** ![](img/f605751c-593e-4a12-888b-d0f0e26d3c83.png) 也应该是一个三层CNN，并且我们必须为这两个网络使用相同的权重集。因此，**网络**
    ![](img/6deb49ca-2fd4-4b61-9f72-407d8308db55.png) 和 **网络** ![](img/f8153a01-1897-4a05-b7a5-38cb2f6927a0.png)
    将分别为输入图像 ![](img/f96bf70f-61ea-4520-b883-aac9af61fcce.png) 和 ![](img/b499e6ce-5bea-4d8a-abad-d0dcf6338639.png)
    提供嵌入。然后，我们将这些嵌入馈送到能量函数中，该函数告诉我们两个输入图像的相似程度。能量函数基本上是任何相似性度量，如欧氏距离和余弦相似度：
- en: '![](img/a023ef9e-46ed-4111-b409-65a54e6af39b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a023ef9e-46ed-4111-b409-65a54e6af39b.png)'
- en: Siamese networks are not only used for face recognition, but are is also used
    extensively in applications where we don't have many data points and tasks where
    we need to learn the similarity between two inputs. The applications of siamese
    networks include signature verification, similar question retrieval, and object
    tracking. We will study siamese networks in detail in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络不仅用于人脸识别，还广泛应用于我们没有多个数据点和需要学习两个输入之间相似性的任务中。孪生网络的应用包括签名验证、相似问题检索和物体跟踪。我们将在下一节详细研究孪生网络。
- en: Architecture of siamese networks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 孪生网络的架构
- en: 'Now that we have a basic understanding of siamese networks, we will explore
    them in detail. The architecture of a siamese network is shown in the following
    figure:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对孪生网络有了基本的理解，接下来我们将详细探讨它们。孪生网络的架构如下图所示：
- en: '![](img/fba47a65-a1b0-4603-a152-fb501e006d7d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fba47a65-a1b0-4603-a152-fb501e006d7d.png)'
- en: As you can see in the preceding figure, a siamese network consists of two identical
    networks, both sharing the same weights and architecture. Let's say we have two
    inputs, ![](img/9077854b-355a-4606-ae06-2e521038b578.png) and ![](img/36a7ea70-62f4-495a-a246-b07e25a85d07.png).
    We feed **Input** ![](img/e2aa271e-c7fc-474a-94e7-d7c5cfb8b422.png) to **Network**
    ![](img/259f6957-4b31-438d-87a3-6d8947fc147d.png), that is, ![](img/313a447d-8f4f-459a-a019-24da4ab85aa9.png),
    and we feed **Input** ![](img/4fb3738e-00c5-48ba-bab4-0c50b08a1091.png) to **Network**
    ![](img/45a57887-05f2-445f-a95b-36f97e73443d.png), that is, ![](img/f5975395-4aee-4902-8149-dd528e1c6954.png).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以在前面的图中看到的那样，siamese 网络由两个相同的网络组成，两者共享相同的权重和架构。假设我们有两个输入，![](img/9077854b-355a-4606-ae06-2e521038b578.png)
    和 ![](img/36a7ea70-62f4-495a-a246-b07e25a85d07.png)。我们将 **Input** ![](img/e2aa271e-c7fc-474a-94e7-d7c5cfb8b422.png)
    输入到 **Network** ![](img/259f6957-4b31-438d-87a3-6d8947fc147d.png)，即 ![](img/313a447d-8f4f-459a-a019-24da4ab85aa9.png)，我们将
    **Input** ![](img/4fb3738e-00c5-48ba-bab4-0c50b08a1091.png) 输入到 **Network** ![](img/45a57887-05f2-445f-a95b-36f97e73443d.png)，即
    ![](img/f5975395-4aee-4902-8149-dd528e1c6954.png)。
- en: 'As you can see, both of these networks have the same weights, ![](img/84870513-e640-4667-b014-56bd8b5f4cd8.png),
    and they will generate embeddings for our input, ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png)
    and ![](img/64a84803-b209-4c29-968a-40722def7dab.png). Then, we feed these embeddings
    to the energy function, ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png), which
    will give us similarity between the two inputs. It can be expressed as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以看到的，这两个网络具有相同的权重，![](img/84870513-e640-4667-b014-56bd8b5f4cd8.png)，它们将为我们的输入
    ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png) 和 ![](img/64a84803-b209-4c29-968a-40722def7dab.png)
    生成嵌入。然后，我们将这些嵌入传入能量函数，![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png)，它将给出两个输入之间的相似度。可以表达如下：
- en: '![](img/60c5a706-06ce-4d7e-b0fc-925a5990908d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60c5a706-06ce-4d7e-b0fc-925a5990908d.png)'
- en: Let's say we use Euclidean distance as our energy function; then the value of
    ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png) will be low if ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png)
    and ![](img/64a84803-b209-4c29-968a-40722def7dab.png) are similar. The value of
    ![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png) will be large if the input values
    are dissimilar.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用欧氏距离作为能量函数；那么当 ![](img/019f1ca5-fc1c-4a4f-b0ca-88accc6eabcc.png) 和 ![](img/64a84803-b209-4c29-968a-40722def7dab.png)
    相似时，![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png) 的值将很低。当输入值不相似时，![](img/b4dda082-9107-4f03-80b7-2477fe7d2500.png)
    的值将很大。
- en: Assume that you have two sentences, sentence 1 and sentence 2\. We feed sentence
    1 to Network ![](img/d473d10d-c2d9-4418-aade-1cf3e4a9f88e.png) and sentence 2
    to Network ![](img/9861dfc9-0ae1-4404-b665-77f3142dbb69.png). Let's say both our
    Network ![](img/a1bc9ed3-da8e-433e-863d-cc78eafdb98b.png) and Network ![](img/6e4329a8-431d-4a05-b1e6-5b54b056f4e3.png)
    are **long short-term memory** (**LSTM**) networks and they share the same weights.
    So, Network ![](img/9f2f26fc-4e1a-4708-ba2a-045709cd84da.png) and Network ![](img/af618eac-a43c-43f6-9d9a-e282ccd75ee8.png)
    will generate the embeddings for sentence 1 and sentence 2 respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有两个句子，句子1和句子2。我们将句子1输入到网络 ![](img/d473d10d-c2d9-4418-aade-1cf3e4a9f88e.png)，句子2输入到网络
    ![](img/9861dfc9-0ae1-4404-b665-77f3142dbb69.png)。假设我们的网络 ![](img/a1bc9ed3-da8e-433e-863d-cc78eafdb98b.png)
    和网络 ![](img/6e4329a8-431d-4a05-b1e6-5b54b056f4e3.png) 都是**长短期记忆**（**LSTM**）网络，并且它们共享相同的权重。因此，网络
    ![](img/9f2f26fc-4e1a-4708-ba2a-045709cd84da.png) 和网络 ![](img/af618eac-a43c-43f6-9d9a-e282ccd75ee8.png)
    将分别为句子1和句子2生成嵌入。
- en: Then, we feed these embeddings to the energy function, which gives us the similarity
    score between the two sentences. But how can we train our siamese networks? How
    should the data be? What are the features and labels? What is our objective function?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些嵌入传入能量函数，该函数给出两个句子之间的相似度分数。但我们如何训练我们的 siamese 网络呢？数据应该如何？特征和标签是什么？我们的目标函数是什么？
- en: 'The input to the siamese networks should be in pairs, ![](img/95ed580d-79e1-4be1-89ea-d5f1e297aaff.png),
    along with their binary label, ![](img/55b589b4-d02c-435f-9325-5599a12cb16d.png),
    stating whether the input pairs are a genuine pair the(same) or an imposite pair
    (different). As you can see in the following table, we have sentences as pairs
    and the label implies whether the sentence pairs are genuine (1) or imposite (0):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 网络的输入应该是成对的，![](img/95ed580d-79e1-4be1-89ea-d5f1e297aaff.png)，以及它们的二进制标签，![](img/55b589b4-d02c-435f-9325-5599a12cb16d.png)，指示输入对是真实对（相同）还是伪造对（不同）。正如您可以在下表中看到的那样，我们有作为对的句子和标签，表明句子对是真实（1）还是伪造（0）的：
- en: '![](img/99490a38-0e57-4305-a8e0-d9f2a1439581.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99490a38-0e57-4305-a8e0-d9f2a1439581.png)'
- en: So, what is the loss function of our siamese network?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的 siamese 网络的损失函数是什么？
- en: 'Since the goal of the siamese network is not to perform a classification task
    but to understand the similarity between the two input values, we use the contrastive
    loss function. It can be expressed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于孪生网络的目标不是执行分类任务而是理解两个输入值之间的相似性，我们使用对比损失函数。可以表达如下：
- en: '![](img/4feefc7f-35a0-48a2-9be7-ca2afa4d4568.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4feefc7f-35a0-48a2-9be7-ca2afa4d4568.png)'
- en: In the preceding equation, the value of ![](img/b9498bd7-3711-489f-b50f-79c68bb299d5.png)
    is the true label, which will be 1 if the two input values are similar and 0 if
    the two input values are dissimilar, and ![](img/3c536afe-2aa2-4305-ba0f-8c9bc7667612.png)
    is our energy function, which can be any distance measure. The term **margin**
    is used to hold the constraint, that is, when two input values are dissimilar,
    and if their distance is greater than a margin, then they do not incur a loss.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![](img/b9498bd7-3711-489f-b50f-79c68bb299d5.png) 的值是真实标签，如果两个输入值相似则为1，如果两个输入值不相似则为0，![](img/3c536afe-2aa2-4305-ba0f-8c9bc7667612.png)
    是我们的能量函数，可以是任何距离度量。术语**边界**用于保持约束，即当两个输入值不相似且它们的距离大于一个边界时，它们不会产生损失。
- en: Prototypical networks
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原型网络
- en: Prototypical networks are yet another simple, efficient, and popular learning
    algorithm. Like siamese networks, they try to learn the metric space to perform
    classification.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络是另一种简单、高效且流行的学习算法。与孪生网络类似，它们试图学习度量空间以执行分类。
- en: The basic idea of the prototypical network is to create a prototypical representation
    of each class and classify a query point (new point) based on the distance between
    the class prototype and the query point.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络的基本思想是为每个类别创建一个原型表示，并基于类原型与查询点之间的距离对查询点（新点）进行分类。
- en: 'Let''s say we have a support set comprising images of lions, elephants, and
    dogs, as shown in the following diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个支持集，其中包含狮子、大象和狗的图像，如下图所示：
- en: '![](img/a44e2b6a-2aaf-491f-82d4-d5c39b8cae7e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a44e2b6a-2aaf-491f-82d4-d5c39b8cae7e.png)'
- en: 'We have three classes (lion, elephant, and dog). Now we need to create a prototypical
    representation for each of these three classes. How can we build the prototype
    of these three classes? First, we will learn the embeddings of each data point
    using some embedding function. The embedding function, ![](img/5345c9dc-00e0-49f3-9e70-189f55e5f7b0.png)
    ,can be any function that can be used to extract features. Since our input is
    an image, we can use the convolutional network as our embedding function, which
    will extract features from the input images, shown as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三个类别（狮子、大象和狗）。现在我们需要为这三个类别中的每一个创建一个原型表示。如何建立这三个类的原型？首先，我们将使用某个嵌入函数学习每个数据点的嵌入。嵌入函数，![](img/5345c9dc-00e0-49f3-9e70-189f55e5f7b0.png)，可以是任何用于提取特征的函数。由于我们的输入是图像，我们可以使用卷积网络作为我们的嵌入函数，它将从输入图像中提取特征，如下所示：
- en: '![](img/cfab0e2b-76b0-4aa9-ac1d-96f005d59402.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfab0e2b-76b0-4aa9-ac1d-96f005d59402.png)'
- en: 'Once we learn the embeddings of each data point, we take the mean embeddings
    of data points in each class and form the class prototype, shown as follows. So,
    a class prototype is basically the mean embeddings of data points in a class:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们学习了每个数据点的嵌入，我们就取每个类中数据点的平均嵌入并形成类原型，如下所示。因此，类原型基本上是类中数据点的平均嵌入：
- en: '![](img/e8abebe0-665b-4307-b3cf-c2c9d17dd17d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8abebe0-665b-4307-b3cf-c2c9d17dd17d.png)'
- en: 'Similarly, when a new data point comes in, that is, a query point for which
    we want to predict the label, we will generate the embeddings for this new data
    point using the same embedding function that we used to create the class prototype:
    that is, we generate the embeddings for our query point using the convolutional
    network:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个新的数据点出现时，即我们希望预测标签的查询点时，我们将使用与创建类原型相同的嵌入函数生成这个新数据点的嵌入：也就是说，我们使用卷积网络生成我们的查询点的嵌入：
- en: '![](img/8db1780e-51b0-4d69-a173-be149de5370a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8db1780e-51b0-4d69-a173-be149de5370a.png)'
- en: 'Once we have the embedding for our query point, we compare the distance between
    class prototypes and query point embeddings to find which class the query point
    belongs to. We can use Euclidean distance as a distance measure for finding the
    distance between the class prototypes and query points embeddings, as shown below:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了查询点的嵌入，我们比较类原型和查询点嵌入之间的距离来确定查询点属于哪个类。我们可以使用欧氏距离作为测量类原型与查询点嵌入之间距离的距离度量，如下所示：
- en: '![](img/ea975337-7c70-454f-bd48-70a4f612fac4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea975337-7c70-454f-bd48-70a4f612fac4.png)'
- en: After finding the distance between the class prototype and query point embeddings,
    we apply softmax to this distance and get the probabilities. Since we have three
    classes, that is, lion, elephant, and dog, we will get three probabilities. The
    class that has high probability will be the class of our query point.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算类原型与查询点嵌入之间的距离后，我们对这个距离应用softmax，并得到概率。由于我们有三类，即狮子、大象和狗，我们将得到三个概率。具有高概率的类将是我们查询点的类别。
- en: Since we want our network to learn from just a few data points, that is, since
    we want to perform few-shot learning, we train our network in the same way. We
    use **episodic training**; for each episode, we randomly sample a few data points
    from each of the classes in our dataset, and we call that a support set, and we
    train the network using only the support set, instead of the whole dataset. Similarly,
    we randomly sample a point from the dataset as a query point and try to predict
    its class. In this way, our network learns how to learn from data points.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望我们的网络只从少量数据点学习，也就是说，我们希望进行少样本学习，我们以相同的方式训练我们的网络。我们使用**情节式训练**；对于每个情节，我们从数据集中的每个类中随机抽样一些数据点，并称之为支持集，并且我们仅使用支持集训练网络，而不是整个数据集。类似地，我们随机从数据集中抽样一个点作为查询点，并尝试预测其类别。通过这种方式，我们的网络学习如何从数据点中学习。
- en: The overall flow of the prototypical network is shown in the following figure.
    As you can see, first, we will generate the embeddings for all the data points
    in our support set and build the class prototype by taking the mean embeddings
    of data points in a class. We also generate the embeddings for our query point.
    Then we compute the distance between the class prototype and the query point embeddings.
    We use Euclidean distance as the distance measure. Then we apply softmax to this
    distance and get the probabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络的整体流程如下图所示。正如您所看到的，首先，我们将为支持集中的所有数据点生成嵌入，并通过取类中数据点的平均嵌入来构建类原型。我们还生成查询点的嵌入。然后我们计算类原型与查询点嵌入之间的距离。我们使用欧氏距离作为距离度量。然后我们对这个距离应用softmax，并得到概率。
- en: 'As you can see in the following diagram, since our query point is a lion, the
    probability for lion is the highest, which is 0.9:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，由于我们的查询点是狮子，狮子的概率最高，为0.9：
- en: '![](img/88bd003b-102f-4f92-974e-2ef36f599819.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88bd003b-102f-4f92-974e-2ef36f599819.png)'
- en: Prototypical networks are not only used for one-shot/few-shot learning, but
    are also used in zero-shot learning. Consider a case where we have no data points
    for each class but we have the meta-information containing a high-level description
    of each class.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络不仅用于一次性/少样本学习，还用于零样本学习。考虑一个情况，我们没有每个类的数据点，但我们有包含每个类高级描述的元信息。
- en: In those cases, we learn the embeddings of meta- information of each class to
    form the class prototype and then perform classification with the class prototype.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们学习每个类的元信息的嵌入以形成类原型，然后使用类原型进行分类。
- en: Relation networks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关系网络
- en: 'Relation networks consist of two important functions: an embedding function,
    denoted by ![](img/e427d14c-2308-4c66-8ecc-9666f64c75cc.png) and the relation
    function, denoted by ![](img/c290ae58-090f-4a8e-b18a-7ac65be6776a.png). The embedding
    function is used for extracting the features from the input. If our input is an
    image, then we can use a convolutional network as our embedding function, which
    will give us the feature vectors/embeddings of an image. If our input is text,
    then we can use LSTM networks to get the embeddings of the text. Let us say, we
    have a support set containing three classes, {lion, elephant, dog} as shown below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关系网络包括两个重要的函数：嵌入函数，用![](img/e427d14c-2308-4c66-8ecc-9666f64c75cc.png)表示，以及关系函数，用![](img/c290ae58-090f-4a8e-b18a-7ac65be6776a.png)表示。嵌入函数用于从输入中提取特征。如果我们的输入是图像，那么我们可以使用卷积网络作为我们的嵌入函数，它将给出图像的特征向量/嵌入。如果我们的输入是文本，那么我们可以使用LSTM网络来获取文本的嵌入。假设我们有一个包含三类的支持集，{狮子，大象，狗}如下所示：
- en: '![](img/319b659d-b9dd-4d2e-a83a-f28726d45f6b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/319b659d-b9dd-4d2e-a83a-f28726d45f6b.png)'
- en: 'And let''s say we have a query image ![](img/7abb45ef-26fe-462f-b7eb-e095ea763f9c.png),
    as shown in the following diagram, and we want to predict the class of this query
    image:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个查询图像 ![](img/7abb45ef-26fe-462f-b7eb-e095ea763f9c.png)，如下图所示，我们想要预测这个查询图像的类别：
- en: '![](img/b34f9f63-5729-4203-a409-32051ab93fa3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b34f9f63-5729-4203-a409-32051ab93fa3.png)'
- en: First, we take each image, ![](img/e840f44c-6502-4cfb-aa78-1b27b42472b9.png),
    from the support set and pass it to the embedding function ![](img/5d2fbbe4-47c9-4941-b668-703c75ab2f90.png)
    for extract the features. Since our support set has images, we can use a convolutional
    network as our embedding function for learning the embeddings. The embedding function
    will give us the feature vector of each of the data points in the support set.
    Similarly, we will learn the embeddings of our query image ![](img/0180e52f-4fcc-4ce5-8b1d-fcefecc78f61.png)
    by passing it to the embedding function ![](img/22e9a6f8-6081-4e64-af6f-2c16a31bb5bc.png).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从支持集中每个图像 ![](img/e840f44c-6502-4cfb-aa78-1b27b42472b9.png) 中取出，并通过嵌入函数
    ![](img/5d2fbbe4-47c9-4941-b668-703c75ab2f90.png) 提取特征。由于我们的支持集包含图像，我们可以将卷积网络作为我们的嵌入函数，用于学习嵌入。嵌入函数将为支持集中每个数据点提供特征向量。类似地，我们通过将查询图像
    ![](img/0180e52f-4fcc-4ce5-8b1d-fcefecc78f61.png) 传递给嵌入函数 ![](img/22e9a6f8-6081-4e64-af6f-2c16a31bb5bc.png)
    来学习查询图像的嵌入。
- en: 'Once we have the feature vectors of the support set ![](img/327568ce-89fd-4121-99ee-6fe6b2d73d76.png)
    and query set ![](img/255c007b-bc4b-4adf-9005-39e9c8b836b5.png), we combine them
    using some operator ![](img/355f516e-7436-4050-b37a-c7d8e349df7c.png). Here, ![](img/794e0b65-28bc-49ad-a9b7-4dcd9f8992b4.png)
    can be any combination operator. We use concatenation as an operator to combining
    the feature vectors of the support and query set:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了支持集的特征向量 ![](img/327568ce-89fd-4121-99ee-6fe6b2d73d76.png) 和查询集的特征向量 ![](img/255c007b-bc4b-4adf-9005-39e9c8b836b5.png)，我们使用某些运算符
    ![](img/355f516e-7436-4050-b37a-c7d8e349df7c.png) 将它们组合起来。这里，![](img/794e0b65-28bc-49ad-a9b7-4dcd9f8992b4.png)
    可以是任何组合运算符。我们使用串联作为运算符来组合支持集和查询集的特征向量：
- en: '![](img/6084602c-eed1-4dd6-9b6c-543b0f494679.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6084602c-eed1-4dd6-9b6c-543b0f494679.png)'
- en: As shown in the following diagram, we will combine the feature vectors of the
    support set, ![](img/215d02a3-99bf-4967-a169-a3ae455f1bc5.png), and, query set,
    ![](img/d0a4b420-75dd-4d5a-97be-6344a359a5c9.png). But what is the use of combining
    like this? Well, it will help us to understand how the feature vector of an image
    in the support set is related to the feature vector of a query image.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们将组合支持集的特征向量，![](img/215d02a3-99bf-4967-a169-a3ae455f1bc5.png)，以及查询集的特征向量，![](img/d0a4b420-75dd-4d5a-97be-6344a359a5c9.png)。但是，这样组合有什么用呢？嗯，这将帮助我们理解支持集中图像的特征向量与查询图像的特征向量之间的关系。
- en: 'In our example, it will help us to understand how the feature vector of a lion
    is related to the feature vector of a query image, how the feature vector of an
    elephant is related to the feature vector of a query image, and how the feature
    vector of dog is related to the feature vector of a query image:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，这将帮助我们理解狮子的特征向量如何与查询图像的特征向量相关联，大象的特征向量如何与查询图像的特征向量相关联，以及狗的特征向量如何与查询图像的特征向量相关联：
- en: '![](img/763d3d50-8c77-42e0-8de9-2687b67962b2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/763d3d50-8c77-42e0-8de9-2687b67962b2.png)'
- en: But how can we measure this relatedness? Well, that is why we use a relation
    function ![](img/1e678152-02f9-4375-90a2-93ec5c438844.png). We pass these combined
    feature vectors to the relation function, which will generate the relation score
    ranging from 0 to 1, representing the similarity between samples in the support
    set ![](img/6eeaa3e6-88e7-4569-a8a8-2f2c319a0982.png) and samples in the query
    set ![](img/a29860ce-c399-457e-8e77-a84ef7da2b68.png).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何衡量这种相关性呢？嗯，这就是为什么我们使用关系函数 ![](img/1e678152-02f9-4375-90a2-93ec5c438844.png)。我们将这些组合特征向量传递给关系函数，它将生成关系分数，范围从0到1，表示支持集
    ![](img/6eeaa3e6-88e7-4569-a8a8-2f2c319a0982.png) 中样本与查询集 ![](img/a29860ce-c399-457e-8e77-a84ef7da2b68.png)
    中样本之间的相似度。
- en: 'The following equation shows how we compute relation score ![](img/b8037d2f-42cd-4d2f-9f9e-ca58ab4b8702.png)
    in the relation network:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的方程显示了我们如何在关系网络中计算关系分数，![](img/b8037d2f-42cd-4d2f-9f9e-ca58ab4b8702.png)：
- en: '![](img/3eb7811a-bc8e-4edf-a2ae-93a358281be6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eb7811a-bc8e-4edf-a2ae-93a358281be6.png)'
- en: Here, ![](img/f08b57bb-b307-4404-b888-1ae0cc33a679.png) denotes the relation
    score representing the similarity between each of the classes in the support set
    and the query image. Since we have three classes in the support set and one image
    in the query set, we will have three scores indicating how all the three classes
    in the support set are similar to the query image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/f08b57bb-b307-4404-b888-1ae0cc33a679.png) 表示关系分数，表示支持集中每个类别与查询图像的相似度。由于支持集中有三个类别和一个查询集中的图像，我们将得到三个分数，表示支持集中所有三个类别与查询图像的相似度。
- en: 'The overall representation of the relation network in a one-shot learning setting
    is shown in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次性学习设置中，关系网络的整体表示如下图所示：
- en: '![](img/476c6b34-8305-41a1-ae56-aeaf9c4cbdca.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/476c6b34-8305-41a1-ae56-aeaf9c4cbdca.png)'
- en: Matching networks
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配网络
- en: Matching networks are yet another simple and efficient one-shot learning algorithm
    published by Google's DeepMind. It can even produce labels for the unobserved
    class in the dataset. Let's say we have a support set, ![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png)
    ,containing ![](img/7743d648-4b0e-4d96-8cde-e98e6790428c.png) examples as ![](img/a916403a-fec3-4114-9523-2d6a8a2dacb8.png).
    When given a query point (new unseen example), ![](img/877e3333-4958-4163-8504-2536ca1cb638.png),
    the matching network predicts the class of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    by comparing it with the support set.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配网络是谷歌DeepMind发布的又一种简单高效的一次性学习算法。它甚至可以为数据集中未观察到的类别生成标签。假设我们有一个支持集 ![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png)，包含
    ![](img/7743d648-4b0e-4d96-8cde-e98e6790428c.png) 个例子作为 ![](img/a916403a-fec3-4114-9523-2d6a8a2dacb8.png)。当给出一个查询点（新的未见示例）
    ![](img/877e3333-4958-4163-8504-2536ca1cb638.png) 时，匹配网络通过与支持集比较来预测 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    的类别。
- en: 'We can define this as ![](img/4a143914-5a02-4c57-b9fa-3ed8b9625ab7.png), where
    ![](img/78643764-3c92-4f94-b70f-dd465c6b31a5.png) is the parameterized neural
    network, ![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png) is the predicted class
    for query point ![](img/419cb580-0426-4c86-9553-1e0497079f52.png), and ![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png)
    is the support set. ![](img/dd9dc097-0ccc-4001-bca8-080b21fc7cb3.png) will return
    the probability of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) belonging
    to each class in the support set. Then we select the class of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    as the one that has the highest probability. But how does this work, exactly?
    How is this probability computed? Let''s see that now. The class, ![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png)
    ,of the query point, ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) ,can be
    predicted as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其定义为 ![](img/4a143914-5a02-4c57-b9fa-3ed8b9625ab7.png)，其中 ![](img/78643764-3c92-4f94-b70f-dd465c6b31a5.png)
    是参数化的神经网络，![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png) 是查询点 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    的预测类别，![](img/3972487a-e95f-438b-8df3-2a9ea7483d47.png) 是支持集。![](img/dd9dc097-0ccc-4001-bca8-080b21fc7cb3.png)
    将返回 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 属于支持集中每个类的概率。然后我们选择具有最高概率的类作为
    ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 的类别。但这究竟是如何工作的？这个概率是如何计算的？让我们现在看看。查询点
    ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 的类别 ![](img/800e6e5b-3b27-4563-8496-5ba473035b4c.png)
    可以预测如下：
- en: '![](img/0e560d60-e3fd-4fda-8606-333a046c21a8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e560d60-e3fd-4fda-8606-333a046c21a8.png)'
- en: 'Let''s decipher this equation. Here ![](img/42f20a66-29b6-47df-b1bd-1d8f3b3fbac5.png)
    and ![](img/93e299a7-1012-43e9-95b8-840fba0bcfe2.png) are the input and labels
    of the support set. ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) is the query
    input, that is, the input to which we want to predict the label. Also ![](img/ec63616e-db01-4cff-b4cb-285dbfa10120.png)
    is the attention mechanism between ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    and ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png). But how do we perform attention?
    Here, we use a simple attention mechanism, which is softmax over the cosine distance
    between ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解析这个方程。这里 ![](img/42f20a66-29b6-47df-b1bd-1d8f3b3fbac5.png) 和 ![](img/93e299a7-1012-43e9-95b8-840fba0bcfe2.png)
    是支持集的输入和标签。![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 是查询输入，即我们希望预测标签的输入。同时
    ![](img/ec63616e-db01-4cff-b4cb-285dbfa10120.png) 是 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    和 ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png) 之间的注意力机制。但是我们如何执行注意力？这里，我们使用了一个简单的注意力机制，即
    ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 和 ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png)
    之间的余弦距离上的 softmax：
- en: '![](img/12ae20db-cc46-4cb0-8022-99858213d603.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12ae20db-cc46-4cb0-8022-99858213d603.png)'
- en: 'We can''t calculate cosine distance between the raw inputs ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png) directly. So, first, we
    will learn their embeddings and calculate the cosine distance between the embeddings.
    We use two different embeddings, ![](img/a7796cc4-7eb6-4e44-96f7-78477d99f9a1.png)
    and ![](img/a3c5f2d6-ab38-41b4-9155-851a406fb2e9.png), for learning the embeddings
    of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png)
    respectively. We will learn how exactly these two embedding functions ![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png)
    and ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png) learn the embeddings in
    the upcoming section. So, we can rewrite our attention equation as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接计算原始输入 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 和 ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png)
    之间的余弦距离。因此，首先，我们将学习它们的嵌入并计算嵌入之间的余弦距离。我们使用两种不同的嵌入，![](img/a7796cc4-7eb6-4e44-96f7-78477d99f9a1.png)
    和 ![](img/a3c5f2d6-ab38-41b4-9155-851a406fb2e9.png)，分别用于学习 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)
    和 ![](img/2c66d51e-7fc3-46b7-9615-0a7bfb96d247.png) 的嵌入。我们将在接下来的部分详细学习这两个嵌入函数
    ![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png) 和 ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png)
    如何学习这些嵌入。因此，我们可以重写我们的注意力方程如下：
- en: '![](img/dfc3af57-80b2-44e3-95a3-819355badafa.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfc3af57-80b2-44e3-95a3-819355badafa.png)'
- en: 'We can rewrite the preceding equation as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上述方程重写如下：
- en: '![](img/58e2d8ec-2b3d-4d84-8ab4-79d9c1cfc48e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58e2d8ec-2b3d-4d84-8ab4-79d9c1cfc48e.png)'
- en: After calculating the attention matrix, ![](img/ec836138-3d93-430b-98bd-68553f00583a.png),
    we multiply our attention matrix with support set labels ![](img/5b35cf4f-0010-4965-9e7a-094e96381d55.png).
    But how can we multiply support set labels with our attention matrix? First, we
    convert our support set labels to the one hot encoded values and then multiply
    them with our attention matrix and, as a result, we get the probability of our
    query point ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) belonging to each
    of the classes in the support set. Then we apply *argmax* and select ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png)
    as the one that has a maximum probability value.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力矩阵 ![](img/ec836138-3d93-430b-98bd-68553f00583a.png) 后，我们将注意力矩阵乘以支持集标签
    ![](img/5b35cf4f-0010-4965-9e7a-094e96381d55.png)。但是我们如何将支持集标签与我们的注意力矩阵相乘呢？首先，我们将支持集标签转换为独热编码值，然后将它们与我们的注意力矩阵相乘，结果得到我们的查询点
    ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 属于支持集各类的概率。然后我们应用 *argmax* 并选择具有最大概率值的
    ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png)。
- en: Still not clear about matching networks? Look at the following diagram; you
    can see we have three classes in our support set (lion, elephant, and dog) and
    we have a new query image ![](img/419cb580-0426-4c86-9553-1e0497079f52.png).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对匹配网络还不清楚？看看以下图表；您可以看到我们的支持集中有三类（狮子、大象和狗），我们有一个新的查询图像 ![](img/419cb580-0426-4c86-9553-1e0497079f52.png)。
- en: 'First, we feed the support set to embedding function ![](img/51ffa424-d10a-4d4e-8b51-a05216f68d7c.png)
    and the query image to the embedding function ![](img/e354a211-1d1d-4776-b33e-43a9fb66a11b.png)
    and learn their embeddings and calculate the cosine distance between them, and
    then we apply softmax attention over this cosine distance. Then we multiply our
    attention matrix with the one-hot encoded support set labels and get the probabilities.
    Next, we select ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png) as the one that
    has the highest probability. As you can see in the following diagram, the query
    set image is an elephant, and we have a high probability at the index 1, so we
    predict the class of ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png) as 1 (elephant):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将支持集输入到嵌入函数 ![](img/51ffa424-d10a-4d4e-8b51-a05216f68d7c.png)，将查询图像输入到嵌入函数
    ![](img/e354a211-1d1d-4776-b33e-43a9fb66a11b.png)，学习它们的嵌入并计算它们之间的余弦距离，然后我们在这个余弦距离上应用
    softmax 注意力。然后，我们将我们的注意力矩阵乘以支持集标签的独热编码，并得到概率。接下来，我们选择概率最高的 ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png)。正如您在以下图表中看到的那样，查询集图像是一只大象，我们在索引
    1 处具有很高的概率，因此我们预测 ![](img/1b274c84-887d-4f18-9afb-8a0b2d476517.png) 的类别为 1（大象）：
- en: '![](img/3bd1eedc-c537-4218-ae99-4b411f7ce247.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bd1eedc-c537-4218-ae99-4b411f7ce247.png)'
- en: We have learned that we use two embedding functions, ![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png)
    and ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png), for learning the embeddings
    of ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) and ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png)
    respectively. Now we will see exactly how these two functions learn the embeddings.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到我们使用两个嵌入函数，![](img/af72d86a-5c95-423b-a453-2b0235951e0a.png) 和 ![](img/2d3e10e5-6f17-4b63-8b46-2e8efa44a783.png)，分别学习
    ![](img/419cb580-0426-4c86-9553-1e0497079f52.png) 和 ![](img/3edef960-050f-46ff-b3e4-5d0706b153ff.png)
    的嵌入。现在我们将看看这两个函数如何学习嵌入。
- en: Support set embedding function
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持集嵌入函数
- en: 'We use the embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png)
    for learning the embeddings of the support set. We use bidirectional LSTM as our
    embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png). We can define
    our embedding function ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png) as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用嵌入函数 ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png) 来学习支持集的嵌入。我们将双向LSTM作为我们的嵌入函数
    ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png)。我们可以定义我们的嵌入函数 ![](img/cc3dbde5-1474-48ec-82c4-a18fda5ae058.png)
    如下：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Query set embedding function
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询集嵌入函数
- en: 'We use the embedding function ![](img/8d4d5a84-14c0-4b6d-bc9f-eb0d49e3e80d.png)
    for learning the embedding of our query point ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    . We use LSTM as our encoding function. Along with ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    as the input, we will also pass the embedding of our support set embeddings, which
    is *g(x)*, and we will pass one more parameter called *K*, which defines the number
    of processing steps. Let''s see how we compute query set embeddings step-by-step.
    First, we will initialize our LSTM cell:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用嵌入函数 ![](img/8d4d5a84-14c0-4b6d-bc9f-eb0d49e3e80d.png) 来学习我们查询点 ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    的嵌入。我们使用LSTM作为我们的编码函数。连同输入 ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)，我们还会传递支持集嵌入的嵌入
    *g(x)*，并且还会传递一个称为 *K* 的参数，该参数定义了处理步骤的数量。让我们逐步看看如何计算查询集嵌入。首先，我们将初始化我们的LSTM单元：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, for the number of processing steps, we do the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在处理步骤的数量上，我们执行以下操作：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We calculate embeddings of the query set, ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png),
    by feeding it to the LSTM cell:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将其馈送到LSTM单元来计算查询集 ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png) 的嵌入：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we perform softmax attention over the support set embeddings: that is,
    `g_embedings`. It helps us to avoid elements that are not required:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对支持集嵌入 `g_embeddings` 执行softmax注意力：即，它帮助我们避免不需要的元素：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We update `previous_state` and repeat these steps for a number of processing
    steps, `K`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新 `previous_state` 并重复这些步骤，执行 `K` 次处理步骤：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The complete code for computing `f_embeddings` is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `f_embeddings` 的完整代码如下：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The architecture of matching networks
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配网络的架构
- en: The overall flow of matching network is shown in the following diagram and it
    is different from the image we saw already. You can see how the support set ![](img/8e559ad8-2621-4773-bc81-a1767a7b5a7c.png)
    and query set ![](img/b5c528a1-c1e1-4e80-ab81-9ab50e7076c8.png) are calculated
    through the embedding functions ![](img/cb68a610-6d09-4529-a1de-985a5abb612e.png)
    and ![](img/7017a935-4dcd-40d7-a4bc-d8162fcefe1f.png) respectively.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配网络的整体流程如下图所示，与我们已经看到的图像不同。您可以看到如何通过嵌入函数 ![](img/cb68a610-6d09-4529-a1de-985a5abb612e.png)
    和 ![](img/7017a935-4dcd-40d7-a4bc-d8162fcefe1f.png) 计算支持集 ![](img/8e559ad8-2621-4773-bc81-a1767a7b5a7c.png)
    和查询集 ![](img/b5c528a1-c1e1-4e80-ab81-9ab50e7076c8.png)。
- en: 'As you can see, the embedding function ![](img/fc7229de-25ee-464e-b14f-fc645b767ee3.png)
    takes the query set along with the support set embeddings as input:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，嵌入函数 ![](img/fc7229de-25ee-464e-b14f-fc645b767ee3.png) 将查询集 ![](img/0b5d811d-c997-4209-83cc-48eafacacc5f.png)
    与支持集嵌入一起作为输入：
- en: '![](img/2de06008-7882-4735-91d6-3beacb5eca89.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2de06008-7882-4735-91d6-3beacb5eca89.png)'
- en: Congratulations again for learning all of the important and popular deep learning
    algorithms! Deep learning is an interesting and very popular field of AI that
    has revolutionized the world. Now that you've finished reading the book, you can
    start exploring various advancements in deep learning and start experimenting
    with various projects. Learn and deep learn!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 再次祝贺您学习了所有重要和流行的深度学习算法！深度学习是一个非常有趣和流行的AI领域，它已经改变了世界。现在您已经完成了书籍的阅读，可以开始探索深度学习的各种进展，并开始尝试各种项目。学习和深入学习！
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started off the chapter by understanding what k-shot learning is. We learned
    that in n-way k-shot learning, n-way implies the number of classes we have in
    our dataset and k-shot implies the number of data points we have in each class;
    and support set and the query set are equivalent to the train and test sets. Then
    we explored siamese networks. We learned how siamese networks use an identical
    network to learn the similarity of two inputs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解k-shot学习开始本章。我们了解到在n-way k-shot学习中，n-way表示数据集中的类别数，k-shot表示每个类别中的数据点数量；支持集和查询集相当于训练集和测试集。然后我们探索了孪生网络。我们学习了孪生网络如何使用相同的网络学习两个输入的相似度。
- en: Followed by this, we learned about prototypical networks, which create a prototypical
    representation of each class and classify a query point (a new point) based on
    the distance between the class prototype and the query point. We also learned
    how relation networks use two different functions embedding and relation function
    to classify an image.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们学习了原型网络，它创建每个类的原型表示，并基于类原型与查询点之间的距离对查询点（新点）进行分类。我们还学习了关系网络如何使用两个不同的函数，嵌入函数和关系函数来分类图像。
- en: At the end of the chapter, we learned about matching networks and how it uses
    different embedding functions for the support set and the query set to classify
    an image.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们学习了匹配网络以及它如何使用支持集和查询集的不同嵌入函数来分类图像。
- en: Deep learning is one of the most interesting branches in the field of AI. Now
    that you've understood various deep learning algorithms, you can start building
    deep learning models and create interesting applications and also contribute to
    deep learning research.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是人工智能领域中最有趣的分支之一。现在您已经了解了各种深度学习算法，可以开始构建深度学习模型，创建有趣的应用，并为深度学习研究做出贡献。
- en: Questions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s assess the knowledge acquired from this chapter by answering the following
    questions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估从本章中获得的知识：
- en: What is few-shot learning?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是少样本学习？
- en: What are the support and query sets?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是支持集和查询集？
- en: Define siamese networks.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义孪生网络。
- en: Define energy functions.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义能量函数。
- en: What is the loss function of siamese networks?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 孪生网络的损失函数是什么？
- en: How does the prototypical network work?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原型网络是如何工作的？
- en: What are the different types of functions used in relation networks?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在关系网络中使用的不同类型函数是什么？
- en: Further reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To learn more about how to learn from a small number of data points, check out
    *Hands-On Meta Learning with Python* by Sudharsan Ravichandiran, published by
    Packt publishing available at, [https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何从少量数据点中学习更多，请查看Sudharsan Ravichandiran撰写、由Packt出版的*Hands-On Meta Learning
    with Python*，可在[https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-meta-learning-python)获取。
