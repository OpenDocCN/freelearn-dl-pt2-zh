- en: Fundamentals of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw practical examples of how to build deep learning
    models to solve classification and regression problems, such as image classification
    and average user view predictions. Similarly, we developed an intuition on how
    to frame a deep learning problem. In this chapter, we will take a look at how
    we can attack different kinds of problems and different tweaks that we will potentially
    end up using to improve our model's performance on our problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore:'
  prefs: []
  type: TYPE_NORMAL
- en: Other forms of problems beyond classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with evaluation, understanding overfitting, underfitting, and techniques
    to solve them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, most of the topics that we discuss in this chapter are common to machine
    learning and deep learning, except for some of the techniques—such as dropout—that
    we use to solve overfitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: Three kinds of machine learning problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all our previous examples, we tried to solve either classification (predicting
    cats or dogs) or regression (predicting the average time users spend in the platform)
    problems. All these are examples of supervised learning, where the goal is to
    map the relationship between training examples and their targets and use it to
    make predictions on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning is just one part of machine learning, and there are other
    different parts of machine learning. There are three different kinds of machine
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look in detail at the kinds of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the successful use cases in the deep learning and machine learning
    space fall under supervised learning. Most of the examples we cover in this book
    will also be part of this. Some of the common examples of supervised learning
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification problems**: Classifying dogs and cats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression problems**: Predicting stock prices, cricket match scores, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image segmentation**: Doing a pixel-level classification. For a self-driving
    car, it is important to identify what each pixel belongs to from the photo taken
    by its camera. The pixel could belong to a car, pedestrian, tree, bus, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: OK Google, Alexa, and Siri are good examples of speech
    recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: Translating speech from one language to another language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When there is no label data, unsupervised learning techniques help in understanding
    the data by visualizing and compressing. The two commonly-used techniques in unsupervised
    learning are:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering helps in grouping all similar data points together. Dimensionality
    reduction helps in reducing the number of dimensions, so that we can visualize
    high-dimensional data to find any hidden patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is the least popular machine learning category. It did
    not find its success in real-world use cases. However, it has changed in recent
    years, and teams from Google DeepMind were able to successfully build systems
    based on reinforcement learning and were able to win the AlphaGo game against
    the world champion. This kind of technology advancement, where a computer can
    beat a human in a game, was considered to take more than a few decades for computers
    to achieve. However, deep learning combined with reinforcement learning was able
    to achieve it far sooner than anyone would have anticipated. These techniques
    have started seeing early success, and it could probably take a few years for
    it to become mainstream.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus mostly on the supervised techniques and some of
    the unsupervised techniques that are specific to deep learning, such as generative
    networks used for creating images of a particular style called **style transfer**
    and **generative adversarial networks**.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last few chapters, we have used lot of terminology that could be completely
    new to you if you are just entering the machine learning or deep learning space.
    We will list a lot of commonly-used terms in machine learning, which are also
    used in the deep learning literature:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample** **or input or** **data point**: These mean particular instances
    of training a set. In our image classification problem seen in the last chapter,
    each image can be referred to as a sample, input, or data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction** **or** **output**: The value our algorithm generates as an output.
    For example, in our previous example our algorithm predicted a particular image
    as 0, which is the label given to cat, so the number 0 is our prediction or output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target** **or label**: The actual tagged label for an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss value** **or prediction error**: Some measure of distance between the
    predicted value and actual value. The smaller the value, the better the accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classes**: Possible set of values or labels for a given dataset. In the example
    in our previous chapter, we had two classes—cats and dogs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**: A classification task where each input example should
    be classified as either one of the two exclusive categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: A classification task where each input example
    can be classified into of more than two different categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-label classification**: An input example can be tagged with multiple
    labels—for example, tagging a restaurant with different types of food it serves
    such as Italian, Mexican, and Indian. Another commonly-used example is object
    detection in an image, where the algorithm identifies different objects in the
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalar regression**: Each input data point will be associated with one scalar
    quality, which is a number. Some examples could be predicting house prices, stock
    prices, and cricket scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector regression**: Where the algorithm needs to predict more than one scalar
    quantity. One good example is when you try to identify the bounding box that contains
    the location of a fish in an image. In order to predict the bounding box, your
    algorithm needs to predict four scalar quantities denoting the edges of a square.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch**: For most cases, we train our algorithm on a bunch of input samples
    referred to as the batch. The batch size varies generally from 2 to 256, depending
    on the GPU''s memory. The weights are also updated for each batch, so the algorithms
    tend to learn faster than when trained on a single example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epoch**: Running the algorithm through a complete dataset is called an **epoch**.
    It is common to train (update the weights) for several epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the example of image classification that we covered in the last chapter,
    we split the data into two different halves, one for training and one for validation.
    It is a good practice to use a separate dataset to test the performance of your
    algorithm, as testing the algorithm on the training set may not give you the true
    generalization power of the algorithm. In most real-world use cases, based on
    the validation accuracy, we often tweak our algorithm in different ways, such
    as adding more layers or different layers, or using different techniques that
    we will cover in the later part of the chapter. So, there is a higher chance that
    your choices for tweaking the algorithm are based on the validation dataset. Algorithms
    trained this way tend to perform well in the training dataset and the validation
    dataset, but fail to generalize well on unseen data. This is due to an information
    leak from your validation dataset, which influences us in tweaking the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the problem of an information leak and improve generalization, it is
    often a common practice to split the dataset into three different parts, namely
    a training, validation, and test dataset. We do the training and do all the hyper
    parameter tuning of the algorithm using the training and validation set. At the
    end, when the entire training is done, then you will test the algorithm on the
    test dataset. There are two types of parameters that we talk about. One is the
    parameters or weights that are used inside an algorithm, which are tuned by the
    optimizer or during backpropagation. The other set of parameters, called **hyper
    parameters**, controls the number of layers used in the network, learning rate,
    and other types of parameter that generally change the architecture, which is
    often done manually.
  prefs: []
  type: TYPE_NORMAL
- en: The phenomenon of a particular algorithm performing better in the training set
    and failing to perform on the validation or test set is called **overfitting**,
    or the lack of the algorithm's ability to generalize. There is an opposite phenomenon
    where the algorithm fails to perform for the training set, which is called **underfitting**.
    We will look at different strategies that will help us in overcoming the overfitting
    and underfitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the various strategies available for splitting the dataset before
    looking at overfitting and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Training, validation, and test split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is best practice to split the data into three parts—training, validation,
    and test datasets. The best approach for using the holdout dataset is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the algorithm on the training dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform hyper parameter tuning based on the validation dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the first two steps iteratively until the expected performance is achieved
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After freezing the algorithm and the hyper parameters, evaluate it on the test
    dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Avoid splitting the data into two parts, as it may lead to an information leak.
    Training and testing it on the same dataset is a clear no-no as it does not guarantee
    algorithm generalization. There are three popular holdout strategies that can
    be used to split the data into training and validation sets. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple holdout validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-fold validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterated k-fold validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple holdout validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Set apart a fraction of the data as your test dataset. What fraction to keep
    may be very problem-specific and could largely depend on the amount of data available.
    For problems particularly in the fields of computer vision and NLP, collecting
    labeled data could be very expensive, so to hold out a large fraction of 30% may
    make it difficult for the algorithm to learn, as it will have less data to train
    on. So, depending on the data availability, choose the fraction of it wisely.
    Once the test data is split, keep it apart until you freeze the algorithm and
    its hyper parameters. For choosing the best hyper parameters for the problem,
    choose a separate validation dataset. To avoid overfitting, we generally divide
    available data into three different sets, as shown in following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a1f5457-52c5-4365-bed5-c7e4f4315177.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We used a simple implementation of the preceding figure in the last chapter
    to create our validation set. Let''s look at a snapshot of the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is one of the simplest holdout strategies and is commonly used to start
    with. There is a disadvantage of using this with small datasets. The validation
    dataset or test dataset may not be statistically representative of the data at
    hand. We can easily recognize this by shuffling the data before holding out. If
    the results obtained are not consistent, then we need to use a better approach.
    To avoid this issue, we often end up using k-fold or iterated k-fold validation.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keep a fraction of the dataset for the test split, then divide the entire dataset
    into k-folds where k can be any number, generally varying from two to ten. At
    any given iteration, we hold one block for validation and train the algorithm
    on the rest of the blocks. The final score is generally the average of all the
    scores obtained across the k-folds. The following diagram shows an implementation
    of k-fold validation where k is four; that is, the data is split into four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52552aac-517c-49b3-8986-04158c3d3e42.png)'
  prefs: []
  type: TYPE_IMG
- en: One key thing to note when using the k-fold validation dataset is that it is
    very expensive, because you run the algorithm several times on different parts
    of the dataset, which can turn out to be very expensive for computation-intensive
    algorithms—particularly in areas of computer vision algorithms, where, sometimes,
    training an algorithm could take anywhere from minutes to days. So, use this technique
    wisely.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold validation with shuffling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make things complex and robust, you can shuffle the data every time you create
    your holdout validation dataset. It is very helpful for solving problems where
    a small boost in performance could have a huge business impact. If your case is
    to quickly build and deploy algorithms and you are OK with compromising a few
    percent in performance difference, then this approach may not be worth it. It
    all boils down to what problem you are trying to solve, and what accuracy means
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few other things that you may need to consider when splitting up
    the data, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Data representativeness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data representativeness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the example we saw in our last chapter, we classified images as either dogs
    or cats. Let's take a scenario where all the images are sorted and the first 60%
    of images are dogs and the rest are cats. If we split this dataset by choosing
    the first 80% as the training dataset and the rest as the validation set, then
    the validation dataset will not be a true representation of the dataset, as it
    will only contain cat images. So, in these cases, care should be taken that we
    have a good mix by shuffling the data before splitting or doing a stratified sampling.
    Stratified sampling refers to picking up data points from each category to create
    validation and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Time sensitivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take the case of predicting stock prices. We have data from January to
    December. In this case, if we do a shuffle or stratified sampling then we end
    up with an information leak, as the prices could be sensitive to time. So, create
    the validation dataset in such a way that there is no information leak. In this
    case, choosing the December data as the validation dataset could make more sense.
    In the case of stock prices it is more complex than this, so domain-specific knowledge
    also comes into play when choosing the validation split.
  prefs: []
  type: TYPE_NORMAL
- en: Data redundancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Duplicates are common in data. Care should be taken so that the data present
    in the training, validation, and test sets are unique. If there are duplicates,
    then the model may not generalize well on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at different ways to split our datasets to build our evaluation
    strategy. In most cases, the data that we receive may not be in a format that
    can be readily used by us for training our algorithms. In this section, we will
    cover some of the preprocessing techniques and feature engineering techniques.
    Though most of the feature engineering techniques are domain-specific, particularly
    in the areas of computer vision and text, there are some common feature engineering
    techniques that are common across the board, which we will discuss in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data preprocessing for neural networks is a process in which we make the data
    more suitable for the deep learning algorithms to train on. The following are
    some of the commonly-used data preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data comes in various formats such as text, sound, images, and video. The very
    first thing that needs to be done is to convert the data into PyTorch tensors.
    In the previous example, we used `torchvision` utility functions to convert **Python
    Imaging Library** (**PIL**) images into a Tensor object, though most of the complexity
    is abstracted away by the PyTorch torchvision libraries. In [Chapter 7](dc189b2e-5166-41a5-8203-aff8b367b53c.xhtml),
    *Generative Networks*, when we deal with **recurrent neural networks** (**RNNs**),
    we will see how text data can be converted into PyTorch tensors. For problems
    involving structured data, the data is already present in a vectorized format;
    all we need to do is convert them into PyTorch tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Value normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a common practice to normalize features before passing the data to any
    machine learning algorithm or deep learning algorithm. It helps in training the
    algorithms faster and helps in achieving more performance. Normalization is the
    process in which you represent data belonging to a particular feature in such
    a way that its mean is zero and standard deviation is one.
  prefs: []
  type: TYPE_NORMAL
- en: In the example of *dogs and cats*, the classification that we covered in the
    last chapter, we normalized the data by using the mean and standard deviation
    of the data available in the `ImageNet` dataset. The reason we chose the `ImageNet`
    dataset's mean and standard deviation for our example is that we are using the
    weights of the ResNet model, which was pretrained on ImageNet. It is also a common
    practice to divide each pixel value by 255 so that all the values fall in the
    range between zero and one, particularly when you are not using pretrained weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalization is also applied for problems involving structured data. Say we
    are working on a house price prediction problem—there could be different features
    that could fall in different scales. For example, distance to the nearest airport
    and the age of the house are variables or features that could be in different
    scales. Using them with neural networks as they are could prevent the gradients
    from converging. In simple words, loss may not go down as expected. So, we should
    be careful to apply normalization to any kind of data before training on our algorithms.
    To ensure that the algorithm or model performs better, ensure that the data follows
    the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Take small values**: Typically in a range between zero and one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Same range**: Ensure all the features are in the same range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing values are quite common in real-world machine learning problems. From
    our previous examples of predicting house prices, certain fields for the age of
    the house could be missing. It is often safe to replace the missing values with
    a number that may not occur otherwise. The algorithms will be able to identify
    the pattern. There are other techniques that are available to handle missing values
    that are more domain-specific.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feature engineering is the process of using domain knowledge about a particular
    problem to create new variables or features that can be passed to the model. To
    understand better, let''s look at a sales prediction problem. Say we have information
    about promotion dates, holidays, competitor''s start date, distance from competitor,
    and sales for a particular day. In the real world, there could be hundreds of
    features that may be useful in predicting the prices of stores. There could be
    certain information that could be important in predicting the sales. Some of the
    important features or derived values are:'
  prefs: []
  type: TYPE_NORMAL
- en: Days until the next promotion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Days left before the next holiday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of days the competitor's business has been open
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There could be many more such features that can be extracted that come from
    domain knowledge. Extracting these kinds of features for any machine learning
    algorithm or deep learning algorithm could be quite challenging for the algorithms
    to perform themselves. For certain domains, particularly in the fields of computer
    vision and text, modern deep learning algorithms help us in getting away with
    feature engineering. Except for these fields, good feature engineering always
    helps in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem can be solved a lot faster with less computational resource.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep learning algorithms can learn features without manually engineering
    them by using huge amounts of data. So, if you are tight on data, then it is good
    to focus on good feature engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding overfitting and underfitting is the key to building successful
    machine learning and deep learning models. At the start of the chapter, we briefly
    covered what underfitting and overfitting are; let's take a look at them in detail
    and how we can solve them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting, or not generalizing, is a common problem in machine learning and
    deep learning. We say a particular algorithm overfits when it performs well on
    the training dataset but fails to perform on unseen or validation and test datasets.
    This mostly occurs due to the algorithm identifying patterns that are too specific
    to the training dataset. In simpler words, we can say that the algorithm figures
    out a way to memorize the dataset so that it performs really well on the training
    dataset and fails to perform on the unseen data. There are different techniques
    that can be used to avoid the algorithm overfitting. Some of the techniques are:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting more data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the size of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying weight regularizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting more data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are able to get more data on which the algorithm can train, that can
    help the algorithm to avoid overfitting by focusing on general patterns rather
    than on patterns specific to small data points. There are several cases where
    getting more labeled data could be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: There are techniques, such as data augmentation, that can be used to generate
    more training data in problems related to computer vision. Data augmentation is
    a technique where you can adjust the images slightly by performing different actions
    such as rotating, cropping, and generating more data. With enough domain understanding,
    you can create synthetic data too if capturing actual data is expensive. There
    are other ways that can help to avoid overfitting when you are unable to get more
    data. Let's look at them.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the size of the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The size of the network in general refers to the number of layers or the number
    of weight parameters used in a network. In the example of image classification
    that we saw in the last chapter, we used a ResNet model that has 18 blocks consisting
    of different layers inside it. The torchvision library in PyTorch comes with ResNet
    models of different sizes starting from 18 blocks and going up to 152 blocks.
    Say, for example, if we are using a ResNet block with 152 blocks and the model
    is overfitting, then we can try using a ResNet with 101 blocks or 50 blocks. In
    the custom architectures we build, we can simply remove some intermediate linear
    layers, thus preventing our PyTorch models from memorizing the training dataset.
    Let''s look at an example code snippet that demonstrates what it means exactly
    to reduce the network size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding architecture has three linear layers, and let''s say it overfits
    our training data. So, let''s recreate the architecture with reduced capacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding architecture has only two linear layers, thus reducing the capacity
    and, in turn, potentially avoiding overfitting the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Applying weight regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the key principles that helps to solve the problem of overfitting or
    generalization is building simpler models. One technique for building simpler
    models is to reduce the complexity of the architecture by reducing its size. The
    other important thing is ensuring that the weights of the network do not take
    larger values. Regularization provides constraints on the network by penalizing
    the model when the weights of the model are larger. Whenever the model uses larger
    weights, the regularization kicks in and increases the loss value, thus penalizing
    the model. There are two types of regularization possible. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 regularization**: The sum of absolute values of weight coefficients are
    added to the cost. It is often referred to as the L1 norm of the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization**: The sum of squares of all weight coefficients are added
    to the cost. It is often referred to as the L2 norm of the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch provides an easy way to use L2 regularization by enabling the `weight_decay`
    parameter in the optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By default, the weight decay parameter is set to zero. We can try different
    values for weight decay; a small value such as `1e-5` works most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dropout is one of the most commonly used and the most powerful regularization
    techniques used in deep learning. It was developed by Hinton and his students
    at the University of Toronto. Dropout is applied to intermediate layers of the
    model during the training time. Let''s look at an example of how dropout is applied
    on a linear layer''s output that generates 10 values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba017512-24f1-4da2-9703-541556e1da4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure shows what happens when dropout is applied to the linear
    layer output with a threshold value of **0.2**. It randomly masks or zeros 20%
    of data, so that the model will not be dependent on a particular set of weights
    or patterns, thus overfitting. Let''s look at another example where we apply a
    dropout with a threshold value of **0.5**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3976da11-f63f-46f1-9398-e0723c74eb82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is often common to use a threshold of dropout values in the range of 0.2
    to 0.5, and the dropout is applied at different layers. Dropouts are used only
    during the training times, and during the testing values are scaled down by the
    factor equal to the dropout. PyTorch provides dropout as another layer, thus making
    it easier to use. The following code snippet shows how to use a dropout layer
    in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The dropout layer accepts an argument called `training`, which needs to be set
    to `True` during the training phase and false during the validation or test phase.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when our model may fail to learn any patterns from our training
    data, which will be quite evident when the model fails to perform well even on
    the dataset it is trained on. One common thing to try when your model underfits
    is to acquire more data for the algorithm to train on. Another approach is to
    increase the complexity of the model by increasing the number of layers or by
    increasing the number of weights or parameters used by the model. It is often
    a good practice not to use any of the aforementioned regularization techniques
    until we actually overfit the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow of a machine learning project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will formalize a solution framework that can be used to
    solve any machine learning problem by bringing together the problem statement,
    evaluation, feature engineering, and avoidance of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition and dataset creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To define the problem, we need two important things; namely, the input data
    and the type of problem.
  prefs: []
  type: TYPE_NORMAL
- en: What will be our input data and target labels? For example, say we want to classify
    restaurants based on their speciality—say Italian, Mexican, Chinese, and Indian
    food—from the reviews given by the customers. To start working with this kind
    of problem, we need to manually hand annotate the training data as one of the
    possible categories before we can train the algorithm on it. Data availability
    is often a challenging factor at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the type of problem will help in deciding whether it is a binary
    classification, multi-classification, scalar regression (house pricing), or vector
    regression (bounding boxes). Sometimes, we may have to use some of the unsupervised
    techniques such as clustering and dimensionality reduction. Once the problem type
    is identified, then it becomes easier to determine what kind of architecture,
    loss function, and optimizer should be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the inputs and have identified the type of the problem, then we
    can start building our models with the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: There are hidden patterns in the data that can help map the input with the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data that we have is sufficient for the model to learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As machine learning practitioners, we need to understand that we may not be
    able to build a model with just some input data and target data. Let's take predicting
    stock prices as an example. Let's assume we have features representing historical
    prices, historical performance, and competition details, but we may still fail
    to build a meaningful model that can predict stock prices, as stock prices could
    actually be influenced by a variety of other factors such as the domestic political
    scenario, international political scenario, natural factors such as having a good
    monsoon, and many other factors that may not be represented by our input data.
    So, there is no way that any machine learning or deep learning model would be
    able to identify patterns. So, based on the domain, carefully pick features that
    can be real indicators of the target variable. All these could be reasons for
    the models to underfit.
  prefs: []
  type: TYPE_NORMAL
- en: There is another important assumption that machine learning makes. Future or
    unseen data will be close to the patterns, as described by the historical data.
    Sometimes, our models could fail, as the patterns never existed in the historical
    data, or the data on which the model was trained did not cover certain seasonalities
    or patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Measure of success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The measure of success will be directly determined by your business goal. For
    example, when trying to predict when the next machine failure will occur in windmills,
    we would be more interested to know how many times the model was able to predict
    the failures. Using simple accuracy can be the wrong metric, as most of the time
    the model will predict correctly when the machine will not fail, as that is the
    most common output. Say we get an accuracy of 98%, and the model was wrong each
    time in predicting the failure rate—such models may not be of any use in the real
    world. Choosing the correct measure of success is crucial for business problems.
    Often, these kinds of problems have imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For balanced classification problems, where all the classes have a likely accuracy,
    ROC and **Area under the curve** (**AUC**) are common metrics. For imbalanced
    datasets, we can use precision and recall. For ranking problems, we can use mean
    average precision.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you decide how you are going to evaluate the current progress, it is important
    to decide how you are going to evaluate on your dataset. We can choose from the
    three different ways of evaluating our progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Holdout validation set**: Most commonly used, particularly when you have
    enough data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-fold cross validation**: When you have limited data, this strategy helps
    you to evaluate on different portions of the data, helping to give us a better
    view of the performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterated k-fold validation**: When you are looking to go the extra mile with
    the performance of the model, this approach will help'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bring different formats of available data into tensors through vectorization
    and ensure that all the features are scaled and normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create a very simple model that beats the baseline score. In our previous example
    of dogs and cats, classification, the baseline accuracy should be 0.5 and our
    simple model should be able to beat this score. If we are not able to beat the
    baseline score, then maybe the input data does not hold the necessary information
    required to make the necessary prediction. Remember not to introduce any regularization
    or dropouts at this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the model work, we have to make three important choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Choice of last layer**: For a regression, it should be a linear layer generating
    a scalar value as output. For a vector regression problem, it would be the same
    linear layer generating more than one scalar output. For a bounding box, it outputs
    four values. For a binary classification, it is often common to use sigmoid, and
    for multi-class classification it is softmax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of loss function**: The type of the problem will help you in deciding
    the loss function. For a regression problem, such as predicting house prices,
    we use the mean squared error, and for classification problems we use categorical
    cross entropy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: Choosing the right optimization algorithm and some of its
    hyper parameters is quite tricky, and we can find them by experimenting with different
    ones. For most of the use cases, an Adam or RMSprop optimization algorithm works
    better. We will cover some of the tricks that can be used for learning rate selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s summarize what kind of loss function and activation function we would
    use for the last layer of the network in our deep learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Problem type** | **Activation function** | **Loss function** |'
  prefs: []
  type: TYPE_TB
- en: '| Binary classification | Sigmoid activation | `nn.CrossEntropyLoss()` |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-class classification | Softmax activation | `nn.CrossEntropyLoss()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-label classification | Sigmoid activation | `nn.CrossEntropyLoss()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Regression | None | MSE |'
  prefs: []
  type: TYPE_TB
- en: '| Vector regression | None | MSE |'
  prefs: []
  type: TYPE_TB
- en: Large model enough to overfit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have a model that has enough capacity to beat your baseline score,
    increase your baseline capacity. A few simple tricks to increase the capacity
    of your architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add more layers to your existing architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more weights to the existing layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train it for more epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We generally train the model for an adequate number of epochs. Stop it when
    the training accuracy keeps increasing and the validation accuracy stops increasing
    and probably starts dropping; that's where the model starts overfitting. Once
    we reach this stage, we need to apply regularization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the number of layers, size of layers, and number of epochs may change
    from problem to problem. A smaller architecture can work for a simple classification
    problem, but for a complex problem such as facial recognition, we would need enough
    expressiveness in our architecture and the model needs to be trained for more
    epochs than for a simple classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Applying regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finding the best way to regularize the model or algorithm is one of the trickiest parts
    of the process, since there are a lot of parameters to be tuned. Some of the parameters
    that we can tune to regularize the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding dropout**: This can be complex as this can be added between different
    layers, and finding the best place is usually done through experimentation. The
    percentage of dropout to be added is also tricky, as it is purely dependent on
    the problem statement we are trying to solve. It is often good practice to start
    with a small number such as 0.2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trying different architectures**: We can try different architectures, activation
    functions, numbers of layers, weights, or parameters inside the layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding L1 or L2 regularization**: We can make use of either one of regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trying different learning rates**: There are different techniques that can
    be used, which we will discuss in the later sections of the chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding more features or more data**: This is probably done by acquiring more
    data or augmenting data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use our validation dataset to tune all the aforementioned hyper parameters.
    As we keep iterating and tweaking the hyper parameters, we may end up with the
    problem of data leakage. So, we should ensure that we have holdout data for testing.
    If the performance of the model on the test data is good in comparison to the
    training and validation, then there is a good chance that our model will perform
    well on unseen data. But, if the model fails to perform on the test data but performs
    on the validation and training data, then there is a chance that the validation
    data is not a good representation of the real-world dataset. In such scenarios,
    we can end up using k-fold validation or iterated k-fold validation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate picking strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finding the right learning rate for training the model is an ongoing area of
    research where a lot of progress has been made. PyTorch provides some of the techniques
    to tune the learning rate, and they are provided in the `torch.optim.lr_sheduler`
    package. We will explore some of the techniques that PyTorch provides to choose
    the learning rates dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: '**StepLR**: This scheduler takes two important parameters. One is step size,
    which denotes for what number of epochs the learning rate has to change, and the
    second parameter is gamma, which decides how much the learning rate has to be
    changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a learning rate of `0.01`, step size of 10, and gamma size of `0.1`, for
    every 10 epochs the learning rate changes by gamma times. That is, for the first
    10 epochs, the learning rate changes to 0.001, and by the end, on the next 10
    epochs, it changes to 0.0001. The following code explains the implementation of
    `StepLR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**MultiStepLR**: MultiStepLR works similarly to StepLR, except for the fact
    that the steps are not at regular intervals; steps are given as lists. For example,
    it is given as a list of 10, 15, 30, and for each step value, the learning rate
    is multiplied by its gamma value. The following code explains the implementation
    of `MultiStepLR`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**ExponentialLR**: This sets the learning rate to a multiple of the learning
    rate with gamma values for each epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReduceLROnPlateau**: This is one of the commonly used learning rate strategies.
    In this case, the learning rate changes when a particular metric, such as training
    loss, validation loss, or accuracy stagnates. It is a common practice to reduce
    the learning rate by two to 10 times its original value. `ReduceLROnPlateau` can
    be implemented as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered some of the common and best practices that are used
    in solving machine learning or deep learning problems. We covered various important
    steps such as creating problem statements, choosing the algorithm, beating the
    baseline score, increasing the capacity of the model until it overfits the dataset,
    applying regularization techniques that can prevent overfitting, increasing the
    generalization capacity, tuning different parameters of the model or algorithms,
    and exploring different learning strategies that can be used to train deep learning
    models optimally and faster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover different components that are responsible
    for building state-of-the-art **Convolutional Neural Networks** (**CNNs**). We
    will also cover transfer learning, which helps us to train image classifiers when
    little data is available. We will also cover techniques that help us to train
    these algorithms more quickly.
  prefs: []
  type: TYPE_NORMAL
