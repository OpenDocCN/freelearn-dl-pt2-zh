["```py\npip install pygame\n```", "```py\n>>> from pygame.image import load\n >>> from pygame.surfarray import pixels_alpha\n >>> from pygame.transform import rotate\n >>> def load_images(sprites_path):\n ...     base_image = load(sprites_path + \n                             'base.png').convert_alpha()\n ...     background_image = load(sprites_path + \n                             'background-black.png').convert()\n ...     pipe_images = [rotate(load(sprites_path + \n                        'pipe-green.png').convert_alpha(), 180),\n ...                    load(sprites_path + \n                             'pipe-green.png').convert_alpha()]\n ...     bird_images = [load(sprites_path + \n                           'redbird-upflap.png').convert_alpha(),\n ...                    load(sprites_path + \n                         'redbird-midflap.png').convert_alpha(),\n ...                    load(sprites_path + \n                         'redbird-downflap.png').convert_alpha()]\n ...     bird_hitmask = [pixels_alpha(image).astype(bool) \n                             for image in bird_images]\n ...     pipe_hitmask = [pixels_alpha(image).astype(bool) \n                             for image in pipe_images]\n ...     return base_image, background_image, pipe_images, \n                 bird_images, bird_hitmask, pipe_hitmask\n```", "```py\n>>> from itertools import cycle\n>>> from random import randint\n>>> import pygame\n```", "```py\n>>> pygame.init()\n>>> fps_clock = pygame.time.Clock() >>> fps = 30\n```", "```py\n>>> screen_width = 288\n >>> screen_height = 512\n >>> screen = pygame.display.set_mode((screen_width, screen_height)) >>> pygame.display.set_caption('Flappy Bird')\n```", "```py\n>>> base_image, background_image, pipe_images, bird_images, bird_hitmask, pipe_hitmask = load_images('sprites/')\n```", "```py\n>>> bird_width = bird_images[0].get_width()\n>>> bird_height = bird_images[0].get_height()\n>>> pipe_width = pipe_images[0].get_width()\n>>> pipe_height = pipe_images[0].get_height() >>> pipe_gap_size = 100\n```", "```py\n>>> bird_index_gen = cycle([0, 1, 2, 1])\n```", "```py\n>>> class FlappyBird(object):\n ...     def __init__(self):\n ...         self.pipe_vel_x = -4\n ...         self.min_velocity_y = -8\n ...         self.max_velocity_y = 10\n ...         self.downward_speed = 1\n ...         self.upward_speed = -9\n ...         self.cur_velocity_y = 0\n ...         self.iter = self.bird_index = self.score = 0\n ...         self.bird_x = int(screen_width / 5)\n ...         self.bird_y = int((screen_height - bird_height) / 2)\n ...         self.base_x = 0\n ...         self.base_y = screen_height * 0.79\n ...         self.base_shift = base_image.get_width() - \n                             background_image.get_width()\n ...         self.pipes = [self.gen_random_pipe(screen_width), \n                         self.gen_random_pipe(screen_width * 1.5)]\n ...         self.is_flapped = False\n```", "```py\n>>>     def gen_random_pipe(self, x):\n ...         gap_y = randint(2, 10) * 10 + int(self.base_y * 0.2)\n ...         return {\"x_upper\": x,\n ...                 \"y_upper\": gap_y - pipe_height,\n ...                 \"x_lower\": x,\n ...                 \"y_lower\": gap_y + pipe_gap_size}\n```", "```py\n>>>     def check_collision(self):\n ...         if bird_height + self.bird_y >= self.base_y - 1:\n ...             return True\n ...         bird_rect = pygame.Rect(self.bird_x, self.bird_y, \n                                     bird_width, bird_height)\n ...         for pipe in self.pipes:\n ...             pipe_boxes = [pygame.Rect(pipe[\"x_upper\"], \n                          pipe[\"y_upper\"], pipe_width, pipe_height),\n ...                           pygame.Rect(pipe[\"x_lower\"], \n                          pipe[\"y_lower\"], pipe_width, pipe_height)]\n ...             # Check if the bird's bounding box overlaps to \n                     the bounding box of any pipe\n ...             if bird_rect.collidelist(pipe_boxes) == -1:\n ...                 return False\n ...             for i in range(2):\n ...                 cropped_bbox = bird_rect.clip(pipe_boxes[i])\n ...                 x1 = cropped_bbox.x - bird_rect.x\n ...                 y1 = cropped_bbox.y - bird_rect.y\n ...                 x2 = cropped_bbox.x - pipe_boxes[i].x\n ...                 y2 = cropped_bbox.y - pipe_boxes[i].y\n ...                 for x in range(cropped_bbox.width):\n ...                     for y in range(cropped_bbox.height):\n ...                         if bird_hitmask[self.bird_index][x1+x, \n                                    y1+y] and pipe_hitmask[i][\n                                    x2+x, y2+y]:\n ...                             return True\n ...         return False\n```", "```py\n>>>     def next_step(self, action):\n ...         pygame.event.pump()\n ...         reward = 0.1\n ...         if action == 1:\n ...             self.cur_velocity_y = self.upward_speed\n ...             self.is_flapped = True\n ...         # Update score\n ...         bird_center_x = self.bird_x + bird_width / 2\n ...         for pipe in self.pipes:\n ...             pipe_center_x = pipe[\"x_upper\"] + \n                                     pipe_width / 2\n ...             if pipe_center_x < bird_center_x \n                                 < pipe_center_x + 5:\n ...                 self.score += 1\n ...                 reward = 1\n ...                 break\n ...         # Update index and iteration\n ...         if (self.iter + 1) % 3 == 0:\n ...             self.bird_index = next(bird_index_gen)\n ...         self.iter = (self.iter + 1) % fps\n ...         self.base_x = -((-self.base_x + 100) % \n                                 self.base_shift)\n ...         # Update bird's position\n ...         if self.cur_velocity_y < self.max_velocity_y \n                             and not self.is_flapped:\n ...             self.cur_velocity_y += self.downward_speed\n ...         self.is_flapped = False\n ...         self.bird_y += min(self.cur_velocity_y, \n                 self.bird_y - self.cur_velocity_y - bird_height)\n ...         if self.bird_y < 0:\n ...             self.bird_y = 0\n ...         # Update pipe position\n ...         for pipe in self.pipes:\n ...             pipe[\"x_upper\"] += self.pipe_vel_x\n ...             pipe[\"x_lower\"] += self.pipe_vel_x\n ...         # Add new pipe when first pipe is     \n                 about to touch left of screen\n ...         if 0 < self.pipes[0][\"x_lower\"] < 5:\n ...             self.pipes.append(self.gen_random_pipe( screen_width + 10))\n ...         # remove first pipe if its out of the screen\n ...         if self.pipes[0][\"x_lower\"] < -pipe_width:\n ...             self.pipes.pop(0)\n ...         if self.check_collision():\n ...             is_done = True\n ...             reward = -1\n ...             self.__init__()\n ...         else:\n ...             is_done = False\n ...         # Draw sprites\n ...         screen.blit(background_image, (0, 0))\n ...         screen.blit(base_image, (self.base_x, self.base_y))\n ...         screen.blit(bird_images[self.bird_index], \n                             (self.bird_x, self.bird_y))\n ...         for pipe in self.pipes:\n ...             screen.blit(pipe_images[0], (pipe[\"x_upper\"], pipe[\"y_upper\"]))\n ...             screen.blit(pipe_images[1], \n                       (pipe[\"x_lower\"], pipe[\"y_lower\"]))\n ...         image = pygame.surfarray.array3d( pygame.display.get_surface())\n ...         pygame.display.update()\n ...         fps_clock.tick(fps)\n ...         return image, reward, is_done\n```", "```py\n>>> import torch\n>>> import torch.nn as nn\n>>> import torch.nn.functional as F\n>>> import numpy as np\n>>> import random\n```", "```py\n>>> class DQNModel(nn.Module):\n ...     def __init__(self, n_action=2):\n ...         super(DQNModel, self).__init__()\n ...         self.conv1 = nn.Conv2d(4, 32, \n                             kernel_size=8, stride=4)\n ...         self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n ...         self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n ...         self.fc = nn.Linear(7 * 7 * 64, 512)\n ...         self.out = nn.Linear(512, n_action)\n ...         self._create_weights()\n ...\n ...     def _create_weights(self):\n ...         for m in self.modules():\n ...             if isinstance(m, nn.Conv2d) or \n                                 isinstance(m, nn.Linear):\n ...                 nn.init.uniform(m.weight, -0.01, 0.01)\n ...                 nn.init.constant_(m.bias, 0)\n ...\n ...     def forward(self, x):\n ...         x = F.relu(self.conv1(x))\n ...         x = F.relu(self.conv2(x))\n ...         x = F.relu(self.conv3(x))\n ...         x = x.view(x.size(0), -1)\n ...         x = F.relu(self.fc(x))\n ...         output = self.out(x)\n ...         return output\n```", "```py\n>>> class DQN():\n ...     def __init__(self, n_action, lr=1e-6):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = DQNModel(n_action)\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>>     def predict(self, s):\n ...         \"\"\"\n ...         Compute the Q values of the state for all \n                 actions using the learning model\n ...         @param s: input state\n ...         @return: Q values of the state for all actions\n ...         \"\"\"\n ...         return self.model(torch.Tensor(s))\n```", "```py\n>>>     def update(self, y_predict, y_target):\n ...         \"\"\"\n ...         Update the weights of the DQN given a training sample\n ...         @param y_predict:\n ...         @param y_target:\n ...         @return:\n ...         \"\"\"\n ...         loss = self.criterion(y_predict, y_target)\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n ...         return loss\n```", "```py\n>>>     def replay(self, memory, replay_size, gamma):\n ...         \"\"\"\n ...         Experience replay\n ...         @param memory: a list of experience\n ...         @param replay_size: the number of samples we \n                 use to update the model each time\n ...         @param gamma: the discount factor\n ...         @return: the loss\n ...         \"\"\"\n ...         if len(memory) >= replay_size:\n ...             replay_data = random.sample(memory, replay_size)\n ...             state_batch, action_batch, next_state_batch, \n                     reward_batch, done_batch = zip(*replay_data)\n ...             state_batch = torch.cat( tuple(state for state in state_batch))\n ...             next_state_batch = torch.cat(    \n                         tuple(state for state in next_state_batch))\n ...             q_values_batch = self.predict(state_batch)\n ...             q_values_next_batch = \n                         self.predict(next_state_batch)\n ...             reward_batch = torch.from_numpy(np.array( reward_batch, dtype=np.float32)[:, None])\n ...             action_batch = torch.from_numpy(\n ...                 np.array([[1, 0] if action == 0 else [0, 1] \n                     for action in action_batch], dtype=np.float32))\n ...             q_value = torch.sum( q_values_batch * action_batch, dim=1)\n ...             td_targets = torch.cat(\n ...             tuple(reward if terminal else reward + \n                         gamma * torch.max(prediction) for\n                         reward, terminal, prediction\n ...                 in zip(reward_batch, done_batch, \n                         q_values_next_batch)))\n ...             loss = self.update(q_value, td_targets)\n ...             return loss\n```", "```py\n>>> import random\n>>> import torch\n>>> from collections import deque\n```", "```py\n>>> def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n ...     def policy_function(state):\n ...         if random.random() < epsilon:\n ...             return random.randint(0, n_action - 1)\n ...         else:\n ...             q_values = estimator.predict(state)\n ...             return torch.argmax(q_values).item()\n ...     return policy_function\n```", "```py\n>>> image_size = 84\n >>> batch_size = 32\n >>> lr = 1e-6\n >>> gamma = 0.99\n >>> init_epsilon = 0.1\n >>> final_epsilon = 1e-4\n >>> n_iter = 2000000\n >>> memory_size = 50000\n >>> n_action = 2\n```", "```py\n>>> saved_path = 'trained_models'\n```", "```py\n>>> torch.manual_seed(123)\n```", "```py\n>>> estimator = DQN(n_action)\n```", "```py\n>>> memory = deque(maxlen=memory_size)\n```", "```py\n>>> env = FlappyBird()\n```", "```py\n>>> image, reward, is_done = env.next_step(0)\n```", "```py\n>>> import cv2\n >>> import numpy as np\n >>> def pre_processing(image, width, height):\n ...     image = cv2.cvtColor(cv2.resize(image, \n                     (width, height)), cv2.COLOR_BGR2GRAY)\n ...     _, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\n ...     return image[None, :, :].astype(np.float32)\n```", "```py\npip install opencv-python\n```", "```py\n>>> image = pre_processing(image[:screen_width, :int(env.base_y)], image_size, image_size)\n```", "```py\n>>> image = torch.from_numpy(image) >>> state = torch.cat(tuple(image for _ in range(4)))[None, :, :, :]\n```", "```py\n>>> for iter in range(n_iter):\n ...     epsilon = final_epsilon + (n_iter - iter) \n                 * (init_epsilon - final_epsilon) / n_iter\n ...     policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...     action = policy(state)\n ...     next_image, reward, is_done = env.next_step(action)\n ...     next_image = pre_processing(next_image[ :screen_width, :int(env.base_y)], image_size, image_size)\n ...     next_image = torch.from_numpy(next_image)\n ...     next_state = torch.cat(( state[0, 1:, :, :], next_image))[None, :, :, :]\n ...     memory.append([state, action, next_state, reward, is_done])\n ...     loss = estimator.replay(memory, batch_size, gamma)\n ...     state = next_state\n ...     print(\"Iteration: {}/{}, Action: {}, \n                 Loss: {}, Epsilon {}, Reward: {}\".format(\n ...             iter + 1, n_iter, action, loss, epsilon, reward))\n ...     if iter+1 % 10000 == 0:\n ...         torch.save(estimator.model, \"{}/{}\".format( saved_path, iter+1))\n```", "```py\nIteration: 1/2000000, Action: 0, Loss: None, Epsilon 0.1, Reward: 0.1 Iteration: 2/2000000, Action: 0, Loss: None, Epsilon 0.09999995005000001, Reward: 0.1\n Iteration: 3/2000000, Action: 0, Loss: None, Epsilon 0.0999999001, Reward: 0.1\n Iteration: 4/2000000, Action: 0, Loss: None, Epsilon 0.09999985015, Reward: 0.1\n ...\n ...\n Iteration: 201/2000000, Action: 1, Loss: 0.040504034608602524, Epsilon 0.09999001000000002, Reward: 0.1\n Iteration: 202/2000000, Action: 1, Loss: 0.010011588223278522, Epsilon 0.09998996005, Reward: 0.1\n Iteration: 203/2000000, Action: 1, Loss: 0.07097195833921432, Epsilon 0.09998991010000001, Reward: 0.1\n Iteration: 204/2000000, Action: 1, Loss: 0.040418840944767, Epsilon 0.09998986015000001, Reward: 0.1\n Iteration: 205/2000000, Action: 1, Loss: 0.00999421812593937, Epsilon 0.09998981020000001, Reward: 0.1\n```", "```py\n>>> torch.save(estimator.model, \"{}/final\".format(saved_path))\n```", "```py\n>>> model = torch.load(\"{}/final\".format(saved_path))\n```", "```py\n>>> n_episode = 100 >>> for episode in range(n_episode):\n ...     env = FlappyBird()\n ...     image, reward, is_done = env.next_step(0)\n ...     image = pre_processing(image[:screen_width, \n                :int(env.base_y)], image_size, image_size)\n ...     image = torch.from_numpy(image)\n ...     state = torch.cat(tuple(image for _ in range(4)))[ None, :, :, :]\n ...     while True:\n ...         prediction = model(state)[0]\n ...         action = torch.argmax(prediction).item()\n ...         next_image, reward, is_done = env.next_step(action)\n ...         if is_done:\n ...             break\n ...         next_image = pre_processing(next_image[:screen_width, :int(env.base_y)], image_size, image_size)\n ...         next_image = torch.from_numpy(next_image)\n ...         next_state = torch.cat((state[0, 1:, :, :], \n                           next_image))[None, :, :, :]\n ...         state = next_state\n```"]