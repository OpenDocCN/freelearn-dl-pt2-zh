- en: '*Chapter 7*: Text Representation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：文本表示'
- en: So far, we have addressed classification and generation problems with the `transformers`
    library. Text representation is another crucial task in modern **Natural Language
    Processing** (**NLP**), especially for unsupervised tasks such as clustering,
    semantic search, and topic modeling. Representing sentences by using various models
    such as **Universal Sentence Encoder** (**USE**) and Siamese BERT (Sentence-BERT)
    with additional libraries such as sentence transformers will be explained here.
    Zero-shot learning using BART will also be explained, and you will learn how to
    utilize it. Few-shot learning methodologies and unsupervised use cases such as
    semantic text clustering and topic modeling will also be described. Finally, one-shot
    learning use cases such as semantic search will be covered.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用`transformers`库解决了分类和生成问题。文本表示是现代**自然语言处理**（**NLP**）中的另一个关键任务，特别是对于无监督任务，如聚类、语义搜索和主题建模。通过使用诸如**Universal
    Sentence Encoder**（**USE**）和带有附加库（如句子转换器）的Siamese BERT（Sentence-BERT）等各种模型来表示句子将在此处进行解释。还将解释使用BART进行零样本学习，并学习如何利用它。还将描述少样本学习方法和无监督使用案例，如语义文本聚类和主题建模。最后，将涵盖一次性学习用例，如语义搜索。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to sentence embeddings
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子嵌入简介
- en: Benchmarking sentence similarity models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子相似度模型的基准测试
- en: Using BART for zero-shot learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BART进行零样本学习
- en: Semantic similarity experiment with FLAIR
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用FLAIR进行语义相似性实验
- en: Text clustering with Sentence-BERT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sentence-BERT进行文本聚类
- en: Semantic search with Sentence-BERT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sentence-BERT进行语义搜索
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using a Jupyter notebook to run our coding exercises. For this,
    you will need Python 3.6+ and the following packages:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Jupyter笔记本来运行我们的编程练习。为此，您将需要Python 3.6+和以下软件包：
- en: '`sklearn`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`transformers >=4.00`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers >=4.00`'
- en: '`datasets`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets`'
- en: '`sentence-transformers`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentence-transformers`'
- en: '`tensorflow-hub`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow-hub`'
- en: '`flair`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flair`'
- en: '`umap-learn`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`umap-learn`'
- en: '`bertopic`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bertopic`'
- en: 'All the notebooks for the coding exercises in this chapter will be available
    at the following GitHub link:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中编程练习的所有笔记本将在以下GitHub链接上提供：
- en: '[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH07](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH07)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH07](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH07)'
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/2VcMCyI](https://bit.ly/2VcMCyI)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看代码演示视频：[https://bit.ly/2VcMCyI](https://bit.ly/2VcMCyI)
- en: Introduction to sentence embeddings
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子嵌入简介
- en: Pre-trained BERT models do not produce efficient and independent sentence embeddings
    as they always need to be fine-tuned in an end-to-end supervised setting. This
    is because we can think of a pre-trained BERT model as an indivisible whole and
    semantics is spread across all layers, not just the final layer. Without fine-tuning,
    it may be ineffective to use its internal representations independently. It is
    also hard to handle unsupervised tasks such as clustering, topic modeling, information
    retrieval, or semantic search. Because we have to evaluate many sentence pairs
    during clustering tasks, for instance, this causes massive computational overhead.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的BERT模型不能产生高效且独立的句子嵌入，因为它们始终需要在端到端的监督设置中进行微调。这是因为我们可以将预训练的BERT模型视为一个不可分割的整体，语义分布在所有层中，而不仅仅是最后一层。如果不进行微调，单独使用其内部表示可能是无效的。而且，难以处理无监督任务，如聚类、主题建模、信息检索或语义搜索。例如，在聚类任务中，我们必须评估许多句子对，这导致了巨大的计算开销。
- en: Luckily, many modifications have been made to the original BERT model, such
    as **Sentence-BERT** (**SBERT**), to derive semantically meaningful and independent
    sentence embeddings. We will talk about these approaches in a moment. In the NLP
    literature, many neural sentence embedding methods have been proposed for mapping
    a single sentence to a common feature space (vector space model) wherein a cosine
    function (or dot product) is usually used to measure similarity and the Euclidean
    distance to measure dissimilarity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对原始BERT模型进行了许多修改，如**Sentence-BERT**（**SBERT**），以生成语义有意义且独立的句子嵌入。我们将在接下来讨论这些方法。在自然语言处理文献中，提出了许多神经句子嵌入方法，用于将单个句子映射到一个公共特征空间（向量空间模型），其中通常使用余弦函数（或点积）来衡量相似度，欧氏距离用于衡量不相似度。
- en: 'The following are some applications that can be efficiently solved with sentence
    embeddings:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以通过句子嵌入有效解决的应用程序：
- en: Sentence-pair tasks
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子对任务
- en: Information retrieval
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索
- en: Question answering
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Duplicate question detection
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复问题检测
- en: Paraphrase detection
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 释义检测
- en: Document clustering
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档聚类
- en: Topic modeling
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: The simplest but most efficient kind of neural sentence embedding is the average-pooling
    operation, which is performed on the embeddings of words in a sentence. To get
    a better representation of this, some early neural methods learned sentence embeddings
    in an unsupervised fashion, such as Doc2Vec, Skip-Thought, FastSent, and Sent2Vec.
    Doc2Vec utilized a token-level distributional theory and an objective function
    to predict adjacent words, similar to Word2Vec. The approach injects an additional
    memory token (called `transformers` library. This additional token acts as a piece
    of memory that represents the context or document embeddings. SkipThought and
    FastSent are considered sentence-level approaches, where the objective function
    is used to predict adjacent sentences. These models extract the sentence's meaning
    to obtain necessary information from the adjacent sentences and their context.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单但最有效的神经句子嵌入操作是对句子中单词的嵌入进行平均池化。为了更好地表示这一点，一些早期的神经方法以无监督方式学习句子嵌入，例如Doc2Vec、Skip-Thought、FastSent和Sent2Vec。Doc2Vec利用了令牌级的分布理论和一个目标函数来预测相邻的单词，类似于Word2Vec。该方法注入了一个额外的内存令牌（称为`transformers`库），这个额外的令牌充当了表示上下文或文档嵌入的一部分的内存。SkipThought和FastSent被认为是句子级方法，其中目标函数用于预测相邻的句子。这些模型从相邻的句子及其上下文中提取句子的含义，以获取必要的信息。
- en: Some other methods, such as InferSent, leveraged supervised learning and multi-task
    transfer learning to learn generic sentence embeddings. InferSent trained various
    supervised tasks to get more efficient embedding. RNN-based supervised models
    such as GRU or LSTM utilize the last hidden state (or stacked entire hidden states)
    to obtain sentence embeddings in a supervised setting. We touched on the RNN approach
    in [*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016), *From Bag-of-Words
    to the Transformers.*
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他方法，例如InferSent，利用监督学习和多任务迁移学习来学习通用句子嵌入。InferSent训练各种监督任务以获得更高效的嵌入。基于RNN的监督模型，如GRU或LSTM，利用最后的隐藏状态（或堆叠的整个隐藏状态）在监督设置中获取句子嵌入。我们在[*第1章*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)中涉及了RNN方法，*从词袋模型到Transformer*。
- en: Cross-encoder versus bi-encoder
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉编码器与双编码器
- en: So far, we have discussed how to train Transformer-based language models and
    fine-tune them in semi-supervised and supervised settings, respectively. As we
    learned in the previous chapters, we got successful results thanks to the transformer
    architectures. Once a task-specific thin linear layer has been put on top of a
    pre-trained model, all the weights of the network (not only the last task-specific
    thin layer) are fine-tuned with task-specific labeled data. We also experienced
    how the BERT architecture has been fine-tuned for two different groups of tasks
    (single-sentence or sentence-pair) without any architectural modifications being
    required. The only difference is that for the sentence-pair tasks, the sentences
    are concatenated and marked with a SEP token. Thus, self-attention is applied
    to all the tokens of the concatenated sentences. This is a big advantage of the
    BERT model, where both input sentences can get the necessary information from
    each other at every layer. In the end, they are encoded simultaneously. This is
    called cross-encoding.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何训练基于Transformer的语言模型，并在半监督和监督环境中对其进行微调。正如我们在前几章中所学到的，多亏了Transformer架构，我们取得了成功的结果。一旦在预训练模型的顶部放置了特定于任务的稀疏线性层，在特定于任务的标记数据上对网络的所有权重进行了微调（不仅仅是最后一个特定于任务的稀疏层），我们也经历了BERT架构如何在不需要任何架构修改的情况下，对两个不同组的任务（单句或句对）进行微调。唯一的不同之处在于，对于句对任务，句子被连接并用SEP标记标记。因此，自注意力应用于连接句子的所有标记。这是BERT模型的一个巨大优势，即输入句子可以在每一层从彼此那里获取必要的信息。最终，它们同时被编码。这被称为交叉编码。
- en: 'However, there are two disadvantages regarding the cross-encoders that were
    addressed by the SBERT authors and *Humeau et al., 2019*, as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关于交叉编码器存在两个缺点，SBERT作者和*Humeau等人，2019年*已经解决了这些缺点，具体如下：
- en: The cross-encoder setup is not convenient for many sentence-pair tasks due to
    too many possible combinations needing to be processed. For instance, to get the
    two closest sentences from a list of 1,000 sentences, the cross-encoder model
    (BERT) requires around 500,000 (*n * (n-1) /2*) inference computation. Therefore,
    it would be very slow compared to alternative solutions such as SBERT or USE.
    This is because these alternatives produce independent sentence embeddings wherein
    the similarity function (cosine similarity) or dissimilarity function (Euclidean
    or Manhattan) can easily be applied. Note that these dis/similarity functions
    can be performed efficiently on modern architectures. Moreover, with the help
    of an optimized index structure, we can reduce computational complexity from many
    hours to a few minutes when comparing or clustering many documents.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要处理太多可能的组合，交叉编码器设置对许多句子对任务并不方便。例如，要从1,000个句子的列表中获取两个最接近的句子，交叉编码器模型（BERT）需要大约500,000（*n*（n-1）/2）个推断计算。因此，与SBERT或USE等替代方案相比，速度会非常慢。这是因为这些替代方案产生独立的句子嵌入，其中相似性函数（余弦相似性）或不相似性函数（欧氏或曼哈顿距离）可以轻松应用。请注意，这些相似性/不相似性函数可以在现代架构上有效执行。此外，借助优化的索引结构，我们可以将比较或聚类许多文档的计算复杂度从许多小时减少到几分钟。
- en: Due to its supervised characteristics, the BERT model can't derive independent
    meaningful sentence embeddings. It is hard to leverage a pre-trained BERT model
    as is for unsupervised tasks such as clustering, semantic search, or topic modeling.
    The BERT model produces a fixed-size vector for each token in a document. In an
    unsupervised setting, the document-level representation may be obtained by averaging
    or pooling token vectors, plus SEP and CLS tokens. Later, we will see that such
    a representation of BERT produces below-average sentence embeddings, and that
    its performance scores are usually worse than word embedding pooling techniques
    such as Word2Vec, FastText, or GloVe.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其监督特性，BERT模型无法推导出独立的有意义的句子嵌入。很难将预训练的BERT模型直接用于无监督任务，例如聚类、语义搜索或主题建模。BERT模型为文档中的每个标记生成固定大小的向量。在无监督设置中，可以通过对标记向量进行平均或汇总，再加上SEP和CLS标记，来获得文档级表示。稍后，我们将看到BERT的这种表示产生的句子嵌入低于平均水平，并且其性能分数通常比Word2Vec、FastText或GloVe等词嵌入汇聚技术差。
- en: Alternatively, bi-encoders (such as SBERT) independently map a sentence pair
    to a semantic vector space, as shown in the following diagram. Since the representations
    are separate, bi-encoders can cache the encoded input representation for each
    input, resulting in fast inference time. One of the successful bi-encoder modifications
    of BERT is SBERT. Based on the Siamese and Triplet network structures, SBERT fine-tunes
    the BERT model to produce semantically meaningful and independent embeddings of
    the sentences.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，双编码器（如SBERT）将一个句子对独立映射到语义向量空间，如下图所示。由于表示是分开的，双编码器可以为每个输入缓存编码的输入表示，从而实现快速推断时间。
    BERT的成功双编码器修改之一是SBERT。基于孪生网络和三重网络结构，SBERT微调BERT模型以产生语义上有意义且独立的句子嵌入。
- en: 'The following diagram shows the bi-encoder architecture:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了双编码器架构：
- en: '![Figure 7.1 – Bi-encoder architecture ](img/B17123_07_001.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 - 双编码器架构](img/B17123_07_001.jpg)'
- en: Figure 7.1 – Bi-encoder architecture
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 - 双编码器架构
- en: You can find hundreds of pre-trained SBERT models that have been trained with
    different objectives at [https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/)找到数百个已经通过不同目标训练的预训练SBERT模型。
- en: We will use some of them in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中使用其中一些。
- en: Benchmarking sentence similarity models
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对句子相似度模型进行基准测试。
- en: There are many *semantic textual similarity* models available, but it is highly
    recommended that you benchmark and understand their capabilities and differences
    using metrics. *Papers With Code* provides a list of these datasets at [https://paperswithcode.com/task/semantic-textual-similarity](https://paperswithcode.com/task/semantic-textual-similarity).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多*语义文本相似性*模型可用，但强烈建议您使用度量标准对它们的能力和差异进行基准测试并加以理解。*Papers With Code*提供了这些数据集的列表，网址为[https://paperswithcode.com/task/semantic-textual-similarity](https://paperswithcode.com/task/semantic-textual-similarity)。
- en: Also, there are many model outputs in each dataset that are ranked by their
    results. These results have been taken from the aforementioned article.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在每个数据集中有许多模型输出，这些输出是按其结果排名的。这些结果是从上述文章中获取的。
- en: 'GLUE provides most of these datasets and tests, but it is not only for semantic
    textual similarity. **GLUE**, which stands for **General Language Understanding
    Evaluation**, is a general benchmark for evaluating a model with different NLP
    characteristics. More details about the GLUE dataset and its usage was provided
    in [*Chapter 2*](B17123_02_Epub_AM.xhtml#_idTextAnchor034), *A Hands-On Introduction
    to the Subject*. Let''s take a look at it before we move on:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE提供了大多数这些数据集和测试，但它不仅适用于语义文本相似性。**GLUE**，代表**General Language Understanding
    Evaluation**，是一个用于评估具有不同NLP特性的模型的通用基准。有关GLUE数据集及其用法的更多详细信息，请参阅[*第2章*](B17123_02_Epub_AM.xhtml#_idTextAnchor034)，*主题的实际介绍*。让我们在继续之前先看看它：
- en: 'To load the metrics and MRPC dataset from the GLUE benchmark, you can use the
    following code:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从GLUE基准加载度量标准和MRPC数据集，您可以使用以下代码：
- en: '[PRE0]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The samples in this dataset are labeled `1` and `0`, which indicates whether
    they are similar or dissimilar, respectively. You can use any model, regardless
    of the architecture, to produce values for two given sentences. In other words,
    the model should classify the two sentences as zeros and ones.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此数据集中的样本标记为`1`和`0`，这表示它们是否相似或不相似。您可以使用任何模型，无论其体系结构如何，来为给定的两个句子生成值。换句话说，模型应将这两个句子分类为零和一。
- en: 'Let''s assume the model produces values and that these values are stored in
    an array called `predictions`. You can easily use this metric with the predictions
    to see the F1 and accuracy values:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设模型产生值，并且这些值存储在一个名为`predictions`的数组中。您可以轻松地使用这个度量标准与预测一起查看F1和准确度值：
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Some semantic textual similarity datasets such as **Semantic Textual Similarity
    Benchmark** (**STSB**) have different metrics. For example, this benchmark uses
    Spearman and Pearson correlations because the outputs and predictions are between
    0 and 5 and are float numbers instead of being 0s and 1s, which is a regression
    problem. The following code shows an example of this benchmark:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些语义文本相似性数据集，如**Semantic Textual Similarity Benchmark** (**STSB**)，具有不同的度量标准。例如，此基准使用Spearman和Pearson相关性，因为输出和预测值在0到5之间，并且是浮点数，而不是在0和1之间，这是一个回归问题。以下代码显示了此基准的一个示例：
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The predictions and references are the same as the ones from the **Microsoft
    Research Paraphrase Corpus** (**MRPC**); the predictions are the model outputs,
    while the references are the dataset labels.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测和参考与**Microsoft Research Paraphrase Corpus** (**MRPC**)中的相同；预测是模型的输出，而参考是数据集的标签。
- en: 'To get a comparative result between two models, we will use a distilled version
    of Roberta and test these two on the STSB. To start, you must load both models.
    The following code shows how to install the required libraries before loading
    and using models:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了得到两个模型之间的比较结果，我们将使用RoBERTa的精简版本，并在STSB上测试这两个模型。首先，您必须加载两个模型。以下代码显示了如何在加载和使用模型之前安装所需的库：
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we mentioned previously, the next step is to load the dataset and metric:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，下一步是加载数据集和度量标准：
- en: '[PRE4]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Afterward, we must load both models:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须加载两个模型：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Both of these models provide embeddings for a given sentence. To compare the
    similarity between two sentences, we will use cosine similarity. The following
    function takes sentences as a batch and provides cosine similarity for each pair
    by utilizing USE:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个模型都为给定的句子提供嵌入。为了比较两个句子之间的相似性，我们将使用余弦相似度。以下函数以批量形式接受句子，并利用USE为每对句子提供余弦相似度：
- en: '[PRE6]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With small modifications, the same function can be used for RoBERTa too. These
    small modifications are only for replacing the embedding function, which is different
    for TensorFlow Hub models and transformers. The following is the modified function:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过小幅修改，相同的函数也可以用于RoBERTa。这些小修改仅用于替换嵌入函数，该函数对于TensorFlow Hub模型和transformers是不同的。以下是修改后的函数：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Applying these functions to the dataset will result in similarity scores for
    each of the models:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些函数应用于数据集将为每个模型生成相似性得分：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using metrics on both results produces the Spearman and Pearson correlation
    values:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对两个结果使用度量标准会产生Spearman和Pearson相关值：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can simply use pandas to see the results as a table in a comparative fashion:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以简单地使用pandas以对比的方式查看结果：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.2 – STSB validation results on DistilRoberta and USE ](img/B17123_07_002.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – DistilRoberta和USE上的STSB验证结果](img/B17123_07_002.jpg)'
- en: Figure 7.2 – STSB validation results on DistilRoberta and USE
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – DistilRoberta和USE上的STSB验证结果
- en: In this section, you learned about the important benchmarks of semantic textual
    similarity. Regardless of the model, you learned how to use any of these metrics
    to quantify model performance. In the next section, you will learn about the few-shot
    learning models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了语义文本相似性的重要基准。无论模型如何，你学会了如何使用这些度量标准之一来量化模型表现。在接下来的部分，你将了解几种少样本学习模型。
- en: Using BART for zero-shot learning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用BART进行零样本学习
- en: In the field of machine learning, zero-shot learning is referred to as models
    that can perform a task without explicitly being trained on it. In the case of
    NLP, it's assumed that there's a model that can predict the probability of some
    text being assigned to classes that are given to the model. However, the interesting
    part about this type of learning is that the model is not trained on these classes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，零样本学习指的是可以执行任务而无需明确训练的模型。在NLP的情况下，假设有一个模型可以预测一些文本被分配给模型给出的类别的概率。然而，这种学习方式的有趣之处在于模型并没有接受这些类别的训练。
- en: With the rise of many advanced language models that can perform transfer learning,
    zero-shot learning came to life. In the case of NLP, this kind of learning is
    performed by NLP models at test time, where the model sees samples belonging to
    new classes where no samples of them were seen before.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随着许多可以进行迁移学习的高级语言模型的崛起，零样本学习应运而生。在NLP的情况下，这种学习是由NLP模型在测试时执行的，模型在那里看到属于新类别的样本，以前没有看到过这些样本。
- en: This kind of learning is usually used for classification tasks, where both the
    classes and the text are represented and the semantic similarity of both is compared.
    The represented form of these two is an embedding vector, while the similarity
    metric (such as cosine similarity or a pre-trained classifier such as a dense
    layer) outputs the probability of the sentence/text being classified as the class.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习通常用于分类任务，其中类别和文本都被表示，比较两者的语义相似性。这两者的表现形式是嵌入向量，而相似度度量（如余弦相似度或一个预训练分类器如一个稠密层）输出句子/文本被分类为类别的概率。
- en: There are many methods and schemes we can use to train such models, but one
    of the earliest methods used crawled pages from the internet containing keyword
    tags in the meta part. For more information, read the following article and blog
    post at [https://amitness.com/2020/05/zero-shot-text-classification/](https://amitness.com/2020/05/zero-shot-text-classification/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多方法和方案来训练这些模型，但最早使用的方法之一是从包含在元部分中的关键字标签的互联网页面中提取。欲了解更多信息，请阅读以下文章和博文：[https://amitness.com/2020/05/zero-shot-text-classification/](https://amitness.com/2020/05/zero-shot-text-classification/)。
- en: Instead of using such huge data, there are language models such as BART that
    use the **Multi-Genre Natural Language Inference** (**MNLI**) dataset to fine-tune
    and detect the relationship between two different sentences. Also, the HuggingFace
    model repository contains many models that have been implemented for zero-shot
    learning. They also provide a zero-shot learning pipeline for ease of use.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用如此庞大的数据，有像BART这样使用**多样式自然语言推理**（MNLI）数据集对其进行微调并检测两个不同句子之间关系的语言模型。此外，HuggingFace模型存储库包含许多为零样本学习实现的模型。他们还提供了一个零样本学习流水线，以方便使用。
- en: 'For example, BART from **Facebook AI Research** (**FAIR**) is being used in
    the following code to perform zero-shot text classification:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，来自**Facebook AI Research（FAIR）**的BART正在以下代码中用于执行零样本文本分类：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The results are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 7.3 – Results of zero-shot learning using BART ](img/B17123_07_003.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 使用BART进行零样本学习的结果](img/B17123_07_003.jpg)'
- en: Figure 7.3 – Results of zero-shot learning using BART
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 使用BART进行零样本学习的结果
- en: As you can see, the travel and exploration labels have the highest probability,
    but the most probable one is travel.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，旅行和探索标签的概率最高，但最可能的是旅行。
- en: 'However, sometimes, one sample can belong to more than one class (multilabel).
    HuggingFace provides a parameter called `multi_label` for this. The following
    example uses this parameter:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时一个样本可能属于多个类别（多标签）。HuggingFace为此提供了一个名为`multi_label`的参数。以下示例使用了这个参数：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Due to this, it is changed to the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它更改为以下内容：
- en: '![Figure 7.4 – Results of zero-shot learning using BART (multi_label = True)
    ](img/B17123_07_004.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 使用BART进行零样本学习的结果（multi_label = True）](img/B17123_07_004.jpg)'
- en: Figure 7.4 – Results of zero-shot learning using BART (multi_label = True)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 使用BART进行零样本学习的结果（multi_label = True）
- en: You can test the results even further and see how the model performs if very
    similar labels to the travel one are used. For example, you can see how it performs
    if `moving` and `going` are added to the label list.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以进一步测试结果，看看模型在使用与旅行标签非常相似的标签时的表现。例如，您可以看看在标签列表中添加`moving`和`going`后，模型的表现如何。
- en: There are other models that also leverage the semantic similarity between labels
    and the context to perform zero-shot classification. In the case of few-shot learning,
    some samples are given to the model, but these samples are not enough to train
    a model alone. Models can use these samples to perform tasks such as semantic
    text clustering, which will be explained shortly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他模型也利用标签和上下文之间的语义相似性来进行零样本分类。在少样本学习的情况下，模型会给出一些样本，但这些样本不足以单独训练一个模型。模型可以利用这些样本来执行诸如语义文本聚类之类的任务，稍后将会解释。
- en: 'Now that you''ve learned how to use BART for zero-shot learning, you should
    learn how it works. BART is fine-tuned on `travel`, for example) and the second
    sentence to the content (`one day I will see the world`, for example). According
    to this, if these two can come after each other, this means that the label and
    the content are semantically related. The following code example shows how to
    directly use the BART model without the zero-shot classification pipeline according
    to the preceding descriptions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何使用BART进行零样本学习，您应该了解它的工作原理。例如，BART是在`旅行`上进行了微调，并将第二个句子的内容（例如`有一天我会看到世界`）。根据这一点，如果这两个句子可以相互接续，那么这意味着标签和内容在语义上相关。以下的代码示例展示了如何直接使用BART模型，而不需要零样本分类的流程以前的描述：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The result is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also call the first sentence the hypothesis and the sentence containing
    the label the premise. According to the result, the premise can entail the hypothesis.
    which means that the hypothesis is labeled as the premise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以将第一个句子称为假设，将包含标签的句子称为前提。根据结果，前提可以蕴涵假设，这意味着假设被标签为前提。
- en: So far, you've learned how to use zero-shot learning by utilizing NLI fine-tuned
    models. Next, you will learn how to perform few-/one-shot learning using semantic
    text clustering and semantic search.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何使用基于NLI微调模型的零样本学习。接下来，您将学习如何利用语义文本聚类和语义搜索来进行少样本/一样本学习。
- en: Semantic similarity experiment with FLAIR
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FLAIR进行语义相似性实验
- en: In this experiment, we will qualitatively evaluate the sentence representation
    models thanks to the `flair` library, which really simplifies obtaining the document
    embeddings for us.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将通过`flair`库对句子表示模型进行定性评估，这对我们来说真的简化了获取文档嵌入的过程。
- en: 'We will perform experiments while taking on the following approaches:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下方法进行实验：
- en: Document average pool embeddings
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档平均池嵌入
- en: RNN-based embeddings
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RNN的嵌入
- en: BERT embeddings
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT嵌入
- en: SBERT embeddings
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBERT嵌入
- en: 'We need to install these libraries before we can start the experiments:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实验之前，我们需要安装这些库：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For qualitative evaluation, we define a list of similar sentence pairs and a
    list of dissimilar sentence pairs (five pairs for each). What we expect from the
    embeddings models is that they should measure a high score and a low score, respectively.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定性评估，我们定义了一组相似的句子对和一组不相似的句子对（每组各五对）。我们期望嵌入模型应该分别度量高分和低分。
- en: The sentence pairs are extracted from the SBS Benchmark dataset, which we are
    already familiar with from the sentence-pair regression part of [*Chapter 6*](B17123_06_Epub_AM.xhtml#_idTextAnchor090),
    *Fine-Tuning Language Models for Token Classification*. For similar pairs, two
    sentences are completely equivalent, and they share the same meaning.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 句子对是从SBS Benchmark数据集中提取的，我们在[*第6章*](B17123_06_Epub_AM.xhtml#_idTextAnchor090)中已经熟悉了句对回归部分，*用于令牌分类的语言模型微调*。对于相似的句对，两个句子完全等价，并且它们分享相同的含义。
- en: 'The pairs with a similarity score of around 5 in the STSB dataset are randomly
    taken, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在STSB数据集中，随机抽取了相似分数约为5的对，如下所示：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.5 – Similar pair list ](img/B17123_07_005.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 相似对列表](img/B17123_07_005.jpg)'
- en: Figure 7.5 – Similar pair list
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 相似对列表
- en: 'Here is the list of dissimilar sentences whose similarity scores are around
    0, taken from the STS-B dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自 STS-B 数据集的相似度得分约为 0 的不相似句子列表：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.6 – Dissimilar pair list ](img/B17123_07_006.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 不相似对列表](img/B17123_07_006.jpg)'
- en: Figure 7.6 – Dissimilar pair list
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 不相似对列表
- en: 'Now, let''s prepare the necessary functions to evaluate the embeddings models.
    The following `sim()` function computes the cosine similarity between two sentences;
    that is, `s1`, `s2`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备好评估嵌入模型所需的函数。以下`sim()`函数计算两个句子之间的余弦相似度；即，`s1`，`s2`：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The document embeddings models that were used in this experiment are all pre-trained
    models. We will pass the document embeddings model object and sentence pair list
    (similar or dissimilar) to the following `evaluate()` function, where, once the
    model encodes the sentence embeddings, it will compute the similarity score for
    each pair in the list, along with the list average. The definition of the function
    is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中使用的文档嵌入模型都是预训练模型。我们将文档嵌入模型对象和句子对列表（相似或不相似）传递给以下`evaluate()`函数，一旦模型编码了句子嵌入，它将计算列表中每对的相似度得分，以及列表的平均值。函数的定义如下：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, it is time to evaluate sentence embedding models. We will start with the
    average pooling method!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候评估句子嵌入模型了。我们将从平均池化方法开始！
- en: Average word embeddings
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均词嵌入
- en: 'Average word embeddings (or **document pooling**) apply the mean pooling operation
    to all the words in a sentence, where the average of all the word embeddings is
    considered to be sentence embedding. The following execution instantiates a document
    pool embedding based on GloVe vectors. Note that although we will use only GloVe
    vectors here, the flair API allows us to use multiple word embeddings. Here is
    the code definition:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 平均词嵌入（或**文档池化**）对句子中的所有单词应用均值池化操作，其中所有单词嵌入的平均值被视为句子嵌入。以下执行实例化了基于 GloVe 向量的文档池嵌入。请注意，虽然我们这里只使用了
    GloVe 向量，但 flair API 允许我们使用多个单词嵌入。以下是代码定义：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s evaluate the GloVe pool model on similar pairs, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估 GloVe 池模型的相似对，如下所示：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The results seem to be good since those resulting values are very high, which
    is what we expect. However, the model produces high scores such as 0.94 on average
    for the dissimilar list as well. Our expectation would be less than 0.4\. We''ll
    talk about why we got this later in this chapter. Here is the execution:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果似乎很好，因为那些得到的值非常高，这是我们期望的。然而，模型也产生了高得分，例如对于不相似列表的平均得分为0.94。我们的期望值应该小于0.4。我们稍后在本章将讨论为什么会得到这个值。以下是执行情况：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, let's evaluate some RNN embeddings on the same problem.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们对同一问题评估一些 RNN 嵌入。
- en: RNN-based document embeddings
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 RNN 的文档嵌入
- en: 'Let''s instantiate a GRU model based on GloVe embeddings, where the default
    model of `DocumentRNNEmbeddings` is a GRU:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于 GloVe 嵌入实例化一个 GRU 模型，默认的`DocumentRNNEmbeddings`模型是 GRU：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Run the evaluation method:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 运行评估方法：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Likewise, we get a high score for the dissimilar list. This is not what we want
    from sentence embeddings.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们得到了不相似列表的高分。这并不是我们从句子嵌入中想要的。
- en: Transformer-based BERT embeddings
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 Transformer 的 BERT 嵌入
- en: 'The following execution instantiates a `bert-base-uncased` model that pools
    the final layer:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下执行实例化了一个池化最终层的`bert-base-uncased`模型：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run the evaluation, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 运行评估，如下所示：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is worse! The score of the dissimilar list is higher than that of the similar
    list.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这更糟糕！不相似列表的得分高于相似列表的得分。
- en: Sentence-BERT embeddings
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子-BERT 嵌入
- en: 'Now, let''s apply Sentence-BERT to the problem of distinguishing similar pairs
    from dissimilar ones, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 Sentence-BERT 应用于区分相似对和不相似对的问题，如下所示：
- en: 'First of all, a warning: we need to ensure that the `sentence-transformers`
    package has already been installed:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，警告：我们需要确保`sentence-transformers`包已经被安装：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As we mentioned previously, Sentence-BERT provides a variety of pre-trained
    models. We will pick the `bert-base-nli-mean-tokens` model for evaluation. Here
    is the code:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Sentence-BERT 提供了各种预训练模型。我们将选择`bert-base-nli-mean-tokens`模型进行评估。以下是代码：
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s evaluate the model:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估模型：
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Well done! The SBERT model produced better results. The model produced a low
    similarity score for the dissimilar list, which is what we expect.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 干得好！SBERT 模型产生了更好的结果。模型为不相似列表产生了较低的相似度得分，这是我们所期望的。
- en: 'Now, we will do a harder test, where we pass contradicting sentences to the
    models. We will define some tricky sentence pairs, as follows:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将进行一个更难的测试，我们将向模型传递相互矛盾的句子。我们将定义一些棘手的句子对，如下所示：
- en: '[PRE30]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Interesting! The scores are very high since the sentence similarity model works
    similar to topic detection and measures content similarity. When we look at the
    sentences, they share the same content, even though they contradict each other.
    The content is about lion and elephant or cat and mat. Therefore, the models produce
    a high similarity score. Since the GloVe embedding method pools the average of
    the words without caring about word order, it measures two sentences as being
    the same. On the other hand, the GRU model produced lower values as it cares about
    word order. Surprisingly, even the SBERT model does not produce efficient scores.
    This may be due to the content similarity-based supervision that's used in the
    SBERT model.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有趣！分数非常高，因为句子相似性模型类似于主题检测，并且衡量内容相似性。当我们看句子时，它们分享相同的内容，尽管它们彼此矛盾。内容是关于狮子和大象或猫和垫子。因此，模型产生了高相似度分数。由于
    GloVe 嵌入方法池化了单词的平均值而不关心单词顺序，它将两个句子视为相同。另一方面，GRU 模型产生了较低的值，因为它关心单词顺序。令人惊讶的是，即使是
    SBERT 模型也没有产生有效的分数。这可能是由于 SBERT 模型中使用的基于内容相似性的监督。
- en: 'To correctly detect the semantics of two sentence pairs with three classes
    – that is, Neutral, Contradiction, and Entailment – we must use a fine-tuned model
    on MNLI. The following code block shows an example of using XLM-Roberta, fine-tuned
    on XNLI with the same examples:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要正确检测具有三个类别（即中性、矛盾和蕴含）的两个句子对的语义，我们必须在 MNLI 上使用一个经过微调的模型。以下代码块显示了一个示例，使用了在 XNLI
    上进行了微调的 XLM-Roberta，使用相同的例子：
- en: '[PRE31]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output will show the correct labels for each:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将显示每个正确的标签：
- en: '[PRE32]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In some problems, In some problems, NLI is a higher priority than semantic textual
    because it is intended to find the contradiction or entailment rather than the
    raw similarity score. For the next sample, use two sentences for entailment and
    contradiction at the same time. This is a bit subjective, but to the model, the
    second sentence pair seems to be a very close call between entailment and contradiction.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些问题中，NLI 比语义文本更优先，因为它旨在找到矛盾或蕴含，而不是原始的相似度得分。对于下一个样本，同时使用两个句子进行蕴含和矛盾。这有点主观，但对于模型来说，第二个句子对似乎是在蕴含和矛盾之间的非常微妙的选择。
- en: Text clustering with Sentence-BERT
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Sentence-BERT 进行文本聚类
- en: 'For clustering algorithms, we will need a model that''s suitable for textual
    similarity. Let''s use the `paraphrase-distilroberta-base-v1` model here for a
    change. We will start by loading the Amazon Polarity dataset for our clustering
    experiment. This dataset includes Amazon web page reviews spanning a period of
    18 years up to March 2013\. The original dataset includes over 35 million reviews.
    These reviews include product information, user information, user ratings, and
    user reviews. Let''s get started:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聚类算法，我们将需要一个适用于文本相似度的模型。让我们在这里使用`paraphrase-distilroberta-base-v1`模型进行变化。我们将首先加载亚马逊极性数据集进行我们的聚类实验。该数据集包括了从
    18 年前到 2013 年 3 月期间的亚马逊网页评论。原始数据集包含超过 3500 万条评论。这些评论包括产品信息、用户信息、用户评分和用户评论。让我们开始吧：
- en: 'First, randomly select 10K reviews by shuffling, as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过洗牌随机选择 10K 条评论，如下所示：
- en: '[PRE33]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The corpus is now ready for clustering. The following code instantiates a sentence-transformer
    object using the pre-trained `paraphrase-distilroberta-base-v1` model:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在语料库已准备好进行聚类。以下代码示例实例化了一个使用预训练的`paraphrase-distilroberta-base-v1`模型的句子转换器对象：
- en: '[PRE34]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The entire corpus is encoded with the following execution, where the model
    maps a list of sentences to a list of embedding vectors:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下执行对整个语料库进行编码，其中模型将一系列句子映射到一系列嵌入向量：
- en: '[PRE35]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here, the vector size is `768`, which is the default embedding size of the
    BERT-base model. From now on, we will proceed with traditional clustering methods.
    We will choose *Kmeans* here since it is a fast and widely used clustering algorithm.
    We just need to set the cluster number (*K*) to `5`. Actually, this number may
    not be optimal. There are many techniques that can determine the optimal number
    of clusters, such as the Elbow or Silhouette method. However, let''s leave these
    issues aside. Here is the execution:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，向量大小为`768`，这是 BERT-base 模型的默认嵌入大小。从现在开始，我们将继续使用传统的聚类方法。我们选择*Kmeans*，因为它是一种快速且广泛使用的聚类算法。我们只需要将聚类数目（*K*）设置为`5`。实际上，这个数目可能不是最优的。有许多技术可以确定最优的聚类数目，比如肘部法或轮廓法。然而，让我们把这些问题放在一边。以下是执行过程：
- en: '[PRE36]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we have obtained five clusters of reviews. As we can see from the output,
    we have fairly distributed clusters. Another issue with clustering is that we
    need to understand what these clusters mean. As a suggestion, we can apply topic
    analysis to each cluster or check cluster-based TF-IDF to understand the content.
    Now, let's look at another way to do this based on the cluster centers. The Kmeans
    algorithm computes cluster centers, called centroids, that are kept in the `kmeans.cluster_centers_`
    attribute. The centroids are simply the average of the vectors in each cluster.
    Therefore, they are all imaginary points, not the existing data points. Let's
    assume that the sentences closest to the centroid will be the most representative
    example for the corresponding cluster.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们得到了五个评论的聚类。从输出中我们可以看到，我们有相当均匀的聚类。聚类的另一个问题是我们需要了解这些聚类意味着什么。作为一个建议，我们可以对每个聚类应用主题分析或检查基于聚类的
    TF-IDF 来了解内容。现在，让我们基于聚类中心来看另一种做法。Kmeans 算法计算聚类中心，称为质心，它们保存在`kmeans.cluster_centers_`属性中。质心只是每个聚类中向量的平均值。因此，它们都是虚拟点，不是现有的数据点。让我们假设距离质心最近的句子将是相应聚类的最具代表性的例子。
- en: 'Let''s try to find only one real sentence embedding, closest to each centroid
    point. If you like, you can capture more than one sentence. Here is the code:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试找到每个中心点最接近的一个真实句子嵌入。如果你愿意，你可以捕捉多于一个句子。以下是代码：
- en: '[PRE37]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.7 – Centroids of the cluster ](img/B17123_07_007.jpg)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.7 – 聚类中心](img/B17123_07_007.jpg)'
- en: Figure 7.7 – Centroids of the cluster
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.7 – 聚类中心
- en: 'From these representative sentences, we can reason about the clusters. It seems
    to be that Kmeans clusters the reviews into five distinct categories: *Electronics*,
    *Audio Cd/Music*, *DVD Film*, *Books*, and *Furniture & Home*. Now, let''s visualize
    both sentence points and cluster centroids in a 2D space. We will use the **Uniform
    Manifold Approximation and Projection** (**UMAP**) library to reduce dimensionality.
    Other widely used dimensionality reduction techniques in NLP that you can use
    include t-SNE and PCA (see [*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016),
    *From Bag-of-Words to the Transformers*).'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这些代表性句子，我们可以推断出聚类。看起来 Kmeans 将评论聚类成了五个不同的类别：*电子产品*、*音频 CD/音乐*、*DVD 电影*、*书籍*
    和 *家具与家居*。现在，让我们在 2D 空间中可视化句子点和聚类中心。我们将使用**均匀流形近似和投影**（**UMAP**）库来降低维度。你还可以使用其他广泛使用的
    NLP 降维技术，如 t-SNE 和 PCA（参见[*第 1 章*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)，*从词袋模型到变压器*）。
- en: 'We need to install the `umap` library, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装`umap`库，如下所示：
- en: '[PRE38]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following execution reduces all the embeddings and maps them into a 2D
    space:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下执行将所有嵌入都降维并映射到 2D 空间：
- en: '[PRE39]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.8 – Cluster points visualization ](img/B17123_07_008.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 聚类点可视化](img/B17123_07_008.jpg)'
- en: Figure 7.8 – Cluster points visualization
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 聚类点可视化
- en: In the preceding output, the points have been colored according to their cluster
    membership and centroids. It looks like we have picked the right number of clusters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，点根据其聚类成员资格和质心的颜色进行了着色。看起来我们选择了正确的聚类数目。
- en: To capture the topics and interpret the clusters, we simply located the sentences
    (one single sentence for each cluster) close to the centroids of the clusters.
    Now, let's look at a more accurate way of capturing the topic with topic modeling.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉主题并解释聚类，我们简单地将句子（每个聚类一个句子）定位在聚类的中心附近。现在，让我们用主题建模的更准确方法来捕捉主题。
- en: Topic modeling with BERTopic
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BERTopic 进行主题建模
- en: You may be familiar with many unsupervised topic modeling techniques that are
    used to extract topics from documents; **Latent-Dirichlet Allocation** (**LDA**)
    topic modeling and **Non-Negative Matrix Factorization** (**NMF**) are well-applied
    traditional techniques in the literature. BERTopic and Top2Vec are two important
    transformer-based topic modeling projects. In this section, we will apply the
    BERTopic model to our Amazon corpus. It leverages BERT embeddings and the class-based
    TF-IDF method to get easily interpretable topics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能熟悉许多用于从文档中提取主题的无监督主题建模技术；**潜在狄利克雷分配** (**LDA**) 主题建模和**非负矩阵分解** (**NMF**)
    是文献中广泛应用的传统技术。BERTopic和Top2Vec是两个重要的基于转换器的主题建模项目。在本节中，我们将把BERTopic模型应用到我们的亚马逊语料库中。它利用了BERT嵌入和基于类的TF-IDF方法，以获取易于解释的主题。
- en: 'First, the BERTopic model starts by encoding the sentences with sentence transformers
    or any sentence embedding model, which is followed by the clustering step. The
    clustering step has two phases: the embedding''s dimensionality is reduced by
    **UMAP** and then the reduced vectors are clustered by **Hierarchical Density-Based
    Spatial Clustering of Applications with Noise** (**HDBSCAN**), which yields groups
    of similar documents. At the final stage, the topics are captured by cluster-wise
    TF-IDF, where the model extracts the most important words per cluster rather than
    per document and obtains descriptions of the topics for each cluster. Let''s get
    started:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，BERTopic模型通过使用句子转换器或任何句子嵌入模型对句子进行编码，然后进行聚类步骤。聚类步骤分为两个阶段：首先通过**UMAP**减少了嵌入的维度，然后通过**Hierarchical
    Density-Based Spatial Clustering of Applications with Noise** (**HDBSCAN**)对减少的向量进行了聚类，从而得到了相似文档的分组。在最后阶段，通过每个聚类的TF-IDF来捕获主题，模型提取了每个聚类的最重要单词，而不是每个文档，并为每个聚类获取主题的描述。让我们开始吧：
- en: 'First, let''s install the necessary library, as follows:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装必要的库，如下所示：
- en: '[PRE40]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Important note
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: You may need to restart the runtime since this installation will update some
    packages that have already been loaded. So, from the Jupyter notebook, go to **Runtime**
    | **Restart Runtime**.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能需要重新启动运行时，因为此安装将更新一些已加载的包。因此，从Jupyter笔记本中，转到**运行时** | **重新启动运行时**。
- en: 'If you want to use your own embedding model, you need to instantiate and pass
    it through the BERTopic model. We will instantiate a Sentence Transformer model
    and pass it to the constructor of BERTopic, as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想使用自己的嵌入模型，您需要实例化并将其传递给BERTopic模型。我们将实例化一个句子转换器模型，并将其传递给BERTopic的构造函数，如下所示：
- en: '[PRE41]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.9 – BERTopic results ](img/B17123_07_009.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – BERTopic结果](img/B17123_07_009.jpg)'
- en: Figure 7.9 – BERTopic results
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – BERTopic结果
- en: 'Please note that different BERTopic runs with the same parameters can yield
    different results since the UMAP model is stochastic. Now, let''s see the word
    distribution of topic five, as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用相同参数的不同BERTopic运行可能会产生不同的结果，因为UMAP模型是随机的。现在，让我们看看第五个主题的词分布，如下所示：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.10 – The fifth topic words of the topic model ](img/B17123_07_010.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 主题模型的第五个主题词](img/B17123_07_010.jpg)'
- en: Figure 7.10 – The fifth topic words of the topic model
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 主题模型的第五个主题词
- en: The topic words are those words whose vectors are close to the topic vector
    in the semantic space. In this experiment, we did not cluster the corpus; instead,
    we applied the technique to the entire corpus. In our previous example, we analyzed
    the clusters with the closest sentence. Now, we can find the topics by applying
    the topic model separately to each cluster. This is pretty straightforward, and
    you can run it yourself.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 主题词是那些在语义空间中与主题向量接近的词语。在这个实验中，我们没有对语料库进行聚类，而是将这种技术应用到整个语料库中。在我们之前的例子中，我们分析了最接近的句子的聚类。现在，我们可以通过将主题模型分别应用到每个聚类中来找到主题。这非常简单，您可以自己运行它。
- en: Please see the Top2Vec project for more details and interesting topic modeling
    applications at [https://github.com/ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息和有趣的主题建模应用，请参阅Top2Vec项目[https://github.com/ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec)。
- en: Semantic search with Sentence-BERT
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Sentence-BERT进行语义搜索
- en: We may already be familiar with keyword-based search (Boolean model), where,
    for a given keyword or pattern, we can retrieve the results that match the pattern.
    Alternatively, we can use regular expressions, where we can define advanced patterns
    such as the lexico-syntactic pattern. These traditional approaches cannot handle
    synonym (for example, *car* is the same as *automobile*) or word sense problems
    (for example, *bank* as the side of a river or *bank* as a financial institute).
    While the first synonym case causes low recall due to missing out the documents
    that shouldn't be missed, the second causes low precision due to catching the
    documents not to be caught. Vector-based or semantic search approaches can overcome
    these drawbacks by building a dense numerical representation of both queries and
    documents.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能已经熟悉基于关键字的搜索（布尔模型），在这种模型中，对于给定的关键字或模式，我们可以检索与模式匹配的结果。或者，我们可以使用正则表达式，我们可以定义高级模式，例如词法-句法模式。这些传统方法无法处理同义词（例如，*car*
    与 *automobile* 相同）或词义问题（例如，*bank* 是指河流的一侧还是金融机构）。虽然第一个同义词情况由于漏掉了不应错过的文件而导致低召回率，但第二个情况由于捕获了不应捕获的文件而导致低精度。基于向量或语义的搜索方法可以通过构建查询和文档的密集数值表示来克服这些缺点。
- en: Let's set up a case study for **Frequently Asked Questions** (**FAQs**) that
    are idle on websites. We will exploit FAQ resources within a semantic search problem.
    FAQs contain frequently asked questions. We will be using the FAQ from the **World
    Wide Fund for Nature** (**WWF**), a nature non-governmental organization ([https://www.wwf.org.uk/](https://www.wwf.org.uk/)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为**常见问题**（**FAQs**）建立一个案例研究，这些问题在网站上处于闲置状态。我们将利用语义搜索问题内的常见问题资源。常见问题包含经常被问到的问题。我们将使用来自**世界自然基金会**（**WWF**）的常见问题，这是一个自然非政府组织（[https://www.wwf.org.uk/](https://www.wwf.org.uk/)）。
- en: Given these descriptions, it is easy to understand that performing a semantic
    search using semantic models is very similar to a one-shot learning problem, where
    we just have a single shot of the class (a single sample), and we want to reorder
    the rest of the data (sentences) according to it. You can redefine the problem
    as searching for samples that are semantically close to the given sample, or a
    binary classification according to the sample. Your model can provide a similarity
    metric, and the results for all the other samples will be reordered using this
    metric. The final ordered list is the search result, which is reordered according
    to semantic representation and the similarity metric.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些描述，很容易理解使用语义模型执行语义搜索与一次性学习问题非常相似，其中我们只有一个类别的一次性学习（一个样本），并且我们希望根据它重新排序其余的数据（句子）。您可以重新定义问题，即根据给定样本搜索与给定样本在语义上接近的样本，或者根据样本进行二元分类。您的模型可以提供相似度度量，并且将使用此度量对所有其他样本进行重新排序。最终的有序列表是搜索结果，根据语义表示和相似度度量进行重新排序。
- en: 'WWF has 18 questions and answers on their web page. We defined them as a Python
    list object called `wf_faq` for this experiment:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: WWF在他们的网页上有18个问题和答案。我们将它们定义为Python列表对象，称为 `wf_faq`，用于此实验：
- en: I haven't received my adoption pack. What should I do?
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我还没有收到我的领养包裹。我该怎么办？
- en: How quickly will I receive my adoption pack?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将多快收到我的领养包裹？
- en: How can I renew my adoption?
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我该如何续订我的领养？
- en: How do I change my address or other contact details?
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我该如何更改我的地址或其他联系方式？
- en: Can I adopt an animal if I don't live in the UK?
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我不住在英国，我可以领养一只动物吗？
- en: If I adopt an animal, will I be the only person who adopts that animal?
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我领养了一只动物，我会是唯一领养该动物的人吗？
- en: My pack doesn't contain a certificate?
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的包裹中没有包含证书？
- en: My adoption is a gift but won't arrive on time. What can I do?
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的领养是一份礼物，但不会准时送达。我该怎么办？
- en: Can I pay for an adoption with a one-off payment?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以用一次性付款支付领养费用吗？
- en: Can I change the delivery address for my adoption pack after I've placed my
    order?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下订单后，我可以更改领养包裹的交付地址吗？
- en: How long will my adoption last for?
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的领养将持续多久？
- en: How often will I receive updates about my adopted animal?
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将多久收到有关我所领养动物的更新？
- en: What animals do you have for adoption?
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你们有哪些动物可以领养？
- en: How can I find out more information about my adopted animal?
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何查找更多关于我所领养动物的信息？
- en: How is my adoption money spent?
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的领养费用如何使用？
- en: What is your refund policy?
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你们的退款政策是什么？
- en: An error has been made with my Direct Debit payment; can I receive a refund?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的直接付款出现了错误；我可以收到退款吗？
- en: How do I change how you contact me?
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我该如何更改你们与我联系的方式？
- en: 'Users are free to ask any question they want. We need to evaluate which question
    in the FAQ is the most similar to the user''s question, which is the objective
    of the `quora-distilbert-base` model. There are two options in the SBERT hub –
    one is for English and another for multilingual, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以自由提出任何问题。我们需要评估FAQ中哪个问题与用户的问题最相似，这是`quora-distilbert-base`模型的目标。SBERT hub中有两个选项
    - 一个用于英语，另一个用于多语言，如下所示：
- en: '`quora-distilbert-base`: This is fine-tuned for Quora Duplicate Questions detection
    retrieval.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quora-distilbert-base`：这是为Quora重复问题检测检索而微调的。'
- en: '`quora-distilbert-multilingual`: This is a multilingual version of `quora-distilbert-base`.
    It''s fine-tuned with parallel data for 50+ languages.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quora-distilbert-multilingual`：这是`quora-distilbert-base`的多语言版本。它使用50多种语言的平行数据进行微调。'
- en: 'Let''s build a semantic search model by following these steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤构建一个语义搜索模型：
- en: 'The following is the SBERT model''s instantiation:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是SBERT模型的实例化：
- en: '[PRE43]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s encode the FAQ, as follows:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对FAQ进行编码，如下所示：
- en: '[PRE44]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s prepare five questions so that they are similar to the first five questions
    in the FAQ, respectively; that is, our first test question should be similar to
    the first question in the FAQ, the second question should be similar to the second
    question, and so on, so that we can easily follow the results. Let''s define the
    questions in the `test_questions` list object and encode it, as follows:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们准备五个问题，使它们分别与常见问题解答中的前五个问题相似；也就是说，我们的第一个测试问题应该与常见问题解答中的第一个问题相似，第二个问题应该与第二个问题相似，依此类推，这样我们就可以轻松地跟踪结果。让我们将问题定义在`test_questions`列表对象中并对其进行编码，如下所示：
- en: '[PRE45]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following code measures the similarity between each test question and each
    question in the FAQ and then ranks them:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码测量每个测试问题与常见问题解答中每个问题之间的相似度，然后对它们进行排名：
- en: '[PRE46]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.11 – Question-question similarity ](img/B17123_07_011.jpg)'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.11 – 问题-问题相似度](img/B17123_07_011.jpg)'
- en: Figure 7.11 – Question-question similarity
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.11 – 问题-问题相似度
- en: Here, we can see indexes `0`, `1`, `2`, `3`, and `4` in order, which means the
    model successfully found the similar questions as expected.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到按顺序的索引`0`、`1`、`2`、`3`和`4`，这意味着模型成功地找到了预期的相似问题。
- en: 'For the deployment, we can design the following `getBest()` function, which
    takes a question and returns `K` most similar questions in the FAQ:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于部署，我们可以设计以下`getBest()`函数，它接受一个问题并返回FAQ中最相似的`K`个问题：
- en: '[PRE47]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s ask a question:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提出一个问题：
- en: '[PRE48]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.12 – Similar question similarity results ](img/B17123_07_012.jpg)'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.12 – 相似问题的相似度结果](img/B17123_07_012.jpg)'
- en: Figure 7.12 – Similar question similarity results
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.12 – 相似问题的相似度结果
- en: 'What if a question that''s used as input is not similar to one from the FAQ?
    Here is such a question:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输入的问题与常见问题解答中的问题不相似怎么办？以下是这样一个问题：
- en: '[PRE49]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.13 – Dissimilar question similarity results ](img/B17123_07_013.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 不相似问题的相似度结果](img/B17123_07_013.jpg)'
- en: Figure 7.13 – Dissimilar question similarity results
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 不相似问题的相似度结果
- en: The best dissimilarity score is 0.35\. So, we need to define a threshold such
    as 0.3 so that the model ignores such questions that are higher than that threshold
    and says `no similar answer found`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳不相似度分数为0.35。因此，我们需要定义一个阈值，例如0.3，以便模型忽略高于该阈值的问题，并显示`未找到相似答案`。
- en: Other than question-question symmetric search similarity, we can also utilize
    SBERT's question-answer asymmetric search models, such as `msmarco-distilbert-base-v3`,
    which is trained on a dataset of around 500K Bing search queries. It is known
    as MSMARCO Passage Ranking. This model helps us measure how related the question
    and context are and checks whether the answer to the question is in the passage.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 除了问题-问题对称搜索相似度之外，我们还可以利用SBERT的问题-答案不对称搜索模型，例如`msmarco-distilbert-base-v3`，它是在大约50万个Bing搜索查询数据集上训练的。它被称为MSMARCO
    Passage Ranking。该模型帮助我们衡量问题和上下文的相关性，并检查答案是否在段落中。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about text representation methods. We learned how
    it is possible to perform tasks such as zero-/few-/one-shot learning using different
    and diverse semantic models. We also learned about NLI and its importance in capturing
    semantics of text. Moreover, we looked at some useful use cases such as semantic
    search, semantic clustering, and topic modeling using Transformer-based semantic
    models. We learned how to visualize the clustering results and understood the
    importance of centroids in such problems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于文本表示方法的知识。我们学习了如何使用不同和多样的语义模型执行零/少/一次学习等任务。我们还学习了关于 NLI 及其在捕获文本语义方面的重要性。此外，我们还研究了一些有用的用例，如基于
    Transformer 的语义模型进行语义搜索、语义聚类和主题建模。我们学习了如何可视化聚类结果，并了解了在此类问题中中心点的重要性。
- en: In the next chapter, you will learn about efficient Transformer models. You
    will learn about distillation, pruning, and quantizing Transformer-based models.
    You will also learn about different and efficient Transformer architectures that
    make improvements to computational and memory efficiency, as well as how to use
    them in NLP problems.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习关于高效 Transformer 模型的知识。您将学习有关蒸馏、修剪和量化基于 Transformer 的模型的知识。您还将学习有关不同和高效的
    Transformer 架构，这些架构改进了计算和内存效率，并学习如何在 NLP 问题中使用它们。
- en: Further reading
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following works/papers for more information about the topics
    that were covered in this chapter:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本章中涉及的以下作品/论文，以获取有关涉及主题的更多信息：
- en: 'Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...
    & Zettlemoyer, L. (2019). *Bart: Denoising sequence-to-sequence pre-training for
    natural language generation, translation, and comprehension*. arXiv preprint arXiv:1910.13461\.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...
    & Zettlemoyer, L. (2019). *Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练*。arXiv 预印本 arXiv:1910.13461。
- en: 'Pushp, P. K., & Srivastava, M. M. (2017). *Train once, test anywhere: Zero-shot
    learning for text classification*. arXiv preprint arXiv:1712.05972\.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pushp, P. K., & Srivastava, M. M. (2017). *一次训练，随处测试：用于文本分类的零样本学习*。arXiv 预印本
    arXiv:1712.05972。
- en: 'Reimers, N., & Gurevych, I. (2019). *Sentence-bert: Sentence embeddings using
    siamese bert-networks*. arXiv preprint arXiv:1908.10084\.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers, N., & Gurevych, I. (2019). *Sentence-bert：使用孪生 BERT 网络的句子嵌入*。arXiv
    预印本 arXiv:1908.10084。
- en: 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V.
    (2019). *Roberta: A robustly optimized bert pretraining approach*. arXiv preprint
    arXiv:1907.11692\.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V.
    (2019). *Roberta：一个鲁棒优化的 BERT 预训练方法*。arXiv 预印本 arXiv:1907.11692。
- en: Williams, A., Nangia, N., & Bowman, S. R. (2017). *A broad-coverage challenge
    corpus for sentence understanding through inference*. arXiv preprint arXiv:1704.05426\.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams, A., Nangia, N., & Bowman, S. R. (2017). *用于通过推理理解句子的广覆盖挑战语料库*。arXiv
    预印本 arXiv:1704.05426。
- en: Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil,
    R. (2018). *Universal sentence encoder*. arXiv preprint arXiv:1803.11175\.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil,
    R. (2018). *通用句子编码器*。arXiv 预印本 arXiv:1803.11175。
- en: Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., ... & Kurzweil,
    R. (2019). *Multilingual universal sentence encoder for semantic retrieval*. arXiv
    preprint arXiv:1907.04307\.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., ... & Kurzweil,
    R. (2019). *用于语义检索的多语言通用句子编码器*。arXiv 预印本 arXiv:1907.04307。
- en: 'Humeau, S., Shuster, K., Lachaux, M. A., & Weston, J. (2019). *Poly-encoders:
    Transformer architectures and pre-training strategies for fast and accurate multi-sentence
    scoring.* arXiv preprint arXiv:1905.01969.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Humeau, S., Shuster, K., Lachaux, M. A., & Weston, J. (2019). *Poly-encoders：用于快速和准确的多句子评分的
    Transformer 架构和预训练策略*。arXiv 预印本 arXiv:1905.01969。
