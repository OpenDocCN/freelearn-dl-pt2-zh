["```py\nfrom torch import nn\ntransformer = nn.Transformer(hidden_dim, nheads, \\\n                     num_encoder_layers, num_decoder_layers) \n```", "```py\n    %pip install -U torch-snippets transformers kaggle\n    from torch_snippets import *\n    from transformers import ViTModel, ViTConfig\n    from torch.optim import Adam\n    model_checkpoint = 'google/vit-base-patch16-224-in21k' \n    ```", "```py\n    %%writefile kaggle.json\n    {\"username\":\"xx\", \"key\":\"xx\"}\n    !mkdir -p ~/.kaggle\n    !cp kaggle.json ~/.kaggle/\n    !chmod 600 /root/.kaggle/kaggle.json\n    !kaggle datasets download -d tongpython/cat-and-dog\n    !unzip cat-and-dog.zip \n    ```", "```py\n    train_data_dir = 'training_set/training_set'\n    test_data_dir = 'test_set/test_set' \n    ```", "```py\n    class CatsDogs(Dataset):\n        def __init__(self, folder):\n            cats = glob(folder+'/cats/*.jpg')\n            dogs = glob(folder+'/dogs/*.jpg')\n            self.fpaths = cats[:500] + dogs[:500]\n            self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                  std=[0.229, 0.224, 0.225])\n            from random import shuffle, seed; seed(10); shuffle(self.fpaths)\n            self.targets = [fpath.split('/')[-1].startswith('dog') \\\n                                        for fpath in self.fpaths]\n        def __len__(self): return len(self.fpaths)\n        def __getitem__(self, ix):\n            f = self.fpaths[ix]\n            target = self.targets[ix]\n            im = (cv2.imread(f)[:,:,::-1])\n            im = cv2.resize(im, (224,224))\n            im = torch.tensor(im/255)\n            im = im.permute(2,0,1)\n            im = self.normalize(im)\n            return im.float().to(device), \\\n                      torch.tensor([target]).float().to(device) \n    ```", "```py\n    class ViT(nn.Module):\n      def __init__(self, config=ViTConfig(), num_labels=1,\n                   model_checkpoint='google/vit-base-patch16-224-in21k'):\n            super(ViT, self).__init__()\n            self.vit = ViTModel.from_pretrained(model_checkpoint, \\\n                                         add_pooling_layer=False)\n            self.classifier1 = (nn.Linear(config.hidden_size, 128))\n            self.classifier2 = (nn.Linear(128, num_labels))\n            self.classifier = nn.Sequential(\n                                            self.classifier1,\n                                            nn.ReLU(),\n                                            self.classifier2)\n            for param in self.vit.parameters():\n                param.requires_grad = False\n      def forward(self, x):\n        x = self.vit(x)['last_hidden_state']\n        # Use the embedding of [CLS] token\n        output = self.classifier(x[:, 0, :])\n        output = torch.sigmoid(output)\n        return output \n    ```", "```py\n    model = ViT().to('cuda')\n    loss_fn = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3) \n    ```", "```py\n    def train_batch(x, y, model, opt, loss_fn):\n        model.train()\n        prediction = model(x)\n        batch_loss = loss_fn(prediction, y)\n        batch_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        return batch_loss.item()\n    @torch.no_grad()\n    def accuracy(x, y, model):\n        model.eval()\n        prediction = model(x)\n        is_correct = (prediction > 0.5) == y\n        return is_correct.cpu().numpy().tolist()\n    def get_data():\n        train = CatsDogs(train_data_dir)\n        trn_dl = DataLoader(train, batch_size=32, shuffle=True,\n                                               drop_last = True)\n        val = CatsDogs(test_data_dir)\n        val_dl = DataLoader(val, batch_size=32, shuffle=True,\n                                               drop_last = True)\n        return trn_dl, val_dl\n    trn_dl, val_dl = get_data() \n    ```", "```py\n    n_epochs = 5\n    report = Report(n_epochs)\n    for epoch in range(n_epochs):\n        train_epoch_losses, train_epoch_accuracies = [], []\n        val_epoch_accuracies = []\n        n = len(trn_dl)\n        for ix, batch in enumerate(iter(trn_dl)):\n            x, y = batch\n            batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n            is_correct = accuracy(x, y, model)\n            report.record(epoch+(ix+1)/n, trn_loss=batch_loss,\n                                  trn_acc=np.mean(is_correct), end='\\r')\n        n = len(val_dl)\n        for ix, batch in enumerate(iter(val_dl)):\n            x, y = batch\n            val_is_correct = accuracy(x, y, model)\n            report.record(epoch+(ix+1)/n,\n                          val_acc=np.mean(val_is_correct), end='\\r')\n        report.report_avgs(epoch+1) \n    ```", "```py\n    report.plot(['trn_loss'], sz=3, figsize=(5,3))\n    report.plot_epochs(['acc','trn_acc'], figsize=(5,3)) \n    ```", "```py\n    !wget https://www.dropbox.com/s/l2ul3upj7dkv4ou/synthetic-data.zip\n    !unzip -qq synthetic-data.zip \n    ```", "```py\n    !pip install torch_snippets torch_summary editdistance jiwer accelerate\n    from torch_snippets import *\n    from torchsummary import summary\n    import editdistance \n    ```", "```py\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    fname2label = lambda fname: stem(fname).split('@')[0]\n    images = Glob('synthetic-data/*') \n    ```", "```py\n    images_list = []\n    labels_list = []\n    for image in images:\n      images_list.append(str(image).split('/')[-1])\n      labels_list.append(fname2label(image))\n    df = pd.DataFrame([images_list[:5000], labels_list[:5000]]).T\n    df.columns = ['file_name', 'text'] \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train_df, test_df = train_test_split(df, test_size=0.1)\n    train_df.reset_index(drop=True, inplace=True)\n    test_df.reset_index(drop=True, inplace=True) \n    ```", "```py\n    class IAMDataset(Dataset):\n        def __init__(self, root_dir, df, processor, max_target_length=128):\n            self.root_dir = root_dir\n            self.df = df\n            self.processor = processor\n            self.max_target_length = max_target_length\n        def __len__(self):\n            return len(self.df)\n        def __getitem__(self, idx):\n            # get file name + text\n            file_name = self.df['file_name'][idx]\n            text = self.df['text'][idx]\n            # prepare image (i.e. resize + normalize)\n            image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n            pixel_values = self.processor(image,\n                               return_tensors=\"pt\").pixel_values\n            # add labels (input_ids) by encoding the text\n            labels = self.processor.tokenizer(text,\n                                           padding=\"max_length\", \n                                 max_length=self.max_target_length).input_ids\n            # important: make sure that PAD tokens are ignored by the loss function\n            labels = [label if label != self.processor.tokenizer.pad_token_id \\\n                                               else -100 for label in labels]\n            encoding = {\"pixel_values\": pixel_values.squeeze(), \\\n                        \"labels\": torch.tensor(labels)}\n            return encoding \n    ```", "```py\n    from transformers import TrOCRProcessor\n    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n    ```", "```py\n    train_dataset = IAMDataset(root_dir='/content/synthetic-data/', \\\n                                  df=train_df, processor=processor)\n    eval_dataset = IAMDataset(root_dir='/content/synthetic-data/', \n                                  df=test_df, processor=processor) \n    ```", "```py\n    from transformers import VisionEncoderDecoderModel\n    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-\n                                                                    stage1\") \n    ```", "```py\n    # set special tokens used for creating the decoder_input_ids from the labels\n    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n    model.config.pad_token_id = processor.tokenizer.pad_token_id\n    # make sure vocab size is set correctly\n    model.config.vocab_size = model.config.decoder.vocab_size\n    # set beam search parameters\n    model.config.eos_token_id = processor.tokenizer.sep_token_id\n    model.config.max_length = 64\n    model.config.early_stopping = True\n    model.config.no_repeat_ngram_size = 3\n    model.config.length_penalty = 2.0\n    model.config.num_beams = 4\n    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n    training_args = Seq2SeqTrainingArguments(\n                                             predict_with_generate=True,\n                                             evaluation_strategy=\"steps\",\n                                             per_device_train_batch_size=8,\n                                             per_device_eval_batch_size=8,\n                                             fp16=True,\n                                             output_dir=\"./\",\n                                             logging_steps=2,\n                                             save_steps=1000,\n                                             eval_steps=100,\n                                             num_train_epochs = 10\n                                             ) \n    ```", "```py\n    from datasets import load_metric\n    cer_metric = load_metric(\"cer\")\n    def compute_metrics(pred):\n        labels_ids = pred.label_ids\n        pred_ids = pred.predictions\n        pred_str = processor.batch_decode(pred_ids,\n                                   skip_special_tokens=True)\n        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n        label_str = processor.batch_decode(labels_ids, \\\n                                   skip_special_tokens=True)\n        cer = cer_metric.compute(predictions=pred_str, references=label_str)\n        return {\"cer\": cer} \n    ```", "```py\n    from transformers import default_data_collator\n    # instantiate trainer\n    trainer = Seq2SeqTrainer(\n                             model=model,\n                             tokenizer=processor.feature_extractor,\n                             args=training_args,\n                             compute_metrics=compute_metrics,\n                             train_dataset=train_dataset,\n                             eval_dataset=eval_dataset,\n                             data_collator=default_data_collator,\n                             )\n    trainer.train() \n    ```", "```py\n    # Load and preprocess the image\n    image = Image.open(\"/content/synthetic-data/American@3WPOqS.png\").convert(\"RGB\")\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n    # Perform inference\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        generated_ids = model.generate(pixel_values.to(device))\n    # Decode the generated ids to text\n    predicted_text = processor.batch_decode(generated_ids, \\\n                               skip_special_tokens=True)[0]\n    show(image)\n    print(\"Predicted Text:\", predicted_text) \n    ```", "```py\n    %pip install transformers[torch] datasets seqeval torch-snippets torchinfo lovely_tensors\n    from torch_snippets import *\n    from builtins import print \n    ```", "```py\n    from datasets import load_dataset\n    dataset = load_dataset('sizhkhy/passports') \n    ```", "```py\n    examples_train = dataset['train']\n    examples_eval = dataset['valid'] \n    ```", "```py\n    id2label = {i:v for i, v in set(list(zip(\\\n                                flatten(examples_train['labels']), \\\n                                flatten(examples_train['label_string']))))}\n    label2id = {v:i for i, v in id2label.items()} \n    ```", "```py\n    from transformers import AutoProcessor\n    processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n    def prepare_examples(examples):\n      images = examples['image']\n      words = examples['words']\n      boxes = examples['boxes']\n      word_labels = examples['labels']\n      encoding = processor(images, words, boxes=boxes, \\\n                           word_labels=word_labels,\n                           truncation=True, padding=\"max_length\")\n      return encoding \n    ```", "```py\n    train_dataset = examples_train.map(\n                                  prepare_examples,\n                                  batched=True,\n                                  remove_columns=examples_train.column_names,\n                                  )\n    eval_dataset = examples_eval.map(\n                                  prepare_examples,\n                                  batched=True,\n                                  remove_columns=examples_eval.column_names,\n                                  ) \n    ```", "```py\n    from datasets import load_metric\n    metric = load_metric(\"seqeval\") \n    ```", "```py\n    return_entity_level_metrics = False\n    def compute_metrics(p):\n        predictions, labels = p\n        predictions = np.argmax(predictions, axis=2)\n        # Remove ignored index (special tokens)\n        true_predictions = [[id2label[p] for (p, l) in \\\n                      zip(prediction, label) if l != -100] \\\n              for prediction, label in zip(predictions, labels)]\n        true_labels = [[id2label[l] for (p, l) in \\\n                       zip(prediction,label) if\\ l != -100] \\\n               for prediction, label in zip(predictions,labels)]\n        results = metric.compute(predictions=true_predictions,\n                                         references=true_labels)\n        if return_entity_level_metrics:\n            # Unpack nested dictionaries\n            final_results = {}\n            for key, value in results.items():\n                if isinstance(value, dict):\n                    for n, v in value.items():\n                        final_results[f\"{key}_{n}\"] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {\n                \"precision\": results[\"overall_precision\"],\n                \"recall\": results[\"overall_recall\"],\n                \"f1\": results[\"overall_f1\"],\n                \"accuracy\": results[\"overall_accuracy\"]} \n    ```", "```py\n    from transformers import LayoutLMv3ForTokenClassification\n    model = LayoutLMv3ForTokenClassification.from_pretrained(\n                         \"microsoft/layoutlmv3-base\",\n                         id2label=id2label,\n                         label2id=label2id\n                         ) \n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(output_dir=\"test\",\n                                  max_steps=100,                                 \n                                  per_device_train_batch_size=2,                                 \n                                  per_device_eval_batch_size=2,\n                                  learning_rate=1e-5,                                 \n                                  evaluation_strategy=\"steps\",\n                                  eval_steps=50,                                 \n                                  load_best_model_at_end=True,                                \n                                  metric_for_best_model=\"f1\") \n    ```", "```py\n    from transformers.data.data_collator import default_data_collator\n    # Initialize our Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=processor,\n        data_collator=default_data_collator,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train() \n    ```", "```py\n    import torch\n    from transformers import AutoProcessor, Blip2ForConditionalGeneration\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n    processor = AutoProcessor.from_pretrained(MODEL_ID)\n    model = Blip2ForConditionalGeneration.from_pretrained(MODEL_ID,\n                                   torch_dtype=torch.float16)\n    model.to(device) \n    ```", "```py\n    import requests\n    from PIL import Image\n    image = Image.open('/content/Tejas.jpeg') \n    ```", "```py\n    inputs = processor(image, return_tensors=\"pt\").to(device,\n                                                 torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=20)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    print(generated_text) \n    ```", "```py\na baby wearing a party hat sits on a bed \n```", "```py\n    prompt = \"Question: what is the color of baby's trousers? Answer:\"\n    inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=10)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    print(generated_text) \n    ```"]