- en: Generating Images Using GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GANs生成图像
- en: So far, we have learned about the discriminative model, which learns to discriminate
    between the classes. That is, given an input, it tells us which class they belong
    to. For instance, to predict whether an email is a spam or ham, the model learns
    the decision boundary that best separates the two classes (spam and ham), and
    when a new email comes in they can tell us which class the new email belongs to.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了判别模型，它学习区分不同类别。也就是说，给定一个输入，它告诉我们它属于哪个类别。例如，要预测一封电子邮件是垃圾邮件还是正常邮件，该模型学习最佳分隔两类（垃圾邮件和正常邮件）的决策边界，当有新的电子邮件进来时，它们可以告诉我们新邮件属于哪个类别。
- en: In this chapter, we will learn about a generative model that learns the class
    distribution, that is, the characteristics of the classes rather than learning
    the decision boundary. As the name suggests, with the generative models, we can
    generate new data points similar to the data points present in the training set.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一种生成模型，该模型学习类分布，即学习类的特征，而不是学习决策边界。顾名思义，生成模型可以生成与训练集中现有数据点类似的新数据点。
- en: We will start off the chapter by understanding the difference between the discriminative
    and generative models in detail. Then, we will deep dive into one of the most
    popularly used generative algorithms, called **Generative Adversarial Networks**
    (**GANs**). We will understand how GANs work and how they are used to generate
    new data points. Going ahead, we will explore the architecture of GANs and we
    will learn about the loss function. Later, we will see how to implement GANs in
    TensorFlow to generate handwritten digits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从深入理解判别模型和生成模型的差异开始本章。然后，我们将深入探讨最广泛使用的生成算法之一，称为**生成对抗网络**（**GANs**）。我们将了解GANs的工作原理以及它们如何用于生成新的数据点。接下来，我们将探索GANs的架构，并学习其损失函数。随后，我们将看到如何在TensorFlow中实现GANs以生成手写数字。
- en: We shall also scrutinize the **Deep Convolutional Generative Adversarial Network**
    (**DCGAN**), which acts as a small extension to the vanilla GAN by using convolutional
    networks in their architecture. Going forward, we will explore **Least Squares
    GAN** (**LSGAN**), which adopts the least square loss for generating better and
    quality images.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将详细研究**深度卷积生成对抗网络**（**DCGAN**），它作为普通GAN的小扩展，使用卷积网络进行架构设计。接下来，我们将探索**最小二乘生成对抗网络**（**LSGAN**），它采用最小二乘损失来生成更好和更质量的图像。
- en: At the end of the chapter, we will get the hang of **Wasserstein GAN** (**WGA****N**)
    which uses the Wasserstein metric in the GAN's loss function for better results.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将了解**Wasserstein生成对抗网络**（**WGAN**），它在GAN的损失函数中使用Wasserstein度量以获得更好的结果。
- en: 'The chapter will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Differences between generative and discriminative models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型和判别模型的区别
- en: GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs
- en: Architecture of GANs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络的架构
- en: Building GANs in TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中构建GANs
- en: Deep convolutional GANs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积GANs
- en: Generating CIFAR images using DCGAN
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DCGAN生成CIFAR图像
- en: Least Squares GANs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小二乘生成对抗网络
- en: Wasserstein GANs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein生成对抗网络
- en: Differences between discriminative and generative models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别模型和生成模型的区别
- en: Given some data points, the discriminative model learns to classify the data
    points into their respective classes by learning the decision boundary that separates
    the classes in an optimal way. The generative models can also classify given data
    points, but instead of learning the decision boundary, they learn the characteristics
    of each of the classes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些数据点，判别模型学习将数据点分类到它们各自的类别中，通过学习最优的决策边界来分隔类别。生成模型也可以对给定的数据点进行分类，但是它们不是学习决策边界，而是学习每个类别的特征。
- en: 'For instance, let''s consider the image classification task for predicting
    whether a given image is an apple or an orange. As shown in the following figure,
    to classify between apple and orange, the discriminative model learns the optimal
    decision boundary that separates the apples and oranges classes, while generative
    models learn their distribution by learning the characteristics of the apple and
    orange classes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑图像分类任务，预测给定图像是苹果还是橙子。如下图所示，为了区分苹果和橙子，判别模型学习最优的决策边界来分隔苹果和橙子类别，而生成模型则通过学习苹果和橙子类别的特征分布来学习它们的分布：
- en: '![](img/4b853059-0261-4c0a-9e2f-723635b0fda2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b853059-0261-4c0a-9e2f-723635b0fda2.png)'
- en: To put it simply, discriminative models learn to find the decision boundary
    that separates the classes in an optimal way, while the generative models learn
    about the characteristics of each class.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，判别模型学习如何以最优方式找到分隔类别的决策边界，而生成模型则学习每个类别的特征。
- en: 'Discriminative models predict the labels conditioned on the input ![](img/88413610-a356-4447-88cf-cb30301d2907.png),
    whereas generative models learn the joint probability distribution ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png).
    Examples of discriminative models include logistic regression, **Support Vector
    Machine** (**SVM**), and so on, where we can directly estimate ![](img/88413610-a356-4447-88cf-cb30301d2907.png)
    from the training set. Examples of generative models include **Markov random fields**
    and **naive Bayes**, where first we estimate ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png)
    to determine ![](img/88413610-a356-4447-88cf-cb30301d2907.png):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 判别模型预测输入条件下的标签 ![](img/88413610-a356-4447-88cf-cb30301d2907.png)，而生成模型学习联合概率分布
    ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png)。判别模型的例子包括逻辑回归、**支持向量机**（**SVM**）等，我们可以直接从训练集中估计
    ![](img/88413610-a356-4447-88cf-cb30301d2907.png)。生成模型的例子包括**马尔可夫随机场**和**朴素贝叶斯**，首先我们估计
    ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png) 来确定 ![](img/88413610-a356-4447-88cf-cb30301d2907.png)：
- en: '![](img/d154d240-35cc-45b1-aa96-4b14e779b280.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d154d240-35cc-45b1-aa96-4b14e779b280.png)'
- en: Say hello to GANs!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说一声你好，GAN！
- en: GAN was first introduced by Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio
    in their paper, *Generative Adversarial Network*s, in 2014.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GAN最初由Ian J Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil
    Ozair、Aaron Courville和Yoshua Bengio在2014年的论文《生成对抗网络》中首次提出。
- en: GANs are used extensively for generating new data points. They can be applied
    to any type of dataset, but they are popularly used for generating images. Some
    of the applications of GANs include generating realistic human faces, converting
    grayscale images to colored images, translating text descriptions into realistic
    images, and many more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GAN广泛用于生成新数据点。它们可以应用于任何类型的数据集，但通常用于生成图像。GAN的一些应用包括生成逼真的人脸，将灰度图像转换为彩色图像，将文本描述转换为逼真图像等。
- en: 'Yann LeCun said the following about GANs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun这样评价GAN：
- en: '"The coolest idea in deep learning in the last 20 years."'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '"过去20年来深度学习中最酷的想法。"'
- en: 'GANs have evolved so much in recent years that they can generate a very realistic
    image. The following figure shows the evolution of GANs in generating images over
    the course of five years:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在最近几年已经发展得非常成熟，能够生成非常逼真的图像。下图展示了GAN在五年间生成图像的演变：
- en: '![](img/c91e9ad5-0074-4728-8253-df58cf767b83.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c91e9ad5-0074-4728-8253-df58cf767b83.png)'
- en: Excited about GANs already? Now, we will see how exactly they work. Before going
    ahead, let's consider a simple analogy. Let's say you are the police and your
    task is to find counterfeit money, and the role of the counterfeiter is to create
    fake money and cheat the police.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对GAN已经感到兴奋了吗？现在，我们将看看它们如何工作。在继续之前，让我们考虑一个简单的类比。假设你是警察，你的任务是找到伪钞，而伪造者的角色是制造假钞并欺骗警察。
- en: 'The counterfeiter constantly tries to create fake money in a way that is so
    realistic that it cannot be differentiated from the real money. But the police
    have to identify whether the money is real or fake. So, the counterfeiter and
    the police essentially play a two-player game where one tries to defeat the other.
    GANs work something like this. They consist of two important components:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 伪造者不断尝试以一种逼真到无法与真钱区分的方式创建假钱。但警察必须识别钱是真还是假。因此，伪造者和警察实质上是在一个两人游戏中互相竞争。GAN的工作原理就类似于这样。它们由两个重要组成部分组成：
- en: Generator
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器
- en: Discriminator
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器
- en: You can perceive the generator as analogous to the counterfeiter, while the
    discriminator is analogous to the police. That is, the role of the generator is
    to create fake money, and the role of the discriminator is to identify whether
    the money is fake or real.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将生成器视为伪造者的类比，而鉴别器则类似于警察。换句话说，生成器的角色是创造假钱，而鉴别器的角色是判断钱是假的还是真的。
- en: 'Without going into detail, first, we will get a basic understanding of GANs.
    Let''s say we want our GAN to generate handwritten digits. How can we do that?
    First, we will take a dataset containing a collection of handwritten digits; say,
    the MNIST dataset. The generator learns the distribution of images in our dataset.
    Thus, it learns the distribution of handwritten digits in our training set. Once,
    it learns the distribution of the images in our dataset, and we feed a random
    noise to the generator, it will convert the random noise into a new handwritten
    digit similar to the one in our training set based on the learned distribution:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入细节之前，我们先基本了解一下GAN。假设我们想让我们的GAN生成手写数字。我们如何做到这一点？首先，我们会获取一个包含手写数字集合的数据集；比如说，MNIST数据集。生成器学习我们数据集中图像的分布。因此，它学习了我们训练集中手写数字的分布。一旦它学习了我们数据集中图像的分布，我们向生成器提供随机噪声，它将根据学习到的分布将随机噪声转换为一个新的手写数字，类似于训练集中的手写数字：
- en: '![](img/982d2c25-cd87-43df-83e0-654aeb6083c0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/982d2c25-cd87-43df-83e0-654aeb6083c0.png)'
- en: 'The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the image is generated by the generator:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的目标是执行分类任务。给定一张图像，它将其分类为真实或虚假；也就是说，图像是来自训练集还是由生成器生成的：
- en: '![](img/c114f411-e07d-4089-ae5b-d330a3781f12.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c114f411-e07d-4089-ae5b-d330a3781f12.png)'
- en: The generator component of GAN is basically a generative model, and the discriminator
    component is basically a discriminative model. Thus, the generator learns the
    distribution of the class and the discriminator learns the decision boundary of
    a class.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的生成器组件基本上是一个生成模型，鉴别器组件基本上是一个判别模型。因此，生成器学习类的分布，而鉴别器学习类的决策边界。
- en: 'As shown in the following figure, we feed a random noise to the generator,
    and it then converts this random noise into a new image *similar* to the one we
    have in our training set, but not *exactly* the same as the images in the training
    set. The image generated by the generator is called a fake image, and the images
    in our training set are called real images. We feed both the real and fake images
    to the discriminator, which tells us the probability of them being real. It returns
    0 if the image is fake and 1 if the image is real:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们向生成器提供随机噪声，它将这个随机噪声转换为一个新的图像，*类似*于我们训练集中的图像，但不*完全*与训练集中的图像相同。生成器生成的图像称为虚假图像，而我们训练集中的图像称为真实图像。我们将真实图像和虚假图像都输入给鉴别器，它告诉我们它们是真实的概率。如果图像是虚假的，则返回0；如果图像是真实的，则返回1：
- en: '![](img/6720009a-087d-4a75-9f17-57cf40fd0280.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6720009a-087d-4a75-9f17-57cf40fd0280.png)'
- en: Now that we have a basic understanding of generators and discriminators, we
    will study each of the components in detail.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对生成器和判别器有了基本的理解，接下来我们将详细研究每个组件。
- en: Breaking down the generator
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解生成器
- en: The generator component of a GAN is a generative model. When we say the generative
    model, there are two types of generative models— an **implicit** and an **explicit**
    density model. The implicit density model does not use any explicit density function
    to learn the probability distribution, whereas the explicit density model, as
    the name suggests, uses an explicit density function. GANs falls into the first
    category. That is, they are an implicit density model. Let's study in detail and
    understand how GANs are an implicit density model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的生成器组件是一个生成模型。当我们说生成模型时，有两种类型的生成模型——**隐式**和**显式**密度模型。隐式密度模型不使用任何显式密度函数来学习概率分布，而显式密度模型则使用显式密度函数。GAN属于第一类，即它们是隐式密度模型。让我们详细研究并理解GAN如何是隐式密度模型。
- en: Let's say we have a generator, ![](img/92ca0c2d-263b-4fe9-8127-821f34e69bd7.png).
    It is basically a neural network parametrized by ![](img/d2325c36-a6ff-49f2-85ed-a3727e564425.png).
    The role of the generator network is to generate new images. How do they do that?
    What should be the input to the generator?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个生成器，![](img/92ca0c2d-263b-4fe9-8127-821f34e69bd7.png)。它基本上是一个由![](img/d2325c36-a6ff-49f2-85ed-a3727e564425.png)参数化的神经网络。生成器网络的作用是生成新的图像。它们是如何做到的？生成器的输入应该是什么？
- en: 'We sample a random noise, ![](img/e81c13be-a7f2-4fce-b65d-bb431e78a44f.png),
    from a normal or uniform distribution, ![](img/7b87aece-3326-440b-b0c2-6946956240c3.png).
    We feed this random noise, ![](img/e81c13be-a7f2-4fce-b65d-bb431e78a44f.png),
    as an input to the generator and then it converts this noise to an image:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从正态或均匀分布中采样一个随机噪声，`img/e81c13be-a7f2-4fce-b65d-bb431e78a44f.png`。我们将这个随机噪声作为输入传递给生成器，然后生成器将这个噪声转换为一张图像：
- en: '![](img/0f9e55cc-f1c7-4e81-8970-10d03f783878.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '`img/0f9e55cc-f1c7-4e81-8970-10d03f783878.png`'
- en: Surprising, isn't it? How does the generator converts a random noise to a realistic
    image?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶，不是吗？生成器是如何将随机噪声转换成逼真图像的？
- en: Let's say we have a dataset containing a collection of human faces and we want
    our generator to generate a new human face. First, the generator learns all the
    features of the face by learning the probability distribution of the images in
    our training set. Once the generator learns the correct probability distribution,
    it can generate totally new human faces.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含人脸图像集合的数据集，我们希望我们的生成器生成一个新的人脸。首先，生成器通过学习训练集中图像的概率分布来学习人脸的所有特征。一旦生成器学会了正确的概率分布，它就能生成全新的人脸图像。
- en: But how does the generator learn the distribution of the training set? That
    is, how does the generator learn the distribution of images of human faces in
    the training set?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但是生成器如何学习训练集的分布？也就是说，生成器如何学习训练集中人脸图像的分布？
- en: A generator is nothing but a neural network. So, what happens is that the neural
    network learns the distribution of the images in our training set implicitly;
    let's call this distribution a generator distribution, ![](img/1761d20a-41ac-42fd-96a9-7020402924b0.png).
    At the first iteration, the generator generates a really noisy image. But over
    a series of iterations, it learns the exact probability distribution of our training
    set and learns to generate a correct image by tuning its ![](img/19a949c3-3c6d-4648-a9a7-b369bea074b7.png)
    parameter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器不过是一个神经网络。因此，神经网络隐式学习我们训练集图像的分布；让我们将这个分布称为生成器分布`img/1761d20a-41ac-42fd-96a9-7020402924b0.png`。在第一次迭代中，生成器生成一个非常嘈杂的图像。但是在一系列迭代中，它学会了准确的训练集概率分布，并通过调整其`img/19a949c3-3c6d-4648-a9a7-b369bea074b7.png`参数来学习生成正确的图像。
- en: It is important to note that we are not using the uniform distribution ![](img/f7abe107-bf51-4c25-a7c3-1c179dcb65b0.png)for
    learning the distribution of our training set. It is only used for sampling random
    noise, and we feed this random noise as an input to the generator. The generator
    network implicitly learns the distribution of our training set and we call this
    distribution a generator distribution, ![](img/9133256a-83f4-442b-b832-92b5219fdea6.png)and
    that is why we call our generator network an implicit density model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们并没有使用均匀分布`img/f7abe107-bf51-4c25-a7c3-1c179dcb65b0.png`来学习我们训练集的分布。它仅用于采样随机噪声，并且我们将这个随机噪声作为输入传递给生成器。生成器网络隐式学习我们训练集的分布，我们称之为生成器分布`img/9133256a-83f4-442b-b832-92b5219fdea6.png`，这也是我们称生成器网络为隐式密度模型的原因。
- en: Breaking down the discriminator
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解鉴别器
- en: As the name suggests, the discriminator is a discriminative model. Let's say
    we have a discriminator, ![](img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png). It
    is also a neural network and it is parametrized by ![](img/1c69a44b-a93f-46cf-b35a-06d8a59516fe.png).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，鉴别器是一个鉴别模型。假设我们有一个鉴别器，`img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png`。它也是一个神经网络，并且由参数化`img/1c69a44b-a93f-46cf-b35a-06d8a59516fe.png`。
- en: 'The goal of the discriminator to discriminate between two classes. That is,
    given an image ![](img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png), it has to identify
    whether the image is from a real distribution or a fake distribution (generator
    distribution). That is, discriminator has to identify whether the given input
    image is from the training set or the fake image generated by the generator:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器的目标是区分两类。也就是说，给定一张图像`img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png`，它必须确定图像是来自真实分布还是生成器分布（虚假分布）。也就是说，鉴别器必须确定给定的输入图像是来自训练集还是生成器生成的虚假图像：
- en: '![](img/34c15144-474c-41d5-98d6-042c6dbdf0ef.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '`img/34c15144-474c-41d5-98d6-042c6dbdf0ef.png`'
- en: Let's call the distribution of our training set the real data distribution,
    which is represented by ![](img/1d47c8df-f7c7-4f75-a9ff-74ee634de483.png). We
    know that the generator distribution is represented by ![](img/c4ae4675-c413-4ad1-9d5f-5676b6e3172a.png).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的训练集的分布称为真实数据分布，由 ![](img/1d47c8df-f7c7-4f75-a9ff-74ee634de483.png) 表示。我们知道生成器的分布由
    ![](img/c4ae4675-c413-4ad1-9d5f-5676b6e3172a.png) 表示。
- en: So, the discriminator ![](img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png) essentially
    tries to discriminate whether the image ![](img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png)
    is from ![](img/658f1dcf-0a4a-403c-8ade-f443321a85b2.png) or ![](img/8e18b1e3-3e32-4a76-924a-931a5ef515de.png).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴别器 ![](img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png) 实质上试图区分图像 ![](img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png)
    是来自于 ![](img/658f1dcf-0a4a-403c-8ade-f443321a85b2.png) 还是 ![](img/8e18b1e3-3e32-4a76-924a-931a5ef515de.png)。
- en: How do they learn though?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但是它们是如何学习的呢？
- en: So far, we just studied the role of the generator and discriminator, but how
    do they learn exactly? How does the generator learn to generate new realistic
    images and how does the discriminator learn to discriminate between images correctly?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是研究了生成器和鉴别器的角色，但它们究竟是如何学习的？生成器如何学习生成新的逼真图像，鉴别器如何学习正确区分图像？
- en: We know that the goal of the generator is to generate an image in such a way
    as to fool the discriminator into believing that the generated image is from a
    real distribution.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器的目标是以一种方式生成图像，使得鉴别器认为生成的图像来自真实分布。
- en: In the first iteration, the generator generates a noisy image. When we feed
    this image to the discriminator, discriminator can easily detect that the image
    is from a generator distribution. The generator takes this as a loss and tries
    to improve itself, as its goal is to fool the discriminator. That is, if the generator
    knows that the discriminator is easily detecting the generated image as a fake
    image, then it means that it is not generating an image similar to those in the
    training set. This implies that it has not learned the probability distribution
    of the training set yet.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，生成器生成了一张嘈杂的图像。当我们将这个图像输入给鉴别器时，鉴别器可以轻易地检测到这是来自生成器分布的图像。生成器将这视为损失并尝试改进自己，因为它的目标是欺骗鉴别器。也就是说，如果生成器知道鉴别器轻易地将生成的图像检测为虚假图像，那么这意味着它没有生成类似于训练集中的图像。这表明它尚未学会训练集的概率分布。
- en: So, the generator tunes its parameters in such a way as to learn the correct
    probability distribution of the training set. As we know that the generator is
    a neural network, we simply update the parameters of the network through backpropagation.
    Once it has learned the probability distribution of the real images, then it can
    generate images similar to the ones in the training set.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，生成器调整其参数，以学习训练集的正确概率分布。由于我们知道生成器是一个神经网络，我们只需通过反向传播更新网络的参数。一旦它学会了真实图像的概率分布，它就可以生成类似于训练集中的图像。
- en: Okay, what about the discriminator? How does it learn? As we know, the role
    of the discriminator is to discriminate between real and fake images.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么鉴别器呢？它如何学习呢？正如我们所知，鉴别器的角色是区分真实和虚假图像。
- en: If the discriminator incorrectly classifies the generated image; that is, if
    the discriminator classifies the fake image as a real image, then it implies that
    the discriminator has not learned to differentiate between the real and fake image.
    So, we update the parameter of the discriminator network through backpropagation
    to make the discriminator learn to classify between real and fake images.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果鉴别器错误地将生成的图像分类；也就是说，如果鉴别器将虚假图像分类为真实图像，那么这意味着鉴别器没有学会区分真实图像和虚假图像。因此，我们通过反向传播更新鉴别器网络的参数，使其学会区分真实和虚假图像。
- en: So, basically, the generator is trying to fool the discriminator by learning
    the real data distribution, ![](img/0f53be09-b9ad-4d19-a3a0-1c2a0922e139.png),
    and the discriminator is trying to find out whether the image is from a real or
    fake distribution. Now the question is, when do we stop training the network in
    light of the fact that both generator and discriminator are competing against
    each other?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基本上，生成器试图通过学习真实数据分布 ![](img/0f53be09-b9ad-4d19-a3a0-1c2a0922e139.png)，欺骗鉴别器，而鉴别器试图找出图像是来自真实分布还是虚假分布。现在的问题是，在生成器和鉴别器相互竞争的情况下，我们何时停止训练网络？
- en: Basically, the goal of the GAN is to generate images similar to the one in the
    training set. Say we want to generate a human face—we learn the distribution of
    images in the training set and generate new faces. So, for a generator, we need
    to find the optimal discriminator. What do we mean by that?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，GAN 的目标是生成类似于训练集中的图像。比如我们想生成一个人脸—我们学习训练集中图像的分布并生成新的人脸。因此，对于生成器，我们需要找到最优的判别器。这是什么意思？
- en: 'We know that a generator distribution is represented by ![](img/7d56ebaa-5f36-4aa5-8880-d2a08def9e42.png)
    and the real data distribution is represented by ![](img/b23d9151-6e6c-4297-a6e7-2be230ca72b2.png).
    If the generator learns the real data distribution perfectly, then ![](img/9c04b9c9-59da-46a7-93fb-33f8723854f3.png)
    equals ![](img/d72cb9b6-eebc-4311-a5e2-1b92b38cda46.png), as shown in the following
    plot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器分布由 ![](img/7d56ebaa-5f36-4aa5-8880-d2a08def9e42.png) 表示，而真实数据分布由 ![](img/b23d9151-6e6c-4297-a6e7-2be230ca72b2.png)
    表示。如果生成器完美地学习了真实数据分布，那么 ![](img/9c04b9c9-59da-46a7-93fb-33f8723854f3.png) 就等于
    ![](img/d72cb9b6-eebc-4311-a5e2-1b92b38cda46.png)，如下图所示：
- en: '![](img/20810e81-09fd-40d0-8334-8e7e0424784e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20810e81-09fd-40d0-8334-8e7e0424784e.png)'
- en: When ![](img/35c692e2-1874-4867-8f91-f077528e5653.png), then the discriminator
    cannot differentiate between whether the input image is from a real or a fake
    distribution, so it will just return 0.5 as a probability, as the discriminator
    will become confused between the two distributions when they are same.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/35c692e2-1874-4867-8f91-f077528e5653.png) 时，鉴别器无法区分输入图像是来自真实分布还是假分布，因此它会返回
    0.5 作为概率，因为鉴别器在这两个分布相同时会变得困惑。
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于生成器，最优判别器可以表示为：
- en: '![](img/de058172-fac0-4d5b-bfd2-5707bb869ca4.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de058172-fac0-4d5b-bfd2-5707bb869ca4.png)'
- en: So, when the discriminator just returns the probability of 0.5 for any image,
    then we can say that the generator has learned the distribution of images in our
    training set and fooled the discriminator successfully.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当鉴别器对任何图像返回概率为 0.5 时，我们可以说生成器已成功学习了我们训练集中图像的分布，并成功愚弄了鉴别器。
- en: Architecture of a GAN
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN 的架构
- en: 'The architecture of a GAN is shown in the following diagram:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的架构如下图所示：
- en: '![](img/e2ee99f3-d6b4-49dc-abc1-3f40a87b16ff.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2ee99f3-d6b4-49dc-abc1-3f40a87b16ff.png)'
- en: As shown in the preceding diagram, **Generator ![](img/ae577575-21e0-4777-b282-e13ce0963f43.png)**
    takes the random noise, ![](img/ea1a722d-53d4-454d-b7bc-57dfd371adbc.png), as
    input by sampling from a uniform or normal distribution and generates a fake image
    by implicitly learning the distribution of the training set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，**生成器 ![](img/ae577575-21e0-4777-b282-e13ce0963f43.png)** 以随机噪声 ![](img/ea1a722d-53d4-454d-b7bc-57dfd371adbc.png)
    作为输入，从均匀或正态分布中抽样，并通过隐式学习训练集的分布生成假图像。
- en: We sample an image, ![](img/f424ed1c-e52f-4f65-88f4-d631e306adf3.png), from
    the real data distribution, ![](img/94ad1212-c2b9-4868-9015-a247dac35a81.png),
    and fake data distribution, ![](img/c4c045b7-06a5-497a-bd75-1f70e20a544a.png),
    and feed it to the discriminator, ![](img/045b1ab3-9b82-4404-b976-33bf35de5dd3.png).
    We feed real and fake images to the discriminator and the discriminator performs
    a binary classification task. That is, it returns 0 when the image is fake and
    1 when the image is real.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从真实数据分布 ![](img/94ad1212-c2b9-4868-9015-a247dac35a81.png) 和假数据分布 ![](img/c4c045b7-06a5-497a-bd75-1f70e20a544a.png)
    中采样一幅图像 ![](img/f424ed1c-e52f-4f65-88f4-d631e306adf3.png)，并将其馈送给鉴别器 ![](img/045b1ab3-9b82-4404-b976-33bf35de5dd3.png)。我们将真实和假图像馈送给鉴别器，鉴别器执行二元分类任务。也就是说，当图像为假时，它返回
    0，当图像为真时返回 1。
- en: Demystifying the loss function
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘损失函数
- en: 'Now we will examine the loss function of GAN. Before going ahead, let''s recap
    the notations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将研究 GAN 的损失函数。在继续之前，让我们回顾一下符号：
- en: A noise that is fed as an input to the generator is represented by ![](img/8f3b3d59-89b3-4a8e-b661-af59d0a87d3c.png)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为生成器的输入的噪声由 ![](img/8f3b3d59-89b3-4a8e-b661-af59d0a87d3c.png) 表示。
- en: The uniform or normal distribution from which the noise ![](img/087d84d8-ffc2-435f-bb18-271187a2a2e0.png)is
    sampled is represented by ![](img/25204cbc-e786-49e4-a289-95bb4f4c33d5.png)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声 ![](img/087d84d8-ffc2-435f-bb18-271187a2a2e0.png) 抽样自的均匀或正态分布由 ![](img/25204cbc-e786-49e4-a289-95bb4f4c33d5.png)
    表示。
- en: An input image is represented by ![](img/6bc56732-9aa1-4d55-9b70-420d868e29db.png)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像由 ![](img/6bc56732-9aa1-4d55-9b70-420d868e29db.png) 表示。
- en: Real data distribution or distribution of our training set is represented by
    ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实数据分布或我们训练集的分布由 ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png) 表示。
- en: Fake data distribution or distribution of the generator is represented by ![](img/809aa100-6d70-4464-a1bd-638439152c15.png)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假数据分布或生成器的分布由 ![](img/809aa100-6d70-4464-a1bd-638439152c15.png) 表示。
- en: When we write ![](img/d41cdb15-ed21-4f3a-8dde-bf3dcdad7df5.png), it implies
    that image ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png) is sampled from the
    real distribution, ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png). Similarly,
    ![](img/12c7a854-fe82-450b-b5ca-35b8db0e8abd.png) denotes that image ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png)
    is sampled from the generator distribution, ![](img/ad274aa3-a892-48fc-b307-d383dd37900c.png),
    and ![](img/2801a0f3-9ea3-4bb4-a77a-44fd75f11ec7.png) implies that the generator
    input, ![](img/4fbf1423-a85d-4d32-b61d-f11f3c96e664.png), is sampled from the
    uniform distribution, ![](img/1ffabeaf-b836-4798-a3e8-b711289a0ab1.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们写下 ![](img/d41cdb15-ed21-4f3a-8dde-bf3dcdad7df5.png) 时，意味着图像 ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png)
    是从真实分布 ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png) 中采样得到的。同样，![](img/12c7a854-fe82-450b-b5ca-35b8db0e8abd.png)
    表示图像 ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png) 是从生成器分布 ![](img/ad274aa3-a892-48fc-b307-d383dd37900c.png)
    中采样得到的，而 ![](img/2801a0f3-9ea3-4bb4-a77a-44fd75f11ec7.png) 则意味着生成器输入 ![](img/4fbf1423-a85d-4d32-b61d-f11f3c96e664.png)
    是从均匀分布 ![](img/1ffabeaf-b836-4798-a3e8-b711289a0ab1.png) 中采样得到的。
- en: We've learned that both the generator and discriminator are neural networks
    and both of them update their parameters through backpropagation. We now need
    to find the optimal generator parameter, ![](img/c6200791-dccf-4a07-bf58-7dab7af2f07d.png),
    and the discriminator parameter, ![](img/e36cb046-2690-4a0a-b80f-784f84e5ae16.png).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习到生成器和判别器都是神经网络，并且它们通过反向传播来更新参数。现在我们需要找到最优的生成器参数 ![](img/c6200791-dccf-4a07-bf58-7dab7af2f07d.png)
    和判别器参数 ![](img/e36cb046-2690-4a0a-b80f-784f84e5ae16.png)。
- en: Discriminator loss
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器损失
- en: Now we will look at the loss function of the discriminator. We know that the
    goal of the discriminator is to classify whether the image is a real or a fake
    image. Let's denote discriminator by ![](img/b0cfa043-b53b-44c1-81f9-8212e217f4f5.png).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看判别器的损失函数。我们知道判别器的目标是分类图像是真实图像还是假图像。让我们用 ![](img/b0cfa043-b53b-44c1-81f9-8212e217f4f5.png)
    表示判别器。
- en: 'The loss function of the discriminator is given as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的损失函数如下所示：
- en: '![](img/6d7532f0-4b05-4cec-a557-b0e09704751b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d7532f0-4b05-4cec-a557-b0e09704751b.png)'
- en: What does this mean, though? Let's understand each of the terms one by one.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这意味着什么呢？让我们逐一理解每一项的含义。
- en: First term
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一项
- en: 'Let''s look at the first term:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看第一项：
- en: '![](img/eb8e27a3-3ccc-4e8a-9ab1-fbac423487d2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb8e27a3-3ccc-4e8a-9ab1-fbac423487d2.png)'
- en: Here, ![](img/a3651ead-3b34-4a33-b0dd-6c614a82aaa7.png) implies that we are
    sampling input ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) from the real
    data distribution, ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png), so ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    is a real image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a3651ead-3b34-4a33-b0dd-6c614a82aaa7.png) 表示我们从真实数据分布 ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png)
    中采样输入 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)，因此 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    是一张真实图像。
- en: '![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png) implies that we are feeding
    the input image ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) to the discriminator
    ![](img/908d4df5-0cf6-4c8e-82b3-410b8a1a2552.png), and the discriminator will
    return the probability of input image ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    to be a real image. As ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) is sampled
    from real data distribution ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png),
    we know that ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) is a real image.
    So, we need to maximize the probability of ![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png) 表示我们将输入图像 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    提交给判别器 ![](img/908d4df5-0cf6-4c8e-82b3-410b8a1a2552.png)，并且判别器将返回输入图像 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    是真实图像的概率。由于 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) 是从真实数据分布 ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png)
    中采样得到的，我们知道 ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) 是一张真实图像。因此，我们需要最大化
    ![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png) 的概率：'
- en: '![](img/fc9b7dd8-32c0-41d8-a88e-b26e643c90c0.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc9b7dd8-32c0-41d8-a88e-b26e643c90c0.png)'
- en: 'But instead of maximizing raw probabilities, we maximize log probabilities;
    as we learned in [Chapter 7](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml), *Learning
    Text Representations*, we can write the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们不是最大化原始概率，而是最大化对数概率；正如我们在 [第7章](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml)
    中学到的，*学习文本表示*，我们可以写成以下形式：
- en: '![](img/0f1ed11f-731b-4773-95e6-29a13778e98a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f1ed11f-731b-4773-95e6-29a13778e98a.png)'
- en: 'So, our final equation becomes the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终方程如下：
- en: '![](img/20ada8c6-4d18-4304-9b6b-1e34b581da5d.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20ada8c6-4d18-4304-9b6b-1e34b581da5d.png)'
- en: '![](img/473ff433-208c-4241-b05e-27790745817b.png) implies the expectations
    of the log likelihood of input images sampled from the real data distribution
    being real.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/473ff433-208c-4241-b05e-27790745817b.png)意味着来自真实数据分布的输入图像的对数似然期望是真实的。'
- en: Second term
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二项
- en: 'Now, let''s look at the second term:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看第二项：
- en: '![](img/f1973703-b496-4ebe-a4c4-9ac27017c421.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1973703-b496-4ebe-a4c4-9ac27017c421.png)'
- en: Here, ![](img/de1d3506-7c7b-443c-b694-51f180dd1429.png) implies that we are
    sampling a random noise ![](img/225c3938-71c7-4484-820c-d516e6776803.png) from
    the uniform distribution ![](img/904684da-46cd-4a21-a34e-46e09f8d0da4.png). ![](img/b07bd0b3-3701-448a-b3f8-53881193c5f8.png)
    implies that the generator ![](img/e6e3fb39-57de-4c67-8cdf-ab528da442ad.png) takes
    the random noise ![](img/225c3938-71c7-4484-820c-d516e6776803.png) as an input
    and returns a fake image based on its implicitly learned distribution ![](img/6d4b20b2-c0bb-4bea-aa9e-67cb0d4497d0.png).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/de1d3506-7c7b-443c-b694-51f180dd1429.png)意味着我们从均匀分布中采样随机噪声![](img/225c3938-71c7-4484-820c-d516e6776803.png)。![](img/b07bd0b3-3701-448a-b3f8-53881193c5f8.png)意味着生成器![](img/e6e3fb39-57de-4c67-8cdf-ab528da442ad.png)将随机噪声![](img/225c3938-71c7-4484-820c-d516e6776803.png)作为输入，并根据其隐式学习的分布返回假图像![](img/6d4b20b2-c0bb-4bea-aa9e-67cb0d4497d0.png)。
- en: '![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png) implies that we are feeding
    the fake image generated by the generator to the discriminator ![](img/c50e7ac5-e821-4f32-a3e9-98fa46869323.png)
    and it will return the probability of the fake input image being a real image.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png)意味着我们将生成器生成的假图像![](img/c50e7ac5-e821-4f32-a3e9-98fa46869323.png)输入到鉴别器中，并返回假输入图像是真实图像的概率。'
- en: 'If we subtract ![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png) from 1, then
    it will return the probability of the fake input image being a fake image:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从1中减去![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png)，那么它将返回假输入图像是假图像的概率：
- en: '![](img/071a1911-3caf-4564-83ab-e2b2fa9338db.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/071a1911-3caf-4564-83ab-e2b2fa9338db.png)'
- en: 'Since we know ![](img/225c3938-71c7-4484-820c-d516e6776803.png) is not a real
    image, the discriminator will maximize this probability. That is, the discriminator
    maximizes the probability of ![](img/225c3938-71c7-4484-820c-d516e6776803.png)
    being classified as a fake image, so we write:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道![](img/225c3938-71c7-4484-820c-d516e6776803.png)不是真实图像，鉴别器将最大化这个概率。也就是说，鉴别器最大化![](img/225c3938-71c7-4484-820c-d516e6776803.png)被分类为假图像的概率，因此我们写成：
- en: '![](img/f59c4f1f-2250-4686-8b1b-5975cb5670c9.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f59c4f1f-2250-4686-8b1b-5975cb5670c9.png)'
- en: 'Instead of maximizing raw probabilities, we maximize the log probability:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是最大化原始概率，而是最大化对数概率：
- en: '![](img/03889e75-e14c-4337-8298-7c70686134dc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03889e75-e14c-4337-8298-7c70686134dc.png)'
- en: '![](img/0a57ca04-134f-478d-92e6-d8b43a254a23.png) implies the expectations
    of the log likelihood of the input images generated by the generator being fake.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/0a57ca04-134f-478d-92e6-d8b43a254a23.png)意味着生成器生成的输入图像的对数似然期望是假的。'
- en: Final term
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终项
- en: 'So, combining these two terms, the loss function of the discriminator is given
    as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结合这两项，鉴别器的损失函数如下所示：
- en: '![](img/6ed40285-82fc-4dab-b842-d806edb3a9ab.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ed40285-82fc-4dab-b842-d806edb3a9ab.png)'
- en: Here, ![](img/e5699988-77b0-4be9-b297-db5eac997077.png) and ![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)
    are the parameters of the generator and discriminator network respectively. So,
    the discriminator's goal is to find the right ![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)
    so that it can classify the image correctly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/e5699988-77b0-4be9-b297-db5eac997077.png)和![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)分别是生成器和鉴别器网络的参数。因此，鉴别器的目标是找到正确的![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)，使其能够正确分类图像。
- en: Generator loss
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The loss function of the generator is given as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下所示：
- en: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
- en: We know that the goal of the generator is to fool the discriminator to classify
    the fake image as a real image.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器的目标是欺骗鉴别器将假图像分类为真实图像。
- en: In the *Discriminator loss* section, we saw that ![](img/81a6a38b-a8fa-4963-8aa9-51d6a666e68f.png)
    implies the probability of classifying the fake input image as a fake image, and
    the discriminator maximizes the probabilities for correctly classifying the fake
    image as fake.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在*鉴别器损失*部分，我们看到![](img/81a6a38b-a8fa-4963-8aa9-51d6a666e68f.png)意味着将假输入图像分类为假图像的概率，并且鉴别器最大化这些概率以正确分类假图像为假。
- en: 'But the generator wants to minimize this probability. As the generator wants
    to fool the discriminator, it minimizes this probability of a fake input image
    being classified as fake by the discriminator. Thus, the loss function of the
    generator can be expressed as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但是生成器希望最小化这个概率。由于生成器想要愚弄判别器，它最小化了判别器将假输入图像分类为假的概率。因此，生成器的损失函数可以表达为以下形式：
- en: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
- en: Total loss
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总损失
- en: 'We just learned the loss function of the generator and the discriminator combining
    these two losses, and we write our final loss function as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了生成器和判别器的损失函数，结合这两个损失函数，我们将我们的最终损失函数写成如下形式：
- en: '![](img/fed2c57d-3af6-4f68-9fe8-9d1110e591c6.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fed2c57d-3af6-4f68-9fe8-9d1110e591c6.png)'
- en: So, our objective function is basically a min-max objective function, that is,
    a maximization for the discriminator and minimization for the generator, and we
    find the optimal generator parameter, ![](img/ac253f96-014e-4bfa-95dd-1141d2bb5483.png),
    and discriminator parameter, ![](img/b9808400-7f5c-4f38-85b1-25b5d7382c2a.png),
    through backpropagating the respective networks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标函数基本上是一个最小最大化目标函数，也就是说，判别器的最大化和生成器的最小化，我们通过反向传播各自的网络来找到最优的生成器参数 ![](img/ac253f96-014e-4bfa-95dd-1141d2bb5483.png)
    和判别器参数 ![](img/b9808400-7f5c-4f38-85b1-25b5d7382c2a.png)。
- en: 'So, we perform gradient ascent; that is, maximization on the discriminator:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们执行梯度上升；也就是说，判别器的最大化：
- en: '![](img/8e914221-73af-4fb1-9ee4-a36b089c0615.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e914221-73af-4fb1-9ee4-a36b089c0615.png)'
- en: 'And, we perform gradient descent; that is, minimization on the generator:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，我们执行梯度下降；也就是说，生成器的最小化：
- en: '![](img/c40edd42-f248-4765-89a8-e61a6d456b58.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c40edd42-f248-4765-89a8-e61a6d456b58.png)'
- en: However, optimizing the preceding generator objective function does not work
    properly and causes a stability issue. So, we introduce a new form of loss called
    **heuristic loss**.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，优化上述生成器的目标函数并不能有效地工作，导致了稳定性问题。因此，我们引入了一种称为**启发式损失**的新形式的损失。
- en: Heuristic loss
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启发式损失
- en: 'There is no change in the loss function of the discriminator. It can be directly
    written as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的损失函数没有变化。可以直接写成如下形式：
- en: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
- en: 'Now, let''s look at the generator loss:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下生成器的损失：
- en: '![](img/67836c00-6eb9-40e4-9f5b-ff6060cdbf1c.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67836c00-6eb9-40e4-9f5b-ff6060cdbf1c.png)'
- en: Can we change the minimization objective in our generator loss function into
    a maximization objective just like the discriminator loss? How can we do that?
    We know that ![](img/52f988fd-6940-40f1-934e-5a2c8e24e64e.png) returns the probability
    of the fake input image being fake, and the generator is minimizing this probability.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否像判别器的损失函数一样将生成器的损失函数的最小化目标改为最大化目标呢？我们如何做到这一点呢？我们知道 ![](img/52f988fd-6940-40f1-934e-5a2c8e24e64e.png)
    返回假输入图像被分类为假的概率，而生成器正在最小化这个概率。
- en: 'Instead of doing this, we can write ![](img/38bf4fa0-4f12-40e9-b554-820f7c851ce3.png).
    It implies the probability of the fake input image being real, and now the generator
    can maximize this probability. It implies a generator is maximizing the probability
    of the fake input image being classified as a real image. So, the loss function
    of our generator now becomes the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是这样做，我们可以写成 ![](img/38bf4fa0-4f12-40e9-b554-820f7c851ce3.png)。这意味着假输入图像被分类为真实的概率，现在生成器可以最大化这个概率。这意味着生成器正在最大化假输入图像被判别器分类为真实图像的概率。因此，我们的生成器的损失函数现在变成了以下形式：
- en: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
- en: 'So now, we have both the loss function of our discriminator and generator in
    maximizing terms:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经将判别器和生成器的损失函数都转化为最大化的术语：
- en: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
- en: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
- en: But, instead of maximizing, if we can minimize the loss, then we can apply our
    favorite gradient descent algorithm. So, how can we convert our maximizing problem
    into a minimization problem? We can do that by simply adding a negative sign.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们可以将最大化问题转化为最小化问题，那么我们就可以应用我们喜爱的梯度下降算法。那么，我们如何将我们的最大化问题转化为最小化问题呢？我们可以通过简单地添加负号来实现这一点。
- en: 'So, our final loss function for the discriminator is given as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们判别器的最终损失函数如下所示：
- en: '![](img/d8551661-694c-4721-95c0-8152c5e22bee.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8551661-694c-4721-95c0-8152c5e22bee.png)'
- en: 'Also, the generator loss is given as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成器的损失如下所示：
- en: '![](img/1e5b9f08-9505-47d9-93bb-8268f9e08ad4.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e5b9f08-9505-47d9-93bb-8268f9e08ad4.png)'
- en: Generating images using GANs in TensorFlow
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用 GAN 生成图像
- en: Let's strengthen our understanding of GANs by building them to generate handwritten
    digits in TensorFlow. You can also check the complete code used in this section
    here at [http://bit.ly/2wwBvRU](http://bit.ly/2wwBvRU).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在 TensorFlow 中构建 GAN 来加深对生成手写数字的理解。您也可以在这一节的完整代码[这里](http://bit.ly/2wwBvRU)查看。
- en: 'First, we will import all the necessary libraries:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所有必要的库：
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Reading the dataset
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据集
- en: 'Load the MNIST dataset:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集：
- en: '[PRE1]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s plot one image:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一张图像：
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The input image looks as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像如下所示：
- en: '![](img/5c9ebcf0-0a0a-4208-8474-d612d72ef1ef.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c9ebcf0-0a0a-4208-8474-d612d72ef1ef.png)'
- en: Defining the generator
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器
- en: 'Generator ![](img/4434a071-305a-479d-a8c5-0bd22491c71f.png) takes the noise
    ![](img/741c2881-b8b5-4486-b60a-a04cdea42a8e.png) as an input and returns an image.
    We define the generator as a feedforward network with three layers. Instead of
    coding the generator network from scratch, we can use `tf.layers.dense()`, which
    can be used to create a dense layer. It takes three parameters: `inputs`, the
    number of `units`, and the `activation` function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器 ![](img/4434a071-305a-479d-a8c5-0bd22491c71f.png) 接受噪声 ![](img/741c2881-b8b5-4486-b60a-a04cdea42a8e.png)
    作为输入，并返回图像。我们将生成器定义为一个三层的前馈网络。而不是从头编写生成器网络，我们可以使用 `tf.layers.dense()`，它可以用来创建一个密集层。它接受三个参数：`inputs`、`units`
    的数量和 `activation` 函数：
- en: '[PRE3]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining the discriminator
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器
- en: 'We know that discriminator ![](img/41441aa5-777c-4f31-98b6-5a0473ab3d5f.png)
    returns the probability of the given image being real. We define the discriminator
    also as a feedforward network with three layers:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道鉴别器 ![](img/41441aa5-777c-4f31-98b6-5a0473ab3d5f.png) 返回给定图像为真实的概率。我们也定义鉴别器作为一个三层的前馈网络：
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the input placeholders
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义输入占位符
- en: 'Now we define the `placeholder` for the input ![](img/86e6d30c-84c5-4bc9-8835-6a14f17e6412.png)
    and the noise ![](img/0d340c85-feb6-46d2-8925-71dae6013f74.png):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义输入 ![](img/86e6d30c-84c5-4bc9-8835-6a14f17e6412.png) 和噪声 ![](img/0d340c85-feb6-46d2-8925-71dae6013f74.png)
    的占位符：
- en: '[PRE5]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Starting the GAN!
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始 GAN！
- en: 'First, we feed the noise ![](img/54c257b7-0532-4089-8b03-8639d2deef7b.png)
    to the generator and it will output the fake image, [![](img/2b698d44-e58f-4fe3-a804-704bf6bca446.png)]:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将噪声输入生成器，并输出假图像，[![](img/2b698d44-e58f-4fe3-a804-704bf6bca446.png)]：
- en: '[PRE6]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we feed the real image to discriminator [![](img/bc3e35f7-f069-41f7-a2e5-c299aae82489.png)]
    and get the probability of the real image being real:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将真实图像提供给鉴别器 [![](img/bc3e35f7-f069-41f7-a2e5-c299aae82489.png)] 并得到真实图像为真的概率：
- en: '[PRE7]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Similarly, we feed the fake image to discriminator ![](img/bc3e35f7-f069-41f7-a2e5-c299aae82489.png)
    and get the probability of the fake image being real:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将假图像提供给鉴别器，并获得假图像为真的概率：
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Computing the loss function
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失函数
- en: Now, we will see how to compute the loss function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何计算损失函数。
- en: Discriminator loss
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器损失
- en: 'The discriminator loss is given as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器损失如下所示：
- en: '![](img/4572482b-20a3-4e23-9ffe-92bf8e4a724a.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4572482b-20a3-4e23-9ffe-92bf8e4a724a.png)'
- en: First, we will implement the first term, ![](img/0a0f6283-d7d7-40f5-a53e-32183646f0cd.png).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现第一项，![](img/0a0f6283-d7d7-40f5-a53e-32183646f0cd.png)。
- en: The first term, ![](img/56f909fb-f334-40ec-9615-1112af4f1d74.png), implies the
    expectations of the log likelihood of images sampled from the real data distribution
    being real.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项，![](img/56f909fb-f334-40ec-9615-1112af4f1d74.png)，意味着从真实数据分布中采样的图像的对数似然的期望是真实的。
- en: 'It is basically the binary cross-entropy loss. We can implement binary cross-entropy
    loss with the `tf.nn.sigmoid_cross_entropy_with_logits()` TensorFlow function.
    It takes two parameters as inputs, `logits` and `labels`, explained as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是二元交叉熵损失。我们可以使用 TensorFlow 函数 `tf.nn.sigmoid_cross_entropy_with_logits()`
    实现二元交叉熵损失。它接受两个参数作为输入，`logits` 和 `labels`，如下所述：
- en: The `logits` input, as the name suggests, is the logits of the network so it
    is `D_logits_real`.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` 输入，顾名思义，是网络的 logits，因此它是 `D_logits_real`。'
- en: The `labels` input, as the name suggests, is the true label. We learned that
    discriminator should return `1` for real images and `0` for fake images. Since
    we are calculating the loss for input images sampled from the real data distribution,
    the true label is `1`.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` 输入，顾名思义，是真实标签。我们了解到鉴别器应该对真实图像返回 `1`，对假图像返回 `0`。由于我们正在计算来自真实数据分布的输入图像的损失，真实标签是
    `1`。'
- en: We use `tf.ones_likes()` for setting the labels to 1 with the same shape as
    `D_logits_real`. That is, `labels = tf.ones_like(D_logits_real)`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.ones_like()` 将标签设置为与 `D_logits_real` 相同形状的 `1`。
- en: 'Then we compute the mean loss using `tf.reduce_mean()`. If you notice, there
    is a minus sign in our loss function, which we added for converting our loss to
    a minimization objective. But, in the following code, there is no minus sign,
    because TensorFlow optimizers will only minimize and not maximize. So we don''t
    have to add minus sign in our implementation because in any case, it will be minimized
    by the TensorFlow optimizer:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 `tf.reduce_mean()` 计算平均损失。如果注意到我们的损失函数中有一个减号，这是为了将我们的损失转换为最小化目标。但在下面的代码中，没有减号，因为
    TensorFlow 优化器只会最小化而不会最大化。因此，在我们的实现中不需要添加减号，因为无论如何，TensorFlow 优化器都会将其最小化：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we will implement the second term, ![](img/42a79faf-bf44-48ec-a5ea-ad1386a8fdef.png).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现第二项，![](img/42a79faf-bf44-48ec-a5ea-ad1386a8fdef.png)。
- en: The second term,![](img/26a031cb-78f8-4c9b-af0a-2ce59962a0bc.png), implies the
    expectations of the log likelihood of images generated by the generator being
    fake.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项，![](img/26a031cb-78f8-4c9b-af0a-2ce59962a0bc.png)，意味着生成器生成的图像被分类为假图像的对数似然期望。
- en: 'Similar to the first term, we can use `tf.nn.sigmoid_cross_entropy_with_logits()`
    for calculating the binary cross-entropy loss. In this, the following holds true:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于第一项，我们可以使用 `tf.nn.sigmoid_cross_entropy_with_logits()` 计算二元交叉熵损失。在此，以下内容成立：
- en: Logits is `D_logits_fake`
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logits 是 `D_logits_fake`。
- en: Since we are calculating the loss for the fake images generated by the generator,
    the `true` label is `0`
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们正在计算生成器生成的假图像的损失，所以 `true` 标签为 `0`。
- en: 'We use `tf.zeros_like()` for setting the labels to `0` with the same shape
    as `D_logits_fake`. That is, `labels = tf.zeros_like(D_logits_fake)`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.zeros_like()` 将标签设置为与 `D_logits_fake` 相同形状的 `0`。也就是说，`labels = tf.zeros_like(D_logits_fake)`：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we will implement the final loss.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现最终的损失。
- en: 'So, combining the preceding two terms, the loss function of the discriminator
    is given as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结合前述两项，判别器的损失函数如下所示：
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Generator loss
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: The generator loss is given as ![](img/740c5960-10db-49c3-8778-8750cb0841ce.png).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失如下给出：![](img/740c5960-10db-49c3-8778-8750cb0841ce.png)。
- en: It implies the probability of the fake image being classified as a real image.
    As we calculated binary cross-entropy in the discriminator, we use `tf.nn.sigmoid_cross_entropy_with_logits()`
    for calculating the loss in the generator.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 它意味着假图像被分类为真实图像的概率。正如我们在判别器中计算二元交叉熵一样，我们在生成器中使用 `tf.nn.sigmoid_cross_entropy_with_logits()`
    计算损失时这一点成立。
- en: 'Here, the following should be borne in mind:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，应注意以下事项：
- en: Logits is `D_logits_fake`.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logits 是 `D_logits_fake`。
- en: Since our loss implies the probability of the fake input image being classified
    as real, the true label is 1\. Because, as we learned, the goal of the generator
    is to generate the fake image and fool the discriminator to classify the fake
    image as a real image.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们的损失意味着生成器生成的假输入图像被分类为真实的概率，所以真实标签为 1。因为正如我们所学到的，生成器的目标是生成假图像并欺骗判别器将假图像分类为真实图像。
- en: 'We use `tf.ones_like()` for setting the labels to 1 with the same shape as
    `D_logits_fake`. That is, `labels = tf.ones_like(D_logits_fake)`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.ones_like()` 将标签设置为与 `D_logits_fake` 相同形状的 `1`。也就是说，`labels = tf.ones_like(D_logits_fake)`：
- en: '[PRE12]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Optimizing the loss
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化损失
- en: 'Now we need to optimize our generator and discriminator. So, we collect the
    parameters of the discriminator and generator as ![](img/7f53317e-2841-4d75-8f6e-8f48eb06d64f.png)
    and ![](img/97d5581a-263f-499a-995e-fda7972d0871.png) respectively:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要优化我们的生成器和判别器。因此，我们分别收集判别器和生成器的参数为 ![](img/7f53317e-2841-4d75-8f6e-8f48eb06d64f.png)
    和 ![](img/97d5581a-263f-499a-995e-fda7972d0871.png)：
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Optimize the loss using the Adam optimizer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器优化损失：
- en: '[PRE14]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Starting the training
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始训练
- en: 'Let''s begin the training by defining the batch size and the number of epochs:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过定义批量大小和 epoch 数量来开始训练：
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize all the variables:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Generating handwritten digits
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成手写数字
- en: 'Start the TensorFlow session and generate handwritten digits:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话并生成手写数字：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Initialize all variables:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To execute this for each epoch:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 epoch 执行以下操作：
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Select the number of batches:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 选择批量数：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To execute this for each batch:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个批次执行以下操作：
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Get the batch of data according to the batch size:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 根据批量大小获取数据批次：
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reshape the data:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑数据：
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Sample the batch noise:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对批量噪声进行采样：
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the feed dictionaries with input `x` as `batch_images` and noise `z`
    as `batch_noise`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用输入 `x` 作为 `batch_images` 和噪声 `z` 作为 `batch_noise` 来定义喂入字典：
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Train the discriminator and generator:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 训练判别器和生成器：
- en: '[PRE26]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Compute loss of discriminator and generator:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 计算判别器和生成器的损失：
- en: '[PRE27]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Feed the noise to a generator on every 100^(th) epoch and generate an image:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 每个第 100^(th) epoch 将噪声输入生成器，并生成一幅图像：
- en: '[PRE28]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'During training, we notice how loss decreases and how GANs learn to generate
    images as shown follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们注意到损失如何减少，以及 GAN 如何学习生成图像，如下所示：
- en: '![](img/92663e9d-4cab-43d1-b46f-ebe087e99064.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92663e9d-4cab-43d1-b46f-ebe087e99064.png)'
- en: DCGAN – Adding convolution to a GAN
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCGAN – 在 GAN 中添加卷积
- en: We just learned how effective GANs are and how can they be used to generate
    images. We know that a GAN has two components generators that generate the image
    and the discriminator, which acts as a critic to the generated image. As you can
    see, both of these generators and discriminators are basically feedforward neural
    networks. Instead of keeping them as feedforward networks, can we use convolutional
    networks?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了 GAN 的有效性以及它们如何用于生成图像。我们知道 GAN 包括两个组件：生成器用于生成图像，判别器用作对生成图像的批评者。正如您所见，这些生成器和判别器都是基本的前馈神经网络。与其将它们保留为前馈网络，不如使用卷积网络？
- en: In [Chapter 6](af0b9e75-a9a0-4bc7-ad23-92d2ac4a2629.xhtml), *Demystifying Convolutional
    Networks*, we have seen the effectiveness of convolutional networks for image-based
    data and how they extract features from images in an unsupervised fashion. Since
    in GANs we are generating images, it is desirable to use convolutional networks
    instead of feedforward networks. So, we introduce a new type of GAN called **DCGAN**.
    It extends the design of GANs with convents. We basically replace the feedforward
    network in the generator and discriminator with a **Convolutional Neural Network**
    (**CNN**).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6 章](af0b9e75-a9a0-4bc7-ad23-92d2ac4a2629.xhtml)《解密卷积网络》中，我们已经看到了卷积网络在基于图像的数据上的有效性，以及它们如何以无监督的方式从图像中提取特征。由于在
    GAN 中我们正在生成图像，因此使用卷积网络而不是前馈网络是可取的。因此，我们引入了一种新型 GAN，称为 **DCGAN**。它通过卷积扩展了 GAN 的设计。我们基本上用卷积神经网络
    (**CNN**) 替换了生成器和判别器中的前馈网络。
- en: The discriminator uses convolutional layers for classifying the image as a fake
    or real image, while the generator uses convolutional transpose layers to generate
    a new image. Now we will go into detail and see how generators and discriminators
    differ in DCGAN compared to the vanilla GANs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器使用卷积层来对图像进行分类，判断是真实图像还是假图像，而生成器使用卷积转置层来生成新图像。现在我们将详细探讨生成器和判别器在 DCGAN 中与传统
    GAN 相比的区别。
- en: Deconvolutional generator
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反卷积生成器
- en: We know that the role of the generator is to generate a new image by learning
    the real data distribution. In DCGAN, a generator is composed of convolutional
    transpose and batch norm layers with ReLU activations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的作用是通过学习真实数据分布来生成新的图像。在 DCGAN 中，生成器由卷积转置层和批量归一化层组成，激活函数为 ReLU。
- en: Note that convolutional transpose operation is also known as deconvolution operation
    or fractionally strided convolutions.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，卷积转置操作也称为反卷积操作或分数步长卷积。
- en: The input to the generator is the noise, ![](img/5e1d4f79-f5c2-41d3-8f9d-ecbf4c3611a7.png),
    which we draw from a standard normal distribution, and it outputs an image of
    the same size as the images in the training data, say, 64 x 64 x 3.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的输入是从标准正态分布中抽取的噪声 ![](img/5e1d4f79-f5c2-41d3-8f9d-ecbf4c3611a7.png)，它输出与训练数据中图像相同大小的图像，例如
    64 x 64 x 3。
- en: 'The architecture of the generator is shown in the following diagram:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的架构如下图所示：
- en: '![](img/f7f65bb2-09ba-4fac-9de4-e82461ceddc5.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7f65bb2-09ba-4fac-9de4-e82461ceddc5.png)'
- en: First, we convert the noise, *z*, of a 100 x 1 shape into 1024 x 4 x 4 to have
    the shape of width, height, and feature map and it is called the **project and
    reshape**. Following this, we perform a series of convolutional operations with
    fractionally strided convolutions. We apply batch normalization to every layer
    except at the last layer. Also, we apply ReLU activations to every layer but the
    last layer. We apply the tanh activation function to scale the generated image
    between -1 and +1.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将形状为 100 x 1 的噪声 *z* 转换为 1024 x 4 x 4，以获得宽度、高度和特征图的形状，并称之为 **投影和重塑**。随后，我们执行一系列卷积操作，使用分数步长卷积。我们在每一层除最后一层外都应用批量归一化。此外，我们在每一层除最后一层外都应用
    ReLU 激活函数。我们应用 tanh 激活函数来将生成的图像缩放到 -1 到 +1 之间。
- en: Convolutional discriminator
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积判别器
- en: Now we will see the architecture of a discriminator in DCGAN. As we know, the
    discriminator takes the image and it tells us whether the image is a real image
    or a fake image. Thus, it is basically a binary classifier. The discriminator
    is composed of a series of convolutional and batch norm layers with leaky ReLU
    activations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到DCGAN中判别器的架构。正如我们所知，判别器接收图像并告诉我们图像是真实图像还是伪造图像。因此，它基本上是一个二元分类器。判别器由一系列卷积和批量归一化层以及泄漏ReLU激活组成。
- en: 'The architecture of the discriminator is shown in the following diagram:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的架构如下图所示：
- en: '![](img/4d66521c-2497-49ca-97fc-d969707b1fa5.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d66521c-2497-49ca-97fc-d969707b1fa5.png)'
- en: As you can see, it takes the input image of the 64 x 64 x 3 shape and performs
    a series of convolutional operations with a leaky ReLU activation function. We
    apply batch normalization at all layers except at the input layer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它接收大小为64 x 64 x 3的输入图像，并使用泄漏ReLU激活函数进行一系列卷积操作。我们在所有层上都应用批量归一化，除了输入层。
- en: Remember, we don't apply a max pooling operation in both the discriminator and
    the generator. Instead, we apply a strided convolution operation (that is convolution
    operation with strides).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在判别器和生成器中我们不使用最大池化操作。相反，我们应用了步幅卷积操作（即带步幅的卷积操作）。
- en: In a nutshell, we enhance the vanilla GAN by replacing the feedforward network
    in the generator and the discriminator with the convolutional network.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，我们通过用卷积网络替换生成器和判别器中的前馈网络来增强香草GAN。
- en: Implementing DCGAN to generate CIFAR images
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DCGAN以生成CIFAR图像
- en: Now we will see how to implement DCGAN in TensorFlow. We will learn how to use
    DCGAN with images from the **Canadian Institute For Advanced Research** (**CIFAR**)-10
    dataset. CIFAR-10 consists of 60,000 images from 10 different classes that include
    airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We
    will examine how we can use DCGAN to generate such images.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何在TensorFlow中实现DCGAN。我们将学习如何使用来自**加拿大高级研究所**（**CIFAR**）-10数据集的图像。CIFAR-10包含来自10个不同类别的60,000张图像，包括飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。我们将探讨如何使用DCGAN生成这些图像。
- en: 'First, import the required libraries:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入所需的库：
- en: '[PRE29]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Exploring the dataset
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据集
- en: 'Load the CIFAR dataset:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 加载CIFAR数据集：
- en: '[PRE30]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s see what we have in our dataset. Define a helper function for plotting
    the image:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据集中有什么。定义一个用于绘制图像的辅助函数：
- en: '[PRE31]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s plot a few images:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制几张图像：
- en: '[PRE32]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The plotted images are shown as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的图像如下所示：
- en: '![](img/6d1d0f1c-2f2e-4917-a53e-bcb70e87aacd.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d1d0f1c-2f2e-4917-a53e-bcb70e87aacd.png)'
- en: Defining the discriminator
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义判别器
- en: 'We define a discriminator as a convolutional network with three convolutional
    layers followed by a fully connected layer. It is composed of a series of convolutional
    and batch norm layers with leaky ReLU activations. We apply batch normalization
    at all layers except at the input layer:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将判别器定义为一个带有三个卷积层和一个全连接层的卷积网络。它由一系列卷积和批量归一化层以及泄漏ReLU激活组成。我们在所有层上都应用批量归一化，除了输入层：
- en: '[PRE33]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'First convolutional layer with leaky ReLU activation:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个带有泄漏ReLU激活的卷积层：
- en: '[PRE34]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Second convolutional layer with batch normalization and leaky ReLU activation:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个带有批量归一化和泄漏ReLU激活的卷积层：
- en: '[PRE35]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Third convolutional layer with batch normalization and leaky ReLU:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个带有批量归一化和泄漏ReLU的卷积层：
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Flatten the output of the final convolutional layer:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 展平最终卷积层的输出：
- en: '[PRE37]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define the fully connected layer and return the `logits`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 定义全连接层并返回`logits`：
- en: '[PRE38]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Defining the generator
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器
- en: 'As we learned, the generator performs the transpose convolutional operation.
    The generator is composed of convolutional transpose and batch norm layers with
    ReLU activations. We apply batch normalization to every layer except for the last
    layer. Also, we apply ReLU activations to every layer, but for the last layer,
    we apply the `tanh` activation function to scale the generated image between -1
    and +1:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们学到的那样，生成器执行转置卷积操作。生成器由卷积转置和批量归一化层以及ReLU激活组成。我们在每一层都应用批量归一化，但是对于最后一层，我们应用`tanh`激活函数将生成的图像缩放到-1到+1之间：
- en: '[PRE39]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'First fully connected layer:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个全连接层：
- en: '[PRE40]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Convert the shape of the input and apply batch normalization followed by ReLU
    activations:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 转换输入的形状并应用批量归一化，然后应用ReLU激活：
- en: '[PRE41]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The second layer, that is the transpose convolution layer, with batch normalization
    and the ReLU activation:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层是转置卷积层，带有批量归一化和 ReLU 激活：
- en: '[PRE42]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the third layer:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第三层：
- en: '[PRE43]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the fourth layer:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第四层：
- en: '[PRE44]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the final layer, we don''t apply batch normalization and instead of ReLU,
    we use `tanh` activation:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一层，我们不应用批量归一化，而是使用 `tanh` 激活函数代替 ReLU：
- en: '[PRE45]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Defining the inputs
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义输入
- en: 'Define the `placeholder` for the input:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为输入定义 `placeholder`：
- en: '[PRE46]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the `placeholder` for the learning rate and training boolean:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 定义学习率和训练布尔值的 `placeholder`：
- en: '[PRE47]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define the batch size and dimension of the noise:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批次大小和噪声的维度：
- en: '[PRE48]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the placeholder for the noise, *z*:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 定义噪声的占位符 *z*：
- en: '[PRE49]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Starting the DCGAN
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 DCGAN
- en: 'First, we feed the noise ![](img/e40108a5-8715-4317-b6e0-3ed7bf3d768d.png)
    to the generator and it will output the fake image, ![](img/5219f5f6-2449-4a52-8e89-eb8b835c9b81.png):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将噪声 ![](img/e40108a5-8715-4317-b6e0-3ed7bf3d768d.png) 输入生成器，生成假图像 ![](img/5219f5f6-2449-4a52-8e89-eb8b835c9b81.png)：
- en: '[PRE50]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now we feed the real image to the discriminator ![](img/8c11cc5f-f780-455e-b9e8-43eee6ebb822.png)
    and get the probability of the real image being real:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将真实图像输入判别器 ![](img/8c11cc5f-f780-455e-b9e8-43eee6ebb822.png) 并得到真实图像被判定为真实的概率：
- en: '[PRE51]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Similarly, we feed the fake image to the discriminator, ![](img/caf99dc2-e457-4211-aece-566c0c3c6d23.png),
    and get the probability of the fake image being real:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将虚假图像输入判别器，![](img/caf99dc2-e457-4211-aece-566c0c3c6d23.png)，并获得虚假图像被判定为真实的概率：
- en: '[PRE52]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Computing the loss function
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失函数
- en: Now we will see how to compute the loss function.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何计算损失函数。
- en: Discriminator loss
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器损失
- en: 'The loss function is the same as for the vanilla GAN:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与普通 GAN 相同：
- en: '![](img/f63bdfc6-d43e-474c-9c7e-e85d82d55fe1.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f63bdfc6-d43e-474c-9c7e-e85d82d55fe1.png)'
- en: 'So, we can directly write the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以直接写出以下内容：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Generator loss
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The generator loss is also the same as for the vanilla GAN:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失与普通 GAN 相同：
- en: '![](img/37464e51-674c-4eea-9109-3569cba6c03a.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37464e51-674c-4eea-9109-3569cba6c03a.png)'
- en: 'We can compute it by using the following code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码计算它：
- en: '[PRE54]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Optimizing the loss
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化损失
- en: 'As we saw in vanilla GANs, we collect the parameters of the discriminator and
    generator as ![](img/0fd6fd44-0de7-492c-ac17-36ad19b49762.png) and ![](img/7a41e1ee-c7ea-4a8e-899f-eb56b8b92e12.png)
    respectively:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在普通 GAN 中看到的，我们分别收集判别器和生成器的参数，分别为 ![](img/0fd6fd44-0de7-492c-ac17-36ad19b49762.png)
    和 ![](img/7a41e1ee-c7ea-4a8e-899f-eb56b8b92e12.png) ：
- en: '[PRE55]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Optimize the loss using the Adam optimizer:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器优化损失：
- en: '[PRE56]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Train the DCGAN
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 DCGAN
- en: 'Let''s begin the training. Define the number of batches, epochs, and learning
    rate:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始训练。定义批次数、epochs 和学习率：
- en: '[PRE57]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Define a helper function for generating and plotting the generated images:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个辅助函数来生成和绘制生成的图像：
- en: '[PRE58]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Start the training:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练：
- en: '[PRE59]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Initialize all variables:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE60]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To execute for each epoch:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个 epoch 执行：
- en: '[PRE61]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define the start and end of the batch:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批次的起始和结束：
- en: '[PRE62]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Sample batch of images:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 样本图像批次：
- en: '[PRE63]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Train the discriminator for every two steps:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 每两步训练一次判别器：
- en: '[PRE64]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Train the generator and discriminator:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成器和判别器：
- en: '[PRE65]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Generate a new image:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 生成新图像：
- en: '[PRE66]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In the first iteration, DCGAN will generate the raw pixels, but over the series
    of iterations, it will learn to generate real images with following parameters:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，DCGAN 将生成原始像素，但在一系列迭代中，它将学会生成具有以下参数的真实图像：
- en: '`Epoch: 0, iteration: 0, Discriminator Loss:1.44706475735, Generator Loss:
    0.726667642593`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '`Epoch: 0, iteration: 0, 判别器损失:1.44706475735, 生成器损失: 0.726667642593`'
- en: 'The following image is generated by DCGAN in the first iteration:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 在第一次迭代中生成了以下图像：
- en: '![](img/9a6f71c4-8fc8-48cb-9cfa-e5abbcfa40e8.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a6f71c4-8fc8-48cb-9cfa-e5abbcfa40e8.png)'
- en: Least squares GAN
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小二乘 GAN
- en: We just learned how GANs are used to generate images. **Least Squares GAN**
    (**LSGAN**) is another simple variant of a GAN. As the name suggests, here, we
    use the least square error as a loss function instead of sigmoid cross-entropy
    loss. With LSGAN, we can improve the quality of images being generated from the
    GAN. But how can we do that? Why do the vanilla GANs generate poor quality images?
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了 GAN 如何用于生成图像。**最小二乘 GAN**（**LSGAN**）是GAN的另一简单变体。顾名思义，这里我们使用最小二乘误差作为损失函数，而不是
    sigmoid 交叉熵损失。通过 LSGAN，我们可以改善从 GAN 生成的图像的质量。但是我们如何做到这一点？为什么普通 GAN 生成的图像质量较差？
- en: If you can recollect the loss function of GAN, we used sigmoid cross-entropy
    as the loss function. The goal of the generator is to learn the distribution of
    the images in the training set, that is, real data distribution, map it to the
    fake distribution, and generate fake samples from the learned fake distribution.
    So, the GANs try to map the fake distribution as close to the true distribution
    as possible.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能回忆起 GAN 的损失函数，我们使用的是 sigmoid 交叉熵作为损失函数。生成器的目标是学习训练集中图像的分布，即真实数据分布，将其映射到假分布，并从学习到的假分布中生成假样本。因此，GAN
    尝试将假分布映射得尽可能接近真实分布。
- en: But once the fake samples are on the correct side of the decision surface, then
    gradients tend to vanish even though the fake samples are far away from the real
    distribution. This is due to the sigmoid cross-entropy loss.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 但一旦生成器生成的假样本位于决策面的正确侧，梯度就会趋于消失，即使假样本远离真实分布。这是由于 sigmoid 交叉熵损失。
- en: Let's understand this with the following figure. A decision boundary of vanilla
    GANs with sigmoid cross-entropy as a loss function is shown in the following figure
    where fake samples are represented by a cross, and real samples are represented
    by a dot, and the fake samples for updating the generator are represented by a
    star.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下图形来理解。使用 sigmoid 交叉熵作为损失函数的普通 GAN 的决策边界如下图所示，在这里假样本用十字表示，真实样本用点表示，更新生成器的假样本用星号表示。
- en: 'As you can observe, once the fake samples (star) generated by the generator
    are on the correct side of the decision surface, that is, once the fake samples
    are on the side of real samples (dot) then the gradients tend to vanish even though
    the fakes samples are far away from real distribution. This is due to the sigmoid
    cross-entropy loss, because it does not care whether the fake samples are close
    to real samples; it only looks for whether the fake samples are on the correct
    side of the decision surface. This leads to a problem that when the gradients
    vanish even though the fake samples are far away from the real data distribution
    then the generator cannot learn the real distribution of the dataset:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一旦生成器生成的假样本（星号）位于决策面的正确侧，也就是假样本位于真实样本（点）的一侧时，梯度就会趋于消失，即使假样本远离真实分布。这是由于
    sigmoid 交叉熵损失，因为它不关心假样本是否接近真样本，而是只关注假样本是否位于决策面的正确侧。这导致了一个问题，即当梯度消失时，即使假样本远离真实数据分布，生成器也无法学习到数据集的真实分布：
- en: '![](img/071cee4c-e99a-4af9-afbe-26ace428c6dc.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/071cee4c-e99a-4af9-afbe-26ace428c6dc.png)'
- en: 'So, we can change this decision surface with sigmoid cross-entropy loss to
    a least squared loss. Now, as you can see in the following diagram, although the
    fake samples generated by the generator are on the correct side of the decision
    surface, gradients will not vanish until the fake samples match the true distribution.
    Least square loss forces the updates to match the fake samples to the true samples:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用最小二乘损失来改变这个决策面。现在，正如你在下图中所见，尽管生成器生成的假样本位于决策面的正确侧，梯度不会消失，直到假样本与真分布匹配。最小二乘损失强制更新将假样本匹配到真样本：
- en: '![](img/ce0e609d-7871-45f5-af48-fc1b858f1812.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce0e609d-7871-45f5-af48-fc1b858f1812.png)'
- en: So, since we are matching a fake distribution to the real distribution, our
    image quality will be improved when we use the least square as a cost function.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于我们将假分布与真实分布匹配，当我们将最小二乘作为成本函数时，我们的图像质量将得到改善。
- en: In a nutshell gradient updates in the vanilla GANs will be stopped when the
    fake samples are correct side of the decision surface even though they are from
    the real samples, that is, the real distribution. This is due to the sigmoid cross-entropy
    loss and it does not care whether the fake samples are close to real samples,
    it only looks for whether the fake samples are on the correct side. This leads
    to the problem that we cannot learn the real data distribution perfectly. So,
    we use LSGAN, which uses least squared error as a loss function, where the gradient
    updates will not be stopped until the fake samples match the real sample, even
    though the fake samples are on the correct side of the decision boundary.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在普通 GAN 中，当假样本位于正确的决策表面的一侧时，渐变更新将会停止，即使它们来自真实样本，也就是真实分布。这是由于 Sigmoid 交叉熵损失，并不关心假样本是否接近真实样本，它只关心假样本是否在正确的一侧。这导致我们无法完美学习真实数据分布。因此，我们使用
    LSGAN，它使用最小二乘误差作为损失函数，渐变更新将不会停止，直到假样本匹配真实样本，即使假样本在决策边界的正确一侧。
- en: Loss function
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: Now that, we have learned that the least square loss function improves the generator's
    image quality, how can we rewrite our GANs loss function in terms of least squares?
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经学会了最小二乘损失函数如何提高生成器图像的质量，我们如何用最小二乘来重写我们的 GAN 损失函数？
- en: 'Let''s say *a* and *b* are the actual labels for the generated images and real
    images respectively, then we can write the loss function of the discriminator
    in terms of least square loss as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *a* 和 *b* 分别是生成图像和真实图像的实际标签，那么我们可以用最小二乘损失的术语来写出鉴别器的损失函数如下：
- en: '![](img/b057e777-4dec-4382-b6bf-99f7bcbb8ae5.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b057e777-4dec-4382-b6bf-99f7bcbb8ae5.png)'
- en: 'Similarly, let''s say *c* is the actual label that the generator wants the
    discriminator to believe that the generated image is the real image, so label
    *c* represents the real image. Then we can write the loss function of a generator
    in terms of least square loss as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设 *c* 是生成器希望鉴别器相信生成图像是真实图像的实际标签，那么标签 *c* 代表真实图像。然后我们可以用最小二乘损失的术语来写出生成器的损失函数如下：
- en: '![](img/15fa12e2-ad1a-4169-ba16-962be484bd24.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15fa12e2-ad1a-4169-ba16-962be484bd24.png)'
- en: 'We set labels for real images as 1 and for fake images 0, so *b* and *c* become
    1 and *a* becomes 0\. So, our final equation can be given as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为真实图像设置标签为 1，对于假图像设置为 0，因此 *b* 和 *c* 变为 1，*a* 变为 0\. 因此，我们的最终方程可以如下给出：
- en: 'The loss function of the discriminator is given as follows:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器的损失函数如下所示：
- en: '![](img/76960fd1-f581-464f-aeb7-0629fdbf4171.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76960fd1-f581-464f-aeb7-0629fdbf4171.png)'
- en: 'The loss function of the generator is given as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失如下所示：
- en: '![](img/71eb6348-2097-425c-8cbd-52f5e6274d3e.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71eb6348-2097-425c-8cbd-52f5e6274d3e.png)'
- en: LSGAN in TensorFlow
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的 LSGAN
- en: Implementing LSGAN is the same as the vanilla GAN, except for the change in
    the loss function. So, instead of looking at the whole code, we will see only
    how to implement LSGAN's loss function in TensorFlow. The complete code for LSGAN
    is available in the GitHub repo at [http://bit.ly/2HMCrrx](http://bit.ly/2HMCrrx).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 LSGAN 与普通 GAN 相同，唯一不同的是损失函数的变化。因此，我们只会看如何在 TensorFlow 中实现 LSGAN 的损失函数，而不是查看整个代码。LSGAN
    的完整代码可在 GitHub 仓库 [http://bit.ly/2HMCrrx](http://bit.ly/2HMCrrx) 中找到。
- en: Let's now see how the loss function of LSGAN is implemented.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何实现 LSGAN 的损失函数。
- en: Discriminator loss
  id: totrans-388
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器损失
- en: 'The discriminator loss is given as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器的损失如下所示：
- en: '![](img/710bce4b-af2e-4d61-9f8e-c501d522e839.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/710bce4b-af2e-4d61-9f8e-c501d522e839.png)'
- en: 'First, we will implement the first term, ![](img/4725ed5b-3c08-4da1-bbdd-79712d987c8f.png):'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现第一个术语，![](img/4725ed5b-3c08-4da1-bbdd-79712d987c8f.png)：
- en: '[PRE67]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now we will implement the second term, ![](img/a77b6611-7c67-47e8-8bdf-5cfe633c25b7.png):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现第二个术语，![](img/a77b6611-7c67-47e8-8bdf-5cfe633c25b7.png)：
- en: '[PRE68]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The final discriminator loss can be written as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的鉴别器损失可以写成如下形式：
- en: '[PRE69]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Generator loss
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The generator loss, ![](img/9465d1e9-e2f7-4869-8ae1-9eeb5274f502.png), is given
    as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失，![](img/9465d1e9-e2f7-4869-8ae1-9eeb5274f502.png)，如下所示：
- en: '[PRE70]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: GANs with Wasserstein distance
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Wasserstein 距离的 GAN
- en: Now we will see another very interesting version of a GAN, called Wasserstein
    GAN (**WGAN**). It uses the Wasserstein distance in the GAN's loss function. First,
    let's understand why we need a Wasserstein distance measure and what's wrong with
    our current loss function.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到GAN的另一个非常有趣的版本，称为Wasserstein GAN（**WGAN**）。它在GAN的损失函数中使用了Wasserstein距离。首先，让我们理解为什么我们需要Wasserstein距离测量以及我们当前损失函数的问题所在。
- en: Before going ahead, first, let's briefly explore two popular divergence measures
    that are used for measuring the similarity between two probability distributions.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们简要探讨两种用于衡量两个概率分布相似性的流行散度测量方法。
- en: 'The **Kullback-Leibler** (**KL**) divergence is one of the most popularly used
    measures for determining how one probability distribution diverges from the other.
    Let''s say we have two discrete probability distributions, ![](img/6c753ab9-5878-41f0-9ec5-edc48f6862f4.png)
    and ![](img/69ed2c31-9bab-4e3f-bf38-949adcf9290a.png), then the KL divergence
    can be expressed as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler**（**KL**）散度是最常用的测量方法之一，用于确定一个概率分布与另一个之间的差异。假设我们有两个离散概率分布，![](img/6c753ab9-5878-41f0-9ec5-edc48f6862f4.png)
    和 ![](img/69ed2c31-9bab-4e3f-bf38-949adcf9290a.png)，那么KL散度可以表示为：'
- en: '![](img/c4182eb2-4c46-4126-879a-7b2a9c5a7462.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4182eb2-4c46-4126-879a-7b2a9c5a7462.png)'
- en: 'When the two distributions are continuous, then the KL divergence can be expressed
    in the integral form as shown:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个分布是连续的时，KL散度可以表示为如下积分形式：
- en: '![](img/a142ef95-a449-4938-8ddc-5defcfa88c18.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a142ef95-a449-4938-8ddc-5defcfa88c18.png)'
- en: 'The KL divergence is not symmetric, meaning the following:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度不对称，意味着以下情况：
- en: '![](img/1f58cb0a-d00a-4e79-b6a3-5cb2dd9cf866.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f58cb0a-d00a-4e79-b6a3-5cb2dd9cf866.png)'
- en: 'The **Jensen-Shanon** (**JS**) divergence is another measure for measuring
    the similarity of two probability distributions. But unlike the KL divergence,
    the JS divergence is symmetric and it can be given as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jensen-Shanon**（**JS**）散度是另一种用于测量两个概率分布相似性的方法。但与KL散度不同，JS散度是对称的，可以表示为：'
- en: '![](img/a4182488-867d-49f6-8c24-4c25433c5dbd.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4182488-867d-49f6-8c24-4c25433c5dbd.png)'
- en: Are we minimizing JS divergence in GANs?
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们在GAN中是否在最小化JS散度？
- en: We know that generators try to learn the real data distribution, ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png),so
    that it can generate new samples from the learned distribution, ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png),
    and the discriminator tells us whether the image is from a real or fake distribution.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器试图学习真实数据分布 ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)，以便能够从学习到的分布
    ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png) 生成新样本，鉴别器告诉我们图像是来自真实分布还是伪造分布。
- en: We also learned that when ![](img/1f97f9d4-7527-4206-84f7-f382f9313d2d.png),
    then the discriminator cannot tell us whether the image is from a real or a fake
    distribution. It just outputs 0.5 because it cannot differentiate between ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也学到，当 ![](img/1f97f9d4-7527-4206-84f7-f382f9313d2d.png) 时，鉴别器无法告诉我们图像来自真实分布还是伪造分布。它只输出0.5，因为无法区分
    ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png) 和 ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)。
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于生成器，最优鉴别器可以表示为：
- en: '![](img/9a8ad343-a6b0-4f4e-93e5-babad0eb630b.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a8ad343-a6b0-4f4e-93e5-babad0eb630b.png)'
- en: 'Let''s recall the loss function of the discriminator:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下鉴别器的损失函数：
- en: '![](img/ad426bbd-9cc9-4705-b5a2-d096c5bf7f2d.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad426bbd-9cc9-4705-b5a2-d096c5bf7f2d.png)'
- en: 'It can be simply written as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 可以简单地写成如下形式：
- en: '![](img/426607b5-7610-4190-9fc7-8afa1a8692ad.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/426607b5-7610-4190-9fc7-8afa1a8692ad.png)'
- en: 'Substituting equation *(1)* in the preceding equation we get the following:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 将等式 *(1)* 替换到前述等式中，我们得到以下结果：
- en: '![](img/120b59bc-88ac-4841-9df9-f02a8377dbc8.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/120b59bc-88ac-4841-9df9-f02a8377dbc8.png)'
- en: 'It can be solved as follows:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样解决：
- en: '![](img/bdec55a6-f7cc-4126-a879-0ae5615e3687.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdec55a6-f7cc-4126-a879-0ae5615e3687.png)'
- en: 'As you can see, we are basically minimizing the JS divergence in the loss function
    of GAN. So, minimizing the loss function of the GAN basically implies that minimizing
    the JS divergence between the real data distribution, ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and the fake data distribution, ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)
    as shown:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们基本上在GAN的损失函数中最小化JS散度。因此，最小化GAN的损失函数基本上意味着最小化真实数据分布 ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    和伪造数据分布 ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png) 之间的JS散度，如下所示：
- en: '![](img/fdd01b16-1305-4882-bdaf-b5f238637878.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdd01b16-1305-4882-bdaf-b5f238637878.png)'
- en: 'Minimizing the JS divergence between ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)denotes that the generator
    ![](img/6faaa333-d580-4b08-af29-4b1426f54fe6.png) makes their distribution ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)
    similar to the real data distribution ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png).
    But there is a problem with JS divergence. As you can see from the following figure,
    there is no overlap between the two distributions. When there is no overlap or
    when the two distributions do not share the same support, JS divergence will explode
    or return a constant value and the GANs cannot learn properly:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png) 和 ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)
    之间的JS散度意味着生成器 ![](img/6faaa333-d580-4b08-af29-4b1426f54fe6.png) 使其分布类似于真实数据分布
    ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)。但JS散度存在问题。正如您从下图中看到的那样，两个分布之间没有重叠。当没有重叠或两个分布不共享相同支持时，JS散度会爆炸或返回常数值，GANs无法正确学习：
- en: '![](img/41053aa8-9b8a-4388-86a6-aead8f1a86de.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41053aa8-9b8a-4388-86a6-aead8f1a86de.png)'
- en: So, to avoid this, we need to change our loss function. Instead of minimizing
    the JS divergence, we use a new distance metric called the Wasserstein distance,
    which tells us how the two distributions are apart from each other even when they
    don't share the same support.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免这种情况，我们需要改变我们的损失函数。我们不再最小化JS散度，而是使用一种称为Wasserstein距离的新距离度量，即使在它们不共享相同支持的情况下，它也告诉我们两个分布之间有多大的差距。
- en: What is the Wasserstein distance?
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Wasserstein距离？
- en: The Wasserstein distance, also known as the **Earth Movers** (**EM**) distance,
    is one of the most popularly used distance measures in the optimal transport problems
    where we need to move things from one configuration to another.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein距离，也称为**地球移动者**（**EM**）距离，在需要将事物从一种配置移动到另一种配置的最优传输问题中，是最常用的距离测量之一。
- en: So, when we have two distributions, ![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png)
    and ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png), **![](img/7136d91b-eeab-48fe-81cb-eebba4d3e159.png)**
    implies that how much amount of work is required for the probability distribution,
    ![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png)to match the probability distribution
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当我们有两个分布，![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png) 和 ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png)，**![](img/7136d91b-eeab-48fe-81cb-eebba4d3e159.png)**
    表示了概率分布 ![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png) 要匹配概率分布 ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png)
    需要多少工作量。
- en: Let’s try to understand the intuition behind the EM distance. We can view the
    probability distribution as a collection of mass. Our goal is to convert one probability
    distribution to another. There are many possible ways to convert one distribution
    to another, but the Wasserstein metric seeks to find the optimal and minimum way
    that has the least cost in conversion.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解EM距离背后的直觉。我们可以将概率分布视为质量的集合。我们的目标是将一个概率分布转换为另一个。有许多可能的转换方式，但Wasserstein度量寻求找到最佳和最小的方式，以最小的转换成本进行转换。
- en: The cost of conversion can be given as a distance multiplied by the mass.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 转换成本可以表示为距离乘以质量。
- en: The amount of information moved from point *x* to point *y* is given as ![](img/d29b995e-bc0c-47d9-b32b-fbd3ef68f5a7.png).
    It is called a **transport plan**. It tells us how much information we need to
    transport from *x* to *y*, and the distance between *x* and *y* is given as ![](img/c0a3dc56-9679-42be-b608-a3befefe83aa.png).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 从点*x*到点*y*的信息量被定义为 ![](img/d29b995e-bc0c-47d9-b32b-fbd3ef68f5a7.png)，称为**运输计划**。它告诉我们需要从*x*到*y*运输多少信息，而*x*和*y*之间的距离如下给出：![](img/c0a3dc56-9679-42be-b608-a3befefe83aa.png)。
- en: 'So, the cost is given as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，成本如下：
- en: '![](img/1e9fe2f6-9d38-4b19-8733-5afac683c832.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e9fe2f6-9d38-4b19-8733-5afac683c832.png)'
- en: 'We have many *(x,y)* pairs so the expectations across all *(x,y)* pairs are
    given as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有许多*(x,y)*对，因此所有*(x,y)*对的期望如下所示：
- en: '![](img/1fd95a28-6445-4341-80ea-3a589d14b8c0.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fd95a28-6445-4341-80ea-3a589d14b8c0.png)'
- en: 'It implies the cost of moving from point *x* to *y*. There are many ways to
    move from *x* to *y*, but we are interested only in the optimal path, that is,
    minimum cost, so we rewrite our preceding equation as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 它暗示了从点*x*到点*y*的移动成本。从*x*到*y*有很多移动方式，但我们只关心最优路径，即最小成本，因此我们将我们之前的方程重写如下：
- en: '![](img/169f8b36-171c-4d3e-89d8-bf8126b35d40.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/169f8b36-171c-4d3e-89d8-bf8126b35d40.png)'
- en: Here, *inf* basically implies the minimum value. ![](img/8a73a5b1-50ee-47e1-b013-0f3688ec3db9.png)
    is the set of all possible joint distributions between ![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)and
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*inf*基本上表示最小值。![](img/8a73a5b1-50ee-47e1-b013-0f3688ec3db9.png)是![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)和![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png)之间所有可能的联合分布的集合。
- en: So, out of all the possible joint distributions between ![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)and
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png) we are finding the minimum cost
    required to make one distribution look like another.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在所有可能的![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)和![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png)之间的联合分布中，我们正在找到使一个分布看起来像另一个分布所需的最小成本。
- en: 'Our final equation can be given as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终方程可以如下给出：
- en: '![](img/604b574b-d605-4a67-889f-373f87cf8ea3.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](img/604b574b-d605-4a67-889f-373f87cf8ea3.png)'
- en: However, calculating the Wasserstein distance is not a simple task because it
    is difficult to exhaust all possible joint distributions, ![](img/778a8f70-67f7-412c-868f-70dca7f2f92c.png),
    and it turns into another optimization problem.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算Wasserstein距离并不是一件简单的任务，因为难以穷尽所有可能的联合分布![](img/778a8f70-67f7-412c-868f-70dca7f2f92c.png)，它变成了另一个优化问题。
- en: 'In order to avoid that, we introduce **Kantorovich-Rubinstein duality**. It
    converts our equation into a simple maximization problem, as follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们引入**康托洛维奇-鲁宾斯坦对偶**。它将我们的方程转化为一个简单的最大化问题，如下所示：
- en: '![](img/654e40c1-f9a8-4f76-acae-21c4b60711a8.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/654e40c1-f9a8-4f76-acae-21c4b60711a8.png)'
- en: Okay, but what does the above equation mean? We are basically applying the **supremum**
    over all **k-Lipschitz function**. Wait. What is the Lipschitz function and what
    is supremum? Let's discuss that in the next section.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但上述方程的意思是什么呢？我们基本上是在所有k-Lipschitz函数上应用"supremum"。等等。Lipschitz函数是什么，"supremum"又是什么？让我们在下一节中讨论这个问题。
- en: Demystifying the k-Lipschitz function
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘k-Lipschitz函数
- en: 'A Lipschitz continuous function is a function that must be continuous and almost
    differentiable everywhere. So, for any function to be a Lipschitz continuous,
    the absolute value of a slope of the function’s graph cannot be more than a constant
    ![](img/432a5132-4fef-4758-9d92-78cf2d93543f.png). This constant ![](img/b01195a3-071a-4904-93fe-575c769ad3c6.png)
    is called the **Lipschitz** **constant**:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Lipschitz连续函数是必须连续的函数，几乎在所有地方都是可微的。因此，对于任何函数来说，要成为Lipschitz连续，函数图的斜率的绝对值不能超过一个常数![](img/432a5132-4fef-4758-9d92-78cf2d93543f.png)。这个常数![](img/b01195a3-071a-4904-93fe-575c769ad3c6.png)被称为**Lipschitz常数**：
- en: '![](img/31ce8ab7-2251-4cbe-99e9-50786dab57fb.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31ce8ab7-2251-4cbe-99e9-50786dab57fb.png)'
- en: To put it in simple terms, we can say a function is Lipschitz continuous when
    the derivation of a function is bounded by some constant *K* and it never exceeds
    the constant.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们可以说一个函数是Lipschitz连续的，当一个函数的导数被某个常数*K*限制，并且从不超过这个常数时。
- en: Let's say ![](img/3b220a73-8c4e-4875-94c9-26d104b13bce.png), for instance, ![](img/edb6c058-177f-491b-b616-145458a44679.png)
    is Lipschitz continuous since its derivative ![](img/cfe04913-e1c4-4893-b20b-b70136c71674.png)
    is bounded by 1\. Similarly, ![](img/ba77a203-e692-4ff1-9a5a-50d7c6445344.png) is
    Lipschitz continuous, since its slope is -1 or 1 everywhere. However, it is not
    differentiable at 0.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说![](img/3b220a73-8c4e-4875-94c9-26d104b13bce.png)，![](img/edb6c058-177f-491b-b616-145458a44679.png)是Lipschitz连续的，因为它的导数![](img/cfe04913-e1c4-4893-b20b-b70136c71674.png)受到1的限制。同样地，![](img/ba77a203-e692-4ff1-9a5a-50d7c6445344.png)也是Lipschitz连续的，因为它的斜率在每个地方都是-1或1。然而，在0处它是不可微的。
- en: 'So, let''s recall our equation:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们回顾一下我们的方程：
- en: '![](img/8fee0720-1bc6-467b-85bf-1e143ddbc99e.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fee0720-1bc6-467b-85bf-1e143ddbc99e.png)'
- en: 'Here, supremum is basically an opposite to infimum. So, supremum over the Lipschitz
    function implies a maximum over k-Lipschitz functions. So, we can write the following:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，"supremum"基本上是"infimum"的对立面。因此，Lipschitz函数的"supremum"意味着k-Lipschitz函数的最大值。因此，我们可以写成如下形式：
- en: '![](img/0b95e49d-de4a-4d0b-940d-4f700bc5e3c7.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b95e49d-de4a-4d0b-940d-4f700bc5e3c7.png)'
- en: The preceding equation basically tells us that we are basically finding a maximum
    distance between the expected value over real samples and the expected value over
    generated samples.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程基本上告诉我们，我们正在寻找实际样本期望值与生成样本期望值之间的最大距离。
- en: The loss function of WGAN
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WGAN的损失函数
- en: Okay, why are we learning all this? We saw previously that there is a problem
    with JS divergence in the loss function, so we resorted to the Wasserstein distance.
    Now, our goal of the discriminator is no longer to say whether the image is from
    the real or fake distribution; instead, it tries to maximize the distance between
    real and generated sample. We train the discriminator to learn the Lipschitz continuous
    function for computing the Wasserstein distance between a real and fake data distribution.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，为什么我们要学习所有这些？我们先前看到损失函数中存在JS散度问题，因此我们转向了Wasserstein距离。现在，我们的判别器的目标不再是判断图像是否来自真实分布还是伪造分布；相反，它试图最大化真实和生成样本之间的距离。我们训练判别器学习Lipschitz连续函数，用于计算真实数据分布和伪造数据分布之间的Wasserstein距离。
- en: 'So, the discriminator loss is given as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，判别器损失如下所示：
- en: '![](img/4c312c9e-f1b8-4e4b-a1cd-6ebd930d3d4c.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c312c9e-f1b8-4e4b-a1cd-6ebd930d3d4c.png)'
- en: Now we need to ensure that our function is a k-Lipschitz function during training.
    So, for every gradient update, we clip the weights of our gradients between a
    lower bound and upper bound, say between -0.01 and +0.01.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要确保我们的函数在训练期间是一个k-Lipschitz函数。因此，对于每次梯度更新，我们剪裁我们的梯度权重在一个下界和上界之间，比如在-0.01和+0.01之间。
- en: 'We know that the discriminator loss is given as:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道判别器的损失如下所示：
- en: '![](img/c2f24144-6878-4458-844e-9c4d299a6670.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2f24144-6878-4458-844e-9c4d299a6670.png)'
- en: 'Instead of maximizing, we convert this into minimization objective by adding
    a negative sign:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再最大化，而是通过添加负号将其转换为最小化目标：
- en: '![](img/a48705a6-d88f-4ad5-b638-c73e19650074.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a48705a6-d88f-4ad5-b638-c73e19650074.png)'
- en: '![](img/3fe43d54-7535-497a-9dd8-36b4b471556f.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fe43d54-7535-497a-9dd8-36b4b471556f.png)'
- en: The generator loss is the same as we learned in vanilla GANs.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失与我们在普通GAN中学到的相同。
- en: 'Thus, the loss function of the discriminator is given as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，判别器的损失函数如下：
- en: '![](img/6f0a0d87-ca3f-432c-a93a-a77153a1e203.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f0a0d87-ca3f-432c-a93a-a77153a1e203.png)'
- en: 'The loss function of the generator is given as:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下：
- en: '![](img/0fb4e9be-ec5e-4155-847a-ddafd2187553.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fb4e9be-ec5e-4155-847a-ddafd2187553.png)'
- en: WGAN in TensorFlow
  id: totrans-474
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的WGAN
- en: Implementing the WGAN is the same as implementing a vanilla GAN except that
    the loss function of WGAN varies and we need to clip the gradients of the discriminator.
    Instead of looking at the whole, we will only see how to implement the loss function
    of the WGAN and how to clip the gradients of the discriminator.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 实现WGAN与实现普通GAN相同，只是WGAN的损失函数不同，我们需要剪裁判别器的梯度。不再看整体，我们只看如何实现WGAN的损失函数以及如何剪裁判别器的梯度。
- en: 'We know that loss of the discriminator is given as:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道判别器的损失如下：
- en: '![](img/4235eb4f-685a-4c68-a4fe-4303214216bb.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4235eb4f-685a-4c68-a4fe-4303214216bb.png)'
- en: 'And it can be implemented as follows:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以如下实现：
- en: '[PRE71]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We know that generator loss is given as:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器损失如下：
- en: '![](img/95d91718-0bb1-4e1a-a461-3fbb11433934.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95d91718-0bb1-4e1a-a461-3fbb11433934.png)'
- en: 'And it can be implemented as follows:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以如下实现：
- en: '[PRE72]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We clip the gradients of the discriminator as follows:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对判别器的梯度进行剪裁如下：
- en: '[PRE73]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Summary
  id: totrans-486
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by understanding the difference between generative and
    discriminative models. We learned that the discriminative models learn to find
    the good decision boundary that separates the classes in an optimal way, while
    the generative models learn about the characteristics of each class.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解生成模型和判别模型的差异开始这一章。我们了解到判别模型学习找到良好的决策边界，以最佳方式分隔类别，而生成模型则学习每个类别的特征。
- en: Later, we understood how GANs work. They basically consist of two neural networks
    called generators and discriminators. The role of the generators is to generate
    a new image by learning the real data distribution, while the discriminator acts
    as a critic and its role is to tell us whether the generated image is from the
    true data distribution or the fake data distribution, basically whether it is
    a real image or a fake image.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们理解了GAN是如何工作的。它们基本上由称为生成器和判别器的两个神经网络组成。生成器的作用是通过学习真实数据分布来生成新图像，而判别器则充当批评者，其角色是告诉我们生成的图像是来自真实数据分布还是伪造数据分布，基本上是真实图像还是伪造图像。
- en: Next, we learned about DCGAN where we basically replace the feedforward neural
    networks in the generator and discriminator with convolutional neural networks.
    The discriminator uses convolutional layers for classifying the image as a fake
    or a real image, while the generator uses convolutional transpose layers to generate
    a new image.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了DCGAN，其中我们基本上用卷积神经网络替换了生成器和鉴别器中的前馈神经网络。鉴别器使用卷积层来将图像分类为假或真实图像，而生成器使用卷积转置层来生成新图像。
- en: Then, we learned about the LSGAN, which replaces the loss function of both the
    generator and the discriminator with a least squared error loss. Because, when
    we use sigmoid cross-entropy as a loss function, our gradients tend to vanish
    once the fake samples on the correct side of the decision boundary even though
    they are not close to the real distribution. So, we replace the cross-entropy
    loss with the least squared error loss where the gradients will not vanish till
    the fake samples match the true distribution. It forces the gradient updates to
    match the fake samples to the real samples.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了LSGAN，它用最小二乘误差损失替换了生成器和鉴别器的损失函数。因为当我们使用sigmoid交叉熵作为损失函数时，即使假样本不接近真实分布，一旦它们在正确的决策边界一侧，梯度也会消失。因此，我们将交叉熵损失替换为最小二乘误差损失，其中梯度直到假样本与真实分布匹配才会消失。这迫使梯度更新将假样本与真实样本匹配。
- en: Finally, we learned another interesting type of GAN called the Wassetrtain GAN
    where we use the Wasserstein distance measure in the discriminator's loss function.
    Because in vanilla GANs we are basically minimizing JS divergence and it will
    be constant or results in 0 when the distributions of real data and fake does
    not overlap. To overcome this, we used the Wasserstein distance measure in the
    discriminator's loss function.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了另一种有趣的GAN类型，称为Wasserstein GAN，其中我们在鉴别器的损失函数中使用了Wasserstein距离度量。因为在普通的GAN中，我们基本上是在最小化JS散度，当真实数据和假数据的分布不重叠时，它将是常数或结果为0。为了克服这一问题，我们在鉴别器的损失函数中使用了Wasserstein距离度量。
- en: In the next chapter, we will learn about several other interesting types of
    GANs called CGAN, InfoGAN, CycleGAN, and StackGAN.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习关于CGAN、InfoGAN、CycleGAN和StackGAN等几种其他有趣的GAN类型。
- en: Questions
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our knowledge of GANs by answering the following questions:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题来评估我们对GAN的知识：
- en: What is the difference between generative and discriminative models?
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成模型和判别模型有什么区别？
- en: Explain the role of a generator.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释生成器的作用。
- en: Explain the role of a discriminator.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释判别器的角色。
- en: What is the loss function of the generator and discriminator?
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器和鉴别器的损失函数是什么？
- en: How does a DCGAN differ from a vanilla GAN?
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DCGAN与普通GAN有何不同？
- en: What is KL divergence?
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是KL散度？
- en: Define the Wasserstein distance.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义Wasserstein距离。
- en: What is the k-Lipschitz continuous function?
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是k-Lipschitz连续函数？
- en: Further reading
  id: totrans-503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following papers for further information:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下论文获取更多信息：
- en: '*Generative Adversarial Nets* by Ian J Goodfellow, et al., [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成对抗网络*，作者为Ian J Goodfellow等，[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)'
- en: '*Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
    Networks* by Alec Radford, Soumith Chintala, and Luke Metz, [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度卷积生成对抗网络中的无监督表示学习*，作者为Alec Radford，Soumith Chintala和Luke Metz，[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)'
- en: '*Least Squares Generative Adversarial Networks* by Xudong Mao, et al., [https://arxiv.org/pdf/1611.04076.pdf](https://arxiv.org/pdf/1611.04076.pdf)'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小二乘生成对抗网络*，作者为Xudong Mao等，[https://arxiv.org/pdf/1611.04076.pdf](https://arxiv.org/pdf/1611.04076.pdf)'
- en: '*Wasserstein GAN* by Martin Arjovsky, Soumith Chintala, and L´eon Bottou, [https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wasserstein GAN*，作者为Martin Arjovsky，Soumith Chintala和L´eon Bottou，[https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)'
