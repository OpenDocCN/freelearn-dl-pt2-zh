- en: Generating Images Using GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about the discriminative model, which learns to discriminate
    between the classes. That is, given an input, it tells us which class they belong
    to. For instance, to predict whether an email is a spam or ham, the model learns
    the decision boundary that best separates the two classes (spam and ham), and
    when a new email comes in they can tell us which class the new email belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about a generative model that learns the class
    distribution, that is, the characteristics of the classes rather than learning
    the decision boundary. As the name suggests, with the generative models, we can
    generate new data points similar to the data points present in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: We will start off the chapter by understanding the difference between the discriminative
    and generative models in detail. Then, we will deep dive into one of the most
    popularly used generative algorithms, called **Generative Adversarial Networks**
    (**GANs**). We will understand how GANs work and how they are used to generate
    new data points. Going ahead, we will explore the architecture of GANs and we
    will learn about the loss function. Later, we will see how to implement GANs in
    TensorFlow to generate handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: We shall also scrutinize the **Deep Convolutional Generative Adversarial Network**
    (**DCGAN**), which acts as a small extension to the vanilla GAN by using convolutional
    networks in their architecture. Going forward, we will explore **Least Squares
    GAN** (**LSGAN**), which adopts the least square loss for generating better and
    quality images.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will get the hang of **Wasserstein GAN** (**WGA****N**)
    which uses the Wasserstein metric in the GAN's loss function for better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Differences between generative and discriminative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building GANs in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep convolutional GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating CIFAR images using DCGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least Squares GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wasserstein GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between discriminative and generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given some data points, the discriminative model learns to classify the data
    points into their respective classes by learning the decision boundary that separates
    the classes in an optimal way. The generative models can also classify given data
    points, but instead of learning the decision boundary, they learn the characteristics
    of each of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s consider the image classification task for predicting
    whether a given image is an apple or an orange. As shown in the following figure,
    to classify between apple and orange, the discriminative model learns the optimal
    decision boundary that separates the apples and oranges classes, while generative
    models learn their distribution by learning the characteristics of the apple and
    orange classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b853059-0261-4c0a-9e2f-723635b0fda2.png)'
  prefs: []
  type: TYPE_IMG
- en: To put it simply, discriminative models learn to find the decision boundary
    that separates the classes in an optimal way, while the generative models learn
    about the characteristics of each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discriminative models predict the labels conditioned on the input ![](img/88413610-a356-4447-88cf-cb30301d2907.png),
    whereas generative models learn the joint probability distribution ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png).
    Examples of discriminative models include logistic regression, **Support Vector
    Machine** (**SVM**), and so on, where we can directly estimate ![](img/88413610-a356-4447-88cf-cb30301d2907.png)
    from the training set. Examples of generative models include **Markov random fields**
    and **naive Bayes**, where first we estimate ![](img/fc21f491-db86-4299-8317-1b53bbc6edcc.png)
    to determine ![](img/88413610-a356-4447-88cf-cb30301d2907.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d154d240-35cc-45b1-aa96-4b14e779b280.png)'
  prefs: []
  type: TYPE_IMG
- en: Say hello to GANs!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GAN was first introduced by Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio
    in their paper, *Generative Adversarial Network*s, in 2014.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are used extensively for generating new data points. They can be applied
    to any type of dataset, but they are popularly used for generating images. Some
    of the applications of GANs include generating realistic human faces, converting
    grayscale images to colored images, translating text descriptions into realistic
    images, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yann LeCun said the following about GANs:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The coolest idea in deep learning in the last 20 years."'
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have evolved so much in recent years that they can generate a very realistic
    image. The following figure shows the evolution of GANs in generating images over
    the course of five years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91e9ad5-0074-4728-8253-df58cf767b83.png)'
  prefs: []
  type: TYPE_IMG
- en: Excited about GANs already? Now, we will see how exactly they work. Before going
    ahead, let's consider a simple analogy. Let's say you are the police and your
    task is to find counterfeit money, and the role of the counterfeiter is to create
    fake money and cheat the police.
  prefs: []
  type: TYPE_NORMAL
- en: 'The counterfeiter constantly tries to create fake money in a way that is so
    realistic that it cannot be differentiated from the real money. But the police
    have to identify whether the money is real or fake. So, the counterfeiter and
    the police essentially play a two-player game where one tries to defeat the other.
    GANs work something like this. They consist of two important components:'
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can perceive the generator as analogous to the counterfeiter, while the
    discriminator is analogous to the police. That is, the role of the generator is
    to create fake money, and the role of the discriminator is to identify whether
    the money is fake or real.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without going into detail, first, we will get a basic understanding of GANs.
    Let''s say we want our GAN to generate handwritten digits. How can we do that?
    First, we will take a dataset containing a collection of handwritten digits; say,
    the MNIST dataset. The generator learns the distribution of images in our dataset.
    Thus, it learns the distribution of handwritten digits in our training set. Once,
    it learns the distribution of the images in our dataset, and we feed a random
    noise to the generator, it will convert the random noise into a new handwritten
    digit similar to the one in our training set based on the learned distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/982d2c25-cd87-43df-83e0-654aeb6083c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the image is generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c114f411-e07d-4089-ae5b-d330a3781f12.png)'
  prefs: []
  type: TYPE_IMG
- en: The generator component of GAN is basically a generative model, and the discriminator
    component is basically a discriminative model. Thus, the generator learns the
    distribution of the class and the discriminator learns the decision boundary of
    a class.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, we feed a random noise to the generator,
    and it then converts this random noise into a new image *similar* to the one we
    have in our training set, but not *exactly* the same as the images in the training
    set. The image generated by the generator is called a fake image, and the images
    in our training set are called real images. We feed both the real and fake images
    to the discriminator, which tells us the probability of them being real. It returns
    0 if the image is fake and 1 if the image is real:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6720009a-087d-4a75-9f17-57cf40fd0280.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have a basic understanding of generators and discriminators, we
    will study each of the components in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generator component of a GAN is a generative model. When we say the generative
    model, there are two types of generative models— an **implicit** and an **explicit**
    density model. The implicit density model does not use any explicit density function
    to learn the probability distribution, whereas the explicit density model, as
    the name suggests, uses an explicit density function. GANs falls into the first
    category. That is, they are an implicit density model. Let's study in detail and
    understand how GANs are an implicit density model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a generator, ![](img/92ca0c2d-263b-4fe9-8127-821f34e69bd7.png).
    It is basically a neural network parametrized by ![](img/d2325c36-a6ff-49f2-85ed-a3727e564425.png).
    The role of the generator network is to generate new images. How do they do that?
    What should be the input to the generator?
  prefs: []
  type: TYPE_NORMAL
- en: 'We sample a random noise, ![](img/e81c13be-a7f2-4fce-b65d-bb431e78a44f.png),
    from a normal or uniform distribution, ![](img/7b87aece-3326-440b-b0c2-6946956240c3.png).
    We feed this random noise, ![](img/e81c13be-a7f2-4fce-b65d-bb431e78a44f.png),
    as an input to the generator and then it converts this noise to an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f9e55cc-f1c7-4e81-8970-10d03f783878.png)'
  prefs: []
  type: TYPE_IMG
- en: Surprising, isn't it? How does the generator converts a random noise to a realistic
    image?
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a dataset containing a collection of human faces and we want
    our generator to generate a new human face. First, the generator learns all the
    features of the face by learning the probability distribution of the images in
    our training set. Once the generator learns the correct probability distribution,
    it can generate totally new human faces.
  prefs: []
  type: TYPE_NORMAL
- en: But how does the generator learn the distribution of the training set? That
    is, how does the generator learn the distribution of images of human faces in
    the training set?
  prefs: []
  type: TYPE_NORMAL
- en: A generator is nothing but a neural network. So, what happens is that the neural
    network learns the distribution of the images in our training set implicitly;
    let's call this distribution a generator distribution, ![](img/1761d20a-41ac-42fd-96a9-7020402924b0.png).
    At the first iteration, the generator generates a really noisy image. But over
    a series of iterations, it learns the exact probability distribution of our training
    set and learns to generate a correct image by tuning its ![](img/19a949c3-3c6d-4648-a9a7-b369bea074b7.png)
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we are not using the uniform distribution ![](img/f7abe107-bf51-4c25-a7c3-1c179dcb65b0.png)for
    learning the distribution of our training set. It is only used for sampling random
    noise, and we feed this random noise as an input to the generator. The generator
    network implicitly learns the distribution of our training set and we call this
    distribution a generator distribution, ![](img/9133256a-83f4-442b-b832-92b5219fdea6.png)and
    that is why we call our generator network an implicit density model.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, the discriminator is a discriminative model. Let's say
    we have a discriminator, ![](img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png). It
    is also a neural network and it is parametrized by ![](img/1c69a44b-a93f-46cf-b35a-06d8a59516fe.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator to discriminate between two classes. That is,
    given an image ![](img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png), it has to identify
    whether the image is from a real distribution or a fake distribution (generator
    distribution). That is, discriminator has to identify whether the given input
    image is from the training set or the fake image generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34c15144-474c-41d5-98d6-042c6dbdf0ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's call the distribution of our training set the real data distribution,
    which is represented by ![](img/1d47c8df-f7c7-4f75-a9ff-74ee634de483.png). We
    know that the generator distribution is represented by ![](img/c4ae4675-c413-4ad1-9d5f-5676b6e3172a.png).
  prefs: []
  type: TYPE_NORMAL
- en: So, the discriminator ![](img/ffac7f05-3330-4b0c-a1b7-4ff2daedeca2.png) essentially
    tries to discriminate whether the image ![](img/19ee6f8d-c60c-4952-8a15-bfd8b22c0b5a.png)
    is from ![](img/658f1dcf-0a4a-403c-8ade-f443321a85b2.png) or ![](img/8e18b1e3-3e32-4a76-924a-931a5ef515de.png).
  prefs: []
  type: TYPE_NORMAL
- en: How do they learn though?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we just studied the role of the generator and discriminator, but how
    do they learn exactly? How does the generator learn to generate new realistic
    images and how does the discriminator learn to discriminate between images correctly?
  prefs: []
  type: TYPE_NORMAL
- en: We know that the goal of the generator is to generate an image in such a way
    as to fool the discriminator into believing that the generated image is from a
    real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the first iteration, the generator generates a noisy image. When we feed
    this image to the discriminator, discriminator can easily detect that the image
    is from a generator distribution. The generator takes this as a loss and tries
    to improve itself, as its goal is to fool the discriminator. That is, if the generator
    knows that the discriminator is easily detecting the generated image as a fake
    image, then it means that it is not generating an image similar to those in the
    training set. This implies that it has not learned the probability distribution
    of the training set yet.
  prefs: []
  type: TYPE_NORMAL
- en: So, the generator tunes its parameters in such a way as to learn the correct
    probability distribution of the training set. As we know that the generator is
    a neural network, we simply update the parameters of the network through backpropagation.
    Once it has learned the probability distribution of the real images, then it can
    generate images similar to the ones in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what about the discriminator? How does it learn? As we know, the role
    of the discriminator is to discriminate between real and fake images.
  prefs: []
  type: TYPE_NORMAL
- en: If the discriminator incorrectly classifies the generated image; that is, if
    the discriminator classifies the fake image as a real image, then it implies that
    the discriminator has not learned to differentiate between the real and fake image.
    So, we update the parameter of the discriminator network through backpropagation
    to make the discriminator learn to classify between real and fake images.
  prefs: []
  type: TYPE_NORMAL
- en: So, basically, the generator is trying to fool the discriminator by learning
    the real data distribution, ![](img/0f53be09-b9ad-4d19-a3a0-1c2a0922e139.png),
    and the discriminator is trying to find out whether the image is from a real or
    fake distribution. Now the question is, when do we stop training the network in
    light of the fact that both generator and discriminator are competing against
    each other?
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the goal of the GAN is to generate images similar to the one in the
    training set. Say we want to generate a human face—we learn the distribution of
    images in the training set and generate new faces. So, for a generator, we need
    to find the optimal discriminator. What do we mean by that?
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that a generator distribution is represented by ![](img/7d56ebaa-5f36-4aa5-8880-d2a08def9e42.png)
    and the real data distribution is represented by ![](img/b23d9151-6e6c-4297-a6e7-2be230ca72b2.png).
    If the generator learns the real data distribution perfectly, then ![](img/9c04b9c9-59da-46a7-93fb-33f8723854f3.png)
    equals ![](img/d72cb9b6-eebc-4311-a5e2-1b92b38cda46.png), as shown in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20810e81-09fd-40d0-8334-8e7e0424784e.png)'
  prefs: []
  type: TYPE_IMG
- en: When ![](img/35c692e2-1874-4867-8f91-f077528e5653.png), then the discriminator
    cannot differentiate between whether the input image is from a real or a fake
    distribution, so it will just return 0.5 as a probability, as the discriminator
    will become confused between the two distributions when they are same.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de058172-fac0-4d5b-bfd2-5707bb869ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: So, when the discriminator just returns the probability of 0.5 for any image,
    then we can say that the generator has learned the distribution of images in our
    training set and fooled the discriminator successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of a GAN is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2ee99f3-d6b4-49dc-abc1-3f40a87b16ff.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, **Generator ![](img/ae577575-21e0-4777-b282-e13ce0963f43.png)**
    takes the random noise, ![](img/ea1a722d-53d4-454d-b7bc-57dfd371adbc.png), as
    input by sampling from a uniform or normal distribution and generates a fake image
    by implicitly learning the distribution of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: We sample an image, ![](img/f424ed1c-e52f-4f65-88f4-d631e306adf3.png), from
    the real data distribution, ![](img/94ad1212-c2b9-4868-9015-a247dac35a81.png),
    and fake data distribution, ![](img/c4c045b7-06a5-497a-bd75-1f70e20a544a.png),
    and feed it to the discriminator, ![](img/045b1ab3-9b82-4404-b976-33bf35de5dd3.png).
    We feed real and fake images to the discriminator and the discriminator performs
    a binary classification task. That is, it returns 0 when the image is fake and
    1 when the image is real.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will examine the loss function of GAN. Before going ahead, let''s recap
    the notations:'
  prefs: []
  type: TYPE_NORMAL
- en: A noise that is fed as an input to the generator is represented by ![](img/8f3b3d59-89b3-4a8e-b661-af59d0a87d3c.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The uniform or normal distribution from which the noise ![](img/087d84d8-ffc2-435f-bb18-271187a2a2e0.png)is
    sampled is represented by ![](img/25204cbc-e786-49e4-a289-95bb4f4c33d5.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An input image is represented by ![](img/6bc56732-9aa1-4d55-9b70-420d868e29db.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real data distribution or distribution of our training set is represented by
    ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fake data distribution or distribution of the generator is represented by ![](img/809aa100-6d70-4464-a1bd-638439152c15.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we write ![](img/d41cdb15-ed21-4f3a-8dde-bf3dcdad7df5.png), it implies
    that image ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png) is sampled from the
    real distribution, ![](img/0e35c559-8d25-4875-a001-36fe0ea20bd6.png). Similarly,
    ![](img/12c7a854-fe82-450b-b5ca-35b8db0e8abd.png) denotes that image ![](img/24d0f91f-6aa2-417c-b2b2-bb90a15e9f28.png)
    is sampled from the generator distribution, ![](img/ad274aa3-a892-48fc-b307-d383dd37900c.png),
    and ![](img/2801a0f3-9ea3-4bb4-a77a-44fd75f11ec7.png) implies that the generator
    input, ![](img/4fbf1423-a85d-4d32-b61d-f11f3c96e664.png), is sampled from the
    uniform distribution, ![](img/1ffabeaf-b836-4798-a3e8-b711289a0ab1.png).
  prefs: []
  type: TYPE_NORMAL
- en: We've learned that both the generator and discriminator are neural networks
    and both of them update their parameters through backpropagation. We now need
    to find the optimal generator parameter, ![](img/c6200791-dccf-4a07-bf58-7dab7af2f07d.png),
    and the discriminator parameter, ![](img/e36cb046-2690-4a0a-b80f-784f84e5ae16.png).
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will look at the loss function of the discriminator. We know that the
    goal of the discriminator is to classify whether the image is a real or a fake
    image. Let's denote discriminator by ![](img/b0cfa043-b53b-44c1-81f9-8212e217f4f5.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of the discriminator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7532f0-4b05-4cec-a557-b0e09704751b.png)'
  prefs: []
  type: TYPE_IMG
- en: What does this mean, though? Let's understand each of the terms one by one.
  prefs: []
  type: TYPE_NORMAL
- en: First term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the first term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb8e27a3-3ccc-4e8a-9ab1-fbac423487d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/a3651ead-3b34-4a33-b0dd-6c614a82aaa7.png) implies that we are
    sampling input ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) from the real
    data distribution, ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png), so ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    is a real image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png) implies that we are feeding
    the input image ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) to the discriminator
    ![](img/908d4df5-0cf6-4c8e-82b3-410b8a1a2552.png), and the discriminator will
    return the probability of input image ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png)
    to be a real image. As ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) is sampled
    from real data distribution ![](img/5db24beb-d71c-4ab8-a61b-314d8351cc10.png),
    we know that ![](img/03ab5f7b-3899-49aa-a86e-c5aacefde4c2.png) is a real image.
    So, we need to maximize the probability of ![](img/d905e6ca-166d-4f59-bd74-658d444912e6.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc9b7dd8-32c0-41d8-a88e-b26e643c90c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But instead of maximizing raw probabilities, we maximize log probabilities;
    as we learned in [Chapter 7](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml), *Learning
    Text Representations*, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f1ed11f-731b-4773-95e6-29a13778e98a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, our final equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20ada8c6-4d18-4304-9b6b-1e34b581da5d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/473ff433-208c-4241-b05e-27790745817b.png) implies the expectations
    of the log likelihood of input images sampled from the real data distribution
    being real.'
  prefs: []
  type: TYPE_NORMAL
- en: Second term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at the second term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1973703-b496-4ebe-a4c4-9ac27017c421.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/de1d3506-7c7b-443c-b694-51f180dd1429.png) implies that we are
    sampling a random noise ![](img/225c3938-71c7-4484-820c-d516e6776803.png) from
    the uniform distribution ![](img/904684da-46cd-4a21-a34e-46e09f8d0da4.png). ![](img/b07bd0b3-3701-448a-b3f8-53881193c5f8.png)
    implies that the generator ![](img/e6e3fb39-57de-4c67-8cdf-ab528da442ad.png) takes
    the random noise ![](img/225c3938-71c7-4484-820c-d516e6776803.png) as an input
    and returns a fake image based on its implicitly learned distribution ![](img/6d4b20b2-c0bb-4bea-aa9e-67cb0d4497d0.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png) implies that we are feeding
    the fake image generated by the generator to the discriminator ![](img/c50e7ac5-e821-4f32-a3e9-98fa46869323.png)
    and it will return the probability of the fake input image being a real image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we subtract ![](img/2a70ae21-cb43-4d61-a7e5-0f693d027888.png) from 1, then
    it will return the probability of the fake input image being a fake image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/071a1911-3caf-4564-83ab-e2b2fa9338db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know ![](img/225c3938-71c7-4484-820c-d516e6776803.png) is not a real
    image, the discriminator will maximize this probability. That is, the discriminator
    maximizes the probability of ![](img/225c3938-71c7-4484-820c-d516e6776803.png)
    being classified as a fake image, so we write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f59c4f1f-2250-4686-8b1b-5975cb5670c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing raw probabilities, we maximize the log probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03889e75-e14c-4337-8298-7c70686134dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0a57ca04-134f-478d-92e6-d8b43a254a23.png) implies the expectations
    of the log likelihood of the input images generated by the generator being fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Final term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, combining these two terms, the loss function of the discriminator is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ed40285-82fc-4dab-b842-d806edb3a9ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/e5699988-77b0-4be9-b297-db5eac997077.png) and ![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)
    are the parameters of the generator and discriminator network respectively. So,
    the discriminator's goal is to find the right ![](img/53fcaa80-26e4-4563-ab03-15362c7bef18.png)
    so that it can classify the image correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The loss function of the generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the goal of the generator is to fool the discriminator to classify
    the fake image as a real image.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Discriminator loss* section, we saw that ![](img/81a6a38b-a8fa-4963-8aa9-51d6a666e68f.png)
    implies the probability of classifying the fake input image as a fake image, and
    the discriminator maximizes the probabilities for correctly classifying the fake
    image as fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the generator wants to minimize this probability. As the generator wants
    to fool the discriminator, it minimizes this probability of a fake input image
    being classified as fake by the discriminator. Thus, the loss function of the
    generator can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daf801da-e6c1-4e53-ba23-e5dbee02dba5.png)'
  prefs: []
  type: TYPE_IMG
- en: Total loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just learned the loss function of the generator and the discriminator combining
    these two losses, and we write our final loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fed2c57d-3af6-4f68-9fe8-9d1110e591c6.png)'
  prefs: []
  type: TYPE_IMG
- en: So, our objective function is basically a min-max objective function, that is,
    a maximization for the discriminator and minimization for the generator, and we
    find the optimal generator parameter, ![](img/ac253f96-014e-4bfa-95dd-1141d2bb5483.png),
    and discriminator parameter, ![](img/b9808400-7f5c-4f38-85b1-25b5d7382c2a.png),
    through backpropagating the respective networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we perform gradient ascent; that is, maximization on the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e914221-73af-4fb1-9ee4-a36b089c0615.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, we perform gradient descent; that is, minimization on the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c40edd42-f248-4765-89a8-e61a6d456b58.png)'
  prefs: []
  type: TYPE_IMG
- en: However, optimizing the preceding generator objective function does not work
    properly and causes a stability issue. So, we introduce a new form of loss called
    **heuristic loss**.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristic loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is no change in the loss function of the discriminator. It can be directly
    written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the generator loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67836c00-6eb9-40e4-9f5b-ff6060cdbf1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Can we change the minimization objective in our generator loss function into
    a maximization objective just like the discriminator loss? How can we do that?
    We know that ![](img/52f988fd-6940-40f1-934e-5a2c8e24e64e.png) returns the probability
    of the fake input image being fake, and the generator is minimizing this probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of doing this, we can write ![](img/38bf4fa0-4f12-40e9-b554-820f7c851ce3.png).
    It implies the probability of the fake input image being real, and now the generator
    can maximize this probability. It implies a generator is maximizing the probability
    of the fake input image being classified as a real image. So, the loss function
    of our generator now becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So now, we have both the loss function of our discriminator and generator in
    maximizing terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ee266f1-c159-46c1-b5ec-a34e0b824a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b46f9282-dfde-41e9-91f6-851168c1f1f7.png)'
  prefs: []
  type: TYPE_IMG
- en: But, instead of maximizing, if we can minimize the loss, then we can apply our
    favorite gradient descent algorithm. So, how can we convert our maximizing problem
    into a minimization problem? We can do that by simply adding a negative sign.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our final loss function for the discriminator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8551661-694c-4721-95c0-8152c5e22bee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the generator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e5b9f08-9505-47d9-93bb-8268f9e08ad4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating images using GANs in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's strengthen our understanding of GANs by building them to generate handwritten
    digits in TensorFlow. You can also check the complete code used in this section
    here at [http://bit.ly/2wwBvRU](http://bit.ly/2wwBvRU).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import all the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Reading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot one image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The input image looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c9ebcf0-0a0a-4208-8474-d612d72ef1ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generator ![](img/4434a071-305a-479d-a8c5-0bd22491c71f.png) takes the noise
    ![](img/741c2881-b8b5-4486-b60a-a04cdea42a8e.png) as an input and returns an image.
    We define the generator as a feedforward network with three layers. Instead of
    coding the generator network from scratch, we can use `tf.layers.dense()`, which
    can be used to create a dense layer. It takes three parameters: `inputs`, the
    number of `units`, and the `activation` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Defining the discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that discriminator ![](img/41441aa5-777c-4f31-98b6-5a0473ab3d5f.png)
    returns the probability of the given image being real. We define the discriminator
    also as a feedforward network with three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Defining the input placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we define the `placeholder` for the input ![](img/86e6d30c-84c5-4bc9-8835-6a14f17e6412.png)
    and the noise ![](img/0d340c85-feb6-46d2-8925-71dae6013f74.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Starting the GAN!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we feed the noise ![](img/54c257b7-0532-4089-8b03-8639d2deef7b.png)
    to the generator and it will output the fake image, [![](img/2b698d44-e58f-4fe3-a804-704bf6bca446.png)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we feed the real image to discriminator [![](img/bc3e35f7-f069-41f7-a2e5-c299aae82489.png)]
    and get the probability of the real image being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we feed the fake image to discriminator ![](img/bc3e35f7-f069-41f7-a2e5-c299aae82489.png)
    and get the probability of the fake image being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Computing the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how to compute the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4572482b-20a3-4e23-9ffe-92bf8e4a724a.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we will implement the first term, ![](img/0a0f6283-d7d7-40f5-a53e-32183646f0cd.png).
  prefs: []
  type: TYPE_NORMAL
- en: The first term, ![](img/56f909fb-f334-40ec-9615-1112af4f1d74.png), implies the
    expectations of the log likelihood of images sampled from the real data distribution
    being real.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is basically the binary cross-entropy loss. We can implement binary cross-entropy
    loss with the `tf.nn.sigmoid_cross_entropy_with_logits()` TensorFlow function.
    It takes two parameters as inputs, `logits` and `labels`, explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `logits` input, as the name suggests, is the logits of the network so it
    is `D_logits_real`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels` input, as the name suggests, is the true label. We learned that
    discriminator should return `1` for real images and `0` for fake images. Since
    we are calculating the loss for input images sampled from the real data distribution,
    the true label is `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `tf.ones_likes()` for setting the labels to 1 with the same shape as
    `D_logits_real`. That is, `labels = tf.ones_like(D_logits_real)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we compute the mean loss using `tf.reduce_mean()`. If you notice, there
    is a minus sign in our loss function, which we added for converting our loss to
    a minimization objective. But, in the following code, there is no minus sign,
    because TensorFlow optimizers will only minimize and not maximize. So we don''t
    have to add minus sign in our implementation because in any case, it will be minimized
    by the TensorFlow optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we will implement the second term, ![](img/42a79faf-bf44-48ec-a5ea-ad1386a8fdef.png).
  prefs: []
  type: TYPE_NORMAL
- en: The second term,![](img/26a031cb-78f8-4c9b-af0a-2ce59962a0bc.png), implies the
    expectations of the log likelihood of images generated by the generator being
    fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the first term, we can use `tf.nn.sigmoid_cross_entropy_with_logits()`
    for calculating the binary cross-entropy loss. In this, the following holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: Logits is `D_logits_fake`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are calculating the loss for the fake images generated by the generator,
    the `true` label is `0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use `tf.zeros_like()` for setting the labels to `0` with the same shape
    as `D_logits_fake`. That is, `labels = tf.zeros_like(D_logits_fake)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we will implement the final loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, combining the preceding two terms, the loss function of the discriminator
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generator loss is given as ![](img/740c5960-10db-49c3-8778-8750cb0841ce.png).
  prefs: []
  type: TYPE_NORMAL
- en: It implies the probability of the fake image being classified as a real image.
    As we calculated binary cross-entropy in the discriminator, we use `tf.nn.sigmoid_cross_entropy_with_logits()`
    for calculating the loss in the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following should be borne in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Logits is `D_logits_fake`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since our loss implies the probability of the fake input image being classified
    as real, the true label is 1\. Because, as we learned, the goal of the generator
    is to generate the fake image and fool the discriminator to classify the fake
    image as a real image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use `tf.ones_like()` for setting the labels to 1 with the same shape as
    `D_logits_fake`. That is, `labels = tf.ones_like(D_logits_fake)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing the loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to optimize our generator and discriminator. So, we collect the
    parameters of the discriminator and generator as ![](img/7f53317e-2841-4d75-8f6e-8f48eb06d64f.png)
    and ![](img/97d5581a-263f-499a-995e-fda7972d0871.png) respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Starting the training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin the training by defining the batch size and the number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Generating handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start the TensorFlow session and generate handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute this for each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the number of batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute this for each batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the batch of data according to the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the batch noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the feed dictionaries with input `x` as `batch_images` and noise `z`
    as `batch_noise`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the discriminator and generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute loss of discriminator and generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the noise to a generator on every 100^(th) epoch and generate an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'During training, we notice how loss decreases and how GANs learn to generate
    images as shown follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92663e9d-4cab-43d1-b46f-ebe087e99064.png)'
  prefs: []
  type: TYPE_IMG
- en: DCGAN – Adding convolution to a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how effective GANs are and how can they be used to generate
    images. We know that a GAN has two components generators that generate the image
    and the discriminator, which acts as a critic to the generated image. As you can
    see, both of these generators and discriminators are basically feedforward neural
    networks. Instead of keeping them as feedforward networks, can we use convolutional
    networks?
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](af0b9e75-a9a0-4bc7-ad23-92d2ac4a2629.xhtml), *Demystifying Convolutional
    Networks*, we have seen the effectiveness of convolutional networks for image-based
    data and how they extract features from images in an unsupervised fashion. Since
    in GANs we are generating images, it is desirable to use convolutional networks
    instead of feedforward networks. So, we introduce a new type of GAN called **DCGAN**.
    It extends the design of GANs with convents. We basically replace the feedforward
    network in the generator and discriminator with a **Convolutional Neural Network**
    (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator uses convolutional layers for classifying the image as a fake
    or real image, while the generator uses convolutional transpose layers to generate
    a new image. Now we will go into detail and see how generators and discriminators
    differ in DCGAN compared to the vanilla GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Deconvolutional generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that the role of the generator is to generate a new image by learning
    the real data distribution. In DCGAN, a generator is composed of convolutional
    transpose and batch norm layers with ReLU activations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that convolutional transpose operation is also known as deconvolution operation
    or fractionally strided convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the generator is the noise, ![](img/5e1d4f79-f5c2-41d3-8f9d-ecbf4c3611a7.png),
    which we draw from a standard normal distribution, and it outputs an image of
    the same size as the images in the training data, say, 64 x 64 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the generator is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7f65bb2-09ba-4fac-9de4-e82461ceddc5.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we convert the noise, *z*, of a 100 x 1 shape into 1024 x 4 x 4 to have
    the shape of width, height, and feature map and it is called the **project and
    reshape**. Following this, we perform a series of convolutional operations with
    fractionally strided convolutions. We apply batch normalization to every layer
    except at the last layer. Also, we apply ReLU activations to every layer but the
    last layer. We apply the tanh activation function to scale the generated image
    between -1 and +1.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see the architecture of a discriminator in DCGAN. As we know, the
    discriminator takes the image and it tells us whether the image is a real image
    or a fake image. Thus, it is basically a binary classifier. The discriminator
    is composed of a series of convolutional and batch norm layers with leaky ReLU
    activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the discriminator is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d66521c-2497-49ca-97fc-d969707b1fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it takes the input image of the 64 x 64 x 3 shape and performs
    a series of convolutional operations with a leaky ReLU activation function. We
    apply batch normalization at all layers except at the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, we don't apply a max pooling operation in both the discriminator and
    the generator. Instead, we apply a strided convolution operation (that is convolution
    operation with strides).
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, we enhance the vanilla GAN by replacing the feedforward network
    in the generator and the discriminator with the convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DCGAN to generate CIFAR images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to implement DCGAN in TensorFlow. We will learn how to use
    DCGAN with images from the **Canadian Institute For Advanced Research** (**CIFAR**)-10
    dataset. CIFAR-10 consists of 60,000 images from 10 different classes that include
    airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We
    will examine how we can use DCGAN to generate such images.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the CIFAR dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we have in our dataset. Define a helper function for plotting
    the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot a few images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The plotted images are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d1d0f1c-2f2e-4917-a53e-bcb70e87aacd.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We define a discriminator as a convolutional network with three convolutional
    layers followed by a fully connected layer. It is composed of a series of convolutional
    and batch norm layers with leaky ReLU activations. We apply batch normalization
    at all layers except at the input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'First convolutional layer with leaky ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Second convolutional layer with batch normalization and leaky ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Third convolutional layer with batch normalization and leaky ReLU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Flatten the output of the final convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the fully connected layer and return the `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Defining the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we learned, the generator performs the transpose convolutional operation.
    The generator is composed of convolutional transpose and batch norm layers with
    ReLU activations. We apply batch normalization to every layer except for the last
    layer. Also, we apply ReLU activations to every layer, but for the last layer,
    we apply the `tanh` activation function to scale the generated image between -1
    and +1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'First fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the shape of the input and apply batch normalization followed by ReLU
    activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The second layer, that is the transpose convolution layer, with batch normalization
    and the ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the third layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the fourth layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the final layer, we don''t apply batch normalization and instead of ReLU,
    we use `tanh` activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Defining the inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the `placeholder` for the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `placeholder` for the learning rate and training boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the batch size and dimension of the noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the noise, *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Starting the DCGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we feed the noise ![](img/e40108a5-8715-4317-b6e0-3ed7bf3d768d.png)
    to the generator and it will output the fake image, ![](img/5219f5f6-2449-4a52-8e89-eb8b835c9b81.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we feed the real image to the discriminator ![](img/8c11cc5f-f780-455e-b9e8-43eee6ebb822.png)
    and get the probability of the real image being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we feed the fake image to the discriminator, ![](img/caf99dc2-e457-4211-aece-566c0c3c6d23.png),
    and get the probability of the fake image being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Computing the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to compute the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The loss function is the same as for the vanilla GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f63bdfc6-d43e-474c-9c7e-e85d82d55fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can directly write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator loss is also the same as for the vanilla GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37464e51-674c-4eea-9109-3569cba6c03a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute it by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing the loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in vanilla GANs, we collect the parameters of the discriminator and
    generator as ![](img/0fd6fd44-0de7-492c-ac17-36ad19b49762.png) and ![](img/7a41e1ee-c7ea-4a8e-899f-eb56b8b92e12.png)
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Train the DCGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin the training. Define the number of batches, epochs, and learning
    rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a helper function for generating and plotting the generated images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute for each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the start and end of the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample batch of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the discriminator for every two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the generator and discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first iteration, DCGAN will generate the raw pixels, but over the series
    of iterations, it will learn to generate real images with following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Epoch: 0, iteration: 0, Discriminator Loss:1.44706475735, Generator Loss:
    0.726667642593`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is generated by DCGAN in the first iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a6f71c4-8fc8-48cb-9cfa-e5abbcfa40e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Least squares GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how GANs are used to generate images. **Least Squares GAN**
    (**LSGAN**) is another simple variant of a GAN. As the name suggests, here, we
    use the least square error as a loss function instead of sigmoid cross-entropy
    loss. With LSGAN, we can improve the quality of images being generated from the
    GAN. But how can we do that? Why do the vanilla GANs generate poor quality images?
  prefs: []
  type: TYPE_NORMAL
- en: If you can recollect the loss function of GAN, we used sigmoid cross-entropy
    as the loss function. The goal of the generator is to learn the distribution of
    the images in the training set, that is, real data distribution, map it to the
    fake distribution, and generate fake samples from the learned fake distribution.
    So, the GANs try to map the fake distribution as close to the true distribution
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: But once the fake samples are on the correct side of the decision surface, then
    gradients tend to vanish even though the fake samples are far away from the real
    distribution. This is due to the sigmoid cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this with the following figure. A decision boundary of vanilla
    GANs with sigmoid cross-entropy as a loss function is shown in the following figure
    where fake samples are represented by a cross, and real samples are represented
    by a dot, and the fake samples for updating the generator are represented by a
    star.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can observe, once the fake samples (star) generated by the generator
    are on the correct side of the decision surface, that is, once the fake samples
    are on the side of real samples (dot) then the gradients tend to vanish even though
    the fakes samples are far away from real distribution. This is due to the sigmoid
    cross-entropy loss, because it does not care whether the fake samples are close
    to real samples; it only looks for whether the fake samples are on the correct
    side of the decision surface. This leads to a problem that when the gradients
    vanish even though the fake samples are far away from the real data distribution
    then the generator cannot learn the real distribution of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/071cee4c-e99a-4af9-afbe-26ace428c6dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can change this decision surface with sigmoid cross-entropy loss to
    a least squared loss. Now, as you can see in the following diagram, although the
    fake samples generated by the generator are on the correct side of the decision
    surface, gradients will not vanish until the fake samples match the true distribution.
    Least square loss forces the updates to match the fake samples to the true samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce0e609d-7871-45f5-af48-fc1b858f1812.png)'
  prefs: []
  type: TYPE_IMG
- en: So, since we are matching a fake distribution to the real distribution, our
    image quality will be improved when we use the least square as a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell gradient updates in the vanilla GANs will be stopped when the
    fake samples are correct side of the decision surface even though they are from
    the real samples, that is, the real distribution. This is due to the sigmoid cross-entropy
    loss and it does not care whether the fake samples are close to real samples,
    it only looks for whether the fake samples are on the correct side. This leads
    to the problem that we cannot learn the real data distribution perfectly. So,
    we use LSGAN, which uses least squared error as a loss function, where the gradient
    updates will not be stopped until the fake samples match the real sample, even
    though the fake samples are on the correct side of the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that, we have learned that the least square loss function improves the generator's
    image quality, how can we rewrite our GANs loss function in terms of least squares?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say *a* and *b* are the actual labels for the generated images and real
    images respectively, then we can write the loss function of the discriminator
    in terms of least square loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b057e777-4dec-4382-b6bf-99f7bcbb8ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, let''s say *c* is the actual label that the generator wants the
    discriminator to believe that the generated image is the real image, so label
    *c* represents the real image. Then we can write the loss function of a generator
    in terms of least square loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15fa12e2-ad1a-4169-ba16-962be484bd24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We set labels for real images as 1 and for fake images 0, so *b* and *c* become
    1 and *a* becomes 0\. So, our final equation can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of the discriminator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76960fd1-f581-464f-aeb7-0629fdbf4171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of the generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71eb6348-2097-425c-8cbd-52f5e6274d3e.png)'
  prefs: []
  type: TYPE_IMG
- en: LSGAN in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing LSGAN is the same as the vanilla GAN, except for the change in
    the loss function. So, instead of looking at the whole code, we will see only
    how to implement LSGAN's loss function in TensorFlow. The complete code for LSGAN
    is available in the GitHub repo at [http://bit.ly/2HMCrrx](http://bit.ly/2HMCrrx).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how the loss function of LSGAN is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/710bce4b-af2e-4d61-9f8e-c501d522e839.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will implement the first term, ![](img/4725ed5b-3c08-4da1-bbdd-79712d987c8f.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will implement the second term, ![](img/a77b6611-7c67-47e8-8bdf-5cfe633c25b7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The final discriminator loss can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator loss, ![](img/9465d1e9-e2f7-4869-8ae1-9eeb5274f502.png), is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: GANs with Wasserstein distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see another very interesting version of a GAN, called Wasserstein
    GAN (**WGAN**). It uses the Wasserstein distance in the GAN's loss function. First,
    let's understand why we need a Wasserstein distance measure and what's wrong with
    our current loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Before going ahead, first, let's briefly explore two popular divergence measures
    that are used for measuring the similarity between two probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Kullback-Leibler** (**KL**) divergence is one of the most popularly used
    measures for determining how one probability distribution diverges from the other.
    Let''s say we have two discrete probability distributions, ![](img/6c753ab9-5878-41f0-9ec5-edc48f6862f4.png)
    and ![](img/69ed2c31-9bab-4e3f-bf38-949adcf9290a.png), then the KL divergence
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4182eb2-4c46-4126-879a-7b2a9c5a7462.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the two distributions are continuous, then the KL divergence can be expressed
    in the integral form as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a142ef95-a449-4938-8ddc-5defcfa88c18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The KL divergence is not symmetric, meaning the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f58cb0a-d00a-4e79-b6a3-5cb2dd9cf866.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Jensen-Shanon** (**JS**) divergence is another measure for measuring
    the similarity of two probability distributions. But unlike the KL divergence,
    the JS divergence is symmetric and it can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4182488-867d-49f6-8c24-4c25433c5dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Are we minimizing JS divergence in GANs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that generators try to learn the real data distribution, ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png),so
    that it can generate new samples from the learned distribution, ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png),
    and the discriminator tells us whether the image is from a real or fake distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned that when ![](img/1f97f9d4-7527-4206-84f7-f382f9313d2d.png),
    then the discriminator cannot tell us whether the image is from a real or a fake
    distribution. It just outputs 0.5 because it cannot differentiate between ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a8ad343-a6b0-4f4e-93e5-babad0eb630b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s recall the loss function of the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad426bbd-9cc9-4705-b5a2-d096c5bf7f2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be simply written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/426607b5-7610-4190-9fc7-8afa1a8692ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(1)* in the preceding equation we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/120b59bc-88ac-4841-9df9-f02a8377dbc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be solved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdec55a6-f7cc-4126-a879-0ae5615e3687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we are basically minimizing the JS divergence in the loss function
    of GAN. So, minimizing the loss function of the GAN basically implies that minimizing
    the JS divergence between the real data distribution, ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and the fake data distribution, ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd01b16-1305-4882-bdaf-b5f238637878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Minimizing the JS divergence between ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png)
    and ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)denotes that the generator
    ![](img/6faaa333-d580-4b08-af29-4b1426f54fe6.png) makes their distribution ![](img/f5588ec8-6f91-4718-b724-e47c7ed9452a.png)
    similar to the real data distribution ![](img/63a081fb-5338-41a2-9b29-1ffe19eb2cba.png).
    But there is a problem with JS divergence. As you can see from the following figure,
    there is no overlap between the two distributions. When there is no overlap or
    when the two distributions do not share the same support, JS divergence will explode
    or return a constant value and the GANs cannot learn properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41053aa8-9b8a-4388-86a6-aead8f1a86de.png)'
  prefs: []
  type: TYPE_IMG
- en: So, to avoid this, we need to change our loss function. Instead of minimizing
    the JS divergence, we use a new distance metric called the Wasserstein distance,
    which tells us how the two distributions are apart from each other even when they
    don't share the same support.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Wasserstein distance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wasserstein distance, also known as the **Earth Movers** (**EM**) distance,
    is one of the most popularly used distance measures in the optimal transport problems
    where we need to move things from one configuration to another.
  prefs: []
  type: TYPE_NORMAL
- en: So, when we have two distributions, ![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png)
    and ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png), **![](img/7136d91b-eeab-48fe-81cb-eebba4d3e159.png)**
    implies that how much amount of work is required for the probability distribution,
    ![](img/a61db979-91a4-4131-815a-bf31ddf33fa7.png)to match the probability distribution
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand the intuition behind the EM distance. We can view the
    probability distribution as a collection of mass. Our goal is to convert one probability
    distribution to another. There are many possible ways to convert one distribution
    to another, but the Wasserstein metric seeks to find the optimal and minimum way
    that has the least cost in conversion.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of conversion can be given as a distance multiplied by the mass.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of information moved from point *x* to point *y* is given as ![](img/d29b995e-bc0c-47d9-b32b-fbd3ef68f5a7.png).
    It is called a **transport plan**. It tells us how much information we need to
    transport from *x* to *y*, and the distance between *x* and *y* is given as ![](img/c0a3dc56-9679-42be-b608-a3befefe83aa.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the cost is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e9fe2f6-9d38-4b19-8733-5afac683c832.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have many *(x,y)* pairs so the expectations across all *(x,y)* pairs are
    given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fd95a28-6445-4341-80ea-3a589d14b8c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It implies the cost of moving from point *x* to *y*. There are many ways to
    move from *x* to *y*, but we are interested only in the optimal path, that is,
    minimum cost, so we rewrite our preceding equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/169f8b36-171c-4d3e-89d8-bf8126b35d40.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *inf* basically implies the minimum value. ![](img/8a73a5b1-50ee-47e1-b013-0f3688ec3db9.png)
    is the set of all possible joint distributions between ![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)and
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png).
  prefs: []
  type: TYPE_NORMAL
- en: So, out of all the possible joint distributions between ![](img/69a7d503-bb1f-4082-bc2b-86d624dfd551.png)and
    ![](img/4f9d5df1-2ec4-4956-8d22-824f05fd46d4.png) we are finding the minimum cost
    required to make one distribution look like another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final equation can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/604b574b-d605-4a67-889f-373f87cf8ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: However, calculating the Wasserstein distance is not a simple task because it
    is difficult to exhaust all possible joint distributions, ![](img/778a8f70-67f7-412c-868f-70dca7f2f92c.png),
    and it turns into another optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to avoid that, we introduce **Kantorovich-Rubinstein duality**. It
    converts our equation into a simple maximization problem, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/654e40c1-f9a8-4f76-acae-21c4b60711a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, but what does the above equation mean? We are basically applying the **supremum**
    over all **k-Lipschitz function**. Wait. What is the Lipschitz function and what
    is supremum? Let's discuss that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the k-Lipschitz function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Lipschitz continuous function is a function that must be continuous and almost
    differentiable everywhere. So, for any function to be a Lipschitz continuous,
    the absolute value of a slope of the function’s graph cannot be more than a constant
    ![](img/432a5132-4fef-4758-9d92-78cf2d93543f.png). This constant ![](img/b01195a3-071a-4904-93fe-575c769ad3c6.png)
    is called the **Lipschitz** **constant**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31ce8ab7-2251-4cbe-99e9-50786dab57fb.png)'
  prefs: []
  type: TYPE_IMG
- en: To put it in simple terms, we can say a function is Lipschitz continuous when
    the derivation of a function is bounded by some constant *K* and it never exceeds
    the constant.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say ![](img/3b220a73-8c4e-4875-94c9-26d104b13bce.png), for instance, ![](img/edb6c058-177f-491b-b616-145458a44679.png)
    is Lipschitz continuous since its derivative ![](img/cfe04913-e1c4-4893-b20b-b70136c71674.png)
    is bounded by 1\. Similarly, ![](img/ba77a203-e692-4ff1-9a5a-50d7c6445344.png) is
    Lipschitz continuous, since its slope is -1 or 1 everywhere. However, it is not
    differentiable at 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s recall our equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fee0720-1bc6-467b-85bf-1e143ddbc99e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, supremum is basically an opposite to infimum. So, supremum over the Lipschitz
    function implies a maximum over k-Lipschitz functions. So, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b95e49d-de4a-4d0b-940d-4f700bc5e3c7.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation basically tells us that we are basically finding a maximum
    distance between the expected value over real samples and the expected value over
    generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function of WGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, why are we learning all this? We saw previously that there is a problem
    with JS divergence in the loss function, so we resorted to the Wasserstein distance.
    Now, our goal of the discriminator is no longer to say whether the image is from
    the real or fake distribution; instead, it tries to maximize the distance between
    real and generated sample. We train the discriminator to learn the Lipschitz continuous
    function for computing the Wasserstein distance between a real and fake data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c312c9e-f1b8-4e4b-a1cd-6ebd930d3d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we need to ensure that our function is a k-Lipschitz function during training.
    So, for every gradient update, we clip the weights of our gradients between a
    lower bound and upper bound, say between -0.01 and +0.01.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the discriminator loss is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2f24144-6878-4458-844e-9c4d299a6670.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing, we convert this into minimization objective by adding
    a negative sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a48705a6-d88f-4ad5-b638-c73e19650074.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3fe43d54-7535-497a-9dd8-36b4b471556f.png)'
  prefs: []
  type: TYPE_IMG
- en: The generator loss is the same as we learned in vanilla GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the loss function of the discriminator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f0a0d87-ca3f-432c-a93a-a77153a1e203.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of the generator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fb4e9be-ec5e-4155-847a-ddafd2187553.png)'
  prefs: []
  type: TYPE_IMG
- en: WGAN in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing the WGAN is the same as implementing a vanilla GAN except that
    the loss function of WGAN varies and we need to clip the gradients of the discriminator.
    Instead of looking at the whole, we will only see how to implement the loss function
    of the WGAN and how to clip the gradients of the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that loss of the discriminator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4235eb4f-685a-4c68-a4fe-4303214216bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And it can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that generator loss is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95d91718-0bb1-4e1a-a461-3fbb11433934.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And it can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We clip the gradients of the discriminator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by understanding the difference between generative and
    discriminative models. We learned that the discriminative models learn to find
    the good decision boundary that separates the classes in an optimal way, while
    the generative models learn about the characteristics of each class.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we understood how GANs work. They basically consist of two neural networks
    called generators and discriminators. The role of the generators is to generate
    a new image by learning the real data distribution, while the discriminator acts
    as a critic and its role is to tell us whether the generated image is from the
    true data distribution or the fake data distribution, basically whether it is
    a real image or a fake image.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about DCGAN where we basically replace the feedforward neural
    networks in the generator and discriminator with convolutional neural networks.
    The discriminator uses convolutional layers for classifying the image as a fake
    or a real image, while the generator uses convolutional transpose layers to generate
    a new image.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about the LSGAN, which replaces the loss function of both the
    generator and the discriminator with a least squared error loss. Because, when
    we use sigmoid cross-entropy as a loss function, our gradients tend to vanish
    once the fake samples on the correct side of the decision boundary even though
    they are not close to the real distribution. So, we replace the cross-entropy
    loss with the least squared error loss where the gradients will not vanish till
    the fake samples match the true distribution. It forces the gradient updates to
    match the fake samples to the real samples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned another interesting type of GAN called the Wassetrtain GAN
    where we use the Wasserstein distance measure in the discriminator's loss function.
    Because in vanilla GANs we are basically minimizing JS divergence and it will
    be constant or results in 0 when the distributions of real data and fake does
    not overlap. To overcome this, we used the Wasserstein distance measure in the
    discriminator's loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about several other interesting types of
    GANs called CGAN, InfoGAN, CycleGAN, and StackGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our knowledge of GANs by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between generative and discriminative models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the role of a generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the role of a discriminator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the loss function of the generator and discriminator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a DCGAN differ from a vanilla GAN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is KL divergence?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the Wasserstein distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the k-Lipschitz continuous function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following papers for further information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative Adversarial Nets* by Ian J Goodfellow, et al., [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
    Networks* by Alec Radford, Soumith Chintala, and Luke Metz, [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Least Squares Generative Adversarial Networks* by Xudong Mao, et al., [https://arxiv.org/pdf/1611.04076.pdf](https://arxiv.org/pdf/1611.04076.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wasserstein GAN* by Martin Arjovsky, Soumith Chintala, and L´eon Bottou, [https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
