- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer Learning for Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that as the number of images available in
    the training dataset increased, the classification accuracy of the model kept
    on increasing, to the extent that a training dataset comprising 8,000 images had
    a higher accuracy on the validation dataset than a training dataset comprising
    1,000 images. However, we do not always have the option of hundreds or thousands
    of images, along with the ground truths of their corresponding classes, in order
    to train a model. This is where transfer learning comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is a technique where we transfer the learning of the model
    on a generic dataset to the specific dataset of interest. Typically, the pretrained
    models used to perform transfer learning are trained on millions of images (which
    are generic and not the dataset of interest to us) and those pretrained models
    are now fine-tuned to our dataset of interest.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about two different families of transfer learning
    architectures – variants of **Visual** **Geometry** **Group** (**VGG**) architecture
    and variants of **residual network** (**ResNet**) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Along with understanding the architectures, we will also understand their application
    in two different use cases, age and gender classification, where we will learn
    about optimizing over both cross-entropy and mean absolute error losses at the
    same time to estimate the age and predict the gender of a person (given an image
    of the person), and facial keypoint detection (detecting the keypoints like eyes,
    eyebrows, and chin contour, given an image of a face as input), where we will
    learn about leveraging neural networks to generate multiple (136 instead of 1)
    continuous outputs in a single prediction. Finally, we will learn about a new
    library that assists in reducing code complexity considerably across the remaining
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the following topics are covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the VGG16 and ResNet architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing facial keypoint detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-task learning: Implementing age estimation and gender classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the `torch_snippets` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code in this chapter is available for reference in the `Chapter05` folder
    of this book’s GitHub repository – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning is a technique where knowledge gained from one task is leveraged
    to solve another similar task.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a model that is trained on millions of images that span thousands of
    object classes (not just cats and dogs). The various filters (kernels) of the
    model would activate for a wide variety of shapes, colors, and textures within
    the images. Those filters can then be reused to learn features on a new set of
    images. Post learning the features, they can be connected to a hidden layer prior
    to the final classification layer for customizing on the new data.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet ([http://www.image-net.org/](http://www.image-net.org/)) is a competition
    hosted to classify approximately 14 million images into 1,000 different classes.
    It has a variety of classes in the dataset, including Indian elephant, lionfish,
    hard disk, hair spray, and jeep.
  prefs: []
  type: TYPE_NORMAL
- en: The deep neural network architectures that we will go through in this chapter
    have been trained on the ImageNet dataset. Furthermore, given the variety and
    the volume of objects that are to be classified in ImageNet, the models are very
    deep so as to capture as much information as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the importance of transfer learning through a hypothetical
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a situation where we are working with images of a road, trying to classify
    them in terms of the objects they contain. Building a model from scratch might
    result in suboptimal results, as the number of images could be insufficient to
    learn the various variations within the dataset (as we saw in the previous use
    case, where training on 8,000 images resulted in a higher accuracy on a validation
    dataset than training on 1,000 images). A pretrained model, trained on ImageNet,
    comes in handy in such a scenario. It would have already learned a lot about the
    traffic-related classes, such as cars, roads, trees, and humans, during training
    on the large ImageNet dataset. Hence, leveraging the already trained model would
    result in faster and more accurate training as the model already knows the generic
    shapes and now has to fit them for the specific images.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the intuition in place, let’s now understand the high-level flow of transfer
    learning, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the input images, normalized by the same *mean* and *standard deviation*
    that was used during the training of the pretrained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch the pretrained model’s architecture. Fetch the weights for this architecture
    that arose as a result of being trained on a large dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discard the last few layers of the pretrained model so that we can fine-tune
    the last layers for this specific dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the truncated pretrained model to a freshly initialized layer (or layers)
    where weights are randomly initialized. Ensure that the output of the last layer
    has as many neurons as the classes/outputs we would want to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the weights of the pretrained model are not trainable (in other
    words, frozen/not updated during backpropagation), but that the weights of the
    newly initialized layer and the weights connecting it to the output layer are
    trainable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do not train the weights of the pretrained model, as we assume those weights
    are already well learned for the task, and hence leverage the learning from a
    large model. In summary, we only *learn* the newly initialized layers for our
    small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Update the trainable parameters over increasing epochs to fit a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have an idea of how to implement transfer learning, let’s understand
    the various architectures, how they are built, and the results when we apply transfer
    learning to the cats versus dogs use case in subsequent sections. First, we will
    cover in detail some of the various architectures that came out of VGG.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the VGG16 architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**VGG** stands for **Visual** **Geometry** **Group**, which is based out of
    the University of Oxford. *16* stands for the number of layers in the model. The
    VGG16 model is trained to classify objects in the ImageNet competition and stood
    as the runner-up architecture in 2014\. The reason we are studying this architecture
    instead of the winning architecture (GoogleNet) is because of its simplicity and
    its broader use by the vision community for several other tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the architecture of VGG16 along with how a VGG16 pretrained
    model is accessible and represented in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `VGG_architecture.ipynb` file located
    in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with using the VGG16 pretrained model in PyTorch, follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `models` module in the `torchvision` package hosts the various pretrained
    models available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the VGG16 model and register the model within the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we have called the `vgg16` method within the `models`
    class. By mentioning `pretrained=True`, we are specifying that we load the weights
    that were used to classify images in the ImageNet competition, and then we are
    registering the model to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding code is available in the associated notebook on
    GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding summary, the 16 layers we mentioned are grouped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The same summary can also be visualized thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic  Description automatically generated](img/B18457_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: VGG16 archietcture'
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are ~138 million parameters (of which ~122 million are the linear
    layers at the end of the network – 102 + 16 + 4 million parameters) in this network,
    which comprises 13 layers of convolution and/or pooling, with an increasing number
    of filters, and 3 linear layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to understand the components of the VGG16 model is by simply printing
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code is available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are three major sub-modules in the model–`features`, `avgpool`,
    and `classifier`. Typically, we would freeze the `features` and `avgpool` modules.
    Delete the `classifier` module (or only a few layers at the bottom) and create
    a new one in its place that will predict the required number of classes corresponding
    to our dataset (instead of the existing 1,000).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing VGG16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now understand how the VGG16 model is used in practice, using the cats
    versus dogs dataset (considering only 500 images in each class for training) in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Implementing_VGG16_for_image_classification.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy the URL from the notebook on GitHub to avoid any issues while
    reproducing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the dataset and specify the training and test directories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assuming that we are working on Google Colab, we perform the following steps,
    where we provide the authentication key and place it in a location where Kaggle
    can use the key to authenticate us and download the dataset:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the dataset and unzip it:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the training and test image folders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Provide the class that returns input-output pairs for the cats and dogs dataset,
    just like we did in the previous chapter. Note that, in this case, we are fetching
    only the first 500 images from each folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The main difference between the `cats_dogs` class in this section and in *Chapter
    4* is the `normalize` function that we are applying using the `Normalize` function
    from the `transforms` module.
  prefs: []
  type: TYPE_NORMAL
- en: When leveraging pretrained models, it is mandatory to resize, permute, and then
    normalize images (as appropriate for that pretrained model), where the images
    are first scaled to a value between 0 and 1 across the 3 channels and then normalized
    to a mean of `[0.485, 0.456, 0.406]` and a standard deviation of `[0.229, 0.224,
    0.225]` across the RGB channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch the images and their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now inspect a sample image and its corresponding class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![A cat wearing a garment  Description automatically generated with low confidence](img/B18457_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Input sample image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the pretrained VGG16 weights and then freeze the `features` module
    and train using the `avgpool` and `classifier` modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we download the pretrained VGG16 model from the `models` class:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify that we want to freeze all the parameters in the model downloaded previously:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace the `avgpool` module to return a feature map of size 1 x 1 instead
    of 7 x 7; in other words, the output is now going to be `batch_size x 512 x 1
    x 1`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have seen `nn.MaxPool2d`, where we are picking the maximum value from every
    section of a feature map. There is a counterpart to this layer called `nn.AvgPool2d`,
    which returns the average of a section instead of the maximum. In both these layers,
    we fix the kernel size. The layer above, `nn.AdaptiveAvgPool2d`, is yet another
    pooling layer with a twist. We specify the output feature map size instead. The
    layer automatically computes the kernel size so that the specified feature map
    size is returned. For example, if the input feature map size dimensions were `batch_size
    x 512 x k x k`, then the pooling kernel size is going to be `k x k`. The major
    advantage of this layer is that whatever the input size, the output from this
    layer is always fixed and, hence, the neural network can accept images of any
    height and width.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `classifier` module of the model, where we first flatten the output
    of the `avgpool` module, connect the 512 units to the 128 units, and perform an
    activation prior to connecting to the output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the loss function (`loss_fn`) and `optimizer`, and return them along
    with the defined model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: Note that, in the preceding code, we have first frozen all the parameters of
    the pretrained model and have then overwritten the `avgpool` and `classifier`
    modules. Now, the rest of the code is going to look similar to what we have seen
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: The output of the preceding code is available in the associated notebook on
    GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the number of trainable parameters is only 65,793 out of a total of
    14.7 million, as we have frozen the `features` module and have overwritten the
    `avgpool` and `classifier` modules. Now, only the `classifier` module will have
    weights that will be learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function to train on a batch, calculate the accuracy, and get data
    just like we did in the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train on a batch of data:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to calculate accuracy on a batch of data:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to fetch the data loaders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `get_data` and `get_model` functions:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over increasing epochs, just like we did in the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the training and test accuracy values over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Training and validation accuracy of VGG16 with 1K training data
    points'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are able to get an accuracy of 98% within the first epoch, even
    on a small dataset of 1,000 images (500 images of each class).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to VGG16, there are the VGG11 and VGG19 pretrained architectures,
    which work just like VGG16 but with a different number of layers. VGG19 has more
    parameters than that of VGG16 as it has a higher number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training and validation accuracy when we use VGG11 and VGG19 in place of
    the VGG16 pretrained model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: (Left) VGG19 accuracy with 1K training data points; (Right) VGG11
    accuracy with 1K training data points'
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the VGG19-based model has slightly better accuracy than that
    of a VGG16-based model with an accuracy of 98% on validation data, the VGG11-based
    model has a slightly lower accuracy of 97%.
  prefs: []
  type: TYPE_NORMAL
- en: From VGG16 to VGG19, we have increased the number of layers, and generally,
    the deeper the neural network, the better its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, if merely increasing the number of layers is the trick, then we could
    keep on adding more layers (while taking care to avoid overfitting) to the model
    to get more accurate results on ImageNet and then fine-tune it for a dataset of
    interest. Unfortunately, that does not turn out to be true.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple reasons why it is not that easy. Any of the following are
    likely to happen as we go deeper in terms of architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: We have to learn a larger number of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanishing gradients arise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is too much information modification at deeper layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet comes into the picture to address this specific scenario of identifying
    when not to learn, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ResNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building too deep a network, there are two problems. In forward propagation,
    the last few layers of the network have almost no information about what the original
    image was. In backpropagation, the first few layers near the input hardly get
    any gradient updates due to vanishing gradients (in other words, they are almost
    zero). To solve both problems, ResNet uses a highway-like connection that transfers
    raw information from the previous few layers to the later layers. In theory, even
    the last layer will have the entire information of the original image due to this
    highway network. And because of the skipping layers, the backward gradients will
    flow freely to the initial layers with little modification.
  prefs: []
  type: TYPE_NORMAL
- en: The term **residual** in the residual network is the additional information
    that the model is expected to learn from the previous layer that needs to be passed
    on to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical residual block appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Residual block'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, while so far we have been interested in extracting the `F(x)`
    value, where `x` is the value coming from the previous layer, in the case of a
    residual network, we are not only extracting the value after passing through the
    weight layers, which is F(x), but also summing up F(x) with the original value,
    which is `x`.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been using standard layers that performed either linear or convolution
    transformations, `F(x)`, along with some non-linear activation. Both of these
    operations in some sense destroy the input information. For the first time, we
    are seeing a layer that not only transforms the input but also preserves it, by
    adding the input directly to the transformation – `F(x) + x`. This way, in certain
    scenarios, the layer has very little burden in remembering what the input is and
    can focus on learning the correct transformation for the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a more detailed look at the residual layer through code by building
    a residual block:'
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this section can be found in the `Implementing_ResNet18_for_image_classification.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a class with the convolution operation (weight layer in the previous
    diagram) in the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in the preceding code, we defined `padding` as the dimension of the
    output when passed through convolution, and the dimension of the input should
    remain the same if we were to sum the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `forward` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are getting an output that is a sum of the input passed
    through the convolution operations and the original input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned how residual blocks work, let’s understand how the
    residual blocks are connected in a pretrained, residual block-based network, ResNet18:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: ResNet18 architecture'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are 18 layers in the architecture, hence it is referred
    to as a ResNet18 architecture. Furthermore, notice how the skip connections are
    made across the network. They are not made at every convolution layer but after
    every two layers instead.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ResNet18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With an understanding of the composition of a ResNet architecture, let’s build
    a model based on the ResNet18 architecture to classify between dogs and cats,
    just like we did in the previous section using VGG16.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a classifier, the code up to *step 3* of the *Implementing VGG16*
    section remains the same as it deals with importing packages, fetching data, and
    inspecting them. So, we will start by understanding the composition of a pretrained
    ResNet18 model:'
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this section can be found in the `Resnet_block_architecture.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Given that a majority of the code is similar to the code in the *Implementing
    VGG16* section, we have only provided additional code for brevity. For the full
    code, do refer to the notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the pretrained ResNet18 model and inspect the modules within the loaded
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The structure of the ResNet18 model contains the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxPooling`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four layers of ResNet blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average pooling (`avgpool`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected layer (`fc`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we did in VGG16, we will freeze all the different modules but update the
    parameters in the `avgpool` and `fc` modules in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture, loss function, and optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding model, the input shape of the `fc` module is 512, as the output
    of `avgpool` has the shape of batch size x 512 x 1 x 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the model, let’s execute *steps 5* and *6* from the
    *Implementing VGG* section. The variation in training and validation accuracies
    after training the model (where the model is ResNet18, ResNet34, ResNet50, ResNet101,
    and ResNet152 for each of the following charts) over increasing epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Training and validation accuracy with varying numbers of ResNet
    layers'
  prefs: []
  type: TYPE_NORMAL
- en: We see that the accuracy of the model, when trained on only 1,000 images, varies
    between 97% and 98%, where accuracy increases with an increase in the number of
    layers in ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: Besides VGG and ResNet, some of the other prominent pretrained models are Inception,
    MobileNet, DenseNet, and SqueezeNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned about leveraging pretrained models to predict for
    a class that is binary, in the next sections, we will learn about leveraging pretrained
    models to solve real-world use cases that involve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-regression**: Prediction of multiple values given an image as input
    – facial keypoint detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-task learning**: Prediction of multiple items in a single shot – age
    estimation and gender classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing facial keypoint detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about predicting classes that are binary (cats versus
    dogs) or are multi-label (Fashion-MNIST). Let’s now learn a regression problem
    and, in so doing, a task where we are predicting not one but several continuous
    outputs (and hence a multi-regression learning).
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a scenario where you are asked to predict the keypoints present on an
    image of a face; for example, the location of the eyes, nose, and chin. In this
    scenario, we need to employ a new strategy to build a model to detect the keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive further, let’s understand what we are trying to achieve through
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18457_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: (Left) Input image; (Right) Input image overlaid with facial keypoints'
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe in the preceding image, facial keypoints denote the markings
    of various keypoints on an image that contains a face.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, we would first have to solve a few other problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Images can be of different shapes. This warrants an adjustment in the keypoint
    locations while adjusting images to bring them all to a standard image size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial keypoints are similar to points on a scatter plot, but scattered based
    on a certain pattern this time. This means that the values are anywhere between
    0 and 224 if the image is resized to a shape of 224 x 224 x 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the dependent variable (the location of facial keypoints) as per the
    size of the image. The keypoint values are always between 0 and 1 if we consider
    their location relative to image dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that the dependent variable values are always between 0 and 1, we can
    use a sigmoid layer at the end to fetch values that will be between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s formulate the pipeline for solving this facial keypoint detection use
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `2D_and_3D facial_keypoints_detection.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy code from the notebook on GitHub to avoid issues when reproducing
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download and import the relevant data. You can download the relevant data that
    contains images and their corresponding facial keypoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A sample of the imported dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with low confidence](img/B18457_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Input dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, column 1 represents the name of the image, even columns
    represent the *x*-axis value corresponding to each of the 68 keypoints of the
    face, and the rest of the odd columns (except the first column) represent the
    *y*-axis value corresponding to each of the 68 keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `FacesData` class, which provides input and output data points for
    the data loader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s define the `__init__` method, which takes the DataFrame of the file
    (`df`) as input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the mean and standard deviation with which images are to be pre-processed
    so that they can be consumed by the pretrained VGG16 model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define the `__len__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__getitem__` method and fetch the path of the image corresponding
    to a given index (`ix`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the expected output values (keypoints) as a proportion of the size
    of the original image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are ensuring that keypoints are provided as a proportion
    of the original image’s size. This is done so that when we resize the original
    image, the location of the keypoints is not changed, as the keypoints are provided
    as a proportion of the original image. Furthermore, by doing so, we have expected
    output values that are between 0 and 1.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Return the keypoints (`kp2`) and image (`img`) after pre-processing the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function to pre-process an image (`preprocess_input`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load the image, which will be useful when we want to visualize
    a test image and the predicted keypoints of the test image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now create a training and test data split and establish training and
    test datasets and data loaders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we have split the training and test datasets by person
    name in the input data frame and fetched their corresponding objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now define the model that we will leverage to identify keypoints in an
    image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the pretrained VGG16 model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that the parameters of the pretrained model are frozen first:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Overwrite and unfreeze the parameters of the last two layers of the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the last layer of the model in the `classifier` module is a sigmoid
    function that returns a value between 0 and 1 and that the expected output will
    always be between 0 and 1 as keypoint locations are a fraction of the original
    image’s dimensions:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the loss function and optimizer and return them along with the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the loss function is `L1Loss`; in other words, we are performing mean
    absolute error reduction on the prediction of the location of facial keypoints
    (which will be predicted as a percentage of the image’s width and height).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the model, the loss function, and the corresponding optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define functions to train on a batch of data points and also to validate on
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Training a batch, as we have done earlier, involves fetching the output of
    passing input through the model, calculating the loss value, and performing backpropagation
    to update the weights:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a function that returns the loss on test data and the predicted keypoints:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model based on training the data loader and test it on test data,
    as we have done hitherto in previous sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the training and test loss over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Training and test loss over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test our model on a random test image’s index, let’s say `0`. Note that in
    the following code, we are leveraging the `load_img` method in the `FacesData`
    class that was created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B18457_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: (Left) Original image; (Right) Original image overlaid with predicted
    facial keypoints'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we see that the model is able to identify the facial
    keypoints fairly accurately.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have built the facial keypoint detector model from scratch.
    However, there are pretrained models that are built both for 2D and 3D point detection.
    Given that there are multiple datasets/sources to fetch images of faces, what
    if we build a model on a larger dataset that contains a larger number of images
    of faces than what we have in the dataset we used previously?
  prefs: []
  type: TYPE_NORMAL
- en: 2D and 3D facial keypoint detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will leverage a pretrained model that can detect the 2D
    and 3D keypoints present in a face in a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `2D_and_3D facial_keypoints.ipynb` file
    located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy the code from the notebook on GitHub to avoid issues when reproducing
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work on this, we will leverage the `face-alignment` library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the face alignment method, where we specify whether we want to fetch
    keypoint landmarks in 2D or 3D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the input image and provide it to the `get_landmarks` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding lines of code, we are leveraging the `get_landmarks` method
    in the `fa` class to fetch the 68 *x* and *y* coordinates corresponding to the
    facial keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the image with the detected keypoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output. Notice the scatter plot
    of + symbols around the 68 possible facial keypoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated
    with medium confidence](img/B18457_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Input image overlaid with predicted keypoints'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar manner, the 3D projections of facial keypoints are obtained as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Note that the only change from the code used in the 2D keypoints scenario is
    that we specified `LandmarksType` to be 3D in place of 2D. The preceding code
    results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Predicted 3D facial keypoints'
  prefs: []
  type: TYPE_NORMAL
- en: With the code leveraging the `face_alignment` library, we see that we are able
    to leverage the pretrained facial keypoint detection models to have high accuracy
    in predicting on new images.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, across different use cases, we have learned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cats versus dogs**: Predicting for binary classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FashionMNIST**: Predicting for a label among 10 possible classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial keypoints**: Predicting multiple values between 0 and 1 for a given
    image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn about predicting a binary class and a regression
    value together in a single shot using a single network.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing age estimation and gender classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-task learning is a branch of research where a single/few inputs are used
    to predict several different but ultimately connected outputs. For example, in
    a self-driving car, the model needs to identify obstacles, plan routes, and give
    the right amount of throttle/brake and steering, to name but a few. It needs to
    do all of these in a split second by considering the same set of inputs (which
    would come from several sensors). Furthermore, multi-task learning helps in learning
    domain-specific features that can be cross-leveraged across different tasks, potentially
    within the same domain.
  prefs: []
  type: TYPE_NORMAL
- en: From the various use cases we have solved so far, we are in a position to train
    a neural network and estimate the age of a person when given an image or predict
    the gender of the person given an image, separately, one task at a time. However,
    we have not looked at a scenario where we will be able to predict both age and
    gender in a single shot from an image. Predicting two different attributes in
    a single shot is important, as the same image is used for both predictions (this
    will be further appreciated as we perform object detection in *Chapter 7*).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about predicting both attributes, continuous
    and categorical predictions, in a single forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this section can be found in the `Age_and_gender_prediction.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    Be sure to copy the code from the notebook on GitHub to avoid any issues while
    reproducing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with predicting both continuous and categorical attributes in a single
    forward pass, follow the steps outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset we downloaded can be loaded and is structured in the following
    way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Input dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, note that the dataset contains the path to the file,
    age, gender, and race corresponding to the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the `GenderAgeClass` class, which takes a filename as input and returns
    the corresponding image, gender, and scaled age. We scale `age` as it is a continuous
    number and, as we have seen in *Chapter 3*, it is better to scale data to avoid
    vanishing gradients and then rescale it during post-processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Provide file paths (`fpaths`) of images in the `__init__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__len__` method as the one that returns the number of images in
    the input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `__getitem__` method that fetches information of an image at a given
    position, `ix`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write a function that pre-processes an image, which involves resizing the image,
    permuting the channels, and performing normalization on a scaled image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `collate_fn` method, which fetches a batch of data where the data
    points are pre-processed as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Process each image using the `process_image` method.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale the age by 80 (the maximum `age` value present in the dataset), so that
    all values are between 0 and 1.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert `gender` to a float value.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`, `age`, and `gender` are each converted into `torch` objects and returned:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now define the training and validation datasets and data loaders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the datasets:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the data loaders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model, loss function, and optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, in the function, we load the pretrained VGG16 model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, freeze the loaded model (by specifying `param.requires_grad = False`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Overwrite the `avgpool` layer with our own layer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now comes the key part. We deviate from what we have learned so far by creating
    two branches of outputs. This is performed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a neural network `class` named `ageGenderClassifier` with the following
    in the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the `intermediate` layer calculations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define `age_classifier` and `gender_classifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: Note that, in the preceding code, the last layers have a sigmoid activation
    since the `age` output will be a value between 0 and 1 (as it is scaled by 80)
    and `gender` has a sigmoid as the output is either a `0` or a `1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `forward` pass method that stacks layers as `intermediate` first,
    followed by `age_classifier` and then `gender_classifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Overwrite the `classifier` module with the class we defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the loss functions of both the gender (binary cross-entropy loss) and
    age (L1 loss) predictions. Define the optimizer and return the model, loss functions,
    and optimizer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Call the `get_model` function to initialize values in the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the function to train on a batch of data and validate on a batch of
    the dataset. The `train_batch` method takes an image, then actual values of gender,
    age, model, optimizer, and loss function, as input, in order to calculate the
    loss, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `train_batch` method with the input arguments in place:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify that we are training the model, reset the optimizer to `zero_grad`,
    and calculate the predicted value of `age` and `gender`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch the loss functions for both age and gender before calculating the loss
    corresponding to age estimation and gender classification:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the overall loss by summing up `gender_loss` and `age_loss` and perform
    backpropagation to reduce the overall loss by optimizing the trainable weights
    of the model and return the overall loss:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `validate_batch` method takes the image, model, and loss functions, as
    well as the actual values of age and gender, as input to calculate the predicted
    values of age and gender along with the loss values, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `validate_batch` function with proper input parameters:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify that we want to evaluate the model, and so no gradient calculations
    are required before predicting the age and gender values by passing the image
    through the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the loss values corresponding to age and gender predictions (`gender_loss`
    and `age_loss`). We squeeze the predictions (which have a shape of `(batch size,
    1)` so that they are reshaped to the same shape as the original values (which
    have a shape of batch size):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the overall loss and the final predicted gender class (`pred_gender`),
    and return the predicted gender, age, and total loss:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over five epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define placeholders to store the train and test loss values and also to specify
    the number of epochs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through different epochs and reinitialize the train and test loss values
    at the start of each epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through the training data loader (`train_loader`) and train the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through the test data loader and calculate gender accuracy as well as
    the `mae` of `age`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the overall accuracy of age prediction and gender classification:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Log the metrics for each epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the age and gender accuracy of the test dataset in each epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the accuracy of age estimation and gender prediction over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: (Left) Gender prediction accuracy; (Right) Mean Absolute Error
    in predicting age'
  prefs: []
  type: TYPE_NORMAL
- en: We are off by 6 years in terms of age prediction and are approximately 84% accurate
    in predicting the gender.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a prediction of age and gender on a random test image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fetch an image. Feel free to choose your own image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image and pass it through the `preprocess_image` method in the `trn`
    object that we created earlier:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the image through the trained model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the image along with printing the original and predicted values:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18457_05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: Prediction on sample image'
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding use case, we can see that we are able to make predictions
    for both age and gender in a single shot. However, we need to note that this is
    highly unstable and that the age value varies considerably with different orientations
    of the image and also lighting conditions. Data augmentation comes in handy in
    such a scenario. Further, as an exercise, do train your own model where you extract
    the facial region only. This way, the background information is not being considered
    to calculate the age and gender of a person.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about transfer learning, pretrained architectures, and
    how to leverage them in two different use cases. You would have also noticed that
    the code is slightly on the lengthier side where we import extensive packages
    manually, create empty lists to log metrics, and constantly read/show images for
    debugging purposes. In the next section, we will learn about a library that the
    authors have built to avoid such verbose code.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the torch_snippets library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have noticed, we are using the same functions in almost all the sections.
    It is a waste of our time to write the same lines of functions again and again.
    For convenience, we, the authors of this book, have written a Python library by
    the name of `torch_snippets` so that our code looks short and clean.
  prefs: []
  type: TYPE_NORMAL
- en: Utilities such as reading an image, showing an image, and the entire training
    loop are quite repetitive. We want to circumvent writing the same functions over
    and over by wrapping them in code that is preferably a single function call. For
    example, to read a color image, we need not write `cv2.imread(...)` followed by
    `cv2.cvtColor(...)` every time. Instead, we can simply call `read(...)`. Similarly,
    for `plt.imshow(...)`, there are numerous hassles, including the fact that the
    size of the image should be optimal, and that the channel dimension should be
    last (remember, PyTorch has them first).
  prefs: []
  type: TYPE_NORMAL
- en: These will always be taken care of by the single function, `show`. Similar to
    `read` and `show`, there are over 20 convenience functions and classes that we
    will be using throughout the book. We will use `torch_snippets` from now on so
    as to focus more on actual deep learning without distractions. Let’s dive in a
    little and understand the salient functions by training `age` and `gender` with
    this library instead so that we can learn how to use these functions and derive
    the maximum benefit.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this section can be found in the `age_gender_torch_snippets.ipynb`
    file located in the `Chapter05` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For brevity, we have only provided the additional code here. For the full code,
    refer to the notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand and utilize the functions for training a model that predicts
    both age and gender, we’ll start by installing and loading the necessary library.
    Follow the steps below to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Right out of the gate, the library allows us to load all the important `torch`
    modules and utilities such as NumPy, pandas, Matplotlib, Glob, Os, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the data and create a dataset as in the previous section. Create a
    dataset class, `GenderAgeClass`, with a few changes, which are shown in bold in
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code block, the line `im = read(file, 1)` is wrapping `cv2.imread`
    and `cv2.COLOR_BGR2RGB` into a single function call. The “1” stands for “read
    as color image” and, if not given, will load a black and white image by default.
    There is also a `resize` function that wraps `cv2.resize`. Next, let’s look at
    the `show` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the training and validation datasets and view the sample images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we are dealing with images throughout the book, it makes sense to wrap `import
    matplotlib.pyplot as plt` and `plt.imshow` into a function. Calling `show(<2D/3D-Tensor>)`
    will do exactly that. Unlike Matplotlib, it can plot torch arrays present on the
    GPU, irrespective of whether the image contains a channel as the first dimension
    or the last dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The keyword `title` will plot a title with the image, and the keyword `sz` (short
    for size) will plot a larger/smaller image based on the integer value passed (if
    not passed, `sz` will pick a sensible default based on image resolution). In the
    object detection chapters (*Chapters 7* and *8*), we will use the same function
    to show bounding boxes as well. Check out `help(show)` for more arguments. Let’s
    create some datasets here and inspect the first batch of images along with their
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create data loaders and inspect the tensors. Inspecting tensors for their data
    type, min, mean, max, and shape is such a common activity that it is wrapped as
    a function. It can accept any number of tensor inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `inspect` output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Create `model`, `optimizer`, `loss_functions`, `train_batch`, and `validate_batch`
    as usual. As each deep learning experiment is unique, there aren’t any wrapper
    functions for this step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we will leverage the `get_model`, `train_batch`, and `validate_batch`
    functions that we defined in the previous section. For brevity, we are not providing
    the code in this section. However, all the relevant code is available in the corresponding
    notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to load all the components and start training. Log the metrics
    over increasing epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is a highly repetitive loop with minimal changes required. We will always
    loop over a fixed number of epochs, first over the training data loader and then
    over the validation data loader. Each batch is called using either `train_batch`
    or `validate_batch`, every time you have to create empty lists of metrics and
    keep track of them after training/validation. At the end of an epoch, you have
    to print the averages of all of these metrics and repeat the task. It is also
    helpful that you know how long (in seconds) each epoch/batch is going to train
    for. Finally, at the end of the training, it is common to plot the same metrics
    using `matplotlib`. All of these are wrapped into a single utility called `Report`.
    This is a Python class that has different methods to understand. The bold parts
    in the following code highlight the functionality of `Report`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The `Report` class is instantiated with the only argument, the number of epochs
    to be trained on, and is instantiated just before the start of training.
  prefs: []
  type: TYPE_NORMAL
- en: At each training/validation step, we can call the `Report.record` method with
    exactly one positional argument, which is the position (in terms of batch number)
    of training/validation we are at (typically, this is `( epoch_number + (1+batch
    number)/(total_N_batches) )`. Following the positional argument, we pass a bunch
    of keyword arguments that we are free to choose. If it’s training loss that needs
    to be captured, the keyword argument could be `trn_loss`. In the preceding, we
    are logging four metrics, `trn_loss`, `val_loss`, `val_gender_acc`, and `val_age_mae`,
    without creating a single empty list.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does it record but it will also print the same losses in the output.
    The use of `'\r'` as an end argument is a special way of saying replace this line
    the next time a new set of losses are to be recorded. Furthermore, `Report` will
    compute the time remaining for training and validation automatically and print
    that too. `Report` will remember when the metric was logged and print all the
    average metrics at that epoch when the `Report.report_avgs` function is called.
    This will be a permanent print.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the same average metrics are plotted as a line chart in the function
    call `Report.plot_epochs`, without the need for formatting (you can also use `Report.plot`
    to plot every batch metric of the entire training, but this might look messy).
    The same function can selectively plot metrics if asked for. By way of an example,
    in the preceding case, if you are interested in plotting only the `trn_loss` and
    `val_loss` metrics, this can be done by calling `log.plot_epochs(['trn_loss, 'val_loss'])`
    or even simply `log.plot_epochs('_loss')`. This will search for a string match
    with all the metrics and figure out what metrics we are asking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once training is complete, the output for the preceding code snippet should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_05_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: Training and validation loss over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output has corresponding training and validation dataset loss
    and accuracy values for age and gender values, even though we did not initialize
    any empty lists to log those metrics in the training and validation datasets (which
    we did in the previous sections).
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a sample image and effect a prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the above steps, we are able to get the task done with much fewer lines
    of code. To summarize, here are the important functions (and the functions they
    are wrapped around) that we will use in the rest of the book wherever needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from torch_snippets import *`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glob (`glob.glob`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose`(np.random.choice)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read (`cv2.imread`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show (`plt.imshow`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subplots (`plt.subplots` – show a list of images)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspect (`tensor.min`, `tensor.mean`, `tensor.max`, `tensor.shape`, and `tensor.dtype`
    – statistics of several tensors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report (keeping track of all metrics while training and plotting them after
    training)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can view the complete list of functions by running `torch_snippets; print(dir(torch_snippets))`.
    For each function, you can print its help using `help(function)` or even simply
    `??function` in a Jupyter notebook. With the understanding of leveraging `torch_snippets`,
    you should be able to simplify the code considerably. You will notice this in
    action starting with the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how transfer learning helps to achieve high
    accuracy, even with a smaller number of data points. We have also learned about
    the popular pretrained models VGG and ResNet. Furthermore, we understood how to
    build models when we are trying to predict different scenarios, such as the location
    of keypoints on a face and combining loss values when training a model to predict
    for both age and gender together, where age is of a certain data type and gender
    is of a different data type.
  prefs: []
  type: TYPE_NORMAL
- en: With this foundation of image classification through transfer learning, in the
    next chapter, we will learn about some of the practical aspects of training an
    image classification model. We will learn how to explain a model, tips and tricks
    for training a model to achieve high accuracy, and finally, the pitfalls that
    a practitioner needs to avoid when implementing a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are VGG and ResNet pre-trained architectures trained on?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does VGG11 have an inferior accuracy to VGG16?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the number 11 in VGG11 represent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the term *residual* mean in “residual network” refer to?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage of a residual network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the various popular pretrained models discussed in the book and what
    is the speciality of each network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During transfer learning, why should images be normalized with the same mean
    and standard deviation as those that were used during the training of the pre-trained
    model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When and why do we freeze certain parameters in a model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we know the various modules that are present in a pre-trained model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we train a model that predicts categorical and numerical values together?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why might age and gender prediction code not always work for an image of your
    own if we were to execute the same code as that which we wrote in the *Implementing
    age estimation and gender classification* section?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we further improve the accuracy of the facial keypoint recognition model
    that we discussed in the *Implementing facial keypoint detection* section?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
