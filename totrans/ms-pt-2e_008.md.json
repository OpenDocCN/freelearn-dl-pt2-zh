["```py\nrwrds = [-1, 0, 0, 0, 2]\nQvals = [[0.0, 0.0],\n         [0.0, 0.0],\n         [0.0, 0.0],\n         [0.0, 0.0],\n         [0.0, 0.0]]\n```", "```py\nend_states = [1, 0, 0, 0, 1]\n```", "```py\ndef eps_greedy_action_mechanism(eps, S):\n  rnd = np.random.uniform()\n  if rnd < eps:\n    return np.random.randint(0, 2)\n  else:\n    return np.argmax(Qvals[S])\n```", "```py\nn_epsds = 100\neps = 1\ngamma = 0.9\nfor e in range(n_epsds):\n  S_initial = 2 # start with state S2\n  S = S_initial\n  while not end_states[S]:\n    a = eps_greedy_action_mechanism(eps, S)\n    R, S_next = take_action(S, a)\n    if end_states[S_next]:\n      Qvals[S][a] = R\n    else:\n      Qvals[S][a] = R + gamma * max(Qvals[S_next])\n    S = S_next\n  eps = eps - 1/n_epsds\n```", "```py\ndef take_action(S, a):\n  if a == 0: # move up\n    S_next = S - 1\n  else:\n    S_next = S + 1\n  return rwrds[S_next], S_next\n```", "```py\n# general imports\nimport cv2\nimport math\nimport numpy as np\nimport random\n# reinforcement learning related imports\nimport re\nimport atari_py as ap\nfrom collections import deque\nfrom gym import make, ObservationWrapper, Wrapper\nfrom gym.spaces import Box\n# pytorch imports\nimport torch\nimport torch.nn as nn\nfrom torch import save\nfrom torch.optim import Adam\n```", "```py\nclass ConvDQN(nn.Module):\n    def __init__(self, ip_sz, tot_num_acts):\n        super(ConvDQN, self).__init__()\n        self._ip_sz = ip_sz\n        self._tot_num_acts = tot_num_acts\n        self.cnv1 = nn.Conv2d(ip_sz[0], 32, kernel_size=8, stride=4)\n        self.rl = nn.ReLU()\n        self.cnv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.cnv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.fc1 = nn.Linear(self.feat_sz, 512)\n        self.fc2 = nn.Linear(512, tot_num_acts)\n```", "```py\n def forward(self, x):\n        op = self.cnv1(x)\n        op = self.rl(op)\n        op = self.cnv2(op)\n        op = self.rl(op)\n        op = self.cnv3(op)\n        op = self.rl(op).view(x.size()[0], -1)\n        op = self.fc1(op)\n        op = self.rl(op)\n        op = self.fc2(op)\n        return op\n```", "```py\n @property\n    def feat_sz(self):\n        x = torch.zeros(1, *self._ip_sz)\n        x = self.cnv1(x)\n        x = self.rl(x)\n        x = self.cnv2(x)\n        x = self.rl(x)\n        x = self.cnv3(x)\n        x = self.rl(x)\n        return x.view(1, -1).size(1)\n    def perf_action(self, stt, eps, dvc):\n        if random.random() > eps:\n            stt=torch.from_numpy(np.float32(stt)).unsqueeze(0).to(dvc)\n            q_val = self.forward(stt)\n            act = q_val.max(1)[1].item()\n        else:\n            act = random.randrange(self._tot_num_acts)\n        return act\n```", "```py\ndef models_init(env, dvc):\n    mdl = ConvDQN(env.observation_space.shape, env.action_space.n).to(dvc)\n    tgt_mdl = ConvDQN(env.observation_space.shape, env.action_space.n).to(dvc)\n    return mdl, tgt_mdl\n```", "```py\nclass RepBfr:\n    def __init__(self, cap_max):\n        self._bfr = deque(maxlen=cap_max)\n    def push(self, st, act, rwd, nxt_st, fin):\n        self._bfr.append((st, act, rwd, nxt_st, fin))\n    def smpl(self, bch_sz):\n        idxs = np.random.choice(len(self._bfr), bch_sz, False)\n        bch = zip(*[self._bfr[i] for i in idxs])\n        st, act, rwd, nxt_st, fin = bch\n        return (np.array(st), np.array(act), np.array(rwd,      dtype=np.float32),np.array(nxt_st), np.array(fin, dtype=np.uint8))\n    def __len__(self):\n        return len(self._bfr)\n```", "```py\ndef gym_to_atari_format(gym_env):\n    ...\ndef check_atari_env(env):\n    ...\n```", "```py\nclass CCtrl(Wrapper):\n    ...\nclass FrmDwSmpl(ObservationWrapper):\n    ...\nclass MaxNSkpEnv(Wrapper):\n    ...\nclass FrRstEnv(Wrapper):\n    ...\nclass FrmBfr(ObservationWrapper):\n    ...\nclass Img2Trch(ObservationWrapper):\n    ...\nclass NormFlts(ObservationWrapper):\n    ... \n```", "```py\ndef wrap_env(env_ip):\n    env = make(env_ip)\n    is_atari = check_atari_env(env_ip)\n    env = CCtrl(env, is_atari)\n    env = MaxNSkpEnv(env, is_atari)\n    try:\n        env_acts = env.unwrapped.get_action_meanings()\n        if \"FIRE\" in env_acts:\n            env = FrRstEnv(env)\n    except AttributeError:\n        pass\n    env = FrmDwSmpl(env)\n    env = Img2Trch(env)\n    env = FrmBfr(env, 4)\n    env = NormFlts(env)\n    return env\n```", "```py\ndef calc_temp_diff_loss(mdl, tgt_mdl, bch, gm, dvc):\n    st, act, rwd, nxt_st, fin = bch        st = torch.from_numpy(np.float32(st)).to(dvc)\n    nxt_st =      torch.from_numpy(np.float32(nxt_st)).to(dvc)\n    act = torch.from_numpy(act).to(dvc)\n    rwd = torch.from_numpy(rwd).to(dvc)\n    fin = torch.from_numpy(fin).to(dvc)     q_vals = mdl(st)\n    nxt_q_vals = tgt_mdl(nxt_st)        q_val = q_vals.gather(1, act.unsqueeze(-1)).squeeze(-1)\n    nxt_q_val = nxt_q_vals.max(1)[0]\n    exp_q_val = rwd + gm * nxt_q_val * (1 - fin)        loss = (q_val -exp_q_val.data.to(dvc)).pow(2).   mean()\n    loss.backward()\n```", "```py\ndef upd_grph(mdl, tgt_mdl, opt, rpl_bfr, dvc, log):\n    if len(rpl_bfr) > INIT_LEARN:\n        if not log.idx % TGT_UPD_FRQ:\n            tgt_mdl.load_state_dict(mdl.state_dict())\n        opt.zero_grad()\n        bch = rpl_bfr.smpl(B_S)\n        calc_temp_diff_loss(mdl, tgt_mdl, bch, G, dvc)\n        opt.step()\n```", "```py\ndef upd_eps(epd):\n    last_eps = EPS_FINL\n    first_eps = EPS_STRT\n    eps_decay = EPS_DECAY\n    eps = last_eps + (first_eps - last_eps) * math.exp(-1 * ((epd + 1) / eps_decay))\n    return eps\n```", "```py\ndef fin_epsd(mdl, env, log, epd_rwd, epd, eps):\n    bst_so_far = log.upd_rwds(epd_rwd)\n    if bst_so_far:\n        print(f\"checkpointing current model weights. highest running_average_reward of\\\n{round(log.bst_avg, 3)} achieved!\")\n        save(mdl.state_dict(), f\"{env}.dat\")\n    print(f\"episode_num {epd}, curr_reward: {epd_rwd},       best_reward: {log.bst_rwd},\\running_avg_reward: {round(log.avg, 3)}, curr_epsilon: {round(eps, 4)}\")\n```", "```py\ndef run_epsd(env, mdl, tgt_mdl, opt, rpl_bfr, dvc, log, epd):\n    epd_rwd = 0.0\n    st = env.reset()\n    while True:\n        eps = upd_eps(log.idx)\n        act = mdl.perf_action(st, eps, dvc)\n        env.render()\n        nxt_st, rwd, fin, _ = env.step(act)\n        rpl_bfr.push(st, act, rwd, nxt_st, fin)\n        st = nxt_st\n        epd_rwd += rwd\n        log.upd_idx()\n        upd_grph(mdl, tgt_mdl, opt, rpl_bfr, dvc, log)\n        if fin:\n            fin_epsd(mdl, ENV, log, epd_rwd, epd, eps)\n            break\n```", "```py\nclass TrMetadata:\n    def __init__(self):\n        self._avg = 0.0\n        self._bst_rwd = -float(\"inf\")\n        self._bst_avg = -float(\"inf\")\n        self._rwds = []\n        self._avg_rng = 100\n        self._idx = 0\n```", "```py\n @property\n    def bst_rwd(self):\n        ...\n    @property\n    def bst_avg(self):\n        ...\n    @property\n    def avg(self):\n        ...\n    @property\n    def idx(self):\n        ...\n    ...\n```", "```py\ndef train(env, mdl, tgt_mdl, opt, rpl_bfr, dvc):\n    log = TrMetadata()\n    for epd in range(N_EPDS):\n        run_epsd(env, mdl, tgt_mdl, opt, rpl_bfr, dvc, log, epd)\n```", "```py\nB_S = 64\nENV = \"Pong-v4\"\nEPS_STRT = 1.0\nEPS_FINL = 0.005\nEPS_DECAY = 100000\nG = 0.99\nINIT_LEARN = 10000\nLR = 1e-4\nMEM_CAP = 20000\nN_EPDS = 2000\nTGT_UPD_FRQ = 1000\n```", "```py\nenv = wrap_env(ENV)\ndvc = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmdl, tgt_mdl = models_init(env, dvc)\nopt = Adam(mdl.parameters(), lr=LR)\nrpl_bfr = RepBfr(MEM_CAP)\ntrain(env, mdl, tgt_mdl, opt, rpl_bfr, dvc)\nenv.close()\n```"]