- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Composing Music with Generative Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成模型作曲音乐
- en: In the preceding chapters, we discussed a number of generative models focused
    on tasks such as image, text, and video generation. In the contexts of very basic
    MNIST digit generation to more involved tasks like mimicking Barack Obama, we
    explored a number of complex works along with their novel contributions, and spent
    time understanding the nuances of the tasks and datasets involved.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们讨论了许多以图像、文本和视频生成为重点的生成模型。从非常基本的 MNIST 数字生成到像模仿巴拉克·奥巴马这样更复杂的任务，我们探讨了许多复杂的作品及其新颖的贡献，并花时间了解所涉及的任务和数据集的细微差别。
- en: We saw, in the previous chapters on text generation, how improvements in the
    field of computer vision helped usher in drastic improvements in the NLP domain
    as well. Similarly, audio is another domain where the cross-pollination of ideas
    from computer vision and NLP domains has broadened the perspective. Audio generation
    is not a new field, but thanks to research in the deep learning space, this domain
    has seen some tremendous improvements in recent years as well.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在之前关于文本生成的章节中，计算机视觉领域的改进如何帮助促进自然语言处理领域的显著进步。同样，音频是另一个领域，在这个领域，来自计算机视觉和自然语言处理领域的思想交叉已经扩大了视角。音频生成并不是一个新领域，但由于深度学习领域的研究，这个领域近年来也取得了一些巨大的进步。
- en: Audio generation has a number of applications. The most prominent and popular
    ones nowadays are a series of smart assistants (Google Assistant, Apple Siri,
    Amazon Alexa, and so on). These virtual assistants not only try to understand
    natural language queries but also respond in a very human-like voice. Audio generation
    also finds applications in the field of assistive technologies, where text-to-speech
    engines are used for reading content onscreen for people with reading disabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 音频生成有许多应用。如今最突出和流行的是一系列智能助手（谷歌助手、苹果 Siri、亚马逊 Alexa 等等）。这些虚拟助手不仅试图理解自然语言查询，而且还以非常人性化的声音作出回应。音频生成还在辅助技术领域找到应用，在那里，文本到语音引擎用于为阅读障碍者阅读屏幕上的内容。
- en: Leveraging such technologies in the field of music generation is increasingly
    being explored as well. The acquisition of AI-based royalty-free music generation
    service Jukedeck by ByteDance (the parent company of social network TikTok) speaks
    volumes for this field's potential value and impact.¹
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在音乐生成领域利用这样的技术越来越受到关注。字节跳动（社交网络 TikTok 的母公司）收购了基于人工智能的免版税音乐生成服务 Jukedeck，这一举动充分展示了这个领域的潜在价值和影响力。¹
- en: In fact, AI-based music generation is a growing trend with a number of competing
    solutions and research work. Commercial offerings for computer-aided music generation,
    such as Apple's GarageBand,² provide a number of easy-to-use interfaces for novices
    to compose high-quality music tracks with just a few clicks. Researchers on Google's
    project Magenta³ are pushing the boundaries of music generation to new limits
    by experimenting with different technologies, tools, and research projects to
    enable people with little to no knowledge of such complex topics to generate impressive
    pieces of music on their own.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，基于人工智能的音乐生成是一个不断增长的趋势，有许多竞争性的解决方案和研究工作。诸如苹果的 GarageBand² 等商业化产品提供了许多易于使用的界面，供新手只需点击几下即可创作出高质量音乐曲目。谷歌的
    Magenta³ 项目的研究人员正在通过尝试不同的技术、工具和研究项目，将音乐生成的边界推向新的极限，使对这些复杂主题几乎没有知识的人也能够自己生成令人印象深刻的音乐作品。
- en: 'In this chapter, we will cover different concepts, architectures, and components
    associated with generative models for audio data. In particular, we will limit
    our focus to music generation tasks only. We will concentrate on the following
    topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍与音频数据生成模型相关的不同概念、架构和组件。特别是，我们将把焦点限制在音乐生成任务上。我们将专注于以下主题：
- en: A brief overview of music representations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐表示的简要概述
- en: RNN-based music generation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 RNN 的音乐生成
- en: A simple setup to understand how GANs can be leveraged for music generation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的设置，以了解如何利用 GANs 进行音乐生成
- en: Polyphonic music generation based on MuseGAN
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 MuseGAN 的复音音乐生成
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的所有代码片段都可以直接在Google Colab中运行。出于空间原因，依赖项的导入语句没有包含在内，但读者可以参考GitHub存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)。
- en: We will begin with an introduction to the task of music generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从音乐生成任务的介绍开始。
- en: Getting started with music generation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习音乐生成
- en: Music generation is an inherently complex and difficult task. Doing so with
    the help of algorithms (machine learning or otherwise) is even more challenging.
    Nevertheless, music generation is an interesting area of research with a number
    of open problems and fascinating works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成是一个固有复杂且困难的任务。通过算法（机器学习或其他方式）进行这样的任务甚至更具挑战性。尽管如此，音乐生成是一个有趣的研究领域，有许多待解决的问题和令人着迷的作品。
- en: In this section, we will build a high-level understanding of this domain and
    understand a few important and foundational concepts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将建立对这一领域的高层次理解，并了解一些重要且基础的概念。
- en: 'Computer-assisted music generation or, more specifically, deep music generation
    (due to the use of deep learning architectures) is a multi-level learning task
    composed of score generation and performance generation as its two major components.
    Let''s briefly discuss each of these components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机辅助音乐生成或更具体地说是深度音乐生成（由于使用深度学习架构）是一个由生成乐谱和表现生成两个主要组成部分组成的多层学习任务。让我们简要讨论每个组件：
- en: '**Score generation**: A score is a symbolic representation of music that can
    be used/read by humans or systems to produce music. To draw an analogy, we can
    safely consider the relationship between scores and music to be similar to that
    between text and speech. Music scores consist of discrete symbols which can effectively
    convey music. Some works use the term *AI Composer* to denote models associated
    with the task of score generation.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成乐谱**：乐谱是音乐的符号表示，可供人类或系统使用/阅读以生成音乐。类比一下，我们可以将乐谱与音乐之间的关系安全地视为文本与言语之间的关系。音乐乐谱由离散符号组成，可以有效地传达音乐。一些作品使用术语*AI作曲家*来表示与生成乐谱任务相关的模型。'
- en: '**Performance generation**: Continuing with the text-speech analogy, performance
    generation (analogous to speech) is where performers use scores to generate music
    using their own characteristics of tempo, rhythm, and so on. Models associated
    with the task of performance generation are sometimes termed *AI Performers* as
    well.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表现生成**：延续文本-语音类比，表现生成（类似于言语）是表演者使用乐谱以其自己的节奏、韵律等特征生成音乐的地方。与表现生成任务相关的模型有时也被称为*AI表演者*。'
- en: 'We can realize different use cases or tasks based on which component is being
    targeted. *Figure 11.1* highlights a few such tasks that are being researched
    in the context of music generation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据目标组件针对不同的用例或任务来实现。*图11.1*强调了在音乐生成的上下文中正在研究的一些任务：
- en: '![](img/B16176_11_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_01.png)'
- en: 'Figure 11.1: Different components of music generation and associated list of
    tasks'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：音乐生成的不同组件及相关任务列表
- en: As shown in the figure, by focusing on score generation alone, we can work toward
    tasks such as melody generation and melody harmonization, as well as music in-painting
    (associated with filling in missing or lost information in a piece of music).
    Apart from the composer and the performer, there is also research being done toward
    building AI DJs. Similar to human disc jockeys (DJs), AI DJs make use of pre-existing
    music components to create medleys, mashups, remixes, and even highly personalized
    playlists.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，通过仅关注乐谱生成，我们可以致力于诸如旋律生成和旋律和声化以及音乐修补（与填补音乐中缺失或丢失的信息相关联）等任务。除了作曲家和表演者之外，还有研究正在进行中，旨在构建AI
    DJ。与人类唱片骑师（DJ）类似，AI DJ利用现有的音乐组件创建串烧、混搭、混音，甚至高度个性化的播放列表。
- en: In the coming sections, we will work mainly toward building our own score generation
    models or AI Composers. Now that we have a high-level understanding of the overall
    music generation landscape, let's focus on understanding how music is represented.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将主要致力于构建我们自己的乐谱生成模型或AI作曲家。既然我们对整体音乐生成的景观有了高层次的理解，让我们专注于理解音乐的表示方式。
- en: Representing music
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示音乐
- en: Music is a work of art that represents mood, rhythm, emotions, and so on. Similar
    to text, which is a collection of alphabets and grammatical rules, music scores
    have their own notation and set of rules. In the previous chapters, we discussed
    how textual data is first transformed into usable vector form before it can be
    leveraged for any NLP task. We would need to do something similar in the case
    of music as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐是代表情绪、节奏、情感等的艺术作品。类似于文本，文本是字母和语法规则的集合，音乐谱有自己的符号和一套规则。在前几章中，我们讨论了如何在任何自然语言处理任务之前，将文本数据首先转换为可用的向量形式。在音乐的情况下，我们也需要做类似的事情。
- en: 'Music representation can be categorized into two main classes: continuous and
    discrete. The continuous representation is also called the **audio domain representation**.
    It handles music data as waveforms. An example of this is presented in *Figure
    11.2 (a)*. Audio domain representation captures rich acoustic details such as
    timbre and articulation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐表示可以分为两大类：连续和离散。连续表示也称为**音频领域表示**。它将音乐数据处理为波形。如图11.2（a）所示，音频领域表示捕捉丰富的声学细节，如音色和发音。
- en: '![Graphical user interface  Description automatically generated](img/B16176_11_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成的描述](img/B16176_11_02.png)'
- en: 'Figure 11.2: Continuous or audio domain representation of music. a) 1D waveforms
    are a direct representation of audio signals. b) 2D representations of audio data
    can be in the form of spectrograms with one axis as time and second axis as frequency'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：音乐的连续或音频领域表示。a）1D波形是音频信号的直接表示。b）音频数据的二维表示可以是以时间为一轴，频率为第二轴的频谱图形式。
- en: 'As shown in the figure, audio domain representations can be directly as 1D
    waveforms or 2D spectrograms:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，音频领域表示可以直接是1D波形或2D频谱图：
- en: The 1D waveform is a direct representation of the audio signal, with the *x*-axis
    as time and the *y*-axis as changes to the signal
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维波形是音频信号的直接表示，其中*x*轴表示时间，*y*轴表示信号的变化。
- en: The 2D spectrogram introduces time as the *x*-axis and frequency as the *y*-axis
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维频谱图将时间作为*x*轴，频率作为*y*轴。
- en: We typically use **Short-Time Fourier Transformation** (**STFT**) to convert
    from 1D waveform to 2D spectrogram. Based on how we achieve the final spectrogram,
    there are different varieties such as Mel-spectrograms or magnitude spectrograms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用**短时傅里叶变换**（**STFT**）将一维波形转换为二维频谱图。根据我们如何获得最终的频谱图，有不同的变体，如梅尔频谱图或幅度频谱图。
- en: 'On the other hand, discrete or **symbolic representation** makes use of discrete
    symbols to capture information related to pitch, duration, chords, and so on.
    Even though less expressive than audio-domain representation, symbolic representation
    is widely used across different music generation works. This popularity is primarily
    due to the ease of understanding and handling such a representation. *Figure 11.3*
    showcases a sample symbolic representation of a music score:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，离散或**符号表示**使用离散符号来捕获与音高、持续时间、和弦等相关的信息。尽管不如音频域表示那样具有表现力，但符号表示被广泛应用于不同的音乐生成工作中。这种流行程度主要是由于易于理解和处理这种表示形式。*图11.3*展示了音乐谱的一个示例符号表示：
- en: '![](img/B16176_11_03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_03.png)'
- en: 'Figure 11.3: Discrete or symbolic representation of music'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：音乐的离散或符号表示
- en: As shown in the figure, a symbolic representation captures information using
    various symbols/positions. **MIDI**, or **Musical Instrument Digital Interface**,
    is an interoperable format used by musicians to create, compose, perform, and
    share music. It is a common format used by various electronic musical instruments,
    computers, smartphones, and even software to read and play music files.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，符号表示使用各种符号/位置捕获信息。**MIDI**，或**音乐乐器数字接口**，是音乐家用来创建、谱写、演奏和分享音乐的可互操作格式。它是各种电子乐器、计算机、智能手机甚至软件用来读取和播放音乐文件的常用格式。
- en: The symbolic representation can be designed to capture a whole lot of events
    such as *note-on*, *note-off*, *time shift*, *bar*, *track*, and so on. To understand
    the upcoming sections and the scope of this chapter, we will only focus on two
    main events, note-on and note-off. The MIDI format captures 16 channels (numbered
    0 to 15), 128 notes, and 128 loudness settings (also called velocity). There are
    a number of other formats as well, but for the purposes of this chapter, we will
    leverage MIDI-based music files only as these are widely used, expressive, interoperable,
    and easy to understand.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 符号表示可以设计成捕捉诸如*note-on*、*note-off*、*时间偏移*、*小节*、*轨道*等许多事件。为了理解即将出现的部分和本章的范围，我们只会关注两个主要事件，即*note-on*和*note-off*。MIDI
    格式捕捉了 16 个通道（编号为 0 到 15）、128 个音符和 128 个响度设置（也称为速度）。还有许多其他格式，但为了本章的目的，我们将仅使用基于
    MIDI 的音乐文件，因为它们被广泛使用、富有表现力、可互操作且易于理解。
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have covered quite some ground so far in terms of understanding the overall
    music generation landscape and a few important representation techniques. Next,
    we will get started with music generation itself.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在理解整体音乐生成格局和一些重要的表示技术方面已经取得了相当大的进展。接下来，我们将开始进行音乐生成本身。
- en: Music generation using LSTMs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行音乐生成
- en: As we saw in the previous section, music is a continuous signal, which is a
    combination of sounds from various instruments and voices. Another characteristic
    is the presence of structural recurrent patterns which we pay attention to while
    listening. In other words, each musical piece has its own characteristic coherence,
    rhythm, and flow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 歌曲是连续信号，由各种乐器和声音的组合构成，这一点我们在前一节已经看到。另一个特点是结构性的循环模式，我们在听歌时要注意。换句话说，每首音乐都有其独特的连贯性、节奏和流畅性。
- en: Such a setup is similar to the case of text generation we saw in *Chapter 9*,
    *The Rise of Methods for Text Generation*. In the case of text generation, we
    saw the power and effectiveness of LSTM-based networks. In this section, we will
    extend a stacked LSTM network for the task of music generation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的设置与我们在 *第 9 章* *文本生成方法的兴起* 中看到的文本生成情况类似。在文本生成的情况下，我们看到了基于 LSTM 的网络的力量和有效性。在本节中，我们将扩展堆叠
    LSTM 网络来执行音乐生成任务。
- en: To keep things simple and easy to implement, we will focus on a single instrument/monophonic
    music generation task. Let's first look at the dataset and think about how we
    would prepare it for our task of music generation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单和易于实现，我们将专注于单个乐器/单声部音乐生成任务。让我们先看看数据集，然后考虑如何为我们的音乐生成任务准备它。
- en: Dataset preparation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集准备
- en: 'MIDI is an easy-to-use format which helps us extract a symbolic representation
    of music contained in the files. For the hands-on exercises in this chapter, we
    will make use of a subset of the massive public MIDI dataset collected and shared
    by reddit user *u/midi_man*, which is available at this link:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: MIDI 是一种易于使用的格式，可以帮助我们提取文件中包含的音乐的符号表示。在本章的实践练习中，我们将利用 reddit 用户*u/midi_man*收集并分享的大规模公共
    MIDI 数据集的子集，该数据集可以在以下链接中找到：
- en: '[https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_inte)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_inte](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_inte)'
- en: The subset is based on classical piano pieces by great musicians such as Beethoven, Bach,
    Bartok, and the like. The subset can be found in a zipped folder, `midi_dataset.zip`,
    along with the code in this book's GitHub repository.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝多芬、巴赫、巴托克等伟大音乐家的古典钢琴作品。该子集可以在压缩文件`midi_dataset.zip`中找到，并且连同本书的代码一起放在 GitHub
    存储库中。
- en: 'As mentioned previously, we will make use of `music21` to process the subset
    of this dataset and prepare our data for training the model. As music is a collection
    of sounds from various instruments and voices/singers, for the purpose of this
    exercise we will first use the `chordify()` function to extract chords from the
    songs. The following snippet helps us to get a list of MIDI scores in the required
    format:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们将利用 `music21` 来处理此数据集的子集，并准备我们的数据来训练模型。由于音乐是各种乐器和声音/歌手的集合，因此为了本练习的目的，我们将首先使用
    `chordify()` 函数从歌曲中提取和弦。以下代码片段可以帮助我们以所需格式获取 MIDI 分数的列表：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we have the list of scores, the next step is to extract notes and their
    corresponding timing information. For extracting these details, `music21` has
    simple-to-use interfaces such as `element.pitch` and `element.duration`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了分数列表，下一步就是提取音符及其对应的时间信息。为了提取这些细节，`music21`具有诸如`element.pitch`和`element.duration`之类的简单易用的接口。
- en: 'The following snippet helps us extract such information from MIDI files and
    prepare two parallel lists:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段帮助我们从MIDI文件中提取这样的信息，并准备两个并行的列表：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now have the mapping ready. In the following snippet, we prepare the training
    dataset as sequences of length 32 with their corresponding target as the very
    next token in the sequence:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好映射。在以下代码片段中，我们将训练数据集准备为长度为32的序列，并将它们的对应目标设为序列中紧接着的下一个标记：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we''ve seen, the dataset preparation stage was mostly straightforward apart
    from a few nuances associated with the handling of MIDI files. The generated sequences
    and their corresponding targets are shown in the following output snippet for
    reference:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，数据集准备阶段除了与处理MIDI文件相关的一些细微差别之外，大部分都是直截了当的。生成的序列及其对应的目标在下面的输出片段中供参考：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The transformed dataset is now a sequence of numbers, just like in the text
    generation case. The next item on the list is the model itself.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据集现在是一系列数字，就像文本生成的情况一样。列表中的下一项是模型本身。
- en: LSTM model for music generation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于音乐生成的LSTM模型
- en: As mentioned earlier, our first music generation model will be an extended version
    of the LSTM-based text generation model from *Chapter 9*, *The Rise of Methods
    for Text Generation*. Yet there are a few caveats that we need to handle and necessary
    changes that need to be made before we can use the model for this task.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的第一个音乐生成模型将是*第9章*，*文本生成方法的崛起*中基于LSTM的文本生成模型的扩展版本。然而，在我们可以将该模型用于这项任务之前，有一些注意事项需要处理和必要的变更需要进行。
- en: Unlike text generation (using Char-RNN) where we had only a handful of input
    symbols (lower- and upper-case alphabets, numbers), the number of symbols in the
    case of music generation is pretty large (~500). Add to this list of symbols a
    few additional ones required for time/duration related information as well. With
    this larger input symbol list, the model requires more training data and capacity
    to learn (capacity in terms of the number of LSTM units, embedding size, and so
    on).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不像文本生成（使用Char-RNN）只有少数输入符号（小写和大写字母、数字），音乐生成的符号数量相当大（~500）。在这个符号列表中，还需要加入一些额外的符号，用于时间/持续时间相关的信息。有了这个更大的输入符号列表，模型需要更多的训练数据和学习能力（学习能力以LSTM单元数量、嵌入大小等方面来衡量）。
- en: The next obvious change we need to take care of is the model's capability to
    take two inputs for every timestep. In other words, the model should be able to
    take the notes as well as duration information as input at every timestep and
    generate an output note with its corresponding duration. To do so, we leverage
    the functional `tensorflow.keras` API to prepare a multi-input multi-output architecture
    in place.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要处理的下一个明显变化是模型能够在每个时间步骤上接受两个输入的能力。换句话说，模型应能够在每个时间步骤上接受音符和持续时间信息，并生成带有相应持续时间的输出音符。为此，我们利用功能性的`tensorflow.keras`
    API，构建一个多输入多输出的架构。
- en: 'As discussed in detail in *Chapter 9*, *The Rise of Methods for Text Generation*,
    stacked LSTMs have a definite advantage in terms of being able to learn more sophisticated
    features over networks with a single LSTM layer. In addition to that, we also
    discussed **attention mechanisms** and how they help in alleviating issues inherent
    to RNNs, such as difficulties in handling long-range dependencies. Since music
    is composed of local as well as global structures which are perceivable in the
    form of rhythm and coherence, attention mechanisms can certainly make an impact.
    The following code snippet prepares a multi-input stacked LSTM network in the
    manner discussed:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*第9章*，*文本生成方法的崛起*中详细讨论的那样，堆叠的LSTM在能够学习更复杂特征方面具有明显优势，这超过了单个LSTM层网络的能力。除此之外，我们还讨论了**注意机制**以及它们如何帮助缓解RNN所固有的问题，比如难以处理长距离依赖关系。由于音乐由在节奏和连贯性方面可感知的局部和全局结构组成，注意机制肯定可以起作用。下面的代码片段按照所讨论的方式准备了一个多输入堆叠的LSTM网络：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Training this model is as simple as calling the `fit()` function on the `keras`
    model object. We train the model for about 100 epochs. *Figure 11.4* depicts the
    learning progress of the model across different epochs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型就像在 `keras` 模型对象上调用 `fit()` 函数一样简单。我们将模型训练约 100 个周期。*图 11.4* 描述了模型在不同周期下的学习进展：
- en: '![Graphical user interface, text  Description automatically generated](img/B16176_11_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本说明自动生成](img/B16176_11_04.png)'
- en: 'Figure 11.4: Model output as the training progresses across epochs'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：模型输出随着训练在不同周期下的进展
- en: As shown in the figure, the model is able to learn a few repeating patterns
    and generate music. Here, we made use of temperature-based sampling as our decoding
    strategy. As discussed in *Chapter 9*, *The Rise of Methods for Text Generation*,
    readers can experiment with techniques such as greedy decoding, pure sampling-based
    decoding, and other such strategies to see how the output music quality changes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，模型能够学习一些重复模式并生成音乐。在这里，我们使用基于温度的抽样作为我们的解码策略。正如在 *第 9 章*，*文本生成方法的兴起* 中讨论的，读者可以尝试诸如贪婪解码、纯抽样解码等技术，以了解输出音乐质量如何变化。
- en: This was a very simple implementation of music generation using deep learning
    models. We drew analogies with concepts learned in the previous two chapters on text
    generation. Next, let's do some music generation using adversarial networks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用深度学习模型进行音乐生成的一个非常简单的实现。我们将之前两章学到的概念与之进行了类比，那两章是关于文本生成的。接下来，让我们使用对抗网络进行一些音乐生成。
- en: Music generation using GANs
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 进行音乐生成
- en: In the previous section, we tried our hand at music generation using a very
    simple LSTM-based model. Now, let's raise the bar a bit and try to see how we
    can generate music using a GAN. In this section, we will leverage the concepts
    related to GANs that we have learned in the previous chapters and apply them to
    generating music.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们尝试使用一个非常简单的基于 LSTM 的模型进行音乐生成。现在，让我们提高一点标准，看看如何使用 GAN 生成音乐。在本节中，我们将利用我们在前几章学到的与
    GAN 相关的概念，并将它们应用于生成音乐。
- en: We've already seen that music is continuous and sequential in nature. LSTMs
    or RNNs in general are quite adept at handling such datasets. We have also seen
    that, over the years, various types of GANs have been proposed to train deep generative
    networks efficiently.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到音乐是连续且序列化的。LSTM 或 RNN 等模型非常擅长处理这样的数据集。多年来，已经提出了各种类型的 GAN，以有效地训练深度生成网络。
- en: 'Combining the power of LSTMs and GAN-based generative networks, Mogren et al.
    presented *Continuous Recurrent Neural Networks with Adversarial Training: C-RNN-GAN*⁴
    in 2016 as a method for music generation. This is a straightforward yet effective
    implementation for music generation. As in the previous section, we will keep
    things simple and focus only on monophonic music generation, even though the original
    paper mentions using features such as tone length, frequency, intensity, and time
    apart from music notes. The paper also mentions a technique called *feature mapping*
    to generate polyphonic music (using the C-RNN-GAN-3 variant). We will only focus
    on understanding the basic architecture and pre-processing steps and not try to
    implement the paper as-is. Let''s begin with defining each of the components of
    our music-generating GAN.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Mogren 等人于 2016 年提出了 *连续循环神经网络与对抗训练：C-RNN-GAN*⁴，结合了 LSTM 和基于 GAN 的生成网络的能力，作为音乐生成的方法。这是一个直接但有效的音乐生成实现。与前一节一样，我们将保持简单，并且只关注单声道音乐生成，尽管原始论文提到了使用音调长度、频率、强度和音符之间的时间等特征。论文还提到了一种称为
    *特征映射* 的技术来生成复调音乐（使用 C-RNN-GAN-3 变体）。我们将只关注理解基本架构和预处理步骤，而不试图按原样实现论文。让我们开始定义音乐生成
    GAN 的各个组件。
- en: Generator network
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器网络
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The generator model is a fairly straightforward implementation which highlights
    the effectiveness of a GAN-based generative model. Next, let us prepare the discriminator
    model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器模型是一个相当简单的实现，突显了基于 GAN 的生成模型的有效性。接下来，让我们准备判别器模型。
- en: Discriminator network
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别器网络
- en: In a GAN setup, the discriminator's job is to differentiate between real and
    generated (or fake) samples. In this case, since the sample to be checked is a
    musical piece, the model needs to have the ability to handle sequential input.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 设置中，判别器的任务是区分真实和生成的（或虚假的）样本。在这种情况下，由于要检查的样本是一首音乐作品，所以模型需要有处理序列输入的能力。
- en: 'In order to handle sequential input samples, we use a simple stacked RNN network.
    The first recurrent layer is an LSTM layer with 512 units, followed by a bi-directional
    LSTM layer. The bi-directionality of the second layer helps the discriminator
    learn about the context better by viewing what came before and after a particular
    chord or note. The recurrent layers are followed by a stack of dense layers and
    a final sigmoid layer for the binary classification task. The discriminator network
    is presented in the following code snippet:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理顺序输入样本，我们使用一个简单的堆叠RNN网络。第一个递归层是一个具有512个单元的LSTM层，后面是一个双向LSTM层。第二层的双向性通过查看特定和弦或音符之前和之后的内容来帮助判别器更好地学习上下文。递归层后面是一堆密集层和一个用于二元分类任务的最终sigmoid层。判别器网络如下代码片段所示：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As shown in the snippet, the discriminator is also a very simple model consisting
    of a few recurrent and dense layers. Next, let us combine all these components
    and train the overall GAN.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码片段所示，判别器也是一个非常简单的模型，由几个递归和密集层组成。接下来，让我们将所有这些组件组合起来并训练整个GAN。
- en: Training and results
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与结果
- en: 'The first step is to instantiate the generator and discriminator models using
    the utilities we presented in the previous sections. Once we have these objects,
    we combine the generator and discriminator into a stack to form the overall GAN.
    The following snippet presents the instantiation of the three networks:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用我们在前几节介绍的实用程序实例化生成器和判别器模型。一旦我们有了这些对象，我们将生成器和判别器组合成一个堆栈，形成整体的GAN。以下片段展示了三个网络的实例化：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we have done in the previous chapters, the discriminator training is first
    set to `false` before it is stacked into the GAN model object. This ensures that
    only the generator weights are updated during the generation cycle and not the
    discriminator weights. We prepare a custom training loop just as we have presented
    a number of times in previous chapters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前几章中所做的那样，在堆叠到GAN模型对象之前，首先将鉴别器训练设置为`false`。这确保只有在生成周期期间更新生成器权重，而不是鉴别器权重。我们准备了一个自定义训练循环，就像我们在之前的章节中多次介绍的那样。
- en: 'For the sake of completeness, we present it again here for reference:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，我们在此提供参考：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We have used the same training dataset as in the previous section. We train
    our setup for about 200 epochs with a batch size of 64\. *Figure 11.5* is a depiction
    of discriminator and generator losses over training cycles, along with a few outputs
    at different intervals:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与上一节相同的训练数据集。我们将我们的设置训练了大约200个时代，批量大小为64。*图11.5*展示了鉴别器和生成器在训练周期中的损失以及在不同时间间隔内的一些输出：
- en: '![](img/B16176_11_05.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_05.png)'
- en: 'Figure 11.5: a) Discriminator and generator losses as the training progresses
    over time. b) The output from the generator model at different training intervals'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：a）随着训练的进行，鉴别器和生成器损失。b）在不同训练间隔内生成器模型的输出
- en: The outputs shown in the figure highlight the potential of a GAN-based music
    generation setup. Readers may choose to experiment with different datasets or
    even the details mentioned in the C-RNN-GAN paper by Mogren et al. The generated
    MIDI files can be played using the MuseScore app.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示的输出突显了基于GAN的音乐生成设置的潜力。读者可以选择尝试不同的数据集，甚至是Mogren等人在C-RNN-GAN论文中提到的细节。生成的MIDI文件可以使用MuseScore应用程序播放。
- en: The output from this GAN-based model as compared to the LSTM-based model from
    the previous section might feel a bit more refined (though this is purely subjective
    given our small datasets). This could be attributed to GANs' inherent ability
    to better model the generation process compared to an LSTM-based model. Please
    refer to *Chapter 6*, *Image Generation with GANs*, for more details on the topology
    of generative models and their respective strengths.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节中基于LSTM的模型相比，基于这个GAN的模型的输出可能会感觉更加精致一些（尽管这纯粹是主观的，考虑到我们的数据集很小）。这可能归因于GAN相对于基于LSTM的模型更好地建模生成过程的固有能力。有关生成模型拓扑结构及其各自优势的更多细节，请参阅*第6章*，*使用GAN生成图像*。
- en: Now that we have seen two variations of monophonic music generation, let us graduate
    toward polyphonic music generation using MuseGAN.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了两种单声部音乐生成的变体，让我们开始尝试使用MuseGAN进行复声音乐生成。
- en: MuseGAN – polyphonic music generation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MuseGAN – 复声音乐生成
- en: The two models we have trained so far have been simplified versions of how music
    is actually perceived. While limited, both the attention-based LSTM model and
    the C-RNN-GAN based model helped us to understand the music generation process
    very well. In this section, we will build on what we've learned so far and make
    a move toward preparing a setup which is as close to the actual task of music
    generation as possible.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们训练的两个模型都是音乐实际感知的简化版本。虽然有限，但基于注意力的LSTM模型和基于C-RNN-GAN的模型都帮助我们很好地理解了音乐生成过程。在本节中，我们将在已学到的基础上进行拓展，朝着准备一个尽可能接近实际音乐生成任务的设置迈出一步。
- en: 'In 2017, Dong et al. presented a GAN-type framework for multi-track music generation
    in their work titled *MuseGAN: Multi-track Sequential Generative Adversarial Networks
    for Symbolic Music Generation and Accompaniment*.⁵ The paper is a detailed explanation
    of various music-related concepts and how Dong and team tackled them. To keep
    things within the scope of this chapter and without losing details, we will touch
    upon the important contributions of the work and then proceed toward the implementation.
    Before we get onto the "how" part, let''s understand the three main properties
    related to music that the MuseGAN work tries to take into account:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '在2017年，Dong等人在他们的作品《MuseGAN: 多轨序列生成对抗网络用于符号音乐生成和伴奏》中提出了一种多轨音乐生成的GAN类型框架⁵。这篇论文详细解释了各种与音乐相关的概念，以及Dong和他的团队是如何应对它们的。为了使事情保持在本章的范围内，又不失细节，我们将涉及这项工作的重要贡献，然后继续进行实现。在我们进入“如何”部分之前，让我们先了解MuseGAN工作试图考虑的与音乐相关的三个主要属性：'
- en: '**Multi-track interdependency**: As we know, most songs that we listen to are
    usually composed of multiple instruments such as drums, guitars, bass, vocals,
    and so on. There is a high level of interdependency in how these components play
    out for the end-user/listener to perceive coherence and rhythm.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多轨互依性**：大多数我们听的歌曲通常由多种乐器组成，如鼓，吉他，贝斯，人声等。在这些组件的演奏方式中存在着很高的互依性，使最终用户/听众能够感知到连贯性和节奏。'
- en: '**Musical texture**: Musical notes are often grouped into chords and melodies.
    These groupings are characterized by a high degree of overlap and not necessarily
    chronological ordering (this simplification of chronological ordering is usually
    applied in most known works associated with music generation). The chronological
    ordering comes not only as part of the need for simplification but also as a generalization
    from the NLP domain, language generation in particular.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音乐结构**：音符常常被分成和弦和旋律。这些分组以高度重叠的方式进行，并不一定是按照时间顺序排列的（这种对时间顺序的简化通常适用于大多数与音乐生成相关的已知作品）。时间顺序的排列不仅是出于简化的需要，也是从NLP领域，特别是语言生成的概括中得出的。'
- en: '**Temporal structure**: Music has a hierarchical structure where a song can
    be seen as being composed of paragraphs (at the highest level). A paragraph is
    composed of various phrases, which are in turn composed of multiple bars, and
    so on. *Figure 11.6* depicts this hierarchy pictorially:![Table  Description automatically
    generated](img/B16176_11_06.png)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间结构**：音乐具有分层结构，一首歌可以看作是由段落组成（在最高级别）。段落由各种短语组成，短语又由多个小节组成，如此类推。*图11.6*以图像方式描述了这种层级结构：![Table
    说明会自动生成](img/B16176_11_06.png)'
- en: 'Figure 11.6: Temporal structure of a song'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：一首歌的时间结构
- en: As shown in the figure, a bar is further composed of beats and at the lowest level,
    we have pixels. The authors of MuseGAN mention a bar as the compositional unit,
    as opposed to notes, which we have been considering the basic unit so far. This
    is done in order to account for grouping of notes from the multi-track setup.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如图所示，一根小节进一步由节拍组成，在最低的级别上，我们有像素。MuseGAN的作者们提到小节作为作曲的单位，而不是音符，这是为了考虑多轨设置中的音符分组。
- en: MuseGAN works toward solving these three major challenges through a unique framework
    based on three music generation approaches. These three basic approaches make
    use of jamming, hybrid, and composer models. We will briefly explain these now.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN通过基于三种音乐生成方法的独特框架来解决这三个主要挑战。这三种基本方法分别采用即兴演奏，混合和作曲家模型。我们现在简要解释一下这些方法。
- en: Jamming model
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即兴演奏模型
- en: 'If we were to extrapolate the simplified-monophonic GAN setup from the previous
    section to a polyphonic setup, the simplest method would be to make use of multiple
    generator-discriminator combinations, one for each instrument. The jamming model
    is precisely this setup, where *M* multiple independent generators prepare music
    from their respective random vectors. Each generator has its own critic/discriminator,
    which helps in training the overall GAN. This setup is depicted in *Figure 11.7*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前一节中的简化单声部GAN设置外推到多声部设置，最简单的方法是利用多个发电机-鉴别器组合，每个乐器一个。干扰模型正是这个设定，其中*M*个独立的发电机从各自的随机向量准备音乐。每个发电机都有自己的评论家/鉴别器，有助于训练整体GAN。此设置如*图11.7*所示：
- en: '![Graphical user interface  Description automatically generated](img/B16176_11_07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成说明](img/B16176_11_07.png)'
- en: 'Figure 11.7: Jamming model composed of M generator and discriminator pairs
    for generating multi-track outputs'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '图11.7: 由M个发电机和鉴别器对组成的干扰模型，用于生成多轨道输出'
- en: As shown in the preceding figure, the jamming setup imitates a grouping of musicians
    who create music by improvising independently and without any predefined arrangement.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，干扰设置模拟了一群音乐家的聚合，他们通过独立即兴创作音乐，没有任何预定义的安排。
- en: Composer model
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作曲家模型
- en: 'As the name suggests, this setup assumes that the generator is a typical human
    composer capable of creating multi-track piano rolls, as shown in *Figure 11.8*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，此设置假设发生器是一个典型的能够创建多轨钢琴卷的人类作曲家，如*图11.8*所示：
- en: '![](img/B16176_11_08.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_08.png)'
- en: 'Figure 11.8: Composer model composed of a single generator capable of generating
    M tracks and a single discriminator for detecting fake versus real samples'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图11.8: 单发电机组成的作曲家模型，能够生成M轨道，一个用于检测假样本和真实样本的鉴别器'
- en: As shown in the figure, the setup only has a single discriminator to detect
    real or fake (generated) samples. This model requires only one common random vector *z* as
    opposed to *M* random vectors in the previous jamming model setup.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，这个设置只有一个鉴别器来检测真实或假的（生成的）样本。与前一个干扰模型设置中的*M*个随机向量不同，这个模型只需要一个公共随机向量*z*。
- en: Hybrid model
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合模型
- en: 'This is an interesting take that arises by combining the jamming and composer
    models. The hybrid model has *M* independent generators that make use of their
    respective random vectors, which are also called *intra-track random vectors*.
    Each generator also takes an additional random vector called the *inter-track
    random vector*. This additional vector is supposed to imitate the composer and
    help in the coordination of the independent generators. *Figure 11.9* depicts
    the hybrid model, with *M* generators each taking intra-track and inter-track
    random vectors as input:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过将干扰和作曲家模型结合而产生的有趣想法。混合模型有*M*个独立的发电机，它们利用各自的随机向量，也被称为*轨内随机向量*。每个发电机还需要另一个称为*轨间随机向量*的额外随机向量。这个额外的向量被认为是模仿作曲家并帮助协调独立的发电机。*图11.9*描述了混合模型，每个发电机都需要轨内和轨间随机向量作为输入：
- en: '![Graphical user interface, logo  Description automatically generated](img/B16176_11_09.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，徽标 自动生成说明](img/B16176_11_09.png)'
- en: 'Figure 11.9: Hybrid model composed of M generators and a single discriminator.
    Each generator takes two inputs in the form of inter-track and intra-track random
    vectors'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图11.9: 由M个发电机和一个单一鉴别器组成的混合模型。每个发电机需要两个输入，即轨间和轨内随机向量的形式。'
- en: As shown in the figure, the hybrid model with its *M* generators works with
    only a single discriminator to predict whether a sample is real or fake. The advantage
    of combining both jamming and composer models is in terms of flexibility and control
    at the generator end. Since we have *M* different generators, the setup allows
    for flexibility to choose different architectures (different input sizes, filters,
    layers, and so on) for different tracks, as well as additional control via the
    inter-track random vector to manage coordination amongst them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，混合模型的*M*发电机仅与一个鉴别器一起工作，以预测一个样本是真实的还是假的。将演奏和作曲家模型结合的优势在于生成器端的灵活性和控制。由于我们有*M*个不同的发电机，这个设定允许在不同的轨道上选择不同的架构（不同的输入大小、过滤器、层等），以及通过轨间随机向量的额外控制来管理它们之间的协调。
- en: Apart from these three variants, the authors of MuseGAN also present a temporal
    model, which we discuss next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三个变体，MuseGAN的作者还提出了一个时间模型，我们将在下面讨论。
- en: Temporal model
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 临时模型
- en: The temporal structure of music was one of the three aspects we discussed as
    top priorities for the MuseGAN setup. The three variants explained in the previous
    sections, jamming, composer, and hybrid models, all work at the bar level. In
    other words, each of these models generates multi-track music bar by bar, but
    possibly with no coherence or continuity between two successive bars. This is
    different from the hierarchical structure where a group of bars makes up a phrase,
    and so on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐的时间结构是我们讨论的 MuseGAN 设置的三个重要方面之一。我们在前几节中解释的三个变体（即即兴、作曲家和混合模型）都在小节级别上工作。换句话说，每个模型都是一小节一小节地生成多音轨音乐，但可能两个相邻小节之间没有连贯性或连续性。这与分层结构不同，分层结构中一组小节组成一个乐句等等。
- en: 'To maintain coherence and the temporal structure of the song being generated,
    the authors of MuseGAN present a temporal model. While generating from scratch
    (as one of the modes), this additional model helps in generating fixed-length
    phrases by taking bar-progression as an additional dimension. This model consists
    of two sub-components, the temporal structure generator *G*[temp] and the bar
    generator *G*[bar]. This setup is presented in *Figure 11.10*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持生成歌曲的连贯性和时间结构，MuseGAN 的作者提出了一个时间模型。在从头开始生成时（作为其中一种模式），该额外的模型通过将小节进行为一个附加维度来生成固定长度的乐句。该模型由两个子组件组成，时间结构生成器
    *G*[时间] 和小节生成器 *G*[小节]。该设置在 *图 11.10* 中呈现：
- en: '![](img/B16176_11_10.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_10.png)'
- en: 'Figure 11.10: Temporal model with its two sub-components, temporal structure
    generator G[temp] and bar generator G[bar]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：时间模型及其两个子组件，时间结构生成器 G[时间] 和小节生成器 G[小节]
- en: 'The temporal structure generator maps a noise vector *z* to a sequence of latent
    vectors ![](img/B16176_11_001.png). This latent vector ![](img/B16176_11_002.png)
    carries temporal information and is then used by *G*[bar] to generate music bar
    by bar. The overall setup for the temporal model is formulated as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 时间结构生成器将噪声向量 *z* 映射到一个潜在向量序列 ![](img/B16176_11_001.png)。这个潜在向量 ![](img/B16176_11_002.png)
    携带时间信息，然后由 *G*[小节] 用于逐小节生成音乐。时间模型的整体设置如下所示：
- en: '![](img/B16176_11_003.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_003.png)'
- en: The authors note that this setup is similar to some of the works on video generation,
    and cite references for further details. The authors also mention another setting
    where a conditional setup is presented for the generation of temporal structure
    by learning from a human-generated track sequence.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，该设置类似于一些关于视频生成的作品，并引用了进一步了解的参考文献。作者还提到了另一种情况，其中呈现了一个条件设置，用于通过学习来生成由人类生成的音轨序列的时间结构。
- en: We have covered details on the specific building blocks for the MuseGAN setup.
    Let's now dive into how these components make up the overall system.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 MuseGAN 设置的具体构建块的细节。现在让我们深入了解这些组件如何构成整个系统。
- en: MuseGAN
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MuseGAN
- en: 'The overall MuseGAN setup is a complex architecture composed of a number of moving
    parts. To bring things into perspective, the paper presents experiments with jamming,
    composer, and hybrid generation approaches at a very high level. To ensure temporal
    structure, the setup makes use of the two-step temporal model approach we discussed
    in the previous section. A simplified version of the MuseGAN architecture is presented
    in *Figure 11.11*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: MuseGAN 的整体设置是一个复杂的架构，由多个活动部分组成。为了使时间结构保持连贯，该设置使用了我们在前一节中讨论的两阶段时间模型方法。*图 11.11*
    展示了 MuseGAN 架构的简化版本：
- en: '![Chart  Description automatically generated with medium confidence](img/B16176_11_11.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图表 11.11 的自动生成说明（中等置信度）](img/B16176_11_11.png)'
- en: 'Figure 11.11: Simplified MuseGAN architecture consisting of a hybrid model
    with M generators and a single discriminator, along with a two-stage temporal
    model for generating phrase coherent output'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：简化的 MuseGAN 架构，由 M 个生成器和一个判别器组成，以及一个用于生成短语连贯输出的两阶段时间模型。
- en: As shown in the figure, the setup makes use of a temporal model for certain
    tracks and direct random vectors for others. The outputs from temporal models,
    as well as direct inputs, are then concatenated (or summed) before they are passed
    to the bar generator model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，该设置使用时间模型用于某些音轨和直接的随机向量用于其他音轨。时间模型和直接输入的输出然后在传递给小节生成器模型之前进行连接（或求和）。
- en: The bar generator then creates music bar by bar and is evaluated using a critic
    or discriminator model. In the coming section, we will briefly touch upon implementation
    details for both the generator and critic models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后小节生成器逐小节创建音乐，并使用评论者或鉴别器模型进行评估。在接下来的部分，我们将简要触及生成器和评论者模型的实现细节。
- en: Please note that the implementation presented in this section is close to the
    original work but not an exact replication. We have taken certain shortcuts in
    order to simplify and ease the understanding of the overall architecture. Interested
    readers may refer to official implementation details and the code repository mentioned
    in the cited work.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本节介绍的实现与原始工作接近，但并非完全复制。为了简化并便于理解整体架构，我们采取了某些捷径。有兴趣的读者可以参考官方实现详情和引文工作中提到的代码库。
- en: Generators
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器
- en: As mentioned in the previous section, the generator setup depends on whether
    we are using the jamming, composer, or hybrid approach. For simplicity, we will
    focus only on the hybrid setup where we have multiple generators, one for each
    track.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，生成器设置取决于我们是使用即兴演奏、作曲家还是混合方法。为简单起见，我们只关注具有多个生成器的混合设置，其中每个音轨都有一个生成器。
- en: 'One set of generators focuses on tracks which require temporal coherence; for
    instance, components such as melody are long sequences (more than one bar long)
    and coherence between them is an important factor. For such tracks, we use a temporal
    architecture as shown in the following snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一组生成器专注于需要时间连贯性的音轨；例如，旋律这样的组件是长序列（超过一小节长），它们之间的连贯性是一个重要因素。对于这样的音轨，我们使用如下片段所示的时间架构：
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As shown, the temporal model firstly reshapes the random vector to desired dimensions,
    then passes it through transposed convolutional layers to expand the output vector
    so that it spans the length of specified bars.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，时间模型首先将随机向量重塑为所需的维度，然后通过转置卷积层将其传递，以扩展输出向量，使其跨越指定小节的长度。
- en: For the tracks where we do not require inter-bar coherence, we directly use
    the random vector *z* as-is. Groove or beat-related information covers such tracks
    in practice.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们不需要小节间连续性的音轨，我们直接使用随机向量 *z*。在实践中，与节奏或节拍相关的信息涵盖了这些音轨。
- en: 'The outputs from the temporal generator and the direct random vectors are first
    concatenated to prepare a larger coordinated vector. This vector then acts as
    an input to the bar generator *G*[bar] shown in the following snippet:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 时序生成器和直接随机向量的输出首先被连结在一起，以准备一个更大的协调向量。然后，这个向量作为输入传递给下面片段所示的小节生成器 *G*[bar]：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Critic
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论者
- en: 'The critic model is simpler compared to the generator we built in the previous
    section. The critic is basically a convolutional WGAN-GP model (similar to WGAN,
    covered in *Chapter 6*, *Image Generation with GANs*), which takes the output
    from the bar generator, as well as real samples, to detect whether the generator
    output is fake or real. The following snippet presents the critic model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 评论者模型相对于我们在前一节中构建的生成器来说更简单。评论者基本上是一个卷积 WGAN-GP 模型（类似于 WGAN，在 *第6章* *使用 GAN 生成图像*
    中涵盖的），它从小节生成器的输出以及真实样本中获取信息，以检测生成器输出是伪造的还是真实的。以下片段呈现了评论者模型：
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: One major point to note here is the use of 3D convolutional layers. We typically
    make use of 2D convolutions for a majority of tasks. In this case, since we have
    4-dimensional inputs, a 3D convolutional layer is required for handling the data
    correctly.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的重点是使用 3D 卷积层。对于大多数任务，我们通常使用 2D 卷积。在这种情况下，由于我们有4维输入，需要使用 3D 卷积层来正确处理数据。
- en: We use these utilities to prepare 4 different generators, one for each track,
    into a common generator model object. In the next step, we prepare the training
    setup and generate some sample music.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些实用工具来为四个不同的音轨准备一个通用的生成器模型对象。在下一步中，我们准备训练设置并生成一些示例音乐。
- en: Training and results
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和结果
- en: 'We have all the components ready. The final step is to combine them and train
    in the manner a typical WGAN-GP is trained. The authors of the paper mention that
    the model achieves stable performance if they update the generators once for every
    5 updates of the discriminator. We follow a similar setup to achieve the results
    shown in *Figure 11.12*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组件都准备就绪。最后一步是将它们组合在一起，并按照典型 WGAN-GP 的训练方式进行训练。论文的作者提到，如果他们每更新5次鉴别器，就更新一次生成器，模型将达到稳定的性能。我们遵循类似的设置来实现
    *图11.12* 中显示的结果：
- en: '![](img/B16176_11_12.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_11_12.png)'
- en: 'Figure 11.12: Results from MuseGAN setup showcase multi-track output, which
    seems to be coherent across bars and has a consistent rhythm to it'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：从MuseGAN设置中得到的结果展示了多轨输出，这在各个小节之间似乎是连贯的，并且具有一致的节奏。
- en: As shown in the figure, the multi-track polyphonic output from MuseGAN indeed
    seems quite impressive. We urge readers to use a MIDI player (or even MuseScore
    itself) to play the generated music samples in order to understand the complexity
    of the output, along with the improvement over the simpler models prepared in
    the earlier sections of the chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，MuseGAN产生的多轨多声部输出确实令人印象深刻。我们鼓励读者使用MIDI播放器（甚至是MuseScore本身）播放生成的音乐样本，以了解输出的复杂性及其相较于前几节中准备的简单模型的改进。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations on completing yet another complex chapter. In this chapter,
    we covered quite a bit of ground in terms of building an understanding of music
    as a source of data, and then various methods of generating music using generative
    models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了另一个复杂的章节。在本章中，我们覆盖了相当多的内容，旨在建立对音乐作为数据源的理解，然后使用生成模型生成音乐的各种方法。
- en: In the first section of this chapter, we briefly discussed the two components
    of music generation, namely *score* and *performance generation*. We also touched
    upon different use cases associated with music generation. The next section focused
    on different methods for music representation. At a high level, we discussed continuous
    and discrete representation techniques. We primarily focused on *1D waveforms*
    and *2D spectrograms* as main representations in the audio or continuous domain.
    For symbolic or discrete representation, we discussed *notes/chords*-based sheet
    music. We also performed a quick hands-on exercise using the `music21` library
    to transform a given MIDI file into readable sheet music.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们简要讨论了音乐生成的两个组成部分，即*乐谱*和*表演生成*。我们还涉及了与音乐生成相关的不同用例。下一部分集中讨论了音乐表示的不同方法。在高层次上，我们讨论了连续和离散的表示技术。我们主要关注*1D波形*和*2D频谱图*作为音频或连续域中的主要表示形式。对于符号或离散表示，我们讨论了基于*音符/和弦*的乐谱。我们还使用`music21`库进行了一个快速的动手练习，将给定的MIDI文件转换成可读的乐谱。
- en: Once we had some basic understanding of how music can be represented, we turned
    toward building music generation models. The first and the simplest method we
    worked upon was based on a stacked LSTM architecture. The model made use of an
    attention mechanism and symbolic representation to generate the next set of notes.
    This LSTM-based model helped us to get a peek into the music generation process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对音乐如何表示有了基本的了解后，我们开始构建音乐生成模型。我们首先研究的最简单方法是基于堆叠的LSTM架构。该模型利用注意力机制和符号表示来生成下一组音符。这个基于LSTM的模型帮助我们窥探了音乐生成的过程。
- en: The next section focused on using a GAN setup to generate music. We designed
    our GAN similar to *C-RNN-GAN* presented by Mogren et al. The results were very
    encouraging and gave us a good insight into how an adversarial network can be
    used for the task of music generation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分集中使用GAN设置来生成音乐。我们设计的GAN类似于Mogren等人提出的*C-RNN-GAN*。结果非常鼓舞人心，让我们深入了解了对抗网络如何被用于音乐生成任务。
- en: During the first two hands-on exercises, we limited our music generation process
    to only monophonic music to keep things simple. In the final section of this chapter,
    our aim was to understand the complexities and techniques required to generate
    polyphonic/multi-track music. We discussed in detail *MuseGAN*, a polyphonic/multi-track
    GAN-based music generation architecture presented by Dong et al. in 2017\. Dong
    and team discussed *multi-track interdependency*, *musical texture*, and *temporal
    structure* as three main aspects that should be handled by any multi-track music
    generation model. They presented three variants for music generation in the form
    of *jamming*, *composer*, and *hybrid models*. They also presented a discussion
    on *temporal* and *bar* generation models to bring things into perspective. The
    MuseGAN paper presents MuseGAN as a complex combination of these smaller components/models
    to handle multi-track/polyphonic music generation. We leveraged this understanding
    to build a simplified version of this work and generate polyphonic music of our
    own.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个动手练习中，我们将我们的音乐生成过程仅限于单声音乐，以保持简单。在本章的最后一节，我们的目标是理解生成复音轨/多轨音乐所需的复杂性和技术。我们详细讨论了*
    MUSEGAN*，这是2017年由Dong等人提出的基于GAN的复音轨/多轨音乐生成架构。Dong和他的团队讨论了*多轨相互依赖*，*音乐纹理*和*时间结构*三个主要方面，这些方面应该由任何多轨音乐生成模型处理。他们提出了音乐生成的三种变体，即*即兴演奏*，*作曲家*和*混合模型*。他们还讨论了*时间*和*小节*生成模型，以便更好地理解这些方面。MUSEGAN论文将混音音乐生成模型作为这些更小组件/模型的复杂组合来处理多轨/复音轨音乐的生成。我们利用了这一理解来构建这项工作的简化版本，并生成了我们自己的复音轨音乐。
- en: This chapter provided us with a look into yet another domain which can be handled
    using generative models. In the next chapter, we will level things up and focus
    on the exciting field of reinforcement learning. Using RL, we will build some
    cool applications as well. Stay tuned.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本章让我们进一步了解了可以使用生成模型处理的另一个领域。在下一章中，我们将升级并专注于令人兴奋的强化学习领域。使用RL，我们也将构建一些很酷的应用程序。请继续关注。
- en: References
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: Butcher, M. (2019, July 23). *It looks like TikTok has acquired Jukedeck, a
    pioneering music AI UK startup*. TechCrunch. [https://techcrunch.com/2019/07/23/it-looks-like-titok-has-acquired-jukedeck-a-pioneering-music-ai-uk-startup/](https://techcrunch.com/2019/07/23/it-looks-like-titok-has-acquired-jukedeck-a-pioneering-music-ai-uk)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Butcher, M. (2019年7月23日). *看起来TikTok已经收购了英国创新音乐人工智能初创公司Jukedeck。* TechCrunch.
    [https://techcrunch.com/2019/07/23/it-looks-like-titok-has-acquired-jukedeck-a-pioneering-music-ai-uk-startup/](https://techcrunch.com/2019/07/23/it-looks-like-titok-has-acquired-jukedeck-a-pioneering-music-ai-uk-startup/)
- en: Apple. (2021). *GarageBand for Mac - Apple*. [https://www.apple.com/mac/garageband/](https://www.apple.com/mac/garageband/)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apple. (2021). *GarageBand for Mac - Apple*. [https://www.apple.com/mac/garageband/](https://www.apple.com/mac/garageband/)
- en: Magenta. (n.d.) *Make Music and Art Using Machine Learning*. [https://magenta.tensorflow.org/](https://magenta.tensorflow.org/)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Magenta. (未知发布日期) *使用机器学习创作音乐和艺术*. [https://magenta.tensorflow.org/](https://magenta.tensorflow.org/)
- en: 'Mogren, O. (2016). *C-RNN-GAN: Continuous recurrent neural networks with adversarial
    training*. Constructive Machine Learning Workshop (CML) at NIPS 2016 in Barcelona,
    Spain, December 10\. [https://arxiv.org/abs/1611.09904](https://arxiv.org/abs/1611.09904)'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mogren, O. (2016). *C-RNN-GAN：带对抗性训练的连续循环神经网络*. NIPS 2016年12月10日，在西班牙巴塞罗那举办的建设性机器学习研讨会(CML)。[https://arxiv.org/abs/1611.09904](https://arxiv.org/abs/1611.09904)
- en: 'Dong, H-W., Hsiao, W-Y., Yang, L-C., & Yang, Y-H. (2017). *MuseGAN: Multi-Track
    Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment*.
    The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18). [https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dong, H-W., Hsiao, W-Y., Yang, L-C., & Yang, Y-H. (2017). *MuseGAN：用于符号音乐生成和伴奏的多轨序列生成对抗网络*.
    第32届AAAI人工智能会议(AAAI-18)。[https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf)
