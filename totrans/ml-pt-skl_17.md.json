["```py\n>>> import torch\n>>> print(torch.__version__)\n1.9.0+cu111\n>>> print(\"GPU Available:\", torch.cuda.is_available())\nGPU Available: True\n>>> if torch.cuda.is_available():\n...     device = torch.device(\"cuda:0\")\n... else:\n...     device = \"cpu\"\n>>> print(device)\ncuda:0 \n```", "```py\n>>> from google.colab import drive\n>>> drive.mount('/content/drive/') \n```", "```py\n>>> import torch.nn as nn\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> ## define a function for the generator:\n>>> def make_generator_network(\n...         input_size=20,\n...         num_hidden_layers=1,\n...         num_hidden_units=100,\n...         num_output_units=784):\n...     model = nn.Sequential()\n...     for i in range(num_hidden_layers):\n...         model.add_module(f'fc_g{i}',\n...                          nn.Linear(input_size, num_hidden_units))\n...         model.add_module(f'relu_g{i}', nn.LeakyReLU())\n...         input_size = num_hidden_units\n...     model.add_module(f'fc_g{num_hidden_layers}',\n...                      nn.Linear(input_size, num_output_units))\n...     model.add_module('tanh_g', nn.Tanh())\n...     return model\n>>> \n>>> ## define a function for the discriminator:\n>>> def make_discriminator_network(\n...         input_size,\n...         num_hidden_layers=1,\n...         num_hidden_units=100,\n...         num_output_units=1):\n...     model = nn.Sequential()\n...     for i in range(num_hidden_layers):\n...         model.add_module(\n...             f'fc_d{i}',\n...             nn.Linear(input_size, num_hidden_units, bias=False)\n...         )\n...         model.add_module(f'relu_d{i}', nn.LeakyReLU())\n...         model.add_module('dropout', nn.Dropout(p=0.5))\n...         input_size = num_hidden_units\n...     model.add_module(f'fc_d{num_hidden_layers}',\n...                      nn.Linear(input_size, num_output_units))\n...     model.add_module('sigmoid', nn.Sigmoid())\n...     return model \n```", "```py\n>>> image_size = (28, 28)\n>>> z_size = 20\n>>> gen_hidden_layers = 1\n>>> gen_hidden_size = 100\n>>> disc_hidden_layers = 1\n>>> disc_hidden_size = 100\n>>> torch.manual_seed(1)\n>>> gen_model = make_generator_network(\n...     input_size=z_size,\n...     num_hidden_layers=gen_hidden_layers,\n...     num_hidden_units=gen_hidden_size,\n...     num_output_units=np.prod(image_size)\n... )\n>>> print(gen_model)\nSequential(\n  (fc_g0): Linear(in_features=20, out_features=100, bias=False)\n  (relu_g0): LeakyReLU(negative_slope=0.01)\n  (fc_g1): Linear(in_features=100, out_features=784, bias=True)\n  (tanh_g): Tanh()\n)\n>>> disc_model = make_discriminator_network(\n...     input_size=np.prod(image_size),\n...     num_hidden_layers=disc_hidden_layers,\n...     num_hidden_units=disc_hidden_size\n... )\n>>> print(disc_model)\nSequential(\n  (fc_d0): Linear(in_features=784, out_features=100, bias=False)\n  (relu_d0): LeakyReLU(negative_slope=0.01)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc_d1): Linear(in_features=100, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n) \n```", "```py\n>>> import torchvision\n>>> from torchvision import transforms\n>>> image_path = './'\n>>> transform = transforms.Compose([\n...     transforms.ToTensor(),\n...     transforms.Normalize(mean=(0.5), std=(0.5)),\n... ])\n>>> mnist_dataset = torchvision.datasets.MNIST(\n...     root=image_path, train=True,\n...     transform=transform, download=False\n... )\n>>> example, label = next(iter(mnist_dataset))\n>>> print(f'Min: {example.min()} Max: {example.max()}')\n>>> print(example.shape)\nMin: -1.0 Max: 1.0\ntorch.Size([1, 28, 28]) \n```", "```py\n>>> def create_noise(batch_size, z_size, mode_z):\n...     if mode_z == 'uniform':\n...         input_z = torch.rand(batch_size, z_size)*2 - 1\n...     elif mode_z == 'normal':\n...         input_z = torch.randn(batch_size, z_size)\n...     return input_z \n```", "```py\n>>> from torch.utils.data import DataLoader\n>>> batch_size = 32\n>>> dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n>>> input_real, label = next(iter(dataloader))\n>>> input_real = input_real.view(batch_size, -1)\n>>> torch.manual_seed(1)\n>>> mode_z = 'uniform'  # 'uniform' vs. 'normal'\n>>> input_z = create_noise(batch_size, z_size, mode_z)\n>>> print('input-z -- shape:', input_z.shape)\n>>> print('input-real -- shape:', input_real.shape)\ninput-z -- shape: torch.Size([32, 20])\ninput-real -- shape: torch.Size([32, 784])\n>>> g_output = gen_model(input_z)\n>>> print('Output of G -- shape:', g_output.shape)\nOutput of G -- shape: torch.Size([32, 784])\n>>> d_proba_real = disc_model(input_real)\n>>> d_proba_fake = disc_model(g_output)\n>>> print('Disc. (real) -- shape:', d_proba_real.shape)\n>>> print('Disc. (fake) -- shape:', d_proba_fake.shape)\nDisc. (real) -- shape: torch.Size([32, 1])\nDisc. (fake) -- shape: torch.Size([32, 1]) \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> ## Loss for the Generator\n>>> g_labels_real = torch.ones_like(d_proba_fake)\n>>> g_loss = loss_fn(d_proba_fake, g_labels_real)\n>>> print(f'Generator Loss: {g_loss:.4f}')\nGenerator Loss: 0.6863\n>>> ## Loss for the Discriminator\n>>> d_labels_real = torch.ones_like(d_proba_real)\n>>> d_labels_fake = torch.zeros_like(d_proba_fake)\n>>> d_loss_real = loss_fn(d_proba_real, d_labels_real)\n>>> d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n>>> print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')\nDiscriminator Losses: Real 0.6226 Fake 0.7007 \n```", "```py\n>>> batch_size = 64\n>>> torch.manual_seed(1)\n>>> np.random.seed(1)\n>>> mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,\n...                       shuffle=True, drop_last=True)\n>>> gen_model = make_generator_network(\n...     input_size=z_size,\n...     num_hidden_layers=gen_hidden_layers,\n...     num_hidden_units=gen_hidden_size,\n...     num_output_units=np.prod(image_size)\n... ).to(device)\n>>> disc_model = make_discriminator_network(\n...     input_size=np.prod(image_size),\n...     num_hidden_layers=disc_hidden_layers,\n...     num_hidden_units=disc_hidden_size\n... ).to(device)\n>>> loss_fn = nn.BCELoss()\n>>> g_optimizer = torch.optim.Adam(gen_model.parameters())\n>>> d_optimizer = torch.optim.Adam(disc_model.parameters()) \n```", "```py\n>>> ## Train the discriminator\n>>> def d_train(x):\n...     disc_model.zero_grad()\n...     # Train discriminator with a real batch\n...     batch_size = x.size(0)\n...     x = x.view(batch_size, -1).to(device)\n...     d_labels_real = torch.ones(batch_size, 1, device=device)\n...     d_proba_real = disc_model(x)\n...     d_loss_real = loss_fn(d_proba_real, d_labels_real)\n...     # Train discriminator on a fake batch\n...     input_z = create_noise(batch_size, z_size, mode_z).to(device)\n...     g_output = gen_model(input_z)\n...     d_proba_fake = disc_model(g_output)\n...     d_labels_fake = torch.zeros(batch_size, 1, device=device)\n...     d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n...     # gradient backprop & optimize ONLY D's parameters\n...     d_loss = d_loss_real + d_loss_fake\n...     d_loss.backward()\n...     d_optimizer.step()\n...     return d_loss.data.item(), d_proba_real.detach(), \\\n...            d_proba_fake.detach()\n>>>\n>>> ## Train the generator\n>>> def g_train(x):\n...     gen_model.zero_grad()\n...     batch_size = x.size(0)\n...     input_z = create_noise(batch_size, z_size, mode_z).to(device)\n...     g_labels_real = torch.ones(batch_size, 1, device=device)\n... \n...     g_output = gen_model(input_z)\n...     d_proba_fake = disc_model(g_output)\n...     g_loss = loss_fn(d_proba_fake, g_labels_real)\n...     # gradient backprop & optimize ONLY G's parameters\n...     g_loss.backward()\n...     g_optimizer.step()\n...     return g_loss.data.item() \n```", "```py\n>>> fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n>>> def create_samples(g_model, input_z):\n...     g_output = g_model(input_z)\n...     images = torch.reshape(g_output, (batch_size, *image_size))\n...     return (images+1)/2.0\n>>> \n>>> epoch_samples = []\n>>> all_d_losses = []\n>>> all_g_losses = []\n>>> all_d_real = []\n>>> all_d_fake = []\n>>> num_epochs = 100\n>>> \n>>> for epoch in range(1, num_epochs+1):\n...     d_losses, g_losses = [], []\n...     d_vals_real, d_vals_fake = [], []\n...     for i, (x, _) in enumerate(mnist_dl):\n...         d_loss, d_proba_real, d_proba_fake = d_train(x)\n...         d_losses.append(d_loss)\n...         g_losses.append(g_train(x))\n...         d_vals_real.append(d_proba_real.mean().cpu())\n...         d_vals_fake.append(d_proba_fake.mean().cpu())\n...         \n...     all_d_losses.append(torch.tensor(d_losses).mean())\n...     all_g_losses.append(torch.tensor(g_losses).mean())\n...     all_d_real.append(torch.tensor(d_vals_real).mean())\n...     all_d_fake.append(torch.tensor(d_vals_fake).mean())\n...     print(f'Epoch {epoch:03d} | Avg Losses >>'\n...           f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'\n...           f' [D-Real: {all_d_real[-1]:.4f}'\n...           f' D-Fake: {all_d_fake[-1]:.4f}]')\n...     epoch_samples.append(\n...         create_samples(gen_model, fixed_z).detach().cpu().numpy()\n...     )\n\nEpoch 001 | Avg Losses >> G/D 0.9546/0.8957 [D-Real: 0.8074 D-Fake: 0.4687]\nEpoch 002 | Avg Losses >> G/D 0.9571/1.0841 [D-Real: 0.6346 D-Fake: 0.4155]\nEpoch ...\nEpoch 100 | Avg Losses >> G/D 0.8622/1.2878 [D-Real: 0.5488 D-Fake: 0.4518] \n```", "```py\n>>> import itertools\n>>> fig = plt.figure(figsize=(16, 6))\n>>> ## Plotting the losses\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> plt.plot(all_g_losses, label='Generator loss')\n>>> half_d_losses = [all_d_loss/2 for all_d_loss in all_d_losses]\n>>> plt.plot(half_d_losses, label='Discriminator loss')\n>>> plt.legend(fontsize=20)\n>>> ax.set_xlabel('Iteration', size=15)\n>>> ax.set_ylabel('Loss', size=15)\n>>> \n>>> ## Plotting the outputs of the discriminator\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> plt.plot(all_d_real, label=r'Real: $D(\\mathbf{x})$')\n>>> plt.plot(all_d_fake, label=r'Fake: $D(G(\\mathbf{z}))$')\n>>> plt.legend(fontsize=20)\n>>> ax.set_xlabel('Iteration', size=15)\n>>> ax.set_ylabel('Discriminator output', size=15)\n>>> plt.show() \n```", "```py\n>>> selected_epochs = [1, 2, 4, 10, 50, 100]\n>>> fig = plt.figure(figsize=(10, 14))\n>>> for i,e in enumerate(selected_epochs):\n...     for j in range(5):\n...         ax = fig.add_subplot(6, 5, i*5+j+1)\n...         ax.set_xticks([])\n...         ax.set_yticks([])\n...         if j == 0:\n...             ax.text(\n...                 -0.06, 0.5, f'Epoch {e}',\n...                 rotation=90, size=18, color='red',\n...                 horizontalalignment='right',\n...                 verticalalignment='center',\n...                 transform=ax.transAxes\n...             )\n...         \n...         image = epoch_samples[e-1][j]\n...         ax.imshow(image, cmap='gray_r')\n...     \n>>> plt.show() \n```", "```py\n>>> def make_generator_network(input_size, n_filters):\n...     model = nn.Sequential(\n...         nn.ConvTranspose2d(input_size, n_filters*4, 4,\n...                            1, 0, bias=False),\n...         nn.BatchNorm2d(n_filters*4),\n...         nn.LeakyReLU(0.2),\n...         nn.ConvTranspose2d(n_filters*4, n_filters*2,\n...                            3, 2, 1, bias=False),\n...         nn.BatchNorm2d(n_filters*2),\n...         nn.LeakyReLU(0.2),\n...         nn.ConvTranspose2d(n_filters*2, n_filters,\n...                            4, 2, 1, bias=False),\n...         nn.BatchNorm2d(n_filters),\n...         nn.LeakyReLU(0.2),\n...         nn.ConvTranspose2d(n_filters, 1, 4, 2, 1,\n...                            bias=False),\n...         nn.Tanh()\n...     )\n...     return model\n>>> \n>>> class Discriminator(nn.Module):\n...     def __init__(self, n_filters):\n...         super().__init__()\n...         self.network = nn.Sequential(\n...             nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n...             nn.LeakyReLU(0.2),\n...             nn.Conv2d(n_filters, n_filters*2,\n...                       4, 2, 1, bias=False),\n...             nn.BatchNorm2d(n_filters * 2),\n...             nn.LeakyReLU(0.2),\n...             nn.Conv2d(n_filters*2, n_filters*4,\n...                       3, 2, 1, bias=False),\n...             nn.BatchNorm2d(n_filters*4),\n...             nn.LeakyReLU(0.2),\n...             nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n...             nn.Sigmoid()\n...         )\n... \n...     def forward(self, input):\n...         output = self.network(input)\n...         return output.view(-1, 1).squeeze(0) \n```", "```py\n>>> z_size = 100\n>>> image_size = (28, 28)\n>>> n_filters = 32\n>>> gen_model = make_generator_network(z_size, n_filters).to(device)\n>>> print(gen_model)\nSequential(\n  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): LeakyReLU(negative_slope=0.2)\n  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): LeakyReLU(negative_slope=0.2)\n  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): LeakyReLU(negative_slope=0.2)\n  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (10): Tanh()\n) \n```", "```py\n>>> disc_model = Discriminator(n_filters).to(device)\n>>> print(disc_model)\nDiscriminator(\n  (network): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2)\n    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (9): Sigmoid()\n  )\n) \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0003)\n>>> d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002) \n```", "```py\n>>> def create_noise(batch_size, z_size, mode_z):\n...     if mode_z == 'uniform':\n...         input_z = torch.rand(batch_size, z_size, 1, 1)*2 - 1\n...     elif mode_z == 'normal':\n...         input_z = torch.randn(batch_size, z_size, 1, 1)\n...     return input_z \n```", "```py\n>>> def d_train(x):\n...     disc_model.zero_grad()\n...     # Train discriminator with a real batch\n...     batch_size = x.size(0)\n...     x = x.to(device)\n...     d_labels_real = torch.ones(batch_size, 1, device=device)\n...     d_proba_real = disc_model(x)\n...     d_loss_real = loss_fn(d_proba_real, d_labels_real)\n...     # Train discriminator on a fake batch\n...     input_z = create_noise(batch_size, z_size, mode_z).to(device)\n...     g_output = gen_model(input_z)\n...     d_proba_fake = disc_model(g_output)\n...     d_labels_fake = torch.zeros(batch_size, 1, device=device)\n...     d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n...     # gradient backprop & optimize ONLY D's parameters\n...     d_loss = d_loss_real + d_loss_fake\n...     d_loss.backward()\n...     d_optimizer.step()\n...     return d_loss.data.item(), d_proba_real.detach(), \\\n...            d_proba_fake.detach() \n```", "```py\n>>> fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n>>> epoch_samples = []\n>>> torch.manual_seed(1)\n>>> for epoch in range(1, num_epochs+1):\n...     gen_model.train()\n...     for i, (x, _) in enumerate(mnist_dl):\n...         d_loss, d_proba_real, d_proba_fake = d_train(x)\n...         d_losses.append(d_loss)\n...         g_losses.append(g_train(x))\n...     print(f'Epoch {epoch:03d} | Avg Losses >>'\n...           f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n...           f'/{torch.FloatTensor(d_losses).mean():.4f}')\n...     gen_model.eval()\n...     epoch_samples.append(\n...         create_samples(\n...             gen_model, fixed_z\n...         ).detach().cpu().numpy()\n...     )\nEpoch 001 | Avg Losses >> G/D 4.7016/0.1035\nEpoch 002 | Avg Losses >> G/D 5.9341/0.0438\n...\nEpoch 099 | Avg Losses >> G/D 4.3753/0.1360\nEpoch 100 | Avg Losses >> G/D 4.4914/0.1120 \n```", "```py\n>>> selected_epochs = [1, 2, 4, 10, 50, 100]\n>>> fig = plt.figure(figsize=(10, 14))\n>>> for i,e in enumerate(selected_epochs):\n...     for j in range(5):\n...         ax = fig.add_subplot(6, 5, i*5+j+1)\n...         ax.set_xticks([])\n...         ax.set_yticks([])\n...         if j == 0:\n...             ax.text(-0.06, 0.5,  f'Epoch {e}',\n...                     rotation=90, size=18, color='red',\n...                     horizontalalignment='right',\n...                     verticalalignment='center',\n...                     transform=ax.transAxes)\n...         \n...         image = epoch_samples[e-1][j]\n...         ax.imshow(image, cmap='gray_r')\n>>> plt.show() \n```", "```py\n>>> def make_generator_network_wgan(input_size, n_filters):\n...     model = nn.Sequential(\n...         nn.ConvTranspose2d(input_size, n_filters*4, 4,\n...                            1, 0, bias=False),\n...         nn.InstanceNorm2d(n_filters*4),\n...         nn.LeakyReLU(0.2),\n... \n...         nn.ConvTranspose2d(n_filters*4, n_filters*2,\n...                            3, 2, 1, bias=False),\n...         nn.InstanceNorm2d(n_filters*2),\n...         nn.LeakyReLU(0.2),\n... \n...         nn.ConvTranspose2d(n_filters*2, n_filters, 4,\n...                            2, 1, bias=False),\n...         nn.InstanceNorm2d(n_filters),\n...         nn.LeakyReLU(0.2),\n... \n...         nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n...         nn.Tanh()\n...     )\n...     return model\n>>> \n>>> class DiscriminatorWGAN(nn.Module):\n...     def __init__(self, n_filters):\n...         super().__init__()\n...         self.network = nn.Sequential(\n...             nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n...             nn.LeakyReLU(0.2),\n... \n...             nn.Conv2d(n_filters, n_filters*2, 4, 2, 1,\n...                       bias=False),\n...             nn.InstanceNorm2d(n_filters * 2),\n...             nn.LeakyReLU(0.2),\n... \n...             nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1,\n...                       bias=False),\n...             nn.InstanceNorm2d(n_filters*4),\n...             nn.LeakyReLU(0.2),\n... \n...             nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n...             nn.Sigmoid()\n...     )\n... \n...     def forward(self, input):\n...         output = self.network(input)\n...         return output.view(-1, 1).squeeze(0) \n```", "```py\n>>> gen_model = make_generator_network_wgan(\n...     z_size, n_filters\n... ).to(device)\n>>> disc_model = DiscriminatorWGAN(n_filters).to(device)\n>>> g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0002)\n>>> d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002) \n```", "```py\n>>> from torch.autograd import grad as torch_grad\n>>> def gradient_penalty(real_data, generated_data):\n...     batch_size = real_data.size(0)\n... \n...     # Calculate interpolation\n...     alpha = torch.rand(real_data.shape[0], 1, 1, 1,\n...                        requires_grad=True, device=device)\n...     interpolated = alpha * real_data + \\\n...                    (1 - alpha) * generated_data\n... \n...     # Calculate probability of interpolated examples\n...     proba_interpolated = disc_model(interpolated)\n... \n...     # Calculate gradients of probabilities\n...     gradients = torch_grad(\n...         outputs=proba_interpolated, inputs=interpolated,\n...         grad_outputs=torch.ones(proba_interpolated.size(),\n...                                 device=device),\n...         create_graph=True, retain_graph=True\n...     )[0]\n... \n...     gradients = gradients.view(batch_size, -1)\n...     gradients_norm = gradients.norm(2, dim=1)\n...     return lambda_gp * ((gradients_norm - 1)**2).mean() \n```", "```py\n>>> def d_train_wgan(x):\n...     disc_model.zero_grad()\n... \n...     batch_size = x.size(0)\n...     x = x.to(device)\n... \n...     # Calculate probabilities on real and generated data\n...     d_real = disc_model(x)\n...     input_z = create_noise(batch_size, z_size, mode_z).to(device)\n...     g_output = gen_model(input_z)\n...     d_generated = disc_model(g_output)\n...     d_loss = d_generated.mean() - d_real.mean() + \\\n...              gradient_penalty(x.data, g_output.data)\n...     d_loss.backward()\n...     d_optimizer.step()\n...     return d_loss.data.item()\n>>> \n>>> def g_train_wgan(x):\n...     gen_model.zero_grad()\n...     \n...     batch_size = x.size(0)\n...     input_z = create_noise(batch_size, z_size, mode_z).to(device)\n...     g_output = gen_model(input_z)\n...     \n...     d_generated = disc_model(g_output)\n...     g_loss = -d_generated.mean()\n... \n...     # gradient backprop & optimize ONLY G's parameters\n...     g_loss.backward()\n...     g_optimizer.step()\n...     return g_loss.data.item() \n```", "```py\n>>> epoch_samples_wgan = []\n>>> lambda_gp = 10.0\n>>> num_epochs = 100\n>>> torch.manual_seed(1)\n>>> critic_iterations = 5\n>>> for epoch in range(1, num_epochs+1):\n...     gen_model.train()\n...     d_losses, g_losses = [], []\n...     for i, (x, _) in enumerate(mnist_dl):\n...         for _ in range(critic_iterations):\n...             d_loss = d_train_wgan(x)\n...         d_losses.append(d_loss)\n...         g_losses.append(g_train_wgan(x))\n...     \n...     print(f'Epoch {epoch:03d} | D Loss >>'\n...           f' {torch.FloatTensor(d_losses).mean():.4f}')\n...     gen_model.eval()\n...     epoch_samples_wgan.append(\n...         create_samples(\n...             gen_model, fixed_z\n...         ).detach().cpu().numpy()\n...     ) \n```"]