["```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\nmnist = input_data.read_data_sets('data/mnist', one_hot=True)\n```", "```py\ndef initialize_weights(shape):\n    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n```", "```py\ndef initialize_bias(shape):\n    return tf.Variable(tf.constant(0.1, shape=shape))\n```", "```py\ndef convolution(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n```", "```py\ndef max_pooling(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n```", "```py\nX_ = tf.placeholder(tf.float32, [None, 784])\n```", "```py\nX = tf.reshape(X_, [-1, 28, 28, 1])\n```", "```py\ny = tf.placeholder(tf.float32, [None, 10])\n```", "```py\nW1 = initialize_weights([5,5,1,32])\n```", "```py\nb1 = initialize_bias([32])\n```", "```py\nconv1 = tf.nn.relu(convolution(X, W1) + b1)\npool1 = max_pooling(conv1)\n```", "```py\nW2 = initialize_weights([5,5,32,64])\n```", "```py\nb2 = initialize_bias([64])\n```", "```py\nconv2 = tf.nn.relu(convolution(pool1, W2) + b2)\npool2 = max_pooling(conv2)\n```", "```py\nflattened = tf.reshape(pool2, [-1, 7*7*64])\n```", "```py\nW_fc = initialize_weights([7*7*64, 1024])\nb_fc = initialize_bias([1024])\n```", "```py\nfc_output = tf.nn.relu(tf.matmul(flattened, W_fc) + b_fc)\n```", "```py\nW_out = initialize_weights([1024, 10])\nb_out = initialize_bias([10])\n```", "```py\nYHat = tf.nn.softmax(tf.matmul(fc_output, W_out) + b_out)\n```", "```py\ncross_entropy = -tf.reduce_sum(y*tf.log(YHat))\n```", "```py\noptimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n```", "```py\npredicted_digit = tf.argmax(y_hat, 1)\nactual_digit = tf.argmax(y, 1)\n\ncorrect_pred = tf.equal(predicted_digit,actual_digit)\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n```", "```py\nfor epoch in range(1000):\n\n    #select some batch of data points according to the batch size (100)\n    X_batch, y_batch = mnist.train.next_batch(batch_size=100)\n\n    #train the network\n    loss, acc, _ = sess.run([cross_entropy, accuracy, optimizer], feed_dict={X_: X_batch, y: y_batch})\n\n    #print the loss on every 100th epoch\n    if epoch%100 == 0:\n        print('Epoch: {}, Loss:{} Accuracy: {}'.format(epoch,loss,acc))\n```", "```py\nEpoch: 0, Loss:631.2734375 Accuracy: 0.129999995232\nEpoch: 100, Loss:28.9199733734 Accuracy: 0.930000007153\nEpoch: 200, Loss:18.2174377441 Accuracy: 0.920000016689\nEpoch: 300, Loss:21.740688324 Accuracy: 0.930000007153\n```", "```py\nplt.imshow(mnist.train.images[7].reshape([28, 28]))\n```", "```py\nimage = mnist.train.images[7].reshape([-1, 784])\nfeature_map = sess.run([conv1], feed_dict={X_: image})[0]\n```", "```py\nfor i in range(32):\n    feature = feature_map[:,:,:,i].reshape([28, 28])\n    plt.subplot(4,8, i + 1)\n    plt.imshow(feature)\n    plt.axis('off')\nplt.show()\n\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n```", "```py\nmnist = input_data.read_data_sets(\"data/mnist\",one_hot=True)\n```", "```py\ndef squash(sj):\n\n    sj_norm = tf.reduce_sum(tf.square(sj), -2, keep_dims=True)\n    scalar_factor = sj_norm / (1 + sj_norm) / tf.sqrt(sj_norm + epsilon)\n\n    vj = scalar_factor * sj \n\n    return vj\n```", "```py\ndef dynamic_routing(ui, bij, num_routing=10):\n```", "```py\n    wij = tf.get_variable('Weight', shape=(1, 1152, 160, 8, 1), dtype=tf.float32,\n\n                        initializer=tf.random_normal_initializer(0.01))\n\n    biases = tf.get_variable('bias', shape=(1, 1, 10, 16, 1))\n```", "```py\n    ui = tf.tile(ui, [1, 1, 160, 1, 1])\n```", "```py\n    u_hat = tf.reduce_sum(wij * ui, axis=3, keep_dims=True)\n```", "```py\n    u_hat = tf.reshape(u_hat, shape=[-1, 1152, 10, 16, 1])\n```", "```py\n    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n```", "```py\n    for r in range(num_routing):\n\n        with tf.variable_scope('iter_' + str(r)):\n\n            #step 1\n            cij = tf.nn.softmax(bij, dim=2)\n\n            #step 2\n            if r == num_routing - 1:\n\n                sj = tf.multiply(cij, u_hat)\n\n                sj = tf.reduce_sum(sj, axis=1, keep_dims=True) + biases\n\n                vj = squash(sj)\n\n            elif r < num_routing - 1: \n\n                sj = tf.multiply(cij, u_hat_stopped)\n\n                sj = tf.reduce_sum(sj, axis=1, keep_dims=True) + biases\n\n                vj = squash(sj)\n\n                vj_tiled = tf.tile(vj, [1, 1152, 1, 1, 1])\n\n                coupling_coeff = tf.reduce_sum(u_hat_stopped * vj_tiled, axis=3, keep_dims=True)\n\n                #step 3\n                bij += coupling_coeff\n   return vj\n```", "```py\ngraph = tf.Graph()\nwith graph.as_default() as g:\n```", "```py\n    x = tf.placeholder(tf.float32, [batch_size, 784])\n    y = tf.placeholder(tf.float32, [batch_size,10])\n    x_image = tf.reshape(x, [-1,28,28,1])\n```", "```py\n    with tf.name_scope('convolutional_input'):\n        input_data = tf.contrib.layers.conv2d(inputs=x_image, num_outputs=256, kernel_size=9, padding='valid')\n```", "```py\n capsules = []\n\n for i in range(8):\n\n with tf.name_scope('capsules_' + str(i)):\n\n #convolution operation \n output = tf.contrib.layers.conv2d(inputs=input_data, num_outputs=32,kernel_size=9, stride=2, padding='valid')\n\n #reshape the output\n output = tf.reshape(output, [batch_size, -1, 1, 1])\n\n #store the output which is capsule in the capsules list\n capsules.append(output)\n```", "```py\n primary_capsule = tf.concat(capsules, axis=2)\n```", "```py\n primary_capsule = squash(primary_capsule)\n```", "```py\n    with tf.name_scope('dynamic_routing'):\n\n        #reshape the primary capsule\n        outputs = tf.reshape(primary_capsule, shape=(batch_size, -1, 1, primary_capsule.shape[-2].value, 1))\n\n        #initialize bij with 0s\n        bij = tf.constant(np.zeros([1, primary_capsule.shape[1].value, 10, 1, 1], dtype=np.float32))\n\n        #compute the digit capsules using dynamic routing algorithm which takes \n        #the reshaped primary capsules and bij as inputs and returns the activity vector \n        digit_capsules = dynamic_routing(outputs, bij)\n\n digit_capsules = tf.squeeze(digit_capsules, axis=1)\n```", "```py\nwith graph.as_default() as g:\n    with tf.variable_scope('Masking'):\n\n        #select the activity vector of given input image using the actual label y and mask out others\n        masked_v = tf.multiply(tf.squeeze(digit_capsules), tf.reshape(y, (-1, 10, 1)))\n```", "```py\nwith tf.name_scope('Decoder'):\n\n    #masked digit capsule\n    v_j = tf.reshape(masked_v, shape=(batch_size, -1))\n\n    #first fully connected layer \n    fc1 = tf.contrib.layers.fully_connected(v_j, num_outputs=512)\n\n    #second fully connected layer\n    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n\n    #reconstructed image\n    reconstructed_image = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n```", "```py\nwith graph.as_default() as g:\n    with tf.variable_scope('accuracy'):\n```", "```py\n        v_length = tf.sqrt(tf.reduce_sum(tf.square(digit_capsules), axis=2, keep_dims=True) + epsilon)\n```", "```py\n        softmax_v = tf.nn.softmax(v_length, dim=1)\n```", "```py\n        argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1)) \n        predicted_digit = tf.reshape(argmax_idx, shape=(batch_size, ))\n```", "```py\n        actual_digit = tf.to_int32(tf.argmax(y, axis=1))\n\n        correct_pred = tf.equal(predicted_digit,actual_digit)\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\nmax_left = tf.square(tf.maximum(0.,0.9 - v_length))\nmax_right = tf.square(tf.maximum(0., v_length - 0.1))\n```", "```py\nT_k = y\n\nlambda_ = 0.5\nL_k = T_k * max_left + lambda_ * (1 - T_k) * max_right\n```", "```py\nmargin_loss = tf.reduce_mean(tf.reduce_sum(L_k, axis=1))\n```", "```py\noriginal_image = tf.reshape(x, shape=(batch_size, -1))\n```", "```py\nsquared = tf.square(reconstructed_image - original_image)\n\n```", "```py\nreconstruction_loss = tf.reduce_mean(squared) \n```", "```py\nalpha = 0.0005\ntotal_loss = margin_loss + alpha * reconstruction_loss\n```", "```py\noptimizer = tf.train.AdamOptimizer(0.0001)\ntrain_op = optimizer.minimize(total_loss)\n```", "```py\nnum_epochs = 100\nnum_steps = int(len(mnist.train.images)/batch_size)\n```", "```py\nwith tf.Session(graph=graph) as sess:\n\n    init_op = tf.global_variables_initializer()\n    sess.run(init_op)\n\n    for epoch in range(num_epochs):\n        for iteration in range(num_steps):\n            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n            feed_dict = {x : batch_data, y : batch_labels}\n\n            _, loss, acc = sess.run([train_op, total_loss, accuracy], feed_dict=feed_dict)\n\n            if iteration%10 == 0:\n                print('Epoch: {}, iteration:{}, Loss:{} Accuracy: {}'.format(epoch,iteration,loss,acc))\n```", "```py\nEpoch: 0, iteration:0, Loss:0.55281829834 Accuracy: 0.0399999991059\nEpoch: 0, iteration:10, Loss:0.541650533676 Accuracy: 0.20000000298\nEpoch: 0, iteration:20, Loss:0.233602654934 Accuracy: 0.40000007153\n```"]