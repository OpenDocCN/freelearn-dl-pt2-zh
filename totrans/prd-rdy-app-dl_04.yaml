- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Developing a Powerful Deep Learning Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个强大的深度学习模型
- en: 'In this chapter, we will describe how to design and train a **deep learning**
    (**DL**) model. Within the notebook context described in the previous chapter,
    data scientists investigate various network designs and model training settings
    to generate a working model for the given task. The main topics of this chapter
    include the theory behind DL and how to train a model using the most popular DL
    frameworks: **PyTorch** and **TensorFlow** (**TF**). At the end of the chapter,
    we will decompose the **StyleGAN** implementation, a popular DL model for image
    generation, to explain how to construct a complex model using the components that
    we have introduced in this chapter.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述如何设计和训练**深度学习**（DL）模型。在上一章节中描述的笔记本上下文中，数据科学家们调查各种网络设计和模型训练设置，以生成适合给定任务的工作模型。本章的主要内容包括DL的理论以及如何使用最流行的DL框架**PyTorch**和**TensorFlow**（**TF**）训练模型。在本章末尾，我们将解构**StyleGAN**的实现，这是一个用于图像生成的流行DL模型，以解释如何使用我们在本章介绍的组件构建复杂模型。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Going through the basic theory of DL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习深度学习基础理论
- en: Understanding the components of DL frameworks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DL框架的组成部分
- en: Implementing and training a model in PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中实现和训练模型
- en: Implementing and training a model in TF
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TF中实现和训练模型
- en: Decomposing a complex, state-of-the-art model implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解构复杂的最新模型实现
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the supplemental material of this chapter from the following
    GitHub link: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_3](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_3).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下GitHub链接下载本章的补充材料：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_3](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_3)。
- en: 'The samples in this chapter can be executed from any Python environment with
    the necessary packages installed. You can use the sample environment introduced
    in the last chapter: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例可以在安装了必要包的任何Python环境中执行。您可以使用上一章介绍的示例环境：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles)。
- en: Going through the basic theory of DL
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习深度学习基础理论
- en: As briefly described in [*Chapter 1*](B18522_01.xhtml#_idTextAnchor014), *Effective
    Planning of Deep-Learning-Driven Projects*, DL is a **machine learning** (**ML**)
    technique based on **artificial neural networks** (**ANNs**). In this section,
    our goal is to explain how ANNs work without going too deep into the math.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 1 章*](B18522_01.xhtml#_idTextAnchor014)中简要描述的，*深度学习驱动项目的有效规划*，DL是基于**人工神经网络**（ANNs）的**机器学习**（ML）技术。本节的目标是在不深入数学的情况下解释ANNs的工作原理。
- en: How does DL work?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DL是如何工作的？
- en: An ANN is basically a set of connected neurons. As shown in *Figure 3.1*, neurons
    from an ANN and neurons from our brain behave in a similar way. Each connection
    in an ANN consists of a tunable parameter called the **weight**. When there is
    a connection from neuron A to neuron B, the output of neuron A gets multiplied
    by the weight of the connection; the weighted value becomes the input of neuron
    B. **Bias** is another tunable parameter within a neuron; a neuron sums up all
    the inputs and adds the bias. The last operation is an activation function that
    maps the computed value into a different range. The value in the new range is
    the output of the neuron, which gets passed to other neurons based on the connections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（ANN）基本上是一组相连的神经元。如*图 3.1*所示，ANN中的神经元和我们大脑中的神经元表现出类似的行为。在ANN中，每个连接都有一个可调参数称为**权重**。当神经元A到神经元B有连接时，神经元A的输出乘以连接的权重，得到的加权值成为神经元B的输入。**偏置**是神经元内的另一个可调参数；神经元将所有输入求和并加上偏置。最后的操作是激活函数，将计算得到的值映射到不同的范围。新范围内的值是神经元的输出，根据连接传递给其他神经元。'
- en: Throughout the research, it has been found that groups of neurons captures different
    patterns based on their organization. Some of the powerful organizations are standardized
    as **layers** and have become the main building block for an ANN, providing a
    layer of abstraction on top of the complicated interactions among the neurons.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究中发现，神经元组根据其组织方式捕捉不同的模式。一些强大的组织形式被标准化为**层**，已成为ANN的主要构建模块，为复杂的神经元间互动提供了一层抽象。
- en: '![Figure 3.1 – A comparison of a biological neuron and a mathematical model
    of an ANN neuron'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 生物神经元与ANN神经元数学模型的比较'
- en: '](img/B18522_03_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_01.jpg)'
- en: Figure 3.1 – A comparison of a biological neuron and a mathematical model of
    an ANN neuron
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 生物神经元与ANN神经元数学模型的比较
- en: As described in the preceding diagram, operations in DL are based on numerical
    values. Therefore, the input data for a network must be converted into a numerical
    value. For example, a **Red, Green, and Blue** (**RGB**) color code is a standard
    way of representing an image using numerical values. In the case of text data,
    word embeddings are often used. Similarly, the output of a network will be a set
    of numerical values. The interpretation of these values can vary based on the
    task and the definition.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的图表所述，DL中的操作基于数值。因此，网络的输入数据必须转换为数值。例如，**红、绿、蓝**（**RGB**）颜色代码是用数值表示图像的标准方式。对于文本数据，通常使用词嵌入。类似地，网络的输出将是一组数值。这些值的解释可以根据任务和定义而异。
- en: DL model training
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DL模型训练
- en: 'Overall, training an ANN is a process of finding a set of weights, biases,
    and activation functions that enable the network to extract meaningful patterns
    from the data. Now, the next question would be the following: *how do we find
    the right set of parameters?* Many researchers have tried to solve this problem
    using various techniques. Out of all the trials, the most effective algorithm
    discovered is an optimization algorithm called **gradient descent**, an iterative
    process that finds the local or global minimum.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，训练ANN是一个寻找一组权重、偏差和激活函数的过程，使得网络能够从数据中提取有意义的模式。接下来的问题是：*我们如何找到合适的参数集？* 许多研究人员尝试使用各种技术解决这个问题。在所有尝试中，发现的最有效算法是一种称为**梯度下降**的优化算法，这是一个迭代过程，用于找到局部或全局最小值。
- en: When training a DL model, we need to define a function that quantizes the difference
    between predictions and ground-truth labels as a numeric value called a **loss**.
    With a loss function clearly defined, we iteratively generate intermediate predictions,
    compute loss values, and update model parameters in the direction toward the minimum
    loss.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练DL模型时，我们需要定义一个函数，将预测值与地面实况标签之间的差异量化为一个称为**损失**的数值。一旦损失函数被明确定义，我们就会迭代生成中间预测，计算损失值，并朝向最小损失方向更新模型参数。
- en: Given that the goal of optimization is to find the minimum loss, model parameters
    need to be updated based on the **train set** samples in the opposite direction
    of the gradient (see *Figure 3.2*). To compute the gradients, the network keeps
    track of the intermediate values computed during the prediction pass (**forward
    propagation**). Then, starting from the last layer, it computes the gradients
    for each parameter exploiting the chain rule (**backward propagation**). Interestingly,
    model performance and training time can differ a lot based on how the parameters
    get updated in each iteration. The different parameter updating rules are captured
    within the concept of optimizers. One of the main tasks in DL is to select the
    type of optimizer that produces the model with the best performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于优化的目标是找到最小损失，模型参数需要根据**训练集**样本在梯度的反方向上进行更新（见*图 3.2*）。为了计算梯度，网络在预测过程中跟踪计算中间值（**前向传播**）。然后，从最后一层开始，利用链式法则计算每个参数的梯度（**反向传播**）。有趣的是，模型的性能和训练时间可能会根据每次迭代中参数更新的方式有很大的差异。不同的参数更新规则包含在优化器的概念中。DL的一个主要任务之一是选择能够产生最佳性能模型的优化器类型。
- en: '![Figure 3.2 – With gradient descent, model parameters will be updated in the
    opposite direction of the gradient at every iteration'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – 利用梯度下降，模型参数将在每次迭代中朝着梯度的相反方向更新'
- en: '](img/B18522_03_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_02.jpg)'
- en: Figure 3.2 – With gradient descent, model parameters will be updated in the
    opposite direction of the gradient at every iteration
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 使用梯度下降，模型参数将在每次迭代中朝着梯度的反方向更新
- en: 'However, there is one caveat to this process. If the model is trained to achieve
    the best performance for the train set specifically, the performance on unseen
    data can possibly deteriorate. This is called **overfitting**; the model is trained
    specifically for the data it has seen before and fails to make correct predictions
    on new data. On the other hand, a shortage of training can lead to **underfitting**,
    a situation in which the model fails to capture the underlying pattern of the
    train set. To prevent these issues, a portion of the train set is put aside for
    evaluating the trained model throughout the training: the **validation set**.
    Overall, training for DL involves a process of updating the model parameters based
    on the train set but selecting the model that performs the best on the validation
    set. The last type of dataset, the **test set**, represents what the model would
    interact with once it is deployed. The test set may or may not be available at
    the time of model training. The purpose of the test set is to understand how the
    trained model would perform in production. To further understand the overall training
    logic, we can look at *Figure 3.3*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个过程有一个需要注意的地方。如果模型被训练来在训练集上获得最佳性能，那么在未见数据上的性能可能会下降。这被称为**过拟合**；模型专门针对它以前见过的数据进行训练，无法正确预测新数据。另一方面，训练不足会导致**欠拟合**，即模型未能捕获训练集的潜在模式。为了避免这些问题，训练集的一部分被保留用于在训练过程中评估训练好的模型：**验证集**。总体而言，DL的训练涉及根据训练集更新模型参数的过程，但选择在验证集上表现最佳的模型。最后一种数据集，**测试集**，代表了模型部署后可能与之交互的数据。测试集在模型训练时可能可用，也可能不可用。测试集的目的是了解训练好的模型在生产环境中的表现。为了进一步理解整体的训练逻辑，我们可以看一下*图3.3*：
- en: '![Figure 3.3 – The steps for training a DL model'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.3 – 训练DL模型的步骤'
- en: '](img/B18522_03_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_03.jpg)'
- en: Figure 3.3 – The steps for training a DL model
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 训练DL模型的步骤
- en: The figure clearly describes what steps there are within the iterative process
    and what role each type of dataset plays in the scene.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图清晰地描述了迭代过程中的步骤，以及每种类型数据集在场景中的角色。
- en: Things to remember
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. Training an ANN is a process of finding a set of weights, biases, and activation
    functions that enable the network to extract meaningful patterns from the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: a. 训练人工神经网络（ANN）是一个寻找一组权重、偏置和激活函数的过程，使网络能够从数据中提取有意义的模式。
- en: b. There are three types of datasets in the training flow. The model parameters
    are updated using the train set, and the one that produces the best performance
    on the validation set is selected. The test set reflects the data distribution
    that the trained model would interact with upon deployment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: b. 在训练流程中有三种类型的数据集。使用训练集更新模型参数，并选择在验证集上表现最佳的模型。测试集反映了训练好的模型在部署时将与之交互的数据分布。
- en: Next, we will look at DL frameworks that are designed to help us with model
    training.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看旨在帮助我们进行模型训练的DL框架。
- en: Components of DL frameworks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL 框架的组成部分
- en: Since the configuration of model training follows the same process regardless
    of the underlying tasks, many engineers and researchers have put together the
    common building blocks into frameworks. Most of the frameworks simplify DL model
    development by keeping data loading logic and model definitions independent from
    the training logic.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型训练的配置遵循相同的流程，无论底层任务如何，许多工程师和研究人员已经将常见的构建模块整合到框架中。大多数框架通过将数据加载逻辑和模型定义与训练逻辑分离，简化了DL模型的开发。
- en: The data loading logic
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载逻辑
- en: '**Data loading** logic includes everything from loading the raw data in memory
    to preparing each sample for training and evaluation. In many cases, data for
    the train set, validation set, and test set are stored in separate locations,
    so that each of them requires a distinct loading and preparation logic. The standard
    frameworks keep these logics separate from the other building blocks so that the
    model can be trained using different datasets in a dynamic way with minimal changes
    on the model side. Furthermore, the frameworks have standardized the way that
    these logics are defined to improve reusability and readability.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据加载**逻辑包括从内存加载原始数据到为每个样本准备训练和评估所需的所有步骤。在许多情况下，训练集、验证集和测试集的数据存储在不同的位置，因此每个集合都需要不同的加载和准备逻辑。标准框架将这些逻辑与其他构建模块分开，以便可以以动态方式使用不同的数据集进行模型训练，并在模型方面进行最小更改。此外，这些框架已经标准化了这些逻辑的定义方式，以提高可重用性和可读性。'
- en: The model definition
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定义
- en: Another building block, **model definition**, refers to the ANN architecture
    itself and corresponding forward and backward propagation logics. Even though
    building up a model using arithmetic operations is an option, the standard frameworks
    provide common layer definitions that users can put together to build up a complex
    model. Therefore, users are responsible for instantiating the necessary network
    components, connecting the components, and defining how the model should behave
    for training and inference.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基础模块，**模型定义**，指的是人工神经网络的架构本身及其对应的前向和反向传播逻辑。尽管使用算术运算建立模型是一种选择，但标准框架提供了常见的层定义，用户可以组合这些定义以构建复杂的模型。因此，用户需负责实例化必要的网络组件，连接这些组件，并定义模型在训练和推断时的行为方式。
- en: 'In the following two sections, *Implementing and training a model in PyTorch*
    and *Implementing and training a model in TF*, we will introduce how to instantiate
    the popular layers in PyTorch and TF, respectively: dense (linear), pooling, normalization,
    dropout, convolution, and recurrent layers.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，*在PyTorch中实现和训练模型* 和 *在TF中实现和训练模型*，我们将介绍如何在PyTorch和TF中实例化流行的层：稠密（线性）、池化、归一化、dropout、卷积和循环层。
- en: Model training logic
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练逻辑
- en: Lastly, we need to combine the two components and define the details of the
    training logic. This wrapper component must clearly describe the essential pieces
    of the model training, such as loss function, learning rate, optimizer, epochs,
    iterations, and batch size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要结合这两个组件并定义训练逻辑的详细信息。这个包装组件必须清晰描述模型训练的关键部分，例如损失函数、学习率、优化器、epochs、迭代次数和批处理大小。
- en: 'Loss functions can be classified into two major categories based on the type
    of learning task: **classification loss** and **regression loss**. The major difference
    between the two categories comes from the output format; the output of the classification
    task is categorical, while the output of the regression task is a continuous value.
    Out of the different losses, we will mainly discuss **Mean Square Error** (**MSE**)
    **loss** and **Mean Absolute Error** (**MAE**) **loss** for regression loss, and
    **Cross-Entropy** (**CE**) **loss** and **Binary Cross-Entropy** (**BCE**) **loss**
    for classification loss.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以根据学习任务的类型分为两大类：**分类损失**和**回归损失**。这两类之间的主要区别来自输出格式；分类任务的输出是分类的，而回归任务的输出是连续值。在不同的损失中，我们将主要讨论用于回归损失的**均方误差**（**MSE**）和**平均绝对误差**（**MAE**）损失，以及用于分类损失的**交叉熵**（**CE**）和**二元交叉熵**（**BCE**）损失。
- en: 'The **learning rate** (**LR**) defines the size of a step that gradient descent
    takes in the direction of the local minimum. Selecting the LR rate will help the
    process to converge faster, but if it’s too high or low, the convergence will
    not be guaranteed (see *Figure 3.4*):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**（**LR**）定义了梯度下降在局部最小值方向上迈出的步伐大小。选择适当的学习率可以帮助过程更快地收敛，但如果学习率过高或过低，收敛性将无法保证（见*图3.4*）：'
- en: '![Figure 3.4 – The impact of the LR within gradient descent'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 学习率在梯度下降中的影响'
- en: '](img/B18522_03_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_04.jpg)'
- en: Figure 3.4 – The impact of the LR within gradient descent
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 学习率在梯度下降中的影响
- en: 'Speaking of **optimizers**, we focus on the two main optimizers: **Stochastic
    Gradient Descent** (**SGD**), a basic optimizer with a fixed LR, and **Adaptive
    Moment Estimation** (**Adam**), an optimizer based on an adaptive LR that works
    the best in most scenarios. If you are interested in learning about different
    optimizers and the mathematics behind them, we recommend reading a survey paper
    by Choi et al ([https://arxiv.org/pdf/1910.05446.pdf](https://arxiv.org/pdf/1910.05446.pdf)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到**优化器**，我们关注两个主要的优化器：**随机梯度下降**（**SGD**），一个具有固定学习率的基本优化器，以及**自适应矩估计**（**Adam**），一种基于自适应学习率的优化器，在大多数情况下表现最佳。如果你有兴趣了解不同优化器及其背后的数学原理，我们推荐阅读Choi等人的调查论文（[https://arxiv.org/pdf/1910.05446.pdf](https://arxiv.org/pdf/1910.05446.pdf)）。
- en: A single **epoch** indicates that every sample in the train set has been passed
    forward and backward through the network and that the network parameters have
    been updated. In many cases, the number of samples in the train set is way too
    huge to be passed through in one queue, so it gets divided into **mini-batches**.
    The **batch size** refers to the number of samples in a single mini-batch. Given
    that a set of mini-batches makes up the whole dataset, the number of iterations
    refers to the number of gradient update events (more precisely, the number of
    mini-batches) that model needs to interact with every sample. For example, if
    a mini-batch has 100 samples and there are 1,000 samples in total, it will require
    10 iterations to complete one epoch. Selecting the right number of epochs is not
    an easy task. It changes depending on the other training parameters such as LR
    and batch size. Therefore, it often requires a trial-and-error process, keeping
    underfitting and overfitting in mind.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**epoch**表示训练集中的每个样本都已经通过网络进行了前向和反向传播，并且网络参数已经更新。在许多情况下，训练集中的样本数量太大，无法一次通过，因此会将其分成**mini-batches**。**batch
    size**指的是单个mini-batch中的样本数量。给定一组mini-batches组成整个数据集，迭代次数指的是模型需要与每个样本进行梯度更新事件（更确切地说是mini-batch的数量）。例如，如果一个mini-batch有100个样本，总共有1,000个样本，那么完成一个epoch需要10次迭代。选择合适的epoch数量并不容易。这取决于其他训练参数，如学习率和batch
    size。因此，通常需要通过试验和错误的过程来确定，同时注意欠拟合和过拟合的问题。
- en: Things to remember
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. The components of model training can be broken down into data loading logic,
    model definition, and model training logic.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: a. 模型训练的组成部分可以分为数据加载逻辑、模型定义和模型训练逻辑。
- en: b. Data loading logic includes everything from loading raw data in the memory
    to preparing each sample for training and evaluation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: b. 数据加载逻辑包括从内存加载原始数据到为训练和评估准备每个样本的所有步骤。
- en: c. Model definition refers to the definition of the network architecture and
    its forward and backward propagation logics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: c. 模型定义指的是网络架构及其正向和反向传播逻辑的定义。
- en: d. Model training logic handles the actual training by putting data loading
    logic and model definition together.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: d. 模型训练逻辑通过将数据加载逻辑和模型定义结合起来处理实际的训练过程。
- en: 'Out of the various frameworks available, we will discuss the two most popular
    in this book: **TF** and **PyTorch**. **Keras** running on TF has gained popularity
    in today, while PyTorch is heavily used for research with its exceptional flexibility
    and simplicity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多可用的框架中，我们将在本书中讨论两个最流行的框架：**TF**和**PyTorch**。在今天，运行在TF上的Keras已经变得非常流行，而PyTorch以其出色的灵活性和简易性在研究中被广泛使用。
- en: Implementing and training a model in PyTorch
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中实现和训练模型
- en: PyTorch is a Python library for Torch, a ML package for Lua. The main features
    of PyTorch include **graphics processing unit**- (**GPU**-) accelerated matrix
    calculation and automatic differentiation for building and training neural networks.
    Creating the computation graph dynamically as the code gets executed, PyTorch
    is gaining popularity for its flexibility and ease of use, as well as its efficiency
    in model training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个用于Lua的ML包的Python库。 PyTorch的主要特性包括**图形处理单元**（**GPU**）加速的矩阵计算和用于构建和训练神经网络的自动微分。在代码执行过程中动态创建计算图，PyTorch因其灵活性、易用性以及在模型训练中的高效性而受到青睐。
- en: Built on top of PyTorch, **PyTorch Lightning** (**PL**) provides another layer
    of abstraction, hiding many boilerplate codes. The new framework pays more attention
    to researchers by decoupling research-related components of PyTorch from the engineering-related
    components. PL codes are typically more scalable and easier to read than PyTorch
    codes. Even though the code snippets in this book put more emphasis on PL, PyTorch
    and PL share a lot of functionalities, so most components are interchangeable.
    If you are willing to dig into the details, we recommend the official site, [https://pytorch.org](https://pytorch.org).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于PyTorch，**PyTorch Lightning**（**PL**）提供了另一层抽象，隐藏了许多样板代码。新框架更多关注研究人员，通过将PyTorch的研究相关组件与工程相关组件解耦。PL的代码通常比PyTorch代码更可扩展和易读。尽管本书中的代码片段更加强调PL，但PyTorch和PL共享许多功能，因此大多数组件是可互换的。如果您愿意深入了解细节，我们推荐访问官方网站，[https://pytorch.org](https://pytorch.org)。
- en: 'There are other extensions of PyTorch available on the market:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上还有其他扩展PyTorch的选择：
- en: Skorch ([https://github.com/skorch-dev/skorch](https://github.com/skorch-dev/skorch))
    – A scikit-learn compatible neural network library that wraps PyTorch
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skorch ([https://github.com/skorch-dev/skorch](https://github.com/skorch-dev/skorch))
    – 一个与scikit-learn兼容的神经网络库，包装了PyTorch。
- en: Catalyst ([https://github.com/catalyst-team/catalyst](https://github.com/catalyst-team/catalyst))
    – A PyTorch framework specialized for reproducibility, rapid experimentation,
    and codebase reuse
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst ([https://github.com/catalyst-team/catalyst](https://github.com/catalyst-team/catalyst))
    – 一个专为可复现性、快速实验和代码库重用而设计的PyTorch框架。
- en: Fastai ([https://github.com/fastai/fastai](https://github.com/fastai/fastai))
    – A library that standardizes not only high-level components for practitioners
    but also delivers low-level components for researchers
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fastai ([https://github.com/fastai/fastai](https://github.com/fastai/fastai))
    – 一个不仅为实践者提供高级组件，还为研究人员提供低级组件的库。
- en: PyTorch Ignite ([https://pytorch.org/ignite/](https://pytorch.org/ignite/))
    – A library designed to help with training and evaluation for practitioners
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Ignite ([https://pytorch.org/ignite/](https://pytorch.org/ignite/))
    – 一个专为实践者提供训练和评估帮助的库。
- en: We will not cover these libraries in this book, but you may find them helpful
    if you are new to this field.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本书中涵盖这些库，但如果您对这个领域是新手，可能会发现它们很有帮助。
- en: Now, let’s dive into PyTorch and PL.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解PyTorch和PL。
- en: PyTorch data loading logic
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch数据加载逻辑
- en: For readability and modularity, PyTorch and PL exploit a class called `Dataset`
    for data management and another class, `DataLoader`, for accessing samples iteratively.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可读性和模块化，PyTorch和PL利用名为`Dataset`的类进行数据管理，并利用另一个名为`DataLoader`的类来迭代访问样本。
- en: While the `Dataset` class handles fetching individual samples, model training
    takes in the input data in batches and requires reshuffling to reduce model overfitting.
    `DataLoader` abstracts this complexity for users by providing a simple API. Furthermore,
    it exploits Python’s multiprocessing features behind the scenes to speed up data
    retrieval.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`Dataset`类处理获取单个样本，但模型训练会批量输入数据，并需要重排以减少模型过拟合。`DataLoader`通过提供简单的API来为用户抽象这种复杂性。此外，它在后台利用Python的多进程功能加速数据检索。
- en: 'The two core functions that must be implemented by the child class of `Dataset`
    are `__len__` and `__getitem__`. As described in the following class outline,
    `__len__` should return the total number of samples and `__getitem__` should return
    a sample for the given index:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`子类必须实现的两个核心函数是`__len__`和`__getitem__`。如下课程大纲所述，`__len__`应返回总样本数，`__getitem__`应返回给定索引的样本：'
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'PL’s `LightningDataModule` encapsulates all the steps needed to process data.
    The key components include downloading and cleaning data, preprocessing each sample,
    and wrapping each type of dataset inside `DataLoader`. The following code snippet
    describes how to create a `LightningDataModule` class. The class has the `prepare_data`
    function for downloading and preprocessing the data, as well as three functions
    for instantiating `DataLoader` for each type of dataset, `train_dataloader`, `val_dataloader`,
    and `test_dataloader`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PL的`LightningDataModule`封装了处理数据所需的所有步骤。关键组件包括下载和清理数据，预处理每个样本，并将每种数据集包装在`DataLoader`中。以下代码片段描述了如何创建`LightningDataModule`类。该类具有`prepare_data`函数用于下载和预处理数据，以及三个函数用于实例化每种数据集的`DataLoader`，分别是`train_dataloader`、`val_dataloader`和`test_dataloader`：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The official documentation for `LightningDataModule` can be found at [https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html](https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`LightningDataModule` 的官方文档可以在 [https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html](https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html)
    找到。'
- en: PyTorch model definition
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 模型定义
- en: 'The key benefit of PL comes from `LightningModule`, which simplifies the organization
    of complex PyTorch codes into six sections:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PL 的关键优势来自 `LightningModule`，它将复杂的 PyTorch 代码简化为六个部分：
- en: Computation (`__init__`)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算（`__init__`）
- en: The train loop (`training_step`)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环（`training_step`）
- en: The validation loop (`validation_step`)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证循环（`validation_step`）
- en: The test loop (`test_step`)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试循环（`test_step`）
- en: The prediction loop (`predict_step`)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测循环（`predict_step`）
- en: Optimizers and LR scheduler (`configure_optimizers`)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器和学习率调度器（`configure_optimizers`）
- en: 'The model architecture is part of the computation section. Necessary layers
    are instantiated inside the `__init__` method, and computational logics are defined
    in the `forward` method. In the following code snippet, three linear layers are
    registered to the `LightningModule` module inside the `__init__` method, and the
    relationships between them are defined inside the `forward` method:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构是计算部分的一部分。必要的层在 `__init__` 方法中实例化，计算逻辑在 `forward` 方法中定义。在以下代码片段中，三个线性层在
    `__init__` 方法内注册到 `LightningModule` 模块，并在 `forward` 方法内定义它们之间的关系：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Another way of defining a network is to use `torch.nn.Sequential`, as shown
    in the following code. With this module, a set of layers can be grouped together,
    and output chaining is automatically achieved:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种定义网络的方法是使用 `torch.nn.Sequential`，如下所示。使用此模块，一组层可以被组合在一起，并且自动实现输出链式化：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, the three linear layers are grouped together and stored
    as a single instance variable, `self.multiple_layers`. In the `forward` method,
    we simply trigger `self.multiple_layers` with the input tensor to pass the tensor
    through each layer one by one.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，三个线性层被组合在一起并存储为单个实例变量 `self.multiple_layers`。在 `forward` 方法中，我们只需触发
    `self.multiple_layers` 与输入张量以逐个通过每个层传递张量。
- en: The following section is designed to introduce popular layer implementations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分旨在介绍常见的层实现。
- en: PyTorch DL layers
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch DL 层
- en: 'One of the major benefits of DL frameworks comes from various layer definitions:
    gradient calculation logics are already part of the layer definitions, so you
    can focus on finding the best model architecture for your task. In this section,
    we will learn about layers that are commonly used across projects. Please refer
    to the official documentation ([https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html))
    if the layer that you are interested in is not covered in this section.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DL 框架的一个主要优势来自于各种层定义：梯度计算逻辑已经成为层定义的一部分，因此您可以专注于找到适合任务的最佳模型架构。在本节中，我们将了解跨项目常用的层。如果您感兴趣的层在本节中未涵盖，请参阅官方文档（[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)）。
- en: PyTorch dense (linear) layer
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 密集（线性）层
- en: 'The first type of layer is `torch.nn.Linear`. As the name suggests, it applies
    a linear transformation to the input tensor. The two main parameters of the function
    are `in_features` and `out_features`, which define the input and output tensor
    dimensions, respectively:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种类型的层是 `torch.nn.Linear`。顾名思义，它对输入张量应用线性变换。函数的两个主要参数是 `in_features` 和 `out_features`，分别定义输入和输出张量的维度：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The layer implementation from the `torch.nn` module already has the `forward`
    function defined, so that you can use the layer variable as if it were a function
    to trigger forward propagation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `torch.nn` 模块的层实现已经定义了 `forward` 函数，因此您可以将层变量像函数一样使用，以触发前向传播。
- en: PyTorch pooling layers
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 池化层
- en: Pooling layers are commonly used for downsampling a tensor. The two most popular
    types are maximum pooling and average pooling. The key parameters for these layers
    are `kernel_size` and `stride`, which define the size of the window and how it
    moves for each pooling operation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层通常用于对张量进行下采样。最流行的两种类型是最大池化和平均池化。这些层的关键参数是 `kernel_size` 和 `stride`，分别定义窗口的大小以及每个池化操作的移动方式。
- en: 'The maximum pooling layer downsamples the input tensor by selecting the largest
    value for each window:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化层通过选择每个窗口中的最大值来对输入张量进行下采样：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On the other hand, the average pooling layer downsamples the input tensor by
    computing an average value for each window:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，平均池化层通过计算每个窗口的平均值来降低输入张量的分辨率：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can find the other types of pooling layers at [https://pytorch.org/docs/stable/nn.html#pooling-layers](https://pytorch.org/docs/stable/nn.html#pooling-layers).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://pytorch.org/docs/stable/nn.html#pooling-layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)
    找到其他类型的池化层。
- en: PyTorch normalization layers
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 归一化层
- en: Commonly used in data processing, the purpose of normalization is to scale numerical
    data to a common scale without distorting the distribution. In the case of DL,
    normalization layers are used to train the network with greater numerical stability
    ([https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据处理中常用的归一化的目的是将数值数据缩放到一个公共尺度，而不改变其分布。在深度学习中，归一化层用于以更大的数值稳定性训练网络 ([https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers))。
- en: 'The most popular normalization layer is the batch normalization layer, which
    scales a set of values in a mini-batch. In the following code snippet, we introduce
    `torch.nn.BatchNorm2d`, a batch normalization layer designed for a mini-batch
    of 2D tensors with an additional channel dimension:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的归一化层是批量归一化层，它对一个小批量的值进行缩放。在下面的代码片段中，我们介绍了 `torch.nn.BatchNorm2d`，一个为带有额外通道维度的二维张量小批量设计的批量归一化层：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Out of the various parameters, the main one that you should be aware of is `num_features`,
    which indicates the number of channels. The input to the layer is a 4D tensor,
    where each index indicates the batch size (`N`), number of channels (`C`), the
    height of the image (`H`), and the width of the image (`W`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种参数中，你应该了解的主要参数是 `num_features`，它表示通道数。层的输入是一个四维张量，其中每个索引表示批量大小 (`N`)，通道数
    (`C`)，图像高度 (`H`) 和图像宽度 (`W`)。
- en: PyTorch dropout layer
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 丢弃层
- en: 'The dropout layer helps the model to extract generic features by randomly setting
    a set of values to zero. This operation prevents the model from overfitting to
    the train set. Having said that, the dropout layer implementation of PyTorch mainly
    operates over a single parameter, `p`, which controls the probability of an element
    being zeroed:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃层帮助模型通过随机将一组值设为零来提取通用特征。这种操作可以防止模型过度拟合训练集。话虽如此，PyTorch 的丢弃层主要通过一个参数 `p` 控制一个元素被设为零的概率：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, we are dropping 50% of the elements (`p=0.5`). Similar to the
    batch normalization layer, the input tensor for `torch.nn.Dropout2d` has a size
    of `N, C, H, W`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将元素的50%设为零 (`p=0.5`)。与批量归一化层类似，`torch.nn.Dropout2d` 的输入张量大小为 `N, C,
    H, W`。
- en: PyTorch convolution layers
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 卷积层
- en: 'Specialized for image processing, the convolutional layer is designed to apply
    convolution operations over the input tensor using a sliding window technique.
    In the case of image processing, where intermediate data is represented as 4D
    tensors of size `N, C, H, W`, `torch.nn.Conv2d` is the standard choice:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 针对图像处理专门设计的卷积层，使用滑动窗口技术在输入张量上应用卷积操作。在图像处理中，中间数据以 `N, C, H, W` 大小的四维张量表示，`torch.nn.Conv2d`
    是标准选择：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first parameter of the `torch.nn.Conv2d` class, `in_channels`, indicates
    the number of channels in the input tensor. The second parameter, `out_channels`,
    indicates the number of channels in the output tensor, which is equal to the number
    of filters. The other parameters, `kernel_size`, `stride`, and `padding`, determine
    how the convolution operations are carried out for the layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Conv2d` 类的第一个参数 `in_channels` 表示输入张量中的通道数。第二个参数 `out_channels` 表示输出张量中的通道数，即滤波器的数量。其他参数
    `kernel_size`，`stride` 和 `padding` 决定了如何对该层进行卷积操作。'
- en: PyTorch recurrent layers
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 循环层
- en: 'Recurrent layers are designed for sequential data. Among the various types
    of recurrent layers, we will cover `torch.nn.RNN` in this section, which applies
    a multi-layer Elman **recurrent neural network** (**RNN**) to the given sequence
    ([https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)).
    If you would like to try different recurrent layers, you can refer to the official
    documentation: [https://pytorch.org/docs/stable/nn.html#recurrent-layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 递归层设计用于序列数据。在各种类型的递归层中，本节将介绍`torch.nn.RNN`，它将多层Elman**递归神经网络**（**RNN**）应用于给定的序列（[https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)）。如果您想尝试不同的递归层，可以参考官方文档：[https://pytorch.org/docs/stable/nn.html#recurrent-layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The three key parameters of `torch.nn.RNN` are `input_size`, `hidden_size`,
    and `num_layers`. They refer to the number of expected features in the input tensor,
    the number of features in the hidden state, and the number of recurrent layers
    to use, respectively. To trigger forward propagation, you need to pass two things,
    an input tensor and a tensor containing the initial hidden state.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.RNN`的三个关键参数是`input_size`、`hidden_size`和`num_layers`。它们分别指的是输入张量中预期的特征数、隐藏状态中的特征数以及要使用的递归层数。为了触发前向传播，您需要传递两个东西，一个输入张量和一个包含初始隐藏状态的张量。'
- en: PyTorch model training
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch模型训练
- en: 'In this section, we describe the model training component of PL. As shown in
    the following code block, `LightningModule` is the base class that you must inherit
    for this component. Its `configure_optimizers` function is used to define the
    optimizer for training. Then, the actual training logic is defined within the
    `training_step` function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了PL的模型训练组件。如下面的代码块所示，`LightningModule`是您必须继承的基类。它的`configure_optimizers`函数用于定义训练的优化器。然后，实际的训练逻辑在`training_step`函数中定义：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Validation, prediction, and the test loop have similar function definitions;
    a batch gets fed into the network to compute the necessary predictions and loss
    values. The collected data can also be stored and displayed using PL’s built-in
    logging system. For details, please refer to the official documentation ([https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 验证、预测和测试循环具有类似的函数定义；一个批次被馈送到网络中以计算必要的预测和损失值。收集的数据也可以使用PL的内置日志记录系统进行存储和显示。有关详细信息，请参阅官方文档（[https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)）：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Under the hood, `LightningModule` executes the following set of simplified
    PyTorch codes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`LightningModule`执行以下一系列简化的PyTorch代码：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Putting `LightningDataModule` and `LightningModule` together, the training
    and inference on the test set can be simply achieved as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将`LightningDataModule`和`LightningModule`结合在一起，可以简单地实现对测试集的训练和推理如下：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By now, you should’ve learned what you need to implement to set up a model training
    using PyTorch. The following two sections are dedicated to loss functions and
    optimizers, the two major components of model training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该已经学会了如何使用PyTorch设置模型训练。接下来的两节专门介绍了损失函数和优化器，这两个模型训练的主要组成部分。
- en: PyTorch loss functions
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch损失函数
- en: First, we will look at the different loss functions available in PL. The loss
    functions in this sections can be found from the `torch.nn` module.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看PL中提供的不同损失函数。本节中的损失函数可以在`torch.nn`模块中找到。
- en: PyTorch MSE / L2 loss function
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch MSE / L2损失函数
- en: 'MSE loss function can be created using `torch.nn.MSELoss`. However, this calculates
    the square error component only and exploits the `reduction` parameter to provide
    variations. When `reduction` is `None`, the calculated value is returned as is.
    On the other hand, when it is set to `sum`, the outputs will be summed up. To
    obtain the exact MSE loss, the reduction must be set to `mean`, as shown in the
    following code snippet:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`torch.nn.MSELoss`创建MSE损失函数。但是，这仅计算平方误差分量，并利用`reduction`参数提供变体。当`reduction`为`None`时，返回计算值。另一方面，当设置为`sum`时，输出将被累加。为了获得精确的MSE损失，必须将reduction设置为`mean`，如下面的代码片段所示：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, let’s have a look at MAE loss.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看MAE损失。
- en: PyTorch MAE / L1 loss function
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch MAE / L1损失函数
- en: 'MAE loss function can be instantiated using `torch.nn.L1Loss`. Similar to MSE
    loss function, this function calculates different values based on the `reduction`
    parameter:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: MAE（Mean Absolute Error，平均绝对误差）损失函数可以通过`torch.nn.L1Loss`来实例化。与MSE损失函数类似，此函数根据`reduction`参数计算不同的值：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can now move on to CE loss, which is used in multi-class classification tasks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以转向CE损失，它在多类分类任务中使用。
- en: PyTorch CE loss functions
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch交叉熵损失函数
- en: '`torch.nn.CrossEntropyLoss` is useful when training a model for a classification
    problem with multiple classes. As shown in the following code snippet, this class
    also has a `reduction` parameter for calculating different variations. You can
    further change the behavior of the loss using `weight` and `ignore_index` parameters,
    which weight each class and ignore specific indices, respectively:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.CrossEntropyLoss`在训练多类别分类问题的模型时非常有用。正如下面的代码片段所示，此类别还具有`reduction`参数以计算不同的变体。您还可以使用`weight`和`ignore_index`参数来进一步改变损失的行为，分别对每个类别进行加权并忽略特定索引：'
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In a similar fashion, we can define BCE loss.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以定义BCE损失。
- en: PyTorch BCE loss functions
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch BCE损失函数
- en: 'Similar to CE loss, PyTorch defines the BCE loss as `torch.nn.BCELoss` with
    the same set of parameters. However, exploiting the close relationship between
    `torch.nn.BCELoss` and the sigmoid operation, PyTorch provides `torch.nn.BCEWithLogitsLoss`,
    which achieves higher numerical stability by combining the `softmax` operation
    and the BCE loss calculation in a single class. The usage is shown in the following
    code snippet:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CE损失，PyTorch将BCE损失定义为`torch.nn.BCELoss`，具有相同的参数集。然而，利用`torch.nn.BCELoss`与sigmoid操作之间的密切关系，PyTorch提供了`torch.nn.BCEWithLogitsLoss`，通过在一个类中组合`softmax`操作和BCE损失计算，实现了更高的数值稳定性。其使用方法如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, let’s have a look at construction of a custom loss in PyTorch.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下如何构建PyTorch中的自定义损失。
- en: PyTorch custom loss functions
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch自定义损失函数
- en: Defining a custom loss function is straightforward. Any function defined with
    PyTorch operations can be used as a loss function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义损失函数非常简单。任何使用PyTorch操作定义的函数都可以用作损失函数。
- en: 'The following is a sample implementation of `torch.nn.MSELoss` using the `mean`
    operator:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`mean`操作符实现的`torch.nn.MSELoss`的样本实现：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, we will move to the overview of optimizers in PyTorch.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将转到PyTorch优化器的概述。
- en: PyTorch optimizers
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch优化器
- en: As described in the *PyTorch model training* section, the `configure_optimizers`
    function of `LightningModule` specifies the optimizer for the training. In PyTorch,
    predefined optimizers can be found from the `torch.optim` module. The optimizer
    instantiation requires model parameters, which can be obtained by calling the
    `parameters` function on the model, as shown in the following sections.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如*PyTorch模型训练*部分所述，`LightningModule`的`configure_optimizers`函数指定了训练的优化器。在PyTorch中，可以从`torch.optim`模块中找到预定义的优化器。优化器实例化需要模型参数，可以通过在模型上调用`parameters`函数来获取，如以下部分所示。
- en: PyTorch SGD optimizer
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch SGD优化器
- en: The following code snippet instantiates an SGD optimizer with an LR of `0.1`
    and demonstrates how a single step of a model parameter update can be achieved.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段实例化了一个LR为`0.1`的SGD优化器，并展示了如何实现模型参数更新的单步操作。
- en: '`torch.optim.SGD` has built-in support for momentum and acceleration, which
    further improves training performance. It can be configured using `momentum` and
    `nesterov` parameters:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.optim.SGD`内置支持动量和加速，进一步提高了训练性能。可以使用`momentum`和`nesterov`参数进行配置：'
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: PyTorch Adam optimizer
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch Adam优化器
- en: 'Similarly, an Adam optimizer can be instantiated using `torch.optim.Adam`,
    as shown in the following line of code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，可以使用`torch.optim.Adam`来实例化Adam优化器，如下面的代码行所示：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you are curious about how optimizers work in PyTorch, we recommend reading
    over the official documentation: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对PyTorch中优化器的工作原理感兴趣，我们建议阅读官方文档：[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)。
- en: Things to remember
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事情
- en: a. PyTorch is a popular DL framework that provides GPU-accelerated matrix calculation
    and automatic differentiation. PyTorch is gaining popularity for its flexibility,
    ease of use, as well as efficiency in model training.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: a. PyTorch是一种流行的深度学习框架，提供了GPU加速的矩阵计算和自动微分功能。PyTorch因其灵活性、易用性以及在模型训练中的效率而受到欢迎。
- en: b. For readability and modularity, PyTorch exploits a class called `Dataset`
    for data management and another class, `DataLoader`, for accessing samples iteratively.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: b. 为了提高可读性和模块化性，PyTorch利用名为`Dataset`的类来进行数据管理，并使用另一个名为`DataLoader`的类来迭代访问样本。
- en: 'c. The key benefit of PL comes from `LightningModule`, which simplifies the
    organization of the complex PyTorch code structure into six sections: computation,
    a train loop, validation loop, test loop, prediction loop, as well as optimizers
    and LR scheduler'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: c. PL的关键优势来自于`LightningModule`，它简化了复杂的PyTorch代码结构组织为六个部分：计算、训练循环、验证循环、测试循环、预测循环，以及优化器和学习率调度器。
- en: d. PyTorch and PL share the `torch.nn` module for various layers and loss functions.
    Predefined optimizers can be found from the `torch.optim` module.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: d. PyTorch和PL共享`torch.nn`模块，用于各种层和损失函数。预定义的优化器可以在`torch.optim`模块中找到。
- en: In the following section, we will look at another DL framework, TF. Training
    set up with TF is remarkably similar to the set up with PyTorch.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看一看另一个深度学习框架，TF。使用TF进行的训练设置与使用PyTorch的设置非常相似。
- en: Implementing and training a model in TF
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TF中实现和训练模型
- en: While PyTorch is oriented towards research projects, TF puts more emphasis on
    industry use cases. While the deployment features of PyTorch, Torch Serve, and
    Torch Mobile are still in the experimental phase, the deployment features of TF,
    TF Serve, and TF Lite are stable and actively in use. The first version of TF
    was introduced by the Google Brain team in 2011 and they have been continuously
    updating TF to make it more flexible, user-friendly, and efficient. The key difference
    between TF and PyTorch was initially much larger, as the first version of TF used
    static graphs. However, this situation has changed with version 2, as it introduces
    eager execution, mimicking dynamic graphs known from PyTorch. TF version 2 is
    often used with **Keras**, an interface for ANN ([https://keras.io](https://keras.io/getting_started/)).
    Keras allows users to quickly develop DL models and run experiments. In the following
    sections, we will walk you through the key components of TF.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PyTorch偏向于研究项目，TF更加注重行业应用案例。尽管PyTorch的部署功能，如Torch Serve和Torch Mobile仍处于实验阶段，TF的部署功能，如TF
    Serve和TF Lite已经稳定并且在积极使用中。TF的第一个版本由Google Brain团队于2011年推出，并且他们持续更新TF以使其更加灵活、用户友好和高效。TF和PyTorch的关键区别最初相差较大，因为TF的第一个版本使用静态图。然而，随着版本2的推出，情况已经改变，它引入了急切执行，模仿了PyTorch中的动态图。TF版本2经常与**Keras**一起使用，这是一个用于人工神经网络的接口（[https://keras.io](https://keras.io/getting_started/)）。Keras允许用户快速开发深度学习模型并运行实验。在接下来的部分中，我们将介绍TF的关键组件。
- en: TF data loading logic
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF数据加载逻辑
- en: Data can be loaded for TF models in various ways. One of the key data manipulation
    modules that you should be aware of is `tf.data`, which helps you to build efficient
    input pipelines. `tf.data` provides `tf.data.Dataset` and `tf.data.TFRecordDataset`
    classes that are designed for loading datasets of different data formats. In addition,
    there are `tensorflow_datasets` (`tfds`) modules ([https://www.tensorflow.org/datasets/api_docs/python/tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds))
    and `tensorflow_addons` modules ([https://www.tensorflow.org/addons](https://www.tensorflow.org/addons))
    that further simplify the data loading process in many cases. It is also worth
    mentioning the TF I/O package ([https://www.tensorflow.org/io/overview](https://www.tensorflow.org/io/overview)),
    which expands the capabilities of the standard TF file system interaction.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: TF模型可以以多种方式加载数据。你应该了解的一个关键数据操作模块是`tf.data`，它帮助你构建高效的输入管道。`tf.data`提供了`tf.data.Dataset`和`tf.data.TFRecordDataset`类，专门设计用于加载不同数据格式的数据集。此外，还有`tensorflow_datasets`（`tfds`）模块（[https://www.tensorflow.org/datasets/api_docs/python/tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds)）和`tensorflow_addons`模块（[https://www.tensorflow.org/addons](https://www.tensorflow.org/addons)），它们进一步简化了许多情况下的数据加载过程。还值得一提的是TF
    I/O包（[https://www.tensorflow.org/io/overview](https://www.tensorflow.org/io/overview)），它扩展了标准TF文件系统交互的功能。
- en: 'Regardless of the package that you are going to use, you should consider creating
    a `DataLoader` class. In this class, you will clearly define how the target data
    will be loaded and how it will be preprocessed before the training. The following
    code snippet is a sample implementation with loading logic:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你将使用哪个包，你都应该考虑创建一个`DataLoader`类。在这个类中，你将清晰地定义目标数据如何加载以及在训练之前如何预处理。以下代码片段展示了一个加载逻辑的示例实现：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding example, we use `tfds` to load data from the external URL
    (`config.data_url`). More information about `tfds.load` can be found online: [https://www.tensorflow.org/datasets/api_docs/python/tfds/load](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用`tfds`从外部URL（`config.data_url`）加载数据。关于`tfds.load`的更多信息可以在线找到：[https://www.tensorflow.org/datasets/api_docs/python/tfds/load](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)。
- en: 'Data is available in various formats. Therefore, it is important that it is
    preprocessed into the format that TF models can consume using the functionalities
    provided by the `tf.data` module. So, let’s have a look at how to use this package
    for reading data of common formats:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以各种格式可用。因此，重要的是将其预处理为TF模型可以使用的格式，使用`tf.data`模块提供的功能。因此，让我们看一下如何使用这个包来读取常见格式的数据：
- en: 'First, data in `tfrecord`, a format designed for storing a sequence of binary
    data, can be read as follows:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，可以按如下方式读取以`tfrecord`格式存储的数据序列：
- en: '[PRE23]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can create a dataset object from a NumPy array using the `tf.data.Dataset.from_tensor_slices`
    function as follows:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`tf.data.Dataset.from_tensor_slices`函数从NumPy数组创建数据集对象，如下所示：
- en: '[PRE24]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Pandas DataFrames can also be loaded as a dataset using the same `tf.data.Dataset.from_tensor_slices`
    function:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas DataFrames也可以使用相同的`tf.data.Dataset.from_tensor_slices`函数加载为数据集：
- en: '[PRE25]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Another option is to use a Python generator. Here is a simple example that
    highlights how to use a generator to feed a paired image and label:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种选项是使用Python生成器。以下是一个简单的示例，演示如何使用生成器来提供配对的图像和标签：
- en: '[PRE26]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As shown in the last code snippet, `tf.data.Dataset` provides us with built-in
    data loading functionalities such as batching, repeating, and shuffling. These
    options are self-explanatory: batching creates mini-batches of a specific size,
    repeating allows us to iterate over dataset multiple times, and shuffling mixes
    up the data entries for every epoch.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一段代码片段所示，`tf.data.Dataset` 提供了内置的数据加载功能，例如批处理、重复和洗牌。这些选项都是不言自明的：批处理创建特定大小的小批量数据，重复允许我们多次迭代数据集，而洗牌则在每个
    epoch 中混淆数据条目。
- en: Before we wrap up this section, we would like to mention that models implemented
    with Keras can directly consume NumPy arrays and Pandas DataFrames.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，我们想提到，使用Keras实现的模型可以直接消耗NumPy数组和Pandas DataFrames。
- en: TF model definition
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF模型定义
- en: 'Similar to how PyTorch and PL handles model definition, TF provides various
    ways of defining network architecture. First, we will look at `Keras.Sequential`,
    which chains a set of layers to construct a network. This class handles the linkage
    for you so that you don’t need to define the linkage between the layers explicitly:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于PyTorch和PL如何处理模型定义，TF提供了多种定义网络架构的方法。首先，我们将看看`Keras.Sequential`，它链式连接一组层以构建网络。这个类为您处理了层之间的链接，因此您不需要显式地定义它们的连接：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding example, we are creating a model that consists of an input
    layer, two dense layers, and an output layer that generates a single neuron as
    an output. This is a simple model that can be used for binary classification.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了一个模型，包括一个输入层、两个密集层和一个生成单个神经元输出的输出层。这是一个简单的模型，可以用于二元分类。
- en: 'If the model definition is more complex and cannot be constructed in a sequential
    manner, another option is to use the `keras.Model` class, as shown in the following
    code snippet:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型定义更复杂，无法按顺序构建，另一种选择是使用`keras.Model`类，如下面的代码片段所示：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this example, we have two inputs with a distinct set of computations. The
    two paths are merged in the last concatenation layer, which transports the concatenated
    tensor into the final dense layer with five neurons. Given that the last layer
    uses `softmax` activation, this model can be used for multi-class classification.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有两个输入，并进行了不同的计算。这两条路径在最后的串联层中合并，将串联的张量传输到最终的具有五个神经元的密集层中。考虑到最后一层使用了`softmax`激活函数，这个模型可以用于多类分类。
- en: 'The third option, as follows, is to create a class that inherits `keras.Model`.
    This option gives you the most flexibility, as it allows you to customize every
    part of the model and the training process:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项如下，是创建一个继承了`keras.Model`的类。这个选项给了你最大的灵活性，允许你自定义模型的每个部分和训练过程：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`SimpleANN`, from the preceding code, inherits `Keras.Model`. Within the `__init__`
    function, we need to define the network architecture using a `tf.keras.layers`
    module or basic TF operations. The forward propagation logic is defined inside
    a `call` method, just as PyTorch has the `forward` method.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleANN`，来自前面的代码，继承自 `Keras.Model`。在 `__init__` 函数中，我们需要使用 `tf.keras.layers`
    模块或基本的 TF 操作来定义网络架构。前向传播逻辑在 `call` 方法内部定义，就像 PyTorch 中有 `forward` 方法一样。'
- en: 'When the model is defined as a distinct class, you can link additional functionalities
    to the class. In the following example, the `build_graph` method is added to return
    a `keras.Model` instance, so you can, for example, use the `summary` function
    to visualize the network architecture as a simpler representation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型被定义为一个独立的类时，您可以将额外的功能链接到该类。在下面的示例中，添加了 `build_graph` 方法以返回一个 `keras.Model`
    实例，因此您可以例如使用 `summary` 函数来可视化网络架构作为更简单的表示：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, let’s look at how TF provides a set of layer implementations through Keras.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 TF 如何通过 Keras 提供一组层实现。
- en: TF DL layers
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF 深度学习层
- en: As mentioned in the previous section, the `tf.keras.layers` module provides
    a set of layer implementations that you can use for building a TF model. In this
    section, we will cover the same set of layers that we described in the *Implementing
    and training a model in PyTorch* section. The complete list of layers available
    in this module can be found at [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，`tf.keras.layers` 模块提供了一组层实现，您可以用来构建 TF 模型。在本节中，我们将涵盖与我们在 *PyTorch 中实现和训练模型*
    部分描述的相同一组层。此模块中可用的层的完整列表可以在 [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)
    找到。
- en: TF dense (linear) layers
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 密集（线性）层
- en: 'The first one is `tf.keras.layers.Dense`, which performs a linear transformation:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是 `tf.keras.layers.Dense`，执行线性转换：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `units` parameter defines the number of neurons in the dense layer (the
    dimensionality of the output). If the `activation` parameter is not defined, the
    output of the layer will be returned as is. As presented in the following code,
    we can apply an `Activation` operation outside of the layer definition as well:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`units` 参数定义了密集层中神经元的数量（输出的维度）。如果未定义 `activation` 参数，则层的输出将原样返回。如下面的代码所示，我们也可以在层定义之外应用
    `Activation` 操作：'
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In some cases, you will need to build a custom layer. The following example
    demonstrates how to create a dense layer using basic TF operations by inheriting
    the `tensorflow.keras.layers.Layer` class:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您需要构建一个自定义层。下面的示例演示了如何使用基本的 TF 操作创建一个密集层，通过继承 `tensorflow.keras.layers.Layer`
    类：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Within the `__init__` function of the `CustomDenseLayer` class, we define the
    dimensionality of the output (`units`). Then, the state of the layer is instantiated
    within the `build` method; we create and initialize the weights and biases for
    the layer. The last method, `call`, defines the computation itself. For a dense
    layer, it consists of multiplying the inputs with the weights and adding biases.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `CustomDenseLayer` 类的 `__init__` 函数中，我们定义输出的维度（`units`）。然后，在 `build` 方法中实例化层的状态；我们为层创建并初始化权重和偏置。最后的
    `call` 方法定义了计算本身。对于密集层，它包括将输入与权重相乘并添加偏置。
- en: TF pooling layers
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 池化层
- en: '`tf.keras.layers` provides different kinds of pooling layers: average, max,
    global average, and global max pooling layers for one-dimensional temporal data,
    two-dimensional, or three-dimensional spatial data. In this section, we will show
    you two-dimensional max pooling and average pooling layers:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers` 提供不同类型的池化层：平均池化、最大池化、全局平均池化和全局最大池化层，用于一维时间数据、二维或三维空间数据。在本节中，我们将展示二维最大池化和平均池化层：'
- en: '[PRE34]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The two layers both take in `pool_size`, which defines the size of the window.
    The `strides` parameter is used to define how the windows move throughout the
    pooling operation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个层都使用 `pool_size` 参数，定义窗口的大小。`strides` 参数用于定义窗口在池化操作中如何移动。
- en: TF normalization layers
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 标准化层
- en: 'In the following example, we demonstrate a layer for batch normalization, `tf.keras.layers.BatchNormalization`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们演示了一个批量标准化层，`tf.keras.layers.BatchNormalization`：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output of this layer will have mean close to `0` and standard deviation
    close to `1`. Details about each parameter can be found at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的输出将具有接近 `0` 的均值和接近 `1` 的标准差。有关每个参数的详细信息，请查看 [https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)。
- en: TF dropout layers
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF dropout 层
- en: 'The `Tf.keras.layers.Dropout` layer applies dropout, a regularization method
    that sets randomly selected values to zero:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tf.keras.layers.Dropout` 层应用了 dropout，这是一种将随机选择的数值设为零的正则化方法：'
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding layer instantiation, the `rate` argument, a float value between
    `0` and `1`, determines the fraction of the input units that will be dropped.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述层的实例化中，`rate` 参数是一个介于 `0` 和 `1` 之间的浮点值，确定将被丢弃的输入单元的分数。
- en: TF convolution layers
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 卷积层
- en: '`tf.keras.layers` provides various implementations of convolutional layers,
    `tf.keras.layers.Conv1D`, `tf.keras.layers.Conv2D`, `tf.keras.layers.Conv3D`,
    and the corresponding transposed convolutional layers (deconvolution layers),
    `tf.keras.layers.Conv1DTranspose`, `tf.keras.layers.Conv2DTranspose`, and `tf.keras.layers.Conv3DTranspose`.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers` 提供了各种卷积层的实现，包括 `tf.keras.layers.Conv1D`，`tf.keras.layers.Conv2D`，`tf.keras.layers.Conv3D`，以及相应的转置卷积层（反卷积层）
    `tf.keras.layers.Conv1DTranspose`，`tf.keras.layers.Conv2DTranspose`，`tf.keras.layers.Conv3DTranspose`。'
- en: 'The following code snippet describes the instantiation of a two-dimensional
    convolution layer:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段描述了二维卷积层的实例化：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The main parameters in the preceding layer definition are `filters` and `kernel_size`.
    The `filters` parameter defines the dimensionality of the output and the `kernel_size`
    parameter defines the size of the two-dimensional convolution window. For the
    other parameters, please look at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述层定义中的主要参数是 `filters` 和 `kernel_size`。`filters` 参数定义了输出的维度，`kernel_size` 参数定义了二维卷积窗口的大小。有关其他参数，请查看
    [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)。
- en: TF recurrent layers
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 循环层
- en: 'The following list of recurrent layers is implemented in Keras: the `LSTM`
    layer, `GRU` layer, `SimpleRNN` layer, `TimeDistributed` layer, `Bidirectional`
    layer, `ConvLSTM2D` layer, and `Base RNN` layer.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中实现了以下一系列的循环层：`LSTM` 层，`GRU` 层，`SimpleRNN` 层，`TimeDistributed` 层，`Bidirectional`
    层，`ConvLSTM2D` 层和 `Base RNN` 层。
- en: 'In the following code snippet, we demonstrate how to instantiate the `Bidirectional`
    and `LSTM` layers:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们展示了如何实例化 `Bidirectional` 和 `LSTM` 层：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding example, the `LSTM` layer is modified by a `Bidirectional`
    wrapper to provide both an initial sequence and a reversed sequence to two copies
    of the hidden layers. The outputs from the two layers get merged for the final
    output. By default, the outputs are concatenated but the `merge_mode` parameter
    allows us to select a different merging option. The dimensionality of the output
    space is defined by the first parameter. To access the hidden state for each input
    at every time step, you can enable `return_sequences`. For more details, please
    look at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，`LSTM` 层通过 `Bidirectional` 封装器进行了修改，以向两个隐藏层的两个副本提供初始序列和反向序列。这两层的输出被合并以得到最终的输出。默认情况下，输出是连接的，但是
    `merge_mode` 参数允许我们选择不同的合并选项。输出空间的维度由第一个参数定义。要在每个时间步访问每个输入的隐藏状态，可以启用 `return_sequences`。更多细节，请查看
    [https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)。
- en: TF model training
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF 模型训练
- en: For Keras models, model training can be achieved by simply calling a `fit` function
    on the model after calling a `compile` function with an optimizer and a loss function.
    The `fit` function trains the model using the provided dataset for the given number
    of epochs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Keras 模型，只需在调用带有优化器和损失函数的 `compile` 函数后，通过在模型上调用 `fit` 函数即可完成模型训练。`fit` 函数使用提供的数据集进行指定轮数的训练。
- en: 'The following code snippet describes the parameters of the `fit` function:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段描述了 `fit` 函数的参数：
- en: '[PRE39]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`x` and `y` represent the input tensor and the labels. They can be provided
    in various formats: NumPy arrays, TF tensors, TF datasets, generators, or `tf.keras.utils.experimental.DatasetCreator`.
    In addition to `fit`, Keras models also have a `train_on_batch` function that
    only executes a gradient update on a single batch of data.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`和`y`表示输入张量和标签。它们可以以多种格式提供：NumPy 数组、TF 张量、TF 数据集、生成器或`tf.keras.utils.experimental.DatasetCreator`。除了`fit`，Keras
    模型还具有`train_on_batch`函数，仅在单个数据批次上执行梯度更新。'
- en: 'While TF version 1 requires computation graph compilation for the training
    loop, TF version 2 allows us to define the training logic without any compilation,
    as in the case of PyTorch. A typical training loop will look as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TF 版本 1 中，训练循环需要计算图编译，而 TF 版本 2 允许我们在不进行任何编译的情况下定义训练逻辑，就像 PyTorch 一样。典型的训练循环如下所示：
- en: '[PRE40]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the preceding code snippet, the outer loop iterates over epochs and the inner
    loop iterates over the train set. The forward propagation and loss calculation
    is within the scope of `GradientTape`, which records operations for automatic
    differentiation for each batch. Outside of the scope, the optimizer uses the computed
    gradients to update the weights. In the preceding example, TF functions execute
    operations immediately, instead of adding the operation to the computation graph,
    as in eager execution. We would like to mention that you will need to use the
    `@tf.function` decorator if you are using TF version 1, where explicit construction
    of the computation graph is necessary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，外部循环遍历各个 epoch，内部循环遍历训练集。前向传播和损失计算在`GradientTape`的范围内，该范围记录每个批次的自动微分操作。在范围外，优化器使用计算出的梯度更新权重。在上述示例中，TF
    函数立即执行操作，而不是像急切执行中那样将操作添加到计算图中。我们想提到的是，如果您使用的是 TF 版本 1，那么需要使用`@tf.function`装饰器，因为那里需要显式构建计算图。
- en: Next, we will have a look at loss functions in TF.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究 TF 中的损失函数。
- en: TF loss functions
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF 损失函数
- en: 'In TF, the loss function needs to be specified when a model is compiled. While
    you can build a custom loss function from scratch, you can use predefined loss
    functions provided by Keras through the `tf.keras.losses` module ([https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)).
    The following example demonstrates how you can use a loss function from Keras
    to compile a model:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TF 中，当模型编译时需要指定损失函数。虽然您可以从头开始构建自定义损失函数，但您可以通过`tf.keras.losses`模块提供的预定义损失函数来使用
    Keras 提供的损失函数。以下示例演示了如何使用 Keras 的损失函数来编译模型：
- en: '[PRE41]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Additionally, you can pass a string alias to a loss parameter, as shown in
    the following code snippet:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以将字符串别名传递给损失参数，如以下代码片段所示：
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this section, we will explain how the loss functions described in the *PyTorch
    loss functions* section can be instantiated in TF.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何在 TF 中实例化*PyTorch 损失函数*部分中描述的损失函数。
- en: TF MSE / L2 loss functions
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF MSE / L2 损失函数
- en: 'The MSE / L2 loss function can be defined as follows ([https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: MSE / L2 损失函数可以定义如下（[https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)）：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This is the most frequently used loss function for regression – it calculates
    the `mean` value of the squared differences between labels and predictions. The
    default settings will calculate the MSE. However, similar to PyTorch implementation,
    we can provide a `reduction` parameter to change that behavior. For example, if
    you would like to apply a `sum` operation instead of a mean operation, you can
    add `reduction=tf.keras.losses.Reduction.SUM` in the loss function. Given that
    `torch.nn.MSELoss` in PyTorch returns the squared difference as is, you can obtain
    the same loss in TF by passing in `reduction=tf.keras.losses.Reduction.NONE` to
    the constructor.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这是回归中最常用的损失函数，它计算标签和预测之间平方差的`平均`值。默认设置将计算 MSE。然而，类似于 PyTorch 的实现，我们可以提供一个`reduction`参数来改变这种行为。例如，如果您希望应用`sum`操作而不是平均操作，您可以在损失函数中添加`reduction=tf.keras.losses.Reduction.SUM`。鉴于
    PyTorch 中的`torch.nn.MSELoss`返回原始的平方差，您可以通过将`reduction=tf.keras.losses.Reduction.NONE`传递给构造函数，在
    TF 中获得相同的损失。
- en: Next, we will look at MAE loss.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究 MAE 损失。
- en: TF MAE / L1 loss functions
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF MAE / L1 损失函数
- en: '`tf.keras.losses.MeanAbsoluteError` is the function for MAE loss in Keras ([https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError)):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.losses.MeanAbsoluteError`是 Keras 中用于 MAE 损失的函数（[https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError)）:'
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As the name suggests, this loss computes the mean of absolute differences between
    the true and predicted values. It also has a `reduction` parameter that can be
    used in the same way as described for `tf.keras.losses.MeanSquaredError`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，此损失计算真实值和预测值之间的绝对差的平均值。它还有一个`reduction`参数，可以像`tf.keras.losses.MeanSquaredError`中描述的那样使用。
- en: Now, let’s have a look at losses for classification, CE loss.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看分类损失，交叉熵损失。
- en: TF CE loss functions
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 交叉熵损失函数
- en: 'CE loss calculates the difference between two probability distributions. Keras
    provides the `tf.keras.losses.CategoricalCrossentropy` class, which is designed
    for classifying multiple classes ([https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)).
    The following code snippet shows the instantiation:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失计算两个概率分布之间的差异。Keras 提供了`tf.keras.losses.CategoricalCrossentropy`类，专门用于多类分类（[https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)）。以下代码片段展示了其实例化：
- en: '[PRE45]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the case of Keras, labels need to be formatted as one hot vectors. For example,
    when the target class is the first one out of five classes, it’d be `[1, 0, 0,
    0, 0]`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 的情况下，标签需要格式化为独热向量。例如，当目标类是五类中的第一类时，它会是`[1, 0, 0, 0, 0]`。
- en: A CE loss designed for binary classification, BCE loss, also exists.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 用于二元分类的交叉熵损失，BCE 损失，也存在。
- en: TF BCE loss functions
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 二元交叉熵损失函数
- en: 'In the case of a binary classification, the labels are either `0` or `1`. The
    loss function designed specifically for binary classification, BCE loss, can be
    defined as follows ([https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy)):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类的情况下，标签是`0`或`1`。专门为二元分类设计的损失函数，BCE 损失，可以定义如下（[https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy)）：
- en: '[PRE46]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The key parameter for this loss is `from_logits`. When this flag is set to `False`,
    we have to provide probabilities, continuous values between `0` and `1`. When
    it is set to `True`, we need to provide logits, values between `-infinity` and
    `+infinity`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此损失的关键参数是`from_logits`。当此标志设置为`False`时，我们需要提供概率，即介于`0`和`1`之间的连续值。当设置为`True`时，我们需要提供
    logits，即介于`-无穷大`和`+无穷大`之间的值。
- en: Lastly, let’s look at how we can define a custom loss in TF.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何在 TF 中定义自定义损失。
- en: TF custom loss functions
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 自定义损失函数
- en: 'To build a custom loss function, we need to create a function that takes predictions
    and labels as parameters and performs desirable calculations. While TF syntax
    only expects these two arguments, we can also add some additional arguments by
    wrapping the function into another function that returns the loss. The following
    example demonstrates how to create Huber Loss as a custom loss function:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个自定义损失函数，我们需要创建一个接受预测值和标签作为参数并执行所需计算的函数。虽然 TF 的语法只期望这两个参数，但我们可以通过将函数包装到另一个返回损失的函数中来添加一些额外的参数。以下示例展示了如何创建
    Huber Loss 作为自定义损失函数：
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Another option is to create a class that inherits the `tf.keras.losses.Loss`
    class. We need to implement `__init__` and `call` methods in this case, as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是创建一个继承`tf.keras.losses.Loss`类的类。在这种情况下，我们需要实现`__init__`和`call`方法，如下所示：
- en: '[PRE48]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In order to use this loss class, you must instantiate it and pass it to the
    `compile` function through a `loss` parameter, as described at the beginning of
    this section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个损失类，您必须实例化它，并通过一个`loss`参数将其传递给`compile`函数，就像本节开头所描述的那样。
- en: TF optimizers
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF 优化器
- en: In this section, we will describe how to set up different optimizers for model
    training in TF. Similar to loss functions in the preceding section, Keras provides
    a set of optimizers for TF through `tf.keras.optimizers`. Out of the various optimizers,
    we will look at the two main optimizers, SGD and Adam optimizers, in the following
    section.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何在 TF 中设置不同的优化器来进行模型训练。与前一节中的损失函数类似，Keras 通过`tf.keras.optimizers`提供了一组优化器。在各种优化器中，我们将在接下来的章节中看到两个主要的优化器，SGD
    和 Adam 优化器。
- en: TF SGD optimizer
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF SGD 优化器
- en: 'Designed with a fixed LR, an SGD optimizer is the most typical optimizer that
    you can use for many models. The following code snippet describes how to instantiate
    an SGD optimizer in TF:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 设计为固定学习率的SGD优化器是许多模型中最典型的优化器。以下代码片段描述了如何在TF中实例化SGD优化器：
- en: '[PRE49]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Similar to PyTorch implementation, `tf.keras.optimizers.SGD` also supports an
    augmented SGD optimizer using the `momentum` and `nesterov` parameters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于PyTorch的实现，`tf.keras.optimizers.SGD`还支持使用`momentum`和`nesterov`参数的增强型SGD优化器。
- en: TF Adam optimizer
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF Adam 优化器
- en: 'As described in the *Model training logic* section, an Adam optimizer is designed
    with an adaptive LR. In TF, it can be instantiated as the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如“模型训练逻辑”部分所述，Adam 优化器采用自适应学习率。在TF中，可以按以下方式实例化：
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For both optimizers, while `learning_rate` plays the most important role of
    defining the initial LR, we recommend that you review the official documentation
    to familiarize yourself with the other parameters too: [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种优化器，虽然`learning_rate`在定义初始学习率时起着最重要的作用，但我们建议您查阅官方文档，以熟悉其他参数：[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)。
- en: TF callbacks
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF 回调函数
- en: In this section, we would like to briefly describe callbacks. These are the
    objects that are used at various stages of training to perform specific actions.
    The most used callbacks are `EarlyStopping`, `ModelCheckpoint`, and `TensorBoard`,
    which stop the training when a specific condition is met, save the model after
    each epoch, and visualize the training status, respectively.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们想简要描述回调函数。这些是在训练的各个阶段执行特定操作的对象。最常用的回调函数是`EarlyStopping`、`ModelCheckpoint`和`TensorBoard`，它们分别在满足特定条件时停止训练、在每个epoch后保存模型并可视化训练状态。
- en: 'Here is an example of the `EarlyStopping` callback that monitors validation
    loss and stops the training if the monitored loss has stopped decreasing:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个监控验证损失并在监控的损失停止减少时停止训练的`EarlyStopping`回调的示例：
- en: '[PRE51]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `min_delta` parameter defines the minimum change in the monitored quantity
    for the change to be considered an improvement and the `patience` parameter defines
    the number of epochs without any improvements after which the training will be
    stopped.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_delta`参数定义了被监控数量的最小变化量，以便将变化视为改进，而`patience`参数定义了在训练停止之前经过的没有改进的epoch数量。'
- en: 'Building a custom callback can be achieved by inheriting `keras.callbacks.Callback`.
    Defining logic for a specific event can be achieved by overwriting its methods,
    which clearly describe which event it binds to:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 通过继承`keras.callbacks.Callback`可以构建自定义回调函数。通过覆盖其方法可以定义特定事件的逻辑，清晰地描述其绑定的事件：
- en: '`on_train_begin`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_train_begin`'
- en: '`on_train_end`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_train_end`'
- en: '`on_epoch_begin`'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_epoch_begin`'
- en: '`on_epoch_end`'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_epoch_end`'
- en: '`on_test_begin`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_test_begin`'
- en: '`on_test_end`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_test_end`'
- en: '`on_predict_begin`'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_predict_begin`'
- en: '`on_predict_end`'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_predict_end`'
- en: '`on_train_batch_begin`'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_train_batch_begin`'
- en: '`on_train_batch_end`'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_train_batch_end`'
- en: '`on_predict_batch_begin`'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_predict_batch_begin`'
- en: '`on_predict_batch_end`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_predict_batch_end`'
- en: '`on_test_batch_begin`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_test_batch_begin`'
- en: or `on_test_batch_end`
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者`on_test_batch_end`
- en: For the complete details, we recommend that you take a look at [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的详细信息，建议您查看[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)。
- en: Things to remember
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事项
- en: a. `tf.data` allows you to build efficient data loading logic. Packages such
    as `tfds`, `tensorflow addons`, or TF I/O are useful for reading data of different
    formats.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: a. `tf.data`允许您构建高效的数据加载逻辑。诸如`tfds`、`tensorflow addons`或TF I/O等包可以用于读取不同格式的数据。
- en: 'b. TF, with support from Keras, allows users to construct models using three
    different approaches: sequential, functional, and subclassing.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: b. TF通过Keras支持三种不同的模型构建方法：顺序模型、函数式模型和子类化模型。
- en: c. To simplify model development using TF, the `tf.keras.layers` module provides
    various layer implementations, the `tf.keras.losses` module includes different
    loss functions, and the `tf.keras.optimizers` module provides a set of standard
    optimizers.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: c. 为了简化使用TF进行模型开发，`tf.keras.layers`模块提供了各种层的实现，`tf.keras.losses`模块包含不同的损失函数，`tf.keras.optimizers`模块提供了一组标准优化器。
- en: d. `Callbacks` can be used to perform specific actions at the various stages
    of training. The commonly used callbacks are `EarlyStopping` and `ModelCheckpoint`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: d. `Callbacks`可以用于在训练的各个阶段执行特定操作。常用的回调包括`EarlyStopping`和`ModelCheckpoint`。
- en: So far, we have learned how to set up a DL model training using the most popular
    DL frameworks, PyTorch and TF. In the following section, we will look at how the
    components that we have described in this section are used in reality.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用最流行的深度学习框架PyTorch和TF设置DL模型训练。在接下来的部分中，我们将探讨我们在本节描述的组件在实际中如何使用。
- en: Decomposing a complex, state-of-the-art model implementation
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解复杂的最新模型实现
- en: 'Even though you have picked up the basics of TF and PyTorch, setting up a model
    training from scratch can be overwhelming. Luckily, the two frameworks have thorough
    documentations and tutorials that are easy to follow:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您已掌握了TF和PyTorch的基础知识，从头开始设置模型训练可能会让人感到不知所措。幸运的是，这两个框架都有详细的文档和易于跟随的教程：
- en: TF
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF
- en: 'Image classification with convolution layers: [https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification).'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积层进行图像分类：[https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification)。
- en: 'Text classification with recurrent layers: [https://www.tensorflow.org/text/tutorials/text_classification_rnn](https://www.tensorflow.org/text/tutorials/text_classification_rnn).'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用循环层进行文本分类：[https://www.tensorflow.org/text/tutorials/text_classification_rnn](https://www.tensorflow.org/text/tutorials/text_classification_rnn)。
- en: PyTorch
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: 'Object detection with convolutional layers: [https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积层进行目标检测：[https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)。
- en: 'Machine translation with recurrent layers: [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用循环层进行机器翻译：[https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)。
- en: In this section, we would like to look at a model that is much more sophisticated,
    StyleGAN. Our main goal is to explain how the components described in the previous
    sections can be put together for a complex DL project. For the complete description
    of the model architecture and performance, we recommend the publication released
    by NVIDIA, available at [https://ieeexplore.ieee.org/document/8953766](https://ieeexplore.ieee.org/document/8953766).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将介绍一个更复杂的模型，StyleGAN。我们的主要目标是解释如何将前面描述的组件组合起来用于复杂的深度学习项目。关于模型架构和性能的完整描述，我们建议参阅由NVIDIA发布的文章，网址为[https://ieeexplore.ieee.org/document/8953766](https://ieeexplore.ieee.org/document/8953766)。
- en: StyleGAN
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGAN
- en: 'StyleGAN, as a variation of a **generative adversarial network** (**GAN**),
    aims to generate new images from latent codes (random noise vectors). Its architecture
    can be broken down into three elements: a mapping network, a generator, and a
    discriminator. At a high level, the mapping network and generator work together
    to generate an image from a set of random values. The discriminator plays a critical
    role of guiding the generator to generate realistic images during training. Let’s
    take a closer look at each component.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN作为**生成对抗网络**（**GAN**）的一个变体，旨在从潜在代码（随机噪声向量）生成新的图像。其架构可以分解为三个元素：映射网络、生成器和鉴别器。在高层次上，映射网络和生成器共同作用，从一组随机值生成图像。鉴别器在训练过程中发挥了指导生成器生成逼真图像的关键作用。让我们更详细地看看每个组件。
- en: The mapping network and generator
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射网络和生成器
- en: 'While generators are designed to process latent codes directly in a traditional
    GAN, latent codes are fed to the mapping network first in StyleGAN, as shown in
    *Figure 3.5*. The output of the mapping network is then fed to each step of the
    generator, changing the style and details of the generated image. The generator
    starts at a lower resolution, constructing outlines for the image at a tensor
    size of 4 x 4 or 8 x 8\. The details of the images are filled as the generator
    handles the bigger tensors. At the last couple of layers, the generator interacts
    with tensors of sizes 64 x 64 and 1024 x 1024 to construct the high-resolution
    features:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的 GAN 中，生成器设计为直接处理潜在代码，而在 StyleGAN 中，潜在代码首先被馈送到映射网络，如 *图 3.5* 所示。映射网络的输出然后被馈送到生成器的每个步骤，改变生成图像的风格和细节。生成器从较低分辨率开始，以
    4 x 4 或 8 x 8 的张量尺寸构建图像的轮廓。随着生成器处理更大的张量，图像的细节被填充。在最后几层，生成器与 64 x 64 和 1024 x 1024
    尺寸的张量交互，构建高分辨率特征：
- en: '![Figure 3.5 – A mapping network (left) and generator (right) of StyleGAN'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – StyleGAN 的映射网络（左）和生成器（右）'
- en: '](img/B18522_03_05.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_05.jpg)'
- en: Figure 3.5 – A mapping network (left) and generator (right) of StyleGAN
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – StyleGAN 的映射网络（左）和生成器（右）
- en: 'In the preceding figure, the network that takes in a latent vector, **z**,
    and generates **w** is the mapping network. The network on the right is the generator,
    **g**, which takes in a set of noise vectors, as well as **w**. The discriminator
    is fairly simple compared to the generator. The layers are depicted in *Figure
    3.6*:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，接受潜在向量 **z** 并生成 **w** 的网络是映射网络。右侧的网络是生成器 **g**，它接受一组噪声向量以及 **w**。与生成器相比，鉴别器相对简单。图层显示在
    *图 3.6* 中：
- en: '![Figure 3.6 – A StyleGAN discriminator architecture for the FFHQ dataset at
    1024 × 1024 resolution'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – StyleGAN 在 FFHQ 数据集上的 1024 × 1024 分辨率的鉴别器架构'
- en: '](img/B18522_03_06.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_06.jpg)'
- en: Figure 3.6 – A StyleGAN discriminator architecture for the FFHQ dataset at 1024
    × 1024 resolution
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – StyleGAN 在 FFHQ 数据集上的 1024 × 1024 分辨率的鉴别器架构
- en: As depicted in the preceding image, the discriminator consists of multiple blocks
    of convolution layers and downsampling operations. It takes in an image of size
    1024 x 1024 and generates a numeric value between `0` and `1`, describing how
    realistic the image is.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示的图像所示，鉴别器由多个卷积层块和下采样操作组成。它接收大小为 1024 x 1024 的图像并生成介于 `0` 和 `1` 之间的数值，描述图像的真实性。
- en: Training StyleGAN
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 StyleGAN
- en: 'Training StyleGAN requires a lot of computations, so multiple GPUs are necessary
    to achieve a reasonable training time. The estimations are summarized in *Figure
    3.7*:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 StyleGAN 需要大量计算，因此需要多个 GPU 才能达到合理的训练时间。估算结果总结在 *图 3.7* 中：
- en: '![Figure 3.7 – The training time for StyleGAN with an FFHQ dataset on Tesla
    V100 GPUs'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 使用 Tesla V100 GPU 训练 StyleGAN 的训练时间'
- en: '](img/B18522_03_07.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_03_07.jpg)'
- en: Figure 3.7 – The training time for StyleGAN with an FFHQ dataset on Tesla V100
    GPUs
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 使用 Tesla V100 GPU 训练 StyleGAN 的训练时间
- en: 'Therefore, if you want to play around with StyleGAN, we recommend following
    the instructions in the official GitHub repositories, where they provide pre-trained
    models: [https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想尝试使用 StyleGAN，我们建议按照官方 GitHub 存储库中提供的预训练模型的说明进行操作：[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)。
- en: Implementation in PyTorch
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 PyTorch 中的实现
- en: 'Unfortunately, NVIDIA has not shared the public implementation of StyleGAN
    in PyTorch. Instead, they have released StyleGAN2, which shares most of the same
    components. Therefore, we will use the StyleGAN2 implementation for our PyTorch
    example: [https://github.com/NVlabs/stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，NVIDIA 尚未分享 StyleGAN 在 PyTorch 中的公共实现。相反，他们发布了 StyleGAN2，它与大多数相同组件共享。因此，我们将使用
    StyleGAN2 实现作为我们的 PyTorch 示例：[https://github.com/NVlabs/stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch)。
- en: 'All the network components are found under `training/network.py`. The three
    components are named as described in the previous section: `MappingNetwork`, `Generator`,
    and `Discriminator`.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络组件都位于 `training/network.py` 下。三个组件的命名如前所述：`MappingNetwork`、`Generator` 和
    `Discriminator`。
- en: The mapping network in PyTorch
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 中的映射网络
- en: 'The implementation of `MappingNetwork` is self-explanatory. The following code
    snippet includes the core logic for the mapping network:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`MappingNetwork` 的实现是不言自明的。以下代码片段包含映射网络的核心逻辑：'
- en: '[PRE52]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In this network definition, `MappingNetwork` inherits `torch.nn.Module`. Within
    the `__init__` function, the necessary `FullyConnectedLayer` instances are initialized.
    The `forward` method feeds the latent vector, `z`, to each layer.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络定义中，`MappingNetwork`也继承了`torch.nn.Module`。在`__init__`函数中初始化了必要的`FullyConnectedLayer`实例。`forward`方法将潜在向量`z`传递给每一层。
- en: The generator in PyTorch
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch中的生成器
- en: 'The following code snippet describes how the generator is implemented. It consists
    of `MappingNetwork` and `SynthesisNetwork`, as depicted in *Figure 3.5*:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段描述了生成器的实现方式。它包括`MappingNetwork`和`SynthesisNetwork`，如*图3.5*所示：
- en: '[PRE53]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The generator network, `Generator`, also inherits `torch.nn.Module`. `SynthesisNetwork`
    and `MappingNetwork` are instantiated within the `__init__` function and get triggered
    sequentially in the `forward` function. The implementation of `SynthesisNetwork`
    is summarized in the following code snippet:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络`Generator`也继承了`torch.nn.Module`。`SynthesisNetwork`和`MappingNetwork`在`__init__`函数中被实例化，并在`forward`函数中按顺序触发。`SynthesisNetwork`的实现总结如下代码片段：
- en: '[PRE54]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`SynthesisNetwork` has multiple blocks of `SynthesisBlock`. `SynthesisBlock`
    receives noise vectors and the output of `MappingNetwork` to generate a tensor
    that eventually becomes the output image.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`SynthesisNetwork`包含多个`SynthesisBlock`。`SynthesisBlock`接收噪声向量和`MappingNetwork`的输出，生成最终成为输出图像的张量。'
- en: The discriminator in PyTorch
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch中的判别器
- en: 'The following code snippet summarizes the PyTorch implementation of `Discriminator`.
    The network architecture follows the structure depicted in *Figure 3.6*:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段总结了`Discriminator`的PyTorch实现。网络架构遵循*图3.6*中描述的结构：
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Similar to `SynthesisNetwork`, `Discriminator` makes use of the `DiscriminatorBlock`
    class to dynamically create a set of convolutional layers of different sizes.
    They are defined in the `__init__` function, and the tensors are fed to each block
    sequentially in the `forward` function.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`SynthesisNetwork`，`Discriminator`利用`DiscriminatorBlock`类动态创建一组不同尺寸的卷积层。它们在`__init__`函数中定义，并且张量在`forward`函数中按顺序传递给每个块。
- en: Model training logic in PyTorch
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch中的模型训练逻辑
- en: 'Training logic is defined in the `training_loop` function in `training/train_loop.py`.
    The original implementation contains a lot of details. In the following code snippet,
    we will look at the main components that align with what we have learned in the
    *PyTorch model training* section:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 训练逻辑在`training/train_loop.py`的`training_loop`函数中定义。原始实现包含许多细节。在以下代码片段中，我们将查看与*PyTorch模型训练*部分所学内容相符的主要组件：
- en: '[PRE56]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This function receives configurations for various training components and trains
    both `Generator` and `Discriminator`. The outer loop iterates over training samples,
    and the inner loop handles gradient calculation and model parameter updates. The
    training settings are configured by a separate script, `main/train.py`.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接收各种训练组件的配置，并训练`Generator`和`Discriminator`。外部循环迭代训练样本，内部循环处理梯度计算和模型参数更新。训练设置由单独的脚本`main/train.py`配置。
- en: This summarizes the structure of PyTorch implementation. Even though the repository
    looks overwhelming due to the large number of files, we have walked you through
    how to break the implementation down into the components that we have described
    in the *Implementing and training a model in PyTorch* section. In the following
    section, we will look at implementation in TF.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了PyTorch实现的结构。尽管存储库由于大量文件而显得庞大，我们已经指导您如何将实现分解为*在PyTorch中实现和训练模型*部分所描述的组件。在接下来的部分中，我们将查看TF中的实现。
- en: Implementation in TF
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF中的实现
- en: Even though the official implementation is in TF ([https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)),
    we will look at a different implementation presented in *Hands-On Image Generation
    with TensorFlow:* *A Practical Guide to Generating Images and Videos Using Deep
    Learning* by Soon Yau Cheong. This version is based on TF version 2 and aligns
    better with what we have described in this book. The implementation can be found
    at [https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter07/ch7_faster_stylegan.ipynb](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter07/ch7_faster_stylegan.ipynb).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 即使官方实现是基于TF的（[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)），我们将看一个不同的实现，该实现出现在*Hands-On
    Image Generation with TensorFlow:* *A Practical Guide to Generating Images and
    Videos Using Deep Learning* by Soon Yau Cheong。这个版本基于TF版本2，更符合我们在本书中描述的内容。该实现可以在以下链接找到：[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter07/ch7_faster_stylegan.ipynb](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter07/ch7_faster_stylegan.ipynb)。
- en: Similar to the PyTorch implementation described in the previous section, the
    original TF implementation consists of `G_mapping` for the mapping network, `G_style`
    for the generator, and `D_basic` for the discriminator.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前一节中描述的PyTorch实现，原始的TF实现包括 `G_mapping` 作为映射网络，`G_style` 作为生成器，以及 `D_basic`
    作为判别器。
- en: The mapping network in TF
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF中的映射网络
- en: 'Let’s look at the mapping network defined at [https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L384](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L384)
    and its TF version 2 implementation shown below:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下在以下链接定义的映射网络：[https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L384](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L384)，以及其TF版本2的实现如下所示：
- en: '[PRE57]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The implementation of `MappingNetwork` is almost self-explanatory. We can see
    that the mapping network starts with vector w constructed from a latent vector,
    z, using a PixelNorm custom layer. The custom layer is defined as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`MappingNetwork` 的实现几乎是不言自明的。我们可以看到映射网络从潜在向量 z 构建出向量 w，使用了一个PixelNorm自定义层。该自定义层定义如下：'
- en: '[PRE58]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As described in the *TF dense (linear) layers* section, `PixelNorm` inherits
    the `tensorflow.keras.layers.Layer` class and defines the computation within the
    `call` function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*TF dense (linear) layers*部分所述，`PixelNorm` 继承了 `tensorflow.keras.layers.Layer`
    类，并在 `call` 函数中定义计算。
- en: The remaining components of `Mapping` are a set of dense layers with `LeakyReLU`
    activations.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mapping` 的其余组件是一组具有 `LeakyReLU` 激活函数的稠密层。'
- en: Next, we will have a look at the generator network.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下生成器网络。
- en: The generator in TF
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF中的生成器
- en: 'The generator in the original code, `G_style`, is composed of two networks:
    `G_mapping` and `G_synthesis`. See the following: [https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L299](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L299).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码中的生成器 `G_style` 由两个网络组成：`G_mapping` 和 `G_synthesis`。参见以下链接：[https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L299](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L299)。
- en: The complete implementation from the repository might look extremely complex
    at first. However, you will soon find out that `G_style` simply calls `G_mapping`
    and `G_synthesis` sequentially.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 从仓库中获取的完整实现一开始可能看起来非常复杂。然而，你很快会发现 `G_style` 只是依次调用 `G_mapping` 和 `G_synthesis`。
- en: 'The implementation of `SynthesisNetwork` is summarized in the following code
    snippet: [https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L440](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L440).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`SynthesisNetwork` 的实现总结在以下代码片段中：[https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L440](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L440)。'
- en: 'In TF version 2, the generator is implemented as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF版本2中，生成器的实现如下：
- en: '[PRE59]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This network follows the architecture depicted in *Figure 3.5*; `SynthesisNetwork`
    is constructed with a set of `AdaIn` and `ConvBlock` custom layers.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络遵循了*图3.5*中描述的架构；`SynthesisNetwork`由一组自定义层 `AdaIn` 和 `ConvBlock` 构成。
- en: Let’s move on to the discriminator network.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看鉴别器网络。
- en: The discriminator in TF
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF 中的鉴别器
- en: 'The `D_basic` function implements the discriminator depicted in *Figure 3.6*.
    ([https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L562](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L562)).
    Since the discriminator consists of a set of convolution layer blocks, `D_basic`
    has a dedicated function, `block`, that builds a block based on the input tensor
    size. The core components of the function look as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `D_basic` 实现了 *图 3.6* 中描述的鉴别器。（[https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L562](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L562)）。由于鉴别器由一组卷积层块组成，`D_basic`
    有一个专用函数 `block`，根据输入张量大小构建块。函数的核心组件如下：
- en: '[PRE60]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the preceding code, the `block` function deals with creating each block in
    the discriminator by combining convolution and downsampling layers. The remaining
    logic of `D_basic` is straightforward, as it simply chains a set of convolution
    layer blocks by passing the output of one block as an input to the next block.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`block` 函数处理通过结合卷积和下采样层创建鉴别器中的每个块。`D_basic` 的剩余逻辑很简单，因为它只是通过将一个块的输出作为下一个块的输入来链式连接一组卷积层块。
- en: Model training logic in TF
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TF 中的模型训练逻辑
- en: The training logic for TF implementation can be found in the `train_step` function.
    Understanding the implementation details should not be challenging as they have
    followed the description we had in the *TF model training* section.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: TF 实现的训练逻辑可以在 `train_step` 函数中找到。理解实现细节不应该是个难题，因为它们遵循了我们在 *TF 模型训练* 部分的描述。
- en: Overall, we have learned how StyleGAN can be implemented in TF version 2 using
    the TF building blocks that we described in this chapter.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们学习了如何在 TF 版本 2 中使用我们在本章中描述的 TF 构建块实现 StyleGAN。
- en: Things to remember
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事情
- en: a. Any DL model training implementation can be broken into three components
    (data loading logic, model definition, and model training logic), regardless of
    the complexity of the implementation.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: a. 无论实现的复杂性如何，任何 DL 模型训练实现都可以分解为三个组件（数据加载逻辑、模型定义和模型训练逻辑）。
- en: At this stage, you should understand how the StyleGAN repository is structured
    in each framework. We strongly recommend that you play around with the pre-trained
    models to generate interesting images. If you master StyleGAN, it should be easy
    to follow the implementation of StyleGAN2 ([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)),
    StyleGAN3 ([https://arxiv.org/abs/2106.12423](https://arxiv.org/abs/2106.12423)),
    and HyperStyle ([https://arxiv.org/abs/2111.15666](https://arxiv.org/abs/2111.15666)).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，您应该了解 StyleGAN 仓库在每个框架中的结构。我们强烈建议您玩弄预训练模型以生成有趣的图像。如果掌握了 StyleGAN，那么跟随
    StyleGAN2 ([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958))、StyleGAN3
    ([https://arxiv.org/abs/2106.12423](https://arxiv.org/abs/2106.12423)) 和 HyperStyle
    ([https://arxiv.org/abs/2111.15666](https://arxiv.org/abs/2111.15666)) 的实现就会很容易。
- en: Summary
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored where the flexibility of DL comes from. DL
    uses a network of mathematical neurons to learn the hidden patterns within a set
    of data. Training a network involves the iterative process of updating model parameters
    based on a train set and selecting the model that performs the best on a validation
    set, with the goal of producing the best performance on a test set.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 DL 的灵活性来自何处。DL 使用数学神经元网络来学习数据集中的隐藏模式。训练网络涉及根据训练集更新模型参数的迭代过程，并选择在验证集上表现最佳的模型，以期在测试集上实现最佳性能。
- en: 'Realizing the repeated processes within model training, many engineers and
    researchers have put together common building blocks into frameworks. We have
    described two of the most popular frameworks: PyTorch and TF. The two frameworks
    are structured in a similar way, allowing users to set up the model training using
    three building blocks: data loading logic, model definition, and model training
    logic. As the final topic of the chapter, we decomposed StyleGAN, one of the most
    popular GAN implementations, to understand how the building blocks are used in
    reality.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练中实现重复过程时，许多工程师和研究人员将常见的构建块整合到框架中。我们描述了两个最流行的框架：PyTorch 和 TF。这两个框架的结构方式相似，允许用户使用三个构建块设置模型训练：数据加载逻辑、模型定义和模型训练逻辑。作为本章的最后一个主题，我们分解了
    StyleGAN，这是最流行的 GAN 实现之一，以了解这些构建块在实际中的使用方式。
- en: As DL requires a large amount of data for successful training, efficient management
    of the data, model implementations, and various training results are critical
    to the success of any project. In the following chapter, we will introduce useful
    tools for DL experiment monitoring.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习需要大量数据进行成功训练，高效管理数据、模型实现以及各种训练结果对于任何项目的成功至关重要。在接下来的章节中，我们将介绍用于深度学习实验监控的有用工具。
