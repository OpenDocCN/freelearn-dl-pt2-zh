- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Transformers to Legal and Financial Documents for AI Text Summarization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored the architecture training, fine-tuning, and usage of several transformer
    ecosystems during the first seven chapters. In *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*, we discovered that OpenAI has begun to experiment
    with zero-shot models that require no fine-tuning, no development, and can be
    implemented in a few lines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The underlying concept of such an evolution relies on how transformers strive
    to teach a machine how to understand a language and express itself in a human-like
    manner. Thus, we have gone from training a model to teaching languages to machines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) designed a transformer meta-model based on a simple
    assertion: every NLP problem can be represented as a text-to-text function. Every
    type of NLP task requires some kind of text context that generates some form of
    text response.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: A text-to-text representation of any NLP task provides a unique framework to
    analyze a transformer’s methodology and practice. The idea is for a transformer
    to learn a language through transfer learning during the training and fine-tuning
    phases with a text-to-text approach.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) named this approach a **T**ext-**T**o-**T**ext **T**ransfer
    **T**ransformer. The 5 Ts became **T5**, and a new model was born.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter by going through the concepts and architecture of
    the T5 transformer model. We will then apply T5 to summarizing documents with
    Hugging Face models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will transpose the text-to-text approach to the show-and-context
    process of GPT-3 engine usage. The mind-blowing, though not perfect, zero-shot
    responses exceed anything a human could imagine.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-text transformer models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of T5 models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T5 methodology
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of transformer models from training to learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face transformer models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a T5 model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a legal text
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a financial text
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limits of transformer models
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 usage
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to explore the text-to-text methodology defined by *Raffel*
    et al. (2019).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Designing a universal text-to-text model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google’s NLP technical revolution started with *Vaswani* et al. (2017), the
    original Transformer, in 2017\. *Attention is All You Need* toppled 30+ years
    of artificial intelligence belief in RNNs and CNNs applied to NLP tasks. It took
    us from the stone age of NLP/NLU to the 21^(st) century in a long-overdue evolution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, summed
    up a second revolution that boiled up and erupted between Google’s *Vaswani* et
    al. (2017) original Transformer and OpenAI’s *Brown* et al. (2020) GPT-3 transformers.
    The original Transformer was focused on performance to prove that attention was
    all we needed for NLP/NLU tasks.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s second revolution, through GPT-3, focused on taking transformer models
    from fine-tuned pretrained models to few-shot trained models that required no
    fine-tuning. The second revolution was to show that a machine can learn a language
    and apply it to downstream tasks as we humans do.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的第二次革命，通过GPT-3，着重将transformer模型从微调的预训练模型提升到无需微调的few-shot训练模型。第二次革命是为了证明机器可以学习一种语言，并将其应用于下游任务，就像我们人类一样。
- en: It is essential to perceive those two revolutions to understand what T5 models
    represent. The first revolution was an attention technique. The second revolution
    was to teach a machine to understand a language (NLU) and then let it solve NLP
    problems as we do.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解T5模型代表什么，理解这两次革命是至关重要的。第一次革命是注意力技术。第二次革命是教会机器理解语言（NLU），然后让它像我们一样解决NLP问题。
- en: In 2019, Google was thinking along the same lines as OpenAI about how transformers
    could be perceived beyond technical considerations and take them to an abstract
    level of natural language understanding.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Google 的想法与OpenAI 的想法一致，即将transformers从技术考虑的层面提升到自然语言理解的抽象水平。
- en: These revolutions became disruptive. It was time to settle down, forget about
    source code and machine resources, and analyze transformers at a higher level.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些革命变得颠覆性。是时候平静下来，忘记源代码和机器资源，以更高的层次分析transformers。
- en: '*Raffel* et al. (2019) designed a conceptual text-to-text model and then implemented
    it.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019）设计了一个概念上的文本到文本模型，然后加以实施。'
- en: 'Let’s go through this representation of the second transformer revolution:
    abstract models.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看第二次转换革命的代表：抽象模型。
- en: The rise of text-to-text transformer models
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到文本transformer模型的崛起
- en: '*Raffel* et al. (2019) set out on a journey as pioneers with one goal: *Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. The
    Google team working on this approach emphasized that it would not modify the original
    Transformer’s fundamental architecture from the start.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019）以一种目标为先驱们开始了一段旅程：*探索统一文本到文本transformer的迁移学习的极限*。该团队强调他们使用的这种方法从一开始就不会修改原始Transformer的基本架构。'
- en: At that point, *Raffel* et al. (2019) wanted to focus on concepts, not techniques.
    Therefore, they showed no interest in producing the latest transformer model as
    we often see a so-called silver bullet transformer model with *n* parameters and
    layers. This time, the T5 team wanted to find out how good transformers could
    be at understanding a language.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在那时，*Raffel*等人（2019）想要关注概念，而不是技术。因此，他们对产生最新的transformer模型没有兴趣，正如我们经常看到的所谓的具有*n*参数和层的灵丹妙药transformer模型。这一次，T5团队想要找出transformers在理解语言方面有多好。
- en: Humans learn a language and then apply that knowledge to a wide range of NLP
    tasks through transfer learning. The core concept of a T5 model is to find an
    abstract model that can do things like us.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人类学习一种语言，然后通过迁移学习将这些知识应用于各种NLP任务。T5模型的核心概念是寻找一个可以像我们一样做事情的抽象模型。
- en: 'When we communicate, we always start with a sequence (A) followed by another
    sequence (B). B, in turn, becomes the start sequence leading to another sequence,
    as shown in *Figure 8.1*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们交流时，我们总是以一个序列（A）开头，然后是另一个序列（B）。B又成为另一个序列的开始，如*图8.1*所示：
- en: '![A picture containing text, clock, screenshot, picture frame  Description
    automatically generated](img/B17948_08_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本、时钟、截图、相框的图片  自动生成的描述](img/B17948_08_01.png)'
- en: 'Figure 8.1: A sequence-to-sequence representation of communication'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：沟通的序列到序列的表示
- en: We also communicate through music with organized sounds. We communicate through
    dancing with organized body movements. We express ourselves through painting with
    coordinated shapes and colors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过音乐与有组织的声音进行交流。我们通过舞蹈与有组织的身体动作进行交流。我们通过绘画以形状和颜色进行表达。
- en: '*We communicate through language with a word or a group of words we call “text.”*
    When we try to understand a text, we pay attention to all of the words in the
    sentence in *all* directions. We try to measure the importance of each term. When
    we do not understand a sentence, we focus on a word and *query* the rest of the
    *keywords* in the sentence to determine their *values* and the *attention* we
    must pay to them. This defines the attention layers of transformers.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Take a few seconds and let this sink in. It seems deceptively simple, right?
    Yet, it took 35+ years to topple the old beliefs surrounding RNNs, CNNs, and the
    thought process accompanying them!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: It is quite fascinating to watch T5 learn, progress, and even help us think
    better sometimes!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The technical revolution of attention layers that simultaneously attend to all
    of the tokens in a sequence led to the T5 conceptual revolution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model can be summed up as a **T**ext-**T**o-**T**ext **T**ransfer **T**ransformer.
    Thus, every NLP task is expressed as a text-to-text problem to solve.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: A prefix instead of task-specific formats
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) still had one problem to solve: unifying task-specific
    formats. The idea was to find a way to have one input format for every task submitted
    to the transformer. That way, the model parameters would be trained for all types
    of tasks with one text-to-text format.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google T5 team came up with a simple solution: adding a prefix to an input
    sequence. We would need thousands of additional vocabularies in many languages
    without the invention of the *prefix* by some long-forgotten genius. For example,
    we would need to find words to describe prepayment, prehistoric, Precambrian,
    and thousands of other words if we did not use “pre” as a prefix.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) proposed to add a *prefix* to an input sequence. A T5
    prefix is not just a tag or indicator like `[CLS]` for classification in some
    transformer models. Instead, a T5 prefix contains the essence of a task a transformer
    needs to solve. A prefix conveys meaning as in the following examples, among others:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '`translate English to German: + [sequence]` for translations, as we did in
    *Chapter 6*, *Machine Translation with the Transformer*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cola sentence: + [sequence]` for *The Corpus of Linguistic Acceptability (CoLA)*,
    as we used in *Chapter 3*, *Fine-Tuning BERT Models*, when we fine-tuned a BERT
    transformer model'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stsb sentence 1:+[sequence]` for semantic textual similarity benchmarks. Natural
    language inferences and entailment are similar problems, as described in *Chapter
    5*, *Downstream NLP Tasks with Transformers*'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summarize + [sequence]` for text summarization problems, as we will solve
    in the *Text summarization with T5* section of this chapter'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve now obtained a unified format for a wide range of NLP tasks, expressed
    in *Figure 8.2*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with low confidence](img/B17948_08_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Unifying the input format of a transformer model'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The unified input format leads to a transformer model that produces a result
    sequence no matter which problem it has to solve in the **T5**. The input and
    output of many NLP tasks have been unified, as shown in *Figure 8.3*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_08_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: The T5 text-to-text framework'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The unification process makes it possible to use the same model, hyperparameters,
    and optimizer for a wide range of tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: We have gone through the standard text-to-text input-output format. Let’s now
    look at the architecture of the T5 transformer model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) focused on designing a standard input format to obtain
    text output. The Google T5 team did not want to try new architectures derived
    from the original Transformer, such as BERT-like encoder-only layers or GPT-like
    decoder-only layers. Instead, the team focused on defining NLP tasks in a standard
    format.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'They chose to use the original Transformer model we defined in *Chapter 2*,
    *Getting Started with the Architecture of the Transformer Model*, as we can see
    in *Figure 8.4*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_08_04.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: The original Transformer model used by T5'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) kept most of the original Transformer architecture and
    terms. However, they emphasized some key aspects. Also, they made some slight
    vocabulary and functional changes. The following list contains some of the main
    aspects of the T5 model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and decoder remain in the model. The encoder and decoder layers
    become “blocks,” and the sublayers become “subcomponents” containing a self-attention
    layer and a feedforward network. The use of the word “blocks” and “subcomponents”
    in a LEGO^®-like language allows you to assemble “blocks,” pieces, and components
    to build your model. Transformer components are standard building blocks you can
    assemble in many ways. You can understand any transformer model once you understand
    the basic building blocks we went through in *Chapter 2*, *Getting Started with
    the Architecture of the Transformer Model*.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention is “order-independent,” meaning it performs operations on sets,
    as we saw in *Chapter 2*. Self-attention uses dot products of matrices, not recurrence.
    It explores the relationship between each word and the others in a sequence. Positional
    encoding is added to the word’s embedding before making the dot products.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original Transformer applied sinusoidal and cosine signals to the Transformer.
    Or it used learned position embeddings. T5 uses relative position embeddings instead
    of adding arbitrary positions to the input. In T5, positional encoding relies
    on an extension of self-attention to make comparisons between pairwise relationships.
    For more, see *Shaw* et al. (2018) in the *References* section of this chapter.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embeddings are shared and re-evaluated through all the layers of
    the model.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have defined the standardization of the input of the T5 transformer model
    through the text-to-text approach.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过文本对文本方法定义了T5变压器模型输入的标准化。
- en: Let’s now use T5 to summarize documents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用T5来总结文档。
- en: Text summarization with T5
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用T5进行文本摘要
- en: NLP summarizing tasks extract succinct parts of a text. This section will start
    by presenting the Hugging Face resources we will use in this chapter. Then we
    will initialize a T5-large transformer model. Finally, we will see how to use
    T5 to summarize any document, including legal and corporate documents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: NLP总结任务提取了文本的简洁部分。本节将首先介绍我们在本章中将要使用的Hugging Face资源。然后我们将初始化一个T5-large变压器模型。最后，我们将看到如何使用T5来总结任何文档，包括法律和公司文件。
- en: Let’s begin by introducing Hugging Face’s framework.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始介绍Hugging Face的框架。
- en: Hugging Face
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Hugging Face designed a framework to implement Transformers at a higher level.
    We used Hugging Face to fine-tune a BERT model in *Chapter 3*, *Fine-Tuning BERT
    Models*, and train a RoBERTa model in *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face设计了一个高级别实现变压器的框架。我们在*第3章*中使用Hugging Face来微调BERT模型，并在*第4章*中训练了一个RoBERTa模型。
- en: To expand our knowledge, we needed to explore other approaches, such as Trax,
    in *Chapter 6*, *Machine Translation with the Transformer*, and OpenAI’s models,
    in *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*. This
    chapter will use Hugging Face’s framework again and explain more about the online
    resources. We end the chapter using the unique potential of a GPT-3 engine.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们的知识，我们需要探索其他方法，例如*第6章*中的Trax和*第7章*中的OpenAI模型。本章将再次使用Hugging Face框架，并更多地解释在线资源。我们以GPT-3引擎的独特潜力结束本章。
- en: 'Hugging Face provides three primary resources within its framework: models,
    datasets, and metrics.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face在其框架内提供了三个主要资源：模型、数据集和指标。
- en: Hugging Face transformer resources
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face变换资源
- en: In this subsection, we will choose the T5 model that we will be implementing
    in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将选择将在本章中实现的T5模型。
- en: 'A wide range of models can be found on the Hugging Face models page, as we
    can see in *Figure 8.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Hugging Face模型页面上找到各种模型，就像我们在*图8.5*中看到的那样：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_08_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序，聊天或文本消息描述](img/B17948_08_05.png)'
- en: 'Figure 8.5: Hugging Face models'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：Hugging Face模型
- en: On this page, [https://huggingface.co/models](https://huggingface.co/models),
    we can search for a model. In our case, we are looking for **t5-large**, a model
    we can smoothly run in Google Colaboratory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上，[https://huggingface.co/models](https://huggingface.co/models)，我们可以搜索一个模型。在我们的情况下，我们正在寻找**t5-large**，一个我们可以在Google
    Colaboratory中顺利运行的模型。
- en: 'We first type `T5` to search for a T5 model and obtain a list of T5 models
    we can choose from:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先键入`T5`来搜索T5模型，并获取我们可以选择的T5模型列表：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_06.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序描述](img/B17948_08_06.png)'
- en: 'Figure 8.6: Searching for a T5 model'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：搜索T5模型
- en: 'We can see that several of the original T5 transformers are available, among
    which are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到原始T5变压器中提供了几种模型，其中包括：
- en: '**base**, which is the baseline model. It was designed to be similar to the
    BERT[BASE] with 12 layers and around 220 million parameters'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础**，是基线模型。它的设计与BERT[BASE]类似，有12层和大约2.2亿个参数'
- en: '**small**, which is a smaller model with 6 layers and 60 million parameters'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**small**，一个更小的模型，有6层和6千万参数'
- en: '**large** is designed to be similar to BERT[LARGE] with 12 layers and 770 million
    parameters'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**large**的设计类似于具有12层和7.7亿参数的BERT[LARGE]'
- en: '**3B** and **11B** use 24-layer encoders and decoders with around 2.8 billion
    and 11 billion parameters'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3B**和**11B**使用24层的编码器和解码器，大约有28亿和110亿个参数'
- en: For more on the description of BERT[BASE] and BERT[LARGE], you can take a few
    minutes now or later to review these models in *Chapter 3*, *Fine-Tuning BERT
    Models*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于BERT[BASE]和BERT[LARGE]的描述，您现在或以后可以花几分钟在*第3章*中查看这些模型。
- en: 'In our case, we select **t5-large**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们选择**t5-large**：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_07.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序描述](img/B17948_08_07.png)'
- en: 'Figure 8.7: How to use a Hugging Face model'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：如何使用Hugging Face模型
- en: '*Figure 8.7* shows how to use the model in the code we will write. We can also
    look into the list of files in the model and the basic configuration file. We
    will look into the configuration file when we initialize the model in the *Initializing
    the T5-large transformer model* section of this chapter.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face also provides datasets and metrics:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets can be used to train and test your models: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The metrics resources can be used to measure the performance of your models:
    [https://huggingface.co/metrics](https://huggingface.co/metrics)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets and metrics are a classical aspect of NLP. In this chapter, we will
    not implement these datasets or metrics. Instead, we will focus on how to implement
    any text to summarize.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by initializing the T5 transformer model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the T5-large transformer model
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will initialize a T5-large model. Open the following
    notebook, `Summarizing_Text_with_T5.ipynb`, which you will find in the directory
    of this chapter on GitHub.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with T5!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with T5
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will install Hugging Face’s framework and then initialize
    a T5 model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first install Hugging Face’s transformers:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: Hugging Face transformers continually evolve, updating libraries and
    modules to adapt to the market. If the default version doesn’t work, you might
    have to pin one with `!pip install transformers==[version that runs with the other
    functions in the notebook]`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'We pinned version `0.1.94` of `sentencepiece` to keep the notebook using Hugging
    Face as stable as possible:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Hugging Face has a GitHub repository that can be cloned. However, Hugging Face’s
    framework provides a range of high-level transformer functions we can implement.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'We can choose to display the architecture of the model or not when we initialize
    the model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we set `display_architecture` to `True`, the structure of the encoder layers,
    decoder layers, and feedforward sublayers will be displayed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now imports `torch` and `json`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Working on transformers means being open to the many transformer architectures
    and frameworks that research labs share with us*. Also, I recommend using PyTorch
    and TensorFlow as much as possible to get used to both environments. What matters
    is the level of abstraction of the transformer model (specific-task models or
    zero-shot models) and its overall performance.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the tokenizer, generation, and configuration classes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will use the `T5-large` model here, but you can select other T5 models in
    the Hugging Face list we went through in this chapter’s *Hugging Face* section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now import the `T5-large` conditional generation model to generate
    text and the T5-large tokenizer:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Initializing a pretrained tokenizer only takes one line. However, nothing proves
    that the tokenized dictionary contains all the vocabulary we need. We will investigate
    the relation between tokenizers and datasets in *Chapter 9*, *Matching Tokenizers
    and Datasets*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 仅需一行即可初始化一个预训练的分词器。但是，并没有证明标记化的词典包含所有我们需要的词汇。我们将在*第9章* *匹配分词器和数据集*中调查分词器和数据集之间的关系。
- en: 'The program now initializes `torch.devic`e with `''cpu.''` A CPU is enough
    for this notebook. The `torch.device` object is the device on which torch tensors
    will be allocated:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在使用`'cpu'`初始化了`torch.device`。这部分笔记足够用了一个CPU。`torch.device`对象是torch张量分配的设备：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are ready to explore the architecture of the T5 model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好探索T5模型的体系结构了。
- en: Exploring the architecture of the T5 model
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索T5模型的体系结构
- en: In this subsection, we will explore the architecture and configuration of a
    T5-large model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将探索T5-large模型的架构和配置。
- en: 'If `display_architecture==true`, we can see the configuration of the model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '如果`display_architecture==true`，我们可以看到模型的配置:'
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For example, we can see the basic parameters of the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到模型的基本参数：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model is a T5 transformer with 16 heads and 24 layers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个带有16个头和24层的T5变压器。
- en: 'We can also see the text-to-text implementation of T5, which adds a *prefix*
    to an input sentence to trigger the task to perform. The *prefix* makes it possible
    to represent a wide range of tasks in a text-to-text format without modifying
    the model’s parameters. In our case, the prefix is `summarization`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到T5的文本到文本实现，它将一个输入句子添加一个*前缀*来触发任务执行。*前缀*使得能够在不修改模型参数的情况下以文本到文本格式表示各种任务。在我们的情况下，前缀是`summarization`：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see that T5:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到T5：
- en: Implements the *beam search* algorithm, which will expand the four most significant
    text completion predictions
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施*波束搜索*算法，它将扩展四个最重要的文本完成预测
- en: Applies early stopping when `num_beam` sentences are completed per batch
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当每批完成`num_beam`句子时，应用早停止
- en: Makes sure not to repeat ngrams equal to `no_repeat_ngram_size`
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保不重复出现 n 克，n 的大小等于 `no_repeat_ngram_size`
- en: Controls the length of the samples with `min_length` and `max_length`
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `min_length` 和 `max_length` 控制样本的长度
- en: Applies a length penalty
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用长度惩罚
- en: 'Another interesting parameter is the vocabulary size:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的参数是词汇量大小：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Vocabulary size is a topic in itself. Too much vocabulary will lead to sparse
    representations. On the other hand, too little vocabulary will distort the NLP
    tasks. We will explore this further in *Chapter 9*, *Matching Tokenizers and Datasets*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量本身就是一个话题。太多的词汇将导致表示稀疏。另一方面，词汇量太少会扭曲NLP任务。我们将在*第9章* *匹配分词器和数据集*中进一步探讨这一点。
- en: 'We can also see the details of the transformer stacks by simply printing the
    `model`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地打印`model`，我们还可以看到变压器堆栈的细节：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For example, we can peek inside a block (`layer`) of the encoder stack (numbered
    from `0` to `23`):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过仅运行您希望运行的单元来查看模型的一个块（`层`）：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that the model runs operations on `1,024` features for the attention
    sublayers and `4,096` for the inner calculations of the feedforward network sublayer
    that will produce outputs of `1,024` features. The symmetrical structure of transformers
    is maintained through all of the layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型在注意子层上运行了`1,024`个特征，在前馈网络子层的内部计算中运行了 `4,096` 个特征，这将产生`1,024`个特征的输出。变压器的对称结构在所有层中都得到了保持。
- en: You can take a few minutes to go through the encoder stacks, the decoder stacks,
    the attention sublayers, and the feedforward sublayers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以花几分钟时间浏览编码器堆栈、解码器堆栈、注意子层和前馈子层。
- en: 'You can also choose to select a specific aspect of the model by only running
    the cells you wish:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以选择仅运行您希望的单元来选择模型的特定方面：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have initialized the T5 transformer. Let’s now summarize documents.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经初始化了T5变压器。现在让我们来总结文件。
- en: Summarizing documents with T5-large
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用T5-large来总结文件
- en: This section will create a summarizing function that you can call with any text
    you wish to summarize. We will summarize legal and financial examples. Finally,
    we will define the limits of the approach.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将创建一个总结函数，您可以用任何您希望总结的文本来调用。我们将总结法律和金融示例。最后，我们将定义方法的限制。
- en: We will first start by creating a summarization function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个总结函数。
- en: Creating a summarization function
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个总结函数
- en: 'First, let’s create a summarizing function named `summarize`. That way, we
    will just send the texts we want to summarize to our function. The function takes
    two parameters. The first parameter is `preprocess_text`, the text to summarize.
    The second parameter is `ml`, the maximum length of the summarized text. Both
    parameters are variables you send to the function each time you call it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hugging Face, among others, provides ready-to-use summarizing functions. However,
    I recommend learning how to build your own functions to customize this critical
    task when necessary.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The context text or ground truth is then stripped of the `\n` characters:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then apply the innovative `T5` task *prefix* `summarize` to the input text:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The T5 model has a unified structure, whatever the task is through the *prefix
    + input sequence* approach. It may seem simple, but it takes NLP transformer models
    closer to universal training and zero-shot downstream tasks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'We can display the processed (stripped) and prepared text (task prefix):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Simple right? Well, it took 35+ years to go from RNNs and CNNs to transformers.
    Then it took some of the brightest research teams in the world to go from transformers
    designed for specific tasks to multi-task models requiring little to no fine-tuning.
    Finally, the Google research team created a standard format for a transformer’s
    input text that contained a prefix that indicates the NLP problem to solve. That
    is quite a feat!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The output displayed contains the preprocessed and prepared text:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see the `summarize` prefix that indicates the task to solve.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The text is now encoded to token IDs and returns them as torch tensors:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The encoded text is ready to be sent to the model to generate a summary with
    the parameters we described in the *Getting started with T5* section:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The number of beams remains the same as in the model we imported. However, `no_repeat_ngram_size`
    has been brought down to `2` instead of `3`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated output is now decoded with the `tokenizer`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We imported, initialized, and defined the summarization function. Let’s now
    experiment with the T5 model with a general topic.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: A general topic sample
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will run a text written by *Project Gutenberg* through
    the T5 model. We will use the sample to run a test on our summarizing function.
    You can copy and paste any other text you wish or load a text by adding code.
    You can also load a dataset of your choice and call the summaries in a loop.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The program’s goal in this chapter is to run a few samples to see how T5 works.
    The input text is the beginning of the *Project Gutenberg* e-book containing the
    *Declaration of Independence of the United States of America*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then call our `summarize` function and send the text we want to summarize
    and the maximum length of the summary:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output shows we sent `534` characters, the original text (ground truth)
    that was preprocessed, and the summary (prediction):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s now use T5 for a more difficult summary.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The Bill of Rights sample
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following sample, taken from the *Bill of Rights*, is more difficult because
    it expresses the special rights of a person:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Remember that transformers are stochastic algorithms, so the output might
    vary each time you run one.* That being said, we can see that T5 did not really
    summarize the input text but simply shortened it:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This sample is significant because it shows the limits that any transformer
    model or other NLP model faces when faced with a text such as this one. We cannot
    just present samples that always work and make users believe that transformers
    have solved all of the NLP challenges we face, no matter how innovative they are.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Maybe we should have provided a longer text to summarize, used other parameters,
    used a larger model, or changed the structure of the T5 model. However, no matter
    how hard you try to summarize a complex text with an NLP model, you will always
    find documents that the model fails to summarize.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: When a model fails on a task, we must be humble and admit it. The SuperGLUE
    human baseline is a difficult one to beat. We need to be patient, work harder,
    and improve transformer models until they can perform better than they do today.
    There is still room for a lot of progress.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2018) chose an appropriate title to describe their approach
    to T5: *Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer*.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Take the necessary time to experiment with examples of your own that you find
    in your legal documents. Explore the limits of transfer learning as a modern-day
    NLP pioneer! Sometimes you will discover exciting results, and sometimes you will
    find areas that need improvement.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try a corporate law sample.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: A corporate law sample
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Corporate law contains many legal subtleties, making summarizing tasks quite
    tricky.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'The input of this sample is an excerpt of the corporate law in the state of
    Montana, USA:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result is satisfying:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, T5 found some of the essential aspects of the text to summarize.
    Take some time to incorporate samples of your own to see what happens. Play with
    the parameters to see if it affects the outcome.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented T5 to summarize texts. It is time to experiment with summarization
    with an OpenAI GPT-3 engine.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Summarization with GPT-3
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was essential to understand the architecture of a T5 transformer. We will
    also see how GPT-3 engines behave on one of the texts. The goal is *not* to benchmark
    companies and models. The goal is for an Industry 4.0 AI Guru to have a broad
    knowledge of NLP.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: First, go to [https://openai.com/](https://openai.com/) and sign up and sign
    in.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Then go to the examples page and select **Summarize for a 2nd grader**:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_08_08.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: GPT-3 examples page'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: A window will open, and we can enter our prompt.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: We submit the text *T* of the corporate sample of the previous section to the
    GPT-3 model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt is *P* = *E* + *T* + *S*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '*E* tells the model to make the explanation simple:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`My second grader asked me what this passage means:`'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The text *T* is the same as in the previous section and is in quotes:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"""The law regarding corporations prescribes that a corporation can be incorporated
    in the state of Montana to serve any lawful purpose. In the state of Montana,
    a corporation has all the powers of a natural person for carrying out its business
    activities.`'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`.../...`'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`The shareholders have the power to change the size of board of directors."""`'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*S* shows the type of task that is expected:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The response produced by the GPT-3 Davinci engine is quite convincing:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`A corporation is a business.`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`A corporation is a person.`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can do everything people can do.`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can be sued.`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can buy, sell, or trade things.`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`A corporation has its own name that is different from the names of the people
    who own it.`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations are formed by people who want to start a business, and they file
    papers with the government to make it official.`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'We can keep *T* and modify *E* and *S*:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '*E* can be changed to:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`My MBA college student asked me what this passage means:`'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*S* can be changed to:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I rephrased it for him, in MBA college language :`'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The GPT-3 engine generated the response of the previous request but added useful
    information for a college student:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can have directors who are in charge of making decisions for
    the business. The directors are elected by the shareholders. Shareholders can
    change the size of the board of directors.`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 models are quite convincing and represent the rising power of Cloud AI.
    We will go deeper into summarizing prompts in *Chapter 16*, *The Emergence of
    Transformer-Driven Copilots*. However, there is more, much more, to explore before
    we do that.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how the T5 transformer models standardized the input
    of the encoder and decoder stacks of the original Transformer. The original Transformer
    architecture has an identical structure for each block (or layer) of the encoder
    and decoder stacks. However, the original Transformer did not have a standardized
    input format for NLP tasks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2018) designed a standard input for a wide range of NLP tasks
    by defining a text-to-text model. They added a prefix to an input sequence, indicating
    the NLP problem type to solve. This led to a standard text-to-text format. The
    **Text-To-Text Transfer Transformer** (**T5**) was born. We saw that this deceivingly
    simple evolution made it possible to use the same model and hyperparameters for
    a wide range of NLP tasks. The invention of T5 takes the standardization process
    of transformer models a step further.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: We then implemented a T5 model that could summarize any text. We tested the
    model on texts that were not part of ready-to-use training datasets. We tested
    the model on constitutional and corporate samples. The results were interesting,
    but we also discovered some of the limits of transformer models, as predicted
    by *Raffel* et al. (2018).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored the tremendous power of a GPT-3 engine’s methodology and
    calculation efficiency. Showing a transformer is a brilliant approach. Having
    one of the most powerful transformer engines in the world helps attain effective,
    though not always perfect, results.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The goal is not to benchmark companies and models but for an Industry 4.0 AI
    Guru to have a deep understanding of transformers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Chapter 9*, *Matching Tokenizers and Datasets*, we will
    explore the limits of tokenizers and define methods to possibly improve NLP tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: T5 models only have encoder stacks like BERT models. (True/False)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models have both encoder and decoder stacks. (True/False)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models use relative positional encoding, not absolute positional encoding.
    (True/False)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-text models are only designed for summarization. (True/False)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-text models apply a prefix to the input sequence that determines the
    NLP task. (True/False)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models require specific hyperparameters for each task. (True/False)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the advantages of text-to-text models is that they use the same hyperparameters
    for all NLP tasks. (True/False)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 transformers do not contain a feedforward network. (True/False)
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hugging Face is a framework that makes transformers easier to implement. (True/False)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI’s transformer engines are game changers. (True/False)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Colin Raffel*, *Noam Shazeer*, *Adam Roberts*, *Katherine Lee*, *Sharan Narang*,
    *Michael Matena*, *Yanqi Zhou*, *Wei Li*, *Peter J. Liu*, 2019, *Exploring the
    Limits of Transfer Learning with a Unified Text-to-Text Transformer*: [https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Peter Shaw*, *Jakob Uszkoreit*, and *Ashish Vaswani*, 2018, *Self-Attention
    with Relative Position Representations*: [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Framework and Resources: [https://huggingface.co/](https://huggingface.co/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'U.S. Legal, *Montana Corporate Laws*: [https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities](https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Declaration of Independence of the United States of America* by Thomas
    Jefferson: [https://www.gutenberg.org/ebooks/1](https://www.gutenberg.org/ebooks/1)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The United States Bill of Rights* by the United States: [https://www.gutenberg.org/ebooks/2](https://www.gutenberg.org/ebooks/2)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
