- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Transformers to Legal and Financial Documents for AI Text Summarization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored the architecture training, fine-tuning, and usage of several transformer
    ecosystems during the first seven chapters. In *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*, we discovered that OpenAI has begun to experiment
    with zero-shot models that require no fine-tuning, no development, and can be
    implemented in a few lines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The underlying concept of such an evolution relies on how transformers strive
    to teach a machine how to understand a language and express itself in a human-like
    manner. Thus, we have gone from training a model to teaching languages to machines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) designed a transformer meta-model based on a simple
    assertion: every NLP problem can be represented as a text-to-text function. Every
    type of NLP task requires some kind of text context that generates some form of
    text response.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: A text-to-text representation of any NLP task provides a unique framework to
    analyze a transformer’s methodology and practice. The idea is for a transformer
    to learn a language through transfer learning during the training and fine-tuning
    phases with a text-to-text approach.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) named this approach a **T**ext-**T**o-**T**ext **T**ransfer
    **T**ransformer. The 5 Ts became **T5**, and a new model was born.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter by going through the concepts and architecture of
    the T5 transformer model. We will then apply T5 to summarizing documents with
    Hugging Face models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will transpose the text-to-text approach to the show-and-context
    process of GPT-3 engine usage. The mind-blowing, though not perfect, zero-shot
    responses exceed anything a human could imagine.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-text transformer models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of T5 models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T5 methodology
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of transformer models from training to learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face transformer models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a T5 model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a legal text
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a financial text
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limits of transformer models
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 usage
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to explore the text-to-text methodology defined by *Raffel*
    et al. (2019).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Designing a universal text-to-text model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google’s NLP technical revolution started with *Vaswani* et al. (2017), the
    original Transformer, in 2017\. *Attention is All You Need* toppled 30+ years
    of artificial intelligence belief in RNNs and CNNs applied to NLP tasks. It took
    us from the stone age of NLP/NLU to the 21^(st) century in a long-overdue evolution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, summed
    up a second revolution that boiled up and erupted between Google’s *Vaswani* et
    al. (2017) original Transformer and OpenAI’s *Brown* et al. (2020) GPT-3 transformers.
    The original Transformer was focused on performance to prove that attention was
    all we needed for NLP/NLU tasks.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s second revolution, through GPT-3, focused on taking transformer models
    from fine-tuned pretrained models to few-shot trained models that required no
    fine-tuning. The second revolution was to show that a machine can learn a language
    and apply it to downstream tasks as we humans do.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的第二次革命，通过GPT-3，着重将transformer模型从微调的预训练模型提升到无需微调的few-shot训练模型。第二次革命是为了证明机器可以学习一种语言，并将其应用于下游任务，就像我们人类一样。
- en: It is essential to perceive those two revolutions to understand what T5 models
    represent. The first revolution was an attention technique. The second revolution
    was to teach a machine to understand a language (NLU) and then let it solve NLP
    problems as we do.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解T5模型代表什么，理解这两次革命是至关重要的。第一次革命是注意力技术。第二次革命是教会机器理解语言（NLU），然后让它像我们一样解决NLP问题。
- en: In 2019, Google was thinking along the same lines as OpenAI about how transformers
    could be perceived beyond technical considerations and take them to an abstract
    level of natural language understanding.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Google 的想法与OpenAI 的想法一致，即将transformers从技术考虑的层面提升到自然语言理解的抽象水平。
- en: These revolutions became disruptive. It was time to settle down, forget about
    source code and machine resources, and analyze transformers at a higher level.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些革命变得颠覆性。是时候平静下来，忘记源代码和机器资源，以更高的层次分析transformers。
- en: '*Raffel* et al. (2019) designed a conceptual text-to-text model and then implemented
    it.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019）设计了一个概念上的文本到文本模型，然后加以实施。'
- en: 'Let’s go through this representation of the second transformer revolution:
    abstract models.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看第二次转换革命的代表：抽象模型。
- en: The rise of text-to-text transformer models
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到文本transformer模型的崛起
- en: '*Raffel* et al. (2019) set out on a journey as pioneers with one goal: *Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. The
    Google team working on this approach emphasized that it would not modify the original
    Transformer’s fundamental architecture from the start.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019）以一种目标为先驱们开始了一段旅程：*探索统一文本到文本transformer的迁移学习的极限*。该团队强调他们使用的这种方法从一开始就不会修改原始Transformer的基本架构。'
- en: At that point, *Raffel* et al. (2019) wanted to focus on concepts, not techniques.
    Therefore, they showed no interest in producing the latest transformer model as
    we often see a so-called silver bullet transformer model with *n* parameters and
    layers. This time, the T5 team wanted to find out how good transformers could
    be at understanding a language.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在那时，*Raffel*等人（2019）想要关注概念，而不是技术。因此，他们对产生最新的transformer模型没有兴趣，正如我们经常看到的所谓的具有*n*参数和层的灵丹妙药transformer模型。这一次，T5团队想要找出transformers在理解语言方面有多好。
- en: Humans learn a language and then apply that knowledge to a wide range of NLP
    tasks through transfer learning. The core concept of a T5 model is to find an
    abstract model that can do things like us.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人类学习一种语言，然后通过迁移学习将这些知识应用于各种NLP任务。T5模型的核心概念是寻找一个可以像我们一样做事情的抽象模型。
- en: 'When we communicate, we always start with a sequence (A) followed by another
    sequence (B). B, in turn, becomes the start sequence leading to another sequence,
    as shown in *Figure 8.1*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们交流时，我们总是以一个序列（A）开头，然后是另一个序列（B）。B又成为另一个序列的开始，如*图8.1*所示：
- en: '![A picture containing text, clock, screenshot, picture frame  Description
    automatically generated](img/B17948_08_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本、时钟、截图、相框的图片  自动生成的描述](img/B17948_08_01.png)'
- en: 'Figure 8.1: A sequence-to-sequence representation of communication'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：沟通的序列到序列的表示
- en: We also communicate through music with organized sounds. We communicate through
    dancing with organized body movements. We express ourselves through painting with
    coordinated shapes and colors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过音乐与有组织的声音进行交流。我们通过舞蹈与有组织的身体动作进行交流。我们通过绘画以形状和颜色进行表达。
- en: '*We communicate through language with a word or a group of words we call “text.”*
    When we try to understand a text, we pay attention to all of the words in the
    sentence in *all* directions. We try to measure the importance of each term. When
    we do not understand a sentence, we focus on a word and *query* the rest of the
    *keywords* in the sentence to determine their *values* and the *attention* we
    must pay to them. This defines the attention layers of transformers.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们通过称为“文本”的一个词或一组词进行语言交流。* 当我们试图理解一段文本时，我们在*所有*方向上都会注意到句子中的所有词。我们尝试衡量每个术语的重要性。当我们不理解一个句子时，我们会专注于一个词并*查询*句子中的其余*关键词*，以确定它们的*值*和我们必须支付的*注意力*。这定义了
    transformer 的注意力层。'
- en: Take a few seconds and let this sink in. It seems deceptively simple, right?
    Yet, it took 35+ years to topple the old beliefs surrounding RNNs, CNNs, and the
    thought process accompanying them!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 花几秒钟，让这个思想深入。看起来似乎非常简单，对吧？然而，推翻围绕 RNN、CNN 及其相应思维过程的旧信念花了 35+ 年的时间！
- en: It is quite fascinating to watch T5 learn, progress, and even help us think
    better sometimes!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 T5 学习、进步，甚至有时帮助我们更好地思考，这非常迷人！
- en: The technical revolution of attention layers that simultaneously attend to all
    of the tokens in a sequence led to the T5 conceptual revolution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同时关注序列中所有标记的注意力层的技术革命导致了 T5 概念的革命。
- en: The T5 model can be summed up as a **T**ext-**T**o-**T**ext **T**ransfer **T**ransformer.
    Thus, every NLP task is expressed as a text-to-text problem to solve.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: T5 模型可以概括为一个**T**ext-**T**o-**T**ext **T**ransfer **T**ransformer。因此，每个 NLP
    任务都被表达为一个文本到文本的问题来解决。
- en: A prefix instead of task-specific formats
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个前缀而不是任务特定格式
- en: '*Raffel* et al. (2019) still had one problem to solve: unifying task-specific
    formats. The idea was to find a way to have one input format for every task submitted
    to the transformer. That way, the model parameters would be trained for all types
    of tasks with one text-to-text format.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel* 等人（2019）仍然有一个问题需要解决：统一任务特定格式。想法是找到一种方法，使每个提交给 transformer 的任务都有一个输入格式。这样，模型参数将用一个文本到文本格式训练所有类型的任务。'
- en: 'The Google T5 team came up with a simple solution: adding a prefix to an input
    sequence. We would need thousands of additional vocabularies in many languages
    without the invention of the *prefix* by some long-forgotten genius. For example,
    we would need to find words to describe prepayment, prehistoric, Precambrian,
    and thousands of other words if we did not use “pre” as a prefix.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌 T5 团队提出了一个简单的解决方案：在输入序列中添加前缀。如果没有一位早已被遗忘的天才发明了*前缀*，我们在许多语言中都需要成千上万个额外的词汇。例如，如果我们不使用“pre”作为前缀，我们就需要找到词来描述预付款、史前的、前寒武纪等成千上万的其他词。
- en: '*Raffel* et al. (2019) proposed to add a *prefix* to an input sequence. A T5
    prefix is not just a tag or indicator like `[CLS]` for classification in some
    transformer models. Instead, a T5 prefix contains the essence of a task a transformer
    needs to solve. A prefix conveys meaning as in the following examples, among others:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel* 等人（2019）提议向输入序列添加一个*前缀*。T5 前缀不仅仅是一些 transformer 模型中用于分类的标记或指示器，比如`[CLS]`。相反，T5
    前缀包含了一个 transformer 需要解决的任务的本质。前缀传达意义，如下面的例子所示，还有其他例子：'
- en: '`translate English to German: + [sequence]` for translations, as we did in
    *Chapter 6*, *Machine Translation with the Transformer*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`translate English to German: + [sequence]` 用于翻译，就像我们在*第 6 章*的*使用 Transformer
    进行机器翻译*中所做的那样'
- en: '`cola sentence: + [sequence]` for *The Corpus of Linguistic Acceptability (CoLA)*,
    as we used in *Chapter 3*, *Fine-Tuning BERT Models*, when we fine-tuned a BERT
    transformer model'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cola sentence: + [sequence]` 用于*语言可接受性语料库（CoLA）*，就像我们在*第 3 章*的*微调 BERT 模型*时使用
    BERT transformer 模型时所做的那样'
- en: '`stsb sentence 1:+[sequence]` for semantic textual similarity benchmarks. Natural
    language inferences and entailment are similar problems, as described in *Chapter
    5*, *Downstream NLP Tasks with Transformers*'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stsb sentence 1:+[sequence]` 用于语义文本相似性基准。自然语言推理和蕴含是相似的问题，如*第 5 章*的*带 Transformer
    的下游 NLP 任务*中所述'
- en: '`summarize + [sequence]` for text summarization problems, as we will solve
    in the *Text summarization with T5* section of this chapter'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summarize + [sequence]` 用于文本摘要问题，就像我们在本章的*使用 T5 进行文本摘要*部分中所做的那样'
- en: 'We’ve now obtained a unified format for a wide range of NLP tasks, expressed
    in *Figure 8.2*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经得到了一个统一的格式，用于广泛的 NLP 任务，在*图 8.2*中表达：
- en: '![Diagram  Description automatically generated with low confidence](img/B17948_08_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated with low confidence](img/B17948_08_02.png)'
- en: 'Figure 8.2: Unifying the input format of a transformer model'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：统一 transformer 模型的输入格式
- en: 'The unified input format leads to a transformer model that produces a result
    sequence no matter which problem it has to solve in the **T5**. The input and
    output of many NLP tasks have been unified, as shown in *Figure 8.3*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的输入格式导致了一个变压器模型，无论要解决什么问题，在**T5**中都会产生一个结果序列。许多自然语言处理任务的输入和输出已经被统一，如*图8.3*所示：
- en: '![Diagram  Description automatically generated](img/B17948_08_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图解说明](img/B17948_08_03.png)'
- en: 'Figure 8.3: The T5 text-to-text framework'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：T5文本到文本框架
- en: The unification process makes it possible to use the same model, hyperparameters,
    and optimizer for a wide range of tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的处理流程使得可以在广泛的任务中使用相同的模型、超参数和优化器。
- en: We have gone through the standard text-to-text input-output format. Let’s now
    look at the architecture of the T5 transformer model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过了标准的文本到文本输入输出格式。现在让我们来看看T5变压器模型的架构。
- en: The T5 model
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5模型
- en: '*Raffel* et al. (2019) focused on designing a standard input format to obtain
    text output. The Google T5 team did not want to try new architectures derived
    from the original Transformer, such as BERT-like encoder-only layers or GPT-like
    decoder-only layers. Instead, the team focused on defining NLP tasks in a standard
    format.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019年）专注于设计一个标准的输入格式以获得文本输出。Google T5团队不希望尝试从原始Transformer派生的新架构，如仅包含编码器层的BERT样式或仅包含解码器层的GPT样式。相反，团队专注于以标准格式定义自然语言处理任务。'
- en: 'They chose to use the original Transformer model we defined in *Chapter 2*,
    *Getting Started with the Architecture of the Transformer Model*, as we can see
    in *Figure 8.4*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 他们选择使用我们在*第2章*中定义的原始Transformer模型，正如我们在*图8.4*中所看到的：
- en: '![Diagram  Description automatically generated](img/B17948_08_04.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图解说明](img/B17948_08_04.png)'
- en: 'Figure 8.4: The original Transformer model used by T5'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：T5使用的原始Transformer模型
- en: '*Raffel* et al. (2019) kept most of the original Transformer architecture and
    terms. However, they emphasized some key aspects. Also, they made some slight
    vocabulary and functional changes. The following list contains some of the main
    aspects of the T5 model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2019年）保留了大部分原始Transformer架构和术语。但是，他们强调了一些关键方面。此外，他们进行了一些轻微的词汇和功能性的更改。以下列表包含T5模型的一些主要方面：'
- en: The encoder and decoder remain in the model. The encoder and decoder layers
    become “blocks,” and the sublayers become “subcomponents” containing a self-attention
    layer and a feedforward network. The use of the word “blocks” and “subcomponents”
    in a LEGO^®-like language allows you to assemble “blocks,” pieces, and components
    to build your model. Transformer components are standard building blocks you can
    assemble in many ways. You can understand any transformer model once you understand
    the basic building blocks we went through in *Chapter 2*, *Getting Started with
    the Architecture of the Transformer Model*.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器保留在模型中。编码器和解码器层变成了“块”，子层变成了包含自注意层和前馈网络的“子组件”。在类似LEGO^®的语言中使用“块”和“子组件”一词，使您可以组装“块”、“片”和组件来构建您的模型。变压器组件是您可以以许多方式组装的标准构建块。一旦您理解了我们在*第2章*中介绍的基本构建块，您就可以理解任何变压器模型的运作方式。
- en: Self-attention is “order-independent,” meaning it performs operations on sets,
    as we saw in *Chapter 2*. Self-attention uses dot products of matrices, not recurrence.
    It explores the relationship between each word and the others in a sequence. Positional
    encoding is added to the word’s embedding before making the dot products.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力是“无序的”，意味着它对集合进行操作，正如我们在*第2章*中看到的那样。自注意力使用矩阵的点积，而不是递归。它探索序列中每个单词与其他单词之间的关系。在进行点积之前，将位置编码添加到单词的嵌入中。
- en: The original Transformer applied sinusoidal and cosine signals to the Transformer.
    Or it used learned position embeddings. T5 uses relative position embeddings instead
    of adding arbitrary positions to the input. In T5, positional encoding relies
    on an extension of self-attention to make comparisons between pairwise relationships.
    For more, see *Shaw* et al. (2018) in the *References* section of this chapter.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始Transformer将正弦和余弦信号应用于变压器。或者它使用了学习到的位置编码。T5使用相对位置编码，而不是将任意位置添加到输入中。在T5中，位置编码依赖于自注意力的扩展，以进行成对关系的比较。更多内容，请参见本章的*参考文献*中的*Shaw*等人（2018年）。
- en: Positional embeddings are shared and re-evaluated through all the layers of
    the model.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码在模型的所有层中共享并重新评估。
- en: We have defined the standardization of the input of the T5 transformer model
    through the text-to-text approach.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过文本对文本方法定义了T5变压器模型输入的标准化。
- en: Let’s now use T5 to summarize documents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用T5来总结文档。
- en: Text summarization with T5
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用T5进行文本摘要
- en: NLP summarizing tasks extract succinct parts of a text. This section will start
    by presenting the Hugging Face resources we will use in this chapter. Then we
    will initialize a T5-large transformer model. Finally, we will see how to use
    T5 to summarize any document, including legal and corporate documents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: NLP总结任务提取了文本的简洁部分。本节将首先介绍我们在本章中将要使用的Hugging Face资源。然后我们将初始化一个T5-large变压器模型。最后，我们将看到如何使用T5来总结任何文档，包括法律和公司文件。
- en: Let’s begin by introducing Hugging Face’s framework.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始介绍Hugging Face的框架。
- en: Hugging Face
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Hugging Face designed a framework to implement Transformers at a higher level.
    We used Hugging Face to fine-tune a BERT model in *Chapter 3*, *Fine-Tuning BERT
    Models*, and train a RoBERTa model in *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face设计了一个高级别实现变压器的框架。我们在*第3章*中使用Hugging Face来微调BERT模型，并在*第4章*中训练了一个RoBERTa模型。
- en: To expand our knowledge, we needed to explore other approaches, such as Trax,
    in *Chapter 6*, *Machine Translation with the Transformer*, and OpenAI’s models,
    in *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*. This
    chapter will use Hugging Face’s framework again and explain more about the online
    resources. We end the chapter using the unique potential of a GPT-3 engine.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们的知识，我们需要探索其他方法，例如*第6章*中的Trax和*第7章*中的OpenAI模型。本章将再次使用Hugging Face框架，并更多地解释在线资源。我们以GPT-3引擎的独特潜力结束本章。
- en: 'Hugging Face provides three primary resources within its framework: models,
    datasets, and metrics.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face在其框架内提供了三个主要资源：模型、数据集和指标。
- en: Hugging Face transformer resources
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face变换资源
- en: In this subsection, we will choose the T5 model that we will be implementing
    in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将选择将在本章中实现的T5模型。
- en: 'A wide range of models can be found on the Hugging Face models page, as we
    can see in *Figure 8.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Hugging Face模型页面上找到各种模型，就像我们在*图8.5*中看到的那样：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_08_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序，聊天或文本消息描述](img/B17948_08_05.png)'
- en: 'Figure 8.5: Hugging Face models'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：Hugging Face模型
- en: On this page, [https://huggingface.co/models](https://huggingface.co/models),
    we can search for a model. In our case, we are looking for **t5-large**, a model
    we can smoothly run in Google Colaboratory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上，[https://huggingface.co/models](https://huggingface.co/models)，我们可以搜索一个模型。在我们的情况下，我们正在寻找**t5-large**，一个我们可以在Google
    Colaboratory中顺利运行的模型。
- en: 'We first type `T5` to search for a T5 model and obtain a list of T5 models
    we can choose from:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先键入`T5`来搜索T5模型，并获取我们可以选择的T5模型列表：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_06.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序描述](img/B17948_08_06.png)'
- en: 'Figure 8.6: Searching for a T5 model'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：搜索T5模型
- en: 'We can see that several of the original T5 transformers are available, among
    which are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到原始T5变压器中提供了几种模型，其中包括：
- en: '**base**, which is the baseline model. It was designed to be similar to the
    BERT[BASE] with 12 layers and around 220 million parameters'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础**，是基线模型。它的设计与BERT[BASE]类似，有12层和大约2.2亿个参数'
- en: '**small**, which is a smaller model with 6 layers and 60 million parameters'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**small**，一个更小的模型，有6层和6千万参数'
- en: '**large** is designed to be similar to BERT[LARGE] with 12 layers and 770 million
    parameters'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**large**的设计类似于具有12层和7.7亿参数的BERT[LARGE]'
- en: '**3B** and **11B** use 24-layer encoders and decoders with around 2.8 billion
    and 11 billion parameters'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3B**和**11B**使用24层的编码器和解码器，大约有28亿和110亿个参数'
- en: For more on the description of BERT[BASE] and BERT[LARGE], you can take a few
    minutes now or later to review these models in *Chapter 3*, *Fine-Tuning BERT
    Models*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于BERT[BASE]和BERT[LARGE]的描述，您现在或以后可以花几分钟在*第3章*中查看这些模型。
- en: 'In our case, we select **t5-large**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们选择**t5-large**：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_07.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面，文本，应用程序描述](img/B17948_08_07.png)'
- en: 'Figure 8.7: How to use a Hugging Face model'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：如何使用Hugging Face模型
- en: '*Figure 8.7* shows how to use the model in the code we will write. We can also
    look into the list of files in the model and the basic configuration file. We
    will look into the configuration file when we initialize the model in the *Initializing
    the T5-large transformer model* section of this chapter.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.7* 展示了我们将要编写的代码中如何使用该模型。我们还可以查看模型中的文件列表和基本配置文件。我们在本章的 *初始化 T5-large transformer
    模型* 部分将查看配置文件。'
- en: 'Hugging Face also provides datasets and metrics:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 还提供了数据集和指标：
- en: 'The datasets can be used to train and test your models: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集可用于训练和测试您的模型：[https://huggingface.co/datasets](https://huggingface.co/datasets)
- en: 'The metrics resources can be used to measure the performance of your models:
    [https://huggingface.co/metrics](https://huggingface.co/metrics)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标资源可用于测量您的模型的性能：[https://huggingface.co/metrics](https://huggingface.co/metrics)
- en: Datasets and metrics are a classical aspect of NLP. In this chapter, we will
    not implement these datasets or metrics. Instead, we will focus on how to implement
    any text to summarize.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和指标是自然语言处理的经典方面。在本章中，我们不会实现这些数据集或指标。相反，我们将专注于如何实现任何文本摘要。
- en: Let’s start by initializing the T5 transformer model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从初始化 T5 transformer 模型开始。
- en: Initializing the T5-large transformer model
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化 T5-large transformer 模型
- en: In this subsection, we will initialize a T5-large model. Open the following
    notebook, `Summarizing_Text_with_T5.ipynb`, which you will find in the directory
    of this chapter on GitHub.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将初始化一个 T5-large 模型。打开以下笔记本，在本章的 GitHub 目录中找到 `Summarizing_Text_with_T5.ipynb`。
- en: Let’s get started with T5!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用 T5！
- en: Getting started with T5
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始使用 T5
- en: In this subsection, we will install Hugging Face’s framework and then initialize
    a T5 model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将安装 Hugging Face 的框架，然后初始化一个 T5 模型。
- en: 'We will first install Hugging Face’s transformers:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装 Hugging Face 的 transformers：
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: Hugging Face transformers continually evolve, updating libraries and
    modules to adapt to the market. If the default version doesn’t work, you might
    have to pin one with `!pip install transformers==[version that runs with the other
    functions in the notebook]`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Hugging Face transformers 不断演进，更新库和模块以适应市场。如果默认版本不起作用，您可能需要使用 `!pip install
    transformers==[与笔记本中其他功能兼容的版本]` 固定一个版本。
- en: 'We pinned version `0.1.94` of `sentencepiece` to keep the notebook using Hugging
    Face as stable as possible:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们固定了 `sentencepiece` 的版本为 `0.1.94`，以尽可能使使用 Hugging Face 的笔记本保持稳定：
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Hugging Face has a GitHub repository that can be cloned. However, Hugging Face’s
    framework provides a range of high-level transformer functions we can implement.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 有一个可以克隆的 GitHub 仓库。但是，Hugging Face 的框架提供了一系列我们可以实现的高级 transformer
    函数。
- en: 'We can choose to display the architecture of the model or not when we initialize
    the model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择在初始化模型时显示模型的架构或不显示：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we set `display_architecture` to `True`, the structure of the encoder layers,
    decoder layers, and feedforward sublayers will be displayed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将 `display_architecture` 设置为 `True`，则将显示编码器层、解码器层和前向子层的结构。
- en: 'The program now imports `torch` and `json`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在导入了 `torch` 和 `json`：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Working on transformers means being open to the many transformer architectures
    and frameworks that research labs share with us*. Also, I recommend using PyTorch
    and TensorFlow as much as possible to get used to both environments. What matters
    is the level of abstraction of the transformer model (specific-task models or
    zero-shot models) and its overall performance.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*从事 transformer 工作意味着要对研究实验室与我们分享的许多 transformer 架构和框架持开放态度*。我也建议尽可能多地使用 PyTorch
    和 TensorFlow 来熟悉两种环境。重要的是 transformer 模型的抽象级别（特定任务模型或零-shot 模型）及其整体性能。'
- en: 'Let’s import the tokenizer, generation, and configuration classes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入 tokenizer、generation 和 configuration 类：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will use the `T5-large` model here, but you can select other T5 models in
    the Hugging Face list we went through in this chapter’s *Hugging Face* section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用 `T5-large` 模型，但您可以在本章的 *Hugging Face* 部分中选择其他 T5 模型。
- en: 'We will now import the `T5-large` conditional generation model to generate
    text and the T5-large tokenizer:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将导入 `T5-large` 有条件生成模型以生成文本和 T5-large tokenizer：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Initializing a pretrained tokenizer only takes one line. However, nothing proves
    that the tokenized dictionary contains all the vocabulary we need. We will investigate
    the relation between tokenizers and datasets in *Chapter 9*, *Matching Tokenizers
    and Datasets*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 仅需一行即可初始化一个预训练的分词器。但是，并没有证明标记化的词典包含所有我们需要的词汇。我们将在*第9章* *匹配分词器和数据集*中调查分词器和数据集之间的关系。
- en: 'The program now initializes `torch.devic`e with `''cpu.''` A CPU is enough
    for this notebook. The `torch.device` object is the device on which torch tensors
    will be allocated:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在使用`'cpu'`初始化了`torch.device`。这部分笔记足够用了一个CPU。`torch.device`对象是torch张量分配的设备：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are ready to explore the architecture of the T5 model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好探索T5模型的体系结构了。
- en: Exploring the architecture of the T5 model
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索T5模型的体系结构
- en: In this subsection, we will explore the architecture and configuration of a
    T5-large model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将探索T5-large模型的架构和配置。
- en: 'If `display_architecture==true`, we can see the configuration of the model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '如果`display_architecture==true`，我们可以看到模型的配置:'
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For example, we can see the basic parameters of the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到模型的基本参数：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model is a T5 transformer with 16 heads and 24 layers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个带有16个头和24层的T5变压器。
- en: 'We can also see the text-to-text implementation of T5, which adds a *prefix*
    to an input sentence to trigger the task to perform. The *prefix* makes it possible
    to represent a wide range of tasks in a text-to-text format without modifying
    the model’s parameters. In our case, the prefix is `summarization`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到T5的文本到文本实现，它将一个输入句子添加一个*前缀*来触发任务执行。*前缀*使得能够在不修改模型参数的情况下以文本到文本格式表示各种任务。在我们的情况下，前缀是`summarization`：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see that T5:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到T5：
- en: Implements the *beam search* algorithm, which will expand the four most significant
    text completion predictions
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施*波束搜索*算法，它将扩展四个最重要的文本完成预测
- en: Applies early stopping when `num_beam` sentences are completed per batch
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当每批完成`num_beam`句子时，应用早停止
- en: Makes sure not to repeat ngrams equal to `no_repeat_ngram_size`
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保不重复出现 n 克，n 的大小等于 `no_repeat_ngram_size`
- en: Controls the length of the samples with `min_length` and `max_length`
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `min_length` 和 `max_length` 控制样本的长度
- en: Applies a length penalty
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用长度惩罚
- en: 'Another interesting parameter is the vocabulary size:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的参数是词汇量大小：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Vocabulary size is a topic in itself. Too much vocabulary will lead to sparse
    representations. On the other hand, too little vocabulary will distort the NLP
    tasks. We will explore this further in *Chapter 9*, *Matching Tokenizers and Datasets*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量本身就是一个话题。太多的词汇将导致表示稀疏。另一方面，词汇量太少会扭曲NLP任务。我们将在*第9章* *匹配分词器和数据集*中进一步探讨这一点。
- en: 'We can also see the details of the transformer stacks by simply printing the
    `model`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地打印`model`，我们还可以看到变压器堆栈的细节：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For example, we can peek inside a block (`layer`) of the encoder stack (numbered
    from `0` to `23`):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过仅运行您希望运行的单元来查看模型的一个块（`层`）：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that the model runs operations on `1,024` features for the attention
    sublayers and `4,096` for the inner calculations of the feedforward network sublayer
    that will produce outputs of `1,024` features. The symmetrical structure of transformers
    is maintained through all of the layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型在注意子层上运行了`1,024`个特征，在前馈网络子层的内部计算中运行了 `4,096` 个特征，这将产生`1,024`个特征的输出。变压器的对称结构在所有层中都得到了保持。
- en: You can take a few minutes to go through the encoder stacks, the decoder stacks,
    the attention sublayers, and the feedforward sublayers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以花几分钟时间浏览编码器堆栈、解码器堆栈、注意子层和前馈子层。
- en: 'You can also choose to select a specific aspect of the model by only running
    the cells you wish:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以选择仅运行您希望的单元来选择模型的特定方面：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have initialized the T5 transformer. Let’s now summarize documents.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经初始化了T5变压器。现在让我们来总结文件。
- en: Summarizing documents with T5-large
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用T5-large来总结文件
- en: This section will create a summarizing function that you can call with any text
    you wish to summarize. We will summarize legal and financial examples. Finally,
    we will define the limits of the approach.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将创建一个总结函数，您可以用任何您希望总结的文本来调用。我们将总结法律和金融示例。最后，我们将定义方法的限制。
- en: We will first start by creating a summarization function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个总结函数。
- en: Creating a summarization function
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个总结函数
- en: 'First, let’s create a summarizing function named `summarize`. That way, we
    will just send the texts we want to summarize to our function. The function takes
    two parameters. The first parameter is `preprocess_text`, the text to summarize.
    The second parameter is `ml`, the maximum length of the summarized text. Both
    parameters are variables you send to the function each time you call it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个名为`summarize`的摘要函数。这样，我们只需将要总结的文本发送到我们的函数即可。该函数接受两个参数。第一个参数是`preprocess_text`，即要总结的文本。第二个参数是`ml`，即摘要的最大长度。这两个参数是您每次调用函数时发送到函数的变量：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hugging Face, among others, provides ready-to-use summarizing functions. However,
    I recommend learning how to build your own functions to customize this critical
    task when necessary.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face等提供了现成的摘要函数。但是，我建议在有必要的时候学习如何构建自己的函数来定制这一关键任务。
- en: 'The context text or ground truth is then stripped of the `\n` characters:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文文本或真实内容，去掉了`\n`字符：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then apply the innovative `T5` task *prefix* `summarize` to the input text:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创新的`T5`任务*前缀*`summarize`应用于输入文本：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The T5 model has a unified structure, whatever the task is through the *prefix
    + input sequence* approach. It may seem simple, but it takes NLP transformer models
    closer to universal training and zero-shot downstream tasks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: T5 模型采用统一的结构，通过*前缀 + 输入序列*的方式来处理任务。这看起来可能很简单，但它将自然语言处理变换模型更接近通用训练和零-shot下游任务。
- en: 'We can display the processed (stripped) and prepared text (task prefix):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示处理（去掉）和准备好的文本（任务前缀）：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Simple right? Well, it took 35+ years to go from RNNs and CNNs to transformers.
    Then it took some of the brightest research teams in the world to go from transformers
    designed for specific tasks to multi-task models requiring little to no fine-tuning.
    Finally, the Google research team created a standard format for a transformer’s
    input text that contained a prefix that indicates the NLP problem to solve. That
    is quite a feat!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 简单吧？嗯，需要35年以上的时间才能从循环神经网络 (RNNs) 和卷积神经网络 (CNNs) 转向变换模型。然后需要世界上一些最优秀的研究团队才能从为特定任务设计的变换模型转向几乎不需要微调的多任务模型。最后，谷歌研究团队创建了一个标准格式，以包含指示需解决的自然语言处理问题的前缀的输入文本。这真是了不起的成就！
- en: 'The output displayed contains the preprocessed and prepared text:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的输出包含预处理并准备好的文本：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see the `summarize` prefix that indicates the task to solve.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`summarize`前缀，表示需要解决的任务。
- en: 'The text is now encoded to token IDs and returns them as torch tensors:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 文本现在已经编码为令牌ID，并将它们作为torch张量返回：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The encoded text is ready to be sent to the model to generate a summary with
    the parameters we described in the *Getting started with T5* section:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 编码文本已经准备好发送到模型中，以使用我们在*开始使用T5*部分描述的参数生成摘要：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The number of beams remains the same as in the model we imported. However, `no_repeat_ngram_size`
    has been brought down to `2` instead of `3`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的横梁数量与我们导入的模型相同。但是`no_repeat_ngram_size`已经从`3`降到了`2`。
- en: 'The generated output is now decoded with the `tokenizer`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出现在使用`tokenizer`进行解码：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We imported, initialized, and defined the summarization function. Let’s now
    experiment with the T5 model with a general topic.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入，初始化并定义了摘要化功能。现在让我们用一个一般性主题来试验T5模型。
- en: A general topic sample
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个一般性主题样本
- en: In this subsection, we will run a text written by *Project Gutenberg* through
    the T5 model. We will use the sample to run a test on our summarizing function.
    You can copy and paste any other text you wish or load a text by adding code.
    You can also load a dataset of your choice and call the summaries in a loop.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将通过T5模型运行由*Project Gutenberg*撰写的文本。我们将使用样本来测试我们的总结功能。您可以复制并粘贴任何其他您希望的文本，也可以通过添加代码加载文本。您还可以加载自己选择的数据集，并在循环中调用摘要。
- en: 'The program’s goal in this chapter is to run a few samples to see how T5 works.
    The input text is the beginning of the *Project Gutenberg* e-book containing the
    *Declaration of Independence of the United States of America*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节的程序目标是运行一些样本来看看T5是如何工作的。输入文本是*Project Gutenberg*电子书中包含*美利坚合众国独立宣言*开头的部分内容：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then call our `summarize` function and send the text we want to summarize
    and the maximum length of the summary:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们调用我们的`summarize`函数，并发送我们想要总结的文本以及摘要的最大长度：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output shows we sent `534` characters, the original text (ground truth)
    that was preprocessed, and the summary (prediction):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们发送了`534`个字符，原始文本（真实内容）经过预处理，还有摘要（预测）：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s now use T5 for a more difficult summary.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用T5来进行更困难的摘要。
- en: The Bill of Rights sample
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《权利法案》样本
- en: 'The following sample, taken from the *Bill of Rights*, is more difficult because
    it expresses the special rights of a person:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个样本取自*权利法案*，更困难，因为它表达的是一个人的特权：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Remember that transformers are stochastic algorithms, so the output might
    vary each time you run one.* That being said, we can see that T5 did not really
    summarize the input text but simply shortened it:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*记住，变压器是随机算法，因此每次运行时输出可能会有所不同。* 话虽如此，我们可以看到T5并没有真正对输入文本进行总结，而只是简单地缩短了它：'
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This sample is significant because it shows the limits that any transformer
    model or other NLP model faces when faced with a text such as this one. We cannot
    just present samples that always work and make users believe that transformers
    have solved all of the NLP challenges we face, no matter how innovative they are.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这份样本意义重大，因为它展示了任何变压器模型或其他NLP模型面对这样的文本时所面临的限制。我们不能只展示总是有效的样本，并让用户相信变压器已经解决了我们所面临的所有NLP挑战，无论它们有多么创新。
- en: Maybe we should have provided a longer text to summarize, used other parameters,
    used a larger model, or changed the structure of the T5 model. However, no matter
    how hard you try to summarize a complex text with an NLP model, you will always
    find documents that the model fails to summarize.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们应该提供更长的文本来总结，使用其他参数，使用更大的模型，或者改变T5模型的结构。然而，无论你如何努力用NLP模型总结复杂的文本，你总会发现模型总结失败的文档。
- en: When a model fails on a task, we must be humble and admit it. The SuperGLUE
    human baseline is a difficult one to beat. We need to be patient, work harder,
    and improve transformer models until they can perform better than they do today.
    There is still room for a lot of progress.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在一个任务上失败时，我们必须谦卑地承认。超级粘性人类基线是一个难以超越的基线。我们需要耐心、更加努力，改进变压器模型，直到它们能比今天表现得更好。还有很多进步空间。
- en: '*Raffel* et al. (2018) chose an appropriate title to describe their approach
    to T5: *Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer*.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*Raffel*等人（2018）选择了一个适合描述T5方法的标题：*探索统一文本到文本变压器的迁移学习极限*。'
- en: Take the necessary time to experiment with examples of your own that you find
    in your legal documents. Explore the limits of transfer learning as a modern-day
    NLP pioneer! Sometimes you will discover exciting results, and sometimes you will
    find areas that need improvement.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间来尝试你在法律文件中发现的自己的示例。将迁移学习的极限作为现代NLP先驱的领域来探索！有时你会发现令人兴奋的结果，有时你会发现需要改进的地方。
- en: Now, let’s try a corporate law sample.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一份公司法样本。
- en: A corporate law sample
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一份公司法样本
- en: Corporate law contains many legal subtleties, making summarizing tasks quite
    tricky.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 公司法包含许多法律细微之处，使得总结任务非常棘手。
- en: 'The input of this sample is an excerpt of the corporate law in the state of
    Montana, USA:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此样本的输入是美国蒙大纳州的公司法节选：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result is satisfying:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人满意：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, T5 found some of the essential aspects of the text to summarize.
    Take some time to incorporate samples of your own to see what happens. Play with
    the parameters to see if it affects the outcome.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，T5找到了文本摘要的一些关键方面。花一些时间加入自己的样本来看看会发生什么。用参数玩耍，看看它是否会影响结果。
- en: We have implemented T5 to summarize texts. It is time to experiment with summarization
    with an OpenAI GPT-3 engine.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实施了T5来进行文本总结。现在是时候通过OpenAI GPT-3引擎进行总结实验了。
- en: Summarization with GPT-3
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GPT-3进行摘要
- en: It was essential to understand the architecture of a T5 transformer. We will
    also see how GPT-3 engines behave on one of the texts. The goal is *not* to benchmark
    companies and models. The goal is for an Industry 4.0 AI Guru to have a broad
    knowledge of NLP.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 了解T5变压器的体系结构非常重要。我们还将看到GPT-3引擎在其中一篇文章上的表现。目标*不是*对公司和模型进行基准测试。目标是让工业4.0人工智能领域的研究专家了解NLP的广泛知识。
- en: First, go to [https://openai.com/](https://openai.com/) and sign up and sign
    in.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，转到[https://openai.com/](https://openai.com/)并注册登录。
- en: 'Then go to the examples page and select **Summarize for a 2nd grader**:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后转到示例页面并选择**为二年级学生总结**：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_08_08.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文字描述的图片 Description générée automatiquement](img/B17948_08_08.png)'
- en: 'Figure 8.8: GPT-3 examples page'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：GPT-3示例页面
- en: A window will open, and we can enter our prompt.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个窗口会弹出，我们可以输入我们的提示。
- en: We submit the text *T* of the corporate sample of the previous section to the
    GPT-3 model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前一节公司样本的文本*T*提交给GPT-3模型。
- en: 'The prompt is *P* = *E* + *T* + *S*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 提示为*P* = *E* + *T* + *S*：
- en: '*E* tells the model to make the explanation simple:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*告诉模型要简化解释：'
- en: '`My second grader asked me what this passage means:`'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我的二年级学生问我这段话是什么意思：`'
- en: 'The text *T* is the same as in the previous section and is in quotes:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本*T*与前一节相同，用引号括起来：
- en: '`"""The law regarding corporations prescribes that a corporation can be incorporated
    in the state of Montana to serve any lawful purpose. In the state of Montana,
    a corporation has all the powers of a natural person for carrying out its business
    activities.`'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"""关于公司的法律规定，公司可以在蒙大拿州成立以服务于任何合法目的。在蒙大拿州，公司具有进行业务活动的自然人的所有权利。`'
- en: '`.../...`'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`.../...`'
- en: '`The shareholders have the power to change the size of board of directors."""`'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`股东有权改变董事会的规模。"""`'
- en: '*S* shows the type of task that is expected:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*显示了预期的任务类型：'
- en: '`I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我用一个二年级学生可以理解的简单语言为他重新表述了："""`'
- en: 'The response produced by the GPT-3 Davinci engine is quite convincing:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-3 DaVinci 引擎生成的响应相当令人信服：
- en: '`A corporation is a business.`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司就是一个企业。`'
- en: '`A corporation is a person.`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司就是一个人。`'
- en: '`Corporations can do everything people can do.`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司可以做人能做的一切。`'
- en: '`Corporations can be sued.`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司可以被起诉。`'
- en: '`Corporations can buy, sell, or trade things.`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司可以购买、出售或交易物品。`'
- en: '`A corporation has its own name that is different from the names of the people
    who own it.`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司有自己的名称，与拥有它的人的名称不同。`'
- en: '`Corporations are formed by people who want to start a business, and they file
    papers with the government to make it official.`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司是由想要创业的人成立的，并向政府提交文件以使其正式化。`'
- en: 'We can keep *T* and modify *E* and *S*:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以保留*T*并修改*E*和*S*：
- en: '*E* can be changed to:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*可以改为：'
- en: '`My MBA college student asked me what this passage means:`'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我的工商管理硕士学生问我这段话是什么意思：`'
- en: '*S* can be changed to:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S*可以改为：'
- en: '`I rephrased it for him, in MBA college language :`'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我用工商管理硕士学院的语言重新表述了这段话：`'
- en: 'The GPT-3 engine generated the response of the previous request but added useful
    information for a college student:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 引擎生成了上一个请求的响应，但为大学生添加了有用的信息：
- en: '`Corporations can have directors who are in charge of making decisions for
    the business. The directors are elected by the shareholders. Shareholders can
    change the size of the board of directors.`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`公司可以有负责业务决策的董事会。董事会由股东选举产生。股东可以改变董事会的规模。`'
- en: GPT-3 models are quite convincing and represent the rising power of Cloud AI.
    We will go deeper into summarizing prompts in *Chapter 16*, *The Emergence of
    Transformer-Driven Copilots*. However, there is more, much more, to explore before
    we do that.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型相当令人信服，并代表了云人工智能的崛起力量。在第16章《变压器驱动的合作伙伴的出现》中，我们将更深入地总结提示。然而，在我们这样做之前，还有更多，更多的探索。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how the T5 transformer models standardized the input
    of the encoder and decoder stacks of the original Transformer. The original Transformer
    architecture has an identical structure for each block (or layer) of the encoder
    and decoder stacks. However, the original Transformer did not have a standardized
    input format for NLP tasks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了 T5 变压器模型如何标准化原始变压器的编码器和解码器堆栈的输入。原始变压器架构对于编码器和解码器堆栈的每个块（或层）具有相同的结构。然而，原始变压器没有为自然语言处理任务制定标准化的输入格式。
- en: '*Raffel* et al. (2018) designed a standard input for a wide range of NLP tasks
    by defining a text-to-text model. They added a prefix to an input sequence, indicating
    the NLP problem type to solve. This led to a standard text-to-text format. The
    **Text-To-Text Transfer Transformer** (**T5**) was born. We saw that this deceivingly
    simple evolution made it possible to use the same model and hyperparameters for
    a wide range of NLP tasks. The invention of T5 takes the standardization process
    of transformer models a step further.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Raffel 等人（2018）通过定义一个文本到文本模型，为广泛的自然语言处理任务设计了一个标准输入。他们为输入序列添加了一个前缀，指示要解决的自然语言处理问题类型。这导致了一个标准的文本到文本格式。**文本到文本传输变压器**（**T5**）应运而生。我们看到这个看似简单的演变使得可以对广泛的自然语言处理任务使用相同的模型和超参数成为可能。T5
    的发明将变压器模型的标准化过程推进了一步。
- en: We then implemented a T5 model that could summarize any text. We tested the
    model on texts that were not part of ready-to-use training datasets. We tested
    the model on constitutional and corporate samples. The results were interesting,
    but we also discovered some of the limits of transformer models, as predicted
    by *Raffel* et al. (2018).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实现了一个可以总结任何文本的 T5 模型。我们测试了该模型在未准备好的训练数据集中不属于的文本上的表现。我们测试了该模型在宪法和公司样本上的表现。结果很有趣，但我们也发现了一些变压器模型的限制，正如
    *Raffel* 等人（2018）预测的那样。
- en: Finally, we explored the tremendous power of a GPT-3 engine’s methodology and
    calculation efficiency. Showing a transformer is a brilliant approach. Having
    one of the most powerful transformer engines in the world helps attain effective,
    though not always perfect, results.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了 GPT-3 引擎方法论和计算效率的巨大力量。展示一个变压器是一个出色的方法。拥有世界上最强大的变压器引擎之一有助于获得有效的结果，尽管不总是完美的。
- en: The goal is not to benchmark companies and models but for an Industry 4.0 AI
    Guru to have a deep understanding of transformers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 目标不是对公司和模型进行基准测试，而是让工业4.0 AI专家对变压器有深入理解。
- en: In the next chapter, *Chapter 9*, *Matching Tokenizers and Datasets*, we will
    explore the limits of tokenizers and define methods to possibly improve NLP tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*第九章*，*匹配标记化器和数据集* 中，我们将探讨标记化器的限制，并定义可能改进自然语言处理任务的方法。
- en: Questions
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: T5 models only have encoder stacks like BERT models. (True/False)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5 模型只有像 BERT 模型那样的编码器堆栈。（是/否）
- en: T5 models have both encoder and decoder stacks. (True/False)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5 模型具有编码器和解码器堆栈。（是/否）
- en: T5 models use relative positional encoding, not absolute positional encoding.
    (True/False)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5 模型使用相对位置编码，而不是绝对位置编码。（是/否）
- en: Text-to-text models are only designed for summarization. (True/False)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本到文本模型仅设计用于摘要。（是/否）
- en: Text-to-text models apply a prefix to the input sequence that determines the
    NLP task. (True/False)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本到文本模型将一个前缀应用到输入序列中，确定自然语言处理任务。（是/否）
- en: T5 models require specific hyperparameters for each task. (True/False)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5 模型需要针对每个任务具体的超参数。（是/否）
- en: One of the advantages of text-to-text models is that they use the same hyperparameters
    for all NLP tasks. (True/False)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本到文本模型的一个优点是，它们对所有自然语言处理任务使用相同的超参数。（是/否）
- en: T5 transformers do not contain a feedforward network. (True/False)
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5 变压器不包含前馈网络。（是/否）
- en: Hugging Face is a framework that makes transformers easier to implement. (True/False)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face 是一个使变压器更易于实现的框架。（是/否）
- en: OpenAI’s transformer engines are game changers. (True/False)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 的变压器引擎改变了游戏规则。（是/否）
- en: References
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Colin Raffel*, *Noam Shazeer*, *Adam Roberts*, *Katherine Lee*, *Sharan Narang*,
    *Michael Matena*, *Yanqi Zhou*, *Wei Li*, *Peter J. Liu*, 2019, *Exploring the
    Limits of Transfer Learning with a Unified Text-to-Text Transformer*: [https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Colin Raffel*，*Noam Shazeer*，*Adam Roberts*，*Katherine Lee*，*Sharan Narang*，*Michael
    Matena*，*Yanqi Zhou*，*Wei Li*，*Peter J. Liu*，2019年，*探索与统一文本到文本转换器的转移学习极限*：[https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ashish Vaswani*，*Noam Shazeer*，*Niki Parmar*，*Jakob Uszkoreit*，*Llion Jones*，*Aidan
    N. Gomez*，*Lukasz Kaiser*，*Illia Polosukhin*，2017年，*注意力就是一切*：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: '*Peter Shaw*, *Jakob Uszkoreit*, and *Ashish Vaswani*, 2018, *Self-Attention
    with Relative Position Representations*: [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Peter Shaw*，*Jakob Uszkoreit* 和 *Ashish Vaswani*，2018年，*自注意力与相对位置表示*：[https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)'
- en: 'Hugging Face Framework and Resources: [https://huggingface.co/](https://huggingface.co/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 框架和资源：[https://huggingface.co/](https://huggingface.co/)
- en: 'U.S. Legal, *Montana Corporate Laws*: [https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities](https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国法律，*蒙大拿州公司法*：[https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities](https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities)
- en: '*The Declaration of Independence of the United States of America* by Thomas
    Jefferson: [https://www.gutenberg.org/ebooks/1](https://www.gutenberg.org/ebooks/1)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*美利坚合众国独立宣言* 由托马斯·杰斐逊：[https://www.gutenberg.org/ebooks/1](https://www.gutenberg.org/ebooks/1)'
- en: '*The United States Bill of Rights* by the United States: [https://www.gutenberg.org/ebooks/2](https://www.gutenberg.org/ebooks/2)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*美国权利法案* 由美国：[https://www.gutenberg.org/ebooks/2](https://www.gutenberg.org/ebooks/2)'
- en: Join our book’s Discord space
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作区，与作者进行每月的 *问我任何事* 交流会：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
