- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Transformers to Legal and Financial Documents for AI Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored the architecture training, fine-tuning, and usage of several transformer
    ecosystems during the first seven chapters. In *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*, we discovered that OpenAI has begun to experiment
    with zero-shot models that require no fine-tuning, no development, and can be
    implemented in a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying concept of such an evolution relies on how transformers strive
    to teach a machine how to understand a language and express itself in a human-like
    manner. Thus, we have gone from training a model to teaching languages to machines.
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) designed a transformer meta-model based on a simple
    assertion: every NLP problem can be represented as a text-to-text function. Every
    type of NLP task requires some kind of text context that generates some form of
    text response.'
  prefs: []
  type: TYPE_NORMAL
- en: A text-to-text representation of any NLP task provides a unique framework to
    analyze a transformer’s methodology and practice. The idea is for a transformer
    to learn a language through transfer learning during the training and fine-tuning
    phases with a text-to-text approach.
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) named this approach a **T**ext-**T**o-**T**ext **T**ransfer
    **T**ransformer. The 5 Ts became **T5**, and a new model was born.'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter by going through the concepts and architecture of
    the T5 transformer model. We will then apply T5 to summarizing documents with
    Hugging Face models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will transpose the text-to-text approach to the show-and-context
    process of GPT-3 engine usage. The mind-blowing, though not perfect, zero-shot
    responses exceed anything a human could imagine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-text transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of T5 models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T5 methodology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of transformer models from training to learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a T5 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a legal text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing a financial text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limits of transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to explore the text-to-text methodology defined by *Raffel*
    et al. (2019).
  prefs: []
  type: TYPE_NORMAL
- en: Designing a universal text-to-text model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google’s NLP technical revolution started with *Vaswani* et al. (2017), the
    original Transformer, in 2017\. *Attention is All You Need* toppled 30+ years
    of artificial intelligence belief in RNNs and CNNs applied to NLP tasks. It took
    us from the stone age of NLP/NLU to the 21^(st) century in a long-overdue evolution.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, summed
    up a second revolution that boiled up and erupted between Google’s *Vaswani* et
    al. (2017) original Transformer and OpenAI’s *Brown* et al. (2020) GPT-3 transformers.
    The original Transformer was focused on performance to prove that attention was
    all we needed for NLP/NLU tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s second revolution, through GPT-3, focused on taking transformer models
    from fine-tuned pretrained models to few-shot trained models that required no
    fine-tuning. The second revolution was to show that a machine can learn a language
    and apply it to downstream tasks as we humans do.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to perceive those two revolutions to understand what T5 models
    represent. The first revolution was an attention technique. The second revolution
    was to teach a machine to understand a language (NLU) and then let it solve NLP
    problems as we do.
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, Google was thinking along the same lines as OpenAI about how transformers
    could be perceived beyond technical considerations and take them to an abstract
    level of natural language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: These revolutions became disruptive. It was time to settle down, forget about
    source code and machine resources, and analyze transformers at a higher level.
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) designed a conceptual text-to-text model and then implemented
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through this representation of the second transformer revolution:
    abstract models.'
  prefs: []
  type: TYPE_NORMAL
- en: The rise of text-to-text transformer models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) set out on a journey as pioneers with one goal: *Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. The
    Google team working on this approach emphasized that it would not modify the original
    Transformer’s fundamental architecture from the start.'
  prefs: []
  type: TYPE_NORMAL
- en: At that point, *Raffel* et al. (2019) wanted to focus on concepts, not techniques.
    Therefore, they showed no interest in producing the latest transformer model as
    we often see a so-called silver bullet transformer model with *n* parameters and
    layers. This time, the T5 team wanted to find out how good transformers could
    be at understanding a language.
  prefs: []
  type: TYPE_NORMAL
- en: Humans learn a language and then apply that knowledge to a wide range of NLP
    tasks through transfer learning. The core concept of a T5 model is to find an
    abstract model that can do things like us.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we communicate, we always start with a sequence (A) followed by another
    sequence (B). B, in turn, becomes the start sequence leading to another sequence,
    as shown in *Figure 8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clock, screenshot, picture frame  Description
    automatically generated](img/B17948_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: A sequence-to-sequence representation of communication'
  prefs: []
  type: TYPE_NORMAL
- en: We also communicate through music with organized sounds. We communicate through
    dancing with organized body movements. We express ourselves through painting with
    coordinated shapes and colors.
  prefs: []
  type: TYPE_NORMAL
- en: '*We communicate through language with a word or a group of words we call “text.”*
    When we try to understand a text, we pay attention to all of the words in the
    sentence in *all* directions. We try to measure the importance of each term. When
    we do not understand a sentence, we focus on a word and *query* the rest of the
    *keywords* in the sentence to determine their *values* and the *attention* we
    must pay to them. This defines the attention layers of transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: Take a few seconds and let this sink in. It seems deceptively simple, right?
    Yet, it took 35+ years to topple the old beliefs surrounding RNNs, CNNs, and the
    thought process accompanying them!
  prefs: []
  type: TYPE_NORMAL
- en: It is quite fascinating to watch T5 learn, progress, and even help us think
    better sometimes!
  prefs: []
  type: TYPE_NORMAL
- en: The technical revolution of attention layers that simultaneously attend to all
    of the tokens in a sequence led to the T5 conceptual revolution.
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model can be summed up as a **T**ext-**T**o-**T**ext **T**ransfer **T**ransformer.
    Thus, every NLP task is expressed as a text-to-text problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: A prefix instead of task-specific formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) still had one problem to solve: unifying task-specific
    formats. The idea was to find a way to have one input format for every task submitted
    to the transformer. That way, the model parameters would be trained for all types
    of tasks with one text-to-text format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google T5 team came up with a simple solution: adding a prefix to an input
    sequence. We would need thousands of additional vocabularies in many languages
    without the invention of the *prefix* by some long-forgotten genius. For example,
    we would need to find words to describe prepayment, prehistoric, Precambrian,
    and thousands of other words if we did not use “pre” as a prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) proposed to add a *prefix* to an input sequence. A T5
    prefix is not just a tag or indicator like `[CLS]` for classification in some
    transformer models. Instead, a T5 prefix contains the essence of a task a transformer
    needs to solve. A prefix conveys meaning as in the following examples, among others:'
  prefs: []
  type: TYPE_NORMAL
- en: '`translate English to German: + [sequence]` for translations, as we did in
    *Chapter 6*, *Machine Translation with the Transformer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cola sentence: + [sequence]` for *The Corpus of Linguistic Acceptability (CoLA)*,
    as we used in *Chapter 3*, *Fine-Tuning BERT Models*, when we fine-tuned a BERT
    transformer model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stsb sentence 1:+[sequence]` for semantic textual similarity benchmarks. Natural
    language inferences and entailment are similar problems, as described in *Chapter
    5*, *Downstream NLP Tasks with Transformers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summarize + [sequence]` for text summarization problems, as we will solve
    in the *Text summarization with T5* section of this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve now obtained a unified format for a wide range of NLP tasks, expressed
    in *Figure 8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with low confidence](img/B17948_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Unifying the input format of a transformer model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unified input format leads to a transformer model that produces a result
    sequence no matter which problem it has to solve in the **T5**. The input and
    output of many NLP tasks have been unified, as shown in *Figure 8.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: The T5 text-to-text framework'
  prefs: []
  type: TYPE_NORMAL
- en: The unification process makes it possible to use the same model, hyperparameters,
    and optimizer for a wide range of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We have gone through the standard text-to-text input-output format. Let’s now
    look at the architecture of the T5 transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) focused on designing a standard input format to obtain
    text output. The Google T5 team did not want to try new architectures derived
    from the original Transformer, such as BERT-like encoder-only layers or GPT-like
    decoder-only layers. Instead, the team focused on defining NLP tasks in a standard
    format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They chose to use the original Transformer model we defined in *Chapter 2*,
    *Getting Started with the Architecture of the Transformer Model*, as we can see
    in *Figure 8.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: The original Transformer model used by T5'
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2019) kept most of the original Transformer architecture and
    terms. However, they emphasized some key aspects. Also, they made some slight
    vocabulary and functional changes. The following list contains some of the main
    aspects of the T5 model:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and decoder remain in the model. The encoder and decoder layers
    become “blocks,” and the sublayers become “subcomponents” containing a self-attention
    layer and a feedforward network. The use of the word “blocks” and “subcomponents”
    in a LEGO^®-like language allows you to assemble “blocks,” pieces, and components
    to build your model. Transformer components are standard building blocks you can
    assemble in many ways. You can understand any transformer model once you understand
    the basic building blocks we went through in *Chapter 2*, *Getting Started with
    the Architecture of the Transformer Model*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention is “order-independent,” meaning it performs operations on sets,
    as we saw in *Chapter 2*. Self-attention uses dot products of matrices, not recurrence.
    It explores the relationship between each word and the others in a sequence. Positional
    encoding is added to the word’s embedding before making the dot products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original Transformer applied sinusoidal and cosine signals to the Transformer.
    Or it used learned position embeddings. T5 uses relative position embeddings instead
    of adding arbitrary positions to the input. In T5, positional encoding relies
    on an extension of self-attention to make comparisons between pairwise relationships.
    For more, see *Shaw* et al. (2018) in the *References* section of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embeddings are shared and re-evaluated through all the layers of
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have defined the standardization of the input of the T5 transformer model
    through the text-to-text approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now use T5 to summarize documents.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization with T5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP summarizing tasks extract succinct parts of a text. This section will start
    by presenting the Hugging Face resources we will use in this chapter. Then we
    will initialize a T5-large transformer model. Finally, we will see how to use
    T5 to summarize any document, including legal and corporate documents.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by introducing Hugging Face’s framework.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face designed a framework to implement Transformers at a higher level.
    We used Hugging Face to fine-tune a BERT model in *Chapter 3*, *Fine-Tuning BERT
    Models*, and train a RoBERTa model in *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*.
  prefs: []
  type: TYPE_NORMAL
- en: To expand our knowledge, we needed to explore other approaches, such as Trax,
    in *Chapter 6*, *Machine Translation with the Transformer*, and OpenAI’s models,
    in *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*. This
    chapter will use Hugging Face’s framework again and explain more about the online
    resources. We end the chapter using the unique potential of a GPT-3 engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face provides three primary resources within its framework: models,
    datasets, and metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face transformer resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will choose the T5 model that we will be implementing
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A wide range of models can be found on the Hugging Face models page, as we
    can see in *Figure 8.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Hugging Face models'
  prefs: []
  type: TYPE_NORMAL
- en: On this page, [https://huggingface.co/models](https://huggingface.co/models),
    we can search for a model. In our case, we are looking for **t5-large**, a model
    we can smoothly run in Google Colaboratory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first type `T5` to search for a T5 model and obtain a list of T5 models
    we can choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Searching for a T5 model'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that several of the original T5 transformers are available, among
    which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**base**, which is the baseline model. It was designed to be similar to the
    BERT[BASE] with 12 layers and around 220 million parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**small**, which is a smaller model with 6 layers and 60 million parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**large** is designed to be similar to BERT[LARGE] with 12 layers and 770 million
    parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3B** and **11B** use 24-layer encoders and decoders with around 2.8 billion
    and 11 billion parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on the description of BERT[BASE] and BERT[LARGE], you can take a few
    minutes now or later to review these models in *Chapter 3*, *Fine-Tuning BERT
    Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we select **t5-large**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: How to use a Hugging Face model'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.7* shows how to use the model in the code we will write. We can also
    look into the list of files in the model and the basic configuration file. We
    will look into the configuration file when we initialize the model in the *Initializing
    the T5-large transformer model* section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face also provides datasets and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets can be used to train and test your models: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The metrics resources can be used to measure the performance of your models:
    [https://huggingface.co/metrics](https://huggingface.co/metrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets and metrics are a classical aspect of NLP. In this chapter, we will
    not implement these datasets or metrics. Instead, we will focus on how to implement
    any text to summarize.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by initializing the T5 transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the T5-large transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will initialize a T5-large model. Open the following
    notebook, `Summarizing_Text_with_T5.ipynb`, which you will find in the directory
    of this chapter on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with T5!
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with T5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will install Hugging Face’s framework and then initialize
    a T5 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first install Hugging Face’s transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: Hugging Face transformers continually evolve, updating libraries and
    modules to adapt to the market. If the default version doesn’t work, you might
    have to pin one with `!pip install transformers==[version that runs with the other
    functions in the notebook]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We pinned version `0.1.94` of `sentencepiece` to keep the notebook using Hugging
    Face as stable as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face has a GitHub repository that can be cloned. However, Hugging Face’s
    framework provides a range of high-level transformer functions we can implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can choose to display the architecture of the model or not when we initialize
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we set `display_architecture` to `True`, the structure of the encoder layers,
    decoder layers, and feedforward sublayers will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now imports `torch` and `json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Working on transformers means being open to the many transformer architectures
    and frameworks that research labs share with us*. Also, I recommend using PyTorch
    and TensorFlow as much as possible to get used to both environments. What matters
    is the level of abstraction of the transformer model (specific-task models or
    zero-shot models) and its overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the tokenizer, generation, and configuration classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `T5-large` model here, but you can select other T5 models in
    the Hugging Face list we went through in this chapter’s *Hugging Face* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now import the `T5-large` conditional generation model to generate
    text and the T5-large tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Initializing a pretrained tokenizer only takes one line. However, nothing proves
    that the tokenized dictionary contains all the vocabulary we need. We will investigate
    the relation between tokenizers and datasets in *Chapter 9*, *Matching Tokenizers
    and Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now initializes `torch.devic`e with `''cpu.''` A CPU is enough
    for this notebook. The `torch.device` object is the device on which torch tensors
    will be allocated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to explore the architecture of the T5 model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the architecture of the T5 model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will explore the architecture and configuration of a
    T5-large model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `display_architecture==true`, we can see the configuration of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, we can see the basic parameters of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The model is a T5 transformer with 16 heads and 24 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the text-to-text implementation of T5, which adds a *prefix*
    to an input sentence to trigger the task to perform. The *prefix* makes it possible
    to represent a wide range of tasks in a text-to-text format without modifying
    the model’s parameters. In our case, the prefix is `summarization`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that T5:'
  prefs: []
  type: TYPE_NORMAL
- en: Implements the *beam search* algorithm, which will expand the four most significant
    text completion predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies early stopping when `num_beam` sentences are completed per batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes sure not to repeat ngrams equal to `no_repeat_ngram_size`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controls the length of the samples with `min_length` and `max_length`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies a length penalty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another interesting parameter is the vocabulary size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Vocabulary size is a topic in itself. Too much vocabulary will lead to sparse
    representations. On the other hand, too little vocabulary will distort the NLP
    tasks. We will explore this further in *Chapter 9*, *Matching Tokenizers and Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the details of the transformer stacks by simply printing the
    `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, we can peek inside a block (`layer`) of the encoder stack (numbered
    from `0` to `23`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the model runs operations on `1,024` features for the attention
    sublayers and `4,096` for the inner calculations of the feedforward network sublayer
    that will produce outputs of `1,024` features. The symmetrical structure of transformers
    is maintained through all of the layers.
  prefs: []
  type: TYPE_NORMAL
- en: You can take a few minutes to go through the encoder stacks, the decoder stacks,
    the attention sublayers, and the feedforward sublayers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also choose to select a specific aspect of the model by only running
    the cells you wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have initialized the T5 transformer. Let’s now summarize documents.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing documents with T5-large
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will create a summarizing function that you can call with any text
    you wish to summarize. We will summarize legal and financial examples. Finally,
    we will define the limits of the approach.
  prefs: []
  type: TYPE_NORMAL
- en: We will first start by creating a summarization function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a summarization function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s create a summarizing function named `summarize`. That way, we
    will just send the texts we want to summarize to our function. The function takes
    two parameters. The first parameter is `preprocess_text`, the text to summarize.
    The second parameter is `ml`, the maximum length of the summarized text. Both
    parameters are variables you send to the function each time you call it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face, among others, provides ready-to-use summarizing functions. However,
    I recommend learning how to build your own functions to customize this critical
    task when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The context text or ground truth is then stripped of the `\n` characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply the innovative `T5` task *prefix* `summarize` to the input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The T5 model has a unified structure, whatever the task is through the *prefix
    + input sequence* approach. It may seem simple, but it takes NLP transformer models
    closer to universal training and zero-shot downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can display the processed (stripped) and prepared text (task prefix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Simple right? Well, it took 35+ years to go from RNNs and CNNs to transformers.
    Then it took some of the brightest research teams in the world to go from transformers
    designed for specific tasks to multi-task models requiring little to no fine-tuning.
    Finally, the Google research team created a standard format for a transformer’s
    input text that contained a prefix that indicates the NLP problem to solve. That
    is quite a feat!
  prefs: []
  type: TYPE_NORMAL
- en: 'The output displayed contains the preprocessed and prepared text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see the `summarize` prefix that indicates the task to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The text is now encoded to token IDs and returns them as torch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoded text is ready to be sent to the model to generate a summary with
    the parameters we described in the *Getting started with T5* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The number of beams remains the same as in the model we imported. However, `no_repeat_ngram_size`
    has been brought down to `2` instead of `3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated output is now decoded with the `tokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We imported, initialized, and defined the summarization function. Let’s now
    experiment with the T5 model with a general topic.
  prefs: []
  type: TYPE_NORMAL
- en: A general topic sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will run a text written by *Project Gutenberg* through
    the T5 model. We will use the sample to run a test on our summarizing function.
    You can copy and paste any other text you wish or load a text by adding code.
    You can also load a dataset of your choice and call the summaries in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program’s goal in this chapter is to run a few samples to see how T5 works.
    The input text is the beginning of the *Project Gutenberg* e-book containing the
    *Declaration of Independence of the United States of America*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call our `summarize` function and send the text we want to summarize
    and the maximum length of the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows we sent `534` characters, the original text (ground truth)
    that was preprocessed, and the summary (prediction):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now use T5 for a more difficult summary.
  prefs: []
  type: TYPE_NORMAL
- en: The Bill of Rights sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following sample, taken from the *Bill of Rights*, is more difficult because
    it expresses the special rights of a person:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Remember that transformers are stochastic algorithms, so the output might
    vary each time you run one.* That being said, we can see that T5 did not really
    summarize the input text but simply shortened it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This sample is significant because it shows the limits that any transformer
    model or other NLP model faces when faced with a text such as this one. We cannot
    just present samples that always work and make users believe that transformers
    have solved all of the NLP challenges we face, no matter how innovative they are.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe we should have provided a longer text to summarize, used other parameters,
    used a larger model, or changed the structure of the T5 model. However, no matter
    how hard you try to summarize a complex text with an NLP model, you will always
    find documents that the model fails to summarize.
  prefs: []
  type: TYPE_NORMAL
- en: When a model fails on a task, we must be humble and admit it. The SuperGLUE
    human baseline is a difficult one to beat. We need to be patient, work harder,
    and improve transformer models until they can perform better than they do today.
    There is still room for a lot of progress.
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2018) chose an appropriate title to describe their approach
    to T5: *Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer*.'
  prefs: []
  type: TYPE_NORMAL
- en: Take the necessary time to experiment with examples of your own that you find
    in your legal documents. Explore the limits of transfer learning as a modern-day
    NLP pioneer! Sometimes you will discover exciting results, and sometimes you will
    find areas that need improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try a corporate law sample.
  prefs: []
  type: TYPE_NORMAL
- en: A corporate law sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Corporate law contains many legal subtleties, making summarizing tasks quite
    tricky.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input of this sample is an excerpt of the corporate law in the state of
    Montana, USA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is satisfying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This time, T5 found some of the essential aspects of the text to summarize.
    Take some time to incorporate samples of your own to see what happens. Play with
    the parameters to see if it affects the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented T5 to summarize texts. It is time to experiment with summarization
    with an OpenAI GPT-3 engine.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization with GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was essential to understand the architecture of a T5 transformer. We will
    also see how GPT-3 engines behave on one of the texts. The goal is *not* to benchmark
    companies and models. The goal is for an Industry 4.0 AI Guru to have a broad
    knowledge of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: First, go to [https://openai.com/](https://openai.com/) and sign up and sign
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then go to the examples page and select **Summarize for a 2nd grader**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: GPT-3 examples page'
  prefs: []
  type: TYPE_NORMAL
- en: A window will open, and we can enter our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: We submit the text *T* of the corporate sample of the previous section to the
    GPT-3 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt is *P* = *E* + *T* + *S*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E* tells the model to make the explanation simple:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`My second grader asked me what this passage means:`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The text *T* is the same as in the previous section and is in quotes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"""The law regarding corporations prescribes that a corporation can be incorporated
    in the state of Montana to serve any lawful purpose. In the state of Montana,
    a corporation has all the powers of a natural person for carrying out its business
    activities.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`.../...`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`The shareholders have the power to change the size of board of directors."""`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*S* shows the type of task that is expected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I rephrased it for him, in plain language a second grader can understand:
    """`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The response produced by the GPT-3 Davinci engine is quite convincing:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`A corporation is a business.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`A corporation is a person.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can do everything people can do.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can be sued.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can buy, sell, or trade things.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`A corporation has its own name that is different from the names of the people
    who own it.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations are formed by people who want to start a business, and they file
    papers with the government to make it official.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can keep *T* and modify *E* and *S*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E* can be changed to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`My MBA college student asked me what this passage means:`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*S* can be changed to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I rephrased it for him, in MBA college language :`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The GPT-3 engine generated the response of the previous request but added useful
    information for a college student:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Corporations can have directors who are in charge of making decisions for
    the business. The directors are elected by the shareholders. Shareholders can
    change the size of the board of directors.`'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 models are quite convincing and represent the rising power of Cloud AI.
    We will go deeper into summarizing prompts in *Chapter 16*, *The Emergence of
    Transformer-Driven Copilots*. However, there is more, much more, to explore before
    we do that.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how the T5 transformer models standardized the input
    of the encoder and decoder stacks of the original Transformer. The original Transformer
    architecture has an identical structure for each block (or layer) of the encoder
    and decoder stacks. However, the original Transformer did not have a standardized
    input format for NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Raffel* et al. (2018) designed a standard input for a wide range of NLP tasks
    by defining a text-to-text model. They added a prefix to an input sequence, indicating
    the NLP problem type to solve. This led to a standard text-to-text format. The
    **Text-To-Text Transfer Transformer** (**T5**) was born. We saw that this deceivingly
    simple evolution made it possible to use the same model and hyperparameters for
    a wide range of NLP tasks. The invention of T5 takes the standardization process
    of transformer models a step further.'
  prefs: []
  type: TYPE_NORMAL
- en: We then implemented a T5 model that could summarize any text. We tested the
    model on texts that were not part of ready-to-use training datasets. We tested
    the model on constitutional and corporate samples. The results were interesting,
    but we also discovered some of the limits of transformer models, as predicted
    by *Raffel* et al. (2018).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored the tremendous power of a GPT-3 engine’s methodology and
    calculation efficiency. Showing a transformer is a brilliant approach. Having
    one of the most powerful transformer engines in the world helps attain effective,
    though not always perfect, results.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is not to benchmark companies and models but for an Industry 4.0 AI
    Guru to have a deep understanding of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Chapter 9*, *Matching Tokenizers and Datasets*, we will
    explore the limits of tokenizers and define methods to possibly improve NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: T5 models only have encoder stacks like BERT models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models have both encoder and decoder stacks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models use relative positional encoding, not absolute positional encoding.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-text models are only designed for summarization. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-text models apply a prefix to the input sequence that determines the
    NLP task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 models require specific hyperparameters for each task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the advantages of text-to-text models is that they use the same hyperparameters
    for all NLP tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T5 transformers do not contain a feedforward network. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hugging Face is a framework that makes transformers easier to implement. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI’s transformer engines are game changers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Colin Raffel*, *Noam Shazeer*, *Adam Roberts*, *Katherine Lee*, *Sharan Narang*,
    *Michael Matena*, *Yanqi Zhou*, *Wei Li*, *Peter J. Liu*, 2019, *Exploring the
    Limits of Transfer Learning with a Unified Text-to-Text Transformer*: [https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Peter Shaw*, *Jakob Uszkoreit*, and *Ashish Vaswani*, 2018, *Self-Attention
    with Relative Position Representations*: [https://arxiv.org/abs/1803.02155](https://arxiv.org/abs/1803.02155)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Framework and Resources: [https://huggingface.co/](https://huggingface.co/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'U.S. Legal, *Montana Corporate Laws*: [https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities](https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Declaration of Independence of the United States of America* by Thomas
    Jefferson: [https://www.gutenberg.org/ebooks/1](https://www.gutenberg.org/ebooks/1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The United States Bill of Rights* by the United States: [https://www.gutenberg.org/ebooks/2](https://www.gutenberg.org/ebooks/2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
