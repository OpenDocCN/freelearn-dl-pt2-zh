["```py\nmaicon@packt:~$ nvidia-smi topo â€“m\n```", "```py\nmaicon@packt:~$ nvidia-smi topo -p -i 0,1Device 0 is connected to device 1 by way of multiple PCIe switches.\n```", "```py\nCUDA_VISIBLE_DEVICES = \"2,3\"\n```", "```py\n    maicon@packt:~$ export CUDA_VISIBLE_DEVICES=\"4,6\"maicon@packt:~$ python training_program.py\n    ```", "```py\n    os.environ['CUDA_VISIBLE_DEVICES'] =\"4,6\"\n    ```", "```py\n    maicon@packt:~$ CUDA_VISIBLE_DEVICES=\"4,6\" python training_program.py\n    ```", "```py\ndist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n```", "```py\ndevice = my_rank\n```", "```py\nTRAINING_SCRIPT=$1NGPU=$2\nTORCHRUN_COMMAND=\"torchrun --nnodes 1 --nproc-per-node $NGPU --master-addr localhost $TRAINING_SCRIPT\"\n$TORCHRUN_COMMAND\n```", "```py\nmaicon@packt:~$ ./launch_multiple_gpu.sh nccl_distributed-efficientnet_cifar10.py 8\n```", "```py\nTRAINING_SCRIPT=$1NGPU=$2\nSIF_IMAGE=$3\nTORCHRUN_COMMAND=\"torchrun --nnodes 1 --nproc-per-node $NGPU --master-addr localhost $TRAINING_SCRIPT\"\napptainer exec --nv $SIF_IMAGE $TORCHRUN_COMMAND\n```"]