["```py\n    %%capture\n    import os\n    if not os.path.exists(\"MCVP2e-CLIP\"):\n      !git clone https://github.com/sizhky/MCVP2e-CLIP.git\n      %pip install -r MCVP2e-CLIP/requirements.txt\n    %cd MCVP2e-CLIP \n    ```", "```py\n    import itertools\n    from torch_snippets import *\n    from clip.core import download_flickr8k_from_kaggle\n    from clip.config import ClipConfig\n    from clip.dataset import CLIPDataset\n    from clip.models import CLIP \n    ```", "```py\n    %%writefile kaggle.json\n    {\"username\":\"XXXX\",\"key\":\"XXXX\"} \n    ```", "```py\n    kaggle_json_path = P(\"kaggle.json\")\n    data_download_path = P(\"/content/flickr-8k-kaggle/\")\n    download_flickr8k_from_kaggle(kaggle_json_path, data_download_path)\n    df = pd.read_csv(data_download_path / \"captions.txt\")\n    df[\"id\"] = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n    df.to_csv(data_download_path / \"captions.csv\") \n    ```", "```py\n    config = ClipConfig()\n    config.image_path = data_download_path / \"Images\"\n    config.captions_csv_path = data_download_path / \"captions.csv\"\n    # Make any other experiment changes, if you want to, below\n    config.debug = False  # Switch to True, in case you want to reduce the dataset size\n    config.epochs = 1\n    config.save_eval_and_logging_steps = 50 \n    ```", "```py\n    class CLIPDataset(Dataset):\n        def __init__(self, df, config, mode):\n            \"\"\"\n            image_filenames and captions must have the same length; so, if there are\n            multiple captions for each image, the image_filenames must have repetitive\n            file names\n            \"\"\"\n            self.config = config\n            self.tokenizer = DistilBertTokenizer.from_pretrained(\n                config.distilbert_text_tokenizer\n            )\n            self.image_filenames = df.image.tolist()\n            self.captions = df.caption.tolist()\n            with notify_waiting(f\"Creating encoded captions for {mode} dataset...\"):\n                self.encoded_captions = self.tokenizer(\n                    self.captions,\n                    padding=True,\n                    truncation=True,\n                    max_length=config.max_length,\n                )\n            self.transforms = get_transforms(config)\n        def __getitem__(self, idx):\n            item = {\n                key: torch.tensor(values[idx])\n                for key, values in self.encoded_captions.items()\n            }\n            image = \\ \n    read(f\"{self.config.image_path}/{self.image_filenames[idx]}\", 1)\n            image = self.transforms(image=image)\n            item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n            item[\"caption\"] = self.captions[idx]\n            return item\n        def __len__(self):\n            return len(self.captions) \n    ```", "```py\n    trn_ds, val_ds = CLIPDataset.train_test_split(config) \n    ```", "```py\n    model = CLIP(config).to(config.device) \n    ```", "```py\n    class ImageEncoder(nn.Module):\n        \"\"\"\n        Encode images to a fixed size vector\n        \"\"\"\n        def __init__(\n            self, model_name=CFG.model_name, \n                  pretrained=CFG.pretrained,  \n                  trainable=CFG.trainable):\n            super().__init__()\n            self.model = timm.create_model(\n                model_name, pretrained, num_classes=0, \n                                       global_pool=\"avg\")\n            for p in self.model.parameters():\n                p.requires_grad = trainable\n        def forward(self, x):\n            return self.model(x) \n    ```", "```py\n    class TextEncoder(nn.Module):\n        def __init__(self, model_name=CFG.text_encoder_model, \n            pretrained=CFG.pretrained, trainable=CFG.trainable):\n            super().__init__()\n            if pretrained:\n                self.model = DistilBertModel.from_pretrained(model_name)\n            else:\n                self.model = DistilBertModel(config=DistilBertConfig())\n\n            for p in self.model.parameters():\n                p.requires_grad = trainable\n            # we are using CLS token hidden representation as   \n            # the sentence's embedding\n            self.target_token_idx = 0\n        def forward(self, input_ids, attention_mask):\n            output = self.model(input_ids=input_ids, \n                                  attention_mask=attention_mask)\n            last_hidden_state = output.last_hidden_state\n            return last_hidden_state[:,self.target_token_idx, :] \n    ```", "```py\n    class ProjectionHead(nn.Module):\n        def __init__(\n            self,\n            embedding_dim,\n            projection_dim=CFG.projection_dim,\n            dropout=CFG.dropout\n        ):\n            super().__init__()\n            self.projection = nn.Linear(embedding_dim, projection_dim)\n            self.gelu = nn.GELU()\n            self.fc = nn.Linear(projection_dim, projection_dim)\n            self.dropout = nn.Dropout(dropout)\n            self.layer_norm = nn.LayerNorm(projection_dim)\n\n        def forward(self, x):\n            projected = self.projection(x)\n            x = self.gelu(projected)\n            x = self.fc(x)\n            x = self.dropout(x)\n            x = x + projected\n            x = self.layer_norm(x)\n            return x \n    ```", "```py\n    class CLIPModel(nn.Module):\n        def __init__(\n            self,\n            temperature=CFG.temperature,\n            image_embedding=CFG.image_embedding,\n            text_embedding=CFG.text_embedding,\n        ):\n            super().__init__()\n            self.image_encoder = ImageEncoder()\n            self.text_encoder = TextEncoder()\n            self.image_projection = \\ \n                  ProjectionHead(embedding_dim=image_embedding)\n            self.text_projection = \\ \n                  ProjectionHead(embedding_dim=text_embedding)\n            self.temperature = temperature\n        def forward(self, batch):\n            # Getting Image and Text Features\n            image_features = self.image_encoder(batch[\"image\"])\n            text_features = self.text_encoder( \\\n                                  input_ids=batch[\"input_ids\"],  \n                        attention_mask=batch[\"attention_mask\"])\n    # Getting Image and Text Embeddings (with same dimension)\n            image_embeddings= self.image_projection(image_features)\n            text_embeddings = self.text_projection(text_features)\n            # Calculating the Loss\n            logits = (text_embeddings @ image_embeddings.T)/self.temperature\n            images_similarity = image_embeddings @ image_embeddings.T\n            texts_similarity = text_embeddings @ text_embeddings.T\n            targets = F.softmax( (images_similarity + texts_similarity) / 2 * \\\n                                               self.temperature, dim=-1)\n            texts_loss = cross_entropy(logits, targets, reduction='none')\n            images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n            loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n            return {\"loss\": loss.mean()} \n    ```", "```py\n    def cross_entropy(preds, targets, reduction='none'):\n        log_softmax = nn.LogSoftmax(dim=-1)\n        loss = (-targets * log_softmax(preds)).sum(1)\n        if reduction == \"none\":\n            return loss\n        elif reduction == \"mean\":\n            return loss.mean() \n    ```", "```py\n    params = [\n        {\"params\": model.image_encoder.parameters(), \"lr\": \n                                       config.image_encoder_lr},\n        {\"params\": model.text_encoder.parameters(), \"lr\": \n                                       config.text_encoder_lr},\n        {\n            \"params\": itertools.chain(\n                model.image_projection.parameters(), \n                model.text_projection.parameters()\n            ),\n            \"lr\": config.head_lr,\n            \"weight_decay\": config.weight_decay,\n        },\n    ]\n    optimizer = torch.optim.AdamW(params, weight_decay=0.0)\n    lr_scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( \\\n                                                    optimizer, mode=\"min\",\n                                                    patience=config.patience,\n                                                    factor=config.factor) \n    ```", "```py\n    from transformers import Trainer, TrainingArguments\n    # Define TrainingArguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",  # Output directory where checkpoints and logs will be saved.\n        num_train_epochs=config.epochs,  # Total number of training epochs.\n        per_device_train_batch_size=config.batch_size,  # Batch size per GPU.\n        per_device_eval_batch_size=config.batch_size,  # Batch size for evaluation\n        evaluation_strategy=\"steps\",  # Evaluation strategy (steps, epoch).\n        logging_strategy=\"steps\",  # Logging strategy (steps, epoch).\n        save_strategy=\"steps\",  # Save strategy (steps, epoch).\n        save_total_limit=2,  # Limit the total amount of checkpoints.\n        learning_rate=5e-5,  # Learning rate.\n        logging_steps=config.save_eval_and_logging_steps,\n        save_steps=config.save_eval_and_logging_steps,# Save checkpoints every N #steps.\n        eval_steps=config.save_eval_and_logging_steps,  # Evaluate every N steps.\n        logging_dir=\"./logs\",  # Directory for storing logs.\n        metric_for_best_model=\"loss\",\n        label_names=[\"image\", \"input_ids\"],\n    )\n    # Create Trainer\n    trainer = Trainer(\n                      model=model,\n                      args=training_args,\n                      train_dataset=trn_ds,\n                      eval_dataset=val_ds,\n                      optimizers=(optimizer, lr_scheduler),\n    )\n    # Train the model\n    trainer.train() \n    ```", "```py\n    def get_image_embeddings(valid_df, model_path):\n        tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n        valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n\n        model = CLIPModel().to(CFG.device)\n        model.load_state_dict(torch.load(model_path, \n                                  map_location=CFG.device))\n        model.eval()\n\n        valid_image_embeddings = []\n        with torch.no_grad():\n            for batch in tqdm(valid_loader):\n                image_features = \\ \n              model.image_encoder(batch[\"image\"].to(CFG.device))\n                image_embeddings = \\ \n                model.image_projection(image_features)\n                valid_image_embeddings.append(image_embeddings)\n        return model, torch.cat(valid_image_embeddings)\n    _, valid_df = make_train_valid_dfs()\n    model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\") \n    ```", "```py\n    def find_matches(model, image_embeddings, query, image_filenames, n=9):\n        tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n        encoded_query = tokenizer([query])\n        batch = {\n            key: torch.tensor(values).to(CFG.device)\n            for key, values in encoded_query.items()\n        }\n        with torch.no_grad():\n            text_features = model.text_encoder(\n                input_ids=batch[\"input_ids\"], \n                attention_mask=batch[\"attention_mask\"]\n            )\n            text_embeddings = model.text_projection(text_features)\n\n        image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n        text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n        dot_similarity = text_embeddings_n@image_embeddings_n.T\n\n        values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n        matches = [image_filenames[idx] for idx in indices[::5]] \n        _, axes = plt.subplots(3, 3, figsize=(10, 10))\n        for match, ax in zip(matches, axes.flatten()):\n            image = cv2.imread(f\"{CFG.image_path}/{match}\")\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            ax.imshow(image)\n            ax.axis(\"off\") \n        plt.show()\n    find_matches(model,\n                 image_embeddings,\n                 query=\"dogs on the grass\",\n                 image_filenames=valid_df['image'].values,\n                 n=9) \n    ```", "```py\n    !pip install ftfy regex tqdm \n    ```", "```py\n    !git clone https://github.com/openai/CLIP.git\n    %cd CLIP \n    ```", "```py\n    import torch\n    import clip\n    from PIL import Image\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, preprocess = clip.load(\"ViT-B/32\", device=device) \n    ```", "```py\n    image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n    text=clip.tokenize([\"a diagram\",\"a dog\",\"a cat\"]).to(device) \n    ```", "```py\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        logits_per_image, logits_per_text = model(image, text)\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n    print(\"Label probs:\", probs)  \n    # prints: [[0.9927937  0.00421068 0.00299572]] \n    ```", "```py\n    !git clone https://github.com/facebookresearch/segment-anything.git\n    %cd segment-anything\n    !pip install -e . \n    ```", "```py\n    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth \n    ```", "```py\n    from segment_anything import SamAutomaticMaskGenerator, \\\n                          sam_model_registry, SamPredictor\n    import cv2\n    sam = sam_model_registry[\"vit_h\"]\\\n               (checkpoint=\"sam_vit_h_4b8939.pth\").to('cuda')\n    mask_generator = SamAutomaticMaskGenerator(sam)\n    predictor = SamPredictor(sam) \n    ```", "```py\n    image = cv2.imread('/content/segment-anything/notebooks/images/truck.jpg')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    predictor.set_image(image) \n    ```", "```py\n    import numpy as np\n    input_point = np.array([[500, 375]])\n    input_label = np.array([1]) \n    ```", "```py\n    masks, scores, logits = predictor.predict(\n                                              point_coords=input_point,\n                                              point_labels=input_label,\n                                              multimask_output=True,\n                                              ) \n    ```", "```py\n    import cv2\n    image = cv2.imread('/content/segment-anything/notebooks/images/truck.jpg')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n    ```", "```py\n    masks = mask_generator.generate(image) \n    ```", "```py\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    import cv2\n    def show_anns(anns):\n        if len(anns) == 0:\n            return\n        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n        img = np.ones((sorted_anns[0]['segmentation'].shape[0], \n                   sorted_anns[0]['segmentation'].shape[1], 4))\n        img[:,:,3] = 0\n        for ann in sorted_anns:\n            m = ann['segmentation']\n            color_mask = np.concatenate([np.random.random(3), [0.35]])\n            img[m] = color_mask\n        ax.imshow(img) \n    ```", "```py\n    plt.figure(figsize=(20,20))\n    plt.imshow(image)\n    show_anns(masks)\n    plt.axis('off')\n    plt.show() \n    ```", "```py\n    !git clone https://github.com/CASIA-IVA-Lab/FastSAM.git \n    ```", "```py\n    !wget https://huggingface.co/spaces/An-619/FastSAM/resolve/main/weights/FastSAM.pt \n    ```", "```py\n    !pip install -r FastSAM/requirements.txt\n    !pip install git+https://github.com/openai/CLIP.git \n    ```", "```py\n    %cd FastSAM \n    ```", "```py\n    import matplotlib.pyplot as plt\n    import cv2\n    from fastsam import FastSAM, FastSAMPrompt \n    ```", "```py\n    model = FastSAM('/content/FastSAM.pt')\n    IMAGE_PATH = '/content/FastSAM/images/cat.jpg'\n    DEVICE = 'cuda' \n    ```", "```py\n    everything_results = model(IMAGE_PATH, device=DEVICE, \n                               retina_masks=True, imgsz=1024,  \n                               conf=0.4, iou=0.9)\n    prompt_process = FastSAMPrompt(IMAGE_PATH, everything_results, \\\n                                                      device=DEVICE)\n    # everything prompt\n    ann = prompt_process.everything_prompt() \n    ```", "```py\n    ann = prompt_process.text_prompt(text='a photo of a cat') \n    ```", "```py\n    prompt_process.plot(annotations=ann,output_path='./output/cat.jpg') \n    ```", "```py\n    %pip install -q diffusers torch-snippets\n    from torch_snippets import *\n    from diffusers import DDPMScheduler, UNet2DModel\n    from torch.utils.data import Subset\n    import torch\n    import torch.nn as nn\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n    device = 'cuda' # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f'Using device: {device}') \n    ```", "```py\n    transform = torchvision.transforms.Compose([\n                                           torchvision.transforms.Resize(32),\n                                           torchvision.transforms.ToTensor()\n                                           ])\n    dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, \\\n                                  download=True, transform=transform) \n    ```", "```py\n    batch_size = 128\n    train_dataloader = DataLoader(dataset, \n                                  batch_size=batch_size, \n                                  shuffle=True) \n    ```", "```py\n    net = UNet2DModel(\n        sample_size=28,  # the target image resolution\n        in_channels=1,  # the number of input channels, 3 for RGB images\n        out_channels=1,  # the number of output channels\n        layers_per_block=1,  # how many ResNet layers to use per UNet block\n        block_out_channels=(32, 64, 128, 256),  # Roughly matching our basic unet example\n        down_block_types=(\n            \"DownBlock2D\", # a regular ResNet downsampling block\n            \"AttnDownBlock2D\", # a ResNet downsampling block with spatial self-attention\n            \"AttnDownBlock2D\",\n            \"AttnDownBlock2D\",\n        ),\n        up_block_types=(\n            \"AttnUpBlock2D\",\n            \"AttnUpBlock2D\",\n            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n            \"UpBlock2D\",   # a regular ResNet upsampling block\n          ),\n    )\n    _ = net.to(device) \n    ```", "```py\n    noise_scheduler = DDPMScheduler(num_train_timesteps=1000) \n    ```", "```py\n    def corrupt(xb, timesteps=None):\n      if timesteps is None:\n        timesteps = torch.randint(0, 999, (len(xb),)).long().to(device)\n      noise = torch.randn_like(xb)\n      noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)\n      return noisy_xb, timesteps \n    ```", "```py\n    n_epochs = 50\n    report = Report(n_epochs)\n    loss_fn = nn.MSELoss()\n    opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n    scheduler = CosineAnnealingLR(opt, T_max=len(train_dataloader), \\\n                                                       verbose=False) \n    ```", "```py\n    for epoch in range(n_epochs):\n        n = len(train_dataloader)\n        for bx, (x, y) in enumerate(train_dataloader):\n            x = x.to(device)  # Data on the GPU\n            noisy_x, timesteps = corrupt(x)  # Create our noisy x\n            pred = net(noisy_x, timesteps).sample\n            loss = loss_fn(pred, x)  # How close is the output to the true 'clean' x?\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            scheduler.step()\n            report.record(epoch + ((bx + 1) / n), loss=loss.item(), end='\\r')\n        report.report_avgs(epoch + 1) \n    ```", "```py\n    net.cpu()\n    noise = torch.randn(5,1,32,32).to(net.device)\n    progress = [noise[:,0]]\n    for ts in np.logspace(np.log10(999), 0.1, 100):\n      ts = torch.Tensor([ts]).long().to(net.device)\n      noise = net(noise, ts).sample.detach().cpu()\n      noise, _ = corrupt(noise, ts)\n      progress.append(noise[:,0])\n    print(len(progress))\n    _n = 10\n    subplots(torch.stack(progress[::_n]).permute(1, 0, 2, 3).reshape(-1, 32, \\\n                                                       32), nc=11, sz=(10,4)) \n    ```", "```py\n    class EmbeddingLayer(nn.Module):\n        def __init__(self, num_embeddings, embedding_dim):\n            super().__init__()\n            self.embedding = nn.Embedding(num_embeddings, \n                                           embedding_dim)\n        def forward(self, labels):\n            return self.embedding(labels)\n    embedding_layer = EmbeddingLayer(num_embeddings=10, embedding_dim=32).to(device) \n    ```", "```py\n    class ConditionalUNet2DModel(UNet2DModel):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.in_channels += 32  # Adjust for embedding dimension \n    ```", "```py\n    net = ConditionalUNet2DModel(\n        sample_size=28,\n        in_channels=1 + 32,  # 1 for original channel, 32 for embedding\n        out_channels=1,\n        layers_per_block=1,\n        block_out_channels=(32, 64, 128, 256),\n        down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \n                        \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n        up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \n                    \"AttnUpBlock2D\", \"UpBlock2D\"),).to(device) \n    ```", "```py\n    def corrupt_with_embedded_labels(xb, labels,timesteps=None):\n        if timesteps is None:\n            timesteps = torch.randint(0, 999, \n                                  (len(xb),)).long().to(device)\n        noise = torch.randn_like(xb)\n        noisy_xb = noise_scheduler.add_noise(xb, noise, \n                                                     timesteps)\n        labels_embedded=embedding_layer(labels).unsqueeze(1).unsqueeze(-1)\n        labels_embedded = labels_embedded.expand(-1, -1, \n                                      xb.shape[2], xb.shape[3])\n        return torch.cat([noisy_xb, labels_embedded], dim=1), timesteps \n    ```", "```py\n    xb = torch.zeros(10, 1, 32, 32)\n    timesteps = torch.randint(999, 1000, (len(xb),)).long().to(device) \n    ```", "```py\n    noise = torch.randn_like(xb)\n    noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps).to(device) \n    ```", "```py\n    labels = torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device) \n    ```", "```py\n    labels_embedded=embedding_layer(labels).unsqueeze(-1).unsqueeze(-1)\n    labels_embedded = labels_embedded.expand(-1, -1, \n                            xb.shape[2], xb.shape[3]).to(device) \n    ```", "```py\n    noisy_x = torch.cat([noisy_xb, labels_embedded], dim=1) \n    ```", "```py\n    pred = net(noisy_x, timesteps).sample.permute(0,2,3,1).reshape(-1, 32, 32) \n    ```", "```py\n    subplots(pred.detach().cpu().numpy()) \n    ```", "```py\nfrom diffusers import StableDiffusionPipeline\nfrom torchinfo import summary\nmodel_id = \"stabilityai/stable-diffusion-2\"\npipe = StableDiffusionPipeline.from_pretrained(model_id)\nsummary(pipe.unet, depth=4) \n```", "```py\nSummary(pipe.unet.down_blocks[0], depth=2) \n```", "```py\n    !huggingface-cli login \n    ```", "```py\n    !pip install -q accelerate diffusers \n    ```", "```py\n    from torch import autocast\n    from diffusers import DiffusionPipeline \n    ```", "```py\n    # Setting a seed would ensure reproducibility of the experiment.\n    generator = torch.Generator(device=\"cuda\").manual_seed(42) \n    ```", "```py\n    # Define the Stable Diffusion pipeline\n    pipeline = DiffusionPipeline.from_pretrained(\n                                             \"CompVis/stable-diffusion-v1-4\",\n                                             torch_dtype=torch.float16,\n                                             )\n    # Set the device for the pipeline\n    pipeline = pipeline.to(\"cuda\") \n    ```", "```py\n    prompt = \"baby in superman dress\"\n    image = pipeline(prompt, generator=generator) \n    ```", "```py\n    image.images[0] \n    ```", "```py\n    # Define the Stable Diffusion XL pipeline\n    pipeline = DiffusionPipeline.from_pretrained(\n                                  \"stabilityai/stable-diffusion-xl-base-1.0\",\n                                  torch_dtype=torch.float16,\n                                  )\n    # Set the device for the pipeline\n    pipeline = pipeline.to(\"cuda\") \n    ```", "```py\n    prompt = \"baby in superman dress\"\n    image = pipe(prompt, generator=generator) \n    ```", "```py\n    prompt = \"baby in superman dress, photorealistic, cinematic\" \n    ```"]