- en: '*Chapter 11*: Attention Visualization and Experiment Tracking'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：注意力可视化和实验跟踪'
- en: In this chapter, we will cover two different technical concepts, **attention
    visualization** and **experiment tracking**, and we will practice them through
    sophisticated tools such as **exBERT** and **BertViz**. These tools provide important
    functions for interpretability and explainability. First, we will discuss how
    to visualize the inner parts of attention by utilizing the tools. It is important
    to interpret the learned representations and to understand the information encoded
    by self-attention heads in the Transformer. We will see that certain heads correspond
    to a certain aspect of syntax or semantics. Secondly, we will learn how to track
    experiments by logging and then monitoring by using **TensorBoard** and **Weights
    & Biases** (**W&B**). These tools enable us to efficiently host and track experimental
    results such as loss or other metrics, which helps us to optimize model training.
    You will learn how to use exBERT and BertViz to see the inner parts of their own
    models and will be able to utilize both TensorBoard and W&B to monitor and optimize
    their models by the end of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖两个不同的技术概念，**注意力可视化** 和 **实验跟踪**，并通过诸如 **exBERT** 和 **BertViz** 等复杂工具来实践它们。这些工具提供了重要的可解释性和可解释性功能。首先，我们将讨论如何利用这些工具来可视化注意力的内部部分。解释所学的表示是很重要的，也要理解
    Transformer 中自注意力头所编码的信息。我们将看到某些头部对应于语法或语义的某个方面。其次，我们将学习如何通过记录和使用 **TensorBoard**
    和 **Weights & Biases**（**W&B**）来监视实验。这些工具使我们能够高效地托管和跟踪实验结果，例如损失或其他指标，这有助于我们优化模型训练。通过本章的学习，您将学会如何使用
    exBERT 和 BertViz 查看自己模型的内部部分，并最终能够利用 TensorBoard 和 W&B 监视和优化模型。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Interpreting attention heads
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释注意力头
- en: Tracking model metrics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪模型指标
- en: Technical requirements
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for this chapter is found at [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11),
    which is the GitHub repository for this book. We will be using Jupyter Notebook
    to run our coding exercises that require Python 3.6.0 or above, and the following
    packages will need to be installed:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在 [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11)
    找到，这是本书的 GitHub 代码库。我们将使用 Jupyter Notebook 运行需要 Python 3.6.0 或更高版本的编码练习，并且需要安装以下包：
- en: '`tensorflow`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`'
- en: '`pytorch`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`Transformers >=4.00`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transformers >=4.00`'
- en: '`tensorboard`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorboard`'
- en: '`wandb`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wandb`'
- en: '`bertviz`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bertviz`'
- en: '`ipywidgets`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipywidgets`'
- en: 'Check out the following link to see Code in Action Video:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看代码演示视频：
- en: '[https://bit.ly/3iM4Y1F](https://bit.ly/3iM4Y1F)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3iM4Y1F](https://bit.ly/3iM4Y1F)'
- en: Interpreting attention heads
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释注意力头
- en: As with most **Deep Learning** (**DL**) architectures, both the success of the
    Transformer models and how they learn have been not fully understood, but we know
    that the Transformers—remarkably—learn many linguistic features of the language.
    A significant amount of learned linguistic knowledge is distributed both in the
    hidden state and in the self-attention heads of the pre-trained model. There have
    been substantial recent studies published and many tools developed to understand
    and to better explain the phenomena.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数**深度学习**（**DL**）架构一样，Transformer 模型的成功以及它们如何学习尚未完全理解，但我们知道，Transformer ——
    令人惊讶地 —— 学到了许多语言特征。大量的学到的语言知识分布在预训练模型的隐藏状态和自注意头中。已经发表了大量的最新研究，并开发了许多工具，以理解和更好地解释这些现象。
- en: Thanks to some **Natural Language Processing** (**NLP**) community tools, we
    are able to interpret the information learned by the self-attention heads in a
    Transformer model. The heads can be interpreted naturally, thanks to the weights
    between tokens. We will soon see that in further experiments in this section,
    certain heads correspond to a certain aspect of syntax or semantics. We can also
    observe surface-level patterns and many other linguistic features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢一些**自然语言处理**（**NLP**）社区工具，我们能够解释 Transformer 模型中自注意力头所学到的信息。由于令牌之间的权重，头部可以被自然地解释。我们很快会在本节的进一步实验中看到，某些头对应于语法或语义的某个方面。我们还可以观察到表面级模式和许多其他语言特征。
- en: 'In this section, we will conduct some experiments using community tools to
    observe these patterns and features in the attention heads. Recent studies have
    already revealed many of the features of self-attention. Let''s highlight some
    of them before we get into the experiments. For example, most of the heads attend
    to delimiter tokens such as **Separator** (**SEP**) and **Classification** (**CLS**),
    since these tokens are never masked out and bear segment-level information in
    particular. Another observation is that most heads pay little attention to the
    current token, but some heads specialize in only attending the next or previous
    tokens, especially in earlier layers. Here is a list of other patterns found in
    recent studies that we can easily observe in our experiments:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用社区工具进行一些实验来观察这些模式和注意头部的特征。最近的研究已经揭示了自注意的许多特征。在我们进行实验之前，让我们先介绍一些。例如，大多数头部倾向于关注分隔符标记，如**分隔符**（**SEP**）和**分类**（**CLS**），因为这些标记永远不被屏蔽，并且特别携带段级信息。另一个观察是，大多数头部很少关注当前标记，但一些头部专门关注下一个或上一个标记，特别是在较早的层中。以下是最近研究中发现的其他模式列表，我们可以在我们的实验中轻松观察到：
- en: Attention heads in the same layer show similar behavior.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一层中的注意头表现出类似的行为。
- en: Particular heads correspond to specific aspects of syntax or semantic relations.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定头对应于语法或语义关系的特定方面。
- en: Some heads encode so that the direct objects tend to attend to their verbs,
    such as *<lesson, take>* or *<car, drive>*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些头部对直接对象倾向于关注它们的动词，例如*<lesson, take>*或*<car, drive>*。
- en: In some heads, the noun modifiers attend to their noun (for example, *the hot
    water*; *the next layer*), or the possessive pronoun attends to the head (for
    example, *her car*).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些头中，名词修饰语会关注它们的名词（例如*the hot water*；*the next layer*），或所有格代词会关注头部（例如*her car*）。
- en: Some heads encode so that passive auxiliary verbs attend to a related verb,
    such as *Been damaged, was taken*.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些头部对被动助动词进行编码，例如*Been damaged, was taken*。
- en: In some heads, coreferent mentions attend to themselves, such as *talks-negotiation,
    she-her, President-Biden*.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些头部中，指代提及关注自己，例如*talks-negotiation, she-her, President-Biden*。
- en: The lower layers usually have information about word positions.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低的层通常包含有关词位置的信息。
- en: Syntactic features are observed earlier in the transformer, while high-level
    semantic information appears in the upper layers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法特征在transformer中较早出现，而高级语义信息出现在更上层。
- en: The final layers are the most task-specific and are therefore very effective
    for downstream tasks.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终层是最具任务特异性的，并因此对下游任务非常有效。
- en: To observe these patterns, we can use two important tools, **exBERT** and **BertViz**,
    here. These tools have almost the same functionality. We will start with exBERT.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察这些模式，我们可以在这里使用两个重要工具**exBERT**和**BertViz**。这些工具功能几乎相同。我们将从exBERT开始。
- en: Visualizing attention heads with exBERT
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用exBERT可视化注意力头部
- en: exBERT is a visualization tool to see the inner parts of Transformers. We will
    use it to visualize the attention heads of the *BERT-base-cased* model, which
    is the default model in the exBERT interface. Unless otherwise stated, the model
    we will use in the following examples is *BERT-base-cased*. This contains 12 layers
    and 12 self-attention heads in each layer, which makes for 144 self-attention
    heads.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: exBERT是一个可视化工具，用于查看Transformers内部部分。我们将使用它来可视化*BERT-base-cased*模型的自注意头部，这是exBERT界面的默认模型。除非另有说明，我们将在以下示例中使用*BERT-base-cased*模型。这包含12层，每层12个自注意头部，共计144个自注意头部。
- en: 'We will learn how to utilize exBERT step by step, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步学习如何使用exBERT，如下所示：
- en: 'Let''s click on the exBERT link hosted by *Hugging Face*: [https://huggingface.co/exbert](https://huggingface.co/exbert).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们点击由*Hugging Face*托管的exBERT链接：[https://huggingface.co/exbert](https://huggingface.co/exbert)。
- en: Enter the sentence **The cat is very sad.** and see the output, as follows:![Figure
    11.1 – exBERT interface ](img/B17123_11_001.jpg)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入句子**The cat is very sad.**并查看输出，如下所示：![图11.1 – exBERT界面 ](img/B17123_11_001.jpg)
- en: Figure 11.1 – exBERT interface
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.1 – exBERT界面
- en: In the preceding screenshot, the left tokens attend to the right tokens. The
    thickness of the lines represents the value of the weights. Because CLS and SEP
    tokens have very frequent and dense connections, we cut off the links associated
    with them for simplicity. Please see the **Hide Special Tokens** toggle switch.
    What we see now is the attention mapping at layer 1, where the lines correspond
    to the sum of the weights on all heads. This is called a **multi-head attention
    mechanism**, in which 12 heads work in parallel to each other. This mechanism
    allows us to capture a wider range of relationships than is possible with single-head
    attention. This is why we see a broadly attending pattern in *Figure 11.1*. We
    can also observe any specific head by clicking the **Head** column.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上面的屏幕截图中，左侧的标记关注右侧的标记。线条的厚度代表权重的值。由于CLS和SEP标记具有非常频繁和密集的连接，我们简化起见切断了与它们相关的链接。请参阅**隐藏特殊标记**切换按钮。现在我们看到的是第1层的关注映射，其中线条对应于所有头部上权重的总和。这被称为**多头注意力机制**，其中12个头部并行工作。这个机制可以让我们捕捉比单头注意力更广泛的关系。这就是为什么我们在*图11.1*中看到一个广泛的关注模式。我们还可以通过点击**头部**列观察任何特定的头部。
- en: 'If you hover over a token at the left, you will see the specific weights of
    that token connecting to the right ones. For more detailed information on using
    the interface, read the paper *exBERT: A Visual Analysis Tool to Explore Learned
    Representations in Transformer Models*, *Benjamin Hoover*, *Hendrik Strobelt*,
    *Sebastian Gehrmann*, *2019* or watch the video at the following link: [https://exbert.net/](https://exbert.net/).'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您在左侧悬停在一个标记上，您将看到将它连接到右侧的具体权重。有关使用界面的更详细信息，请阅读文章*exBERT：一个用于探索变压器模型中学到表示的可视分析工具*，作者*本杰明·胡佛*，*亨德里克·施特罗贝尔特*，*塞巴斯蒂安·格赫尔曼*，*2019年*，或观看以下链接的视频：[https://exbert.net/](https://exbert.net/)。
- en: Now, we will try to support the findings of other researchers addressed in the
    introductory part of this section. Let's take the *some heads specialize in only
    attending the next or previous tokens, especially in earlier layers* pattern,
    and see if there's a head that supports this.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将尝试支持本节介绍部分中其他研究人员的发现。让我们以*某些头部专门关注下一个或前一个标记，尤其是在较早的层*模式为例，看是否有一个头部支持这一点。
- en: We will use **<Layer-No, Head-No>** notation to denote a certain self-attention
    head for the rest of the chapter, where the indices start at 1 for exBERT and
    start at 0 for BertViz—for example, *<3,7>* denotes the seventh head at the third
    layer for exBERT. When you select the **<2,5> (or <4,12> or <6,2 >)** head, you
    will get the following output, where each token attends to the previous token
    only:![Figure 11.2 – Previous-token attention pattern ](img/B17123_11_002.jpg)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章的其余部分中，我们将使用**<层编号，头部编号>**符号来表示特定的自注意头部，在这里，指数从exBERT的1开始，从BertViz的0开始。例如，*<3,7>*表示exBERT第三层的第七个头部。当您选择**<2,5>（或<4,12>或<6,2>）**头部时，您将得到以下输出，其中每个标记只关注前一个标记：![图11.2
    – 上一个标记关注模式](img/B17123_11_002.jpg)
- en: Figure 11.2 – Previous-token attention pattern
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.2 – 上一个标记关注模式
- en: For the **<2, 12>** and **<3, 4>** heads, you will get a pattern whereby each
    token attends to the next token, as follows:![Figure 11.3 – Next-token attention
    pattern ](img/B17123_11_003.jpg)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**<2, 12>**和**<3, 4>**头部，您将得到一个模式，其中每个标记都与下一个标记关联，如下所示：![图11.3 – 下一个标记关注模式](img/B17123_11_003.jpg)
- en: Figure 11.3 – Next-token attention pattern
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.3 – 下一个标记关注模式
- en: These heads serve the same functionality for other input sentences—that is,
    they work independently of the input. You can try different sentences yourself.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些头部对其他输入句子具有相同的功能，即它们独立于输入工作。您可以自己尝试不同的句子。
- en: We can use an attention head for advanced semantic tasks such as pronoun resolution
    using a **probing classifier**. First, we will qualitatively check if the internal
    representation has such a capacity for pronoun resolution (or coreference resolution)
    or not. Pronoun resolution is considered a challenging semantic relation task
    since the distance between the pronoun and its antecedent is usually very long.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用注意力头部进行先进的语义任务，例如使用**探测分类器**进行代词消解。首先，我们将从定性上检查内部表示是否具有代词消解（或核指代消解）的能力。代词消解被认为是一项具有挑战性的语义关系任务，因为代词与其先行词之间的距离通常非常大。
- en: Now, we take the sentence *The cat is very sad. Because it could not find food
    to eat.* When you check each head, you will notice that the *<9,9>* and *<9,12>*
    heads encode the pronoun relation. When hovering over **it** at the *<9,9>* head,
    we get the following output:![Figure 11.4 – The coreference pattern at the <9,9>
    head ](img/B17123_11_004.jpg)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们拿句子*The cat is very sad. Because it could not find food to eat.* 作为例子。当你检查每个头部时，你会注意到*<9,9>*和*<9,12>*头部编码了代词关系。当你悬停在*<9,9>*头上的**it**时，我们会得到以下输出：![图
    11.4 – <9,9>头部的指代模式](img/B17123_11_004.jpg)
- en: Figure 11.4 – The coreference pattern at the <9,9> head
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.4 – <9,9>头部的指代模式
- en: 'The *<9,12>* head also works for pronoun relation. Again, on hovering over
    **it**, we get the following output:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*<9,12>*头也用于代词关系。同样，在悬停在**it**上时，我们得到以下输出：'
- en: '![Figure 11.5 – The coreference pattern at the <9,12> head ](img/B17123_11_005.jpg)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 11.5 – <9,12>头部的指代模式](img/B17123_11_005.jpg)'
- en: Figure 11.5 – The coreference pattern at the <9,12> head
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.5 – <9,12>头部的指代模式
- en: 'From the preceding screenshot, we see that the **it** pronoun strongly attends
    to its antecedent, **cat**. We change the sentence a bit so that the **it** pronoun
    now refers to the **food** token instead of **cat**, as in **the cat did not eat
    the food because it was not fresh**. As seen in the following screenshot, which
    relates to the *<9,9>* head, **it** properly attends to its antecedent **food**,
    as expected:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从上述截图中，我们看到**it**代词强烈关注它的先行词**cat**。我们稍微改变一下句子，让**it**代词现在指代**food**标记而不是**cat**，就像**the
    cat did not eat the food because it was not fresh**中一样。如下截图所示，与*<9,9>*头相关的截图中，**it**正确地关注了它的先行词**food**，如预期所示：
- en: '![Figure 11.6 – The pattern at the <9,9> head for the second example ](img/B17123_11_006.jpg)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 11.6 – 第二个例子的<9,9>头部的模式](img/B17123_11_006.jpg)'
- en: Figure 11.6 – The pattern at the <9,9> head for the second example
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.6 – 第二个例子的<9,9>头部的模式
- en: Let's take another run, where the pronoun refers to **cat**, as in **The cat
    did not eat the food because it was very angry**. In the *<9,9>* head, the **it**
    token mostly attends to the **cat** token, as shown in the following screenshot:![Figure
    11.7 – The pattern at the <9,9> head for the second input ](img/B17123_11_007.jpg)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再做一次运行，这次代词指代的是**cat**，就像**The cat did not eat the food because it was very
    angry**中一样。在*<9,9>*头中，**it** 标记大部分关注**cat**标记，如下截图所示：![图 11.7 – 第二个输入的<9,9>头部的模式](img/B17123_11_007.jpg)
- en: Figure 11.7 – The pattern at the <9,9> head for the second input
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.7 – 第二个输入的<9,9>头部的模式
- en: 'I think those are enough examples. Now, we will use the exBERT model differently
    to evaluate the model capacity. Let''s restart the exBERT interface, select the
    last layer (*layer 12*), and keep all heads. Then, enter the sentence **the cat
    did not eat the food.** and mask out the **food** token. Double-clicking masks
    the **food** token out, as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我觉得这些例子已经足够了。现在，我们将以不同的方式使用 exBERT 模型来评估模型的容量。让我们重新启动 exBERT 界面，选择最后一层（*第 12
    层*），并保留所有头部。然后，输入句子**the cat did not eat the food.**并将**food**标记掩码。双击掩码掩码了**food**标记，如下所示：
- en: '![Figure 11.8 – Evaluating the model by masking ](img/B17123_11_008.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 通过掩码评估模型](img/B17123_11_008.jpg)'
- en: Figure 11.8 – Evaluating the model by masking
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 通过掩码评估模型
- en: When you hover on that masked token, you can see the prediction distribution
    of the *Bert-base-cased* model, as shown in the preceding screenshot. The first
    prediction is **food**, which is expected. For more detailed information about
    the tool, you can use exBERT's web page, at [https://exbert.net/](https://exbert.net/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你悬停在那个被掩码的标记上时，你可以看到*Bert-base-cased*模型的预测分布，如上述截图所示。第一个预测是**food**，这是预期的。有关工具的更详细信息，你可以使用
    exBERT 的网页，位于[https://exbert.net/](https://exbert.net/)。
- en: Well done! In the next section, we will work with BertViz and write some Python
    code to access the attention heads.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！在下一节中，我们将使用 BertViz 并编写一些 Python 代码来访问注意力头。
- en: Multiscale visualization of attention heads with BertViz
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BertViz 进行多尺度注意力头的可视化
- en: 'Now, we will write some code to visualize heads with BertViz, which is a tool
    to visualize attention in the Transformer model, as is exBERT. It was developed
    by Jesse Vig in 2019 (*A Multiscale Visualization of Attention in the Transformer
    Model*, *Jesse Vig*, *2019*). It is the extension of the work of the Tensor2Tensor
    visualization tool (Jones, 2017). We can monitor the inner parts of a model with
    multiscale qualitative analysis. The advantage of BertViz is that we can work
    with most Hugging Face-hosted models (such as **Bidirectional Encoder Representations
    from Transformers** (**BERT**), **Generated Pre-trained Transformer** (**GPT**),
    and **Cross-lingual Language Model** (**XLM**)) through the Python **Application
    Programming Interface** (**API**). Therefore, we will be able to work with non-English
    models as well, or any pre-trained model. We will examine such examples together
    shortly. You can access BertViz resources and other information from the following
    GitHub link: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将编写一些代码来使用 BertViz 可视化头，这是一个可视化 Transformer 模型中注意力的工具，就像 exBERT 一样。它是由
    Jesse Vig 在 2019 年开发的（*Transformer 模型中注意力的多尺度可视化*，*Jesse Vig*，*2019*）。它是 Tensor2Tensor
    可视化工具（Jones, 2017）工作的延伸。我们可以通过 Python **应用程序编程接口**（**API**）监视模型的内部部分进行多尺度定性分析。BertViz
    的优点在于，我们可以通过 Python **应用程序编程接口**（**API**）使用大多数 Hugging Face 托管的模型（如**双向编码器转换器表示**（**BERT**）、**生成的预训练转换器**（**GPT**）和**跨语言语言模型**（**XLM**））。因此，我们也将能够使用非英语模型或任何预训练模型。我们将很快一起检查这样的示例。您可以从以下
    GitHub 链接访问 BertViz 资源和其他信息：[https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)。
- en: 'As with exBERT, BertViz visualizes attention heads in a single interface. Additionally,
    it supports a **bird''s eye view** and a low-level **neuron view**, where we observe
    how individual neurons interact to build attention weights. A useful demonstration
    video can be found at the following link: [https://vimeo.com/340841955](https://vimeo.com/340841955).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与 exBERT 类似，BertViz 在一个界面中可视化注意力头。此外，它支持**鸟瞰视角**和低级别的**神经元视角**，我们可以观察个别神经元如何相互作用以构建注意力权重。一个有用的演示视频可在以下链接找到：[https://vimeo.com/340841955](https://vimeo.com/340841955)。
- en: 'Before starting, we need to install the necessary libraries, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要安装必要的库，如下所示：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then import the following modules:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们导入以下模块：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'BertViz supports three views: a **head view**, a **model view**, and a **neuron
    view**. Let''s examine these views one by one. First of all, though, it is important
    to point out that we started from 1 to index layers and heads in exBERT. But in
    BertViz, we start from 0 for indexing, as in Python programming. If I say a *<9,9>*
    head in exBERT, its BertViz counterpart is *<8,8>.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BertViz 支持三种视图：**头视图**、**模型视图**和**神经元视图**。让我们逐一检查这些视图。不过，首先需要指出的是，在 exBERT 中，我们从
    1 开始索引层和头。但在 BertViz 中，我们从 0 开始索引，与 Python 编程相同。如果我说 exBERT 中的一个 *<9,9>* 头，它的
    BertViz 对应物是 *<8,8>*。
- en: Let's start with the head view.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头视图开始。
- en: Attention head view
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力头视图
- en: 'The head view is the BertViz equivalent of what we have experienced so far
    with exBERT in the previous section. The **attention head view** visualizes the
    attention patterns based on one or more attention heads in a selected layer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 头视图是 BertViz 中我们在前一节中使用 exBERT 已经体验过的等价物。**注意力头视图**可视化基于一个或多个注意力头的注意力模式在所选层中：
- en: 'First, we define a `get_bert_attentions()` function to retrieve attentions
    and tokens for a given model and a given pair of sentences. The function definition
    is shown in the following code block:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义一个`get_bert_attentions()`函数，用于检索给定模型和给定句对的注意力和标记。函数定义如下代码块所示：
- en: '[PRE2]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the following code snippet, we load the `bert-base-cased` model and retrieve
    the tokens and corresponding attentions of the given two sentences. We then call
    the `head_view()` function at the end to visualize the attentions. Here is the
    code execution:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们加载了 `bert-base-cased` 模型，并检索了给定两个句子的标记和相应的注意力。然后我们在最后调用了 `head_view()`
    函数来可视化注意力。以下是代码执行：
- en: '[PRE3]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The code output is an interface, as displayed here:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码输出是一个界面，如下所示：
- en: '![Figure 11.9 – Head-view output of BertViz ](img/B17123_11_009.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 11.9 - BertViz 的头视图输出](img/B17123_11_009.jpg)'
- en: Figure 11.9 – Head-view output of BertViz
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.9 - BertViz 的头视图输出
- en: The interface on the left of *Figure 11.9* comes first. Hovering over any token
    on the left will show the attention going from that token. The colored tiles at
    the top correspond to the attention head. Double-clicking on any of them will
    select it and discard the rest. The thicker attention lines denote higher attention
    weights.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图11.9* 左侧是首先出现的界面。在任何左侧的令牌上悬停将显示从该令牌出发的注意力。顶部的彩色块对应于注意力头。双击其中的任何一个将选择它并丢弃其余部分。较粗的注意力线表示更高的注意力权重。'
- en: Please remember that in the preceding exBERT examples, we observed that the
    *<9,9>* head (the equivalent in BertViz is *<8, 8>*, due to indexing) bears a
    pronoun-antecedent relationship. We observe the same pattern in *Figure 11.9*,
    selecting layer 8 and head 8\. Then, we see the interface on the right of *Figure
    11.9* when we hover on *it*, where *it* strongly attends to the *cat* and *it*
    tokens. So, can we observe these semantic patterns in other pre-trained language
    models? Although the heads are not exactly the same in other models, some heads
    can encode these semantic properties. We also know from recent work that semantic
    features are mostly encoded in the higher layers.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，在前面的exBERT示例中，我们观察到 *<9,9>* 头（由于索引的原因，在BertViz中等效于 *<8, 8>*）具有代词-先行关系。我们观察到相同的模式在
    *图11.9* 中，选择第8层和第8头。然后，当我们悬停在 *它* 上时，在 *图11.9* 的右侧看到了界面，*它* 强烈关注 *cat* 和 *it*
    令牌。那么，在其他预训练语言模型中我们能观察到这些语义模式吗？虽然其它模型中头部不一定完全相同，但一些头部可能会编码这些语义特性。我们也从最近的工作中得知，语义特征主要编码在较高的层中。
- en: 'Let''s look for a coreference pattern in a Turkish language model. The following
    code loads a Turkish `bert-base` model and takes a sentence pair. We observe here
    that the *<8,8>* head has the same semantic feature in Turkish as in the English
    model, as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在土耳其语言模型中寻找一个指代模式。以下代码加载了一个土耳其`bert-base`模型并取了一个句子对。我们在这里观察到，*<8,8>* 头在土耳其语言模型中具有与英语模型相同的语义特征，如下所示：
- en: '[PRE4]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the preceding code, `sentence_a` and `sentence_b` mean *The cat is sad*
    and *Because it ate too much food as usual*, respectively. When hovering over
    **o** (**it**), **it** attends to **Kedi** (**cat**), as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的代码中，`sentence_a` 和 `sentence_b` 意味着 *The cat is sad* 和 *Because it ate too
    much food as usual*，当悬停在 **o**（**它**）上时，**它** 关注 **Kedi**（**猫**），如下所示：
- en: '![Figure 11.10 – Coreference pattern in the Turkish language model ](img/B17123_11_010.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图11.10 – 土耳其语言模型中的指代模式](img/B17123_11_010.jpg)'
- en: Figure 11.10 – Coreference pattern in the Turkish language model
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.10 – 土耳其语言模型中的指代模式
- en: All other tokens except **o** mostly attend to the SEP delimiter token, which
    is a dominant behavior pattern in all heads in the BERT architecture.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了 **o** 之外的所有其他标记大多数关注到了SEP分隔符标记，这是BERT架构中所有头部中的一种主导行为模式。
- en: As a final example for the head view, we will interpret another language model
    and move on to the model view feature. This time, we choose the `bert-base-german-cased`
    German language model and visualize it for the input—that is, the German equivalent
    of the same-sentence pair we used for Turkish.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为头视图的最后一个示例，我们将解释另一个语言模型，并继续查看模型视图特性。这次，我们选择`bert-base-german-cased`德语语言模型并将其可视化为输入——即，我们用于土耳其语的同一个句对的德语等价句对。
- en: 'The following code loads a German model, consumes a pair of sentences, and
    visualizes them:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码加载了一个德语模型，消耗了一对句子，并对其进行了可视化：
- en: '[PRE5]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When we examine the heads, we can see the coreference pattern in the 8th layer
    again, but this time in the 11th head. To select the *<8,11>* head, pick layer
    8 from the drop-down menu and double-click on the last head, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们检查头部时，我们可以再次在第8层中看到指代模式，但这次是在第11个头部。要选择 *<8,11>* 头部，从下拉菜单中选择第8层并双击最后的头部，如下所示：
- en: '![Figure 11.11 – Coreference relation pattern in the German language model
    ](img/B17123_11_011.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 德语语言模型中的指代关系模式](img/B17123_11_011.jpg)'
- en: Figure 11.11 – Coreference relation pattern in the German language model
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 德语语言模型中的指代关系模式
- en: As you see, when hovering over **sie**, you will see strong attentions to the
    **Die Katze**. While this *<8,11>* head is the strongest one for coreference relations
    (known as anaphoric relations in computational linguistics literature), this relationship
    may have spread to many other heads. To observe it, we will have to check all
    the heads one by one.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在悬停在 **sie** 上时，你会看到对 **Die Katze** 的强烈关注。虽然这个 *<8,11>* 头是最强大的指代关系头（在计算语言学文献中被称为先行关系），这种关系可能已经传播到许多其他头中。为了观察它，我们必须逐个检查所有的头。
- en: On the other hand, BertViz's model view feature gives us a basic bird's-eye
    view to see all heads at once. Let's take a look at it in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，BertViz的模型视图功能让我们基本了解所有头一次看到。 让我们在下一节中看一下。
- en: Model view
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型视图
- en: '**Model view** allows us to have a bird''s-eye view of attentions across all
    heads and layers. Self-attention heads are shown in tabular form, with rows and
    columns corresponding to layers and heads, respectively. Each head is visualized
    in the form of a clickable thumbnail that includes the broad shape of the attention
    model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型视图**允许我们全面查看所有头和层的注意力。 自注意力头以表格形式显示，行和列分别对应层和头。 每个头以可点击的缩略图形式呈现，其中包括注意力模型的总体形状。'
- en: 'The view can tell us how BERT works and makes it easier to interpret. Many
    recent studies, such as *A Primer in BERTology: What We Know About How BERT Works*,
    *Anna Rogers*, *Olga Kovaleva*, *Anna Rumshisky*, *2021*, found some clues about
    the behavior of the layers and came to a consensus. We already listed some of
    them in the *Interpreting attention heads* section. You can test these facts yourself
    using BertViz''s model view.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图可以告诉我们BERT是如何工作的，并且更容易解释。 许多最近的研究，比如*BERTology初级入门：我们知道BERT如何工作*，*Anna Rogers*，*Olga
    Kovaleva*，*Anna Rumshisky*，*2021*，找到了一些关于层行为的线索并达成了共识。 我们已经在*解释注意头*部分列出了其中一些。
    您可以使用BertViz的模型视图自行测试这些事实。
- en: 'Let''s view the German language model that we just used, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下我们刚刚使用的德语语言模型，如下：
- en: 'First, import the following modules:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入以下模块：
- en: '[PRE6]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will use a `show_model_view()` wrapper function developed by Jesse
    Vig. You can find the original code at the following link: [https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb](https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb).'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 Jesse Vig 开发的`show_model_view()`包装函数。 您可以在以下链接找到原始代码：[https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb](https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb)。
- en: 'You can also find the function definition in our book''s GitHub link, at [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11).
    We are just dropping the function header here:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以在我们书本的GitHub链接中找到函数定义，链接地址为[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11)。
    这里只是放出函数头部：
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s load the German model again. If you have already loaded it, you can
    skip the first five lines. Here is the code you''ll need:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次加载德语模型。 如果您已经加载了它，可以跳过前五行。 这是您需要的代码：
- en: '[PRE8]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the output:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 11.12 – The model view of the German language model ](img/B17123_11_012.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图11.12 – 德语语言模型的模型视图](img/B17123_11_012.jpg)'
- en: Figure 11.12 – The model view of the German language model
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.12 – 德语语言模型的模型视图
- en: This view helps us easily observe many patterns such as next-token (or previous-token)
    attention patterns. As we mentioned earlier in the *Interpreting attention heads*
    section, tokens often tend to attend to delimiters—specifically, CLS delimiters
    at lower layers and SEP delimiters at upper layers. Because these tokens are not
    masked out, they can ease the flow of information. In the last layers, we only
    observe SEP-delimiter-focused attention patterns. It could be speculated that
    SEP is used to collect segment-level information, which can be used then for inter-sentence
    tasks such as **Next Sentence Prediction** (**NSP**) or for encoding sentence-level
    meaning.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个视图帮助我们轻松观察到许多模式，比如下一个令牌（或上一个令牌）的注意力模式。 正如我们在*解释注意头*部分中提到的，令牌通常倾向于关注定界符，具体来说是在下层关注CLS定界符，在上层关注SEP定界符。
    因为这些令牌未被屏蔽，它们可以促进信息的流动。 在最后的层中，我们只观察到 SEP 定界符为中心的注意力模式。 可以推测，SEP 用于收集段级信息，然后用于跨句任务，比如**下一句预测**（**NSP**）或编码句级含义。
- en: On the other hand, we observe that coreference relation patterns are mostly
    encoded in the *<8,1>, <8,11>, <10,1>, and <10,7>* heads. Again, it can be clearly
    said that the *<8, 11>* head is the strongest head that encodes the coreference
    relation in the German model, which we already discussed.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，我们观察到指代关系模式主要编码在 *<8,1>, <8,11>, <10,1>, and <10,7>* 头部。 再次可以明确说，*<8, 11>*
    头部是编码德语模型中指代关系的最强头部，我们已经讨论过。
- en: 'When you click on that thumbnail, you will see the same output, as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您点击该缩略图时，您将看到相同的输出，如下：
- en: '![Figure 11.13 – Close-up of the <8,11> head in model view  ](img/B17123_11_013.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – 模型视图中<8,11>头部的放大图](img/B17123_11_013.jpg)'
- en: Figure 11.13 – Close-up of the <8,11> head in model view
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – 模型视图中<8,11>头部的放大图
- en: You can again hover over the tokens and see the mappings.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以再次悬停在标记上，查看映射关系。
- en: I think that's enough work for the head view and the model view. Now, let's
    deconstruct the model with the help of the neuron view and try to understand how
    these heads calculate weights.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为头视图和模型视图的工作已经足够了。现在，让我们通过神经元视图拆解模型，并试图理解这些头部是如何计算权重的。
- en: Neuron view
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元视图
- en: So far, we have visualized computed weights for a given input. The **neuron
    view** visualizes the neurons and the key vectors in a query and how the weights
    between tokens are computed based on interactions. We can trace the computation
    phase between any two tokens.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经可视化了给定输入的计算权重。**神经元视图**可视化了查询中的神经元和关键向量，以及基于相互作用计算标记之间的权重。我们可以追踪任意两个标记之间的计算阶段。
- en: 'Again, we will load the German model and visualize the same-sentence pair we
    just worked with, to be coherent. We execute the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载德语模型，并可视化刚刚处理过的同一句对，以保持连贯性。我们执行以下代码：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出：
- en: '![Figure 11.14 – Neuron view of the coreference relation pattern (head <8,11>)
    ](img/B17123_11_014.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 核指关系模式的神经元视图（头部<8,11>）](img/B17123_11_014.jpg)'
- en: Figure 11.14 – Neuron view of the coreference relation pattern (head <8,11>)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 核指关系模式的神经元视图（头部<8,11>）
- en: The view helps us to trace the computation of attention from the **sie** token
    that we selected on the left to the other tokens on the right. Positive values
    are blue and negative values are orange. Color intensity represents the magnitude
    of the numerical value. The query of **sie** is very similar to the keys of **Die**
    and **Katze**. If you look at the patterns carefully, you will notice how similar
    these vectors are. Therefore, their dot product goes higher than the other comparison,
    which establishes strong attention between those tokens. We also trace the dot
    product and the Softmax function output as we go to the right. When clicking on
    the other tokens on the left, you can trace other computations as well.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图帮助我们追踪了我们在左边选择的**sie**标记到右边其他标记的注意力计算。正值是蓝色的，负值是橙色的。颜色的强度代表了数值的大小。查询**sie**与**Die**和**Katze**的键非常相似。如果你仔细观察图案，你会注意到这些向量有多么相似。因此，它们的点积要比其他的比较更高，这建立了这些标记之间强烈的注意力。我们还可以追踪点积和Softmax函数的输出，当我们向右移动时。当点击左边的其他标记时，你也可以追踪其他的计算。
- en: 'Now, let''s select a head-bearing next-token attention pattern for the same
    input, and trace it. To do so, we select the *<2,6>* head. In this pattern, virtually
    all the attention is focused on the next word. We click the **sie** token once
    again, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为相同的输入选择一个头部承载的下一个标记的注意力模式，并对其进行追踪。为此，我们选择*<2,6>* 头部。在这个模式中，几乎所有的注意力都集中在下一个字上。我们再次点击**sie**标记，如下所示：
- en: '![Figure 11.15 – Neuron view of next-token attention patterns (the <2,6> head)
    ](img/B17123_11_015.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – 下一个标记注意力模式的神经元视图（<2,6>头部）](img/B17123_11_015.jpg)'
- en: Figure 11.15 – Neuron view of next-token attention patterns (the <2,6> head)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 下一个标记注意力模式的神经元视图（<2,6>头部）
- en: Now, the **sie** token is focused on the next token instead of its own antecedent
    (**Die Katze**). When we carefully look at the query and the candidate keys, the
    most similar key to the query of **sie** is the next token, **zu**. Likewise,
    we observe how the dot product and Softmax function are applied in order.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**sie** 标记的注意力集中在下一个标记上，而不是它自己的先行词(**Die Katze**)。当我们仔细观察查询和候选键时，我们会发现与**sie**的查询最相似的键是下一个标记**zu**。同样地，在这过程中我们可以看到点乘和Softmax函数是如何应用的。
- en: In the next section, we will briefly talk about probing classifiers for interpreting
    Transformers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节，我们将简要讨论解释变压器（Transformers）的探针分类器。
- en: Understanding the inner parts of BERT with probing classifiers
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过解读变压器的探针分类器理解BERT的内部部分
- en: The opacity of what DL learns has led to a number of studies on the interpretation
    of such models. We attempt to answer the question of which parts of a Transformer
    model are responsible for certain language features, or which parts of the input
    lead the model to make a particular decision. To do so, other than visualizing
    internal representations, we can train a classifier on the representations to
    predict some external morphological, syntactic, or semantic properties. Hence,
    we can determine if we associate internal *representations* with external *properties*.
    The successful training of the model would be quantitative evidence of such an
    association—that is, the language model has learned information relevant for an
    external property. This approach is called a **probing-classifier** approach,
    which is a prominent analysis technique in NLP and other DL studies. An attention-based
    probing classifier takes an attention map as input and predicts external properties
    such as coreference relations or head-modifier relations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DL 学到的内容的不透明性导致了许多关于这些模型解释的研究。我们试图回答这样的问题：Transformer 模型的哪些部分负责某些语言特征，或者输入的哪些部分导致模型做出特定的决定。为了做到这一点，除了可视化内部表示之外，我们还可以对表示进行分类器训练，以预测一些外部形态、句法或语义属性。因此，我们可以确定是否将内部
    *表示* 与外部 *属性* 关联起来。模型的成功训练将是这种关联的定量证据——即，语言模型已经学会了与外部属性相关的信息。这种方法称为 **探测分类器**
    方法，是自然语言处理和其他 DL 研究中的一种重要分析技术。基于注意力的探测分类器以注意力映射作为输入，并预测外部属性，如共指关系或头部修饰关系。
- en: As seen in the preceding experiments, we get the self-attention weights for
    a given input with the `get_bert_attention()` function. Instead of visualizing
    these weights, we can directly transfer them to a classification pipeline. So,
    with supervision, we can determine which head is suitable for which semantic feature—for
    example, we can figure out which heads are suitable for coreference with labeled
    data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的实验所示，我们可以使用`get_bert_attention()`函数获取给定输入的自注意权重。我们可以直接将这些权重传递到分类管道中，而不是将它们可视化。因此，在监督下，我们可以确定哪个头适合哪个语义特征——例如，我们可以通过标记数据找出哪些头适合共指。
- en: Now, let's move on to the model-tracking part, which is crucial for building
    efficient models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向模型跟踪部分，这对于构建高效的模型至关重要。
- en: Tracking model metrics
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪模型指标
- en: So far, we have trained language models and simply analyzed the final results.
    We have not observed the training process or made a comparison of training using
    different options. In this section, we will briefly discuss how to monitor model
    training. For this, we will handle how to track the training of the models we
    developed before in [*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081), *Fine-Tuning
    Language Models for Text Classification*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练了语言模型，并简单分析了最终结果。我们还没有观察训练过程或比较使用不同选项进行训练。在本节中，我们将简要讨论如何监视模型训练。为此，我们将处理如何跟踪之前在[*第
    5 章*](B17123_05_Epub_AM.xhtml#_idTextAnchor081)中开发的模型的训练，*文本分类的语言模型微调*。
- en: There are two important tools developed in this area—one is TensorBoard, and
    the other is W&B. With the former, we save the training results to a local drive
    and visualize them at the end of the experiment. With the latter, we are able
    to monitor the model-training progress live in a cloud platform.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域开发了两个重要的工具——一个是 TensorBoard，另一个是 W&B。前者可以将训练结果保存到本地驱动器，并在实验结束时将其可视化。后者则可以在云平台上实时监视模型训练进度。
- en: This section will be a short introduction to these tools without going into
    much detail about them, as this is beyond the scope of this chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将是对这些工具的简要介绍，不会详细讨论它们，因为这超出了本章的范围。
- en: Let's start with TensorBoard.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 TensorBoard 开始。
- en: Tracking model training with TensorBoard
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 跟踪模型训练
- en: TensorBoard is a visualization tool specifically for DL experiments. It has
    many features such as tracking, training, projecting embeddings to a lower space,
    and visualizing model graphs. We mostly use it for tracking and visualizing metrics
    such as loss. Tracking a metric with TensorBoard is so easy for Transformers that
    adding a couple of lines to model-training code will be enough. Everything is
    kept almost the same.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是专门用于 DL 实验的可视化工具。它有许多功能，如跟踪、训练、将嵌入投射到较低的空间和可视化模型图。我们主要用它来跟踪和可视化诸如损失之类的指标。对于
    Transformers 来说，使用 TensorBoard 跟踪指标是如此容易，只需在模型训练代码中添加几行就足够了。一切几乎保持不变。
- en: Now, we will repeat the **Internet Movie Database** (**IMDb**) sentiment fine-tuning
    experiment we did in [*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081),
    *Fine-Tuning Language Models for Text Classification*, and will track the metrics.
    In that chapter, we already trained a sentiment model with an IMDb dataset consisting
    of a **4 kilo** (**4K**) training dataset, a 1K validation set, and a 1K test
    set. Now, we will adapt it to TensorBoard. For more details about TensorBoard,
    please visit [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重复[*第 5 章*](B17123_05_Epub_AM.xhtml#_idTextAnchor081)中进行的**互联网电影数据库**（**IMDb**）情感微调实验，*对文本分类进行语言模型微调*，并将跟踪指标。在该章节中，我们已经使用由**4
    千**（**4K**）个训练数据集、1 千验证集和 1 千测试集组成的 IMDb 数据集训练了一个情感模型。现在，我们将其调整为适用于 TensorBoard。有关
    TensorBoard 的更多详细信息，请访问 [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)。
- en: 'Let''s begin:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: 'First, we install TensorBoard if it is not already installed, like this:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，如果尚未安装 TensorBoard，我们要这样安装：
- en: '[PRE10]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Keeping the other code lines of IMDb sentiment analysis as-is from [*Chapter
    5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081), *Fine-Tuning Language Models for
    Text Classification*, we set the training argument as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[*第 5 章*](B17123_05_Epub_AM.xhtml#_idTextAnchor081)，*对文本分类进行语言模型微调*，保持 IMDb
    情感分析的其他代码行不变，我们将训练参数设置如下：
- en: '[PRE11]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code snippet, the value of `logging_dir` will soon be passed
    to TensorBoard as a parameter. As the training dataset size is 4K and the training
    batch size is 16, we have 250 steps (4K/16) for each epoch, which means 750 steps
    for three epochs.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上述代码片段中，`logging_dir` 的值将很快作为参数传递给 TensorBoard。由于训练数据集大小为 4K，训练批大小为 16，每个时期有
    250 步（4K/16），意味着三个时期有 750 步。
- en: We set `logging_steps` to 50, which is a sampling interval. As the interval
    is decreased, more details about where model performance rises or falls are recorded.
    We'll do another experiment later on, reducing this sampling interval at step
    27\.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 `logging_steps` 设置为 50，这是一个采样间隔。随着间隔的减小，记录了有关模型性能上升或下降的更多细节。稍后我们将做另一个实验，在第
    27 步降低这个采样间隔。
- en: Now, at every 50 steps, the model performance is measured in terms of the metrics
    that we define in `compute_metrics()`. The metrics to be measured are Accuracy,
    F1, Precision, and Recall. As a result, we will have 15 (750/50) performance measurements
    to be recorded. When we run `trainer.train()`, this starts the training process
    and records the logs under the `logging_dir='./logs'` directory.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，每隔 50 步，模型性能将根据我们在 `compute_metrics()` 中定义的指标进行测量。所要测量的指标包括准确率、F1 值、精确率和召回率。因此，我们将记录
    15 次（750/50）性能测量。当我们运行 `trainer.train()` 时，这将开始训练过程，并在 `logging_dir='./logs'`
    目录下记录日志。
- en: We set `load_best_model_at_end` to `True` so that the pipeline loads whichever
    checkpoint has the best performance in terms of loss. Once the training is completed,
    you will notice that the best model is loaded from `checkpoint-250` with a loss
    score of `0.263`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 `load_best_model_at_end` 设置为 `True`，以便管道加载在损失方面表现最佳的任何检查点。一旦训练完成，您将注意到最佳模型从
    `checkpoint-250` 加载，其损失得分为 `0.263`。
- en: 'Now, the only thing we need to do is to call the following code to launch TensorBoard:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们唯一需要做的就是调用以下代码来启动 TensorBoard：
- en: '[PRE12]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the output:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 11.16 – TensorBoard visualization for training history ](img/B17123_11_016.jpg)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 11.16 – TensorBoard 训练历史可视化](img/B17123_11_016.jpg)'
- en: Figure 11.16 – TensorBoard visualization for training history
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.16 – TensorBoard 训练历史可视化
- en: 'As you may have noticed, we can trace the metrics that we defined before. The
    horizontal axis goes from 0 to 750 steps, which is what we calculated before.
    We will not discuss TensorBoard in detail here. Let''s just look at the **eval/loss**
    chart only. When you click on the maximization icon at the left-hand bottom corner,
    you will see the following chart:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我们可以跟踪之前定义的指标。水平轴从 0 到 750 步，这是我们之前计算的步数。我们不会在这里详细讨论 TensorBoard。让我们只看一下**eval/loss**
    图表。当您点击左下角的最大化图标时，您将会看到以下图表：
- en: '![Figure 11.17 – TensorBoard eval/loss chart for logging steps of 50  ](img/B17123_11_017.jpg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 11.17 – TensorBoard eval/loss 图表，记录步长为50](img/B17123_11_017.jpg)'
- en: Figure 11.17 – TensorBoard eval/loss chart for logging steps of 50
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.17 – TensorBoard eval/loss 图表，记录步长为50
- en: In the preceding screenshot, we set the smoothing to 0 with the slider control
    on the left of the TensorBoard dashboard to see scores more precisely and focus
    on the global minimum. If your experiment has very high volatility, the smoothing
    feature can work well to see overall trends. It functions as a **Moving Average**
    (**MA**). This chart supports our previous observation, in which the best loss
    measurement is **0.2658** at step **250**.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们将TensorBoard仪表板左侧的滑块控制器的平滑度设置为0，以更精确地查看分数并关注全局最小值。如果你的实验波动性很高，平滑功能可以很好地显示整体趋势。它的功能类似于**滑动平均**（**MA**）。这张图表支持我们之前的观察，最佳损失测量值为**0.2658**，在第**250**步时获得。
- en: 'As `logging_steps` is set to **10**, we get a high resolution, as in the following
    screenshot. As a result, we will have 75 (750 steps/10 steps) performance measurements
    to be recorded. When we rerun the entire flow with this resolution, we get the
    best model at step 220, with a loss score of 0.238, which is better than the previous
    experiment. The result can be seen in the following screenshot. We naturally observe
    more fluctuations due to higher resolution:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`logging_steps`设置为**10**，我们获得了高分辨率，如下面的截图所示。因此，我们将记录75（750步/10步）个性能测量值。当我们以这个分辨率重新运行整个流程时，我们在第220步获得了最佳模型，损失得分为0.238，比之前的实验更好。结果可以在下面的截图中看到。由于分辨率更高，我们自然会观察到更多的波动：
- en: '![Figure 11.18 – Higher-resolution eval/loss chart for logging steps of 10
    ](img/B17123_11_018.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18 - 记录步骤为10的更高分辨率eval/loss图表](img/B17123_11_018.jpg)'
- en: Figure 11.18 – Higher-resolution eval/loss chart for logging steps of 10
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 - 记录步骤为10的更高分辨率eval/loss图表
- en: We are done with TensorBoard for now. Let's work with W&B!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们不再使用TensorBoard。让我们开始使用W&B！
- en: Tracking model training live with W&B
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用W&B实时跟踪模型训练
- en: W&B, unlike TensorBoard, provides a dashboard in a cloud platform, and we can
    trace and back up all experiments in a single hub. It also allows us to work with
    a team for development and sharing. The training code is run on our local machine,
    while the logs are kept in the W&B cloud. Most importantly, we can follow the
    training process live and share the result immediately with the community or team.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorBoard不同，W&B在云平台提供了一个仪表板，我们可以在一个单一的中心追踪和备份所有实验。它还允许我们与团队进行开发和共享。训练代码在我们的本地机器上运行，而日志保存在W&B云中。最重要的是，我们可以实时跟踪训练过程并立即与社区或团队分享结果。
- en: 'We can enable W&B for our experiments by making very small changes to our existing
    code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对现有代码进行非常小的更改来为我们的实验启用W&B：
- en: 'First of all, we need to create an account in `wandb.ai`, then install the
    Python library, as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要在`wandb.ai`创建一个帐户，然后安装Python库，如下所示：
- en: '[PRE13]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Again, we will take the IMDb sentiment-analysis code and make minor changes
    to it. First, let''s import the library and log in to `wandB`, as follows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们将采用IMDb情感分析代码并进行微小的更改。首先，让我们导入库并登录到`wandB`，如下所示：
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`wandb` requests an API key that you can easily find at the following link:
    [https://wandb.ai/authorize](https://wandb.ai/authorize).'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`wandb`请求一个API密钥，你可以很容易地在以下链接找到：[https://wandb.ai/authorize](https://wandb.ai/authorize)。'
- en: 'Alternatively, you can set the `WANDB_API_KEY` environment variable to your
    API key, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，你可以将`WANDB_API_KEY`环境变量设置为你的API密钥，如下所示：
- en: '[PRE15]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Again, keeping the entire code as-is, we only add two parameters, `report_to="wandb"`
    and `run_name="..."`, to `TrainingArguments`, which enables logging in to W&B,
    as shown in the following code block:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次保持整个代码不变，我们只需向`TrainingArguments`添加两个参数，`report_to="wandb"`和`run_name="..."`，即可在W&B中启用日志记录，如下所示：
- en: '[PRE16]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, as soon as you call `trainer.train()`, logging starts on the cloud. After
    the call, please check the cloud dashboard and see how it changes. Once the `trainer.train()`
    call has completed successfully, we execute the following line to tell `wandB`
    we are done:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你调用`trainer.train()`后，日志将开始记录在云端。调用后，请检查云端仪表板并查看其变化。一旦`trainer.train()`调用成功完成，我们执行以下代码告诉`wandB`我们已完成：
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The execution also outputs run history locally, as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行还会在本地输出运行历史，如下所示：
- en: '![Figure 11.19 – The local output of W&B ](img/B17123_11_019.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图11.19 - W&B的本地输出](img/B17123_11_019.jpg)'
- en: Figure 11.19 – The local output of W&B
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19 - W&B的本地输出
- en: 'When you connect to the link provided by W&B, you will get to an interface
    that looks something like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当你连接到W&B提供的链接时，你将进入一个看起来类似于这样的界面：
- en: '![Figure 11.20 – The online visualization of a single run on the W&B dashboard
    ](img/B17123_11_020.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图11.20 - W&B仪表板上单个运行的在线可视化](img/B17123_11_020.jpg)'
- en: Figure 11.20 – The online visualization of a single run on the W&B dashboard
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 – 在W&B仪表板上单次运行的在线可视化
- en: This visualization gives us a summarized performance result for a single run.
    As you see, we can trace the metrics that we defined in the `compute_metric()`
    function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该可视化为单次运行的结果提供了一个总结的性能结果。正如您所看到的，我们可以追踪在`compute_metric()`函数中定义的指标。
- en: 'Now, let''s take a look at the evaluation loss. The following screenshot shows
    exactly the same plot that TensorBoard provided, where the minimum loss is around
    0.2658, occurring at step 250:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下评估损失。下面的截图显示了与TensorBoard提供的完全相同的绘图，其中最小损失约为0.2658，在第250步发生：
- en: '![Figure 11.21 – The eval/loss plot of IMDb experiment on the W&B dashboard  ](img/B17123_11_021.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图11.21 – 在W&B仪表板上IMDb实验的eval/loss绘图](img/B17123_11_021.jpg)'
- en: Figure 11.21 – The eval/loss plot of IMDb experiment on the W&B dashboard
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21 – 在W&B仪表板上IMDb实验的eval/loss绘图
- en: We have only visualized a single run so far. W&B allows us to explore the results
    dynamically across lots of runs at once—for example, we can visualize the results
    of models using different hyperparameters such as learning rate or batch size.
    To do so, we instantiate a `TrainingArguments` object properly with another different
    hyperparameter setting and change `run_name="..."` accordingly for each run.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只对单次运行进行了可视化。W&B允许我们跨多次运行动态地探索结果—例如，我们可以可视化使用不同超参数的模型的结果，如学习率或批量大小。为此，我们适当地实例化一个`TrainingArguments`对象，并为每次运行相应地更改`run_name="..."`。
- en: 'The following screenshot shows our several IMDb sentiment-analysis runs using
    different hyperparameters. We can also see the batch size and learning rate that
    we changed:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了我们使用不同超参数进行的几个IMDb情感分析运行。我们还可以看到我们更改的批量大小和学习率：
- en: '![Figure 11.22 – Exploring the results across several runs on the W&B dashboard
    ](img/B17123_11_022.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图11.22 – 在W&B仪表板上探索多次运行的结果](img/B17123_11_022.jpg)'
- en: Figure 11.22 – Exploring the results across several runs on the W&B dashboard
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22 – 在W&B仪表板上探索多次运行的结果
- en: 'W&B provides useful functionality—for instance, it automates hyperparameter
    optimization and searching the space of possible models, called W&B Sweeps. Other
    than that, it also provides system logs relating to **Graphics Processing Unit**
    (**GPU**) consumption, **Central Processing Unit** (**CPU**) utilization, and
    so on. For more detailed information, please check the following website: [https://wandb.ai/home](https://wandb.ai/home).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: W&B提供了有用的功能—例如，它自动化了超参数优化和搜索可能模型的空间，称为W&B Sweeps。除此之外，它还提供了与**图形处理单元**(**GPU**)消耗有关的系统日志，**中央处理单元**(**CPU**)利用率等等。更详细的信息，请查看以下网站：[https://wandb.ai/home](https://wandb.ai/home)。
- en: Well done! In the last section, *References*, we will focus more on technical
    tools, since it's crucial to use such utility tools to develop better models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！在最后一节*参考资料*中，我们将更多关注技术工具，因为使用这样的实用工具对于开发更好的模型至关重要。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we introduced two different technical concepts: attention
    visualization and experiment tracking. We visualized attention heads with the
    exBERT online interface first. Then, we studied BertViz, where we wrote Python
    code to see three BertViz visualizations: head view, model view, and neuron view.
    The BertViz interface gave us more control so that we could work with different
    language models. Moreover, we were also able to observe how attention weights
    between tokens are computed. These tools provide us with important functions for
    interpretability and exploitability. We also learned how to track our experiments
    to obtain higher-quality models and do error analysis. We utilized two tools to
    monitor training: TensorBoard and W&B. These tools were used to effectively track
    experiments and to optimize model training.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了两个不同的技术概念：注意力可视化和实验跟踪。我们首先使用exBERT在线界面可视化了注意力头。然后，我们研究了BertViz，我们编写Python代码来查看三个BertViz可视化：头部视图，模型视图和神经元视图。BertViz界面给了我们更多控制，以便我们可以使用不同的语言模型。此外，我们还能观察到如何计算tokens之间的注意权重。这些工具为我们提供了解释性和利用性的重要功能。我们还学会了如何跟踪我们的实验以获得更高质量的模型和进行错误分析。我们利用了两种工具来监视训练：TensorBoard和W&B。这些工具用来有效地跟踪实验并优化模型训练。
- en: Congratulations! You've finished reading this book by demonstrating great perseverance
    and persistence throughout this journey. You can now feel confident as you are
    well equipped with the tools you need, and you are prepared for developing and
    implementing advanced NLP applications.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你通过展现出的极大毅力和坚持不懈完成了这本书的阅读之旅。现在你可以放心了，因为你已经装备好了需要的工具，并且准备好开发和实现高级自然语言处理应用。
- en: References
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
    Models*, *Benjamin Hoover*, *Hendrik Strobelt*, *Sebastian Gehrmann*, *2019*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*exBERT：一种用于探索Transformer模型中学习表示的视觉分析工具*，*本杰明·胡佛*，*亨德里克·斯特罗贝尔特*，*塞巴斯蒂安·盖尔曼*，*2019*。'
- en: '*Vig, J.*, *2019*. *A multiscale visualization of attention in the Transformer
    model*. *arXiv preprint arXiv:1906.05714*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vig, J.*, *2019*. *Transformer模型中的多尺度注意力可视化*。*arXiv预印本 arXiv:1906.05714*。'
- en: '*Clark, K.*, *Khandelwal, U.*, *Levy, O.* and *Manning, C.D.*, *2019*. *What
    does bert look at? An analysis of bert''s attention*. *arXiv preprint arXiv:1906.04341.7*'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Clark, K.*, *Khandelwal, U.*, *Levy, O.* 和 *Manning, C.D.*, *2019*. *BERT看什么？BERT注意力分析*。*arXiv预印本
    arXiv:1906.04341.7*'
- en: '*Biewald, L.*, *Experiment tracking with weights and biases*, *2020*. Software
    available from `wandb.com`, *2(5)*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Biewald, L.*, *权重与偏差的实验追踪*, *2020*。软件可在 `wandb.com` 获得，*2(5)*。'
- en: '*Rogers, A.*, *Kovaleva, O.* and *Rumshisky, A.*,*2020*. *A primer in BERTology:
    What we know about how BERT works*. *Transactions of the Association for Computational
    Linguistics*, *8*, *pp.842-866*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rogers, A.*, *Kovaleva, O.* 和 *Rumshisky, A.*, *2020*. *BERT学习入门：我们对BERT如何工作的了解*.
    *计算语言学协会交易*, *8*, *pp.842-866*。'
- en: '*W&B*: [https://wandb.ai](https://wandb.ai)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W&B*：[https://wandb.ai](https://wandb.ai)'
- en: '*TensorBoard*: [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorBoard*：[https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)'
- en: '*exBert—Hugging Face*: [https://huggingface.co/exbert](https://huggingface.co/exbert)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*exBert—Hugging Face*：[https://huggingface.co/exbert](https://huggingface.co/exbert)'
- en: '*exBERT*: [https://exbert.net/](https://exbert.net/)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*exBERT*：[https://exbert.net/](https://exbert.net/)'
