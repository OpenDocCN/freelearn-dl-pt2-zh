- en: '*Chapter 11*: Attention Visualization and Experiment Tracking'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover two different technical concepts, **attention
    visualization** and **experiment tracking**, and we will practice them through
    sophisticated tools such as **exBERT** and **BertViz**. These tools provide important
    functions for interpretability and explainability. First, we will discuss how
    to visualize the inner parts of attention by utilizing the tools. It is important
    to interpret the learned representations and to understand the information encoded
    by self-attention heads in the Transformer. We will see that certain heads correspond
    to a certain aspect of syntax or semantics. Secondly, we will learn how to track
    experiments by logging and then monitoring by using **TensorBoard** and **Weights
    & Biases** (**W&B**). These tools enable us to efficiently host and track experimental
    results such as loss or other metrics, which helps us to optimize model training.
    You will learn how to use exBERT and BertViz to see the inner parts of their own
    models and will be able to utilize both TensorBoard and W&B to monitor and optimize
    their models by the end of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting attention heads
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking model metrics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter is found at [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11),
    which is the GitHub repository for this book. We will be using Jupyter Notebook
    to run our coding exercises that require Python 3.6.0 or above, and the following
    packages will need to be installed:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Transformers >=4.00`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorboard`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wandb`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bertviz`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipywidgets`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out the following link to see Code in Action Video:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3iM4Y1F](https://bit.ly/3iM4Y1F)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting attention heads
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with most **Deep Learning** (**DL**) architectures, both the success of the
    Transformer models and how they learn have been not fully understood, but we know
    that the Transformers—remarkably—learn many linguistic features of the language.
    A significant amount of learned linguistic knowledge is distributed both in the
    hidden state and in the self-attention heads of the pre-trained model. There have
    been substantial recent studies published and many tools developed to understand
    and to better explain the phenomena.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to some **Natural Language Processing** (**NLP**) community tools, we
    are able to interpret the information learned by the self-attention heads in a
    Transformer model. The heads can be interpreted naturally, thanks to the weights
    between tokens. We will soon see that in further experiments in this section,
    certain heads correspond to a certain aspect of syntax or semantics. We can also
    observe surface-level patterns and many other linguistic features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will conduct some experiments using community tools to
    observe these patterns and features in the attention heads. Recent studies have
    already revealed many of the features of self-attention. Let''s highlight some
    of them before we get into the experiments. For example, most of the heads attend
    to delimiter tokens such as **Separator** (**SEP**) and **Classification** (**CLS**),
    since these tokens are never masked out and bear segment-level information in
    particular. Another observation is that most heads pay little attention to the
    current token, but some heads specialize in only attending the next or previous
    tokens, especially in earlier layers. Here is a list of other patterns found in
    recent studies that we can easily observe in our experiments:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用社区工具进行一些实验来观察这些模式和注意头部的特征。最近的研究已经揭示了自注意的许多特征。在我们进行实验之前，让我们先介绍一些。例如，大多数头部倾向于关注分隔符标记，如**分隔符**（**SEP**）和**分类**（**CLS**），因为这些标记永远不被屏蔽，并且特别携带段级信息。另一个观察是，大多数头部很少关注当前标记，但一些头部专门关注下一个或上一个标记，特别是在较早的层中。以下是最近研究中发现的其他模式列表，我们可以在我们的实验中轻松观察到：
- en: Attention heads in the same layer show similar behavior.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一层中的注意头表现出类似的行为。
- en: Particular heads correspond to specific aspects of syntax or semantic relations.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定头对应于语法或语义关系的特定方面。
- en: Some heads encode so that the direct objects tend to attend to their verbs,
    such as *<lesson, take>* or *<car, drive>*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些头部对直接对象倾向于关注它们的动词，例如*<lesson, take>*或*<car, drive>*。
- en: In some heads, the noun modifiers attend to their noun (for example, *the hot
    water*; *the next layer*), or the possessive pronoun attends to the head (for
    example, *her car*).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些头中，名词修饰语会关注它们的名词（例如*the hot water*；*the next layer*），或所有格代词会关注头部（例如*her car*）。
- en: Some heads encode so that passive auxiliary verbs attend to a related verb,
    such as *Been damaged, was taken*.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些头部对被动助动词进行编码，例如*Been damaged, was taken*。
- en: In some heads, coreferent mentions attend to themselves, such as *talks-negotiation,
    she-her, President-Biden*.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些头部中，指代提及关注自己，例如*talks-negotiation, she-her, President-Biden*。
- en: The lower layers usually have information about word positions.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低的层通常包含有关词位置的信息。
- en: Syntactic features are observed earlier in the transformer, while high-level
    semantic information appears in the upper layers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法特征在transformer中较早出现，而高级语义信息出现在更上层。
- en: The final layers are the most task-specific and are therefore very effective
    for downstream tasks.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终层是最具任务特异性的，并因此对下游任务非常有效。
- en: To observe these patterns, we can use two important tools, **exBERT** and **BertViz**,
    here. These tools have almost the same functionality. We will start with exBERT.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察这些模式，我们可以在这里使用两个重要工具**exBERT**和**BertViz**。这些工具功能几乎相同。我们将从exBERT开始。
- en: Visualizing attention heads with exBERT
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用exBERT可视化注意力头部
- en: exBERT is a visualization tool to see the inner parts of Transformers. We will
    use it to visualize the attention heads of the *BERT-base-cased* model, which
    is the default model in the exBERT interface. Unless otherwise stated, the model
    we will use in the following examples is *BERT-base-cased*. This contains 12 layers
    and 12 self-attention heads in each layer, which makes for 144 self-attention
    heads.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: exBERT是一个可视化工具，用于查看Transformers内部部分。我们将使用它来可视化*BERT-base-cased*模型的自注意头部，这是exBERT界面的默认模型。除非另有说明，我们将在以下示例中使用*BERT-base-cased*模型。这包含12层，每层12个自注意头部，共计144个自注意头部。
- en: 'We will learn how to utilize exBERT step by step, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步学习如何使用exBERT，如下所示：
- en: 'Let''s click on the exBERT link hosted by *Hugging Face*: [https://huggingface.co/exbert](https://huggingface.co/exbert).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们点击由*Hugging Face*托管的exBERT链接：[https://huggingface.co/exbert](https://huggingface.co/exbert)。
- en: Enter the sentence **The cat is very sad.** and see the output, as follows:![Figure
    11.1 – exBERT interface ](img/B17123_11_001.jpg)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入句子**The cat is very sad.**并查看输出，如下所示：![图11.1 – exBERT界面 ](img/B17123_11_001.jpg)
- en: Figure 11.1 – exBERT interface
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.1 – exBERT界面
- en: In the preceding screenshot, the left tokens attend to the right tokens. The
    thickness of the lines represents the value of the weights. Because CLS and SEP
    tokens have very frequent and dense connections, we cut off the links associated
    with them for simplicity. Please see the **Hide Special Tokens** toggle switch.
    What we see now is the attention mapping at layer 1, where the lines correspond
    to the sum of the weights on all heads. This is called a **multi-head attention
    mechanism**, in which 12 heads work in parallel to each other. This mechanism
    allows us to capture a wider range of relationships than is possible with single-head
    attention. This is why we see a broadly attending pattern in *Figure 11.1*. We
    can also observe any specific head by clicking the **Head** column.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you hover over a token at the left, you will see the specific weights of
    that token connecting to the right ones. For more detailed information on using
    the interface, read the paper *exBERT: A Visual Analysis Tool to Explore Learned
    Representations in Transformer Models*, *Benjamin Hoover*, *Hendrik Strobelt*,
    *Sebastian Gehrmann*, *2019* or watch the video at the following link: [https://exbert.net/](https://exbert.net/).'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will try to support the findings of other researchers addressed in the
    introductory part of this section. Let's take the *some heads specialize in only
    attending the next or previous tokens, especially in earlier layers* pattern,
    and see if there's a head that supports this.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use **<Layer-No, Head-No>** notation to denote a certain self-attention
    head for the rest of the chapter, where the indices start at 1 for exBERT and
    start at 0 for BertViz—for example, *<3,7>* denotes the seventh head at the third
    layer for exBERT. When you select the **<2,5> (or <4,12> or <6,2 >)** head, you
    will get the following output, where each token attends to the previous token
    only:![Figure 11.2 – Previous-token attention pattern ](img/B17123_11_002.jpg)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 11.2 – Previous-token attention pattern
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the **<2, 12>** and **<3, 4>** heads, you will get a pattern whereby each
    token attends to the next token, as follows:![Figure 11.3 – Next-token attention
    pattern ](img/B17123_11_003.jpg)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 11.3 – Next-token attention pattern
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These heads serve the same functionality for other input sentences—that is,
    they work independently of the input. You can try different sentences yourself.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can use an attention head for advanced semantic tasks such as pronoun resolution
    using a **probing classifier**. First, we will qualitatively check if the internal
    representation has such a capacity for pronoun resolution (or coreference resolution)
    or not. Pronoun resolution is considered a challenging semantic relation task
    since the distance between the pronoun and its antecedent is usually very long.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we take the sentence *The cat is very sad. Because it could not find food
    to eat.* When you check each head, you will notice that the *<9,9>* and *<9,12>*
    heads encode the pronoun relation. When hovering over **it** at the *<9,9>* head,
    we get the following output:![Figure 11.4 – The coreference pattern at the <9,9>
    head ](img/B17123_11_004.jpg)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 11.4 – The coreference pattern at the <9,9> head
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The *<9,12>* head also works for pronoun relation. Again, on hovering over
    **it**, we get the following output:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The coreference pattern at the <9,12> head ](img/B17123_11_005.jpg)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.5 – The coreference pattern at the <9,12> head
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, we see that the **it** pronoun strongly attends
    to its antecedent, **cat**. We change the sentence a bit so that the **it** pronoun
    now refers to the **food** token instead of **cat**, as in **the cat did not eat
    the food because it was not fresh**. As seen in the following screenshot, which
    relates to the *<9,9>* head, **it** properly attends to its antecedent **food**,
    as expected:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – The pattern at the <9,9> head for the second example ](img/B17123_11_006.jpg)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.6 – The pattern at the <9,9> head for the second example
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's take another run, where the pronoun refers to **cat**, as in **The cat
    did not eat the food because it was very angry**. In the *<9,9>* head, the **it**
    token mostly attends to the **cat** token, as shown in the following screenshot:![Figure
    11.7 – The pattern at the <9,9> head for the second input ](img/B17123_11_007.jpg)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 11.7 – The pattern at the <9,9> head for the second input
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'I think those are enough examples. Now, we will use the exBERT model differently
    to evaluate the model capacity. Let''s restart the exBERT interface, select the
    last layer (*layer 12*), and keep all heads. Then, enter the sentence **the cat
    did not eat the food.** and mask out the **food** token. Double-clicking masks
    the **food** token out, as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Evaluating the model by masking ](img/B17123_11_008.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Evaluating the model by masking
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: When you hover on that masked token, you can see the prediction distribution
    of the *Bert-base-cased* model, as shown in the preceding screenshot. The first
    prediction is **food**, which is expected. For more detailed information about
    the tool, you can use exBERT's web page, at [https://exbert.net/](https://exbert.net/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Well done! In the next section, we will work with BertViz and write some Python
    code to access the attention heads.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Multiscale visualization of attention heads with BertViz
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will write some code to visualize heads with BertViz, which is a tool
    to visualize attention in the Transformer model, as is exBERT. It was developed
    by Jesse Vig in 2019 (*A Multiscale Visualization of Attention in the Transformer
    Model*, *Jesse Vig*, *2019*). It is the extension of the work of the Tensor2Tensor
    visualization tool (Jones, 2017). We can monitor the inner parts of a model with
    multiscale qualitative analysis. The advantage of BertViz is that we can work
    with most Hugging Face-hosted models (such as **Bidirectional Encoder Representations
    from Transformers** (**BERT**), **Generated Pre-trained Transformer** (**GPT**),
    and **Cross-lingual Language Model** (**XLM**)) through the Python **Application
    Programming Interface** (**API**). Therefore, we will be able to work with non-English
    models as well, or any pre-trained model. We will examine such examples together
    shortly. You can access BertViz resources and other information from the following
    GitHub link: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'As with exBERT, BertViz visualizes attention heads in a single interface. Additionally,
    it supports a **bird''s eye view** and a low-level **neuron view**, where we observe
    how individual neurons interact to build attention weights. A useful demonstration
    video can be found at the following link: [https://vimeo.com/340841955](https://vimeo.com/340841955).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting, we need to install the necessary libraries, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then import the following modules:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'BertViz supports three views: a **head view**, a **model view**, and a **neuron
    view**. Let''s examine these views one by one. First of all, though, it is important
    to point out that we started from 1 to index layers and heads in exBERT. But in
    BertViz, we start from 0 for indexing, as in Python programming. If I say a *<9,9>*
    head in exBERT, its BertViz counterpart is *<8,8>.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the head view.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Attention head view
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The head view is the BertViz equivalent of what we have experienced so far
    with exBERT in the previous section. The **attention head view** visualizes the
    attention patterns based on one or more attention heads in a selected layer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a `get_bert_attentions()` function to retrieve attentions
    and tokens for a given model and a given pair of sentences. The function definition
    is shown in the following code block:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the following code snippet, we load the `bert-base-cased` model and retrieve
    the tokens and corresponding attentions of the given two sentences. We then call
    the `head_view()` function at the end to visualize the attentions. Here is the
    code execution:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The code output is an interface, as displayed here:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Head-view output of BertViz ](img/B17123_11_009.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.9 – Head-view output of BertViz
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The interface on the left of *Figure 11.9* comes first. Hovering over any token
    on the left will show the attention going from that token. The colored tiles at
    the top correspond to the attention head. Double-clicking on any of them will
    select it and discard the rest. The thicker attention lines denote higher attention
    weights.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图11.9* 左侧是首先出现的界面。在任何左侧的令牌上悬停将显示从该令牌出发的注意力。顶部的彩色块对应于注意力头。双击其中的任何一个将选择它并丢弃其余部分。较粗的注意力线表示更高的注意力权重。'
- en: Please remember that in the preceding exBERT examples, we observed that the
    *<9,9>* head (the equivalent in BertViz is *<8, 8>*, due to indexing) bears a
    pronoun-antecedent relationship. We observe the same pattern in *Figure 11.9*,
    selecting layer 8 and head 8\. Then, we see the interface on the right of *Figure
    11.9* when we hover on *it*, where *it* strongly attends to the *cat* and *it*
    tokens. So, can we observe these semantic patterns in other pre-trained language
    models? Although the heads are not exactly the same in other models, some heads
    can encode these semantic properties. We also know from recent work that semantic
    features are mostly encoded in the higher layers.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，在前面的exBERT示例中，我们观察到 *<9,9>* 头（由于索引的原因，在BertViz中等效于 *<8, 8>*）具有代词-先行关系。我们观察到相同的模式在
    *图11.9* 中，选择第8层和第8头。然后，当我们悬停在 *它* 上时，在 *图11.9* 的右侧看到了界面，*它* 强烈关注 *cat* 和 *it*
    令牌。那么，在其他预训练语言模型中我们能观察到这些语义模式吗？虽然其它模型中头部不一定完全相同，但一些头部可能会编码这些语义特性。我们也从最近的工作中得知，语义特征主要编码在较高的层中。
- en: 'Let''s look for a coreference pattern in a Turkish language model. The following
    code loads a Turkish `bert-base` model and takes a sentence pair. We observe here
    that the *<8,8>* head has the same semantic feature in Turkish as in the English
    model, as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在土耳其语言模型中寻找一个指代模式。以下代码加载了一个土耳其`bert-base`模型并取了一个句子对。我们在这里观察到，*<8,8>* 头在土耳其语言模型中具有与英语模型相同的语义特征，如下所示：
- en: '[PRE4]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the preceding code, `sentence_a` and `sentence_b` mean *The cat is sad*
    and *Because it ate too much food as usual*, respectively. When hovering over
    **o** (**it**), **it** attends to **Kedi** (**cat**), as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的代码中，`sentence_a` 和 `sentence_b` 意味着 *The cat is sad* 和 *Because it ate too
    much food as usual*，当悬停在 **o**（**它**）上时，**它** 关注 **Kedi**（**猫**），如下所示：
- en: '![Figure 11.10 – Coreference pattern in the Turkish language model ](img/B17123_11_010.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图11.10 – 土耳其语言模型中的指代模式](img/B17123_11_010.jpg)'
- en: Figure 11.10 – Coreference pattern in the Turkish language model
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.10 – 土耳其语言模型中的指代模式
- en: All other tokens except **o** mostly attend to the SEP delimiter token, which
    is a dominant behavior pattern in all heads in the BERT architecture.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了 **o** 之外的所有其他标记大多数关注到了SEP分隔符标记，这是BERT架构中所有头部中的一种主导行为模式。
- en: As a final example for the head view, we will interpret another language model
    and move on to the model view feature. This time, we choose the `bert-base-german-cased`
    German language model and visualize it for the input—that is, the German equivalent
    of the same-sentence pair we used for Turkish.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为头视图的最后一个示例，我们将解释另一个语言模型，并继续查看模型视图特性。这次，我们选择`bert-base-german-cased`德语语言模型并将其可视化为输入——即，我们用于土耳其语的同一个句对的德语等价句对。
- en: 'The following code loads a German model, consumes a pair of sentences, and
    visualizes them:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码加载了一个德语模型，消耗了一对句子，并对其进行了可视化：
- en: '[PRE5]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When we examine the heads, we can see the coreference pattern in the 8th layer
    again, but this time in the 11th head. To select the *<8,11>* head, pick layer
    8 from the drop-down menu and double-click on the last head, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们检查头部时，我们可以再次在第8层中看到指代模式，但这次是在第11个头部。要选择 *<8,11>* 头部，从下拉菜单中选择第8层并双击最后的头部，如下所示：
- en: '![Figure 11.11 – Coreference relation pattern in the German language model
    ](img/B17123_11_011.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 德语语言模型中的指代关系模式](img/B17123_11_011.jpg)'
- en: Figure 11.11 – Coreference relation pattern in the German language model
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 德语语言模型中的指代关系模式
- en: As you see, when hovering over **sie**, you will see strong attentions to the
    **Die Katze**. While this *<8,11>* head is the strongest one for coreference relations
    (known as anaphoric relations in computational linguistics literature), this relationship
    may have spread to many other heads. To observe it, we will have to check all
    the heads one by one.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在悬停在 **sie** 上时，你会看到对 **Die Katze** 的强烈关注。虽然这个 *<8,11>* 头是最强大的指代关系头（在计算语言学文献中被称为先行关系），这种关系可能已经传播到许多其他头中。为了观察它，我们必须逐个检查所有的头。
- en: On the other hand, BertViz's model view feature gives us a basic bird's-eye
    view to see all heads at once. Let's take a look at it in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Model view
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Model view** allows us to have a bird''s-eye view of attentions across all
    heads and layers. Self-attention heads are shown in tabular form, with rows and
    columns corresponding to layers and heads, respectively. Each head is visualized
    in the form of a clickable thumbnail that includes the broad shape of the attention
    model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'The view can tell us how BERT works and makes it easier to interpret. Many
    recent studies, such as *A Primer in BERTology: What We Know About How BERT Works*,
    *Anna Rogers*, *Olga Kovaleva*, *Anna Rumshisky*, *2021*, found some clues about
    the behavior of the layers and came to a consensus. We already listed some of
    them in the *Interpreting attention heads* section. You can test these facts yourself
    using BertViz''s model view.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s view the German language model that we just used, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following modules:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will use a `show_model_view()` wrapper function developed by Jesse
    Vig. You can find the original code at the following link: [https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb](https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb).'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can also find the function definition in our book''s GitHub link, at [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH11).
    We are just dropping the function header here:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s load the German model again. If you have already loaded it, you can
    skip the first five lines. Here is the code you''ll need:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the output:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.12 – The model view of the German language model ](img/B17123_11_012.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.12 – The model view of the German language model
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This view helps us easily observe many patterns such as next-token (or previous-token)
    attention patterns. As we mentioned earlier in the *Interpreting attention heads*
    section, tokens often tend to attend to delimiters—specifically, CLS delimiters
    at lower layers and SEP delimiters at upper layers. Because these tokens are not
    masked out, they can ease the flow of information. In the last layers, we only
    observe SEP-delimiter-focused attention patterns. It could be speculated that
    SEP is used to collect segment-level information, which can be used then for inter-sentence
    tasks such as **Next Sentence Prediction** (**NSP**) or for encoding sentence-level
    meaning.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other hand, we observe that coreference relation patterns are mostly
    encoded in the *<8,1>, <8,11>, <10,1>, and <10,7>* heads. Again, it can be clearly
    said that the *<8, 11>* head is the strongest head that encodes the coreference
    relation in the German model, which we already discussed.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When you click on that thumbnail, you will see the same output, as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Close-up of the <8,11> head in model view  ](img/B17123_11_013.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – 模型视图中<8,11>头部的放大图](img/B17123_11_013.jpg)'
- en: Figure 11.13 – Close-up of the <8,11> head in model view
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – 模型视图中<8,11>头部的放大图
- en: You can again hover over the tokens and see the mappings.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以再次悬停在标记上，查看映射关系。
- en: I think that's enough work for the head view and the model view. Now, let's
    deconstruct the model with the help of the neuron view and try to understand how
    these heads calculate weights.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为头视图和模型视图的工作已经足够了。现在，让我们通过神经元视图拆解模型，并试图理解这些头部是如何计算权重的。
- en: Neuron view
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元视图
- en: So far, we have visualized computed weights for a given input. The **neuron
    view** visualizes the neurons and the key vectors in a query and how the weights
    between tokens are computed based on interactions. We can trace the computation
    phase between any two tokens.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经可视化了给定输入的计算权重。**神经元视图**可视化了查询中的神经元和关键向量，以及基于相互作用计算标记之间的权重。我们可以追踪任意两个标记之间的计算阶段。
- en: 'Again, we will load the German model and visualize the same-sentence pair we
    just worked with, to be coherent. We execute the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载德语模型，并可视化刚刚处理过的同一句对，以保持连贯性。我们执行以下代码：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出：
- en: '![Figure 11.14 – Neuron view of the coreference relation pattern (head <8,11>)
    ](img/B17123_11_014.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 核指关系模式的神经元视图（头部<8,11>）](img/B17123_11_014.jpg)'
- en: Figure 11.14 – Neuron view of the coreference relation pattern (head <8,11>)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 核指关系模式的神经元视图（头部<8,11>）
- en: The view helps us to trace the computation of attention from the **sie** token
    that we selected on the left to the other tokens on the right. Positive values
    are blue and negative values are orange. Color intensity represents the magnitude
    of the numerical value. The query of **sie** is very similar to the keys of **Die**
    and **Katze**. If you look at the patterns carefully, you will notice how similar
    these vectors are. Therefore, their dot product goes higher than the other comparison,
    which establishes strong attention between those tokens. We also trace the dot
    product and the Softmax function output as we go to the right. When clicking on
    the other tokens on the left, you can trace other computations as well.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图帮助我们追踪了我们在左边选择的**sie**标记到右边其他标记的注意力计算。正值是蓝色的，负值是橙色的。颜色的强度代表了数值的大小。查询**sie**与**Die**和**Katze**的键非常相似。如果你仔细观察图案，你会注意到这些向量有多么相似。因此，它们的点积要比其他的比较更高，这建立了这些标记之间强烈的注意力。我们还可以追踪点积和Softmax函数的输出，当我们向右移动时。当点击左边的其他标记时，你也可以追踪其他的计算。
- en: 'Now, let''s select a head-bearing next-token attention pattern for the same
    input, and trace it. To do so, we select the *<2,6>* head. In this pattern, virtually
    all the attention is focused on the next word. We click the **sie** token once
    again, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为相同的输入选择一个头部承载的下一个标记的注意力模式，并对其进行追踪。为此，我们选择*<2,6>* 头部。在这个模式中，几乎所有的注意力都集中在下一个字上。我们再次点击**sie**标记，如下所示：
- en: '![Figure 11.15 – Neuron view of next-token attention patterns (the <2,6> head)
    ](img/B17123_11_015.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – 下一个标记注意力模式的神经元视图（<2,6>头部）](img/B17123_11_015.jpg)'
- en: Figure 11.15 – Neuron view of next-token attention patterns (the <2,6> head)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 下一个标记注意力模式的神经元视图（<2,6>头部）
- en: Now, the **sie** token is focused on the next token instead of its own antecedent
    (**Die Katze**). When we carefully look at the query and the candidate keys, the
    most similar key to the query of **sie** is the next token, **zu**. Likewise,
    we observe how the dot product and Softmax function are applied in order.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**sie** 标记的注意力集中在下一个标记上，而不是它自己的先行词(**Die Katze**)。当我们仔细观察查询和候选键时，我们会发现与**sie**的查询最相似的键是下一个标记**zu**。同样地，在这过程中我们可以看到点乘和Softmax函数是如何应用的。
- en: In the next section, we will briefly talk about probing classifiers for interpreting
    Transformers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节，我们将简要讨论解释变压器（Transformers）的探针分类器。
- en: Understanding the inner parts of BERT with probing classifiers
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过解读变压器的探针分类器理解BERT的内部部分
- en: The opacity of what DL learns has led to a number of studies on the interpretation
    of such models. We attempt to answer the question of which parts of a Transformer
    model are responsible for certain language features, or which parts of the input
    lead the model to make a particular decision. To do so, other than visualizing
    internal representations, we can train a classifier on the representations to
    predict some external morphological, syntactic, or semantic properties. Hence,
    we can determine if we associate internal *representations* with external *properties*.
    The successful training of the model would be quantitative evidence of such an
    association—that is, the language model has learned information relevant for an
    external property. This approach is called a **probing-classifier** approach,
    which is a prominent analysis technique in NLP and other DL studies. An attention-based
    probing classifier takes an attention map as input and predicts external properties
    such as coreference relations or head-modifier relations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding experiments, we get the self-attention weights for
    a given input with the `get_bert_attention()` function. Instead of visualizing
    these weights, we can directly transfer them to a classification pipeline. So,
    with supervision, we can determine which head is suitable for which semantic feature—for
    example, we can figure out which heads are suitable for coreference with labeled
    data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the model-tracking part, which is crucial for building
    efficient models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model metrics
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have trained language models and simply analyzed the final results.
    We have not observed the training process or made a comparison of training using
    different options. In this section, we will briefly discuss how to monitor model
    training. For this, we will handle how to track the training of the models we
    developed before in [*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081), *Fine-Tuning
    Language Models for Text Classification*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: There are two important tools developed in this area—one is TensorBoard, and
    the other is W&B. With the former, we save the training results to a local drive
    and visualize them at the end of the experiment. With the latter, we are able
    to monitor the model-training progress live in a cloud platform.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: This section will be a short introduction to these tools without going into
    much detail about them, as this is beyond the scope of this chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with TensorBoard.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model training with TensorBoard
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard is a visualization tool specifically for DL experiments. It has
    many features such as tracking, training, projecting embeddings to a lower space,
    and visualizing model graphs. We mostly use it for tracking and visualizing metrics
    such as loss. Tracking a metric with TensorBoard is so easy for Transformers that
    adding a couple of lines to model-training code will be enough. Everything is
    kept almost the same.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will repeat the **Internet Movie Database** (**IMDb**) sentiment fine-tuning
    experiment we did in [*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081),
    *Fine-Tuning Language Models for Text Classification*, and will track the metrics.
    In that chapter, we already trained a sentiment model with an IMDb dataset consisting
    of a **4 kilo** (**4K**) training dataset, a 1K validation set, and a 1K test
    set. Now, we will adapt it to TensorBoard. For more details about TensorBoard,
    please visit [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we install TensorBoard if it is not already installed, like this:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Keeping the other code lines of IMDb sentiment analysis as-is from [*Chapter
    5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081), *Fine-Tuning Language Models for
    Text Classification*, we set the training argument as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code snippet, the value of `logging_dir` will soon be passed
    to TensorBoard as a parameter. As the training dataset size is 4K and the training
    batch size is 16, we have 250 steps (4K/16) for each epoch, which means 750 steps
    for three epochs.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set `logging_steps` to 50, which is a sampling interval. As the interval
    is decreased, more details about where model performance rises or falls are recorded.
    We'll do another experiment later on, reducing this sampling interval at step
    27\.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, at every 50 steps, the model performance is measured in terms of the metrics
    that we define in `compute_metrics()`. The metrics to be measured are Accuracy,
    F1, Precision, and Recall. As a result, we will have 15 (750/50) performance measurements
    to be recorded. When we run `trainer.train()`, this starts the training process
    and records the logs under the `logging_dir='./logs'` directory.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set `load_best_model_at_end` to `True` so that the pipeline loads whichever
    checkpoint has the best performance in terms of loss. Once the training is completed,
    you will notice that the best model is loaded from `checkpoint-250` with a loss
    score of `0.263`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, the only thing we need to do is to call the following code to launch TensorBoard:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the output:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.16 – TensorBoard visualization for training history ](img/B17123_11_016.jpg)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.16 – TensorBoard visualization for training history
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you may have noticed, we can trace the metrics that we defined before. The
    horizontal axis goes from 0 to 750 steps, which is what we calculated before.
    We will not discuss TensorBoard in detail here. Let''s just look at the **eval/loss**
    chart only. When you click on the maximization icon at the left-hand bottom corner,
    you will see the following chart:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.17 – TensorBoard eval/loss chart for logging steps of 50  ](img/B17123_11_017.jpg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.17 – TensorBoard eval/loss chart for logging steps of 50
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the preceding screenshot, we set the smoothing to 0 with the slider control
    on the left of the TensorBoard dashboard to see scores more precisely and focus
    on the global minimum. If your experiment has very high volatility, the smoothing
    feature can work well to see overall trends. It functions as a **Moving Average**
    (**MA**). This chart supports our previous observation, in which the best loss
    measurement is **0.2658** at step **250**.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As `logging_steps` is set to **10**, we get a high resolution, as in the following
    screenshot. As a result, we will have 75 (750 steps/10 steps) performance measurements
    to be recorded. When we rerun the entire flow with this resolution, we get the
    best model at step 220, with a loss score of 0.238, which is better than the previous
    experiment. The result can be seen in the following screenshot. We naturally observe
    more fluctuations due to higher resolution:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Higher-resolution eval/loss chart for logging steps of 10
    ](img/B17123_11_018.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Higher-resolution eval/loss chart for logging steps of 10
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We are done with TensorBoard for now. Let's work with W&B!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model training live with W&B
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: W&B, unlike TensorBoard, provides a dashboard in a cloud platform, and we can
    trace and back up all experiments in a single hub. It also allows us to work with
    a team for development and sharing. The training code is run on our local machine,
    while the logs are kept in the W&B cloud. Most importantly, we can follow the
    training process live and share the result immediately with the community or team.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'We can enable W&B for our experiments by making very small changes to our existing
    code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we need to create an account in `wandb.ai`, then install the
    Python library, as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Again, we will take the IMDb sentiment-analysis code and make minor changes
    to it. First, let''s import the library and log in to `wandB`, as follows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`wandb` requests an API key that you can easily find at the following link:
    [https://wandb.ai/authorize](https://wandb.ai/authorize).'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Alternatively, you can set the `WANDB_API_KEY` environment variable to your
    API key, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Again, keeping the entire code as-is, we only add two parameters, `report_to="wandb"`
    and `run_name="..."`, to `TrainingArguments`, which enables logging in to W&B,
    as shown in the following code block:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, as soon as you call `trainer.train()`, logging starts on the cloud. After
    the call, please check the cloud dashboard and see how it changes. Once the `trainer.train()`
    call has completed successfully, we execute the following line to tell `wandB`
    we are done:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The execution also outputs run history locally, as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.19 – The local output of W&B ](img/B17123_11_019.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – The local output of W&B
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'When you connect to the link provided by W&B, you will get to an interface
    that looks something like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – The online visualization of a single run on the W&B dashboard
    ](img/B17123_11_020.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – The online visualization of a single run on the W&B dashboard
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: This visualization gives us a summarized performance result for a single run.
    As you see, we can trace the metrics that we defined in the `compute_metric()`
    function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the evaluation loss. The following screenshot shows
    exactly the same plot that TensorBoard provided, where the minimum loss is around
    0.2658, occurring at step 250:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – The eval/loss plot of IMDb experiment on the W&B dashboard  ](img/B17123_11_021.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – The eval/loss plot of IMDb experiment on the W&B dashboard
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We have only visualized a single run so far. W&B allows us to explore the results
    dynamically across lots of runs at once—for example, we can visualize the results
    of models using different hyperparameters such as learning rate or batch size.
    To do so, we instantiate a `TrainingArguments` object properly with another different
    hyperparameter setting and change `run_name="..."` accordingly for each run.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows our several IMDb sentiment-analysis runs using
    different hyperparameters. We can also see the batch size and learning rate that
    we changed:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – Exploring the results across several runs on the W&B dashboard
    ](img/B17123_11_022.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – Exploring the results across several runs on the W&B dashboard
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'W&B provides useful functionality—for instance, it automates hyperparameter
    optimization and searching the space of possible models, called W&B Sweeps. Other
    than that, it also provides system logs relating to **Graphics Processing Unit**
    (**GPU**) consumption, **Central Processing Unit** (**CPU**) utilization, and
    so on. For more detailed information, please check the following website: [https://wandb.ai/home](https://wandb.ai/home).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Well done! In the last section, *References*, we will focus more on technical
    tools, since it's crucial to use such utility tools to develop better models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced two different technical concepts: attention
    visualization and experiment tracking. We visualized attention heads with the
    exBERT online interface first. Then, we studied BertViz, where we wrote Python
    code to see three BertViz visualizations: head view, model view, and neuron view.
    The BertViz interface gave us more control so that we could work with different
    language models. Moreover, we were also able to observe how attention weights
    between tokens are computed. These tools provide us with important functions for
    interpretability and exploitability. We also learned how to track our experiments
    to obtain higher-quality models and do error analysis. We utilized two tools to
    monitor training: TensorBoard and W&B. These tools were used to effectively track
    experiments and to optimize model training.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You've finished reading this book by demonstrating great perseverance
    and persistence throughout this journey. You can now feel confident as you are
    well equipped with the tools you need, and you are prepared for developing and
    implementing advanced NLP applications.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
    Models*, *Benjamin Hoover*, *Hendrik Strobelt*, *Sebastian Gehrmann*, *2019*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vig, J.*, *2019*. *A multiscale visualization of attention in the Transformer
    model*. *arXiv preprint arXiv:1906.05714*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clark, K.*, *Khandelwal, U.*, *Levy, O.* and *Manning, C.D.*, *2019*. *What
    does bert look at? An analysis of bert''s attention*. *arXiv preprint arXiv:1906.04341.7*'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Biewald, L.*, *Experiment tracking with weights and biases*, *2020*. Software
    available from `wandb.com`, *2(5)*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rogers, A.*, *Kovaleva, O.* and *Rumshisky, A.*,*2020*. *A primer in BERTology:
    What we know about how BERT works*. *Transactions of the Association for Computational
    Linguistics*, *8*, *pp.842-866*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W&B*: [https://wandb.ai](https://wandb.ai)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorBoard*: [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exBert—Hugging Face*: [https://huggingface.co/exbert](https://huggingface.co/exbert)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exBERT*: [https://exbert.net/](https://exbert.net/)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
