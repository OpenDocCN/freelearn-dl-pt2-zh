- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Blocks of Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The wide range of generative AI models that we will implement in this book
    are all built on the foundation of advances over the last decade in *deep learning*
    and neural networks. While in practice we could implement these projects without
    reference to historical developments, it will give you a richer understanding
    of *how* and *why* these models work to retrace their underlying components. In
    this chapter, we will dive into this background, showing you how generative AI
    models are built from the ground up, how smaller units are assembled into complex
    architectures, how the loss functions in these models are optimized, and some
    current theories as to why these models are so effective. Armed with this background
    knowledge, you should be able to understand in greater depth the reasoning behind
    the more advanced models and topics that start in *Chapter 4**,* *Teaching Networks
    to Generate Digits*, of this book. Generally speaking, we can group the building
    blocks of neural network models into a number of choices regarding how the model
    is constructed and trained, which we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Which neural network architecture to use:'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilayer perceptron** (**MLP**)/feedforward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory Networks** (**LSTMs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gated Recurrent Units** (**GRUs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which activation functions to use in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What optimization algorithm to use to tune the parameters of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** (**SGD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaGrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ADAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaDelta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hessian-free optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to initialize the parameters of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xavier initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can appreciate, the products of these decisions can lead to a huge number
    of potential neural network variants, and one of the challenges of developing
    these models is determining the right search space within each of these choices.
    In the course of describing the history of neural networks we will discuss the
    implications of each of these model parameters in more detail. Our overview of
    this field begins with the origin of the discipline: the humble perceptron model.'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons – a brain in a function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest neural network architecture – the perceptron – was inspired by
    biological research to understand the basis of mental processing in an attempt
    to represent the function of the brain with mathematical formulae. In this section
    we will cover some of this early research and how it inspired what is now the
    field of deep learning and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: From tissues to TLUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recent popularity of AI algorithms might give the false impression that
    this field is new. Many recent models are based on discoveries made decades ago
    that have been reinvigorated by the massive computational resources available
    in the cloud and customized hardware for parallel matrix computations such as
    **Graphical Processing Units** (**GPUs**), **Tensor Processing Units** (**TPUs**),
    and **Field Programmable Gate Array** (**FPGAs**). If we consider research on
    neural networks to include their biological inspiration as well as computational
    theory, this field is over a hundred years old. Indeed, one of the first neural
    networks described appears in the detailed anatomical illustrations of 19th Century
    scientist Santiago Ramón y Cajal, whose illustrations based on experimental observations
    of layers of interconnected neuronal cells inspired the Neuron Doctrine – the
    idea that the brain is composed of individual, physically distinct and specialized
    cells, rather than a single continuous network.¹ The distinct layers of the retina
    observed by Cajal were also the inspiration for particular neural network architectures
    such as the CNN, which we will discuss later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: The networks of interconnected neurons illustrated by Santiago
    Ramón y Cajal³'
  prefs: []
  type: TYPE_NORMAL
- en: 'This observation of simple neuronal cells interconnected in large networks
    led computational researchers to hypothesize how mental activity might be represented
    by simple, logical operations that, combined, yield complex mental phenomena.
    The original "automata theory" is usually traced to a 1943 article by Warren McCulloch
    and Walter Pitts of the Massachusetts Institute of Technology.³ They described
    a simple model known as the **Threshold Logic Unit** (**TLU**), in which binary
    inputs are translated into a binary output based on a threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where *I* is the input values, *W* is the weights with ranges from (0, 1) or
    (-1, 1), and f is a threshold function that converts these inputs into a binary
    output depending upon whether they exceed a threshold *T*:⁴
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Visually and conceptually, there is some similarity between McCulloch and Pitts'
    model and the biological neuron that inspired it (*Figure 3.2*). Their model integrates
    inputs into an output signal, just as the natural dendrites (short, input "arms"
    of the neuron that receive signals from other cells) of a neuron synthesize inputs
    into a single output via the axon (the long "tail" of the cell, which passes signals
    received from the dendrites along to other neurons). We might imagine that, just
    as neuronal cells are composed into networks to yield complex biological circuits,
    these simple units might be connected to simulate sophisticated decision processes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_02_a+b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: The TLU model and the biological neuron^(5 6)'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, using this simple model, we can already start to represent several logical
    operations. If we consider a simple case of a neuron with one input, we can see
    that a TLU can solve an identity or negation function (*Tables 3.1* and *3.2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'For an identity operation that simply returns the input as output, the weight
    matrix would have 1s on the diagonal (or be simply the scalar 1, for a single
    numerical input, as illustrated in *Table 1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Identity |'
  prefs: []
  type: TYPE_TB
- en: '| Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: TLU logic for identity operations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for a negation operation, the weight matrix could be a negative
    identity matrix, with a threshold at 0 flipping the sign of the output from the
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Negation |'
  prefs: []
  type: TYPE_TB
- en: '| Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.2: TLU logic for negation operations'
  prefs: []
  type: TYPE_NORMAL
- en: Given two inputs, a TLU could also represent operations such as AND and OR.
    Here, a threshold could be set such that combined input values either have to
    exceed `2` (to yield an output of `1`) for an AND operation (*Table 3.3*) or `1`
    (to yield an output of `1` if either of the two inputs are `1`) in an OR operation
    (*Table 3.4*).
  prefs: []
  type: TYPE_NORMAL
- en: '| AND |'
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.3: TLU logic for AND operations'
  prefs: []
  type: TYPE_NORMAL
- en: '| OR |'
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.4: TLU logic for OR operations'
  prefs: []
  type: TYPE_NORMAL
- en: However, a TLU cannot capture patterns such as `Exclusive OR` (`XOR`), which
    emits `1` if and *only if* the `OR` condition is true (*Table 3.5*).
  prefs: []
  type: TYPE_NORMAL
- en: '| XOR |'
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.5: TLU logic for XOR operations'
  prefs: []
  type: TYPE_NORMAL
- en: To see why this is true, consider a TLU with two inputs and positive weights
    of `1` for each unit. If the threshold value `T` is `1`, then inputs of (`0`,
    `0`), (`1`, `0`), and (`0`, `1`) will yield the correct value. What happens with
    (`1`, `1`) though? Because the threshold function returns `1` for any inputs summing
    to greater than `1`, it cannot represent `XOR` (*Table 3.5*), which would require
    a second threshold to compute a different output once a different, higher value
    is exceeded. Changing one or both of the weights to negative values won't help
    either; the problem is that the decision threshold operates only in one direction
    and can't be reversed for larger inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the TLU can''t represent the negation of the `Exclusive NOR`, `XNOR`
    (*Table 3.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| XNOR |'
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.6: TLU logic for XNOR operations'
  prefs: []
  type: TYPE_NORMAL
- en: As with the `XOR` operation (*Table 3.5*), the impossibility of the `XNOR` operation
    (*Table 3.6*) being represented by a TLU function can be illustrated by considering
    a weight matrix of two 1s; for two inputs (1, 0) or (0, 1), we obtain the correct
    value if we set a threshold of 2 for outputting 1\. As with the `XOR` operation,
    we run into a problem with an input of (0, 0), as we can't set a second threshold
    to output 1 at a sum of 0.
  prefs: []
  type: TYPE_NORMAL
- en: From TLUs to tuning perceptrons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides these limitations for representing the `XOR` and `XNOR` operations,
    there are additional simplifications that cap the representational power of the
    TLU model; the weights are fixed, and the output can only be binary (0 or 1).
    Clearly, for a system such as a neuron to "learn," it needs to respond to the
    environment and determine the relevance of different inputs based on feedback
    from prior experiences. This idea was captured in the 1949 book *Organization
    of Behavior* by Canadian Psychologist Donald Hebb, who proposed that the activity
    of nearby neuronal cells would tend to synchronize over time, sometimes paraphrased
    at Hebb''s Law: *Neurons that fire together wire together*^(7 8). Building on
    Hebb''s proposal that weights changed over time, researcher Frank Rosenblatt of
    the Cornell Aeronautical Laboratory proposed the perceptron model in the 1950s.⁹
    He replaced the fixed weights in the TLU model with adaptive weights and added
    a bias term, giving a new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We note that the inputs *I* have been denoted *X* to underscore the fact that
    they could be any value, not just binary `0` or `1`. Combining Hebb''s observations
    with the TLU model, the weights of the perceptron would be updated according to
    a simple learning rule:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a set of J samples *x*(1) …. x(*j*). These samples all have a label
    y which is 0 or 1, giving labeled data (*y*, *x*)(1) …. (*y*, *x*)(*j*). These
    samples could have either a single value, in which case the perceptron has a single
    input, or be a vector with length *N* and indices *i* for multi-value input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize all weights *w* to a small random value or 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the estimated value, *yhat*, for all the examples *x* using the perceptron
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the weights using a learning rate *r* to more closely match the input
    to the desired output for each step *t* in training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B16176_03_004.png), for all *J* samples and *N* features. Conceptually,
    note that if *y* is 0 and the target is 1, we want to increase the value of the
    weight by some increment *r*; likewise, if the target is 0 and the estimate is
    1, we want to decrease the weight so the inputs do not exceed the threshold.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Repeat *steps 3-4* until the difference between the predicted and actual outputs,
    *y* and *yhat*, falls below some desired threshold. In the case of a non-zero
    bias term, *b*, an update can be computed as well using a similar formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While simple, you can appreciate that many patterns could be learned from such
    a classifier, though still not the `XOR` function. However, by combining several
    perceptrons into multiple layers, these units could represent any simple Boolean
    function,^(10) and indeed McCulloch and Pitts had previously speculated on combining
    such simple units into a universal computation engine, or Turing Machine, that
    could represent any operation in a standard programming language. However, the
    preceding learning algorithm operates on each unit independently, meaning it could
    be extended to networks composed of many layers of perceptrons (*Figure 3.3*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: A multi-layer perceptron^(11)'
  prefs: []
  type: TYPE_NORMAL
- en: However, the 1969 book *Perceptrons*, by MIT computer scientists Marvin Minksy
    and Seymour Papert, demonstrated that a three-layer feed-forward network required
    complete (non-zero weight) connections between at least one of these units (in
    the first layer) and all inputs to compute all possible logical outputs^(12).
    This meant that instead of having a very sparse structure, like biological neurons,
    which are only connected to a few of their neighbors, these computational models
    required very dense connections.
  prefs: []
  type: TYPE_NORMAL
- en: While connective sparsity has been incorporated in later architectures, such
    as CNNs, such dense connections remain a feature of many modern models too, particularly
    in the *fully connected* layers that often form the second to last hidden layers
    in models. In addition to these models being computationally unwieldy on the hardware
    of the day, the observation that sparse models could not compute all logical operations
    was interpreted more broadly by the research community as *Perceptrons cannot
    compute XOR*. While erroneous,^(13) this message led to a drought in funding for
    AI in subsequent years, a period sometimes referred to as the **AI Winter**^(14).
  prefs: []
  type: TYPE_NORMAL
- en: The next revolution in neural network research would require a more efficient
    way to compute the required parameters updated in complex models, a technique
    that would become known as **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptrons and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While large research funding for neural networks declined until the 1980s after
    the publication of *Perceptrons*, researchers still recognized that these models
    had value, particularly when assembled into multi-layer networks, each composed
    of several perceptron units. Indeed, when the mathematical form of the output
    function (that is, the output of the model) was relaxed to take on many forms
    (such as a linear function or a sigmoid), these networks could solve both regression
    and classification problems, with theoretical results showing that 3-layer networks
    could effectively approximate any output.^(15) However, none of this work addressed
    the practical limitations of computing the solutions to these models, with rules
    such as the perceptron learning algorithm described earlier proving a great limitation
    to the applied use of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Renewed interest in neural networks came with the popularization of the backpropagation
    algorithm, which, while discovered in the 1960s, was not widely applied to neural
    networks until the 1980s, following several studies highlighting its usefulness
    for learning the weights in these models.^(16) As you saw with the perceptron
    model, a learning rule to update weights is relatively easy to derive as long
    as there are no "hidden" layers. The input is transformed once by the perceptron
    to compute an output value, meaning the weights can be directly tuned to yield
    the desired output. When there are hidden layers between the input and output,
    the problem becomes more complex: when do we change the internal weights to compute
    the activations that feed into the final output? How do we modify them in relation
    to the input weights?'
  prefs: []
  type: TYPE_NORMAL
- en: The insight of the backpropagation technique is that we can use the chain rule
    from calculus to efficiently compute the derivatives of each parameter of a network
    with respect to a loss function and, combined with a learning rule, this provides
    a scalable way to train multilayer networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s illustrate backpropagation with an example: consider a network like
    the one shown in *Figure 3.3*. Assume that the output in the final layer is computed
    using a sigmoidal function, which yields a value between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, the value *y*, the sum of the inputs to the final neuron, is a
    weighted sum of the sigmoidal inputs of the hidden units:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also need a notion of when the network is performing well or badly at its
    task. A straightforward error function to use here is squared loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: where *yhat* is the estimated value (from the output of the model) and *y* is
    the real value, summed over all the input examples *J* and the outputs of the
    network *K* (where *K=1*, since there is only a single output value). Backpropagation
    begins with a "forward pass" where we compute the values of all the outputs in
    the inner and outer layers, to obtain the estimated values of *yhat*. We then
    proceed with a backward step to compute gradients to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our overall objective is to compute partial derivatives for the weights *w*
    and bias terms b in each neuron: ![](img/B16176_03_008.png) and ![](img/B16176_03_009.png),
    which will allow us to compute the updates for *b* and *w*. Towards this goal,
    let''s start by computing the update rule for the inputs in the final neuron;
    we want to date the partial derivative of the error *E* with respect to each of
    these inputs (in this example there are five, corresponding to the five hidden
    layer neurons), using the chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can get the value ![](img/B16176_03_011.png) by differentiating the loss
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which for an individual example is just the difference between the input and
    output value. For ![](img/B16176_03_013.png), we need to take the partial derivative
    of the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_014.png)![](img/B16176_03_015.png)![](img/B16176_03_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting it all together, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to compute the gradient for a particular parameter of *x*, such
    as a weight *w* or bias term *b*, we need one more step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We already know the first term and *x* depends on *w* only through the inputs
    from the lower layers *y* since it is a linear function, so we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to compute this derivative for one of the neurons in the hidden
    layer, we likewise take the partial derivative with respect to this input *y*[i],
    which is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in total we can sum over all units that feed into this hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_021.png)'
  prefs: []
  type: TYPE_IMG
- en: We can repeat this process recursively for any units in deeper layers to obtain
    the desired update rule, since we now know how to calculate the gradients for
    *y* or *w* at any layer. This makes the process of updating weights efficient
    since once we have computed the gradients through the backward pass we can combine
    consecutive gradients through the layers to get the required gradient at any depth
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the gradients for each *w* (or other parameter of the neuron
    we might want to calculate), how can we make a "learning rule" to update the weights?
    In their paper,^(17) Hinton et al. noted that we could apply an update to the
    model parameters after computing gradients on each sample batch but suggested
    instead applying an update calculated after averaging over all samples. The gradient
    represents the direction in which the error function is changing with the greatest
    magnitude with respect to the parameters; thus, to update, we want to push the
    weight in the *opposite* direction, with ![](img/B16176_03_022.png) the update,
    and *e* a small value (a step size):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then at each time *t* during training we update the weight using this calculated
    gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Extending this approach, Hinton et al. proposed an exponentially weighted update
    of the current gradient plus prior updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_025.png)'
  prefs: []
  type: TYPE_IMG
- en: where alpha is a decay parameter to weight the contribution of prior updates
    ranging from 0 to 1\. Following this procedure, we would initialize the weights
    in the network with some small random values, choose a step size *e* and iterate
    with forward and backward passes, along with updates to the parameters, until
    the loss function reaches some desired value.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have described the formal mathematics behind backpropagation, let
    us look at how it is implemented in practice in software packages such as TensorFlow
    2.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While it is useful to go through this derivation in order to understand how
    the update rules for a deep neural network are derived, this would clearly quickly
    become unwieldy for large networks and complex architectures. It''s fortunate,
    therefore, that TensorFlow 2 handles the computation of these gradients automatically.
    During the initialization of the model, each gradient is computed as an intermediate
    node between tensors and operations in the graph: as an example, see *Figure 3.4:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Inserting gradient operations into the TensorFlow graph^(18)'
  prefs: []
  type: TYPE_NORMAL
- en: The left side of the preceding figure shows a cost function C computed from
    the output of a **Rectified Linear Unit** (**ReLU**) – a type of neuron function
    we'll cover later in this chapter), which in turn is computed from multiplying
    a weight vector by an input *x* and adding a bias term *b*. On the right, you
    can see that this graph has been augmented by TensorFlow to compute all the intermediate
    gradients required for backpropagation as part of the overall control flow.
  prefs: []
  type: TYPE_NORMAL
- en: After storing these intermediate values, the task of combining them, as shown
    in the calculation in *Figure 3.4*, into a complete gradient through recursive
    operations falls to the GradientTape API. Under the hood, TensorFlow uses a method
    called **reverse-mode automatic differentiation** to compute gradients; it holds
    the dependent variable (the output *y*) fixed, and recursively computes backwards
    to the beginning of the network the required gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s consider a neural network of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Reverse-mode automatic differentiation^(19)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to compute the derivative of the output *y* with respect to an input
    *x* we need to repeatedly substitute the outermost expression^(20):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_026.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, to compute the desired gradient we need to just traverse the graph from
    top to bottom, storing each intermediate gradient as we calculate it. These values
    are stored on a record, referred to as a tape in reference to early computers
    in which information was stored on a magnetic tape,^(21) which is then used to
    replay the values for calculation. The alternative would be to use forward-mode
    automatic differentiation, computing from bottom to top. This requires two instead
    of one pass (for each branch feeding into the final value), but is conceptually
    simpler to implement and doesn't require the storage memory of reverse mode. More
    importantly, though, reverse-mode mimics the derivation of backpropagation that
    I described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tape (also known as the **Wengert Tape**, after one of its developers)
    is actually a data structure that you can access in the TensorFlow Core API. As
    an example, import the core library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The tape is then available using the `tf.GradientTape()` method, with which
    you can evaluate gradients with respect to intermediate values within the graph^(22):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the memory resources used by `GradientTape()` are released once
    `gradient()` is called; however, you can also use the `persistent` argument to
    store these results^(23):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that you've seen how TensorFlow computes gradients in practice to evaluate
    backpropagation, let's return to the details of how the backpropagation technique
    evolved over time in response to challenges in practical implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The shortfalls of backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the backpropagation procedure provides a way to update interior weights
    within the network in a principled way, it has several shortcomings that made
    deep networks difficult to use in practice. One is the problem of **vanishing
    gradients**. In our derivation of the backpropagation formulas, you saw that gradients
    for weights deeper in the network are a product of successive partial derivatives
    from higher layers. In our example, we used the sigmoid function; if we plot out
    the value of the sigmoid and its first derivative, we can see a potential problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: The sigmoid function and its gradient^(24)'
  prefs: []
  type: TYPE_NORMAL
- en: As the value of the sigmoid function increases or decreases towards the extremes
    (0 or 1, representing being either "off" or "on"), the values of the gradient
    vanish to near zero. This means that the updates to `w` and `b`, which are products
    of these gradients from hidden activation functions `y`, shrink towards zero,
    making the weights change little between iterations and making the parameters
    of the hidden layer neurons change very slowly during backpropagation. Clearly
    one problem here is that the sigmoid function saturates; thus, choosing another
    nonlinearity might circumvent this problem (this is indeed one of the solutions
    that was proposed as the ReLU, as we'll cover later).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problem is more subtle, and has to do with how the network utilizes
    its available free parameters. As you saw in *Chapter 1*, *An Introduction to
    Generative AI: "Drawing" Data from Models*, a posterior probability of a variable
    can be computed as a product of a likelihood and a prior distribution. We can
    see deep neural networks as a graphical representation of this kind of probability:
    the output of the neuron, depending upon its parameters, is a product of all the
    input values and the distributions on those inputs (the priors). A problem occurs
    when those values become tightly coupled. As an illustration, consider the competing
    hypotheses for a headache:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: The explaining away effect'
  prefs: []
  type: TYPE_NORMAL
- en: If a patient has cancer, the evidence is so overwhelming that whether they have
    a cold or not provides no additional value; in essence, the value of the two prior
    hypotheses becomes coupled because of the influence of one. This makes it intractable
    to compute the relative contribution of different parameters, particularly in
    a deep network; we will cover this problem in our discussion of Restricted Boltzmann
    Machine and Deep Belief Networks in *Chapter 4*, *Teaching Networks to Generate
    Digits*. As we will describe in more detail in that chapter, a 2006 study^(25)
    showed how to counteract this effect, and was one of the first demonstrations
    of tractable inference in deep neural networks, a breakthrough that relied upon
    a generative model that produced images of hand-drawn digits.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these concerns, other challenges in the more widespread adoption of neural
    networks in the 1990s and early 2000s were the availability of methods such as
    Support Vector Machines^(26), Gradient and Stochastic Gradient Boosting Models,^(27)
    Random Forests,^(28) and even penalized regression methods such as LASSO^(29)
    and Elastic Net,^(30) for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While, in theory, deep neural networks had potentially greater representational
    power than these models since they built hierarchical representations of the input
    data through successive layers in contrast to the "shallow" representation given
    by a single transformation such as a regression weight or decision tree, in practice
    the challenges of training deep networks made these "shallow" methods more attractive
    for practical applications. This was coupled with the fact that larger networks
    required tuning thousands or even millions of parameters, requiring large-scale
    matrix calculations that were infeasible before the explosion of cheap compute
    resources – including GPUs and TPUs especially suited to rapid matrix calculations
    – available from cloud vendors made these experiments practical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve covered the basics of training simple network architectures,
    let''s turn to more complex models that will form the building blocks of many
    of the generative models in the rest of the book: CNNs and sequence models (RNNs,
    LSTMs, and others).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Varieties of networks: Convolution and recursive'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now we've primarily discussed the basics of neural networks by referencing
    feedforward networks, where every input is connected to every output in each layer.
    While these feedforward networks are useful for illustrating how deep networks
    are trained, they are only one class of a broader set of architectures used in
    modern applications, including generative models. Thus, before covering some of
    the techniques that make training large networks practical, let's review these
    alternative deep models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks for seeing: Convolutional architectures'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As noted at the beginning of this chapter, one of the inspirations for deep
    neural network models is the biological nervous system. As researchers attempted
    to design computer vision systems that would mimic the functioning of the visual
    system, they turned to the architecture of the retina, as revealed by physiological
    studies by neurobiologists David Huber and Torsten Weisel in the 1960s.^(31) As
    previously described, the physiologist Santiago Ramon Y Cajal provided visual
    evidence that neural structures such as the retina are arranged in vertical networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_08_a+b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: The "deep network" of the retina^(32 33)'
  prefs: []
  type: TYPE_NORMAL
- en: Huber and Weisel studied the retinal system in cats, showing how their perception
    of shapes is composed of the activity of individual cells arranged in a column.
    Each column of cells is designed to detect a specific orientation of an edge in
    an input image; images of complex shapes are stitched together from these simpler
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Early CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This idea of columns inspired early research into CNN architectures^(34). Instead
    of learning individual weights between units as in a feedforward network, this
    architecture (*Figure 3.9*) uses shared weights within a group of neurons specialized
    to detect a specific edge in an image. The initial layer of the network (denoted
    **H1**) consists of 12 groups of 64 neurons each. Each of these groups is derived
    by passing a 5 x 5 grid over the 16 x 16-pixel input image; each of the 64 5 x
    5 grids in this group share the same weights, but are tied to different spatial
    regions of the input. You can see that there must be 64 neurons in each group
    to cover the input image if their receptive fields overlap by two pixels.
  prefs: []
  type: TYPE_NORMAL
- en: When combined, these 12 groups of neurons in layer **H1** form 12 8 x 8 grids
    representing the presence or absence of a particular edge within a part of the
    image – the 8 x 8 grid is effectively a down-sampled version of the image (*Figure
    3.9*). This weight sharing makes intuitive sense in that the kernel represented
    by the weight is specified to detect a distinct color and/or shape, regardless
    of where it appears in the image. An effect of this down-sampling is a degree
    of positional invariance; we only know the edge occurred somewhere within a region
    of the image, but not the exact location due to the reduced resolution from downsampling.
    Because they are computed by multiplying a 5 x 5 matrix (kernel) with a part of
    the image, an operation used in image blurring and other transformations, these
    5 x 5 input features are known as **convolutional kernels**, and give the network
    its name.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: The CNN^(35)'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have these 12 8 x 8 downsampled versions of the image, the next layer
    (**H2**) also has 12 groups of neurons; here, the kernels are 5 x 5 x 8 – they
    traverse the surface of an 8 x 8 map from **H1**, across 8 of the 12 groups. We
    need 16 neurons of these 5 x 5 x 8 groups since a 5 x 5 grid can be moved over
    four times up and down on an 8 x 8 grid to cover all the pixels in the 8 x 8 grid.
  prefs: []
  type: TYPE_NORMAL
- en: Just like deeper cells in the visual cortex, the deeper layers in the network
    integrate across multiple columns to combine information from different edge detectors
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the third hidden layer of this network (**H3**) contains all-to-all
    connections between 30 hidden units and the 12 x 16 units in the **H2**, just
    as in a traditional feedforward network; a final output of 10 units classifies
    the input image as one of 10 hand-drawn digits.
  prefs: []
  type: TYPE_NORMAL
- en: Through weight sharing, the overall number of free parameters in this network
    is reduced, though it is still large in absolute terms. While backpropagation
    was used successfully for this task, it required a carefully designed network
    for a rather limited set of images with a restricted set of outcomes – for real-world
    applications, such as detecting objects from hundreds or thousands of possible
    categories, other approaches would be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet and other CNN innovations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A 2012 article that produced state-of-the-art results classifying the 1.3 million
    images in ImageNet into 1,000 classes using a model termed AlexNet demonstrates
    some of the later innovations that made training these kinds of models practical.^(36)
    One, as I''ve alluded to before, is using ReLUs^(37) in place of sigmoids or hyperbolic
    tangent functions. A ReLU is a function of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_027.png)'
  prefs: []
  type: TYPE_IMG
- en: In contrast to the sigmoid function, or tanh, in which the derivative shrinks
    to 0 as the function is saturated, the ReLU function has a constant gradient and
    a discontinuity at 0 (*Figure 3.10*). This means that the gradient does not saturate
    and causes deeper layers of the network to train more slowly, leading to intractable
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Gradients of alternative activation functions^(38)'
  prefs: []
  type: TYPE_NORMAL
- en: 'While advantageous due to non-vanishing gradients and their low computational
    requirements (as they are simply thresholded linear transforms), ReLU functions
    have the downside that they can "turn off" if the input falls below 0, leading
    again to a 0 gradient. This deficiency was resolved by later work in which a "leak"
    below 0 was introduced^(39):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A further refinement is to make this threshold adaptive with a slope *a*, the
    **Parameterized Leak ReLU** (**PReLU**)^(40):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_029.png)'
  prefs: []
  type: TYPE_IMG
- en: Another trick used by AlexNet is dropout.^(41) The idea of dropout is inspired
    by ensemble methods in which we average the predictions of many models to obtain
    more robust results. Clearly for deep neural networks this is prohibitive; thus
    a compromise is to randomly set the values of a subset of neurons to 0 with a
    probability of 0.5\. These values are reset with every forward pass of backpropagation,
    allowing the network to effectively sample different architectures since the "dropped
    out" neurons don't participate in the output in that pass.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Dropout'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another enhancement used in AlexNet is local response normalization. Even
    though ReLUs don''t saturate in the same manner as other units, the authors of
    the model still found value in constraining the range of output. For example,
    in an individual kernel, they normalized the input using values of adjacent kernels,
    meaning the overall response was rescaled^(42):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *a* is the unnormalized output at a given *x*, *y* location on an image,
    the sum over *j* is over adjacent kernels, and *B*, *k*, and alpha are hyperparameters.
    This rescaling is reminiscent of a later innovation used widely in both convolutional
    and other neural network architectures, batch normalization^(43). Batch normalization
    also applies a transformation on "raw" activations within a network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_031.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x* is the unnormalized output, and *B* and *y* are scale and shift parameters.
    This transformation is widely applied in many neural network architectures to
    accelerate training, through the exact reason why it is effective remains a topic
    of debate.^(44)
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an idea of some of the methodological advances that made training
    large CNNs possible, let's examine the structure of AlexNet to see some additional
    architectural components that we will use in the CNNs we implement in generative
    models in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the architecture of AlexNet shown in *Figure 3.12* might look intimidating,
    it is not so difficult to understand once we break up this large model into individual
    processing steps. Let's start with the input images and trace how the output classification
    is computed for each image through a series of transformations performed by each
    subsequent layer of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: AlexNet'
  prefs: []
  type: TYPE_NORMAL
- en: The input images to AlexNet are size 224 x 224 x 3 (for RGB channels). The first
    layer consists of groups of 96 units and 11 x 11 x 3 kernels; the output is response
    normalized (as described previously) and max pooled. Max pooling is an operation
    that takes the maximum value over an *n* x *n* grid to register whether a pattern
    appeared *anywhere* in the input; this is again a form of positional invariance.
  prefs: []
  type: TYPE_NORMAL
- en: The second layer is also a set of kernels of size 5 x 5 x 8 in groups of 256\.
    The third through to fifth hidden layers have additional convolutions, without
    normalization, followed by two fully connected layers and an output of size 1,000
    representing the possible image classes in ImageNet. The authors of AlexNet used
    several GPUs to train the model, and this acceleration is important to the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Image kernels from AlexNet'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the features learned during training in the initial 11 x 11 x 3 convolutions
    (*Figure 3.13*), we can see recognizable edges and colors. While the authors of
    AlexNet don't show examples of neurons higher in the network that synthesize these
    basic features, an illustration is provided by another study in which researchers
    trained a large CNN to classify images in YouTube videos, yielding a neuron in
    the upper reaches of the network that appeared to be a cat detector (*Figure 3.14*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: A cat detector learned from YouTube videos^(45)'
  prefs: []
  type: TYPE_NORMAL
- en: This overview should give you an idea of *why* CNN architectures look the way
    they do, and what developments have allowed them to become more tractable as the
    basis for image classifiers or image-based generative models over time. We will
    now turn to a second class of more specialized architectures – RNNs – that's used
    to develop time or sequence-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Networks for sequence data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to image data, natural language text has also been a frequent topic
    of interest in neural network research. However, unlike the datasets we've examined
    thus far, language has a distinct *order* that is important to its meaning. Thus,
    to accurately capture the patterns in language or time-dependent data, it is necessary
    to utilize networks designed for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs and LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s imagine we are trying to predict the next word in a sentence, given
    the words up until this point. A neural network that attempted to predict the
    next word would need to take into account not only the current word but a variable
    number of prior inputs. If we instead used only a simple feedforward MLP, the
    network would essentially process the entire sentence or each word as a vector.
    This introduces the problem of either having to pad variable-length inputs to
    a common length and not preserving any notion of correlation (that is, which words
    in the sentence are more relevant than others in generating the next prediction),
    or only using the last word at each step as the input, which removes the context
    of the rest of the sentence and all the information it can provide. This kind
    of problem inspired the "vanilla" RNN^(46), which incorporates not only the current
    input but the prior step''s hidden state in computing a neuron''s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_032.png)'
  prefs: []
  type: TYPE_IMG
- en: One way to visualize this is to imagine each layer feeding recursively into
    the next timestep in a sequence. In effect, if we "unroll" each part of the sequence,
    we end up with a very deep neural network, where each layer shares the same weights.^(47)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: The unrolled RNN^(48)'
  prefs: []
  type: TYPE_NORMAL
- en: The same difficulties that characterize training deep feedforward networks also
    apply to RNNs; gradients tend to die out over long distances using traditional
    activation functions (or explode if the gradients become greater than 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, unlike feedforward networks, RNNs aren''t trained with traditional
    backpropagation, but rather a variant known as **backpropagation through time**
    (**BPTT**): the network is unrolled, as before, and backpropagation is used, averaging
    over errors *at each time point* (since an "output," the hidden state, occurs
    at each step).^(49) Also, in the case of RNNs, we run into the problem that the
    network has a very short memory; it only incorporates information from the most
    recent unit before the current one and has trouble maintaining long-range context.
    For applications such as translation, this is clearly a problem, as the interpretation
    of a word at the end of a sentence may depend on terms near the beginning, not
    just those directly preceding it.'
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM network was developed to allow RNNs to maintain a context or state
    over long sequences.^(50)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: LSTM network'
  prefs: []
  type: TYPE_NORMAL
- en: In a vanilla RNN, we only maintain a short-term memory *h* coming from the prior
    step's hidden unit activations. In addition to this short-term memory, the LSTM
    architecture introduces an additional layer *c*, the "long-term" memory, which
    can persist over many timesteps. The design is in some ways reminiscent of an
    electrical capacitor, which can use the *c* layer to store up or hold "charge,"
    and discharge it once it has reached some threshold. To compute these updates,
    an LSTM unit consists of a number of related neurons, or gates, that act together
    to transform the input at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an input vector *x*, and the hidden state *h*, at the previous time *t-1*,
    at each time step an LSTM first computes a value from 0 to 1 for each element
    of *c* representing what fraction of information is "forgotten" of each element
    of the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We make a second, similar calculation to determine what from the input value
    to preserve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now know which elements of *c* are updated; we can compute this update as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_035.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B16176_03_036.png) is a Hadamard product (element-wise multiplication).
    In essence this equation tells us how to compute updates using the tanh transform,
    filter them using the input gate, and combine them with the prior time step's
    long-term memory using the forget gate to potentially filter out old values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the output at each time step, we compute another output gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And to compute the final output at each step (the hidden layer fed as short-term
    memory to the next step) we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_038.png)'
  prefs: []
  type: TYPE_IMG
- en: Many variants of this basic design have been proposed; for example, the "peephole"
    LSTM substituted *h*(*t*-*1*) with *c*(*t*-*1*) (thus each operation gets to "peep"
    at the long-term memory cell),^(51) while the GRU^(52) simplifies the overall
    design by removing the output gate. What these designs all have in common is that
    they avoid the vanishing (or exploding) gradient difficulties seen during the
    training of RNNs, since the long-term memory acts as a buffer to maintain the
    gradient and propagate neuronal activations over many timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: Building a better optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we have so far discussed several examples in which better neural
    network architectures allowed for breakthroughs; however, just as (and perhaps
    even more) important is the *optimization procedure* used to minimize the error
    function in these problems, which "learns" the parameters of the network by selecting those
    that yield the lowest error. Referring to our discussion of backpropagation, this
    problem has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to initialize the weights**: In many applications historically, we see
    that the authors used random weights within some range, and hoped that the use
    of backpropagation would result in at least a locally minimal loss function from
    this random starting point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to find the local minimum loss**: In basic backpropagation, we used gradient
    descent using a fixed learning rate and a first derivative update to traverse
    the potential solution space of weight matrices; however, there is good reason
    to believe there might be more efficient ways to find a local minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, both of these have turned out to be key considerations towards progress
    in deep learning research.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent to ADAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in our discussion of backpropagation, the original version proposed
    in 1986 for training neural networks averaged the loss over the *entire dataset*
    before taking the gradient and updating the weights. Obviously, this is quite
    slow and makes distributing the model difficult, as we can't split up the input
    data and model replicas; if we use them, each needs to have access to the whole
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, SGD computes gradient updates after *n* samples, where *n* could
    a range from 1 to *N*, the size of the dataset. In practice, we usually perform
    *mini-batch* gradient descent, in which *n* is relatively small, and we randomize
    assignment of data to the *n* batches after each epoch (a single pass through
    the data).
  prefs: []
  type: TYPE_NORMAL
- en: However, SGD can be slow, leading researchers to propose alternatives that accelerate
    the search for a minimum. As seen in the original backpropagation algorithm, one
    idea is to use a form of exponentially weighted momentum that remembers prior
    steps and continues in promising directions. Variants have been proposed, such
    as *Nesterov Momentum*, which adds a term to increase this acceleration.^(53)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_039.png)![](img/B16176_03_040.png)'
  prefs: []
  type: TYPE_IMG
- en: In comparison to the momentum term used in the original backpropagation algorithm,
    the addition of the current momentum term to the gradient helps keep the momentum
    component aligned with the gradient changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another optimization, termed **Adaptive Gradient** (**Adagrad**)^(54), scales
    the learning rate for each update by the running the sum of squares (*G*) of the
    gradient of that parameter; thus, elements that are frequently updated are downsampled,
    while those that are infrequently updated are pushed to update with greater magnitude:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_041.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach has the downside that as we continue to train the neural network,
    the sum *G* will increase indefinitely, ultimately shrinking the learning rate
    to a very small value. To fix this shortcoming, two variant methods, RMSProp^(55)
    (frequently applied to RNNs) and AdaDelta^(56) impose fixed-width windows of n
    steps in the computation of *G*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive Momentum Estimation** (**ADAM**)^(57) can be seen as an attempt
    to combine momentum and AdaDelta; the momentum calculation is used to preserve
    the history of past gradient updates, while the sum of decaying squared gradients
    within a fixed update window used in AdaDelta is applied to scale the resulting
    gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods mentioned here all share the property of being *first order*: they
    involve only the first derivative of the loss with respect to the input. While
    simple to compute, this may introduce practical challenges with navigating the
    complex solution space of neural network parameters. As shown in *Figure 3.17*,
    if we visualize the landscape of weight parameters as a ravine, then first-order
    methods will either move too quickly in areas in which the curvature is changing
    quickly (the top image) overshooting the minima, or will change too slowly within
    the minima "ravine," where the curvature is low. An ideal algorithm would take
    into account not only the curvature but the *rate of change* of the curvature,
    allowing an optimizer order method to take larger step sizes when the curvature
    changes very slowly and vice versa (the bottom image).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Complex landscapes and second-order methods^(58)'
  prefs: []
  type: TYPE_NORMAL
- en: Because they make use of the rate of change of the derivative (the **second
    derivative**), these methods are known as **second order**, and have demonstrated
    some success in optimizing neural network models.^(59)
  prefs: []
  type: TYPE_NORMAL
- en: However, the computation required for each update is larger than for first-order
    methods, and because most second-order methods involve large matrix inversions
    (and thus memory utilization), approximations are required to make these methods
    scale. Ultimately, however, one of the breakthroughs in practically optimizing
    networks comes not just from the optimization algorithm, but how we initialize
    the weights in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Xavier initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As noted previously, in earlier research it was common to initialize weights
    in a neural network with some range of random values. Breakthroughs in the training
    of Deep Belief Networks in 2006, as you will see in *Chapter 4*, *Teaching Networks
    to Generate Digits*, used pre-training (through a generative modeling approach)
    to initialize weights before performing standard backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: If you've ever used a layer in the TensorFlow Keras module, you will notice
    that the default initialization for layer weights draws from either a truncated
    normal or uniform distribution. Where does this choice come from? As I described
    previously, one of the challenges with deep networks using sigmoidal or hyperbolic
    activation functions is that they tend to become saturated, since the values for
    these functions are capped with very large or negative input. We might interpret
    the challenge of initializing networks then as keeping weights in such a range
    that they don't saturate the neuron's output. Another way to understand this is
    to assume that the input and output values of the neuron have similar variance;
    the signal is not massively amplifying or diminishing while passing through the
    neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, for a linear neuron, *y* = *wx* + *b*, we could compute the variance
    of the input and output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *b* is constant, so we are left with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since there are *N* elements in the weight matrix, and we want *var*(*y*) to
    equal *var*(*x*), this gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_03_044.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, for a weight matrix *w*, we can use a truncated normal or uniform
    distribution with variance 1/*N* (the average number of input and output units,
    so the number of weights).^(60) Variations have also been applied to ReLU units:^(61)
    these methods are referred to by their original authors' names as Xavier or He
    initialization.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we've reviewed several common optimizers used under the hood in
    TensorFlow 2, and discussed how they improve upon the basic form of SGD. We've
    also discussed how clever weight initialization schemes work together with these
    optimizers to allow us to train ever more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered the basic vocabulary of deep learning – how initial
    research into perceptrons and MLPs led to simple learning rules being abandoned
    for backpropagation. We also looked at specialized neural network architectures
    such as CNNs, based on the visual cortex, and recurrent networks, specialized
    for sequence modeling. Finally, we examined variants of the gradient descent algorithm
    proposed originally for backpropagation, which have advantages such as momentum,
    and described weight initialization schemes that place the parameters of the network
    in a range that is easier to navigate to a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: With this context in place, we are all set to dive into projects in generative
    modeling, beginning with the generation of MNIST digits using Deep Belief Networks
    in *Chapter 4*, *Teaching Networks to Generate Digits*.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'López-Muñoz F., Boya J., Alamo C. (2006). *Neuron theory, the cornerstone of
    neuroscience, on the centenary of the Nobel Prize award to Santiago Ramón y Cajal*.
    Brain Research Bulletin. 70 (4–6): 391–405\. [https://pubmed.ncbi.nlm.nih.gov/17027775/](https://pubmed.ncbi.nlm.nih.gov/17027775/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ramón y Cajal, Santiago (1888). *Estructura de los centros nerviosos de las
    aves*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: McCulloch, W.S., Pitts, W. (1943). *A logical calculus of the ideas immanent
    in nervous activity. Bulletin of Mathematical Biophysics* 5, 115–133\. [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rashwan M., Ez R., reheem G. (2017). *Computational Intelligent Algorithms For
    Arabic Speech Recognition*. Journal of Al-Azhar University Engineering Sector.
    12\. 886-893\. 10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rashwan M., Ez R., reheem G. (2017). *Computational Intelligent Algorithms For
    Arabic Speech Recognition*. Journal of Al-Azhar University Engineering Sector.
    12\. 886-893\. 10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Artificial neuron*. Wikipedia. Retrieved April 26, 2021, from [https://en.wikipedia.org/wiki/Artificial_neuron](https://en.wikipedia.org/wiki/Artificial_neuron)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shackleton-Jones Nick. (2019, May 3). *How People Learn: Designing Education
    and Training that Works to Improve Performance*. Kogan Page. London, United Kingdom'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hebb, D. O. (1949). *The Organization of Behavior: A Neuropsychological Theory*.
    New York: Wiley and Sons'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rosenblatt, Frank (1957). *The Perceptron—a perceiving and recognizing automaton*.
    Report 85-460-1\. Cornell Aeronautical Laboratory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Marvin Minsky and Seymour Papert, 1972 (2^(nd) edition with corrections, first
    edition 1969) *Perceptrons: An Introduction to Computational Geometry*, The MIT
    Press, Cambridge MA'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hassan, Hassan & Negm, Abdelazim & Zahran, Mohamed & Saavedra, Oliver. (2015).
    *Assessment of Artificial Neural Network for Bathymetry Estimation Using High
    Resolution Satellite Imagery in Shallow Lakes: Case Study El Burullus Lake*. International
    Water Technology Journal. 5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Marvin Minsky and Seymour Papert, 1972 (2nd edition with corrections, first
    edition 1969) *Perceptrons: An Introduction to Computational Geometry*, The MIT
    Press, Cambridge MA'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pollack, J. B. (1989). "No Harm Intended: A Review of the Perceptrons expanded
    edition". *Journal of Mathematical Psychology*. 33 (3): 358–365.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Crevier, Daniel (1993), *AI: The Tumultuous Search for Artificial Intelligence*,
    New York, NY: BasicBooks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cybenko, G. *Approximation by superpositions of a sigmoidal function*. Math.
    Control Signal Systems 2, 303–314 (1989). [https://doi.org/10.1007/BF02551274](https://doi.org/10.1007/BF02551274)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). *6.5 Back-Propagation
    and Other Differentiation Algorithms*. Deep Learning. MIT Press. pp. 200–220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rumelhart, D., Hinton, G. & Williams, R. (1986) *Learning representations by
    back-propagating errors*. *Nature* 323, 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Guess A R., (2015, November 10). *Google Open-Sources Machine Learning Library,
    TensorFlow*. DATAVERSITY. [https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/](https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Berland (2007). *ReverseaccumulationAD.png*. Wikipedia. Available from: [https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png](https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Automatic differentiation*. Wikipedia. [https://en.wikipedia.org/wiki/Automatic_differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R.E. Wengert (1964). *A simple automatic derivative evaluation program*. Comm.
    ACM. 7 (8): 463–464.;Bartholomew-Biggs, Michael; Brown, Steven; Christianson,
    Bruce; Dixon, Laurence (2000). *Automatic differentiation of algorithms*. Journal
    of Computational and Applied Mathematics. 124 (1–2): 171–190.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The TensorFlow Authors (2018). *automatic_differentiation.ipynb*. Available
    from: [https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/notebooks/automatic_differentiation.ipynb#scrollTo=t09eeeR5prIJ](https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/py)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The TensorFlow Authors. *Introduction to gradients and automatic differentiation*.
    TensorFlow. Available from: [https://www.tensorflow.org/guide/autodiff](https://www.tensorflow.org/guide/autodiff)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thomas (2018). *The vanishing gradient problem and ReLUs – a TensorFlow investigation*.
    Adventures in Machine Learning. Available from: [https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/](https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hinton, Osindero, Yee-Whye (2005). *A Fast Learning Algorithm for Deep Belief
    Nets*. Univeristy of Toronto, Computer Science. Available from: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cortes, C., Vapnik, V. *Support-vector networks*. Mach Learn 20, 273–297 (1995).
    [https://doi.org/10.1007/BF00994018](https://doi.org/10.1007/BF00994018)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Friedman, J. H. (February 1999). *Greedy Function Approximation: A Gradient
    Boosting Machine* (PDF)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Breiman, L. *Random Forests*. Machine Learning 45, 5–32 (2001). [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tibshirani R. (1996). *Regression Shrinkage and Selection via the lasso*. Journal
    of the Royal Statistical Society. Series B (methodological). Wiley. 58 (1): 267–88.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zou H., Hastie T. (2005). *Regularization and variable selection via the elastic
    net*. Journal of the Royal Statistical Society, Series B: 301–320'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hubel D. H., Wiesel T. N. (1962) *Receptive fields, binocular interaction and
    functional architecture in the cat''s visual cortex*. J Physiol, 1962, 160: 106-154\.
    [https://doi.org/10.1113/jphysiol.1962.sp006837](https://doi.org/10.1113/jphysiol.1962.sp006837)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif](http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wolfe, Kluender, Levy (2009). *Sensation and Perception*. Sunderland: Sinauer
    Associates Inc..'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LeCun, Yann, et al. *Backpropagation applied to handwritten zip code recognition*.
    Neural computation 1.4 (1989): 541-551.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LeCun, Yann, et al. *Backpropagation applied to handwritten zip code recognition*.
    Neural computation 1.4 (1989): 541-551.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*ImageNet Classification with Deep Convolutional Neural Networks*: [https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convoluti)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nair V., Hinton G E. (2010). *Rectified Linear Units Improve Restricted Boltzmann
    Machines*. Proceedings of the 27 th International Conference on Machine Learning,
    Haifa, Israel, 2010.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Agarap A F. (2019, September 5). *Avoiding the vanishing gradients problem using
    gradient noise addition*. Towards Data Science. [https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343](https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maas A L., Hannun A Y., Ng A Y. (2013). *Rectifer Nonlinearities Improve Neural
    Network Acoustic Models*. Proceedings of the 30 th International Conference on
    Machine Learning, Atlanta, Georgia, USA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'He, K., Zhang, X., Ren, S., Sun, J. (2015). *Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification*. arXiv:1502.01852\.
    [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton, G E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov,
    R R. (2012). *Improving neural networks by preventing co-adaptation of feature
    detectors*. arXiv:1207.0580\. [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580
    )
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Krizhevsky A., Sutskever I., Hinton G E. (2012). *ImageNet Classification with
    Deep Convolutional Neural Networks*. Part of Advances in Neural Information Processing
    Systems 25 (NIPS 2012). [https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ioffe, S., Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*. arXiv:1502.03167\. [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Santurkar, S., Tsipras, D., Ilyas, A., Madry, A. (2019). *How Does Batch Normalization
    Help Optimization?*. arXiv:1805.11604\. [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dean J., Ng, A Y. (2012). *Using large-scale brain simulations for machine learning
    and A.I.*. The Keyword | Google. [https://blog.google/technology/ai/using-large-scale-brain-simulations-for/](https://blog.google/technology/ai/using-large-scale-brain-simulations-for/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rumelhart, D., Hinton, G. & Williams, R. (1986) *Learning representations by
    back-propagating errors*. *Nature* 323, 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LeCun, Y., Bengio, Y. & Hinton, (2015) G. *Deep learning*. *Nature* 521, 436–444\.
    [https://www.nature.com/articles/nature14539.epdf](https://www.nature.com/articles/nature14539.epdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Olah (2015). *Understanding LSTM Networks*. colah''s blog. Available from:
    [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mozer, M. C. (1995). *A Focused Backpropagation Algorithm for Temporal Pattern
    Recognition*. In Chauvin, Y.; Rumelhart, D. (eds.). *Backpropagation: Theory,
    architectures, and applications*. ResearchGate. Hillsdale, NJ: Lawrence Erlbaum
    Associates. pp. 137–169'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Greff K., Srivastava, R K., Koutník, J., Steunebrink, B R., Schmidhuber, J.
    (2017). *LSTM: A Search Space Odyssey*. arXiv:1503.04069v2\. [https://arxiv.org/abs/1503.04069v2](https://arxiv.org/abs/1503.04069v2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gers FA, Schmidhuber E. *LSTM recurrent networks learn simple context-free
    and context-sensitive languages*. IEEE Trans Neural Netw. 2001;12(6):1333-40\.
    doi: 10.1109/72.963769\. PMID: 18249962.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H., Bengio, Y. (2014). *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*. arXiv:1406.1078\. [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sutskever, I., Martens, J., Dahl, G. & Hinton, G. (2013). *On the importance
    of initialization and momentum in deep learning*. Proceedings of the 30th International
    Conference on Machine Learning, in PMLR 28(3):1139-1147.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Duchi J., Hazan E., Singer Y. (2011). *Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization*. Journal of Machine Learning Research 12
    (2011) 2121-2159.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hinton, Srivastava, Swersky. *Neural Networks for Machine Learning*, Lecture
    6a. Available from: [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zeiler, M D. (2012). *ADADELTA: An Adaptive Learning Rate Method*. arXiv:1212.5701\.
    [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701 )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kingma, D P., Ba, J. (2017). *Adam: A Method for Stochastic Optimization*.
    arXiv:1412.6980\. [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Martens J. (2010). *Deep Learning via Hessian-free Optimization*. ICML. Vol.
    27\. 2010.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Martens J. (2010). *Deep Learning via Hessian-free Optimization*. ICML. Vol.
    27\. 2010.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Glorot X., Bengio Y., (2010). *Understanding the difficulty of training deep
    feedforward neural networks*. Proceedings of the thirteenth international conference
    on artificial intelligence and statistics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'He, K., Zhang, X., Ren, S., Sun, J. (2015). *Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification*. arXiv:1502.01852\.
    [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
