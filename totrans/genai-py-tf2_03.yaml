- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Building Blocks of Deep Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络的构建模块
- en: 'The wide range of generative AI models that we will implement in this book
    are all built on the foundation of advances over the last decade in *deep learning*
    and neural networks. While in practice we could implement these projects without
    reference to historical developments, it will give you a richer understanding
    of *how* and *why* these models work to retrace their underlying components. In
    this chapter, we will dive into this background, showing you how generative AI
    models are built from the ground up, how smaller units are assembled into complex
    architectures, how the loss functions in these models are optimized, and some
    current theories as to why these models are so effective. Armed with this background
    knowledge, you should be able to understand in greater depth the reasoning behind
    the more advanced models and topics that start in *Chapter 4**,* *Teaching Networks
    to Generate Digits*, of this book. Generally speaking, we can group the building
    blocks of neural network models into a number of choices regarding how the model
    is constructed and trained, which we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将实现的广泛范围的生成式人工智能模型都是建立在过去十年来在*深度学习*和神经网络方面的进步基础上的。虽然在实践中我们可以在不参考历史发展的情况下实现这些项目，但追溯它们的基本组成部分将使您对这些模型*如何*和*为什么*工作有更深入的理解。在本章中，我们将深入探讨这一背景，向您展示生成式人工智能模型是如何从基础构建的，如何将较小的单元组装成复杂的架构，这些模型中的损失函数是如何优化的，以及一些当前的理论解释为什么这些模型如此有效。掌握了这些背景知识，您应该能够更深入地理解从本书的第4章开始的更高级模型和主题背后的推理，《教网络生成数字》。一般来说，我们可以将神经网络模型的构建模块分为一些关于模型如何构建和训练的选择，我们将在本章中进行介绍：
- en: 'Which neural network architecture to use:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种神经网络架构：
- en: Perceptron
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器
- en: '**Multilayer perceptron** (**MLP**)/feedforward'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）/ 前馈'
- en: '**Convolutional Neural Networks** (**CNNs**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）'
- en: '**Recurrent Neural Networks** (**RNNs**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）'
- en: '**Long Short-Term Memory Networks** (**LSTMs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆网络**（**LSTMs**）'
- en: '**Gated Recurrent Units** (**GRUs**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRUs**）'
- en: 'Which activation functions to use in the network:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中使用哪些激活函数：
- en: Linear
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性
- en: Sigmoid
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Tanh
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: ReLU
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: PReLU
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PReLU
- en: 'What optimization algorithm to use to tune the parameters of the network:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用什么优化算法来调整网络参数：
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）'
- en: RMSProp
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSProp
- en: AdaGrad
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaGrad
- en: ADAM
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ADAM
- en: AdaDelta
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaDelta
- en: Hessian-free optimization
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无Hessian优化
- en: 'How to initialize the parameters of the network:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如何初始化网络的参数：
- en: Random
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机
- en: Xavier initialization
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xavier 初始化
- en: He initialization
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 初始化
- en: 'As you can appreciate, the products of these decisions can lead to a huge number
    of potential neural network variants, and one of the challenges of developing
    these models is determining the right search space within each of these choices.
    In the course of describing the history of neural networks we will discuss the
    implications of each of these model parameters in more detail. Our overview of
    this field begins with the origin of the discipline: the humble perceptron model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所理解的，这些决策的产物可能导致大量潜在的神经网络变种，开发这些模型的一个挑战之一是确定每个选择中的正确搜索空间。在描述神经网络历史的过程中，我们将更详细地讨论这些模型参数的影响。我们对这个领域的概述始于这一学科的起源：谦逊的感知器模型。
- en: Perceptrons – a brain in a function
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器——一个功能中的大脑
- en: The simplest neural network architecture – the perceptron – was inspired by
    biological research to understand the basis of mental processing in an attempt
    to represent the function of the brain with mathematical formulae. In this section
    we will cover some of this early research and how it inspired what is now the
    field of deep learning and generative AI.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经网络架构——感知器——受生物研究的启发，旨在理解心理加工的基础，试图用数学公式表示大脑的功能。在本节中，我们将涵盖一些早期研究，以及它是如何激发了现在的深度学习和生成式人工智能领域的。
- en: From tissues to TLUs
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从组织到TLUs
- en: The recent popularity of AI algorithms might give the false impression that
    this field is new. Many recent models are based on discoveries made decades ago
    that have been reinvigorated by the massive computational resources available
    in the cloud and customized hardware for parallel matrix computations such as
    **Graphical Processing Units** (**GPUs**), **Tensor Processing Units** (**TPUs**),
    and **Field Programmable Gate Array** (**FPGAs**). If we consider research on
    neural networks to include their biological inspiration as well as computational
    theory, this field is over a hundred years old. Indeed, one of the first neural
    networks described appears in the detailed anatomical illustrations of 19th Century
    scientist Santiago Ramón y Cajal, whose illustrations based on experimental observations
    of layers of interconnected neuronal cells inspired the Neuron Doctrine – the
    idea that the brain is composed of individual, physically distinct and specialized
    cells, rather than a single continuous network.¹ The distinct layers of the retina
    observed by Cajal were also the inspiration for particular neural network architectures
    such as the CNN, which we will discuss later in this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: AI算法的近期受欢迎可能会给人一种错误的印象，认为这个领域是新的。许多近期的模型基于几十年前的发现，这些发现因云端的大规模计算资源以及用于并行矩阵计算的定制硬件（如**图形处理单元**（**GPUs**）、**张量处理单元**（**TPUs**）和**可编程门阵列**（**FPGAs**））而得到了重振。如果我们认为神经网络的研究包括其生物启发和计算理论，那么这个领域已经有上百年的历史了。事实上，19世纪科学家Santiago
    Ramón y Cajal详细解剖插图中描述的其中一个最早的神经网络，这些插图基于对相互连接的神经细胞层的实验观察，启发了神经元学说—即大脑是由单独的、物理上不同且专门的细胞组成，而不是一个连续的网络。¹
    Cajal观察到的视网膜的不同层也启发了特定的神经网络架构，比如我们将在本章后面讨论的CNN。
- en: '![](img/B16176_03_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_01.png)'
- en: 'Figure 3.1: The networks of interconnected neurons illustrated by Santiago
    Ramón y Cajal³'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：由Santiago Ramón y Cajal绘制的神经元相互连接的网络³
- en: 'This observation of simple neuronal cells interconnected in large networks
    led computational researchers to hypothesize how mental activity might be represented
    by simple, logical operations that, combined, yield complex mental phenomena.
    The original "automata theory" is usually traced to a 1943 article by Warren McCulloch
    and Walter Pitts of the Massachusetts Institute of Technology.³ They described
    a simple model known as the **Threshold Logic Unit** (**TLU**), in which binary
    inputs are translated into a binary output based on a threshold:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单神经细胞相互连接的观察使得计算研究人员推测精神活动可能如何由简单的逻辑运算表示，进而产生复杂的精神现象。最初的“自动机理论”通常被追溯到麻省理工学院的Warren
    McCulloch和Walter Pitts于1943年发表的一篇文章。³ 他们描述了一个简单的模型，即**阈值逻辑单元**（**TLU**），其中二进制输入根据阈值转换为二进制输出：
- en: '![](img/B16176_03_001.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_001.png)'
- en: where *I* is the input values, *W* is the weights with ranges from (0, 1) or
    (-1, 1), and f is a threshold function that converts these inputs into a binary
    output depending upon whether they exceed a threshold *T*:⁴
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*I* 代表输入值，*W* 代表权重范围为 (0, 1) 或 (-1, 1)，而 f 是一个阈值函数，根据输入是否超过阈值 *T* 将这些输入转换成二进制输出：⁴
- en: '![](img/B16176_03_002.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_002.png)'
- en: Visually and conceptually, there is some similarity between McCulloch and Pitts'
    model and the biological neuron that inspired it (*Figure 3.2*). Their model integrates
    inputs into an output signal, just as the natural dendrites (short, input "arms"
    of the neuron that receive signals from other cells) of a neuron synthesize inputs
    into a single output via the axon (the long "tail" of the cell, which passes signals
    received from the dendrites along to other neurons). We might imagine that, just
    as neuronal cells are composed into networks to yield complex biological circuits,
    these simple units might be connected to simulate sophisticated decision processes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉上和概念上，McCulloch 和 Pitts 的模型与启发它的生物神经元（*图3.2*）之间存在一定的相似性。他们的模型将输入整合成输出信号，就像神经元的自然树突（神经元的短输入“臂”，从其他细胞接收信号）将输入通过轴突（细胞的长“尾巴”，将从树突接收到的信号传递给其他神经元）合成一个单一的输出。我们可以想象，就像神经细胞被组成网络以产生复杂的生物学电路一样，这些简单的单元可能被连接起来以模拟复杂的决策过程。
- en: '![](img/B16176_03_02_a+b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_02_a+b.png)'
- en: 'Figure 3.2: The TLU model and the biological neuron^(5 6)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：TLU模型和生物神经元^(5 6)
- en: Indeed, using this simple model, we can already start to represent several logical
    operations. If we consider a simple case of a neuron with one input, we can see
    that a TLU can solve an identity or negation function (*Tables 3.1* and *3.2*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'For an identity operation that simply returns the input as output, the weight
    matrix would have 1s on the diagonal (or be simply the scalar 1, for a single
    numerical input, as illustrated in *Table 1*):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '| Identity |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Input | Output |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: TLU logic for identity operations'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for a negation operation, the weight matrix could be a negative
    identity matrix, with a threshold at 0 flipping the sign of the output from the
    input:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '| Negation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Input | Output |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: 'Table 3.2: TLU logic for negation operations'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Given two inputs, a TLU could also represent operations such as AND and OR.
    Here, a threshold could be set such that combined input values either have to
    exceed `2` (to yield an output of `1`) for an AND operation (*Table 3.3*) or `1`
    (to yield an output of `1` if either of the two inputs are `1`) in an OR operation
    (*Table 3.4*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '| AND |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: 'Table 3.3: TLU logic for AND operations'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '| OR |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: 'Table 3.4: TLU logic for OR operations'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: However, a TLU cannot capture patterns such as `Exclusive OR` (`XOR`), which
    emits `1` if and *only if* the `OR` condition is true (*Table 3.5*).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '| XOR |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: 'Table 3.5: TLU logic for XOR operations'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: To see why this is true, consider a TLU with two inputs and positive weights
    of `1` for each unit. If the threshold value `T` is `1`, then inputs of (`0`,
    `0`), (`1`, `0`), and (`0`, `1`) will yield the correct value. What happens with
    (`1`, `1`) though? Because the threshold function returns `1` for any inputs summing
    to greater than `1`, it cannot represent `XOR` (*Table 3.5*), which would require
    a second threshold to compute a different output once a different, higher value
    is exceeded. Changing one or both of the weights to negative values won't help
    either; the problem is that the decision threshold operates only in one direction
    and can't be reversed for larger inputs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the TLU can''t represent the negation of the `Exclusive NOR`, `XNOR`
    (*Table 3.6*):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '| XNOR |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| Input 1 | Input 2 | Output |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: 'Table 3.6: TLU logic for XNOR operations'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: As with the `XOR` operation (*Table 3.5*), the impossibility of the `XNOR` operation
    (*Table 3.6*) being represented by a TLU function can be illustrated by considering
    a weight matrix of two 1s; for two inputs (1, 0) or (0, 1), we obtain the correct
    value if we set a threshold of 2 for outputting 1\. As with the `XOR` operation,
    we run into a problem with an input of (0, 0), as we can't set a second threshold
    to output 1 at a sum of 0.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与`XOR`操作类似（*表3.5*），通过考虑一个含有两个1的权重矩阵，可以说明无法通过TLU函数来表示`XNOR`操作（*表3.6*）；对于两个输入（1,
    0）或（0, 1），如果我们设置输出1的阈值为2，则获得了正确的值。与`XOR`操作类似，当输入为（0, 0）时会遇到问题，因为我们无法设置第二个阈值来使和为0时输出1。
- en: From TLUs to tuning perceptrons
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从TLUs到调谐感知器
- en: 'Besides these limitations for representing the `XOR` and `XNOR` operations,
    there are additional simplifications that cap the representational power of the
    TLU model; the weights are fixed, and the output can only be binary (0 or 1).
    Clearly, for a system such as a neuron to "learn," it needs to respond to the
    environment and determine the relevance of different inputs based on feedback
    from prior experiences. This idea was captured in the 1949 book *Organization
    of Behavior* by Canadian Psychologist Donald Hebb, who proposed that the activity
    of nearby neuronal cells would tend to synchronize over time, sometimes paraphrased
    at Hebb''s Law: *Neurons that fire together wire together*^(7 8). Building on
    Hebb''s proposal that weights changed over time, researcher Frank Rosenblatt of
    the Cornell Aeronautical Laboratory proposed the perceptron model in the 1950s.⁹
    He replaced the fixed weights in the TLU model with adaptive weights and added
    a bias term, giving a new function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些有关表示`XOR`和`XNOR`操作的限制外，还有一些附加的简化会限制TLU模型的表达能力；权重是固定的，输出只能是二进制（0或1）。显然，对于像神经元这样的系统来说，“学习”需要对环境做出响应，并根据先前的经验反馈确定不同输入的相关性。这个观点在加拿大心理学家唐纳德·赫布（Donald
    Hebb）1949年的著作《行为的组织》中有所体现，他提出，附近的神经细胞的活动随着时间会趋同，有时被简化为赫布定律：“放电在一起联结在一起”^(7 8)。基于赫布的权重随时间变化的提议，康奈尔航空实验室的研究员弗兰克·罗森布拉特（Frank
    Rosenblatt）在1950年代提出了感知器（perceptron）模型。⁹ 他用自适应权重替代了TLU模型中的固定权重，并增加了偏置项，得到了一个新的函数：
- en: '![](img/B16176_03_003.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_003.png)'
- en: 'We note that the inputs *I* have been denoted *X* to underscore the fact that
    they could be any value, not just binary `0` or `1`. Combining Hebb''s observations
    with the TLU model, the weights of the perceptron would be updated according to
    a simple learning rule:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，输入*I*已被标记为*X*以突显它们可以是任何值，而不仅仅是二进制`0`或`1`。将赫布的观察与TLU模型相结合，感知器的权重将根据简单的学习规则进行更新：
- en: Start with a set of J samples *x*(1) …. x(*j*). These samples all have a label
    y which is 0 or 1, giving labeled data (*y*, *x*)(1) …. (*y*, *x*)(*j*). These
    samples could have either a single value, in which case the perceptron has a single
    input, or be a vector with length *N* and indices *i* for multi-value input.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一组J个样本*x*(1) …. x(*j*)出发。这些样本都有标签y，可以是0或1，提供有标记的数据（*y*, *x*)(1) …. (*y*, *x*)(*j*)。这些样本可以是单个值，此时感知器有单个输入，也可以是长度为*N*且具有*
    i*的多值输入的向量。
- en: Initialize all weights *w* to a small random value or 0.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有权重*w*为小的随机值或0。
- en: Compute the estimated value, *yhat*, for all the examples *x* using the perceptron
    function.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用感知器函数计算所有示例*x*的估计值*yhat*。
- en: 'Update the weights using a learning rate *r* to more closely match the input
    to the desired output for each step *t* in training:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习速率*r*更新权重，以更接近于每一步*t*中训练的期望输出值：
- en: '![](img/B16176_03_004.png), for all *J* samples and *N* features. Conceptually,
    note that if *y* is 0 and the target is 1, we want to increase the value of the
    weight by some increment *r*; likewise, if the target is 0 and the estimate is
    1, we want to decrease the weight so the inputs do not exceed the threshold.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/B16176_03_004.png)，对于所有的*J*个样本和*N*个特征。概念上，需要注意如果*y*为0且目标值为1，我们希望通过一定的增量*r*增加权重的值；同样，如果目标值为0且估计值为1，我们希望减小权重，使得输入值不超过阈值。'
- en: Repeat *steps 3-4* until the difference between the predicted and actual outputs,
    *y* and *yhat*, falls below some desired threshold. In the case of a non-zero
    bias term, *b*, an update can be computed as well using a similar formula.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤3-4*，直到预测输出*y*和实际输出*yhat*之间的差值低于某个期望的阈值。在有非零偏置项*b*的情况下，也可以使用类似的公式来计算更新。
- en: While simple, you can appreciate that many patterns could be learned from such
    a classifier, though still not the `XOR` function. However, by combining several
    perceptrons into multiple layers, these units could represent any simple Boolean
    function,^(10) and indeed McCulloch and Pitts had previously speculated on combining
    such simple units into a universal computation engine, or Turing Machine, that
    could represent any operation in a standard programming language. However, the
    preceding learning algorithm operates on each unit independently, meaning it could
    be extended to networks composed of many layers of perceptrons (*Figure 3.3*).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单，你可以理解这样的分类器可以学习到许多模式，但仍然不能学习到`XOR`函数。然而，通过将几个感知机组合成多个层，这些单元可以表示任何简单的布尔函数，^(10)而且麦卡洛克和皮茨此前已经推测过将这些简单单元组合成一个通用计算引擎或图灵机，可以表示标准编程语言中的任何操作。然而，前述学习算法对每个单元独立操作，这意味着它可以扩展到由许多层感知机组成的网络（*图3.3*）。
- en: '![](img/B16176_03_03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_03.png)'
- en: 'Figure 3.3: A multi-layer perceptron^(11)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：一个多层感知机^(11)
- en: However, the 1969 book *Perceptrons*, by MIT computer scientists Marvin Minksy
    and Seymour Papert, demonstrated that a three-layer feed-forward network required
    complete (non-zero weight) connections between at least one of these units (in
    the first layer) and all inputs to compute all possible logical outputs^(12).
    This meant that instead of having a very sparse structure, like biological neurons,
    which are only connected to a few of their neighbors, these computational models
    required very dense connections.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，麻省理工学院的计算机科学家马文·明斯基和西摩·帕珀特在1969年的书籍《感知机》中表明，一个三层前馈网络需要至少有一个这些单元（在第一层）与所有输入之间的完全（非零权重）连接才能计算所有可能的逻辑输出^(12)。这意味着，与生物神经元只连接到少数邻居相比，这些计算模型需要非常密集的连接。
- en: While connective sparsity has been incorporated in later architectures, such
    as CNNs, such dense connections remain a feature of many modern models too, particularly
    in the *fully connected* layers that often form the second to last hidden layers
    in models. In addition to these models being computationally unwieldy on the hardware
    of the day, the observation that sparse models could not compute all logical operations
    was interpreted more broadly by the research community as *Perceptrons cannot
    compute XOR*. While erroneous,^(13) this message led to a drought in funding for
    AI in subsequent years, a period sometimes referred to as the **AI Winter**^(14).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然后来的架构中已经融入了连接的稀疏性，比如CNNs，但这种密集的连接仍然是许多现代模型的特征，特别是在通常形成模型倒数第二层隐藏层的*全连接*层中。除了这些模型在当时的硬件上计算上不便利外，对于稀疏模型无法计算所有逻辑运算的观察被研究界更广泛地解释为*感知机无法计算XOR*。虽然是错误的，^(13)但这个观点导致了AI在随后的几年里资金的枯竭，有时这段时期被称为**AI冬季**^(14)。
- en: The next revolution in neural network research would require a more efficient
    way to compute the required parameters updated in complex models, a technique
    that would become known as **backpropagation**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络研究的下一次革命将需要一种更有效的方法来计算复杂模型中更新所需的参数，这种技术将被称为**反向传播**。
- en: Multi-layer perceptrons and backpropagation
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机和反向传播
- en: While large research funding for neural networks declined until the 1980s after
    the publication of *Perceptrons*, researchers still recognized that these models
    had value, particularly when assembled into multi-layer networks, each composed
    of several perceptron units. Indeed, when the mathematical form of the output
    function (that is, the output of the model) was relaxed to take on many forms
    (such as a linear function or a sigmoid), these networks could solve both regression
    and classification problems, with theoretical results showing that 3-layer networks
    could effectively approximate any output.^(15) However, none of this work addressed
    the practical limitations of computing the solutions to these models, with rules
    such as the perceptron learning algorithm described earlier proving a great limitation
    to the applied use of them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自从《感知机》出版后，直到1980年代，神经网络的大规模研究资金都在下降，但研究人员仍然认识到这些模型有价值，特别是当它们被组装成由多个感知机单元组成的多层网络时。事实上，当输出函数的数学形式（即模型的输出）被放宽为多种形式（如线性函数或Sigmoid函数）时，这些网络可以解决回归和分类问题，理论结果表明，3层网络可以有效逼近任何输出。^(15)然而，这项工作没有解决这些模型的计算解的实际限制，而之前描述的感知机学习算法等规则对它们的应用造成了很大的限制。
- en: 'Renewed interest in neural networks came with the popularization of the backpropagation
    algorithm, which, while discovered in the 1960s, was not widely applied to neural
    networks until the 1980s, following several studies highlighting its usefulness
    for learning the weights in these models.^(16) As you saw with the perceptron
    model, a learning rule to update weights is relatively easy to derive as long
    as there are no "hidden" layers. The input is transformed once by the perceptron
    to compute an output value, meaning the weights can be directly tuned to yield
    the desired output. When there are hidden layers between the input and output,
    the problem becomes more complex: when do we change the internal weights to compute
    the activations that feed into the final output? How do we modify them in relation
    to the input weights?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络的重新关注始于反向传播算法的普及，该算法尽管在20世纪60年代已经被发现，但直到20世纪80年代才被广泛应用于神经网络，此前的多项研究强调了它在学习这些模型中的权重方面的有用性。^(16)
    正如你在感知机模型中所看到的，更新权重的学习规则在没有“隐藏”层的情况下是相对容易推导出来的。输入只被感知机一次性地转换为输出值，意味着可以直接调整权重以产生期望的输出。当输入和输出之间有隐藏层时，问题就变得更加复杂：我们何时改变内部权重以计算输入权重遍历到最终输出的激活值？我们如何根据输入权重来修改它们？
- en: The insight of the backpropagation technique is that we can use the chain rule
    from calculus to efficiently compute the derivatives of each parameter of a network
    with respect to a loss function and, combined with a learning rule, this provides
    a scalable way to train multilayer networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播技术的见解在于，我们可以利用微积分中的链式法则来高效地计算网络中每个参数相对于损失函数的导数，并且结合学习规则，这为训练多层网络提供了一种可扩展的方法。
- en: 'Let''s illustrate backpropagation with an example: consider a network like
    the one shown in *Figure 3.3*. Assume that the output in the final layer is computed
    using a sigmoidal function, which yields a value between 0 and 1:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明反向传播：考虑一个像*图3.3*中所示的网络。假设最终层中的输出是使用S形函数计算的，这将产生一个值在0到1之间：
- en: '![](img/B16176_03_005.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_005.png)'
- en: 'Furthermore, the value *y*, the sum of the inputs to the final neuron, is a
    weighted sum of the sigmoidal inputs of the hidden units:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值*y*，即最终神经元的输入之和，是隐藏单元的S形输入的加权和：
- en: '![](img/B16176_03_006.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_006.png)'
- en: 'We also need a notion of when the network is performing well or badly at its
    task. A straightforward error function to use here is squared loss:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个概念，来判断网络在完成任务时是表现良好还是不良好。在这里可以使用的一个直观的误差函数是平方损失：
- en: '![](img/B16176_03_007.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_007.png)'
- en: where *yhat* is the estimated value (from the output of the model) and *y* is
    the real value, summed over all the input examples *J* and the outputs of the
    network *K* (where *K=1*, since there is only a single output value). Backpropagation
    begins with a "forward pass" where we compute the values of all the outputs in
    the inner and outer layers, to obtain the estimated values of *yhat*. We then
    proceed with a backward step to compute gradients to update the weights.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '其中*yhat*是估计值（来自模型输出）,*y*是所有输入示例*J*和网络*K*的输出的实际值的总和（其中*K=1*，因为只有一个输出值）。 反向传播开始于“前向传递”，在这一步中我们计算内层和外层所有输出的值，从而得到*yhat*的估计值。然后我们进行后向传递来计算梯度来更新权重。 '
- en: 'Our overall objective is to compute partial derivatives for the weights *w*
    and bias terms b in each neuron: ![](img/B16176_03_008.png) and ![](img/B16176_03_009.png),
    which will allow us to compute the updates for *b* and *w*. Towards this goal,
    let''s start by computing the update rule for the inputs in the final neuron;
    we want to date the partial derivative of the error *E* with respect to each of
    these inputs (in this example there are five, corresponding to the five hidden
    layer neurons), using the chain rule:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的总体目标是计算每个神经元的权重*w*和偏置项b的偏导数：![](img/B16176_03_008.png)和![](img/B16176_03_009.png)，这将使我们能够计算出*b*和*w*的更新。为了实现这个目标，让我们从计算最终神经元输入的更新规则开始；我们希望使用链式规则来计算误差*E*对于每个这些输入的偏导数（在本例中有五个，对应于五个隐藏层神经元）：
- en: '![](img/B16176_03_010.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_010.png)'
- en: 'We can get the value ![](img/B16176_03_011.png) by differentiating the loss
    function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对损失函数求导来得到值![](img/B16176_03_011.png)：
- en: '![](img/B16176_03_012.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_012.png)'
- en: 'which for an individual example is just the difference between the input and
    output value. For ![](img/B16176_03_013.png), we need to take the partial derivative
    of the sigmoid function:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个示例，这只是输入和输出值之间的差异。对于![](img/B16176_03_013.png)，我们需要对Sigmoid函数进行偏导数：
- en: '![](img/B16176_03_014.png)![](img/B16176_03_015.png)![](img/B16176_03_016.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_014.png)![](img/B16176_03_015.png)![](img/B16176_03_016.png)'
- en: 'Putting it all together, we have:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 综上所述，我们有：
- en: '![](img/B16176_03_017.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_017.png)'
- en: 'If we want to compute the gradient for a particular parameter of *x*, such
    as a weight *w* or bias term *b*, we need one more step:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算特定参数*x*（如权重*w*或偏置项*b*）的梯度，我们需要多做一步：
- en: '![](img/B16176_03_018.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_018.png)'
- en: 'We already know the first term and *x* depends on *w* only through the inputs
    from the lower layers *y* since it is a linear function, so we obtain:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道第一项，且*x*仅通过来自下层*y*的输入依赖于*w*，因为这是一个线性函数，所以我们得到：
- en: '![](img/B16176_03_019.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_019.png)'
- en: 'If we want to compute this derivative for one of the neurons in the hidden
    layer, we likewise take the partial derivative with respect to this input *y*[i],
    which is simply:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想为隐藏层中的一个神经元计算此导数，我们以同样的方式对这个输入*y*[i]进行偏导数计算，这很简单：
- en: '![](img/B16176_03_020.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_020.png)'
- en: 'So, in total we can sum over all units that feed into this hidden layer:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们总共可以对所有输入到这个隐藏层的单元求和：
- en: '![](img/B16176_03_021.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_021.png)'
- en: We can repeat this process recursively for any units in deeper layers to obtain
    the desired update rule, since we now know how to calculate the gradients for
    *y* or *w* at any layer. This makes the process of updating weights efficient
    since once we have computed the gradients through the backward pass we can combine
    consecutive gradients through the layers to get the required gradient at any depth
    of the network.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以递归地重复这个过程以获得所需的更新规则，因为我们现在知道如何在任何层计算*y*或*w*的梯度。这使得更新权重的过程变得高效，因为一旦我们通过反向传播计算了梯度，我们就可以结合连续的梯度通过层来得到网络任何深度所需的梯度。
- en: 'Now that we have the gradients for each *w* (or other parameter of the neuron
    we might want to calculate), how can we make a "learning rule" to update the weights?
    In their paper,^(17) Hinton et al. noted that we could apply an update to the
    model parameters after computing gradients on each sample batch but suggested
    instead applying an update calculated after averaging over all samples. The gradient
    represents the direction in which the error function is changing with the greatest
    magnitude with respect to the parameters; thus, to update, we want to push the
    weight in the *opposite* direction, with ![](img/B16176_03_022.png) the update,
    and *e* a small value (a step size):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经得到了每个*w*（或其他需要计算的神经元参数）的梯度，我们如何制定"学习规则"来更新权重？在他们的论文中，Hinton等人指出，我们可以在每个样本批处理上计算梯度后应用更新，但建议在所有样本上计算平均值后应用更新。梯度表示误差函数相对于参数发生最大变化的方向；因此，为了更新，我们希望将权重推向*相反*的方向，*e*是一个小值（步长）：
- en: '![](img/B16176_03_023.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_023.png)'
- en: 'Then at each time *t* during training we update the weight using this calculated
    gradient:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在训练过程中的每个时间点*t*，我们使用计算出的梯度更新权重：
- en: '![](img/B16176_03_024.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_024.png)'
- en: 'Extending this approach, Hinton et al. proposed an exponentially weighted update
    of the current gradient plus prior updates:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展这个方法，Hinton等人提出了一个当前梯度的指数加权更新加上先前更新的方法：
- en: '![](img/B16176_03_025.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_025.png)'
- en: where alpha is a decay parameter to weight the contribution of prior updates
    ranging from 0 to 1\. Following this procedure, we would initialize the weights
    in the network with some small random values, choose a step size *e* and iterate
    with forward and backward passes, along with updates to the parameters, until
    the loss function reaches some desired value.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中alpha是一个衰减参数，用于加权先前更新的贡献，取值范围从0到1。根据这个过程，我们将使用一些小的随机值初始化网络中的权重，选择步长*e*，并通过前向和后向传播以及参数更新进行迭代，直到损失函数达到某个期望值。
- en: Now that we have described the formal mathematics behind backpropagation, let
    us look at how it is implemented in practice in software packages such as TensorFlow
    2.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了反向传播背后的形式数学，让我们看看它在实践中如何在TensorFlow 2等软件包中实现。
- en: Backpropagation in practice
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的反向传播
- en: 'While it is useful to go through this derivation in order to understand how
    the update rules for a deep neural network are derived, this would clearly quickly
    become unwieldy for large networks and complex architectures. It''s fortunate,
    therefore, that TensorFlow 2 handles the computation of these gradients automatically.
    During the initialization of the model, each gradient is computed as an intermediate
    node between tensors and operations in the graph: as an example, see *Figure 3.4:*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过这种推导来理解深度神经网络的更新规则是有用的，但对于大型网络和复杂架构来说，这显然会很快变得难以管理。因此，幸运的是，TensorFlow 2
    可以自动处理这些梯度的计算。在模型初始化期间，每个梯度都被计算为图中张量和操作之间的中间节点：例如，参见*图3.4*：
- en: '![](img/B16176_03_04.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_04.png)'
- en: 'Figure 3.4: Inserting gradient operations into the TensorFlow graph^(18)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：将梯度操作插入到 TensorFlow 图中^(18)
- en: The left side of the preceding figure shows a cost function C computed from
    the output of a **Rectified Linear Unit** (**ReLU**) – a type of neuron function
    we'll cover later in this chapter), which in turn is computed from multiplying
    a weight vector by an input *x* and adding a bias term *b*. On the right, you
    can see that this graph has been augmented by TensorFlow to compute all the intermediate
    gradients required for backpropagation as part of the overall control flow.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图的左侧显示了一个成本函数 *C*，它是从**修正线性单元**（**ReLU**）的输出中计算得到的（一种我们将在本章后面介绍的神经元函数），而这个输出又是通过将一个权重向量乘以输入
    *x* 并添加一个偏置项 *b* 计算得到的。在右侧，你可以看到 TensorFlow 已经扩展了这个图，以计算作为整个控制流一部分所需的所有中间梯度。
- en: After storing these intermediate values, the task of combining them, as shown
    in the calculation in *Figure 3.4*, into a complete gradient through recursive
    operations falls to the GradientTape API. Under the hood, TensorFlow uses a method
    called **reverse-mode automatic differentiation** to compute gradients; it holds
    the dependent variable (the output *y*) fixed, and recursively computes backwards
    to the beginning of the network the required gradients.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储了这些中间值之后，通过递归操作将它们组合成完整的梯度的任务交给了GradientTape API。在幕后，TensorFlow 使用一种称为**反向模式自动微分**的方法来计算梯度；它将依赖变量（输出*y*）固定，并且从网络的末端递归地向前计算所需的梯度。
- en: 'For example, let''s consider a neural network of the following form:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑以下形式的神经网络：
- en: '![](img/B16176_03_05.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_05.png)'
- en: 'Figure 3.5: Reverse-mode automatic differentiation^(19)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：反向模式自动微分^(19)
- en: 'If we want to compute the derivative of the output *y* with respect to an input
    *x* we need to repeatedly substitute the outermost expression^(20):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算输出 *y* 关于输入 *x* 的导数，我们需要重复地代入最外层的表达式^(20)：
- en: '![](img/B16176_03_026.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_026.png)'
- en: Thus, to compute the desired gradient we need to just traverse the graph from
    top to bottom, storing each intermediate gradient as we calculate it. These values
    are stored on a record, referred to as a tape in reference to early computers
    in which information was stored on a magnetic tape,^(21) which is then used to
    replay the values for calculation. The alternative would be to use forward-mode
    automatic differentiation, computing from bottom to top. This requires two instead
    of one pass (for each branch feeding into the final value), but is conceptually
    simpler to implement and doesn't require the storage memory of reverse mode. More
    importantly, though, reverse-mode mimics the derivation of backpropagation that
    I described earlier.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算所需的梯度，我们只需从上到下遍历图，当我们计算时存储每个中间梯度。这些值被存储在一个记录上，被称为磁带，这是一个对早期计算机的参考，其中信息存储在磁带上，^(21)然后用于重放值以进行计算。另一种方法是使用前向模式自动微分，从下到上计算。这需要两次而不是一次传递（对于每个馈入到最终值的分支），但在概念上更容易实现，不需要反向模式的存储内存。然而，更重要的是，反向模式模仿了我之前描述的反向传播的推导。
- en: 'The tape (also known as the **Wengert Tape**, after one of its developers)
    is actually a data structure that you can access in the TensorFlow Core API. As
    an example, import the core library:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个磁带（也称为**Wengert Tape**，以其开发者之一命名）实际上是一个数据结构，你可以在 TensorFlow Core API 中访问到。例如，导入核心库：
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The tape is then available using the `tf.GradientTape()` method, with which
    you can evaluate gradients with respect to intermediate values within the graph^(22):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用 `tf.GradientTape()` 方法来获取这个磁带，在其中你可以评估与图中间值相关的梯度^(22)：
- en: '[PRE1]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By default, the memory resources used by `GradientTape()` are released once
    `gradient()` is called; however, you can also use the `persistent` argument to
    store these results^(23):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`GradientTape()` 使用的内存资源在调用 `gradient()` 后被释放；但是，你也可以使用 `persistent` 参数来存储这些结果^(23)：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you've seen how TensorFlow computes gradients in practice to evaluate
    backpropagation, let's return to the details of how the backpropagation technique
    evolved over time in response to challenges in practical implementation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到 TensorFlow 如何实际计算梯度以评估反向传播，让我们回顾一下反向传播技术是如何随着时间的推移而发展，以应对实际实现中的挑战的细节。
- en: The shortfalls of backpropagation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播的缺陷
- en: 'While the backpropagation procedure provides a way to update interior weights
    within the network in a principled way, it has several shortcomings that made
    deep networks difficult to use in practice. One is the problem of **vanishing
    gradients**. In our derivation of the backpropagation formulas, you saw that gradients
    for weights deeper in the network are a product of successive partial derivatives
    from higher layers. In our example, we used the sigmoid function; if we plot out
    the value of the sigmoid and its first derivative, we can see a potential problem:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然反向传播过程提供了一种以合理方式更新网络内部权重的方法，但它存在几个缺点，使得深度网络在实践中难以使用。其中一个是 **梯度消失** 的问题。在我们推导反向传播公式时，你看到网络中更深层次的权重的梯度是来自更高层的连续偏导数的乘积。在我们的例子中，我们使用了
    Sigmoid 函数；如果我们绘制出 Sigmoid 的值及其一阶导数，我们可以看到一个潜在的问题：
- en: '![](img/B16176_03_06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_06.png)'
- en: 'Figure 3.6: The sigmoid function and its gradient^(24)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：Sigmoid 函数及其梯度^(24)
- en: As the value of the sigmoid function increases or decreases towards the extremes
    (0 or 1, representing being either "off" or "on"), the values of the gradient
    vanish to near zero. This means that the updates to `w` and `b`, which are products
    of these gradients from hidden activation functions `y`, shrink towards zero,
    making the weights change little between iterations and making the parameters
    of the hidden layer neurons change very slowly during backpropagation. Clearly
    one problem here is that the sigmoid function saturates; thus, choosing another
    nonlinearity might circumvent this problem (this is indeed one of the solutions
    that was proposed as the ReLU, as we'll cover later).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Sigmoid 函数的值向极端值（0 或 1，代表“关闭”或“打开”）增加或减少，梯度的值趋近于零。这意味着从隐藏激活函数 `y` 的这些梯度得到的更新值
    `w` 和 `b` 会趋向于零，使得权重在迭代之间变化很小，使得反向传播过程中隐藏层神经元的参数变化非常缓慢。很显然，这里的一个问题是 Sigmoid 函数饱和；因此，选择另一个非线性函数可能会规避这个问题（这确实是作为
    ReLU 提出的解决方案之一，我们稍后会讨论）。
- en: 'Another problem is more subtle, and has to do with how the network utilizes
    its available free parameters. As you saw in *Chapter 1*, *An Introduction to
    Generative AI: "Drawing" Data from Models*, a posterior probability of a variable
    can be computed as a product of a likelihood and a prior distribution. We can
    see deep neural networks as a graphical representation of this kind of probability:
    the output of the neuron, depending upon its parameters, is a product of all the
    input values and the distributions on those inputs (the priors). A problem occurs
    when those values become tightly coupled. As an illustration, consider the competing
    hypotheses for a headache:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题更微妙，与网络如何利用其可用的自由参数有关。正如你在 *第 1 章*，*生成型 AI 简介：“从模型中“绘制”数据* 中看到的，变量的后验概率可以计算为似然和先验分布的乘积。我们可以将深度神经网络看作是这种概率的图形表示：神经元的输出，取决于其参数，是所有输入值和这些输入上的分布（先验）的乘积。当这些值变得紧密耦合时就会出现问题。举个例子，考虑一下头痛的竞争性假设：
- en: '![](img/B16176_03_07.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_07.png)'
- en: 'Figure 3.7: The explaining away effect'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：解释逆效应
- en: If a patient has cancer, the evidence is so overwhelming that whether they have
    a cold or not provides no additional value; in essence, the value of the two prior
    hypotheses becomes coupled because of the influence of one. This makes it intractable
    to compute the relative contribution of different parameters, particularly in
    a deep network; we will cover this problem in our discussion of Restricted Boltzmann
    Machine and Deep Belief Networks in *Chapter 4*, *Teaching Networks to Generate
    Digits*. As we will describe in more detail in that chapter, a 2006 study^(25)
    showed how to counteract this effect, and was one of the first demonstrations
    of tractable inference in deep neural networks, a breakthrough that relied upon
    a generative model that produced images of hand-drawn digits.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个病人患有癌症，那么关于他们是否感冒的证据是如此压倒性，以至于没有提供额外价值；实际上，两个先前的假设的价值由于其中一个的影响而变得耦合。这使得计算不同参数的相对贡献变得棘手，特别是在深层网络中；我们将在我们关于《第四章，教网络生成数字》中讨论受限玻尔兹曼机和深度信念网络的问题。正如我们在该章节中将更详细地描述的那样，一项2006年的研究^(25)展示了如何抵消这种效应，这是对深度神经网络中可行推断的最早的一次突破，这一突破依赖于产生手绘数字图像的生成模型。
- en: Beyond these concerns, other challenges in the more widespread adoption of neural
    networks in the 1990s and early 2000s were the availability of methods such as
    Support Vector Machines^(26), Gradient and Stochastic Gradient Boosting Models,^(27)
    Random Forests,^(28) and even penalized regression methods such as LASSO^(29)
    and Elastic Net,^(30) for classification and regression tasks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些问题之外，在20世纪90年代和21世纪初，神经网络更广泛应用的其他挑战还包括像支持矢量机^(26)、梯度和随机梯度提升模型^(27)、随机森林^(28)甚至是惩罚回归方法如LASSO^(29)和Elastic
    Net^(30)这样的方法，用于分类和回归任务。
- en: While, in theory, deep neural networks had potentially greater representational
    power than these models since they built hierarchical representations of the input
    data through successive layers in contrast to the "shallow" representation given
    by a single transformation such as a regression weight or decision tree, in practice
    the challenges of training deep networks made these "shallow" methods more attractive
    for practical applications. This was coupled with the fact that larger networks
    required tuning thousands or even millions of parameters, requiring large-scale
    matrix calculations that were infeasible before the explosion of cheap compute
    resources – including GPUs and TPUs especially suited to rapid matrix calculations
    – available from cloud vendors made these experiments practical.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理论上，深度神经网络的表征能力可能比这些模型更强，因为它们通过连续层构建输入数据的分层表示，与通过单一转换给出的“浅”表示如回归权重或决策树相反，但在实践中，训练深层网络的挑战使得这些“浅”方法对实际应用更有吸引力。这也与较大网络需要调整成千上万甚至是百万参数的事实相搭上了较大计算资源的事实，使这些实验在云供应商提供的廉价计算资源的爆炸之前是不可行的，包括GPU和TPU特别适用于快速矩阵计算。
- en: 'Now that we''ve covered the basics of training simple network architectures,
    let''s turn to more complex models that will form the building blocks of many
    of the generative models in the rest of the book: CNNs and sequence models (RNNs,
    LSTMs, and others).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了训练简单网络架构的基础知识，让我们转向更复杂的模型，这些模型将构成书中许多生成模型的基础：CNNs和序列模型（RNNs，LSTMs等）。
- en: 'Varieties of networks: Convolution and recursive'
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络的种类：卷积和递归
- en: Up until now we've primarily discussed the basics of neural networks by referencing
    feedforward networks, where every input is connected to every output in each layer.
    While these feedforward networks are useful for illustrating how deep networks
    are trained, they are only one class of a broader set of architectures used in
    modern applications, including generative models. Thus, before covering some of
    the techniques that make training large networks practical, let's review these
    alternative deep models.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要通过引用前馈网络来讨论神经网络的基础知识，其中每个输入都连接到每个层的每个输出。虽然这些前馈网络有助于说明深层网络的训练方式，但它们只是现代应用中使用的一类更广泛架构的一部分，包括生成模型。因此，在讨论使训练大型网络变得实用的一些技术之前，让我们回顾一下这些替代的深度模型。
- en: 'Networks for seeing: Convolutional architectures'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉网络：卷积架构
- en: 'As noted at the beginning of this chapter, one of the inspirations for deep
    neural network models is the biological nervous system. As researchers attempted
    to design computer vision systems that would mimic the functioning of the visual
    system, they turned to the architecture of the retina, as revealed by physiological
    studies by neurobiologists David Huber and Torsten Weisel in the 1960s.^(31) As
    previously described, the physiologist Santiago Ramon Y Cajal provided visual
    evidence that neural structures such as the retina are arranged in vertical networks:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头所指出的，深度神经网络模型的灵感之一是生物神经系统。当研究人员试图设计可以模仿视觉系统功能的计算机视觉系统时，他们转向了视网膜的结构，这是在
    20 世纪 60 年代神经生物学家 David Huber 和 Torsten Weisel 的生理学研究中揭示的。^(31) 正如以前所描述的，生理学家
    Santiago Ramon Y Cajal 提供了神经结构如视网膜被安排在垂直网络中的视觉证据。
- en: '![](img/B16176_03_08_a+b.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_08_a+b.png)'
- en: 'Figure 3.8: The "deep network" of the retina^(32 33)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：视网膜的“深层神经网络”^(32 33)
- en: Huber and Weisel studied the retinal system in cats, showing how their perception
    of shapes is composed of the activity of individual cells arranged in a column.
    Each column of cells is designed to detect a specific orientation of an edge in
    an input image; images of complex shapes are stitched together from these simpler
    images.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 和 Weisel 研究了猫的视网膜系统，展示了它们对形状的知觉是由排列在一列中的单个细胞的活动所组成的。每一列细胞都被设计用来检测输入图像中边缘的特定方向；复杂形状的图像是由这些简单图像拼接在一起的。
- en: Early CNNs
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期的 CNNs
- en: This idea of columns inspired early research into CNN architectures^(34). Instead
    of learning individual weights between units as in a feedforward network, this
    architecture (*Figure 3.9*) uses shared weights within a group of neurons specialized
    to detect a specific edge in an image. The initial layer of the network (denoted
    **H1**) consists of 12 groups of 64 neurons each. Each of these groups is derived
    by passing a 5 x 5 grid over the 16 x 16-pixel input image; each of the 64 5 x
    5 grids in this group share the same weights, but are tied to different spatial
    regions of the input. You can see that there must be 64 neurons in each group
    to cover the input image if their receptive fields overlap by two pixels.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这种列的概念启发了对 CNN 结构的早期研究^(34)。与前馈网络中学习单元之间的个体权重不同，这种结构（*图 3.9*）使用了专门用于检测图像中特定边缘的一组神经元中的共享权重。网络的初始层（标记为
    **H1**）由每个 64 个神经元的 12 组组成。这些组中的每个都是通过在 16 x 16 像素的输入图像上传递一个 5 x 5 的网格来得到的；这个组中的每一个
    64 个 5 x 5 的网格共享相同的权重，但与输入的不同空间区域相关联。你可以看到，如果它们的接受域重叠了两个像素，那么每个组中必须有 64 个神经元来覆盖输入图像。
- en: When combined, these 12 groups of neurons in layer **H1** form 12 8 x 8 grids
    representing the presence or absence of a particular edge within a part of the
    image – the 8 x 8 grid is effectively a down-sampled version of the image (*Figure
    3.9*). This weight sharing makes intuitive sense in that the kernel represented
    by the weight is specified to detect a distinct color and/or shape, regardless
    of where it appears in the image. An effect of this down-sampling is a degree
    of positional invariance; we only know the edge occurred somewhere within a region
    of the image, but not the exact location due to the reduced resolution from downsampling.
    Because they are computed by multiplying a 5 x 5 matrix (kernel) with a part of
    the image, an operation used in image blurring and other transformations, these
    5 x 5 input features are known as **convolutional kernels**, and give the network
    its name.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当这 12 组神经元在 **H1** 层中结合在一起时，它们形成了 12 个表示图像中特定边缘的存在或不存在的 8 x 8 网格（*图 3.9*）。这种权重共享在直观上是有意义的，因为由权重表示的卷积核被指定用来检测图像中的不同颜色和/或形状，不管它出现在图像的哪个位置。这种降采样的效果是一定程度上的位置不变性；我们只知道边缘发生在图像某个区域内，但由于降采样导致的分辨率降低，我们无法知道确切位置。因为它们是通过将一个
    5 x 5 的矩阵（卷积核）与图像的一部分相乘得到的，这种操作被用在图像模糊和其他转换中，这 5 x 5 的输入特征被称为 **卷积核**，也给网络起了名字。
- en: '![](img/B16176_03_09.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_09.png)'
- en: 'Figure 3.9: The CNN^(35)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：卷积神经网络^(35)
- en: Once we have these 12 8 x 8 downsampled versions of the image, the next layer
    (**H2**) also has 12 groups of neurons; here, the kernels are 5 x 5 x 8 – they
    traverse the surface of an 8 x 8 map from **H1**, across 8 of the 12 groups. We
    need 16 neurons of these 5 x 5 x 8 groups since a 5 x 5 grid can be moved over
    four times up and down on an 8 x 8 grid to cover all the pixels in the 8 x 8 grid.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有了这12个缩小了的8 x 8图像时，下一层（**H2**）还有12组神经元；在这里，卷积核是5 x 5 x 8——它们横跨从**H1**上的一个8
    x 8地图，遍及12个中的8个组。由于一个5 x 5的网格可以在8 x 8的网格上上下移动四次以覆盖8 x 8网格中的所有像素，我们需要16个这样的5 x
    5 x 8组的神经元。
- en: Just like deeper cells in the visual cortex, the deeper layers in the network
    integrate across multiple columns to combine information from different edge detectors
    together.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就像视觉皮层中更深层的细胞一样，网络中的更深层对来自不同边缘检测器的多个列进行整合，将信息组合在一起。
- en: Finally, the third hidden layer of this network (**H3**) contains all-to-all
    connections between 30 hidden units and the 12 x 16 units in the **H2**, just
    as in a traditional feedforward network; a final output of 10 units classifies
    the input image as one of 10 hand-drawn digits.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，该网络的第三个隐藏层（**H3**）包含30个隐藏单元和**H2**中的12 x 16个单元之间的全全连接，就像在传统的前馈网络中一样；最终的10个输出单元将输入图像分类为10个手写数字之一。
- en: Through weight sharing, the overall number of free parameters in this network
    is reduced, though it is still large in absolute terms. While backpropagation
    was used successfully for this task, it required a carefully designed network
    for a rather limited set of images with a restricted set of outcomes – for real-world
    applications, such as detecting objects from hundreds or thousands of possible
    categories, other approaches would be necessary.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过权重共享，在该网络中的自由参数总数得到了减少，虽然在绝对术语中仍然很大。虽然反向传播成功地用于此任务，但需要为一组成员受限的图像设计精心的网络，这些图像具有局限性的结果——对于如检测来自数百或数千个可能类别的对象等实际应用，需要采用其他方法。
- en: AlexNet and other CNN innovations
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet和其他CNN创新技术
- en: 'A 2012 article that produced state-of-the-art results classifying the 1.3 million
    images in ImageNet into 1,000 classes using a model termed AlexNet demonstrates
    some of the later innovations that made training these kinds of models practical.^(36)
    One, as I''ve alluded to before, is using ReLUs^(37) in place of sigmoids or hyperbolic
    tangent functions. A ReLU is a function of the form:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年的一篇文章产生了最先进的结果，使用一个被称为AlexNet的模型将ImageNet中的130万张图像分类为1000种分类。这些模型要实现训练，需要采用一些后来的创新技术。（36）如我之前提到的一样，一个是使用ReLU（37）替代sigmoid或双曲正切函数。ReLU是以下形式的函数：
- en: '![](img/B16176_03_027.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_027.png)'
- en: In contrast to the sigmoid function, or tanh, in which the derivative shrinks
    to 0 as the function is saturated, the ReLU function has a constant gradient and
    a discontinuity at 0 (*Figure 3.10*). This means that the gradient does not saturate
    and causes deeper layers of the network to train more slowly, leading to intractable
    optimization.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与sigmoid函数或tanh相比，在函数饱和时，其导数会缩小至0，而ReLU函数具有恒定的梯度和0处的不连续性（*图3.10*）。这意味着梯度不会饱和，导致网络的深层训练更慢，导致优化困难。
- en: '![](img/B16176_03_10.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_10.png)'
- en: 'Figure 3.10: Gradients of alternative activation functions^(38)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：替代激活函数的梯度（38）
- en: 'While advantageous due to non-vanishing gradients and their low computational
    requirements (as they are simply thresholded linear transforms), ReLU functions
    have the downside that they can "turn off" if the input falls below 0, leading
    again to a 0 gradient. This deficiency was resolved by later work in which a "leak"
    below 0 was introduced^(39):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ReLU函数具有非消失梯度和低计算要求的优势（因为它们只是阈值线性变换），但缺点是如果输入低于0，则它们可能会“关闭”，导致再次出现0梯度。这个问题在之后的工作中得到解决，在0以下引入了一个“泄漏”。（39）
- en: '![](img/B16176_03_028.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_028.png)'
- en: 'A further refinement is to make this threshold adaptive with a slope *a*, the
    **Parameterized Leak ReLU** (**PReLU**)^(40):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的改进是使此阈值自适应，具有斜率为*a*的**参数化泄漏ReLU**（**PReLU**）。（40）
- en: '![](img/B16176_03_029.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_029.png)'
- en: Another trick used by AlexNet is dropout.^(41) The idea of dropout is inspired
    by ensemble methods in which we average the predictions of many models to obtain
    more robust results. Clearly for deep neural networks this is prohibitive; thus
    a compromise is to randomly set the values of a subset of neurons to 0 with a
    probability of 0.5\. These values are reset with every forward pass of backpropagation,
    allowing the network to effectively sample different architectures since the "dropped
    out" neurons don't participate in the output in that pass.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet使用的另一个技巧是辍学。^(41) 辍学的想法受到合奏方法的启发，在合奏方法中，我们对许多模型的预测进行平均，以获得更稳健的结果。显然，对于深度神经网络来说，这是不可行的；因此，一个妥协方案是以0.5的概率随机将某些神经元的值设为0。这些值在每次反向传播的前向传递中被重置，允许网络有效地对不同的架构进行采样，因为“辍学”的神经元在该传递中不参与输出。
- en: '![](img/B16176_03_11.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_11.png)'
- en: 'Figure 3.11: Dropout'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：辍学
- en: 'Yet another enhancement used in AlexNet is local response normalization. Even
    though ReLUs don''t saturate in the same manner as other units, the authors of
    the model still found value in constraining the range of output. For example,
    in an individual kernel, they normalized the input using values of adjacent kernels,
    meaning the overall response was rescaled^(42):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet中使用的另一个增强是局部响应归一化。尽管ReLU不像其他单元那样饱和，模型的作者仍然发现限制输出范围有价值。例如，在一个单个卷积核中，他们使用相邻卷积核的值对输入进行归一化，这意味着总体响应被重新缩放^(42)：
- en: '![](img/B16176_03_030.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_030.png)'
- en: 'where *a* is the unnormalized output at a given *x*, *y* location on an image,
    the sum over *j* is over adjacent kernels, and *B*, *k*, and alpha are hyperparameters.
    This rescaling is reminiscent of a later innovation used widely in both convolutional
    and other neural network architectures, batch normalization^(43). Batch normalization
    also applies a transformation on "raw" activations within a network:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a*是图像上给定*x*，*y*位置处的非标准化输出，*j*的总和是在相邻卷积核上，*B*，*k*和alpha是超参数。这种重新缩放让人想起后来被广泛应用于卷积和其他神经网络架构中的一种创新，批量归一化^(43)。批量归一化还对网络内部的“原始”激活应用转换：
- en: '![](img/B16176_03_031.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_031.png)'
- en: where *x* is the unnormalized output, and *B* and *y* are scale and shift parameters.
    This transformation is widely applied in many neural network architectures to
    accelerate training, through the exact reason why it is effective remains a topic
    of debate.^(44)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是非标准化输出，*B*和*y*是尺度和偏移参数。这种转换被广泛应用于许多神经网络架构，以加速训练，尽管它的有效原因仍然是争论的话题。^(44)
- en: Now that you have an idea of some of the methodological advances that made training
    large CNNs possible, let's examine the structure of AlexNet to see some additional
    architectural components that we will use in the CNNs we implement in generative
    models in later chapters.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对使大型CNN训练成为可能的一些方法论进步有了一些了解，让我们来研究AlexNet的结构，看看我们将在后面章节中实现的生成模型中使用的一些额外的架构组件。
- en: AlexNet architecture
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet架构
- en: While the architecture of AlexNet shown in *Figure 3.12* might look intimidating,
    it is not so difficult to understand once we break up this large model into individual
    processing steps. Let's start with the input images and trace how the output classification
    is computed for each image through a series of transformations performed by each
    subsequent layer of the neural network.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*图3.12*中的AlexNet架构看起来令人生畏，但一旦我们将这个大型模型分解为单独的处理步骤，就不那么难理解了。让我们从输入图像开始，跟踪通过每个后续神经网络层的一系列转换为每个图像计算输出分类的方法。
- en: '![](img/B16176_03_12.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_12.png)'
- en: 'Figure 3.12: AlexNet'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：AlexNet
- en: The input images to AlexNet are size 224 x 224 x 3 (for RGB channels). The first
    layer consists of groups of 96 units and 11 x 11 x 3 kernels; the output is response
    normalized (as described previously) and max pooled. Max pooling is an operation
    that takes the maximum value over an *n* x *n* grid to register whether a pattern
    appeared *anywhere* in the input; this is again a form of positional invariance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到AlexNet的图像大小为224 x 224 x 3（对于RGB通道）。第一层由96个单元和11 x 11 x 3卷积核组成；输出经过响应归一化（如前所述）和最大化池化。最大化池化是一种采取*n*
    x *n*网格上的最大值来记录输入中是否“任何位置”出现模式的操作；这又是一种位置不变性的形式。
- en: The second layer is also a set of kernels of size 5 x 5 x 8 in groups of 256\.
    The third through to fifth hidden layers have additional convolutions, without
    normalization, followed by two fully connected layers and an output of size 1,000
    representing the possible image classes in ImageNet. The authors of AlexNet used
    several GPUs to train the model, and this acceleration is important to the output.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层也是一组规模为5 x 5 x 8的卷积，以256个为一组。第三层到第五层都有额外的卷积，没有规范化，接着是两个全连接层和一个输出大小为1,000表示ImageNet中可能的图像类。AlexNet的作者使用了几个GPU来训练模型，这种加速对输出非常重要。
- en: '![](img/B16176_03_13.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_13.png)'
- en: 'Figure 3.13: Image kernels from AlexNet'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：来自AlexNet的图像核
- en: Looking at the features learned during training in the initial 11 x 11 x 3 convolutions
    (*Figure 3.13*), we can see recognizable edges and colors. While the authors of
    AlexNet don't show examples of neurons higher in the network that synthesize these
    basic features, an illustration is provided by another study in which researchers
    trained a large CNN to classify images in YouTube videos, yielding a neuron in
    the upper reaches of the network that appeared to be a cat detector (*Figure 3.14*).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始的11 x 11 x 3卷积中，即训练过程中学到的特征中（*图3.13*），我们可以看到可识别的边缘和颜色。虽然AlexNet的作者没有展示出网络中更高层次的神经元合成这些基本特征的例子，但另一项研究提供了一个示例，在该研究中，研究人员训练了一个大型的CNN来对YouTube视频中的图像进行分类，得到了网络最上层的一个神经元，它似乎是一个猫探测器（*图3.14*）。
- en: '![](img/B16176_03_14.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_14.png)'
- en: 'Figure 3.14: A cat detector learned from YouTube videos^(45)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：从YouTube视频中学习到的猫探测器^(45)
- en: This overview should give you an idea of *why* CNN architectures look the way
    they do, and what developments have allowed them to become more tractable as the
    basis for image classifiers or image-based generative models over time. We will
    now turn to a second class of more specialized architectures – RNNs – that's used
    to develop time or sequence-based models.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概述应该让你明白CNN架构看起来的样子，以及什么样的发展使得它们随着时间的推移而成为图像分类器或基于图像的生成模型的基础更加可行。现在我们将转向另一类更专业的架构——RNN，这种架构用于开发时间或基于序列的模型。
- en: Networks for sequence data
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据的网络
- en: In addition to image data, natural language text has also been a frequent topic
    of interest in neural network research. However, unlike the datasets we've examined
    thus far, language has a distinct *order* that is important to its meaning. Thus,
    to accurately capture the patterns in language or time-dependent data, it is necessary
    to utilize networks designed for this purpose.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像数据，自然语言文本也一直是神经网络研究中的一个热门话题。然而，与我们迄今为止检查的数据集不同，语言有一个重要的*顺序*与其含义相关。因此，为了准确地捕捉语言或时间相关数据中的模式，有必要使用专门设计用于此目的的网络。
- en: RNNs and LSTMs
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN和LSTM
- en: 'Let''s imagine we are trying to predict the next word in a sentence, given
    the words up until this point. A neural network that attempted to predict the
    next word would need to take into account not only the current word but a variable
    number of prior inputs. If we instead used only a simple feedforward MLP, the
    network would essentially process the entire sentence or each word as a vector.
    This introduces the problem of either having to pad variable-length inputs to
    a common length and not preserving any notion of correlation (that is, which words
    in the sentence are more relevant than others in generating the next prediction),
    or only using the last word at each step as the input, which removes the context
    of the rest of the sentence and all the information it can provide. This kind
    of problem inspired the "vanilla" RNN^(46), which incorporates not only the current
    input but the prior step''s hidden state in computing a neuron''s output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们试图预测句子中的下一个词，给定到目前为止的词。试图预测下一个词的神经网络不仅需要考虑当前词，还需要考虑可变数量的先前输入。如果我们只使用一个简单的前馈MLP，该网络实际上会将整个句子或每个词都处理为一个向量。这引入了这样一个问题：要么必须将可变长度的输入填充到一个共同的长度，并且不保留任何相关性的概念（也就是说，在生成下一个预测时，句子中哪些单词比其他单词更相关），或者在每一步中只使用上一个词作为输入，这样会丢失句子其余部分的上下文和提供的所有信息。这种问题激发了“原生”RNN^(46)，它在计算一个神经元的输出时，不仅考虑当前输入，还考虑前一步的隐藏状态：
- en: '![](img/B16176_03_032.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_032.png)'
- en: One way to visualize this is to imagine each layer feeding recursively into
    the next timestep in a sequence. In effect, if we "unroll" each part of the sequence,
    we end up with a very deep neural network, where each layer shares the same weights.^(47)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将这个过程想象为每一层递归地馈送到下一个时间步骤的序列中。实际上，如果我们“展开”序列的每个部分，我们最终得到一个非常深的神经网络，其中每一层共享相同的权重。^(47)
- en: '![](img/B16176_03_15.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_15.png)'
- en: 'Figure 3.15: The unrolled RNN^(48)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15：展开的RNN^(48)
- en: The same difficulties that characterize training deep feedforward networks also
    apply to RNNs; gradients tend to die out over long distances using traditional
    activation functions (or explode if the gradients become greater than 1).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度前馈网络所具有的困难也同样适用于循环神经网络；使用传统激活函数时，梯度往往在长距离上衰减（或者如果梯度大于1，则爆炸）。
- en: 'However, unlike feedforward networks, RNNs aren''t trained with traditional
    backpropagation, but rather a variant known as **backpropagation through time**
    (**BPTT**): the network is unrolled, as before, and backpropagation is used, averaging
    over errors *at each time point* (since an "output," the hidden state, occurs
    at each step).^(49) Also, in the case of RNNs, we run into the problem that the
    network has a very short memory; it only incorporates information from the most
    recent unit before the current one and has trouble maintaining long-range context.
    For applications such as translation, this is clearly a problem, as the interpretation
    of a word at the end of a sentence may depend on terms near the beginning, not
    just those directly preceding it.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与前馈网络不同，RNNs不是用传统的反向传播进行训练，而是用一种称为**时间反向传播**（**BPTT**）的变体：网络被展开，如前所述，使用反向传播，对每个时间点的误差进行平均处理（因为每一步都有一个“输出”，即隐藏状态）。^(49)此外，在RNNs的情况下，我们遇到的问题是网络的记忆非常短暂；它只包含最近单元的信息，而当前单元之前的信息则难以保持长期上下文。对于翻译等应用来说，这显然是一个问题，因为句子末尾的单词的解释可能依赖于句子开头的术语，而不仅仅是直接前面的术语。
- en: The LSTM network was developed to allow RNNs to maintain a context or state
    over long sequences.^(50)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络的开发是为了使RNNs能够在长序列上保持上下文或状态。^(50)
- en: '![](img/B16176_03_16.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_16.png)'
- en: 'Figure 3.16: LSTM network'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16：LSTM网络
- en: In a vanilla RNN, we only maintain a short-term memory *h* coming from the prior
    step's hidden unit activations. In addition to this short-term memory, the LSTM
    architecture introduces an additional layer *c*, the "long-term" memory, which
    can persist over many timesteps. The design is in some ways reminiscent of an
    electrical capacitor, which can use the *c* layer to store up or hold "charge,"
    and discharge it once it has reached some threshold. To compute these updates,
    an LSTM unit consists of a number of related neurons, or gates, that act together
    to transform the input at each time step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的RNN中，我们只保留来自前一步隐藏单元激活的短期记忆*h*。除了这个短期记忆外，LSTM架构引入了一个额外的层*c*，即“长期”记忆，它可以持续多个时间步长。从某种意义上说，这种设计让人想起了电容器，它可以使用*c*层来储存或保持“电荷”，一旦达到某个阈值就释放它。为了计算这些更新，一个LSTM单元由许多相关的神经元或门组成，这些门在每个时间步骤上一起作用来转换输入。
- en: 'Given an input vector *x*, and the hidden state *h*, at the previous time *t-1*,
    at each time step an LSTM first computes a value from 0 to 1 for each element
    of *c* representing what fraction of information is "forgotten" of each element
    of the vector:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入向量*x*和前一时刻*t-1*的隐藏状态*h*，在每个时间步长，LSTM首先计算了一个值，从0到1表示*c*的每个元素中“遗忘”了多少信息：
- en: '![](img/B16176_03_033.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_033.png)'
- en: 'We make a second, similar calculation to determine what from the input value
    to preserve:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行第二次类似的计算来确定要保留输入值的哪些部分：
- en: '![](img/B16176_03_034.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_034.png)'
- en: 'We now know which elements of *c* are updated; we can compute this update as
    follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了*c*的哪些元素被更新了；我们可以计算这个更新如下：
- en: '![](img/B16176_03_035.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_035.png)'
- en: where ![](img/B16176_03_036.png) is a Hadamard product (element-wise multiplication).
    In essence this equation tells us how to compute updates using the tanh transform,
    filter them using the input gate, and combine them with the prior time step's
    long-term memory using the forget gate to potentially filter out old values.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![图片](img/B16176_03_036.png)是一个Hadamard积（逐元素乘法）。本质上，这个方程告诉我们如何使用tanh变换计算更新，使用输入门过滤它们，并使用忘记门将它们与前一个时间步的长期记忆结合起来，以潜在地过滤掉旧值。
- en: 'To compute the output at each time step, we compute another output gate:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算每个时间步的输出，我们计算另一个输出门：
- en: '![](img/B16176_03_037.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16176_03_037.png)'
- en: 'And to compute the final output at each step (the hidden layer fed as short-term
    memory to the next step) we have:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在每一步计算最终输出时（隐藏层作为下一步的短期记忆提供给下一步），我们有：
- en: '![](img/B16176_03_038.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_038.png)'
- en: Many variants of this basic design have been proposed; for example, the "peephole"
    LSTM substituted *h*(*t*-*1*) with *c*(*t*-*1*) (thus each operation gets to "peep"
    at the long-term memory cell),^(51) while the GRU^(52) simplifies the overall
    design by removing the output gate. What these designs all have in common is that
    they avoid the vanishing (or exploding) gradient difficulties seen during the
    training of RNNs, since the long-term memory acts as a buffer to maintain the
    gradient and propagate neuronal activations over many timesteps.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了许多这种基本设计的变体；例如，“窥视孔”LSTM用*c*(*t*-*1*)替代了*h*(*t*-*1*)（因此每个操作都可以“窥视”长期记忆单元），^(51)而GRU^(52)通过删除输出门简化了整体设计。这些设计的共同之处在于，它们避免了训练RNN时出现的梯度消失（或爆炸）困难，因为长期记忆充当缓冲区，以维持梯度并在许多时间步骤上传播神经元激活。
- en: Building a better optimizer
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建更好的优化器
- en: 'In this chapter we have so far discussed several examples in which better neural
    network architectures allowed for breakthroughs; however, just as (and perhaps
    even more) important is the *optimization procedure* used to minimize the error
    function in these problems, which "learns" the parameters of the network by selecting those
    that yield the lowest error. Referring to our discussion of backpropagation, this
    problem has two components:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经讨论了几个例子，其中更好的神经网络架构实现了突破；然而，与此同样（甚至更加）重要的是用于在这些问题中最小化误差函数的*优化过程*，通过选择产生最低误差的参数来“学习”网络的参数。回顾我们对反向传播的讨论，这个问题有两个组成部分：
- en: '**How to initialize the weights**: In many applications historically, we see
    that the authors used random weights within some range, and hoped that the use
    of backpropagation would result in at least a locally minimal loss function from
    this random starting point.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何初始化权重**：在许多历史应用中，我们看到作者使用了一定范围内的随机权重，并希望通过反向传播的使用从这个随机起始点至少得到一个局部最小化的损失函数。'
- en: '**How to find the local minimum loss**: In basic backpropagation, we used gradient
    descent using a fixed learning rate and a first derivative update to traverse
    the potential solution space of weight matrices; however, there is good reason
    to believe there might be more efficient ways to find a local minimum.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何找到局部最小损失**：在基本的反向传播中，我们使用梯度下降和固定学习率以及一阶导数更新来遍历权重矩阵的潜在解空间；然而，有充分的理由相信可能存在更有效的方法来找到局部最小值。'
- en: In fact, both of these have turned out to be key considerations towards progress
    in deep learning research.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这两者都被证明是深度学习研究进展的关键考虑因素。
- en: Gradient descent to ADAM
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降到ADAM
- en: As we saw in our discussion of backpropagation, the original version proposed
    in 1986 for training neural networks averaged the loss over the *entire dataset*
    before taking the gradient and updating the weights. Obviously, this is quite
    slow and makes distributing the model difficult, as we can't split up the input
    data and model replicas; if we use them, each needs to have access to the whole
    dataset.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在反向传播的讨论中看到的那样，1986年提出的用于训练神经网络的原始版本在获取梯度并更新权重之前对*整个数据集*进行了损失平均。显然，这相当慢，并且使模型的分发变得困难，因为我们无法分割输入数据和模型副本；如果我们使用它们，每个副本都需要访问整个数据集。
- en: In contrast, SGD computes gradient updates after *n* samples, where *n* could
    a range from 1 to *N*, the size of the dataset. In practice, we usually perform
    *mini-batch* gradient descent, in which *n* is relatively small, and we randomize
    assignment of data to the *n* batches after each epoch (a single pass through
    the data).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，SGD在*n*个样本后计算梯度更新，其中*n*可以是从1到*N*（数据集的大小）的范围。在实践中，我们通常执行*小批量*梯度下降，其中*n*相对较小，而且我们在每个epoch（数据的一次遍历）后随机分配数据给*n*批次。
- en: However, SGD can be slow, leading researchers to propose alternatives that accelerate
    the search for a minimum. As seen in the original backpropagation algorithm, one
    idea is to use a form of exponentially weighted momentum that remembers prior
    steps and continues in promising directions. Variants have been proposed, such
    as *Nesterov Momentum*, which adds a term to increase this acceleration.^(53)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，SGD可能会很慢，导致研究人员提出加速搜索最小值的替代方案。正如在原始反向传播算法中所见，一个想法是使用一种记住先前步骤并在前进方向继续的指数加权动量形式。已经有提出了各种变体，如*Nesterov
    Momentum*，它增加了一个项来增加这种加速^（53）。
- en: '![](img/B16176_03_039.png)![](img/B16176_03_040.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_039.png)![](img/B16176_03_040.png)'
- en: In comparison to the momentum term used in the original backpropagation algorithm,
    the addition of the current momentum term to the gradient helps keep the momentum
    component aligned with the gradient changes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始反向传播算法中使用的动量项相比，将当前动量项加到梯度中有助于保持动量部分与梯度变化保持一致。
- en: 'Another optimization, termed **Adaptive Gradient** (**Adagrad**)^(54), scales
    the learning rate for each update by the running the sum of squares (*G*) of the
    gradient of that parameter; thus, elements that are frequently updated are downsampled,
    while those that are infrequently updated are pushed to update with greater magnitude:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种优化方法，称为**自适应梯度**（**Adagrad**）^（54），通过该参数梯度的平方和（*G*）来缩放每次更新的学习率；因此，经常更新的元素被降采样，而不经常更新的元素被推动以更大的幅度进行更新：
- en: '![](img/B16176_03_041.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_041.png)'
- en: This approach has the downside that as we continue to train the neural network,
    the sum *G* will increase indefinitely, ultimately shrinking the learning rate
    to a very small value. To fix this shortcoming, two variant methods, RMSProp^(55)
    (frequently applied to RNNs) and AdaDelta^(56) impose fixed-width windows of n
    steps in the computation of *G*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，随着我们继续训练神经网络，总和*G*将无限增加，最终将学习率缩小到一个非常小的值。为了解决这个缺点，提出了两种变体方法，RMSProp^（55）（经常应用于RNN）和AdaDelta^（56），在计算*G*时加入固定宽度窗口的n步。
- en: '**Adaptive Momentum Estimation** (**ADAM**)^(57) can be seen as an attempt
    to combine momentum and AdaDelta; the momentum calculation is used to preserve
    the history of past gradient updates, while the sum of decaying squared gradients
    within a fixed update window used in AdaDelta is applied to scale the resulting
    gradient.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应动量估计**（**ADAM**）^（57）可以看作是一种尝试将动量和AdaDelta结合起来；动量计算用于保留过去梯度更新的历史，而在AdaDelta中使用的固定更新窗口内的衰减平方梯度总和用于调整结果梯度的大小。'
- en: 'The methods mentioned here all share the property of being *first order*: they
    involve only the first derivative of the loss with respect to the input. While
    simple to compute, this may introduce practical challenges with navigating the
    complex solution space of neural network parameters. As shown in *Figure 3.17*,
    if we visualize the landscape of weight parameters as a ravine, then first-order
    methods will either move too quickly in areas in which the curvature is changing
    quickly (the top image) overshooting the minima, or will change too slowly within
    the minima "ravine," where the curvature is low. An ideal algorithm would take
    into account not only the curvature but the *rate of change* of the curvature,
    allowing an optimizer order method to take larger step sizes when the curvature
    changes very slowly and vice versa (the bottom image).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提到的方法都具有*一阶*的特性：它们只涉及损失对输入的一阶导数。虽然计算简单，但这可能导致在神经网络参数的复杂解空间中导航时出现实际挑战。如*图3.17*所示，如果我们将权重参数的景观视为一条沟壑，那么一阶方法要么在曲率快速变化的区域移动得太快（顶部图像），超调极小值，要么在曲率较低的极小值“沟壑”中移动得太慢。理想的算法将考虑曲率和曲率变化的*变化速率*，允许优化器顺序方法在曲率变化特别缓慢时采用更大的步长，反之亦然（底部图像）。
- en: '![](img/B16176_03_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_17.png)'
- en: 'Figure 3.17: Complex landscapes and second-order methods^(58)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17：复杂的景观和二阶方法^（58）
- en: Because they make use of the rate of change of the derivative (the **second
    derivative**), these methods are known as **second order**, and have demonstrated
    some success in optimizing neural network models.^(59)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们利用了导数的改变速率（**二阶导数**），这些方法被称为**二阶**，并且在优化神经网络模型中已经取得了一定的成功^（59）。
- en: However, the computation required for each update is larger than for first-order
    methods, and because most second-order methods involve large matrix inversions
    (and thus memory utilization), approximations are required to make these methods
    scale. Ultimately, however, one of the breakthroughs in practically optimizing
    networks comes not just from the optimization algorithm, but how we initialize
    the weights in the model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每次更新所需的计算量比一阶方法大，因为大多数二阶方法涉及大型矩阵求逆（因此内存利用率高），需要近似来使这些方法可扩展。然而，最终，实际优化网络的突破之一不仅来自于优化算法，还包括我们如何初始化模型中的权重。
- en: Xavier initialization
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xavier 初始化
- en: As noted previously, in earlier research it was common to initialize weights
    in a neural network with some range of random values. Breakthroughs in the training
    of Deep Belief Networks in 2006, as you will see in *Chapter 4*, *Teaching Networks
    to Generate Digits*, used pre-training (through a generative modeling approach)
    to initialize weights before performing standard backpropagation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，在早期研究中，常常用一定范围的随机值初始化神经网络的权重。2006 年在深度置信网络的训练中取得的突破，正如您将在*第四章*，*教授网络生成数字*中看到的那样，使用了预训练（通过生成建模方法）来在执行标准反向传播之前初始化权重。
- en: If you've ever used a layer in the TensorFlow Keras module, you will notice
    that the default initialization for layer weights draws from either a truncated
    normal or uniform distribution. Where does this choice come from? As I described
    previously, one of the challenges with deep networks using sigmoidal or hyperbolic
    activation functions is that they tend to become saturated, since the values for
    these functions are capped with very large or negative input. We might interpret
    the challenge of initializing networks then as keeping weights in such a range
    that they don't saturate the neuron's output. Another way to understand this is
    to assume that the input and output values of the neuron have similar variance;
    the signal is not massively amplifying or diminishing while passing through the
    neuron.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您曾经在 TensorFlow Keras 模块中使用过一个层，您会注意到层权重的默认初始化是从截断的正态分布或均匀分布中抽取的。这个选择是从哪里来的？正如我之前描述的，使用
    S 型或双曲线激活函数的深度网络的一个挑战是，它们倾向于变得饱和，因为这些函数的值受到非常大或负的输入的限制。我们可以解释初始化网络的挑战是保持权重在这样一个范围内，以至于它们不会使神经元的输出饱和。另一种理解方法是假设神经元的输入和输出值具有类似的方差；信号在通过神经元时不会被大幅放大或减小。
- en: 'In practice, for a linear neuron, *y* = *wx* + *b*, we could compute the variance
    of the input and output as:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，对于一个线性神经元，*y* = *wx* + *b*，我们可以计算输入和输出的方差为：
- en: '![](img/B16176_03_042.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_042.png)'
- en: 'The *b* is constant, so we are left with:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*b*是常数，因此我们剩下：'
- en: '![](img/B16176_03_043.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_043.png)'
- en: 'Since there are *N* elements in the weight matrix, and we want *var*(*y*) to
    equal *var*(*x*), this gives:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重矩阵中有*N*个元素，并且我们希望*var*(*y*)等于*var*(*x*)，这给出了：
- en: '![](img/B16176_03_044.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_03_044.png)'
- en: Therefore, for a weight matrix *w*, we can use a truncated normal or uniform
    distribution with variance 1/*N* (the average number of input and output units,
    so the number of weights).^(60) Variations have also been applied to ReLU units:^(61)
    these methods are referred to by their original authors' names as Xavier or He
    initialization.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于权重矩阵*w*，我们可以使用方差为 1/*N*（输入和输出单元的平均数量，因此权重的数量）的截断正态分布或均匀分布。^(60)变体也已经应用于
    ReLU 单元：^(61)这些方法被称为它们原始作者的名字，如 Xavier 或 He 初始化。
- en: In summary, we've reviewed several common optimizers used under the hood in
    TensorFlow 2, and discussed how they improve upon the basic form of SGD. We've
    also discussed how clever weight initialization schemes work together with these
    optimizers to allow us to train ever more complex models.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们回顾了 TensorFlow 2 中底层使用的几种常见优化器，并讨论了它们如何改进基本的 SGD 形式。我们还讨论了聪明的权重初始化方案如何与这些优化器共同作用，使我们能够训练越来越复杂的模型。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've covered the basic vocabulary of deep learning – how initial
    research into perceptrons and MLPs led to simple learning rules being abandoned
    for backpropagation. We also looked at specialized neural network architectures
    such as CNNs, based on the visual cortex, and recurrent networks, specialized
    for sequence modeling. Finally, we examined variants of the gradient descent algorithm
    proposed originally for backpropagation, which have advantages such as momentum,
    and described weight initialization schemes that place the parameters of the network
    in a range that is easier to navigate to a local minimum.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了深度学习的基本词汇 - 如起始对感知器和多层感知器的研究导致了简单的学习规则被放弃，而采用反向传播。我们还研究了专门的神经网络架构，如基于视觉皮层的卷积神经网络（CNNs），以及专门用于序列建模的循环网络。最后，我们检查了最初为反向传播提出的梯度下降算法的变体，这些变体的优点包括*动量*，并描述了将网络参数放在更容易导航到局部最小值范围的权重初始化方案。
- en: With this context in place, we are all set to dive into projects in generative
    modeling, beginning with the generation of MNIST digits using Deep Belief Networks
    in *Chapter 4*, *Teaching Networks to Generate Digits*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，我们将着手进行生成模型的项目，首先是使用深度信念网络生成MNIST数字的项目，见*第4章*，*教授网络生成数字*。
- en: References
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'López-Muñoz F., Boya J., Alamo C. (2006). *Neuron theory, the cornerstone of
    neuroscience, on the centenary of the Nobel Prize award to Santiago Ramón y Cajal*.
    Brain Research Bulletin. 70 (4–6): 391–405\. [https://pubmed.ncbi.nlm.nih.gov/17027775/](https://pubmed.ncbi.nlm.nih.gov/17027775/)'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: López-Muñoz F., Boya J., Alamo C. (2006). *神经元理论，神经科学的基石，颁给圣地亚哥·拉蒙·伊·卡哈尔的诺贝尔奖100周年*。《大脑研究公报》.
    70 （4–6）：391–405\. [https://pubmed.ncbi.nlm.nih.gov/17027775/](https://pubmed.ncbi.nlm.nih.gov/17027775/)
- en: Ramón y Cajal, Santiago (1888). *Estructura de los centros nerviosos de las
    aves*.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ramón y Cajal, Santiago (1888). *鸟类中枢神经中枢结构*。
- en: McCulloch, W.S., Pitts, W. (1943). *A logical calculus of the ideas immanent
    in nervous activity. Bulletin of Mathematical Biophysics* 5, 115–133\. [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: McCulloch, W.S., Pitts, W. (1943). *神经活动中所固有的思想的逻辑演算。数理生物物理学通报*5, 115–133\.
    [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259)
- en: Rashwan M., Ez R., reheem G. (2017). *Computational Intelligent Algorithms For
    Arabic Speech Recognition*. Journal of Al-Azhar University Engineering Sector.
    12\. 886-893\. 10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意：Rashwan M., Ez R., reheem G. (2017). *阿拉伯语言语音识别的计算智能算法*.《开罗大学工程领域杂志》. 12\.
    886-893\. 10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
- en: Rashwan M., Ez R., reheem G. (2017). *Computational Intelligent Algorithms For
    Arabic Speech Recognition*. Journal of Al-Azhar University Engineering Sector.
    12\. 886-893\. 10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rashwan M., Ez R., reheem G. (2017). *阿拉伯语言语音识别的计算智能算法*.《开罗大学工程领域杂志》. 12\. 886-893\.
    10.21608/auej.2017.19198\. [http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
- en: '*Artificial neuron*. Wikipedia. Retrieved April 26, 2021, from [https://en.wikipedia.org/wiki/Artificial_neuron](https://en.wikipedia.org/wiki/Artificial_neuron)'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*人工神经元*. 维基百科. 检索日期：2021年4月26日，网址：[https://en.wikipedia.org/wiki/Artificial_neuron](https://en.wikipedia.org/wiki/Artificial_neuron)'
- en: 'Shackleton-Jones Nick. (2019, May 3). *How People Learn: Designing Education
    and Training that Works to Improve Performance*. Kogan Page. London, United Kingdom'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shackleton-Jones Nick. (2019年5月3日).*人们如何学习：设计教育和培训，以提高绩效*。Kogan Page。英国伦敦
- en: 'Hebb, D. O. (1949). *The Organization of Behavior: A Neuropsychological Theory*.
    New York: Wiley and Sons'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hebb, D. O. (1949). *行为组织：神经心理学理论*。纽约：Wiley和Sons出版社
- en: Rosenblatt, Frank (1957). *The Perceptron—a perceiving and recognizing automaton*.
    Report 85-460-1\. Cornell Aeronautical Laboratory.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rosenblatt, Frank (1957). *感知器-一个认知和识别自动装置*。报告85-460-1. 康奈尔航空实验室。
- en: 'Marvin Minsky and Seymour Papert, 1972 (2^(nd) edition with corrections, first
    edition 1969) *Perceptrons: An Introduction to Computational Geometry*, The MIT
    Press, Cambridge MA'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Marvin Minsky和Seymour Papert，1972年（第二版，第一版1969年）《感知器：计算几何的介绍》，MIT出版社，剑桥，马萨诸塞州
- en: 'Hassan, Hassan & Negm, Abdelazim & Zahran, Mohamed & Saavedra, Oliver. (2015).
    *Assessment of Artificial Neural Network for Bathymetry Estimation Using High
    Resolution Satellite Imagery in Shallow Lakes: Case Study El Burullus Lake*. International
    Water Technology Journal. 5.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hassan, Hassan & Negm, Abdelazim & Zahran, Mohamed & Saavedra, Oliver. (2015).
    *利用高分辨率卫星图像评估人工神经网络进行浅水湖泊水深估计：以 El Burullus Lake 为例*. 国际水技术期刊. 5.
- en: 'Marvin Minsky and Seymour Papert, 1972 (2nd edition with corrections, first
    edition 1969) *Perceptrons: An Introduction to Computational Geometry*, The MIT
    Press, Cambridge MA'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Marvin Minsky 和 Seymour Papert, 1972 (第二版带有更正，第一版 1969) *感知机：计算几何简介*, The MIT
    Press, 剑桥 MA
- en: 'Pollack, J. B. (1989). "No Harm Intended: A Review of the Perceptrons expanded
    edition". *Journal of Mathematical Psychology*. 33 (3): 358–365.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pollack, J. B. (1989). "无意伤害：感知机扩展版评论". *数学心理学杂志*. 33 (3): 358–365.'
- en: 'Crevier, Daniel (1993), *AI: The Tumultuous Search for Artificial Intelligence*,
    New York, NY: BasicBooks.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Crevier, Daniel (1993), *AI：人工智能的动荡探索*, 纽约，纽约: BasicBooks.'
- en: Cybenko, G. *Approximation by superpositions of a sigmoidal function*. Math.
    Control Signal Systems 2, 303–314 (1989). [https://doi.org/10.1007/BF02551274](https://doi.org/10.1007/BF02551274)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cybenko, G. *通过 S 型函数的叠加进行逼近*. 数学. 控制信号系统 2, 303–314 (1989). [https://doi.org/10.1007/BF02551274](https://doi.org/10.1007/BF02551274)
- en: Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). *6.5 Back-Propagation
    and Other Differentiation Algorithms*. Deep Learning. MIT Press. pp. 200–220
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). *6.5 反向传播和其他差分算法*.
    深度学习. MIT 出版社. pp. 200–220
- en: Rumelhart, D., Hinton, G. & Williams, R. (1986) *Learning representations by
    back-propagating errors*. *Nature* 323, 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart, D., Hinton, G. & Williams, R. (1986) *通过反向传播误差学习表示*. *自然* 323, 533–536\.
    [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
- en: Guess A R., (2015, November 10). *Google Open-Sources Machine Learning Library,
    TensorFlow*. DATAVERSITY. [https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/](https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Guess A R., (2015年11月10日). *Google 开源机器学习库 TensorFlow*. DATAVERSITY. [https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/](https://www.dataversity.net/google-open-sources-machine-learning-library-tensorflow/)
- en: 'Berland (2007). *ReverseaccumulationAD.png*. Wikipedia. Available from: [https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png](https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png)'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Berland (2007). *ReverseaccumulationAD.png*. 维基百科. 可从: [https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png](https://commons.wikimedia.org/wiki/File:ReverseaccumulationAD.png)'
- en: '*Automatic differentiation*. Wikipedia. [https://en.wikipedia.org/wiki/Automatic_differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*自动微分*. 维基百科. [https://en.wikipedia.org/wiki/Automatic_differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)'
- en: 'R.E. Wengert (1964). *A simple automatic derivative evaluation program*. Comm.
    ACM. 7 (8): 463–464.;Bartholomew-Biggs, Michael; Brown, Steven; Christianson,
    Bruce; Dixon, Laurence (2000). *Automatic differentiation of algorithms*. Journal
    of Computational and Applied Mathematics. 124 (1–2): 171–190.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R.E. Wengert (1964). *一个简单的自动导数评估程序*. Comm. ACM. 7 (8): 463–464.;Bartholomew-Biggs,
    Michael; Brown, Steven; Christianson, Bruce; Dixon, Laurence (2000). *算法的自动微分*.
    计算与应用数学杂志. 124 (1–2): 171–190.'
- en: 'The TensorFlow Authors (2018). *automatic_differentiation.ipynb*. Available
    from: [https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/notebooks/automatic_differentiation.ipynb#scrollTo=t09eeeR5prIJ](https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/py)'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'TensorFlow 作者 (2018). *automatic_differentiation.ipynb*. 可从: [https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/notebooks/automatic_differentiation.ipynb#scrollTo=t09eeeR5prIJ](https://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/py)'
- en: 'The TensorFlow Authors. *Introduction to gradients and automatic differentiation*.
    TensorFlow. Available from: [https://www.tensorflow.org/guide/autodiff](https://www.tensorflow.org/guide/autodiff)'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'TensorFlow 作者. *梯度和自动微分简介*. TensorFlow. 可从: [https://www.tensorflow.org/guide/autodiff](https://www.tensorflow.org/guide/autodiff)'
- en: 'Thomas (2018). *The vanishing gradient problem and ReLUs – a TensorFlow investigation*.
    Adventures in Machine Learning. Available from: [https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/](https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/)'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thomas (2018). *梯度消失问题和ReLU – TensorFlow调查*. 机器学习冒险。查阅：[https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/](https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/)
- en: 'Hinton, Osindero, Yee-Whye (2005). *A Fast Learning Algorithm for Deep Belief
    Nets*. Univeristy of Toronto, Computer Science. Available from: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, Osindero, Yee-Whye (2005). *深度信念网络的快速学习算法*. 多伦多大学，计算机科学。查阅：[http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)
- en: Cortes, C., Vapnik, V. *Support-vector networks*. Mach Learn 20, 273–297 (1995).
    [https://doi.org/10.1007/BF00994018](https://doi.org/10.1007/BF00994018)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cortes, C., Vapnik, V. *支持向量网络*. 机器学习 20, 273–297 (1995). [https://doi.org/10.1007/BF00994018](https://doi.org/10.1007/BF00994018)
- en: 'Friedman, J. H. (February 1999). *Greedy Function Approximation: A Gradient
    Boosting Machine* (PDF)'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Friedman, J. H. (February 1999). *贪婪函数逼近：梯度增强机* (PDF)
- en: Breiman, L. *Random Forests*. Machine Learning 45, 5–32 (2001). [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Breiman, L. *随机森林*. 机器学习 45, 5–32 (2001). [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)
- en: 'Tibshirani R. (1996). *Regression Shrinkage and Selection via the lasso*. Journal
    of the Royal Statistical Society. Series B (methodological). Wiley. 58 (1): 267–88.'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tibshirani R. (1996). *通过套索实现回归收缩和选择*. 英国皇家统计学会杂志。Wiley. 58 (1): 267–88.'
- en: 'Zou H., Hastie T. (2005). *Regularization and variable selection via the elastic
    net*. Journal of the Royal Statistical Society, Series B: 301–320'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zou H., Hastie T. (2005). *通过弹性网络实现正规化和变量选择*. 英国皇家统计学会杂志B系列：301–320
- en: 'Hubel D. H., Wiesel T. N. (1962) *Receptive fields, binocular interaction and
    functional architecture in the cat''s visual cortex*. J Physiol, 1962, 160: 106-154\.
    [https://doi.org/10.1113/jphysiol.1962.sp006837](https://doi.org/10.1113/jphysiol.1962.sp006837)'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hubel D. H., Wiesel T. N. (1962) *感觉野，视交互及猫脑视觉皮层功能体系结构*. 生理学杂志，1962, 160: 106-154。[https://doi.org/10.1113/jphysiol.1962.sp006837](https://doi.org/10.1113/jphysiol.1962.sp006837)'
- en: '[http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif](http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif)'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif](http://charlesfrye.github.io/FoundationalNeuroscience/img/corticalLayers.gif)'
- en: 'Wolfe, Kluender, Levy (2009). *Sensation and Perception*. Sunderland: Sinauer
    Associates Inc..'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wolfe, Kluender, Levy (2009). *感知和知觉*. 坎伯兰：Sinauer Associates Inc.。
- en: 'LeCun, Yann, et al. *Backpropagation applied to handwritten zip code recognition*.
    Neural computation 1.4 (1989): 541-551.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann, et al. *反向传播应用于手写邮政编码识别*. 神经计算，1.4 (1989): 541-551.'
- en: 'LeCun, Yann, et al. *Backpropagation applied to handwritten zip code recognition*.
    Neural computation 1.4 (1989): 541-551.'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann, et al. *反向传播应用于手写邮政编码识别*. 神经计算，1.4 (1989): 541-551.'
- en: '*ImageNet Classification with Deep Convolutional Neural Networks*: [https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convoluti)'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用深度卷积神经网络进行ImageNet分类*：[https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convoluti)'
- en: Nair V., Hinton G E. (2010). *Rectified Linear Units Improve Restricted Boltzmann
    Machines*. Proceedings of the 27 th International Conference on Machine Learning,
    Haifa, Israel, 2010.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nair V., Hinton G E. (2010). *修正线性单元改进限制玻尔兹曼机*. 机器学习国际会议论文集，2010年，以色列海法。
- en: Agarap A F. (2019, September 5). *Avoiding the vanishing gradients problem using
    gradient noise addition*. Towards Data Science. [https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343](https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Agarap A F. (2019, September 5). *通过梯度噪音添加来避免伴随梯度消失的问题*. 朝着数据科学。[https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343](https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343)
- en: Maas A L., Hannun A Y., Ng A Y. (2013). *Rectifer Nonlinearities Improve Neural
    Network Acoustic Models*. Proceedings of the 30 th International Conference on
    Machine Learning, Atlanta, Georgia, USA.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Maas A L., Hannun A Y., Ng A Y. (2013). *修正线性非线性改进神经网络声学模型*. 机器学习国际会议论文集，2013年，美国佐治亚州亚特兰大市。
- en: 'He, K., Zhang, X., Ren, S., Sun, J. (2015). *Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification*. arXiv:1502.01852\.
    [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: He，K.，Zhang，X.，Ren，S.，Sun，J.（2015）。 *深入挖掘整流器：在ImageNet分类上超越人类水平性能*。 arXiv：1502.01852。[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)
- en: Hinton, G E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov,
    R R. (2012). *Improving neural networks by preventing co-adaptation of feature
    detectors*. arXiv:1207.0580\. [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580
    )
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton，G E.，Srivastava，N.，Krizhevsky，A.，Sutskever，I.，Salakhutdinov，R R.（2012）。
    *通过防止特征检测器的协同适应来改进神经网络*。 arXiv：1207.0580。[https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580)
- en: Krizhevsky A., Sutskever I., Hinton G E. (2012). *ImageNet Classification with
    Deep Convolutional Neural Networks*. Part of Advances in Neural Information Processing
    Systems 25 (NIPS 2012). [https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Krizhevsky A.，Sutskever I.，Hinton G E.（2012）。 *使用深度卷积神经网络的ImageNet分类*。神经信息处理系统25（NIPS
    2012）的一部分。[https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- en: 'Ioffe, S., Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*. arXiv:1502.03167\. [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ioffe，S.，Szegedy，C.（2015）。 *批量归一化：通过减少内部协变量转移加速深层网络训练*。 arXiv：1502.03167。 [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)
- en: Santurkar, S., Tsipras, D., Ilyas, A., Madry, A. (2019). *How Does Batch Normalization
    Help Optimization?*. arXiv:1805.11604\. [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604)
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Santurkar，S。，Tsipras，D。，Ilyas，A。，Madry，A.（2019）。 *批量归一化如何帮助优化？* arXiv：1805.11604。
    [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604)
- en: Dean J., Ng, A Y. (2012). *Using large-scale brain simulations for machine learning
    and A.I.*. The Keyword | Google. [https://blog.google/technology/ai/using-large-scale-brain-simulations-for/](https://blog.google/technology/ai/using-large-scale-brain-simulations-for/)
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dean J.，Ng，A Y.（2012）。 *使用大规模脑模拟进行机器学习和人工智能*。The Keyword | Google。[https://blog.google/technology/ai/using-large-scale-brain-simulations-for/](https://blog.google/technology/ai/using-large-scale-brain-simulations-for/)
- en: Rumelhart, D., Hinton, G. & Williams, R. (1986) *Learning representations by
    back-propagating errors*. *Nature* 323, 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart，D.，Hinton，G.和Williams，R.（1986年）*通过反向传播错误学习表示*。 *自然* 323，533–536。 [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0)
- en: LeCun, Y., Bengio, Y. & Hinton, (2015) G. *Deep learning*. *Nature* 521, 436–444\.
    [https://www.nature.com/articles/nature14539.epdf](https://www.nature.com/articles/nature14539.epdf)
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LeCun，Y.，Bengio，Y.和Hinton G.（2015）。 *深度学习*。 *自然* 521，436–444。 [https://www.nature.com/articles/nature14539.epdf](https://www.nature.com/articles/nature14539.epdf)
- en: 'Olah (2015). *Understanding LSTM Networks*. colah''s blog. Available from:
    [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Olah（2015年）。 *理解LSTM网络*. colah的博客。可从[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)获取。
- en: 'Mozer, M. C. (1995). *A Focused Backpropagation Algorithm for Temporal Pattern
    Recognition*. In Chauvin, Y.; Rumelhart, D. (eds.). *Backpropagation: Theory,
    architectures, and applications*. ResearchGate. Hillsdale, NJ: Lawrence Erlbaum
    Associates. pp. 137–169'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mozer，M. C.（1995）。 *用于时间模式识别的聚焦反向传播算法*。在Chauvin，Y .; Rumelhart，D.（eds。）。 *反向传播：理论，体系结构和应用*。
    ResearchGate。 Hillsdale，NJ：劳伦斯Erlbaum凯斯。第137-169页。
- en: 'Greff K., Srivastava, R K., Koutník, J., Steunebrink, B R., Schmidhuber, J.
    (2017). *LSTM: A Search Space Odyssey*. arXiv:1503.04069v2\. [https://arxiv.org/abs/1503.04069v2](https://arxiv.org/abs/1503.04069v2)'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Greff K.，Srivastava，R K。，Koutník，J.，Steunebrink，B R。，Schmidhuber，J.（2017）。 *LSTM：搜索空间奥德赛*。
    arXiv：1503.04069v2。 [https://arxiv.org/abs/1503.04069v2](https://arxiv.org/abs/1503.04069v2)
- en: 'Gers FA, Schmidhuber E. *LSTM recurrent networks learn simple context-free
    and context-sensitive languages*. IEEE Trans Neural Netw. 2001;12(6):1333-40\.
    doi: 10.1109/72.963769\. PMID: 18249962.'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gers FA, Schmidhuber E. *LSTM循环网络学习简单的无上下文和有上下文的语言*. IEEE交易神经网络。 2001年;12（6）：1333-40.
    doi：10.1109/72.963769。 PMID：18249962。
- en: Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H., Bengio, Y. (2014). *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*. arXiv:1406.1078\. [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H., Bengio, Y. (2014). *使用RNN编码器-解码器学习短语表示用于统计机器翻译*。arXiv:1406.1078。[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
- en: Sutskever, I., Martens, J., Dahl, G. & Hinton, G. (2013). *On the importance
    of initialization and momentum in deep learning*. Proceedings of the 30th International
    Conference on Machine Learning, in PMLR 28(3):1139-1147.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutskever, I., Martens, J., Dahl, G. & Hinton, G. (2013). *初始化和动量在深度学习中的重要性*。第30届国际机器学习大会论文集,
    PMLR 28(3):1139-1147.
- en: Duchi J., Hazan E., Singer Y. (2011). *Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization*. Journal of Machine Learning Research 12
    (2011) 2121-2159.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Duchi J., Hazan E., Singer Y. (2011). *用于在线学习和随机优化的自适应次梯度方法*。机器学习研究杂志12 (2011)
    2121-2159.
- en: 'Hinton, Srivastava, Swersky. *Neural Networks for Machine Learning*, Lecture
    6a. Available from: [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, Srivastava, Swersky. *神经网络用于机器学习*，第6a讲。可从：[http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
- en: 'Zeiler, M D. (2012). *ADADELTA: An Adaptive Learning Rate Method*. arXiv:1212.5701\.
    [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701 )'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zeiler, M D. (2012). *ADADELTA：一种自适应学习率方法*。arXiv:1212.5701。[https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)
- en: 'Kingma, D P., Ba, J. (2017). *Adam: A Method for Stochastic Optimization*.
    arXiv:1412.6980\. [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kingma, D P., Ba, J. (2017). *Adam：一种随机优化方法*。arXiv:1412.6980。[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
- en: Martens J. (2010). *Deep Learning via Hessian-free Optimization*. ICML. Vol.
    27\. 2010.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Martens J. (2010). *通过无Hessian优化的深度学习*。ICML. Vol. 27. 2010.
- en: Martens J. (2010). *Deep Learning via Hessian-free Optimization*. ICML. Vol.
    27\. 2010.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Martens J. (2010). *通过无Hessian优化的深度学习*。ICML. Vol. 27. 2010.
- en: Glorot X., Bengio Y., (2010). *Understanding the difficulty of training deep
    feedforward neural networks*. Proceedings of the thirteenth international conference
    on artificial intelligence and statistics.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Glorot X., Bengio Y., (2010). *理解训练深度前馈神经网络的困难*。第十三届人工智能与统计国际会议论文集。
- en: 'He, K., Zhang, X., Ren, S., Sun, J. (2015). *Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification*. arXiv:1502.01852\.
    [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: He, K., Zhang, X., Ren, S., Sun, J. (2015). *深入研究整流器：在ImageNet分类上超越人类水平性能*。arXiv:1502.01852。[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)
