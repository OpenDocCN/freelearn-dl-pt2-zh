["```py\nX = tf.placeholder(dtype=tf.float64)\nY = tf.placeholder(dtype=tf.float64)\nnum_hidden=128\n# Build a hidden layer\nW_hidden = tf.Variable(np.random.randn(784, num_hidden))\nb_hidden = tf.Variable(np.random.randn(num_hidden))\np_hidden = tf.nn.sigmoid( tf.add(tf.matmul(X, W_hidden), b_hidden) )\n# Build another hidden layer\nW_hidden2 = tf.Variable(np.random.randn(num_hidden, num_hidden))\nb_hidden2 = tf.Variable(np.random.randn(num_hidden))\np_hidden2 = tf.nn.sigmoid( tf.add(tf.matmul(p_hidden, W_hidden2), b_hidden2) )\n# Build the output layer\nW_output = tf.Variable(np.random.randn(num_hidden, 10))\nb_output = tf.Variable(np.random.randn(10))\np_output = tf.nn.softmax( tf.add(tf.matmul(p_hidden2, W_output), \n           b_output) )\nloss = tf.reduce_mean(tf.losses.mean_squared_error(\n        labels=Y,predictions=p_output))\naccuracy=1-tf.sqrt(loss)\nminimization_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\nfeed_dict = {\n    X: x_train.reshape(-1,784),\n    Y: pd.get_dummies(y_train)\n}\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    for step in range(10000):\n        J_value = session.run(loss, feed_dict)\n        acc = session.run(accuracy, feed_dict)\n        if step % 100 == 0:\n            print(\"Step:\", step, \" Loss:\", J_value,\" Accuracy:\", acc)\n            session.run(minimization_op, feed_dict)\n    pred00 = session.run([p_output], feed_dict={X: x_test.reshape(-1,784)}) \n```", "```py\nimport TensorFlow as tf\nfrom TensorFlow.keras.layers import Input, Dense\nfrom keras.models import Model\nl = tf.keras.layers\nmodel = tf.keras.Sequential([\n    l.Flatten(input_shape=(784,)),\n    l.Dense(128, activation='relu'),\n    l.Dense(128, activation='relu'),\n    l.Dense(10, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam',\n              metrics = ['accuracy'])\nmodel.summary()\nmodel.fit(x_train.reshape(-1,784),pd.get_dummies(y_train),nb_epoch=15,batch_size=128,verbose=1) \n```", "```py\ngit clone git@github.com:PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2.git \n```", "```py\ndocker run hello-world \n```", "```py\nFROM public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow:master-abf9ec48\n# install - requirements.txt\nCOPY --chown=jovyan:users requirements.txt /tmp/requirements.txt\nRUN python3 -m pip install -r /tmp/requirements.txt --quiet --no-cache-dir \\\n && rm -f /tmp/requirements.txt \n```", "```py\nDocker build -f <Dockerfilename> -t <image name:tag> \n```", "```py\nDocker run <image name:tag> \n```", "```py\nDocker run <image name:tag> <command> \n```", "```py\nDocker push <image name:tag> \n```", "```py\nversion: '3'\nservices:\n  web:\n    build: .\n    ports:\n    - \"5000:5000\"\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\nvolumes:\n  logvolume01: {} \n```", "```py\ndocker-compose up \n```", "```py\nkubectl --help \n```", "```py\nkubectl apply -f <file.yaml> \n```", "```py\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx-svc\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80 \n```", "```py\nkubectl apply -k <kustomization.yaml> \n```", "```py\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  # Or\n# github.com/kubeflow/pipelines/manifests/kustomize/env/gcp?ref=1.0.0\n  - ../env/gcp\n  # Kubeflow Pipelines servers are capable of \n  # collecting Prometheus metrics.\n  # If you want to monitor your Kubeflow Pipelines servers \n  # with those metrics, you'll need a Prometheus server \n  # in your Kubeflow Pipelines cluster.\n  # If you don't already have a Prometheus server up, you \n  # can uncomment the following configuration files for Prometheus.\n  # If you have your own Prometheus server up already \n  # or you don't want a Prometheus server for monitoring, \n  # you can comment the following line out.\n  # - ../third_party/prometheus\n  # - ../third_party/grafana\n# Identifier for application manager to apply ownerReference.\n# The ownerReference ensures the resources get garbage collected\n# when application is deleted.\ncommonLabels:\n  application-crd-id: kubeflow-pipelines\n# Used by Kustomize\nconfigMapGenerator:\n  - name: pipeline-install-config\n    env: params.env\n    behavior: merge\nsecretGenerator:\n  - name: mysql-secret\n    env: params-db-secret.env\n    behavior: merge\n# !!! If you want to customize the namespace,\n# please also update \n# sample/cluster-scoped-resources/kustomization.yaml's \n# namespace field to the same value\nnamespace: kubeflow\n#### Customization ###\n# 1\\. Change values in params.env file\n# 2\\. Change values in params-db-secret.env \n# file for CloudSQL username and password\n# 3\\. kubectl apply -k ./\n#### \n```", "```py\nkustomize edit set namespace mykube \n```", "```py\nkustomize edit add configmap configMapGenerator --from-literal=myVar=myVal \n```", "```py\nkustomize build . |kubectl apply -f - \n```", "```py\nvagrant init arrikto/minikf\nvagrant up \n```", "```py\n    aws configure \n    ```", "```py\n    tar -xvf kfctl_v0.7.1_<platform>.tar.gz \n    ```", "```py\n    mkdir -p ${KF_DIR}\n    cd ${KF_DIR}\n    kfctl build -V -f ${CONFIG_URI} \n    ```", "```py\n    export CONFIG_FILE=${KF_DIR}/kfctl_aws.0.7.1.yaml \n    ```", "```py\n    cd ${KF_DIR}\n    rm -rf kustomize/ \n    kfctl apply -V -f ${CONFIG_FILE} \n    ```", "```py\n    kubectl -n kubeflow get all \n    ```", "```py\n    kubectl get ingress -n istio-system \n    ```", "```py\n    gcloud --help \n    ```", "```py\n    tar -xvf kfctl_v0.7.1_<platform>.tar.gz \n    ```", "```py\n    gcloud auth login\n    gcloud auth application-default login \n    ```", "```py\n    mkdir -p ${KF_DIR}\n    cd ${KF_DIR}\n    kfctl apply -V -f ${CONFIG_URI} \n    ```", "```py\n    kubectl -n istio-system get ingress \n    ```", "```py\n    az \n    ```", "```py\n    az login \n    ```", "```py\n    \"You have logged in. Now let us find all the subscriptions to which you have access\": …\n    [\n    { \n        \"cloudName\": …\n        \"id\" ….\n    …\n        \"user\": {\n    …\n    }\n    }\n    ] \n    ```", "```py\n    az group create -n ${RESOURCE_GROUP_NAME} -l ${LOCATION} \n    ```", "```py\n    az aks create -g ${RESOURCE_GROUP_NAME} -n ${NAME} -s ${AGENT_SIZE} -c ${AGENT_COUNT} -l ${LOCATION} --generate-ssh-keys \n    ```", "```py\n    az aks get-credentials -n ${NAME}  -g ${RESOURCE_GROUP_NAME} \n    ```", "```py\n    tar -xvf kfctl_v0.7.1_<platform>.tar.gz \n    ```", "```py\n    mkdir -p ${KF_DIR}\n    cd ${KF_DIR}\n    kfctl apply -V -f ${CONFIG_URI} \n    ```", "```py\n    kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 \n    ```", "```py\nterraform apply \n```", "```py\nvariable \"efs_throughput_mode\" {\n   description = \"EFS performance mode\"\n   default = \"bursting\"\n   type = string\n} \n```", "```py\nprovider \"aws\" {\n  region                  = var.region\n  shared_credentials_file = var.credentials\nresource \"aws_eks_cluster\" \"eks_cluster\" {\n  name            = var.cluster_name\n  role_arn        = aws_iam_role.cluster_role.arn\n  version         = var.k8s_version\n\n  vpc_config {\n    security_group_ids = [aws_security_group.cluster_sg.id]\n    subnet_ids         = flatten([aws_subnet.subnet.*.id])\n  }\n\n  depends_on = [\n    aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy,\n    aws_iam_role_policy_attachment.cluster_AmazonEKSServicePolicy,\n  ]\n\n  provisioner \"local-exec\" {\n    command = \"aws --region ${var.region} eks update-kubeconfig --name ${aws_eks_cluster.eks_cluster.name}\"\n  }\n\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"kubectl config unset current-context\"\n  }\n\n}\n  profile   = var.profile\n} \n```", "```py\nresource \"aws_eks_cluster\" \"eks_cluster\" {\n  name     = var.cluster_name\n  role_arn = aws_iam_role.cluster_role.arn\n  version  = var.k8s_version\n\n  vpc_config {\n    security_group_ids = [aws_security_group.cluster_sg.id]\n    subnet_ids         = flatten([aws_subnet.subnet.*.id])\n  }\n\n  depends_on = [\n    aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy,\n    aws_iam_role_policy_attachment.cluster_AmazonEKSServicePolicy,\n  ]\n\n  provisioner \"local-exec\" {\n    command = \"aws --region ${var.region} eks update-kubeconfig --name ${aws_eks_cluster.eks_cluster.name}\"\n  }\n\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"kubectl config unset current-context\"\n  }\n\n} \n```", "```py\napiVersion: kubeflow.org/v1beta1\nkind: Profile\nmetadata:\n  name: profileName  \nspec:\n  owner:\n    kind: User\n    name: userid@email.com \n```", "```py\nkubectl create -f profile.yaml \n```", "```py\n@kfp.dsl.component\ndef my_component(my_param):\n  ...\n  return kfp.dsl.ContainerOp(\n    name='My component name',\n    image='gcr.io/path/to/container/image'\n  )\nor a function written in Python itself:\n@kfp.dsl.python_component(\n  name='My awesome component',\n  description='Come and play',\n)\ndef my_python_func(a: str, b: str) -> str: \n```", "```py\nmy_op = compiler.build_python_component(\n  component_func=my_python_func,\n  staging_gcs_path=OUTPUT_DIR,\n  target_image=TARGET_IMAGE) \n```", "```py\n@kfp.dsl.pipeline(\n  name='My pipeline',\n  description='My machine learning pipeline'\n)\ndef my_pipeline(param_1: PipelineParam, param_2: PipelineParam):\n  my_step = my_op(a='a', b='b') \n```", "```py\nkfp.compiler.Compiler().compile(my_pipeline, 'my-pipeline.zip') \n```", "```py\nclient = kfp.Client()\nmy_experiment = client.create_experiment(name='demo')\nmy_run = client.run_pipeline(my_experiment.id, 'my-pipeline', \n  'my-pipeline.zip') \n```", "```py\napiVersion: \"kubeflow.org/v1alpha3\"\nkind: Experiment\nmetadata:\n  namespace: kubeflow\n  name: tfjob-example\nspec:\n  parallelTrialCount: 3\n  maxTrialCount: 12\n  maxFailedTrialCount: 3\n  objective:\n    type: maximize\n    goal: 0.99\n    objectiveMetricName: accuracy_1\n  algorithm:\n    algorithmName: random\n  metricsCollectorSpec:\n    source:\n      fileSystemPath:\n        path: /train\n        kind: Directory\n    collector:\n      kind: TensorFlowEvent\n  parameters:\n    - name: --learning_rate\n      parameterType: double\n      feasibleSpace:\n        min: \"0.01\"\n        max: \"0.05\"\n    - name: --batch_size\n      parameterType: int\n      feasibleSpace:\n        min: \"100\"\n        max: \"200\"\n  trialTemplate:\n    goTemplate:\n        rawTemplate: |-\n          apiVersion: \"kubeflow.org/v1\"\n          kind: TFJob\n          metadata:\n            name: {{.Trial}}\n            namespace: {{.NameSpace}}\n          spec:\n           tfReplicaSpecs:\n            Worker:\n              replicas: 1 \n              restartPolicy: OnFailure\n              template:\n                spec:\n                  containers:\n                    - name: tensorflow \n                      image: gcr.io/kubeflow-ci/tf-mnist-with-\n                             summaries:1.0\n                      imagePullPolicy: Always\n                      command:\n                        - \"python\"\n                        - \"/var/tf_mnist/mnist_with_summaries.py\"\n                        - \"--log_dir=/train/metrics\"\n                        {{- with .HyperParameters}}\n                        {{- range .}}\n                        - \"{{.Name}}={{.Value}}\"\n                        {{- end}}\n                        {{- end}} \n```", "```py\nkubectl apply -f https://raw.githubusercontent.com/kubeflow/katib/master/examples/v1alpha3/tfjob-example.yaml \n```"]