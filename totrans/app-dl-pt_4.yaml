- en: '*Chaper 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain the training process of convolutional neural networks (CNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply batch normalization to a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve an image classification problem using a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you'll be introduced to CNN. You'll learn concepts such as
    convolutions, pooling, padding, and stride.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though all neural network domains are popular nowadays, CNNs are probably
    the most popular of all neural network architectures. This is mainly because,
    although they work in many domains, they are particularly good at dealing with
    images, and advances in technology have allowed the collection of large amounts
    of images to be possible in order to tackle a great variety of today's challenges.
  prefs: []
  type: TYPE_NORMAL
- en: From image classification to object detection, CNNs are being used to diagnose
    cancer patients and detect fraud in systems, as well as to construct well thought-out
    self-driving vehicles that will revolutionize the future.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on explaining the reasons why CNNs outperform other
    architectures when dealing with images, as well as explaining the building blocks
    of their architecture in greater detail. It will cover the main coding structure
    for building a CNN to solve an image classification data problem.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it will explore the concepts of data augmentation and batch normalization,
    which will be used to improve the performance of the model. The ultimate goal
    of this chapter will be to compare the results of three different approaches to
    tackling an image classification problem using CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, the GitHub repository containing all the code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is widely known that CNNs are the way to go when dealing with an image data
    problem. However, they are often underused, as they are typically known for image
    classification alone, when the reality is that their abilities extend to further
    domains in regard to images. This chapter will not only explain the reasons why
    CNNs are so good at understanding images, but will also identify the different
    tasks that can be tackled, as well as give some examples of real-life applications.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, this chapter will explore the different building blocks of CNNs and
    their application using PyTorch to ultimately build a model that solves a data
    problem using one of PyTorch's datasets for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Why CNNs?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An image is a matrix of pixels, so you might ask why we don''t just flatten
    the matrix into a vector and process it using a traditional neural network architecture?
    The answer is that, even with the simplest image, there are some pixel dependencies
    that alter the meaning of the image. For instance, the representation of a cat''s
    eye, a car tire, or even the edge of an object is constructed of several pixels
    laid out in a certain way. By flattening the image, these dependencies are lost
    and so is the accuracy of a traditional model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11865_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Representation of a flattened matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the other hand, a CNN is capable of capturing the spatial dependencies of
    images, as it processes them as matrices and analyzes entire chunks of an image
    at a time, depending on the size of the filter. For example, a convolutional layer
    using a filter of size 3x3, will analyze 9 pixels at a time until it has covered
    the entire image.
  prefs: []
  type: TYPE_NORMAL
- en: Each chunk of the image is given a set of parameters (weight and bias) that
    will refer to the relevance of that set of pixels to the entire image, depending
    on the filter at hand. This means that a vertical edge filter will assign greater
    weights to the chunks of the image that contain a vertical edge. According to
    this, by reducing the number of parameters and by analyzing the image in chunks,
    CNNs are capable of rendering a better representation of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The Inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, the typical inputs of a CNN are images in the form of matrices.
    Each value of the matrix represents a pixel in the image, where the number is
    determined by the intensity of the color, with values ranging from 0 to 255.
  prefs: []
  type: TYPE_NORMAL
- en: In gray-scaled images, white pixels are represented by the number 255 and black
    pixels by the number 0\. Gray pixels are any number in between, depending on the
    intensity of the color; the lighter the gray, the closer the number is to 255.
  prefs: []
  type: TYPE_NORMAL
- en: Colored images are usually represented using the RGB system, which represents
    each color as the combination of red, green, and blue. Here, each pixel will have
    three dimensions, one for each color. The values in each dimension will range
    from 0 to 255\. Here, the more intense the color, the closer the number to 255.
  prefs: []
  type: TYPE_NORMAL
- en: According to the preceding paragraph, the matrix of a given image is three-dimensional,
    where the first dimension refers to the height of the image (in the number of
    pixels), the second dimension refers to the width of the image (in the number
    of pixels), and the third dimension is known as the channel and refers to the
    color scheme of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The channel for colored images is three (one channel for each color in the
    RGB system). On the other hand, grey-scaled images only have one channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Matrix representation of an image. To the left, a colored image.
    To the right, a grey-scaled image.](img/C11865_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Matrix representation of an image. To the left, a colored image.
    To the right, a grey-scaled image.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Different to text data, images fed into CNNs do not require much preprocessing.
    Images are usually fed as they are, with the only changes being that values are
    normalized to speed up the learning process and improve performance, and that
    images can be downsized as a good practice, considering that CNN models are usually
    built using smaller images, which also helps to speed up the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to normalize inputs is to take the value of each pixel and
    divide it by 255, ending up with values ranging between 0 and 1\. Nevertheless,
    there are different methodologies to normalize an image, such as the mean-centering
    technique. The decision to choose one or the other is, most of the time, a matter
    of preference; however, when using pre-trained models, it is highly recommended
    that you use the same technique used to train the model the first time based on
    information that is always available in the documentation of the pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although CNNs are mainly used for computer vision problems, it is important
    to mention their capability to solve other learning problems, mainly in regard
    to analyzing sequences of data. For instance, CNNs have been known to perform
    well on sequences of text, audio, and video, sometimes in combination with other
    network architectures, or by converting the sequences into images that can be
    processed by CNNs. Some of the specific data problems that can be tackled using
    CNNs with sequences of data are machine translations of text, natural language
    processing, and video frame tagging, among many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, there are different tasks that CNNs can perform that apply to all
    supervised learning problems. However, from now on, this chapter will focus on
    computer vision. The following is a brief explanation of each of these tasks,
    along with a real-life example of each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: This is the most commonly known task in computer vision.
    The main idea is to classify the general contents of an image into a set of categories,
    known as labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, classification can determine whether an image is of a dog, a
    cat, or any other animal. This classification is done by outputting the probability
    of the image belonging to each of the classes, as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Classification task](img/C11865_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Classification task'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Localization**: The main purpose is to generate a bounding box that describes
    the object''s location in the image. The output consists of a class label and
    a bounding box.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be used in sensors to determine whether an object is to the left or
    right of the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Localization task](img/C11865_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Localization task'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Detection**: This task consists of performing object localization on all
    objects in the image. The output consists of multiple bounding boxes, as well
    as multiple class labels (one for each box).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is used in the construction of self-driving cars, with the objective of
    being able to locate the traffic signs, the road, other cars, pedestrians, and
    any other object that may be relevant to ensure a safe drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Detection task](img/C11865_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Detection task'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Segmentation**: The task here is to output both a class label and an outline
    of each object present in the image. This is mainly used to mark important objects
    of an image for further analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, it can be used to strictly delimit the area corresponding to
    a tumor in an image of the entire lung of a patient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Segmentation task](img/C11865_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Segmentation task'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this section onward, this chapter will focus on training a model to perform
    image classification, using one of PyTorch's image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Building Blocks of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, a deep convolutional network is one that takes an image
    as an input, passes it through a series of convolutional layers with filters,
    pooling layers, and fully connected layers, to finally apply a `softmax` activation
    function that classifies the image into a class label. The classification, as
    with ANNs, is performed by calculating the probability of the image belonging
    to each of the class labels, giving each class label a value between zero and
    one. The class label with the higher probability is the one selected as the final
    prediction for that image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a detailed explanation of each of these layers found, along
    with coding examples of how to define such layers in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layers**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the first step to extract features from an image. The objective is to
    maintain the relation between nearby pixels by learning the features over small
    sections of the image.
  prefs: []
  type: TYPE_NORMAL
- en: A mathematical operation occurs in this layer, where two inputs are given (the
    image and the filter) and an output is obtained. As explained before, the operation
    consists of convolving the filter and a section of the image of the same size
    of the filter. This operation is repeated for all subsections of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Revisit *Chapter 2*, *Building Blocks of Neural Networks*, the section titled
    Introduction to CNNs, for a reminder of the exact calculation performed between
    the input and the filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting matrix will have a shape depending on the shapes of the inputs,
    where an image matrix of size (h x w x c) and a filter of size (fh x fw x c) will
    output a matrix according to the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.7: Output height, width, and depth from a convolutional layer](img/C11865_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 4.7: Output height, width, and depth from a convolutional layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, h refers to the height of the input image, w is the width, c refers to
    the depth (also known as channels), and fh and fw are values set by the user concerning
    the size of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Dimensions of input, filter, and output](img/C11865_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Dimensions of input, filter, and output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is important to mention that, in a single convolutional layer, several filters
    can be applied to the same image, all of the same shape. Considering this, the
    output shape of a convolutional layer that applies two filters to its input, in
    terms of its depth, is equal to two, as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Convolutional layer with two filters](img/C11865_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Convolutional layer with two filters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each of these filters will perform a different operation, in order to discover
    different features from an image. For instance, in a single convolutional layer
    with two filters, the operations could be vertical edge detection and horizontal
    edge detection. Moreover, as the network grows in terms of the number of layers,
    the filters will perform more complex operations that make use of previously detected
    features, for example, the detection of the outline of a person by using the inputs
    from the edge detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, filters typically increase in each layer. This means that, while
    the first convolutional layer has eight filters, it is common to create the second
    convolutional layer to have twice this number (16), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to mention that, in PyTorch, as in many other frameworks,
    you should only define the number of filters to be used and not the type of filters
    (for instance, a vertical edge detector). Each filter configuration (the numbers
    that it contains to detect a specific feature) is part of the variables of the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two additional concepts to be introduced to the subject of convolutional
    layers, which will be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: The padding feature, as the name indicates, pads the image with zeros. This
    means that it adds additional pixels to each side of the image, which are filled
    with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows an example of an image that has been padded by one
    to each side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Graphical representation of an input image padded by one](img/C11865_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Graphical representation of an input image padded by one'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is used to maintain the shape of the input matrix once it has passed through
    the filter. This is because, especially in the first couple of layers, the objective
    should be to preserve as much information from the original input as possible,
    in order to extract the most features out of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the concept of padding, consider the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying a three by three filter to a colored image of shape 32 x 32 x 3 would
    result in a matrix of shape 30 x 30 x 1\. This means that the input for the following
    layer has shrunk. On the other hand, by adding padding of one to the input image,
    the shape of the input is changed to 34 x 34 x 3, which results in an output of
    32 x 32 x 1, using the same filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation can be used to calculate the output width when using
    padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.11: Output width after a convolutional layer using padding](img/C11865_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Output width after a convolutional layer using padding'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, W refers to the width of the input matrix, F refers to the width of the
    filter, and P refers to the padding. The same equation can be adapted to calculate
    the height of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain an output matrix of equal shape as the input, use the following equation
    to calculate the value for the padding (considering that the stride is equal to
    one):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.12: Padding number to get an output matrix of an equal size to
    the input](img/C11865_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Padding number to get an output matrix of an equal size to the
    input'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keep in mind that the number of output channels (depth) will always be equal
    to the number of filters applied over the input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stride**:'
  prefs: []
  type: TYPE_NORMAL
- en: This parameter refers to the number of pixels that the filter will shift over
    the input matrix, both horizontally and vertically. As we have seen so far, the
    filter is passed through the top-left corner of the image, then it shifts over
    to the right by one pixel, and so on until it has gone through all sections of
    the image vertically and horizontally. This example is one of a convolutional
    layer, with stride equal to one, which is the default configuration for this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'When stride equals two, the shift would be of two pixels instead, as seen in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: Graphical representation of a convolutional layer with stride
    of two](img/C11865_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Graphical representation of a convolutional layer with stride
    of two'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As can be seen, the initial operation occurs in the top-left corner, then, by
    shifting two pixels to the right, the second calculation occurs in the top-right
    corner. Next, the calculation shifts two pixels downward to perform the calculations
    on the bottom-left corner, and finally, by shifting again two pixels to the right,
    the final calculation occurs in the bottom-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The numbers in *Figure 4.13* are made up, and not actual calculations. The focus
    should be on the boxes that explain the shifting process when the stride is equal
    to two.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation can be used to calculate the output width when using
    stride:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.14: Output width after a convolutional layer using stride](img/C11865_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 4.14: Output width after a convolutional layer using stride'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, W refers to the width of the input matrix, F refers to the width of the
    filter and S refers to the stride. The same equation can be adapted to calculate
    the height of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once these parameters are introduced, the final equation to calculate the output
    shape (width and height) of the matrix derived from a convolutional layer is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.15: Output width after a convolutional layer using padding and
    stride](img/C11865_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 4.15: Output width after a convolutional layer using padding and stride'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whenever the value is a float, it should be rounded down. This basically means
    that some areas of the input are being ignored and no features are extracted from
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once the input has been passed through all the filters, the output
    is fed to an activation function in order to break linearity, similar to the process
    of traditional neural networks. Although there are several activation functions
    to be used in this step, the preferred one is the ReLU function since it has shown
    outstanding results in CNNs. The output obtained here becomes the input of the
    subsequent layer, which is usually a pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Calculating the Output Shape of a Convolutional Layer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considering the equations given, consider the following scenarios and calculate
    the shape of the output matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This exercise does not require coding, but rather consists of a practice exercise
    on the concepts mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'An input of shape 64 x 64 x 3\. A filter of shape 3 x 3 x 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An input of shape 32 x 32 x 3\. 10 filters of shape 5 x 5 x 3\. Padding of
    2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An input of shape 128 x 128 x 1\. 5 filters of shape 5 x 5 x 1\. Stride of
    3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An input of shape 64 x 64 x 1\. A filter of shape 8 x 8 x 1\. Padding of 3
    and stride of 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully calculated the output shape of the matrix
    derived from a convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding a convolutional layer in PyTorch is very simple. Using custom modules,
    it would only require the creation of the `network` class, which would have an
    `__init__` function containing the layers of the network, and a `forward` function
    that defines the step to pass the information through the different layers previously
    defined, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When defining the convolutional layer, the arguments that are passed through
    from left to right refer to the input channels, output channels (number of filters),
    kernel size (filter size), stride, and padding.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, the preceding example consists of a convolutional layer with
    three input channel, 18 filters, each of size 3, and stride and padding equal
    to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another valid approach, equivalent to the previous example, consists of the
    combination of the syntax from custom modules and the use of `Sequential` containers,
    as can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, the definition of layers occurs inside the `Sequential` container. Typically,
    one container would include a convolutional layer, an activation function, and
    a pooling layer. A new set of layers would be included in a different container
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pooling layers**'
  prefs: []
  type: TYPE_NORMAL
- en: Conventionally, pooling layers are the last part of the feature selection step,
    which is why a pooling layer can mostly be found after a convolutional layer.
    As explained in previous chapters, the idea is to extract the most relevant information
    out of subsections of the image. The size of the pooling layer is typically two,
    and the stride is equal to its size.
  prefs: []
  type: TYPE_NORMAL
- en: According to the preceding paragraph, pooling layers commonly reduce the input's
    height and weight by half. This is important considering that, in order for convolutional
    layers to find all features in an image, several filters need to be used, and
    the output from this operation can become too large, which means there are many
    parameters to consider. Pooling layers aim to reduce the number of parameters
    in the network by keeping the most relevant features. The selection of relevant
    features out of subsections of the image occurs either by grabbing the maximum
    number or by averaging the numbers in that region.
  prefs: []
  type: TYPE_NORMAL
- en: For image classification tasks, it is most common to use max pooling layers,
    over average pooling layers. This is because the former has shown better results
    in tasks where preserving the most relevant features is key, while the latter
    has been proven to work better in tasks such as smoothing images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the shape of the output matrix, the following equation can be
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 4.16: Output matrix width after a pooling layer](img/C11865_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 4.16: Output matrix width after a pooling layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, W refers to the width of the input, F refers to the size of the filter,
    and S refers to the stride. The same equation can be adapted to calculate the
    output height
  prefs: []
  type: TYPE_NORMAL
- en: The channels or depth of the input remains unchanged as the pooling layer will
    perform the same operation over all channels of the image. This means that the
    result from a pooling layer only affects the input in terms of width and length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Calculating the Output Shape of a set of Convolutional and Pooling
    Layers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following exercise will combine both convolutional and pooling layers. The
    objective is to determine the size of the output matrix after going through a
    set of layers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This activity does not require coding, but rather consists of a practice exercise
    on the concepts mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following sets of layers and specify the shape of the output layer
    at the end of all the transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image of size 256 x 256 x 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolutional layer with 16 filters of size three, and stride and padding
    of one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A pooling layer with a filter of size two and stride of size two as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolutional layer with eight filters of size seven, stride of one, and padding
    of three.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A pooling layer with a filter of size two and a stride of two as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Below, the output size of the matrix after going through each of this layers
    is shown:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully calculated the output shapes of the matrix
    derived from a series of convolutional and pooling layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the same coding examples as before, the PyTorch way to define pooling
    layers is shown in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the arguments that go into the max pooling layers, from left to right,
    are the size of the filter and the stride.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, an equally valid approach is shown here, with the use of custom modules
    and `Sequential` containers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As mentioned before, the pooling layer is also included in the same container
    as the convolutional layer and the activation function. A subsequent set of layers
    (convolutional, activation, and pooling) would be defined below, in a new `Sequential`
    container.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully connected layers**'
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected (FC) layer or layers are defined at the end of the network
    architecture, after the input has gone through a set of convolutional and pooling
    layers. The output data from the layer preceding the first fully-connected layer
    is flattened from a matrix to a vector, which can be fed to the fully connected
    layer (the same as a hidden layer from traditional neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of these FC layers is to consider all the features detected
    by the previous layers, in order to classify the image.
  prefs: []
  type: TYPE_NORMAL
- en: The different FC layers are passed through an activation function, which is
    typically the ReLU, unless it is the final layer, which will use a softmax function
    to output the probability of the input belonging to each of the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: The input size of the first fully connected layer corresponds to the size of
    the flattened output matrix from the previous layer. The output size is defined
    by the user, and, again, as with ANNs, there is not an exact science to setting
    this number. The last FC layer should have an output size equal to the number
    of class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define a set of FC layers in PyTorch, consider the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, two fully connected layers are added to the network. Next, inside the
    forward function, the output from the pooling layer is flattened using the `view()`
    function. Then, it is passed through the first FC layer, which applies an activation
    function. Finally, the data is passed through a final FC layer, along with its
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for defining fully connected layers using both custom modules and
    the `Sequential` container can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once the architecture of the network has been defined, the following steps of
    defining the different parameters (including the loss function and optimization
    algorithm), as well as the training process, can be handled in the same way as
    ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Side Note â€“ Downloading Datasets from PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To load a dataset from PyTorch, use the following code. Besides downloading
    the dataset, it shows how to use data loaders to save resources by loading the
    images by batches, rather than all at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the dataset to be downloaded is the MNIST. This is a
    popular dataset that contains images of hand-written gray-scaled numbers going
    from zero to nine. The `transform` variable defined before downloading the dataset
    is in charge of performing some transformations on the dataset. In this case,
    the dataset will be both converted into tensors and normalized in all its dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The `SubsetRandomSampler()` function from PyTorch is used to divide the original
    training set into training and validations by randomly sampling indexes. Moreover,
    the `DataLoader()` functions are the ones in charge of loading the images by batches.
    The resulting variables (`train_loader`, `dev_loader`, and `test_loader`) of this
    function will contain the values for the features and the target separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Building a CNN for an Image Classification Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The more complex the problem and the deeper the network, the longer it takes
    for the model to train. Considering this, the activities in this chapter may take
    longer than the ones in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following activity, a CNN will be trained on an image dataset from PyTorch.
    Let''s look at the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: You work at an artificial intelligence company that develops custom-made models
    for the needs of its customers. Your team is currently creating a model that can
    differentiate between vehicles and animals, and, more specifically, a model able
    to differentiate between different animals and different types of vehicles. They
    have provided you with a dataset containing 60,000 images and want you to build
    such a model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the activities within this chapter, you will need to have Python 3.6, Jupyter,
    NumPy, and Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The dataset to be used is CIFAR10 from PyTorch, which contains a total of 60,000
    images of vehicles and animals. There are 10 different class labels. The training
    set contains 50,000 images, while the testing set contains the remaining 10,000.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Set the transformations to be performed on the data, which will be the conversion
    of the data to tensors and the normalization of the pixel values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a batch size of 100 images and download both the training and testing data
    from the CIFAR10 dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a validation size of 20%, define the training and validation sampler that
    will be used to divide the dataset into those two sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `DataLoader()` function to define the batches to be used for each set
    of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the architecture of your network. Use the following information to do
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conv1: A convolutional layer that takes as input the colored image and passes
    it through 10 filters of size 3\. Both the padding and the stride should be set
    to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv2: A convolutional layer that passes the input data through 20 filters
    of size 3\. Both the padding and the stride should be set to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv3: A convolutional layer that passes the input data through 40 filters
    of size three. Both the padding and the stride should be set to 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the ReLU activation function after each convolutional layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A pooling layer after each convolutional layer, with a filter size and stride
    of 2.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A dropout term set to 20% after flattening the image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear1: A fully-connected layer that receives as input the flattened matrix
    from the previous layer and generates an output of 100 units. Use the ReLU activation
    function for this layer. A dropout term here is set to 20%.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear2: A fully-connected layer that generates 10 outputs, one for each class
    label. Use the `log_softmax` activation function for the output layer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Define all parameters required to train your model. Train for 50 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train your network and be sure to save the values for the loss and accuracy
    of both the training and validation sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the loss and accuracy of both sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Check the model's accuracy on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 204.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learning how to effectively code a neural network is one of the steps involved
    in developing state-of-the-art solutions. Additionally, to develop great deep
    learning solutions, it is also crucial to find an area of interest in which we
    can provide a solution to a current challenge (not an easy task, by the way).
    But, once all of that is done, we are typically faced with the same issue: getting
    a dataset of a decent size to get a good performance from our models, either by
    self-gathering or from the internet and other available sources.'
  prefs: []
  type: TYPE_NORMAL
- en: As you might imagine, and even though it is now possible to gather and store
    vast amounts of data, this is not an easy task due to the costs associated to
    it. And so, most of the time, we are stuck working with a dataset containing tens
    of thousands of entries, and even fewer when referring to images.
  prefs: []
  type: TYPE_NORMAL
- en: 'This becomes a relevant issue when developing a solution for a computer vision
    problem, mainly due to two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The larger the dataset, the better the results, and larger datasets are crucial
    to arrive at decent enough models. This is true considering that training a model
    is a matter of tuning a bunch of parameters such that it is capable of mapping
    a relationship between an input and an output, while minimizing the loss function
    by making the predicted value come as close to the ground truth as possible. Here,
    the more complex the model, the more parameters it requires.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering this, it is necessary to feed to the model a fair number of examples
    so that it is capable of finding such patterns, where the number of training examples
    should be proportional to the number of parameters to be tuned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moreover, one of the biggest challenges in computer vision problems is getting
    your model to perform well over several variations of an image. This means that
    images do not need to be fed following a specific alignment or have a set quality,
    but can instead be fed in their original formats, including different positions,
    angles, lighting, and other distortions. Because of this, it is necessary to find
    a way to feed the model with such variations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the data augmentation technique was designed. In simple words, it is a measure
    to increase the number of training examples by slightly modifying the existing
    examples. For example, you could duplicate the instances currently available and
    add some noise to those duplicates to make sure they are not exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision problems, this means incrementing the number of images in
    the training dataset by altering the existing images, which can be done by slightly
    altering the current images to create duplicated versions that are slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: These minor adjustments to the images can be in the form of slight rotations,
    changes in the position of the object in the frame, horizontal or vertical flips,
    different color schemes, and distortions, among others. This technique works considering
    that CNNs will consider each of these images a different image.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the following figure shows three images of a dog that, while
    to the human eye are the same image with certain variations, to the neural network
    are completely different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17: Augmented images](img/C11865_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Augmented images'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A CNN capable of recognizing an object in an image independently of any sort
    of variation is considered to have the property of invariance. In fact, a CNN
    can be invariant to each type of variation.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performing data augmentation in PyTorch using the `torchvision` package is very
    easy. The package, in addition to containing popular datasets and model architectures,
    also has common image transformation functions to perform on datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, a few of these image transformations will be mentioned. To
    get the entire list of possible transformations, visit [https://pytorch.org/docs/stable/torchvision/transforms.html](https://pytorch.org/docs/stable/torchvision/transforms.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the process used in the previous activity to normalize and convert
    the dataset to tensors, performing data augmentation requires us to first define
    the desired transformations, then to apply them to the dataset, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, the data to be downloaded will undergo a horizontal flip (considering
    the probability value, which defines whether the image will be flipped), and will
    be converted to grayscale (also considering the probability). Then, the data is
    converted to tensors and normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that a model is trained in an iterative process, in which the training
    data is fed multiple times, these transformations ensure that a second run through
    the dataset does not feed the exact same images to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is important to mention that different transformations can be set
    for different sets. This is useful because the purpose of data augmentation is
    to increment the number of training examples, but the images that will be used
    for testing the model should be left mostly unaltered. Nevertheless, the testing
    set should be resized in order to feed equally sized images to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be accomplished as shown in the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, a dictionary containing a set of transformations for the training
    and testing sets is defined. Then, they are called to apply the transformations
    to each of the sets, accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Implementing Data Augmentation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following activity, data augmentation will be introduced to the model
    created in the previous activity in order to test whether its accuracy can be
    improved. Let''s look at the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: The model that you have created is good, but the accuracy does not impress anyone
    yet. They have asked you to think of a methodology that could improve the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate the notebook from the previous activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the definition of the `transform` variable to include, in addition to
    normalizing and converting the data into tensors, the following transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the training/validation sets, a `RandomHorizontalFlip` function with a probability
    of 50% (0.5) and a `RandomGrayscale` function with a probability of 10% (0.1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For the testing set, do not add any other transformation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model for 100 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the accuracy of the resulting model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 209.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is typical to normalize the input layer in an attempt to speed up learning,
    as well as to improve performance by rescaling all features to the same scale.
    So, the question is, if the model benefits from the normalization of the input
    layer, why not normalize the output of all hidden layers in an attempt to improve
    the training speed even more?
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch normalization**, as its name suggests, normalizes the outputs from
    the hidden layers so that it reduces the variance from each layer, which is also
    known as covariance shift. This reduction of the covariance shift is useful as
    it allows the model to also work well over images that follow a different distribution
    than the images used to train it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take for instance a network that has the purpose of detecting whether an animal
    is a cat. When the network is trained only using images of black cats, batch normalization
    can help the network also classify new images of cats of different colors by normalizing
    the data so that both the black and colored cat images follow a similar distribution.
    Such problem is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18: Cat classifier. Model able to recognize colored cats, even after
    being trained using only black cats](img/C11865_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Cat classifier. Model able to recognize colored cats, even after
    being trained using only black cats'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Moreover, in addition to the above, batch normalization introduces the following
    benefits to the process of training the model, which ultimately helps you to arrive
    at a better performing model:'
  prefs: []
  type: TYPE_NORMAL
- en: It allows a higher learning rate to be set, as batch normalization helps to
    ensure that none of the outputs go too high or too low. A higher learning rate
    is equivalent to faster learning times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps to reduce overfitting because it has a regularization effect. This
    makes it possible to set the dropout probability at a lower value, which means
    that less information is ignored in each forward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to mention that we should not rely mainly on batch normalization
    to deal with overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As explained in previous layers, the normalization of the output of a hidden
    layer is done by subtracting the batch mean and dividing by the batch standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is important to mention that batch normalization is typically
    performed on the convolutional layers, as well as the fully connected layers (excluding
    the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In PyTorch, adding batch normalization is as simple as adding a new layer to
    the network architecture, considering that there are two different types, as explained
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BatchNorm1d**: This layer is used to implement batch normalization on a two-dimensional
    or three-dimensional input. It receives the number of output nodes from the previous
    layer as an argument. This is commonly used on fully connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**BatchNorm2d**: This applies batch normalization on four-dimensional inputs.
    Again, the argument that it takes in is the number of output nodes from the previous
    layer. It is commonly used on convolutional layers, meaning that the argument
    that it takes in should be equal to the number of channels from the previous layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this, the implementation of batch normalization in a CNN is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, batch normalization layers are initially defined in a similar
    way to any other layer. Next, each is applied to the output of its corresponding
    layer after the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Implementing Batch Normalization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the following activity, we will implement batch normalization on the architecture
    of the previous activity in order to see if it is possible to further improve
    the performance of the model on the testing set. Let''s look at the following
    scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wow! You have impressed your teammates with the last improvement in performance,
    and now they are expecting more from you. They have asked you to give improving
    the model one last try so that the accuracy goes up to 80%:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate the notebook from the previous activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add batch normalization to each convolutional layer, as well as to the first
    FC layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model for 100 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the accuracy of the resulting model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 211.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous chapter focused on CNNs, which consist of a kind of neural network
    architecture that performs outstandingly well on computer vision problems. It
    started by explaining the main reasons why CNNs are widely used for dealing with
    image datasets, as well as providing an introduction to the different tasks that
    can be solved through their use.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the chapter explained the different building blocks of the network's
    architecture, starting by explaining the nature of convolutional layers, then
    moving on to pooling layers, and finally explaining the fully connected layers.
    In each section, an explanation of the purpose of each layer was included, as
    well as the code snippets to effectively code the architecture in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: This led to the introduction of an image classification problem to be solved
    using the building blocks previously explained.
  prefs: []
  type: TYPE_NORMAL
- en: Next, data augmentation was introduced as a tool to improve a network's performance
    by incrementing the number of training examples, without the need to gather more
    images. This technique focuses on performing some variations on the existing images
    to create "new" images to be fed to the model.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing data augmentation, the second activity of the chapter aimed
    to solve the same image classification problem, with the objective of comparing
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter explained the concept of batch normalization. It consists
    of normalizing the output from each hidden layer in order to speed up learning.
    After explaining the process of applying batch normalization in PyTorch, the last
    activity of this chapter, once again, aimed to solve the same image classification
    problem using batch normalization.
  prefs: []
  type: TYPE_NORMAL
