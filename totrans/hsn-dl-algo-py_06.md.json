["```py\nU = np.random.uniform(-np.sqrt(1.0 / input_dim), np.sqrt(1.0 / input_dim), (hidden_dim, input_dim))\n\nW = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim), (hidden_dim, hidden_dim))\n\nV = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim), (input_dim, hidden_dim))\n```", "```py\nnum_time_steps = len(x)\n```", "```py\nhidden_state = np.zeros((num_time_steps + 1, hidden_dim))\n```", "```py\nhidden_state[-1] = np.zeros(hidden_dim)\n```", "```py\nYHat = np.zeros((num_time_steps, output_dim))\n```", "```py\nfor t in np.arange(num_time_steps):\n\n    #h_t = tanh(UX + Wh_{t-1})\n    hidden_state[t] = np.tanh(U[:, x[t]] + W.dot(hidden_state[t - 1]))\n\n    # yhat_t = softmax(vh)\n    YHat[t] = softmax(V.dot(hidden_state[t]))\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nimport numpy as np\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```", "```py\ndf = pd.read_csv('data/songdata.csv')\n```", "```py\ndf.head()\n```", "```py\ndf.shape[0]\n\n57650\n```", "```py\nlen(df['artist'].unique())\n\n643\n```", "```py\ndf['artist'].value_counts()[:10]\n\nDonna Summer        191\nGordon Lightfoot    189\nGeorge Strait       188\nBob Dylan           188\nLoretta Lynn        187\nCher                187\nAlabama             187\nReba Mcentire       187\nChaka Khan          186\nDean Martin         186\nName: artist, dtype: int64\n```", "```py\ndf['artist'].value_counts().values.mean()\n\n89\n```", "```py\ndata = ', '.join(df['text'])\n```", "```py\ndata[:369]\n\n\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\"\n```", "```py\nchars = sorted(list(set(data)))\n```", "```py\nvocab_size = len(chars)\n```", "```py\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for i, ch in enumerate(chars)}\n```", "```py\nprint char_to_ix['s']\n\n68\n```", "```py\nprint ix_to_char[68]\n\n's'\n```", "```py\nvocabSize = 7\nchar_index = 4\n\nprint np.eye(vocabSize)[char_index]\n\narray([0., 0., 0., 0., 1., 0., 0.])\n```", "```py\ndef one_hot_encoder(index):\n    return np.eye(vocab_size)[index]\n```", "```py\nhidden_size = 100\n```", "```py\nseq_length = 25\n```", "```py\nlearning_rate = 1e-1\n```", "```py\nseed_value = 42\ntf.set_random_seed(seed_value)\nrandom.seed(seed_value)\n```", "```py\ninputs = tf.placeholder(shape=[None, vocab_size],dtype=tf.float32, name=\"inputs\")\ntargets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")\n```", "```py\ninit_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n```", "```py\ninitializer = tf.random_normal_initializer(stddev=0.1)\n```", "```py\nwith tf.variable_scope(\"RNN\") as scope:\n    h_t = init_state\n    y_hat = []\n\n    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n        if t > 0:\n            scope.reuse_variables() \n\n        #input to hidden layer weights\n        U = tf.get_variable(\"U\", [vocab_size, hidden_size], initializer=initializer)\n\n        #hidden to hidden layer weights\n        W = tf.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n\n        #output to hidden layer weights\n        V = tf.get_variable(\"V\", [hidden_size, vocab_size], initializer=initializer)\n\n        #bias for hidden layer\n        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n\n        #bias for output layer\n        by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n\n        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n\n        y_hat_t = tf.matmul(h_t, V) + by\n\n        y_hat.append(y_hat_t) \n```", "```py\noutput_softmax = tf.nn.softmax(y_hat[-1])\noutputs = tf.concat(y_hat, axis=0)\n```", "```py\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n```", "```py\nhprev = h_t\n```", "```py\nminimizer = tf.train.AdamOptimizer()\n```", "```py\ngradients = minimizer.compute_gradients(loss)\n```", "```py\nthreshold = tf.constant(5.0, name=\"grad_clipping\")\n```", "```py\nclipped_gradients = []\nfor grad, var in gradients:\n    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n    clipped_gradients.append((clipped_grad, var))\n```", "```py\nupdated_gradients = minimizer.apply_gradients(clipped_gradients)\n```", "```py\nsess = tf.Session()\n\ninit = tf.global_variables_initializer()\n\nsess.run(init)\n```", "```py\npointer = 0\n```", "```py\ninput_sentence = data[pointer: pointer + seq_length]\n```", "```py\n\"Look at her face, it's a \"\n```", "```py\noutput_sentence = data[pointer + 1: pointer + seq_length + 1]\n```", "```py\n\"ook at her face, it's a w\"\n```", "```py\ninput_indices = [char_to_ix[ch] for ch in input_sentence]\ntarget_indices = [char_to_ix[ch] for ch in output_sentence]\n```", "```py\ninput_vector = one_hot_encoder(input_indices)\ntarget_vector = one_hot_encoder(target_indices)\n```", "```py\nhprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients], feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n```", "```py\nsample_length = 500\n```", "```py\nrandom_index = random.randint(0, len(data) - seq_length)\n```", "```py\nsample_input_sent = data[random_index:random_index + seq_length]\n```", "```py\nsample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n```", "```py\nsample_prev_state_val = np.copy(hprev_val)\n```", "```py\npredicted_indices = []\n```", "```py\nsample_input_vector = one_hot_encoder(sample_input_indices)\n```", "```py\nprobs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n```", "```py\nix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n```", "```py\nsample_input_indices = sample_input_indices[1:] + [ix]\n```", "```py\npredicted_indices.append(ix)\n```", "```py\npredicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n```", "```py\n text = ''.join(predicted_chars)\n```", "```py\nprint ('\\n')\nprint (' After %d iterations' %(iteration))\nprint('\\n %s \\n' % (text,)) \nprint('-'*115)\n```", "```py\npointer += seq_length\niteration += 1\n```", "```py\n After 0 iterations\n\n Y?a6C.-eMSfk0pHD v!74YNeI 3YeP,h- h6AADuANJJv:HA(QXNeKzwCjBnAShbavSrGw7:ZcSv[!?dUno Qt?OmE-PdY wrqhSu?Yvxdek?5Rn'Pj!n5:32a?cjue  ZIj\nXr6qn.scqpa7)[MSUjG-Sw8n3ZexdUrLXDQ:MOXBMX EiuKjGudcznGMkF:Y6)ynj0Hiajj?d?n2Iapmfc?WYd BWVyB-GAxe.Hq0PaEce5H!u5t: AkO?F(oz0Ma!BUMtGtSsAP]Oh,1nHf5tZCwU(F?X5CDzhOgSNH(4Cl-Ldk? HO7 WD9boZyPIDghWUfY B:r5z9Muzdw2'WWtf4srCgyX?hS!,BL GZHqgTY:K3!wn:aZGoxr?zmayANhMKJsZhGjpbgiwSw5Z:oatGAL4Xenk]jE3zJ?ymB6v?j7(mL[3DFsO['Hw-d7htzMn?nm20o'?6gfPZhBa\nNlOjnBd2n0 T\"d'e1k?OY6Wwnx6d!F \n\n----------------------------------------------------------------------------------------------\n\n After 50000 iterations\n\n Hem-:]  \n[Ex\" what  \nAkn'lise  \n[Grout his bring bear.  \nGnow ourd?  \nThelf  \nAs cloume  \nThat hands, Havi Musking me Mrse your leallas, Froking the cluse (have: mes.  \nI slok and if a serfres me the sky withrioni flle rome.....Ba tut get make ome  \nBut it lives I dive.  \n[Lett it's to the srom of and a live me it's streefies  \nAnd is.  \nAs it and is me dand a serray]  \n[zrtye:\"  \nChay at your hanyer  \n[Every rigbthing with farclets  \n\n[Brround.  \nMad is trie  \n[Chare's a day-Mom shacke?\n\n, I  \n\n-------------------------------------------------------------------------------------------------\n```"]