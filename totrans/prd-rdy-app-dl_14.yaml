- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Deep Learning on Mobile Devices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移动设备上的深度学习
- en: In this chapter, we will introduce how to deploy **deep learning** (**DL**)
    models developed with **TensorFlow** (**TF**) and **PyTorch** on mobile devices
    using **TensorFlow Lite** (**TF Lite**) and **PyTorch Mobile**, respectively.
    First, we will discuss how to convert a TF model into a TF Lite model. Then, we
    will explain how to convert a PyTorch model into a TorchScript model that PyTorch
    Mobile can consume. Finally, the last two sections of this chapter will cover
    how to integrate the converted models into Android and iOS applications (apps).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何在移动设备上部署**深度学习**（**DL**）模型，这些模型是使用**TensorFlow**（**TF**）和**PyTorch**开发的，并使用**TensorFlow
    Lite**（**TF Lite**）和**PyTorch Mobile**分别进行部署。首先，我们将讨论如何将TF模型转换为TF Lite模型。然后，我们将解释如何将PyTorch模型转换为TorchScript模型，以便PyTorch
    Mobile可以使用。最后，本章的最后两节将涵盖如何将转换后的模型集成到Android和iOS应用程序（应用）中。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将涵盖以下主要主题：
- en: Preparing DL models for mobile devices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为移动设备准备DL模型
- en: Creating iOS apps with a DL model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DL模型创建iOS应用程序
- en: Creating Android apps with a DL model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DL模型创建Android应用程序
- en: Preparing DL models for mobile devices
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为移动设备准备DL模型
- en: Mobile devices have reshaped how we carry out our daily lives by enabling easy
    access to the internet; many of our daily tasks heavily depend on mobile devices.
    Hence, if we can deploy DL models on mobile apps, we should be able to achieve
    the next level of convenience. Popular use cases include translation among different
    languages, object detection, and digit recognition, to name a few.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备通过便捷地访问互联网改变了我们日常生活的进行方式；我们许多日常任务都严重依赖移动设备。因此，如果我们能在移动应用中部署DL模型，我们应该能够实现更高水平的便利。流行的用例包括不同语言之间的翻译、目标检测和数字识别等。
- en: 'The following screenshots provide some example use cases:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了一些示例用例：
- en: '![Figure 11.1 – From left to right, the listed apps handle plant identification,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 - 从左到右，列出的应用程序处理植物识别，'
- en: object detection, and machine translation, exploiting the flexibility of DL
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和机器翻译，利用DL的灵活性
- en: '](img/B18522_11_011_Merged.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_11_011_Merged.jpg)'
- en: Figure 11.1 – From left to right, the listed apps handle plant identification,
    object detection, and machine translation, exploiting the flexibility of DL
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 - 从左到右，列出的应用程序处理植物识别、目标检测和机器翻译，利用DL的灵活性
- en: 'There exist many **operating systems** (**OSs**) for mobile devices. However,
    two OSs are dominating the mobile market currently: iOS and Android. iOS is the
    OS for devices from Apple, such as iPhone and iPad. Similarly, Android is the
    standard OS for devices produced by companies such as—for example—Samsung and
    Google. In this chapter, we focus on deployments targeted at the two dominating
    OSs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备存在许多**操作系统**（**OSs**）。然而，目前两种OSs在移动市场占据主导地位：iOS和Android。iOS是苹果设备（如iPhone和iPad）的操作系统。同样，Android是由三星和谷歌等公司生产的设备的标准操作系统。在本章中，我们专注于针对这两种主导OSs的部署。
- en: Unfortunately, TF and PyTorch models cannot be deployed on mobile devices in
    their original format. We need to convert them into formats that can run the inference
    logic on mobile devices. In the case of TF, we need a TF Lite model; we will first
    discuss how to convert a TF model into a TF Lite model using the `tensorflow`
    library. PyTorch, on the other hand, involves the PyTorch Mobile framework, which
    can only consume a TorchScript model. Following TF Lite conversion, we will learn
    how to convert a PyTorch model into a TorchScript model. Additionally, we will
    explain how to optimize certain layers of a PyTorch model for the target mobile
    environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，TF和PyTorch模型不能直接在移动设备上部署。我们需要将它们转换为可以在移动设备上运行推断逻辑的格式。对于TF，我们需要一个TF Lite模型；我们将首先讨论如何使用`tensorflow`库将TF模型转换为TF
    Lite模型。另一方面，PyTorch涉及PyTorch Mobile框架，该框架只能消耗TorchScript模型。在讨论了TF Lite转换后，我们将学习如何将PyTorch模型转换为TorchScript模型。此外，我们还将解释如何优化PyTorch模型的特定层，以适应目标移动环境。
- en: 'It is worth noting that a TF model or a PyTorch model can be converted to **open
    neural network exchange** (**ONNX**) Runtime and deployed on mobile ([https://onnxruntime.ai/docs/tutorials/mobile](https://onnxruntime.ai/docs/tutorials/mobile)).
    Additionally, SageMaker provides built-in support for loading DL models onto edge
    devices: SageMaker Edge Manager ([https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html](https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，TF 模型或 PyTorch 模型可以转换为**开放神经网络交换**（**ONNX**）运行时，并部署到移动设备上（[https://onnxruntime.ai/docs/tutorials/mobile](https://onnxruntime.ai/docs/tutorials/mobile)）。此外，SageMaker
    提供了内置支持，可将 DL 模型加载到边缘设备上：SageMaker Edge Manager（[https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html](https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html)）。
- en: Generating a TF Lite model
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成 TF Lite 模型
- en: 'TF Lite is a library used to deploy models on mobile devices, microcontrollers,
    and other edge devices ([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)).
    A trained TF model needs to be converted into a TF Lite model to be runnable on
    edge devices. As shown in the following code snippet, the `tensorflow` library
    has built-in support for converting a TF model to a TF Lite model (a `.tflite`
    file):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TF Lite 是一个用于在移动设备、微控制器和其他边缘设备上部署模型的库（[https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)）。训练好的
    TF 模型需要转换为 TF Lite 模型，才能在边缘设备上运行。如下面的代码片段所示，`tensorflow` 库内置支持将 TF 模型转换为 TF Lite
    模型（`.tflite` 文件）：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding Python code, the `from_saved_model` function of the `tf.lite.TFLiteConverter`
    class loads a trained TF model file. The `convert` method of this class converts
    the loaded TF model into a TF Lite model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述 Python 代码中，`tf.lite.TFLiteConverter` 类的 `from_saved_model` 函数加载训练好的 TF 模型文件。该类的
    `convert` 方法将加载的 TF 模型转换为 TF Lite 模型。
- en: As discussed in [*Chapter 10*](B18522_10.xhtml#_idTextAnchor212), *Improving
    Inference Efficiency*, TF Lite has diverse support for model compression techniques.
    Popular techniques available from TF Lite include network pruning and network
    quantization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 10 章*](B18522_10.xhtml#_idTextAnchor212)讨论的那样，*提升推理效率*，TF Lite 支持各种模型压缩技术。从
    TF Lite 中流行的技术包括网络剪枝和网络量化。
- en: Next, let’s look at how to convert a PyTorch model into a TorchScript model
    for PyTorch Mobile.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下如何将 PyTorch 模型转换为 TorchScript 模型以用于 PyTorch Mobile。
- en: Generating a TorchScript model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成 TorchScript 模型
- en: Running a PyTorch model on mobile devices can be achieved using the PyTorch
    Mobile framework ([https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/)).
    Similar to the case of TF, a trained PyTorch model has to be converted into a
    TorchScript model in order to run the model using PyTorch Mobile ([https://pytorch.org/docs/master/jit.html](https://pytorch.org/docs/master/jit.html)).
    The main advantage of a `torch.jit` module developed for TorchScript is the capability
    of running a PyTorch module outside of the Python environment, such as C++ environment.
    This is important when deploying a DL model to mobile devices as they do not support
    Python but support C++. The `torch.jit.script` method exports the graph of the
    given DL model into a low-level representation that can be executed in a C++ environment.
    Complete details on the cross-language support can be found at [https://pytorch.org/docs/stable/jit_language_reference.html#language-reference](https://pytorch.org/docs/stable/jit_language_reference.html#language-reference). Please
    note that TorchScript is still in a beta state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 PyTorch Mobile 框架在移动设备上运行 PyTorch 模型（[https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/)）。类似于
    TF 的情况，必须将训练好的 PyTorch 模型转换为 TorchScript 模型，以便使用 PyTorch Mobile 运行模型（[https://pytorch.org/docs/master/jit.html](https://pytorch.org/docs/master/jit.html)）。TorchScript
    模块的主要优势在于能够在 Python 以外的环境（如 C++ 环境）中运行 PyTorch 模块。`torch.jit.script` 方法将给定 DL
    模型的图导出为低级表示，可以在 C++ 环境中执行。有关跨语言支持的完整细节，请参阅[https://pytorch.org/docs/stable/jit_language_reference.html#language-reference](https://pytorch.org/docs/stable/jit_language_reference.html#language-reference)。请注意，TorchScript
    目前仍处于 beta 状态。
- en: 'In order to obtain a TorchScript model from a PyTorch model, you need to pass
    the trained model to the `torch.jit.script` function, as shown in the following
    code snippet. The TorchScript model can be further optimized for mobile environments
    by fusing `Conv2D` and `BatchNorm` layers or removing unnecessary `Dropout` layers
    using the `optimize_for_mobile` method of the `torch.utils.mobile_optimizer` module
    ([https://pytorch.org/docs/stable/mobile_optimizer.html](https://pytorch.org/docs/stable/mobile_optimizer.html)).
    Please keep in mind that the `mobile_optimizer` method is also in a beta state:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 PyTorch 模型获取 TorchScript 模型，需要将训练好的模型传递给 `torch.jit.script` 函数，如下面的代码片段所示。可以通过
    `torch.utils.mobile_optimizer` 模块的 `optimize_for_mobile` 方法来进一步优化 TorchScript
    模型，以适应移动环境，例如融合 `Conv2D` 和 `BatchNorm` 层或者移除不必要的 `Dropout` 层（详情请参考 [https://pytorch.org/docs/stable/mobile_optimizer.html](https://pytorch.org/docs/stable/mobile_optimizer.html)）。请注意，`mobile_optimizer`
    方法目前仍处于 beta 状态。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding example, we first load the trained model in memory (`torch.load("model.pt")`).
    The model should be in `eval` mode for the conversion. In the next line, we use
    the `torch.jit.script` function to convert the PyTorch model into a TorchScript
    model (`torchscript_model`). The TorchScript model is further optimized for the
    mobile environment using the `optimize_for_mobile` method; it generates an optimized
    TorchScript model (`torch_script_model_optimized`). The optimized TorchScript
    model can be saved as an independent `.pt` file (`mobile_optimized.pt`) using
    the `torch.jit.save` method.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们首先将训练好的模型加载到内存中（`torch.load("model.pt")`）。模型在进行转换时应处于 `eval` 模式。接下来，我们使用
    `torch.jit.script` 函数将 PyTorch 模型转换为 TorchScript 模型（`torchscript_model`）。使用 `optimize_for_mobile`
    方法进一步优化 TorchScript 模型，生成优化后的 TorchScript 模型（`torch_script_model_optimized`）。最后，可以使用
    `torch.jit.save` 方法将优化后的 TorchScript 模型保存为独立的 `.pt` 文件（`mobile_optimized.pt`）。
- en: Things to remember
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: a. Running a TF model on mobile devices involves the TF Lite framework. The
    trained model needs to be converted into a TF Lite model. The `TFLiteCoverter`
    class from the `tensorflow.lite` library is used for the conversion.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: a. 在移动设备上运行 TF 模型涉及 TF Lite 框架。训练好的模型需要转换成 TF Lite 模型。使用 `tensorflow.lite` 库中的
    `TFLiteConverter` 类来进行转换。
- en: b. Running a PyTorch model on a mobile device involves the PyTorch Mobile framework.
    Given that PyTorch Mobile only supports TorchScript models, the trained model
    needs to be converted into a `TorchScript` model using torch.jit library.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: b. 在移动设备上运行 PyTorch 模型涉及 PyTorch Mobile 框架。鉴于 PyTorch Mobile 仅支持 TorchScript
    模型，需要使用 torch.jit 库将训练好的模型转换为 `TorchScript` 模型。
- en: Next, we will learn how to integrate TF Lite and TorchScript models into an
    iOS app.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何将 TF Lite 和 TorchScript 模型集成到 iOS 应用中。
- en: Creating iOS apps with a DL model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DL 模型创建 iOS 应用
- en: In this section, we will cover how to write inference code for TF Lite and TorchScript
    models for an iOS app. While Swift and Objective-C are the native languages for
    iOS and can be used together for a single project, we will mainly look at Swift
    use cases as it is more popular than Objective-C nowadays.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何为 iOS 应用编写 TF Lite 和 TorchScript 模型的推断代码。虽然 Swift 和 Objective-C
    是 iOS 的本地语言，可以在一个项目中同时使用，但我们主要关注 Swift 的用例，因为它比 Objective-C 更受欢迎。
- en: 'The chapter would be lengthy if we explain every step of iOS app development.
    Therefore, we relegate the basics to the official tutorial provided by Apple:
    [https://developer.apple.com/tutorials/app-dev-training](https://developer.apple.com/tutorials/app-dev-training).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们详细解释 iOS 应用开发的每一个步骤，本章将会很冗长。因此，我们将基础内容放在了苹果提供的官方教程中：[https://developer.apple.com/tutorials/app-dev-training](https://developer.apple.com/tutorials/app-dev-training)。
- en: Running TF Lite model inference on iOS
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 iOS 上运行 TF Lite 模型推断
- en: 'In this section, we show how a TF Lite model can be loaded in an iOS app using
    `TensorFlowLiteSwift`, the native iOS library for TF Lite ([https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift)).
    Installing `TensorFlowLiteSwift` can be achieved through CocoaPods, the standard
    package manager for iOS app development ([https://cocoapods.org](https://cocoapods.org)).
    To download CocoaPods on macOS, you can run the `brew install cocoapods` command
    on the terminal. Each iOS app development involves a Podfile that lists the libraries
    that the app development depends on The `TensorFlowLiteSwift` library has to be
    added to this file, as shown in the following code snippet:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在iOS应用程序中加载TF Lite模型，使用`TensorFlowLiteSwift`，这是TF Lite的iOS本地库（[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift)）。可以通过CocoaPods安装`TensorFlowLiteSwift`，这是iOS应用程序开发的标准包管理器（[https://cocoapods.org](https://cocoapods.org)）。要在macOS上下载CocoaPods，可以在终端上运行`brew
    install cocoapods`命令。每个iOS应用程序开发都涉及一个Podfile，列出了应用程序开发所依赖的库。必须将`TensorFlowLiteSwift`库添加到此文件中，如以下代码片段所示：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To install all the libraries in a Podfile, you can run the `pod install` command.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Podfile中安装所有库，可以运行`pod install`命令。
- en: 'The following steps describe how to load a TF Lite model for your iOS app and
    run the inference logic. Complete details on the execution can be found at [https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的步骤描述了如何在您的iOS应用程序中加载TF Lite模型并运行推理逻辑。有关执行的完整细节，请参阅[https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift)：
- en: 'The installed libraries can be loaded using the `import` keyword:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`import`关键字加载安装的库：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize an `Interpreter` class by providing the path to the input TF Lite
    model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提供输入TF Lite模型的路径来初始化`Interpreter`类：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In order to pass the input data to the model, you need to use the `self.interpreter.copy`
    method to copy the input data into the input `Tensor` object at index `0`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将输入数据传递给模型，您需要使用`self.interpreter.copy`方法将输入数据复制到索引为`0`的输入`Tensor`对象中：
- en: '[PRE5]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the input `Tensor` object is ready, the `self.interpreter.invoke` method
    can be used to run the inference logic:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦输入的`Tensor`对象准备好，就可以使用`self.interpreter.invoke`方法运行推理逻辑：
- en: '[PRE6]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The generated output can be retrieved using `self.interpreter.output` as a
    `Tensor` object that can be further deserialized into an array using the `UnsafeMutableBufferPointer`
    class:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`self.interpreter.output`检索生成的输出，作为可以进一步使用`UnsafeMutableBufferPointer`类反序列化为数组的`Tensor`对象：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this section, we learned how to run TF Lite model inference in an iOS app.
    Next, we will introduce how to run TorchScript model inference in an iOS app.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在iOS应用程序中运行TF Lite模型推理。接下来，我们将介绍如何在iOS应用程序中运行TorchScript模型推理。
- en: Running TorchScript model inference on iOS
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在iOS上运行TorchScript模型推理
- en: 'In this section, we will learn how to deploy a TorchScript model on an iOS
    app using PyTorch Mobile. We will start with a Swift code snippet that uses the
    `TorchModule` module to load a trained TorchScript model. The library you need
    for PyTorch Mobile is called `LibTorch_Lite`. This library is also available through
    CocoaPods. All you need to do is to add the following line to the Podfile:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将学习如何在iOS应用程序上使用PyTorch Mobile部署TorchScript模型。我们将从使用`TorchModule`模块加载训练好的TorchScript模型的Swift代码片段开始。您需要用于PyTorch
    Mobile的库称为`LibTorch_Lite`。该库也可通过CocoaPods获得。您只需将以下行添加到Podfile中：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As described in the last section, you can run the `pod install` command to install
    the library.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一节所述，您可以运行`pod install`命令来安装库。
- en: Given a TorchScript model is designed for C++, Swift code cannot run model inference
    directly. To bridge this gap, there exists the `TorchModule` class, an Objective-C
    wrapper for `torch::jit::mobile::Module`. To use this functionality in your app,
    a folder named `TorchBridge` needs to be created under the project and contains
    `TorchModule.mm` (Objective-C implementation file), `TorchModule.h` (header file),
    and a bridging header file with the naming convention of a `-Bridging-Header.h`
    postfix (to allow Swift to load the Objective-C library). The complete sample
    setup can be found at [https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge](https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 TorchScript 模型是为 C++ 设计的，Swift 代码不能直接运行模型推断。为了弥合这一差距，存在 `TorchModule` 类，它是
    `torch::jit::mobile::Module` 的 Objective-C 包装器。要在应用程序中使用此功能，需要在项目下创建一个名为 `TorchBridge`
    的文件夹，其中包含 `TorchModule.mm`（Objective-C 实现文件）、`TorchModule.h`（头文件）和一个命名约定为 `-Bridging-Header.h`
    后缀的桥接头文件（以允许 Swift 加载 Objective-C 库）。完整的示例设置可以在 [https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge](https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge)
    找到。
- en: 'Throughout the following steps, we will show how to load a TorchScript model
    and trigger model prediction:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将展示如何加载 TorchScript 模型并触发模型预测：
- en: 'First, you need to import the `TorchModule` class to the project:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要将 `TorchModule` 类导入到项目中：
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, instantiate `TorchModule` by providing a path to the TorchScript model
    file:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过提供 TorchScript 模型文件的路径来实例化 `TorchModule`：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `predict` method of the `TorchModule` class handles the model inference.
    An input needs to be provided to the `predict` method and the output will be returned.
    Under the hood, the `predict` method will call the `forward` function of the model
    through the Objective-C wrapper. The code is illustrated in the following snippet:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TorchModule` 类的 `predict` 方法处理模型推断。需要向 `predict` 方法提供输入，然后将返回输出。在幕后，`predict`
    方法将通过 Objective-C 包装器调用模型的 `forward` 函数。以下代码中有所示：'
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you are curious about how inference actually works behind the scenes, we
    recommend that you read the *Run inference* section of [https://pytorch.org/mobile/ios/](https://pytorch.org/mobile/ios/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对推断的幕后实际工作原理感兴趣，建议阅读 [https://pytorch.org/mobile/ios/](https://pytorch.org/mobile/ios/)
    中的 *Run inference* 部分。
- en: Things to remember
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. Swift and Objective-C are the standard languages for developing iOS apps.
    A project can consist of files written in both languages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: a. Swift 和 Objective-C 是开发 iOS 应用程序的标准语言。一个项目可以包含用这两种语言编写的文件。
- en: b. The `TensorFlowSwift` library is the TF library for Swift. The `Interpreter`
    class supports TF Lite model inference on iOS.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: b. `TensorFlowSwift` 库是 Swift 的 TF 库。`Interpreter` 类支持 iOS 上 TF Lite 模型的推断。
- en: c. The `LibTorch_Lite` library supports TorchScript model inference on an iOS
    app through the `TorchModule` class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: c. `LibTorch_Lite` 库通过 `TorchModule` 类支持在 iOS 应用程序上进行 TorchScript 模型推断。
- en: Next, we will introduce how to run inference for TF Lite and TorchScript models
    on Android.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍如何在 Android 上运行 TF Lite 和 TorchScript 模型的推断。
- en: Creating Android apps with a DL model
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DL 模型创建 Android 应用程序
- en: In this section, we will discuss how Android supports TF Lite and PyTorch Mobile.
    Java and **Java Virtual Machine** (**JVM**)-based languages (for example, Kotlin)
    are the preferred languages for Android apps. In this section, we will be using
    Java. The basics of Android app development can be found at [https://developer.android.com](https://developer.android.com).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 Android 如何支持 TF Lite 和 PyTorch Mobile。Java 和 **Java 虚拟机**（**JVM**）为
    Android 应用程序提供的首选语言（例如 Kotlin）。在本节中，我们将使用 Java。有关 Android 应用程序开发的基础知识，请访问 [https://developer.android.com](https://developer.android.com)。
- en: We first focus on running TF Lite model inference on Android using the `org.tensorflow:tensorflow-lite-support`
    library. Then, we discuss how to run TorchScript model inference using the `org.pytorch:pytorch_android_lite`
    library.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先专注于使用 `org.tensorflow:tensorflow-lite-support` 库在 Android 上运行 TF Lite 模型推断。然后，我们讨论如何使用
    `org.pytorch:pytorch_android_lite` 库运行 TorchScript 模型推断。
- en: Running TF Lite model inference on Android
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Android 上运行 TF Lite 模型推断
- en: First, let’s look at how to run a TF Lite model on Android using Java. The `org.tensorflow:tensorflow-lite-support`
    library is used to deploy a TF Lite model on an Android app. The library supports
    Java, C++ (beta), and Swift (beta). A complete list of supported environments
    can be found at [https://github.com/tensorflow/tflite-support](https://github.com/tensorflow/tflite-support).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用 Java 在 Android 上运行 TF Lite 模型。使用 `org.tensorflow:tensorflow-lite-support`
    库可以在 Android 应用上部署 TF Lite 模型。该库支持 Java、C++（测试版）和 Swift（测试版）。支持的环境完整列表可在 [https://github.com/tensorflow/tflite-support](https://github.com/tensorflow/tflite-support)
    找到。
- en: 'Android app development involves Gradle, a build automation tool that manages
    dependencies ([https://gradle.org](https://gradle.org)). Each project will have
    a `.gradle` file that specifies the project specification in JVM-based languages
    such as Groovy or Kotlin. In the following code snippet, we list the libraries
    that the project is dependent on under the `dependencies` section:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Android 应用开发涉及 Gradle，这是一个管理依赖项的构建自动化工具 ([https://gradle.org](https://gradle.org))。每个项目都会有一个
    `.gradle` 文件，该文件指定了使用基于 JVM 的语言（如 Groovy 或 Kotlin）的项目规范。在以下代码片段中，我们列出了项目在 `dependencies`
    部分下依赖的库：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding Gradle code in Groovy, we have specified the `org.tensorflow:tensorflow-lite-support`
    library as one of our dependencies. A sample Gradle file can be found at [https://docs.gradle.org/current/samples/sample_building_java_applications.html](https://docs.gradle.org/current/samples/sample_building_java_applications.html).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Groovy Gradle 代码中，我们已经指定了 `org.tensorflow:tensorflow-lite-support` 库作为我们的依赖项之一。可以在
    [https://docs.gradle.org/current/samples/sample_building_java_applications.html](https://docs.gradle.org/current/samples/sample_building_java_applications.html)
    找到一个示例 Gradle 文件。
- en: 'In the following steps, we will look at how to load a TF Lite model and run
    the inference logic. You can find the complete details about this process at [https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将学习如何加载 TF Lite 模型并运行推理逻辑。有关此过程的完整详细信息可以在 [https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter)
    找到：
- en: 'The first is to import the `org.tensorflow.lite` library, which contains the
    `Interpreter` class for TF Lite model inference:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是导入包含用于 TF Lite 模型推理的 `Interpreter` 类的 `org.tensorflow.lite` 库：
- en: '[PRE13]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we can instantiate `Interpreter` class by providing a model path:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过提供模型路径来实例化 `Interpreter` 类：
- en: '[PRE14]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `run` method of the `Interpreter` class instance is used to run the inference
    logic. It takes in only one `input` instance of type `HashMap` and provides only
    one `output` instance of `HashMap`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Interpreter` 类的 `run` 方法用于运行推理逻辑。它只接受一个 `input` 类型为 `HashMap` 的实例，并提供一个类型为
    `HashMap` 的 `output` 实例：'
- en: '[PRE15]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next section, we will learn how to load a TorchScript model into an Android
    app.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何将 TorchScript 模型加载到 Android 应用中。
- en: Running TorchScript model inference on Android
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Android 上运行 TorchScript 模型推理
- en: 'In this section, we will explain how to run a TorchScript model in an Android
    app. To run TorchScript model inference in an Android app, you need a Java wrapper
    provided by the `org.pytorch:pytorch_android_lite` library. Again, you can specify
    the necessary library in the `.gradle` file, as shown in the following code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何在 Android 应用中运行 TorchScript 模型。要在 Android 应用中运行 TorchScript 模型推理，您需要使用
    `org.pytorch:pytorch_android_lite` 库提供的 Java 包装器。同样，您可以在 `.gradle` 文件中指定必需的库，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running TorchScript model inference in an Android app can be achieved by following
    the steps presented next. The key is to use the `Module` class from the `org.pytorch`
    library, which calls a C++ function for inference behind the scenes ([https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html](https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html)):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Android 应用中运行 TorchScript 模型推理可以通过以下步骤来实现。关键是使用来自 `org.pytorch` 库的 `Module`
    类，该类在后台调用 C++ 函数进行推理（[https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html](https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html)）：
- en: 'First of all, you need to import the `Module` class:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要导入 `Module` 类：
- en: '[PRE17]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `Module` class provides a `load` function that creates a Module instance
    by loading the model file provided:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Module` 类提供了一个 `load` 函数，通过加载提供的模型文件创建一个 `Module` 实例：'
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `forward` method of the `Module` instance is used to run the inference
    logic and generate an output of type `org.pytorch.Tensor`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Module` 实例的 `forward` 方法用于运行推理逻辑并生成 `org.pytorch.Tensor` 类型的输出：'
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'While the preceding steps cover basic usage of the `org.pytorch` module, you
    can find other details from their official documentation: [https://pytorch.org/mobile/android](https://pytorch.org/mobile/android).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的步骤涵盖了`org.pytorch`模块的基本用法，您可以在官方文档中找到其他细节：[https://pytorch.org/mobile/android](https://pytorch.org/mobile/android)。
- en: Things to remember
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事项
- en: a. Java and JVM-based languages (for example, Kotlin) are the native languages
    for Android apps.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: a. Java 和基于 JVM 的语言（例如 Kotlin）是 Android 应用程序的本地语言。
- en: b. The `org.tensorflow:tensorflow-lite-support` library is used to deploy a
    TF Lite model on Android. The `run` method of the `Interpreter` class instance
    handles model inference.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: b. `org.tensorflow:tensorflow-lite-support`库用于在 Android 上部署 TF Lite 模型。`Interpreter`类实例的`run`方法处理模型推断。
- en: c. The `org.pytorch:pytorch_android_lite` library is designed for running the
    TorchScript model in an Android app. The `forward` method from the `Module` class
    handles the inference logic.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: c. `org.pytorch:pytorch_android_lite`库专为在 Android 应用程序中运行 TorchScript 模型而设计。`Module`类的`forward`方法处理推断逻辑。
- en: That completes DL model deployment on Android. Now, you should be able to integrate
    any TF and PyTorch models into an Android app.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了在 Android 上部署 DL 模型。现在，您应该能够将任何 TF 和 PyTorch 模型集成到 Android 应用程序中。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to integrate TF and PyTorch models into iOS
    and Android apps. We started the chapter by describing necessary conversions from
    a TF model to the TF Lite model and from a PyTorch model to the TorchScript model.
    Next, we provided complete examples for loading TF Lite and TorchScript models
    and running inference using the loaded models on iOS and Android.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何将 TF 和 PyTorch 模型集成到 iOS 和 Android 应用程序中。我们从描述从 TF 模型到 TF Lite 模型的必要转换以及从
    PyTorch 模型到 TorchScript 模型开始本章。接下来，我们提供了加载 TF Lite 和 TorchScript 模型并在 iOS 和 Android
    上使用加载模型进行推断的完整示例。
- en: In the next chapter, we will learn how to keep our eyes on the deployed models.
    We will look at a set of tools developed for model monitoring and describe how
    to efficiently monitor models deployed on **Amazon Elastic Kubernetes Service**
    (**Amazon EKS**) and Amazon SageMaker.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何关注部署模型。我们将查看一组用于模型监控的工具，并描述如何有效监控部署在**亚马逊弹性 Kubernetes 服务**（**Amazon
    EKS**）和 Amazon SageMaker 上的模型。
