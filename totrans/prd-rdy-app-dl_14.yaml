- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning on Mobile Devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce how to deploy **deep learning** (**DL**)
    models developed with **TensorFlow** (**TF**) and **PyTorch** on mobile devices
    using **TensorFlow Lite** (**TF Lite**) and **PyTorch Mobile**, respectively.
    First, we will discuss how to convert a TF model into a TF Lite model. Then, we
    will explain how to convert a PyTorch model into a TorchScript model that PyTorch
    Mobile can consume. Finally, the last two sections of this chapter will cover
    how to integrate the converted models into Android and iOS applications (apps).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing DL models for mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating iOS apps with a DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Android apps with a DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing DL models for mobile devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mobile devices have reshaped how we carry out our daily lives by enabling easy
    access to the internet; many of our daily tasks heavily depend on mobile devices.
    Hence, if we can deploy DL models on mobile apps, we should be able to achieve
    the next level of convenience. Popular use cases include translation among different
    languages, object detection, and digit recognition, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshots provide some example use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – From left to right, the listed apps handle plant identification,'
  prefs: []
  type: TYPE_NORMAL
- en: object detection, and machine translation, exploiting the flexibility of DL
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_11_011_Merged.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – From left to right, the listed apps handle plant identification,
    object detection, and machine translation, exploiting the flexibility of DL
  prefs: []
  type: TYPE_NORMAL
- en: 'There exist many **operating systems** (**OSs**) for mobile devices. However,
    two OSs are dominating the mobile market currently: iOS and Android. iOS is the
    OS for devices from Apple, such as iPhone and iPad. Similarly, Android is the
    standard OS for devices produced by companies such as—for example—Samsung and
    Google. In this chapter, we focus on deployments targeted at the two dominating
    OSs.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, TF and PyTorch models cannot be deployed on mobile devices in
    their original format. We need to convert them into formats that can run the inference
    logic on mobile devices. In the case of TF, we need a TF Lite model; we will first
    discuss how to convert a TF model into a TF Lite model using the `tensorflow`
    library. PyTorch, on the other hand, involves the PyTorch Mobile framework, which
    can only consume a TorchScript model. Following TF Lite conversion, we will learn
    how to convert a PyTorch model into a TorchScript model. Additionally, we will
    explain how to optimize certain layers of a PyTorch model for the target mobile
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that a TF model or a PyTorch model can be converted to **open
    neural network exchange** (**ONNX**) Runtime and deployed on mobile ([https://onnxruntime.ai/docs/tutorials/mobile](https://onnxruntime.ai/docs/tutorials/mobile)).
    Additionally, SageMaker provides built-in support for loading DL models onto edge
    devices: SageMaker Edge Manager ([https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html](https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Generating a TF Lite model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TF Lite is a library used to deploy models on mobile devices, microcontrollers,
    and other edge devices ([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)).
    A trained TF model needs to be converted into a TF Lite model to be runnable on
    edge devices. As shown in the following code snippet, the `tensorflow` library
    has built-in support for converting a TF model to a TF Lite model (a `.tflite`
    file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding Python code, the `from_saved_model` function of the `tf.lite.TFLiteConverter`
    class loads a trained TF model file. The `convert` method of this class converts
    the loaded TF model into a TF Lite model.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 10*](B18522_10.xhtml#_idTextAnchor212), *Improving
    Inference Efficiency*, TF Lite has diverse support for model compression techniques.
    Popular techniques available from TF Lite include network pruning and network
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at how to convert a PyTorch model into a TorchScript model
    for PyTorch Mobile.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a TorchScript model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running a PyTorch model on mobile devices can be achieved using the PyTorch
    Mobile framework ([https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/)).
    Similar to the case of TF, a trained PyTorch model has to be converted into a
    TorchScript model in order to run the model using PyTorch Mobile ([https://pytorch.org/docs/master/jit.html](https://pytorch.org/docs/master/jit.html)).
    The main advantage of a `torch.jit` module developed for TorchScript is the capability
    of running a PyTorch module outside of the Python environment, such as C++ environment.
    This is important when deploying a DL model to mobile devices as they do not support
    Python but support C++. The `torch.jit.script` method exports the graph of the
    given DL model into a low-level representation that can be executed in a C++ environment.
    Complete details on the cross-language support can be found at [https://pytorch.org/docs/stable/jit_language_reference.html#language-reference](https://pytorch.org/docs/stable/jit_language_reference.html#language-reference). Please
    note that TorchScript is still in a beta state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain a TorchScript model from a PyTorch model, you need to pass
    the trained model to the `torch.jit.script` function, as shown in the following
    code snippet. The TorchScript model can be further optimized for mobile environments
    by fusing `Conv2D` and `BatchNorm` layers or removing unnecessary `Dropout` layers
    using the `optimize_for_mobile` method of the `torch.utils.mobile_optimizer` module
    ([https://pytorch.org/docs/stable/mobile_optimizer.html](https://pytorch.org/docs/stable/mobile_optimizer.html)).
    Please keep in mind that the `mobile_optimizer` method is also in a beta state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we first load the trained model in memory (`torch.load("model.pt")`).
    The model should be in `eval` mode for the conversion. In the next line, we use
    the `torch.jit.script` function to convert the PyTorch model into a TorchScript
    model (`torchscript_model`). The TorchScript model is further optimized for the
    mobile environment using the `optimize_for_mobile` method; it generates an optimized
    TorchScript model (`torch_script_model_optimized`). The optimized TorchScript
    model can be saved as an independent `.pt` file (`mobile_optimized.pt`) using
    the `torch.jit.save` method.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Running a TF model on mobile devices involves the TF Lite framework. The
    trained model needs to be converted into a TF Lite model. The `TFLiteCoverter`
    class from the `tensorflow.lite` library is used for the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: b. Running a PyTorch model on a mobile device involves the PyTorch Mobile framework.
    Given that PyTorch Mobile only supports TorchScript models, the trained model
    needs to be converted into a `TorchScript` model using torch.jit library.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn how to integrate TF Lite and TorchScript models into an
    iOS app.
  prefs: []
  type: TYPE_NORMAL
- en: Creating iOS apps with a DL model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover how to write inference code for TF Lite and TorchScript
    models for an iOS app. While Swift and Objective-C are the native languages for
    iOS and can be used together for a single project, we will mainly look at Swift
    use cases as it is more popular than Objective-C nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter would be lengthy if we explain every step of iOS app development.
    Therefore, we relegate the basics to the official tutorial provided by Apple:
    [https://developer.apple.com/tutorials/app-dev-training](https://developer.apple.com/tutorials/app-dev-training).'
  prefs: []
  type: TYPE_NORMAL
- en: Running TF Lite model inference on iOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we show how a TF Lite model can be loaded in an iOS app using
    `TensorFlowLiteSwift`, the native iOS library for TF Lite ([https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift)).
    Installing `TensorFlowLiteSwift` can be achieved through CocoaPods, the standard
    package manager for iOS app development ([https://cocoapods.org](https://cocoapods.org)).
    To download CocoaPods on macOS, you can run the `brew install cocoapods` command
    on the terminal. Each iOS app development involves a Podfile that lists the libraries
    that the app development depends on The `TensorFlowLiteSwift` library has to be
    added to this file, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To install all the libraries in a Podfile, you can run the `pod install` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how to load a TF Lite model for your iOS app and
    run the inference logic. Complete details on the execution can be found at [https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The installed libraries can be loaded using the `import` keyword:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize an `Interpreter` class by providing the path to the input TF Lite
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to pass the input data to the model, you need to use the `self.interpreter.copy`
    method to copy the input data into the input `Tensor` object at index `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the input `Tensor` object is ready, the `self.interpreter.invoke` method
    can be used to run the inference logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The generated output can be retrieved using `self.interpreter.output` as a
    `Tensor` object that can be further deserialized into an array using the `UnsafeMutableBufferPointer`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we learned how to run TF Lite model inference in an iOS app.
    Next, we will introduce how to run TorchScript model inference in an iOS app.
  prefs: []
  type: TYPE_NORMAL
- en: Running TorchScript model inference on iOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to deploy a TorchScript model on an iOS
    app using PyTorch Mobile. We will start with a Swift code snippet that uses the
    `TorchModule` module to load a trained TorchScript model. The library you need
    for PyTorch Mobile is called `LibTorch_Lite`. This library is also available through
    CocoaPods. All you need to do is to add the following line to the Podfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As described in the last section, you can run the `pod install` command to install
    the library.
  prefs: []
  type: TYPE_NORMAL
- en: Given a TorchScript model is designed for C++, Swift code cannot run model inference
    directly. To bridge this gap, there exists the `TorchModule` class, an Objective-C
    wrapper for `torch::jit::mobile::Module`. To use this functionality in your app,
    a folder named `TorchBridge` needs to be created under the project and contains
    `TorchModule.mm` (Objective-C implementation file), `TorchModule.h` (header file),
    and a bridging header file with the naming convention of a `-Bridging-Header.h`
    postfix (to allow Swift to load the Objective-C library). The complete sample
    setup can be found at [https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge](https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge).
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the following steps, we will show how to load a TorchScript model
    and trigger model prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to import the `TorchModule` class to the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, instantiate `TorchModule` by providing a path to the TorchScript model
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `predict` method of the `TorchModule` class handles the model inference.
    An input needs to be provided to the `predict` method and the output will be returned.
    Under the hood, the `predict` method will call the `forward` function of the model
    through the Objective-C wrapper. The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are curious about how inference actually works behind the scenes, we
    recommend that you read the *Run inference* section of [https://pytorch.org/mobile/ios/](https://pytorch.org/mobile/ios/).
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Swift and Objective-C are the standard languages for developing iOS apps.
    A project can consist of files written in both languages.
  prefs: []
  type: TYPE_NORMAL
- en: b. The `TensorFlowSwift` library is the TF library for Swift. The `Interpreter`
    class supports TF Lite model inference on iOS.
  prefs: []
  type: TYPE_NORMAL
- en: c. The `LibTorch_Lite` library supports TorchScript model inference on an iOS
    app through the `TorchModule` class.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce how to run inference for TF Lite and TorchScript models
    on Android.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Android apps with a DL model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how Android supports TF Lite and PyTorch Mobile.
    Java and **Java Virtual Machine** (**JVM**)-based languages (for example, Kotlin)
    are the preferred languages for Android apps. In this section, we will be using
    Java. The basics of Android app development can be found at [https://developer.android.com](https://developer.android.com).
  prefs: []
  type: TYPE_NORMAL
- en: We first focus on running TF Lite model inference on Android using the `org.tensorflow:tensorflow-lite-support`
    library. Then, we discuss how to run TorchScript model inference using the `org.pytorch:pytorch_android_lite`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Running TF Lite model inference on Android
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s look at how to run a TF Lite model on Android using Java. The `org.tensorflow:tensorflow-lite-support`
    library is used to deploy a TF Lite model on an Android app. The library supports
    Java, C++ (beta), and Swift (beta). A complete list of supported environments
    can be found at [https://github.com/tensorflow/tflite-support](https://github.com/tensorflow/tflite-support).
  prefs: []
  type: TYPE_NORMAL
- en: 'Android app development involves Gradle, a build automation tool that manages
    dependencies ([https://gradle.org](https://gradle.org)). Each project will have
    a `.gradle` file that specifies the project specification in JVM-based languages
    such as Groovy or Kotlin. In the following code snippet, we list the libraries
    that the project is dependent on under the `dependencies` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding Gradle code in Groovy, we have specified the `org.tensorflow:tensorflow-lite-support`
    library as one of our dependencies. A sample Gradle file can be found at [https://docs.gradle.org/current/samples/sample_building_java_applications.html](https://docs.gradle.org/current/samples/sample_building_java_applications.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following steps, we will look at how to load a TF Lite model and run
    the inference logic. You can find the complete details about this process at [https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is to import the `org.tensorflow.lite` library, which contains the
    `Interpreter` class for TF Lite model inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can instantiate `Interpreter` class by providing a model path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `run` method of the `Interpreter` class instance is used to run the inference
    logic. It takes in only one `input` instance of type `HashMap` and provides only
    one `output` instance of `HashMap`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, we will learn how to load a TorchScript model into an Android
    app.
  prefs: []
  type: TYPE_NORMAL
- en: Running TorchScript model inference on Android
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will explain how to run a TorchScript model in an Android
    app. To run TorchScript model inference in an Android app, you need a Java wrapper
    provided by the `org.pytorch:pytorch_android_lite` library. Again, you can specify
    the necessary library in the `.gradle` file, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Running TorchScript model inference in an Android app can be achieved by following
    the steps presented next. The key is to use the `Module` class from the `org.pytorch`
    library, which calls a C++ function for inference behind the scenes ([https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html](https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, you need to import the `Module` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Module` class provides a `load` function that creates a Module instance
    by loading the model file provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `forward` method of the `Module` instance is used to run the inference
    logic and generate an output of type `org.pytorch.Tensor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While the preceding steps cover basic usage of the `org.pytorch` module, you
    can find other details from their official documentation: [https://pytorch.org/mobile/android](https://pytorch.org/mobile/android).'
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Java and JVM-based languages (for example, Kotlin) are the native languages
    for Android apps.
  prefs: []
  type: TYPE_NORMAL
- en: b. The `org.tensorflow:tensorflow-lite-support` library is used to deploy a
    TF Lite model on Android. The `run` method of the `Interpreter` class instance
    handles model inference.
  prefs: []
  type: TYPE_NORMAL
- en: c. The `org.pytorch:pytorch_android_lite` library is designed for running the
    TorchScript model in an Android app. The `forward` method from the `Module` class
    handles the inference logic.
  prefs: []
  type: TYPE_NORMAL
- en: That completes DL model deployment on Android. Now, you should be able to integrate
    any TF and PyTorch models into an Android app.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how to integrate TF and PyTorch models into iOS
    and Android apps. We started the chapter by describing necessary conversions from
    a TF model to the TF Lite model and from a PyTorch model to the TorchScript model.
    Next, we provided complete examples for loading TF Lite and TorchScript models
    and running inference using the loaded models on iOS and Android.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to keep our eyes on the deployed models.
    We will look at a set of tools developed for model monitoring and describe how
    to efficiently monitor models deployed on **Amazon Elastic Kubernetes Service**
    (**Amazon EKS**) and Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
