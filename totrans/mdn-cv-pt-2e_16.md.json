["```py\n    !wget https://www.dropbox.com/s/g6b6gtvmdu0h77x/ShoeV2_photo.zip\n    !pip install torch_snippets\n    !pip install torch_summary\n    from torch_snippets import *\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    def detect_edges(img):\n        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        img_gray = cv2.bilateralFilter(img_gray, 5, 50, 50)\n        img_gray_edges = cv2.Canny(img_gray, 45, 100)\n        # invert black/white\n        img_gray_edges = cv2.bitwise_not(img_gray_edges)         \n        img_edges=cv2.cvtColor(img_gray_edges, cv2.COLOR_GRAY2RGB)\n        return img_edges \n    ```", "```py\n    IMAGE_SIZE = 256\n    preprocess = T.Compose([\n                 T.Lambda(lambda x: torch.Tensor(x.copy())\\\n                           .permute(2, 0, 1).to(device))])\n    normalize = lambda x: (x - 127.5)/127.5 \n    ```", "```py\n    class ShoesData(Dataset):\n        def __init__(self, items):\n            self.items = items\n        def __len__(self): return len(self.items) \n    ```", "```py\n     def __getitem__(self, ix):\n            f = self.items[ix]\n            try: im = read(f, 1)\n            except:\n                blank = preprocess(Blank(IMAGE_SIZE, IMAGE_SIZE, 3))\n                return blank, blank\n            edges = detect_edges(im) \n    ```", "```py\n     im, edges = resize(im, IMAGE_SIZE), resize(edges, IMAGE_SIZE)\n            im, edges = normalize(im), normalize(edges) \n    ```", "```py\n     self._draw_color_circles_on_src_img(edges, im)\n            im, edges = preprocess(im), preprocess(edges)\n            return edges, im \n    ```", "```py\n     def _draw_color_circles_on_src_img(self, img_src,\n                                           img_target):\n            non_white_coords = self._get_non_white_coordinates(img_target)\n            for center_y, center_x in non_white_coords:\n                self._draw_color_circle_on_src_img(img_src,\n                            img_target, center_y, center_x)\n        def _get_non_white_coordinates(self, img):\n            non_white_mask = np.sum(img, axis=-1) < 2.75\n            non_white_y, non_white_x = np.nonzero(non_white_mask)\n            # randomly sample non-white coordinates\n            n_non_white = len(non_white_y)\n            n_color_points = min(n_non_white, 300)\n            idxs = np.random.choice(n_non_white, n_color_points, \n                                                  replace=False)\n            non_white_coords = list(zip(non_white_y[idxs],\n                                        non_white_x[idxs]))\n            return non_white_coords\n        def _draw_color_circle_on_src_img(self, img_src,\n                                img_target, center_y, center_x):\n            assert img_src.shape == img_target.shape\n            y0, y1, x0, x1= self._get_color_point_bbox_coords(center_y,                                                          center_x)\n            color= np.mean(img_target[y0:y1, x0:x1],axis=(0, 1))\n            img_src[y0:y1, x0:x1] = color\n        def _get_color_point_bbox_coords(self, center_y,center_x):\n            radius = 2\n            y0 = max(0, center_y-radius+1)\n            y1 = min(IMAGE_SIZE, center_y+radius)\n            x0 = max(0, center_x-radius+1)\n            x1 = min(IMAGE_SIZE, center_x+radius)\n            return y0, y1, x0, x1\n        def choose(self): return self[randint(len(self))] \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train_items, val_items = train_test_split(Glob('ShoeV2_photo/*.png'),\n                             test_size=0.2, random_state=2)\n    trn_ds, val_ds = ShoesData(train_items), ShoesData(val_items)\n    trn_dl = DataLoader(trn_ds, batch_size=32, shuffle=True)\n    val_dl = DataLoader(val_ds, batch_size=32, shuffle=True) \n    ```", "```py\n    def weights_init_normal(m):\n        classname = m.__class__.__name__\n        if classname.find(\"Conv\") != -1:\n            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find(\"BatchNorm2d\") != -1:\n            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n            torch.nn.init.constant_(m.bias.data, 0.0) \n    ```", "```py\n    class UNetDown(nn.Module):\n        def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n            super(UNetDown, self).__init__()\n            layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_size))\n            layers.append(nn.LeakyReLU(0.2))\n            if dropout:\n                layers.append(nn.Dropout(dropout))\n            self.model = nn.Sequential(*layers)\n        def forward(self, x):\n            return self.model(x)\n    class UNetUp(nn.Module):\n        def __init__(self, in_size, out_size, dropout=0.0):\n            super(UNetUp, self).__init__()\n            layers = [\n                nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n                nn.InstanceNorm2d(out_size),\n                nn.ReLU(inplace=True),\n            ]\n            if dropout:\n                layers.append(nn.Dropout(dropout))\n            self.model = nn.Sequential(*layers)\n        def forward(self, x, skip_input):\n            x = self.model(x)\n            x = torch.cat((x, skip_input), 1)\n            return x \n    ```", "```py\n    class GeneratorUNet(nn.Module):\n        def __init__(self, in_channels=3, out_channels=3):\n            super(GeneratorUNet, self).__init__()\n            self.down1 = UNetDown(in_channels, 64,normalize=False)\n            self.down2 = UNetDown(64, 128)\n            self.down3 = UNetDown(128, 256)\n            self.down4 = UNetDown(256, 512, dropout=0.5)\n            self.down5 = UNetDown(512, 512, dropout=0.5)\n            self.down6 = UNetDown(512, 512, dropout=0.5)\n            self.down7 = UNetDown(512, 512, dropout=0.5)\n            self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n            self.up1 = UNetUp(512, 512, dropout=0.5)\n            self.up2 = UNetUp(1024, 512, dropout=0.5)\n            self.up3 = UNetUp(1024, 512, dropout=0.5)\n            self.up4 = UNetUp(1024, 512, dropout=0.5)\n            self.up5 = UNetUp(1024, 256)\n            self.up6 = UNetUp(512, 128)\n            self.up7 = UNetUp(256, 64)\n            self.final = nn.Sequential(\n                nn.Upsample(scale_factor=2),\n                nn.ZeroPad2d((1, 0, 1, 0)),\n                nn.Conv2d(128, out_channels, 4, padding=1),\n                nn.Tanh(),\n            )\n        def forward(self, x):\n            d1 = self.down1(x)\n            d2 = self.down2(d1)\n            d3 = self.down3(d2)\n            d4 = self.down4(d3)\n            d5 = self.down5(d4)\n            d6 = self.down6(d5)\n            d7 = self.down7(d6)\n            d8 = self.down8(d7)\n            u1 = self.up1(d8, d7)\n            u2 = self.up2(u1, d6)\n            u3 = self.up3(u2, d5)\n            u4 = self.up4(u3, d4)\n            u5 = self.up5(u4, d3)\n            u6 = self.up6(u5, d2)\n            u7 = self.up7(u6, d1)\n            return self.final(u7) \n    ```", "```py\n    class Discriminator(nn.Module):\n        def __init__(self, in_channels=3):\n            super(Discriminator, self).__init__()\n            def discriminator_block(in_filters, out_filters, \n                                          normalization=True):\n                \"\"\"Returns downsampling layers of each\n                discriminator block\"\"\"\n                layers = [nn.Conv2d(in_filters, out_filters, 4, \n                                          stride=2, padding=1)]\n                if normalization:\n                   layers.append(nn.InstanceNorm2d(out_filters))\n                layers.append(nn.LeakyReLU(0.2, inplace=True))\n                return layers\n            self.model = nn.Sequential(\n                *discriminator_block(in_channels * 2, 64,\n                                     normalization=False),\n                *discriminator_block(64, 128),\n                *discriminator_block(128, 256),\n                *discriminator_block(256, 512),\n                nn.ZeroPad2d((1, 0, 1, 0)),\n                nn.Conv2d(512, 1, 4, padding=1, bias=False)\n                                      )\n        def forward(self, img_A, img_B):\n            img_input = torch.cat((img_A, img_B), 1)\n            return self.model(img_input) \n    ```", "```py\n    generator = GeneratorUNet().to(device)\n    discriminator = Discriminator().to(device)\n    from torchsummary import summary\n    print(summary(generator, torch.zeros(3, 3, IMAGE_SIZE,\n                                IMAGE_SIZE).to(device)))\n    print(summary(discriminator, torch.zeros(3, 3, IMAGE_SIZE, IMAGE_SIZE).\\to(device), torch.zeros(3, 3, IMAGE_SIZE, IMAGE_SIZE).to(device))) \n    ```", "```py\n    def discriminator_train_step(real_src, real_trg, fake_trg):\n        d_optimizer.zero_grad() \n    ```", "```py\n     prediction_real = discriminator(real_trg, real_src)\n        error_real = criterion_GAN(prediction_real,\n                                   torch.ones(len(real_src), \n                                          1, 16, 16).to(device))\n        error_real.backward() \n    ```", "```py\n     prediction_fake = discriminator( real_src, fake_trg.detach())\n        error_fake = criterion_GAN(prediction_fake,\n                                   torch.zeros(len(real_src), 1, \n                                        16, 16).to(device))\n        error_fake.backward() \n    ```", "```py\n     d_optimizer.step()\n        return error_real + error_fake \n    ```", "```py\n    def generator_train_step(real_src, fake_trg):\n        g_optimizer.zero_grad()\n        prediction = discriminator(fake_trg, real_src)\n        loss_GAN = criterion_GAN(prediction, \n                torch.ones(len(real_src), 1, 16, 16).to(device))\n        loss_pixel = criterion_pixelwise(fake_trg, real_trg)\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n        loss_G.backward()\n        g_optimizer.step()\n        return loss_G \n    ```", "```py\n    denorm = T.Normalize((-1, -1, -1), (2, 2, 2))\n    def sample_prediction():\n        \"\"\"Saves a generated sample from the validation set\"\"\"\n        data = next(iter(val_dl))\n        real_src, real_trg = data\n        fake_trg = generator(real_src)\n        img_sample = torch.cat([denorm(real_src[0]),\n                                denorm(fake_trg[0]),\n                                denorm(real_trg[0])], -1)\n        img_sample = img_sample.detach().cpu().permute(1,2,0).numpy()\n        show(img_sample, title='Source::Generated::GroundTruth',sz=12) \n    ```", "```py\n    generator.apply(weights_init_normal)\n    discriminator.apply(weights_init_normal) \n    ```", "```py\n    criterion_GAN = torch.nn.MSELoss()\n    criterion_pixelwise = torch.nn.L1Loss()\n    lambda_pixel = 100\n    g_optimizer = torch.optim.Adam(generator.parameters(),\n                 \t             lr=0.0002, betas=(0.5, 0.999))\n    d_optimizer= torch.optim.Adam(discriminator.parameters(),\n                                    lr=0.0002, betas=(0.5, 0.999)) \n    ```", "```py\n    epochs = 100\n    log = Report(epochs)\n    for epoch in range(epochs):\n        N = len(trn_dl)\n        for bx, batch in enumerate(trn_dl):\n            real_src, real_trg = batch\n            fake_trg = generator(real_src)\n            errD = discriminator_train_step(real_src,  real_trg, fake_trg)\n            errG = generator_train_step(real_src, fake_trg)\n            log.record(pos=epoch+(1+bx)/N, errD=errD.item(),\n                       errG=errG.item(), end='\\r')\n        [sample_prediction() for _ in range(2)] \n    ```", "```py\n    [sample_prediction() for _ in range(2)] \n    ```", "```py\n    !wget https://www.dropbox.com/s/2xltmolfbfharri/apples_oranges.zip\n    !unzip apples_oranges.zip \n    ```", "```py\n    !pip install torch_snippets torch_summary\n    import itertools\n    from PIL import Image\n    from torch_snippets import *\n    from torchvision import transforms\n    from torchvision.utils import make_grid\n    from torchsummary import summary \n    ```", "```py\n    IMAGE_SIZE = 256\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    transform = transforms.Compose([\n        transforms.Resize(int(IMAGE_SIZE*1.33)),\n        transforms.RandomCrop((IMAGE_SIZE,IMAGE_SIZE)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n    ```", "```py\n    class CycleGANDataset(Dataset):\n        def __init__(self, apples, oranges):\n            self.apples = Glob(apples)\n            self.oranges = Glob(oranges)\n        def __getitem__(self, ix):\n            apple = self.apples[ix % len(self.apples)]\n            orange = choose(self.oranges)\n            apple = Image.open(apple).convert('RGB')\n            orange = Image.open(orange).convert('RGB')\n            return apple, orange\n        def __len__(self): return max(len(self.apples), len(self.oranges))\n        def choose(self): return self[randint(len(self))]\n        def collate_fn(self, batch):\n            srcs, trgs = list(zip(*batch))\n            srcs=torch.cat([transform(img)[None] for img in \\\n                             srcs], 0).to(device).float()\n            trgs=torch.cat([transform(img)[None] for img in \\ \n                             trgs], 0).to(device).float()\n            return srcs.to(device), trgs.to(device) \n    ```", "```py\n    trn_ds = CycleGANDataset('apples_train', 'oranges_train')\n    val_ds = CycleGANDataset('apples_test', 'oranges_test')\n    trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True,\n                        collate_fn=trn_ds.collate_fn)\n    val_dl = DataLoader(val_ds, batch_size=5, shuffle=True,\n                        collate_fn=val_ds.collate_fn) \n    ```", "```py\n    def weights_init_normal(m):\n        classname = m.__class__.__name__\n        if classname.find(\"Conv\") != -1:\n            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n            if hasattr(m, \"bias\") and m.bias is not None:\n                torch.nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find(\"BatchNorm2d\") != -1:\n            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n            torch.nn.init.constant_(m.bias.data, 0.0) \n    ```", "```py\n    class ResidualBlock(nn.Module):\n        def __init__(self, in_features):\n            super(ResidualBlock, self).__init__()\n            self.block = nn.Sequential(\n                nn.ReflectionPad2d(1),\n                nn.Conv2d(in_features, in_features, 3),\n                nn.InstanceNorm2d(in_features),\n                nn.ReLU(inplace=True),\n                nn.ReflectionPad2d(1),\n                nn.Conv2d(in_features, in_features, 3),\n                nn.InstanceNorm2d(in_features),\n            )\n        def forward(self, x):\n            return x + self.block(x) \n    ```", "```py\n    class GeneratorResNet(nn.Module):\n        def __init__(self, num_residual_blocks=9):\n            super(GeneratorResNet, self).__init__()\n            out_features = 64\n            channels = 3\n            model = [\n                nn.ReflectionPad2d(3),\n                nn.Conv2d(channels, out_features, 7),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n            # Downsampling\n            for _ in range(2):\n                out_features *= 2\n                model += [\n                    nn.Conv2d(in_features, out_features, 3,\n                              stride=2, padding=1),\n                    nn.InstanceNorm2d(out_features),\n                    nn.ReLU(inplace=True),\n                         ]\n                in_features = out_features\n            # Residual blocks\n            for _ in range(num_residual_blocks):\n                model += [ResidualBlock(out_features)]\n            # Upsampling\n            for _ in range(2):\n                out_features //= 2\n                model += [\n                    nn.Upsample(scale_factor=2),\n                    nn.Conv2d(in_features, out_features, 3,\n                              stride=1, padding=1),\n                    nn.InstanceNorm2d(out_features),\n                    nn.ReLU(inplace=True),\n                         ]\n                in_features = out_features\n            # Output layer\n            model += [nn.ReflectionPad2d(channels),\n                      nn.Conv2d(out_features, channels, 7),\n                      nn.Tanh()]\n            self.model = nn.Sequential(*model)\n            self.apply(weights_init_normal)\n        def forward(self, x):\n            return self.model(x) \n    ```", "```py\n    class Discriminator(nn.Module):\n        def __init__(self):\n            super(Discriminator, self).__init__()\n            channels, height, width = 3, IMAGE_SIZE, IMAGE_SIZE\n            def discriminator_block(in_filters, out_filters,  \n                                            normalize=True):\n                \"\"\"Returns downsampling layers of each\n                discriminator block\"\"\"\n                layers = [nn.Conv2d(in_filters, out_filters, 4, \n                                    stride=2, padding=1)]\n                if normalize:\n                   layers.append(nn.InstanceNorm2d(out_filters))\n                layers.append(nn.LeakyReLU(0.2, inplace=True))\n                return layers\n            self.model = nn.Sequential(\n              *discriminator_block(channels,64,normalize=False),\n              *discriminator_block(64, 128),\n              *discriminator_block(128, 256),\n              *discriminator_block(256, 512),\n              nn.ZeroPad2d((1, 0, 1, 0)),\n              nn.Conv2d(512, 1, 4, padding=1)\n            )\n            self.apply(weights_init_normal)\n        def forward(self, img):\n            return self.model(img) \n    ```", "```py\n    @torch.no_grad()\n    def generate_sample():\n        data = next(iter(val_dl))\n        G_AB.eval()\n        G_BA.eval()   \n        real_A, real_B = data\n        fake_B = G_AB(real_A)\n        fake_A = G_BA(real_B)\n        # Arange images along x-axis\n        real_A = make_grid(real_A, nrow=5, normalize=True)\n        real_B = make_grid(real_B, nrow=5, normalize=True)\n        fake_A = make_grid(fake_A, nrow=5, normalize=True)\n        fake_B = make_grid(fake_B, nrow=5, normalize=True)\n        # Arange images along y-axis\n        image_grid = torch.cat((real_A,fake_B,real_B,fake_A), 1)\n    show(image_grid.detach().cpu().permute(1,2,0).numpy(),sz=12) \n    ```", "```py\n    def generator_train_step(Gs, optimizer, real_A, real_B): \n    ```", "```py\n     G_AB, G_BA = Gs \n    ```", "```py\n     optimizer.zero_grad() \n    ```", "```py\n     loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) / 2 \n    ```", "```py\n     fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B),\n                        torch.Tensor(np.ones((len(real_A), 1,\n                                          16, 16))).to(device))\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A),\n                        torch.Tensor(np.ones((len(real_A), 1,\n                                          16, 16))).to(device))\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2 \n    ```", "```py\n     recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2 \n    ```", "```py\n     loss_G = loss_GAN + lambda_cyc * loss_cycle + \\\n                lambda_id * loss_identity\n        loss_G.backward()\n        optimizer.step()\n        return loss_G, loss_identity, loss_GAN, loss_cycle,\n                loss_G, fake_A, fake_B \n    ```", "```py\n    def discriminator_train_step(D, real_data, fake_data, optimizer):\n        optimizer.zero_grad()\n        loss_real = criterion_GAN(D(real_data),\n                      torch.Tensor(np.ones((len(real_data), 1,\n                                       16, 16))).to(device))\n        loss_fake = criterion_GAN(D(fake_data.detach()),\n                      torch.Tensor(np.zeros((len(real_data), 1,\n                                       16, 16))).to(device))\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer.step()\n        return loss_D \n    ```", "```py\n    G_AB = GeneratorResNet().to(device)\n    G_BA = GeneratorResNet().to(device)\n    D_A = Discriminator().to(device)\n    D_B = Discriminator().to(device)\n    criterion_GAN = torch.nn.MSELoss()\n    criterion_cycle = torch.nn.L1Loss()\n    criterion_identity = torch.nn.L1Loss()\n    optimizer_G = torch.optim.Adam(\n        itertools.chain(G_AB.parameters(), G_BA.parameters()),\n        lr=0.0002, betas=(0.5, 0.999))\n    optimizer_D_A = torch.optim.Adam(D_A.parameters(),\n                            lr=0.0002, betas=(0.5, 0.999))\n    optimizer_D_B = torch.optim.Adam(D_B.parameters(),\n                            lr=0.0002, betas=(0.5, 0.999))\n    lambda_cyc, lambda_id = 10.0, 5.0 \n    ```", "```py\n    n_epochs = 10\n    log = Report(n_epochs)\n    for epoch in range(n_epochs):\n        N = len(trn_dl)\n        for bx, batch in enumerate(trn_dl):\n            real_A, real_B = batch\n            loss_G, loss_identity, loss_GAN, loss_cycle, \\\n            loss_G, fake_A, fake_B = generator_train_step(\\\n                                      (G_AB,G_BA), optimizer_G, \n                                             real_A, real_B)\n            loss_D_A = discriminator_train_step(D_A, real_A, \n                                      fake_A, optimizer_D_A)\n            loss_D_B = discriminator_train_step(D_B, real_B, \n                                      fake_B, optimizer_D_B)\n            loss_D = (loss_D_A + loss_D_B) / 2\n\n            log.record(epoch+(1+bx)/N, loss_D=loss_D.item(),\n              loss_G=loss_G.item(), loss_GAN=loss_GAN.item(),\n                loss_cycle=loss_cycle.item(),\n               loss_identity=loss_identity.item(), end='\\r')\n            if bx%100==0: generate_sample()\n        log.report_avgs(epoch+1) \n    ```", "```py\n    generate_sample() \n    ```", "```py\n    import os\n    if not os.path.exists('pytorch_stylegan_encoder'):\n        !git clone https://github.com/sizhky/pytorch_stylegan_encoder.git\n        %cd pytorch_stylegan_encoder\n        !git submodule update --init --recursive\n        !wget -q https://github.com/jacobhallberg/pytorch_stylegan_encoder/releases/download/v1.0/trained_models.zip\n        !unzip -q trained_models.zip\n        !rm trained_models.zip\n        !pip install -qU torch_snippets\n        !mv trained_models/stylegan_ffhq.pth InterFaceGAN/models/pretrain\n    else:\n        %cd pytorch_stylegan_encoder\n\n    from torch_snippets import * \n    ```", "```py\n    from InterFaceGAN.models.stylegan_generator import StyleGANGenerator\n    from models.latent_optimizer import PostSynthesisProcessing\n    synthesizer=StyleGANGenerator(\"stylegan_ffhq\").model.synthesis\n    mapper = StyleGANGenerator(\"stylegan_ffhq\").model.mapping\n    trunc = StyleGANGenerator(\"stylegan_ffhq\").model.truncation \n    ```", "```py\n    post_processing = PostSynthesisProcessing()\n    post_process = lambda image: post_processing(image)\\\n                    .detach().cpu().numpy().astype(np.uint8)[0]\n    def latent2image(latent):\n        img = post_process(synthesizer(latent))\n        img = img.transpose(1,2,0)\n        return img \n    ```", "```py\n    rand_latents = torch.randn(1,512).cuda() \n    ```", "```py\n    show(latent2image(trunc(mapper(rand_latents))), sz=5) \n    ```", "```py\n    !wget https://www.dropbox.com/s/lpw10qawsc5ipbn/MyImage.JPG\\\n     -O MyImage.jpg\n    !git clone https://github.com/Puzer/stylegan-encoder.git\n    !mkdir -p stylegan-encoder/raw_images\n    !mkdir -p stylegan-encoder/aligned_images\n    !mv MyImage.jpg stylegan-encoder/raw_images \n    ```", "```py\n    !python stylegan-encoder/align_images.py \\\n            stylegan-encoder/raw_images/ \\\n            stylegan-encoder/aligned_images/\n    !mv stylegan-encoder/aligned_images/* ./MyImage.jpg \n    ```", "```py\n    from PIL import Image\n    img = Image.open('MyImage.jpg')\n    show(np.array(img), sz=4, title='original')\n    !python encode_image.py ./MyImage.jpg\\\n     pred_dlatents_myImage.npy\\\n     --use_latent_finder true\\\n     --image_to_latent_path ./trained_models/image_to_latent.pt\n    pred_dlatents = np.load('pred_dlatents_myImage.npy')\n    pred_dlatent = torch.from_numpy(pred_dlatents).float().cuda()\n    pred_image = latent2image(pred_dlatent)\n    show(pred_image, sz=4, title='synthesized') \n    ```", "```py\n    idxs_to_swap = slice(0,3)\n    my_latents=torch.Tensor(np.load('pred_dlatents_myImage.npy',\n                                             allow_pickle=True))\n    A, B = latent2image(my_latents.cuda()),\n           latent2image(trunc(mapper(rand_latents)))\n    generated_image_latents = trunc(mapper(rand_latents))\n    x = my_latents.clone()\n    x[:,idxs_to_swap] = generated_image_latents[:,idxs_to_swap]\n    a = latent2image(x.float().cuda())\n    x = generated_image_latents.clone()\n    x[:,idxs_to_swap] = my_latents[:,idxs_to_swap]\n    b = latent2image(x.float().cuda())\n    subplots([A,a,B,b], figsize=(7,8), nc=2,\n             suptitle='Transfer high level features') \n    ```", "```py\n    !python InterFaceGAN/edit.py\\\n     -m stylegan_ffhq\\\n     -o results_new_smile\\\n     -b InterFaceGAN/boundaries/stylegan_ffhq_smile_w_boundary.npy\\\n     -i pred_dlatents_myImage.npy\\\n     -s WP\\\n     --steps 20\n    generated_faces = glob.glob('results_new_smile/*.jpg')\n    subplots([read(im,1) for im in sorted(generated_faces)], \n                                            figsize=(10,10)) \n    ```", "```py\n    import os\n    if not os.path.exists('srgan.pth.tar'):\n        !pip install -q torch_snippets\n        !wget -q https://raw.githubusercontent.com/sizhky/a-PyTorch-Tutorial-to-Super-Resolution/master/models.py -O models.py\n        from pydrive.auth import GoogleAuth\n        from pydrive.drive import GoogleDrive\n        from google.colab import auth\n        from oauth2client.client import GoogleCredentials\n        auth.authenticate_user()\n        gauth = GoogleAuth()\n        gauth.credentials = GoogleCredentials.get_application_default()\n        drive = GoogleDrive(gauth)\n        downloaded = drive.CreateFile({'id': \\\n                        '1_PJ1Uimbr0xrPjE8U3Q_bG7XycGgsbVo'})\n        downloaded.GetContentFile('srgan.pth.tar')\n        from torch_snippets import *\n        device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    model = torch.load('srgan.pth.tar', map_location='cpu')['generator'].to(device)\n    model.eval() \n    ```", "```py\n    !wget https://www.dropbox.com/s/nmzwu68nrl9j0lf/Hema6.JPG \n    ```", "```py\n    preprocess = T.Compose([\n                    T.ToTensor(),\n                    T.Normalize([0.485, 0.456, 0.406],\n                                [0.229, 0.224, 0.225]),\n                    T.Lambda(lambda x: x.to(device))\n                ])\n    postprocess = T.Compose([\n                    T.Lambda(lambda x: (x.cpu().detach()+1)/2),\n                    T.ToPILImage()\n                ]) \n    ```", "```py\n    image = readPIL('Hema6.JPG')\n    image.size\n    # (260,181)\n    image = image.resize((130,90))\n    im = preprocess(image) \n    ```", "```py\n    sr = model(im[None])[0]\n    sr = postprocess(sr) \n    ```", "```py\n    subplots([image, sr], nc=2, figsize=(10,10),\n             titles=['Original image','High resolution image']) \n    ```"]