["```py\ncorpus = \"movie_corpus\"\ncorpus_name = \"movie_corpus\"\ndatafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\nwith open(datafile, 'rb') as file:\n    lines = file.readlines()\n\nfor line in lines[:3]:\n    print(str(line) + '\\n')\n```", "```py\n    PAD_token = 0 \n    SOS_token = 1\n    EOS_token = 2\n    class Vocabulary:\n        def __init__(self, name):\n            self.name = name\n            self.trimmed = False\n            self.word2index = {}\n            self.word2count = {}\n            self.index2word = {PAD_token: \"PAD\", SOS_token:                           \"SOS\", EOS_token: \"EOS\"}\n            self.num_words = 3\n    ```", "```py\n    def addWord(self, w):\n        if w not in self.word2index:\n            self.word2index[w] = self.num_words\n            self.word2count[w] = 1\n            self.index2word[self.num_words] = w\n            self.num_words += 1\n        else:\n            self.word2count[w] += 1\n    ```", "```py\n    def addSentence(self, sent):\n        for word in sent.split(' '):\n            self.addWord(word)\n    ```", "```py\n    def trim(self, min_cnt):\n        if self.trimmed:\n            return\n        self.trimmed = True\n        words_to_keep = []\n        for k, v in self.word2count.items():\n            if v >= min_cnt:\n                words_to_keep.append(k)\n        print('Words to Keep: {} / {} = {:.2%}'.format(\n            len(words_to_keep), len(self.word2index),    \n            len(words_to_keep) / len(self.word2index)))\n    ```", "```py\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\",\\\n                           SOS_token: \"SOS\",\\\n                           EOS_token: \"EOS\"}\n        self.num_words = 3\n        for w in words_to_keep:\n            self.addWord(w)\n    ```", "```py\n    def unicodeToAscii(s):\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn'\n        )\n    ```", "```py\n    def cleanString(s):\n        s = unicodeToAscii(s.lower().strip())\n        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n        s = re.sub(r\"\\s+\", r\" \", s).strip()\n        return s\n    ```", "```py\n    def readVocs(datafile, corpus_name):\n        lines = open(datafile, encoding='utf-8').\\\n            read().strip().split('\\n')\n        pairs = [[cleanString(s) for s in l.split('\\t')]               for l in lines]\n        voc = Vocabulary(corpus_name)\n        return voc, pairs\n    ```", "```py\n    def filterPair(p, max_length):\n        return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n    def filterPairs(pairs, max_length):\n        return [pair for pair in pairs if filterPair(pair,             max_length)]\n    ```", "```py\n    def loadData(corpus, corpus_name, datafile, save_dir, max_length):\n        voc, pairs = readVocs(datafile, corpus_name)\n        print(str(len(pairs)) + \" Sentence pairs\")\n        pairs = filterPairs(pairs,max_length)\n        print(str(len(pairs))+ \" Sentence pairs after           trimming\")\n        for p in pairs:\n            voc.addSentence(p[0])\n            voc.addSentence(p[1])\n        print(str(voc.num_words) + \" Distinct words in           vocabulary\")\n        return voc, pairs\n    max_length = 10 \n    voc, pairs = loadData(corpus, corpus_name, datafile,                       max_length)\n    ```", "```py\n    print(\"Example Pairs:\")\n    for pair in pairs[-10:]:\n        print(pair)\n    ```", "```py\n    def removeRareWords(voc, all_pairs, minimum):\n        voc.trim(minimum)\n    ```", "```py\n    pairs_to_keep = []\n    for p in all_pairs:\n        keep = True\n        for word in p[0].split(' '):\n            if word not in voc.word2index:\n                keep = False\n                break\n        for word in p[1].split(' '):\n            if word not in voc.word2index:\n                keep = False\n                break\n        if keep:\n            pairs_to_keep.append(p)\n    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".\\\n           format(len(all_pairs), len(pairs_to_keep),\n                  len(pairs_to_keep)/ len(all_pairs)))\n    return pairs_to_keep\n    minimum_count = 3\n    pairs = removeRareWords(voc, pairs, minimum_count)\n    ```", "```py\n    def indexFromSentence(voc, sentence):\n        return [voc.word2index[word] for word in\\\n                sent.split(' ')] + [EOS_token]\n    ```", "```py\n    def zeroPad(l, fillvalue=PAD_token):\n        return list(itertools.zip_longest(*l,\\\n                    fillvalue=fillvalue))\n    ```", "```py\n    def inputVar(l, voc):\n        indexes_batch = [indexFromSentence(voc, sentence)\\\n                         for sentence in l]\n        padList = zeroPad(indexes_batch)\n        padTensor = torch.LongTensor(padList)\n        lengths = torch.tensor([len(indexes) for indexes\\                            in indexes_batch])\n        return padTensor, lengths\n    ```", "```py\n    def getMask(l, value=PAD_token):\n        m = []\n        for i, seq in enumerate(l):\n            m.append([])\n            for token in seq:\n                if token == PAD_token:\n                    m[i].append(0)\n                else:\n                    m[i].append(1)\n        return m\n    ```", "```py\n    def outputVar(l, voc):\n        indexes_batch = [indexFromSentence(voc, sentence) \n                         for sentence in l]\n        max_target_len = max([len(indexes) for indexes in\n                              indexes_batch])\n        padList = zeroPad(indexes_batch)\n        mask = torch.BoolTensor(getMask(padList))\n        padTensor = torch.LongTensor(padList)\n        return padTensor, mask, max_target_len\n    ```", "```py\n    def batch2Train(voc, batch):\n        batch.sort(key=lambda x: len(x[0].split(\" \")),\\\n                   reverse=True)\n\n        input_batch = []\n        output_batch = []\n\n        for p in batch:\n            input_batch.append(p[0])\n            output_batch.append(p[1])\n\n        inp, lengths = inputVar(input_batch, voc)\n        output, mask, max_target_len = outputVar(output_                                   batch, voc)\n\n        return inp, lengths, output, mask, max_target_len\n    ```", "```py\n    test_batch_size = 5\n    batches = batch2Train(voc, [random.choice(pairs) for _\\                            in range(test_batch_size)])\n    input_variable, lengths, target_variable, mask, max_target_len = batches\n    ```", "```py\n    class EncoderRNN(nn.Module):\n        def __init__(self, hidden_size, embedding,\\\n                     n_layers=1, dropout=0):\n            super(EncoderRNN, self).__init__()\n            self.n_layers = n_layers\n            self.hidden_size = hidden_size\n            self.embedding = embedding\n    ```", "```py\n    self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                      dropout=(0 if n_layers == 1 else\\\n                               dropout), bidirectional=True)\n    ```", "```py\n    def forward(self, input_seq, input_lengths, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded,\n                                          input_lengths)\n        outputs, hidden = self.gru(packed, hidden)\n    ```", "```py\n    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n    outputs = outputs[:, :, :self.hidden_size] + a \\\n              outputs[:, : ,self.hidden_size:]\n    return outputs, hidden\n    ```", "```py\n    class Attn(nn.Module):\n        def __init__(self, hidden_size):\n            super(Attn, self).__init__()\n            self.hidden_size = hidden_size\n    ```", "```py\n    def dot_score(self, hidden, encoder_output):\n        return torch.sum(hidden * encoder_output, dim=2)\n    ```", "```py\n    def forward(self, hidden, encoder_outputs):\n        attn_energies = self.dot_score(hidden, \\\n                                       encoder_outputs)\n        attn_energies = attn_energies.t()\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n    ```", "```py\n    class DecoderRNN(nn.Module):\n        def __init__(self, embedding, hidden_size, \\\n                     output_size, n_layers=1, dropout=0.1):\n            super(DecoderRNN, self).__init__()\n            self.hidden_size = hidden_size\n            self.output_size = output_size\n            self.n_layers = n_layers\n            self.dropout = dropout\n    ```", "```py\n    self.embedding = embedding\n    self.embedding_dropout = nn.Dropout(dropout)\n    self.gru = nn.GRU(hidden_size, hidden_size, n_layers,  dropout=(0 if n_layers == 1 else dropout))\n    self.concat = nn.Linear(2 * hidden_size, hidden_size)\n    self.out = nn.Linear(hidden_size, output_size)\n    self.attn = Attn(hidden_size)\n    ```", "```py\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n    ```", "```py\n    attn_weights = self.attn(rnn_output, encoder_outputs)\n    context = attn_weights.bmm(encoder_outputs.transpose(0,\n                                                         1))\n    ```", "```py\n    rnn_output = rnn_output.squeeze(0)\n    context = context.squeeze(1)\n    concat_input = torch.cat((rnn_output, context), 1)\n    concat_output = torch.tanh(self.concat(concat_input))\n    ```", "```py\n    output = self.out(concat_output)\n    output = F.softmax(output, dim=1)\n    return output, hidden\n    ```", "```py\n    def NLLMaskLoss(inp, target, mask):\n        TotalN = mask.sum()\n        CELoss = -torch.log(torch.gather(inp, 1,\\                        target.view(-1, 1)).squeeze(1))\n        loss = CELoss.masked_select(mask).mean()\n        loss = loss.to(device)\n        return loss, TotalN.item()\n    ```", "```py\n    def train(input_variable, lengths, target_variable,\\\n              mask, max_target_len, encoder, decoder,\\\n              embedding, encoder_optimizer,\\\n              decoder_optimizer, batch_size, clip,\\\n              max_length=max_length):\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n        input_variable = input_variable.to(device)\n        lengths = lengths.to(device)\n        target_variable = target_variable.to(device)\n        mask = mask.to(device)\n        loss = 0\n        print_losses = []\n        n_totals = 0\n    ```", "```py\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n    ```", "```py\n    decoder_input = torch.LongTensor([[SOS_token for _ in \\\n                                       range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n    ```", "```py\n    use_TF = True if random.random() < teacher_forcing_ratio else False\n    ```", "```py\n    for t in range(max_target_len):\n    decoder_output, decoder_hidden = decoder(\n      decoder_input, decoder_hidden, encoder_outputs)\n    decoder_input = target_variable[t].view(1, -1)\n    mask_loss, nTotal = NLLMaskLoss(decoder_output, \\\n         target_variable[t], mask[t])\n    loss += mask_loss\n    print_losses.append(mask_loss.item() * nTotal)\n    n_totals += nTotal\n    ```", "```py\n    _, topi = decoder_output.topk(1)\n    decoder_input = torch.LongTensor([[topi[i][0] for i in \\\n                                       range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n    ```", "```py\n    loss.backward()\n    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    return sum(print_losses) / n_totals\n    ```", "```py\n    def trainIters(model_name, voc, pairs, encoder, decoder,\\\n                   encoder_optimizer, decoder_optimizer,\\\n                   embedding, encoder_n_layers, \\\n                   decoder_n_layers, save_dir, n_iteration,\\\n                   batch_size, print_every, save_every, \\\n                   clip, corpus_name, loadFilename):\n        training_batches = [batch2Train(voc,\\\n                           [random.choice(pairs) for _ in\\\n                            range(batch_size)]) for _ in\\\n                            range(n_iteration)]\n    ```", "```py\n    print('Starting ...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n    ```", "```py\n    print(\"Beginning Training...\")\n    for iteration in range(start_iteration, n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        input_variable, lengths, target_variable, mask, \\\n              max_target_len = training_batch\n        loss = train(input_variable, lengths,\\\n                     target_variable, mask, max_target_len,\\\n                     encoder, decoder, embedding, \\\n                     encoder_optimizer, decoder_optimizer,\\\n                     batch_size, clip)\n        print_loss += loss\n    ```", "```py\n    if iteration % print_every == 0:\n        print_loss_avg = print_loss / print_every\n        print(\"Iteration: {}; Percent done: {:.1f}%;\\\n        Mean loss: {:.4f}\".format(iteration,\n                              iteration / n_iteration \\\n                              * 100, print_loss_avg))\n        print_loss = 0\n    ```", "```py\n    if (iteration % save_every == 0):\n        directory = os.path.join(save_dir, model_name,\\\n                                 corpus_name, '{}-{}_{}'.\\\n                                 format(encoder_n_layers,\\\n                                 decoder_n_layers, \\\n                                 hidden_size))\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n                torch.save({\n                    'iteration': iteration,\n                    'en': encoder.state_dict(),\n                    'de': decoder.state_dict(),\n                    'en_opt': encoder_optimizer.state_dict(),\n                    'de_opt': decoder_optimizer.state_dict(),\n                    'loss': loss,\n                    'voc_dict': voc.__dict__,\n                    'embedding': embedding.state_dict()\n                }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n    ```", "```py\n    class GreedySearchDecoder(nn.Module):\n        def __init__(self, encoder, decoder):\n            super(GreedySearchDecoder, self).__init__()\n            self.encoder = encoder\n            self.decoder = decoder\n    ```", "```py\n    def forward(self, input_seq, input_length, max_length):\n        encoder_outputs, encoder_hidden = \\\n                        self.encoder(input_seq, input_length)\n        decoder_hidden = encoder_hidden[:decoder.n_layers]\n    ```", "```py\n    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n    all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n    all_scores = torch.zeros([0], device=device)\n    ```", "```py\n    for _ in range(max_length):\n        decoder_output, decoder_hidden = self.decoder\\\n            (decoder_input, decoder_hidden, encoder_outputs)\n        decoder_scores, decoder_input = \\\n             torch.max (decoder_output, dim=1)\n        all_tokens = torch.cat((all_tokens, decoder_input),\\\n                                dim=0)\n        all_scores = torch.cat((all_scores, decoder_scores),\\\n                                dim=0)\n        decoder_input = torch.unsqueeze(decoder_input, 0)\n    return all_tokens, all_scores\n    ```", "```py\n    def evaluate(encoder, decoder, searcher, voc, sentence,\\\n                 max_length=max_length):\n        indices = [indexFromSentence(voc, sentence)]\n        lengths = torch.tensor([len(indexes) for indexes \\\n                                in indices])\n        input_batch = torch.LongTensor(indices).transpose(0, 1)\n    ```", "```py\n    input_batch = input_batch.to(device)\n    lengths = lengths.to(device)\n    tokens, scores = searcher(input_batch, lengths, \\\n                              max_length)\n    decoded_words = [voc.index2word[token.item()] for \\\n                     token in tokens]\n    return decoded_words\n    ```", "```py\n    def runchatbot(encoder, decoder, searcher, voc):\n        input_sentence = ''\n        while(1):\n            try:\n                input_sentence = input('> ')\n                if input_sentence == 'quit': break\n    ```", "```py\n    input_sentence = cleanString(input_sentence)\n    output_words = evaluate(encoder, decoder, searcher,\\\n                            voc, input_sentence)\n    ```", "```py\n    output_words[:] = [x for x in output_words if \\\n                       not (x == 'EOS' or x == 'PAD')]\n    print('Response:', ' '.join(output_words))\n    ```", "```py\n    model_name = 'chatbot_model'\n    hidden_size = 500\n    encoder_n_layers = 2\n    decoder_n_layers = 2\n    dropout = 0.15\n    batch_size = 64\n    ```", "```py\n    loadFilename = None\n    checkpoint_iter = 4000\n    if loadFilename:\n        checkpoint = torch.load(loadFilename)\n        encoder_sd = checkpoint['en']\n        decoder_sd = checkpoint['de']\n        encoder_optimizer_sd = checkpoint['en_opt']\n        decoder_optimizer_sd = checkpoint['de_opt']\n        embedding_sd = checkpoint['embedding']\n        voc.__dict__ = checkpoint['voc_dict']\n    ```", "```py\n    embedding = nn.Embedding(voc.num_words, hidden_size)\n    if loadFilename:\n        embedding.load_state_dict(embedding_sd)\n    ```", "```py\n    encoder = EncoderRNN(hidden_size, embedding, \\\n                         encoder_n_layers, dropout)\n    decoder = DecoderRNN(embedding, hidden_size, \\ \n                         voc.num_words, decoder_n_layers,\n                         dropout)\n    if loadFilename:\n        encoder.load_state_dict(encoder_sd)\n        decoder.load_state_dict(decoder_sd)\n    ```", "```py\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n    print('Models built and ready to go!')\n    ```", "```py\n    save_dir = './'\n    clip = 50.0\n    teacher_forcing_ratio = 1.0\n    learning_rate = 0.0001\n    decoder_learning_ratio = 5.0\n    epochs = 4000\n    print_every = 1\n    save_every = 500\n    ```", "```py\n    encoder.train()\n    decoder.train()\n    ```", "```py\n    print('Building optimizers ...')\n    encoder_optimizer = optim.Adam(encoder.parameters(), \\\n                                   lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), \n                   lr=learning_rate * decoder_learning_ratio)\n    if loadFilename:\n        encoder_optimizer.load_state_dict(\\\n                                       encoder_optimizer_sd)\n        decoder_optimizer.load_state_dict(\\\n                                       decoder_optimizer_sd)\n    ```", "```py\n    for state in encoder_optimizer.state.values():\n        for k, v in state.items():\n            if isinstance(v, torch.Tensor):\n                state[k] = v.cuda()\n    for state in decoder_optimizer.state.values():\n        for k, v in state.items():\n            if isinstance(v, torch.Tensor):\n                state[k] = v.cuda()\n    ```", "```py\n    print(\"Starting Training!\")\n    trainIters(model_name, voc, pairs, encoder, decoder,\\\n               encoder_optimizer, decoder_optimizer, \\\n               embedding, encoder_n_layers, \\\n               decoder_n_layers, save_dir, epochs, \\\n                batch_size,print_every, save_every, \\\n                clip, corpus_name, loadFilename)\n    ```", "```py\n    encoder.eval()\n    decoder.eval()\n    ```", "```py\n    searcher = GreedySearchDecoder(encoder, decoder)\n    ```", "```py\n    runchatbot(encoder, decoder, searcher, voc)\n    ```"]