["```py\nimport gym\nimport numpy as np\nimport time, pickle, os\nenv = gym.make('FrozenLake-v0')\n```", "```py\n# Epsilon for an epsilon greedy approach\nepsilon = 0.95 \ntotal_episodes = 1000\n# Maximum number of steps to be run for every episode\nmaximum_steps = 100\nlearning_rate = 0.75\n# The discount factor\ngamma = 0.96 \n```", "```py\nQ = np.zeros((env.observation_space.n, env.action_space.n))\n```", "```py\ndef select_action(state):\n    action=0\n    if np.random.uniform(0, 1) < epsilon:\n\n        # If the random number sampled is smaller than epsilon then a random action is chosen.\n\n        action = env.action_space.sample()\n    else:\n        # If the random number sampled is greater than epsilon then we choose an action having the maximum value in the Q-table\n        action = np.argmax(Q[state, :])\n    return action\n\ndef agent_learn(state, state_next, reward, action):\n    predict = Q[state, action]\n    target = reward + gamma * np.max(Q[state_next, :])\n    Q[state, action] = Q[state, action] + learning_rate * (target - predict)\n```", "```py\nfor episode in range(total_episodes):\n    state = env.reset()\n    t = 0\n\n    while t < maximum_steps:\n        env.render()\n        action = select_action(state) \n        state_next, reward, done, info = env.step(action) \n        agent_learn(state, state_next, reward, action)\n        state = state_next\n        t += 1       \n        if done:\n            break\n        time.sleep(0.1)\n\nprint(Q)\nwith open(\"QTable_FrozenLake.pkl\", 'wb') as f:\n    pickle.dump(Q, f)\n```", "```py\nimport numpy as np\nimport gym\nfrom gym import wrappers\n```", "```py\ndef run_episode_return_reward(environment, policy, gamma_value = 1.0, render = False):\n    \"\"\" Runs an episode and return the total reward \"\"\"\n    obs = environment.reset()\n    total_reward = 0\n    step_index = 0\n    while True:\n        if render:\n            environment.render()\n        obs, reward, done , _ = environment.step(int(policy[obs]))\n        total_reward += (gamma_value ** step_index * reward)\n        step_index += 1\n        if done:\n            break\n    return total_reward\n```", "```py\ndef evaluate_policy(environment, policy, gamma_value = 1.0, n = 200):\n    model_scores = [run_episode_return_reward(environment, policy, gamma_value, False) for _ in range(n)]\n    return np.mean(model_scores)\n```", "```py\ndef extract_policy(v, gamma_value = 1.0):\n    \"\"\" Extract the policy for a given value function \"\"\"\n    policy = np.zeros(environment.nS)\n    for s in range(environment.nS):\n        q_sa = np.zeros(environment.nA)\n        for a in range(environment.nA):\n            q_sa[a] = sum([p * (r + gamma_value * v[s_]) for p, s_, r, _ in environment.P[s][a]])\n        policy[s] = np.argmax(q_sa)\n    return policy\n```", "```py\ndef compute_policy_v(environment, policy, gamma_value=1.0):\n    \"\"\" Iteratively evaluate the value-function under policy.\n    Alternatively, we could formulate a set of linear equations in terms of v[s] \n    and solve them to find the value function.\n    \"\"\"\n    v = np.zeros(environment.nS)\n    eps = 1e-10\n    while True:\n        prev_v = np.copy(v)\n        for s in range(environment.nS):\n            policy_a = policy[s]\n            v[s] = sum([p * (r + gamma_value * prev_v[s_]) for p, s_, r, _ in environment.P[s][policy_a]])\n        if (np.sum((np.fabs(prev_v - v))) <= eps):\n            # value converged\n            break\n    return v\n```", "```py\nenv_name = 'FrozenLake-v0'\nenvironment = gym.make(env_name)\noptimal_policy = policy_iteration(environment, gamma_value = 1.0)\nscores = evaluate_policy(environment, optimal_policy, gamma_value = 1.0)\nprint('Average scores = ', np.mean(scores))\n```", "```py\nimport gym\nimport numpy as np\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.distributions import Categorical\n```", "```py\nclass PolicyGradient(nn.Module):\n    def __init__(self):\n        super(PolicyGradient, self).__init__()\n\n        # Define the action space and state space\n        self.action_space = env.action_space.n\n        self.state_space = env.observation_space.shape[0]\n\n        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n        self.l2 = nn.Linear(128, self.action_space, bias=False)\n\n        self.gamma_value = gamma_value\n\n        # Episode policy and reward history \n        self.history_policy = Variable(torch.Tensor()) \n        self.reward_episode = []\n\n        # Overall reward and loss history\n        self.history_reward = []\n        self.history_loss = []\n\n    def forward(self, x): \n        model = torch.nn.Sequential(\n            self.l1,\n            nn.Dropout(p=0.5),\n            nn.ReLU(),\n            self.l2,\n            nn.Softmax(dim=-1)\n        )\n        return model(x)\n\npolicy = PolicyGradient()\noptimizer = optim.Adam(policy.parameters(), lr=l_rate)\n```", "```py\ndef choose_action(state):\n    # Run the policy model and choose an action based on the probabilities in state\n    state = torch.from_numpy(state).type(torch.FloatTensor)\n    state = policy(Variable(state))\n    c = Categorical(state)\n    action = c.sample() \n    if policy.history_policy.dim() != 0:\n        try:\n            policy.history_policy = torch.cat([policy.history_policy, c.log_prob(action)])\n        except:\n            policy.history_policy = (c.log_prob(action))\n    else:\n        policy.history_policy = (c.log_prob(action))\n    return action\n```", "```py\ndef update_policy():\n    R = 0\n    rewards = []\n\n    # Discount future rewards back to the present using gamma\n    for r in policy.reward_episode[::-1]:\n        R = r + policy.gamma_value * R\n        rewards.insert(0,R)\n\n    # Scale rewards\n    rewards = torch.FloatTensor(rewards)\n    x = np.finfo(np.float32).eps\n    x = np.array(x)\n    x = torch.from_numpy(x)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + x)\n    # Calculate the loss loss\n    loss = (torch.sum(torch.mul(policy.history_policy, Variable(rewards)).mul(-1), -1))\n\n    # Update the weights of the network\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    #Save and intialize episode history counters\n    policy.history_loss.append(loss.data[0])\n    policy.history_reward.append(np.sum(policy.reward_episode))\n    policy.history_policy = Variable(torch.Tensor())\n    policy.reward_episode= []\n```", "```py\ndef main_function(episodes):\n    running_total_reward = 50\n    for e in range(episodes):\n        # Reset the environment and record the starting state\n        state = env.reset() \n        done = False \n\n        for time in range(1000):\n            action = choose_action(state)\n            # Step through environment using chosen action\n            state, reward, done, _ = env.step(action.data.item())\n\n            # Save reward\n            policy.reward_episode.append(reward)\n            if done:\n                break\n\n        # Used to determine when the environment is solved.\n        running_total_reward = (running_total_reward * 0.99) + (time * 0.01)\n\n        update_policy()\n\n        if e % 50 == 0:\n            print('Episode number {}, Last length: {:5d}, Average length: {:.2f}'.format(e, time, running_total_reward))\n\n        if running_total_reward > env.spec.reward_threshold:\n            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_total_reward, time))\n            break\n\nepisodes = 2000\nmain_function(episodes)\n```", "```py\ntransition_type = namedtuple('transition_type',\n                        ('state', 'action', 'next_state', 'reward'))\n\nclass ExperienceReplayMemory(object):\n    def __init__(self, model_capacity):\n        self.model_capacity = model_capacity\n        self.environment_memory = []\n        self.pole_position = 0\n\n    def push(self, *args):\n        \"\"\"Saves a transition.\"\"\"\n        if len(self.environment_memory) < self.model_capacity:\n            self.environment_memory.append(None)\n        self.environment_memory[self.pole_position] = transition_type(*args)\n        self.pole_position = (self.pole_position + 1) % self.model_capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.environment_memory, batch_size)\n\n    def __len__(self):\n        return len(self.environment_memory)\n```", "```py\nclass DQNAlgorithm(nn.Module):\n\n    def __init__(self, h, w, outputs):\n        super(DQNAlgorithm, self).__init__()\n        self.conv_layer1 = nn.Conv2d(3, 8, kernel_size=5, stride=2)\n        self.batch_norm1 = nn.BatchNorm2d(8)\n        self.conv_layer2 = nn.Conv2d(8, 32, kernel_size=5, stride=2)\n        self.batch_norm2 = nn.BatchNorm2d(32)\n        self.conv_layer3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.batch_norm3 = nn.BatchNorm2d(32)\n\n        # The number of linear input connections depends on the number of conv2d layers\n        def conv2d_layer_size_out(size, kernel_size = 5, stride = 2):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n        convw = conv2d_layer_size_out(conv2d_layer_size_out(conv2d_layer_size_out(w)))\n        convh = conv2d_layer_size_out(conv2d_layer_size_out(conv2d_layer_size_out(h)))\n        linear_input_size = convw * convh * 32\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Determines next action during optimisation\n    def forward(self, x):\n        x = F.relu(self.batch_norm1(self.conv_layer1(x)))\n        x = F.relu(self.batch_norm2(self.conv_layer2(x)))\n        x = F.relu(self.batch_norm3(self.conv_layer3(x)))\n        return self.head(x.view(x.size(0), -1))\n```", "```py\nBATCH_SIZE = 128\nGAMMA_VALUE = 0.95\nEPISODE_START = 0.9\nEPISODE_END = 0.05\nEPISODE_DECAY = 200\nTARGET_UPDATE = 20\n\ninit_screen = get_screen()\ndummy_1, dummy_2, height_screen, width_screen = init_screen.shape\n\nnumber_actions = environment.action_space.n\n\npolicy_network = DQNAlgorithm(height_screen, width_screen, number_actions).to(device)\ntarget_network = DQNAlgorithm(height_screen, width_screen, number_actions).to(device)\ntarget_network.load_state_dict(policy_network.state_dict())\ntarget_network.eval()\n\noptimizer = optim.RMSprop(policy_network.parameters())\nmemory = ExperienceReplayMemory(1000)\n\nsteps_done = 0\n\ndef choose_action(state):\n    global steps_done\n    sample = random.random()\n    episode_threshold = EPISODE_END + (EPISODE_START - EPISODE_END) * \\\n        math.exp(-1\\. * steps_done / EPISODE_DECAY)\n    steps_done += 1\n    if sample > episode_threshold:\n        with torch.no_grad():\n            return policy_network(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(number_actions)]], device=device, dtype=torch.long)\n\ndurations_per_episode = []\n\ndef plot_durations():\n    plt.figure(2)\n    plt.clf()\n    durations_timestep = torch.tensor(durations_per_episode, dtype=torch.float)\n    plt.title('Training in progress...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_timestep.numpy())\n    if len(durations_timestep) >= 50:\n        mean_values = durations_per_episode.unfold(0, 100, 1).mean(1).view(-1)\n        mean_values = torch.cat((torch.zeros(99), mean_values))\n        plt.plot(mean_values.numpy())\n\n    plt.pause(0.001) \n    plt.show()\n```", "```py\ndef optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions_memory = memory.sample(BATCH_SIZE)\n    batch = Transition(*zip(*transitions_memory))\n```", "```py\n    not_final_mask = torch.tensor(tuple(map(lambda x: x is not None,\n                                          batch.next_state)), device=device, dtype=torch.uint8)\n    not_final_next_states = torch.cat([x for x in batch.next_state if x is not None])\n\n    state_b = torch.cat(batch.state)\n    action_b = torch.cat(batch.action)\n    reward_b = torch.cat(batch.reward)\n```", "```py\n    state_action_values = policy_network(state_b).gather(1, action_b)\n\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[not_final_mask] = target_net(not_final_next_states).max(1)[0].detach()\n```", "```py\n    expected_state_action_values = (next_state_values * GAMMA_VALUE) + reward_b\n```", "```py\n    hb_loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n```", "```py\n    optimizer.zero_grad()\n    hb_loss.backward()\n    for param in policy_network.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()\n\nnumber_episodes = 100\nfor i in range(number_episodes):\n    environment.reset()\n    last_screen = get_screen()\n    current_screen = get_screen()\n    current_state = current_screen - last_screen\n    for t in count():\n        # Here we both select and perform an action\n        action = choose_action(current_state)\n        _, reward, done, _ = environment.step(action.item())\n        reward = torch.tensor([reward], device=device)\n```", "```py\n        last_screen = current_screen\n        current_screen = get_screen()\n        if not done:\n            next_state = current_screen - last_screen\n        else:\n            next_state = None\n```", "```py\n        memory.push(current_state, action, next_state, reward)\n\n        # Move to the next state\n        current_state = next_state\n```", "```py\n        optimize_model()\n        if done:\n            durations_per_episode.append(t + 1)\n            plot_durations()\n            break\n```", "```py\n    if i % TARGET_UPDATE == 0:\n        target_network.load_state_dict(policy_network.state_dict())\n\nprint('Complete')\nenvironment.render()\nenvironment.close()\nplt.ioff()\nplt.show()\n```", "```py\nHistoricalAction = namedtuple('HistoricalAction', ['log_prob', 'value'])\n\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.linear = nn.Linear(4, 128)\n        self.head_action = nn.Linear(128, 2)\n        self.head_value = nn.Linear(128, 1)\n\n        self.historical_actions = []\n       self.rewards = []\n\n    def forward(self, x):\n        x = F.relu(self.linear(x))\n        scores_actions = self.head_action(x)\n        state_values = self.head_value(x)\n        return F.softmax(scores_actions, dim=-1), state_values\n```", "```py\nac_model = ActorCritic()\noptimizer = optim.Adam(ac_model.parameters(), lr=3e-2)\neps = np.finfo(np.float32).eps.item()\n```", "```py\ndef choose_action(current_state):\n    current_state = torch.from_numpy(current_state).float()\n    probabilities, state_value = ac_model(current_state)\n    m = Categorical(probabilities)\n    action = m.sample()\n    ac_model.historical_actions.append(HistoricalAction(m.log_prob(action), state_value))\n    return action.item()\n```", "```py\ndef end_episode():\n    R = 0\n    historical_actions = ac_model.historical_actions\n    losses_policy = []\n    losses_value = []\n    returns = []\n    for r in ac_model.rewards[::-1]:\n        R = r + gamma * R\n        returns.insert(0, R)\n    returns = torch.tensor(returns)\n    returns = (returns - returns.mean()) / (returns.std() + eps)\n    for (log_prob, value), R in zip(historical_actions, returns):\n        advantage = R - value.item()\n        losses_policy.append(-log_prob * advantage)\n        losses_value.append(F.smooth_l1_loss(value, torch.tensor([R])))\n    optimizer.zero_grad()\n    loss = torch.stack(losses_policy).sum() + torch.stack(losses_value).sum()\n    loss.backward()\n    optimizer.step()\n    del ac_model.rewards[:]\n    del ac_model.historical_actions[:]\n```", "```py\n    running_reward = 10\n    for i_episode in count(1):\n        current_state, ep_reward = environment.reset(), 0\n        for t in range(1, 10000): \n            action = choose_action(current_state)\n            current_state, reward, done, _ = environment.step(action)\n            if render:\n                environment.render()\n            ac_model.rewards.append(reward)\n            ep_reward += reward\n            if done:\n                break\n\n        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n        end_episode()\n        if i_episode % log_interval == 0:\n            print('Episode number {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                  i_episode, ep_reward, running_reward))\n        if running_reward > environment.spec.reward_threshold:\n            print(\"Solved! Running reward is {} and \"\n                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n            break\n```"]