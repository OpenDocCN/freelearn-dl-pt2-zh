- en: Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we built several simple networks to solve regression and classification
    problems. These illustrated the basic code structure and concepts involved in
    building ANNs with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will extend simple linear models by adding layers and using
    convolutional layers to solve nonlinear problems found in real-world examples.
    Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyper-parameters and multilayered networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a simple benchmarking function to train and test models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-parameters and multilayered networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you understand the process of building, training, and testing models,
    you will see that expanding these simple networks to increase performance is relatively
    straightforward. You will find that nearly all models we build consist, essentially,
    of the following six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import data and create iterable data-loader objects for the training and test
    sets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and instantiate a model class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate a loss class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate an optimizer class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, once we complete these steps, we will want to improve our models
    by adjusting a set of hyper-parameters and repeating the steps. It should be mentioned
    that although we generally consider hyper-parameters things that are specifically
    set by a human, the setting of these hyper-parameters can be partially automated,
    as we shall see in the case of the learning rate. Here are the most common hyper-parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate of gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of epochs to run the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of nonlinear activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The depth of the network, that is, the number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The width of the network, that is, the number of neurons in each layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The connectivity of the network (for example, convolutional networks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already worked with some of these hyper-parameters. We know the learning
    rate, if set too small, will take more time than necessary to find the optimum,
    and if set too large, will overshoot and behave erratically. The number of epochs
    is the number of complete passes over the training set. We would expect that as
    we increase the number of epochs, the accuracy will improve on each epoch, given
    limitations on the dataset and the algorithm used. At some point, the accuracy
    will plateau and training over more epochs is a waste of resources. If the accuracy
    decreases over the first few epochs, one of the most likely causes is that the
    learning rate is set too high.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions play a critical role in classification tasks and the effect
    of different types of activation can be somewhat subtle. It is generally agreed
    that the ReLU, or rectified linear function, performs best on the most common
    practice datasets. This is not to say that other activation functions, particularly
    the hyperbolic tangent or tanh function and variations on these, such as leaky
    ReLU, can produce better results under certain conditions.
  prefs: []
  type: TYPE_NORMAL
- en: As we increase the depth, or number of layers, we increase the learning power
    of the network, enabling it to capture more complex features of the training set.
    Obviously this increased ability is very much dependant on the size and complexity
    of the dataset and the task. With small datasets and relatively simple tasks,
    such as digit classification with MNIST, a very small number of layers (one or
    two) can give excellent results. Too many layers waste resources and tend to make
    the network overfit or behave erratically.
  prefs: []
  type: TYPE_NORMAL
- en: Much of this is true when we come to increasing the width, that is, the number
    of units in each layer. Increasing the width of a linear network is one of the
    most the most efficient ways to boost learning power. When it comes to convolutional
    networks, as we will see, not every unit is connected to every unit in the next
    forward layer; connectivity, that is, the number of input and output channels
    in each layer, is critical. We will look at convolutional networks shortly, but
    first we need to develop a framework to test and evaluate our models.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Benchmarking and evaluation are core to the success of any deep learning exploration.
    We will develop some simple code to evaluate two key performance measures: the
    accuracy and the training time. We will use the following model template:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/790342c4-4149-482a-9d14-6ef55cd08597.png)'
  prefs: []
  type: TYPE_IMG
- en: This model is the most common and basic linear template for solving MNIST. You
    can see we initialize each layer, in the`**init** `method, by creating a class
    variable that is assigned to a PyTorch `nn` object. Here, we initialize two linear
    functions and a ReLU function. The `nn.Linear` function takes an input size of
    `28*28` or `784`. This is the size of each of the training images. The output
    channels or the width of the network are set to `100`. This can be set to anything,
    and in general a higher number will give better performance within the constraints
    of computing resources and the tendency for wider networks to overfit training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward `method, we create an `out` variable. You can see that the out
    variable is passed through an ordered sequence consisting of a linear function,
    a ReLU function, and another linear function before being returned. This is a
    fairly typical network architecture, consisting of alternating linear and nonlinear
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now create two more models, replacing the ReLU function with the tanh
    and sigmoid activation functions. Here is the tanh version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a95d92db-69da-4560-97c7-dde99e4ed630.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see we simply changed the name and replaced the `nn.RelU()` function
    with `nn.Tanh()`. Create a third model in exactly the same way, replacing `nn.Tanh()`
    with `nn.Sigmoid()`. Don't forget to change the name in the super constructor
    and in the variable used to instantiate the model. Also remember to change the
    forward function accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a simple `benchmark` function that we can use to run and
    record the accuracy and training time of each of these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8af116f-59a1-4f5e-89e3-a7ac588c5788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hopefully, this is fairly self-explanatory. The `benchmark` function takes
    two required parameters: the data and the model to be evaluated. We set default
    values for epochs and the learning rate. We need to initialize the model so we
    can run it more than once at a time on the same model, otherwise the model parameters
    will accumulate, distorting our results. The running code is identical to the
    code used for the previous models. Finally, we print out the accuracy and the
    time taken to train. The training time calculated here is really only an approximate
    measure, since training time will be affected by whatever else is going on on
    in the processor, the amount of memory, and other factors beyond our control.
    We should only use this result as a relative indication of a model''s time performance.
    Finally, we need a function to calculate the accuracy, and this is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d96b16b-388a-492e-be3c-4b51dff43beb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember to load the training and test datasets and make them iterable exactly
    as we did before. Now, we can run our three models and compare them using something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e57882e-d618-47be-9393-b92872fbf2db.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that both the `Tanh` and `ReLU` functions perform significantly better
    than `sigmoid`. For most networks, the `ReLU activation` function on hidden layers
    give the best results, both in terms of accuracy and the time it takes to train.
    The `ReLU` activation is not used on output layers. For the output layers, since
    we need to calculate the loss, we use the `softmax` function. This is the criterion
    for the loss class and, as before, we use `CrossEntropy Loss()`, which, if you
    remember, includes the `softmax` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways we can improve from here; one obvious way is simply
    to add more layers. This is typically done by adding alternating pairs of nonlinear
    and linear layers. In the following, we use `nn.Sequential` to organize our layers.
    In our forward layer, we simply have to call sequential objects, rather than every
    individual layer and function. This makes our code more compact and easier to
    read:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe2948dd-ad5b-46a7-9781-4144cdacec83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we add two more layers: a linear layer and a nonlinear `ReLU` layer.
    It is particularly important how we set the input and output sizes. In the first
    linear layer, the input size is `784`, this is the image size. The output of this
    layer, something we choose, is set to `100`. The input to the second linear layer,
    therefore, must be `100`. This is the width, the number of kernels and feature
    maps, of the output. The output of the second linear layer is something we choose,
    but the general idea is to decrease the size, since we are trying to filter down
    the features to just `10`, our target classes. For fun, create some models and
    try out different input and output sizes, remembering that the input to any layer
    must be the same size as the output of the previous layer. The following is the
    output of three models, where we print the output sizes of each of the hidden
    layers to give you an idea of what is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/045fc1ee-c6ba-4054-a71c-3ea8e16b6f03.png)'
  prefs: []
  type: TYPE_IMG
- en: We can continue to add as many layers and kernels as we desire, however this
    is not always a good idea. How we set up input and output sizes in a network is
    intimately connected to the size, shape, and complexity of the data. For simple
    datasets, such as MNIST, it is pretty clear that a few linear layers gets very
    good results. At some point, simply adding linear layers, and increasing the number
    of kernels will not capture the highly nonlinear features of complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have used fully connected layers in our networks, where each input
    unit represents a pixel in an image. With convolutional networks, on the other
    hand, each input unit is assigned a small localized **receptive field**. The idea
    of the receptive field, like ANNs themselves, is modelled on the human brain.
    In 1958, it was discovered that neurons in the visual cortex of the brain respond
    to stimuli in a limited region of a field of vision. More intriguing is that sets
    of neurons respond exclusively to certain basic shapes. For example, a set of
    neurons may respond to horizontal lines, while others respond only to lines at
    other orientations. It was observed that sets of neurons could have the same receptive
    field, but respond to different shapes. It was also noticed that neurons were
    organized into layers with deeper layers responding to more complex patterns.
    This, it turns out, is a remarkably effective way for a computer to learn and
    categorize a set of images.
  prefs: []
  type: TYPE_NORMAL
- en: A single convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional layers are organized so the units in the first layer only respond
    to their respective receptive fields. Each unit in the next layer is connected
    only to a small region of the first layer, and each unit in the second hidden
    layer is connected to a limited region in the third layer, and so on. In this
    way, the network can be trained to assemble higher level features from the low-level
    features present in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, this works by using a **filter**, or **convolution kernel**, to
    scan an image to generate what is known as a **feature map**. The kernel is just
    a matrix that is the size of the receptive field. We can think of this as a camera
    scanning an image in discrete strides. We calculate a feature map matrix by an
    element-wise multiplication of the kernel matrix with the values in the receptive
    field of an image. The resultant matrix is then summed to compute a single number
    in the feature map. The values in the kernel matrix represent a feature we want
    to extract from the image. These are the parameters that we ultimately want the
    model to learn. Consider a simple example where we are attempting to detect horizontal
    and vertical lines in an image. To simplify things, we will use one input dimension;
    this is either black, represented by a **1**, or white, represented by a **0**.
    Remember that in practice these would be scaled and normalized floating-point
    numbers representing a grayscale or color value. Here, we set the kernel to 4
    x 4 pixels and we scan using a stride of **1**. A stride is simply the distance
    we move the kernel, so a stride of **1** moves the kernel one pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2284909-cad9-45c5-bed5-d11069caca26.png)'
  prefs: []
  type: TYPE_IMG
- en: One convolution is one complete scan of the image and each convolution generates
    a feature map. On each stride, we do an element-wise multiplication of the kernel
    with the receptive field of the image and we sum the resulting matrix.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that as we move the kernel across the image, as shown in the
    preceding diagram, **stride 1** samples the top-left corner, **stride 2** samples
    the patch-one pixel to the left, **stride** **3** would sample one pixel to the
    left again, and so on. When we reach the end of the first row, we need to add
    a padding pixel, so set the value to **0** in order to sample the edges of the
    image. Padding input data with zeros is called **valid padding**. If we did not
    pad the image, the feature map would be smaller, in dimensions, than the original
    image. Padding is used to ensure that there is no loss of information from the
    original.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand the relationship between input and output sizes,
    kernel size, padding, and stride. They can be expressed quite neatly in the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/401dc8f1-0bee-460b-bc5d-f3e1698a6599.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *O* *=* output size, *W* = input height or width, *K* = kernel size, *P*
    = padding, and *S* = stride. Note that the input height, or width, assumes these
    two are the same—that is, the input image is square, not rectangular. If the input
    image is a rectangle, we need to calculate output values separately for the width
    and height.
  prefs: []
  type: TYPE_NORMAL
- en: 'The padding can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c458dcce-9a0c-4160-a232-eef73b5344a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In each convolution, we can include multiple kernels. Each kernel in a convolution
    generates its own feature map. The number of kernels is the number of output channels,
    which is also the number of feature maps generated by the convolutional layer.
    We can generate further feature maps by using another kernel. As an exercise,
    calculate the feature map that would be generated by the following kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e09d64cd-e011-4778-b7a6-8e366bf3ea6d.png)'
  prefs: []
  type: TYPE_IMG
- en: By stacking kernels, or filters, and using kennels of different sizes and values,
    we can extract a variety of features from an image.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that each kernel is not restricted to one input dimension. For
    example, if we are processing an RGB color image, each kernel would have an input
    dimension of three. Since we are doing element-wise multiplication, the kernel
    must be the same size as the receptive field. When we have three dimensions, the
    kernel needs to have an input depth of three. So our greyscale 2 x 2 kernel becomes
    a 2 x 2 x 3 matrix for a color image. We still generate a single feature map on
    each convolution for each kernel. We are still able to do element-wise multiplication,
    since the kernel size is the same as the receptive field, except now when we do
    the summation, we sum across the three dimensions to get the single number required
    on each stride.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, there are a large number of ways we can scan an image. We
    can change the size and value of the kernel, or we can change its stride, include
    padding and even include noncontiguous pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better idea of some of the possibilities, check out vdumoulin''s excellent
    animations: [https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with the fully connected linear layers, we can add multiple convolutional
    layers. As with linear layers, the same restrictions apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations on time and memory (computational load)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tendency to overfit a training set and not generalize to a test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires larger datasets to work effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefit of the appropriate addition of convolution layers is that, progressively,
    they are able to extract more complex, nonlinear features from datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional layers are typically stacked using **pooling layers**. The purpose
    of a pooling layer is to reduce the size, but not the depth, of the feature map
    generated by the preceding convolution. A pooling layer retains the RGB information
    but compresses the spatial information. The reason we do this is to enable kernels
    to focus selectively on certain nonlinear features. This means we can reduce the
    computational load by focusing on the parameters that have the strongest influence.
    Having fewer parameters also reduces the tendency to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three major reasons why pool layers are used to reduce dimensions
    of the output feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduces computational load by discarding irrelevant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smaller number of parameters, so less likely to overfit data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to extract features that are transformed in some way, for example images
    of an object from different perspectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers are very similar to normal convolution layers in that they use
    a kernel matrix, or filter, to sample an image. The difference with pooling layers
    is that we downsample the input. Downsampling reduces the input dimensions. This
    can be achieved by either increasing the size of the kernel or increasing the
    stride, or both. Check the formula in the section on single convolutional layers
    to confirm this is true.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in a convolution all we are doing is multiplying two tensors on each
    stride, over an image. Each subsequent stride in a convolution samples another
    part of the input. This sampling is achieved by element-wise multiplication of
    the kernel with the output of the previous convolution layer, encompassed by that
    particular stride. The result of this sampling is a single number. With a convolution
    layer, this single number is the sum of the element-wise multiplication. With
    a pooling layer, this single number is typically generated by either the average
    or the maximum of the element-wise multiplication. The terms **average pooling **and **max **pooling
    refer to these different pooling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Building a single-layer CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So now we should have enough theory to build a simple convolution network and
    understand how it works. Here is a model class template we can start with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daea6d68-3442-411a-864b-145fb4f88e86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic convolutional unit we will be using is in PyTorch is the `nn.Conv2d`
    module. It is characterized by the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The values of these parameters are constrained by the size of the input data
    and the formulae discussed in the last section. In this example, `in_channels`
    is set to `1`. This refers to the fact that our input image has one color dimension.
    If we were working with a three-channel color image, this would be set to `3`.
    `out_channel` is the number of kernels. We can set this to anything, but remember
    there are computational penalties, and improved performance is dependant on having
    larger, more complex datasets. For this example, we set the number of output channels
    to `16`. The number of output channels, or kernels, is essentially the number
    of low-level features we think might be indicative of the target class. We set
    the stride to `1` and the padding to `2`. This ensures the output size remains
    the same as the input; this can be verified by plugging these values into the
    output formula in the section on single convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` method, you will notice we instantiate a convolutional layer,
    a `ReLU` activation function, a `MaxPool2d` layer, and a fully connected linear
    layer. The important thing here is to understand how we derive the values we pass
    to the `nn.Linear()` function. This is the output size of the MaxPool layer. We
    can calculate this using our output formula. We know that the output from the
    convolutional layer is the same as the input. Because the input image is square,
    we can use 28, the height or width, to represent the input, and consequently the
    output size of the convolutional layer. We also know that we have set a kernel
    size of `2`. By default, `MaxPool2d` assigns the stride to the kernel size and
    uses implied padding. For practical purposes, this means that when we use default
    values for stride and padding, we can simply divide the input, here 28, by the
    kernel size. Since our kernel size is 2, we can calculate an output size of 14\.
    Since we are using a fully connected linear layer, we need to flatten the width,
    height, and the number of channels. We have 32 channels, as set in the `out_channels`
    parameter of `nn.Conv2d`. Therefore, the input size is 16 X 14 X 14\. The output
    size is 10 because, as with the linear networks, we use the output to distinguish
    between the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` function of the model is fairly straightforward. We simply pass
    the `out` variable through the convolutional layer, the activation function, the
    pooling layer, and the fully connected linear layer. Notice that we need to resize
    the input for the linear layer. Assuming the batch size is `100`, the output of
    the pooling layer is a four-dimensional tensor: `100, 32, 14, 14`. Here, `out.view(out.size(0),
    -1)` reshapes this four-dimensional tensor to a two-dimensional tensor: `100,
    32*14*14`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this a little more concrete, let''s train our model and look at a few
    variables. We can use almost identical code to train the convolutional model.
    We do, however, need to change one line in our `benchmark()` function. Since convolution
    layers can accept multiple input dimensions, we do not need to flatten the width
    and height of the input. For the previous linear models, in our running code,
    we used the following to flatten the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For our convolutional layer, we do not need to do this; we can simply pass
    the model the image, as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This line must also be changed in the `accuracy()` function we defined in the
    section on bench marking earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building a multiple-layer CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you would expect, we can improve this result by adding another convolutional
    layer. When we are adding multiple layers, it is convenient to bundle each layer
    into a sequence. It is here that `nn.Sequential` comes in handy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f49fd6a-83f1-471a-939b-0ada448115f9.png)'
  prefs: []
  type: TYPE_IMG
- en: We initialize two hidden layers and a fully connected linear output layer. Note
    the parameters passed to the `Conv2d` instances and the linear output. As before,
    we have one input dimension. From this, our convolutional layer outputs `16` feature
    maps or output channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram represents the two-layered convolutional network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d137b335-9748-4a3d-8627-bf7b083c8ee9.png)'
  prefs: []
  type: TYPE_IMG
- en: This should make it clear how we calculate the output sizes, and in particular
    how we derive the input size for the linear output layer. We know, using the output
    formula, that the output size of the first convolutional layer, before max pooling,
    is the same as the input size, that is 28 x 28\. Since we are using 16 kernels
    or channels, generating 16 feature maps, the input to the max pooling layer is
    a 16 x 28 x 28 tensor. The max pooling layer, with a kernel size of 2, a stride
    of 2, and the default implicit padding means that we simply divide the feature
    map size by 2 to calculate the max pool out put size. This gives us an output
    size of 16 x 14 x 14\. This is the input size to the second convolutional layer.
    Once again, using the output formula, we can calculate that the second convolutional
    layer, before max pooling, generates 14 x 14 feature maps, the same size as its
    input. Since we set the number of kernels to 32, the input to the second max pooling
    layer is a 32 x 14 x 14 matrix. Our second max pooling layer is identical to the
    first, with the kernel size and stride set to 2 and default implicit padding.
    Once again, we can simply divide by 2 to calculate the output size, and therefore
    the input to the linear output layer. Finally, we need to flatten this matrix
    to one dimension. So the input size for the linear output layer is a single dimension
    of 32 * 7 * 7, or 1,568\. As usual, we need the output size of the final linear
    layer to be the number of classes, which in this case is 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inspect the model parameters to see that is exactly what is happening
    when we run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eaf08c21-d999-4be8-9a99-a0efaaa5fa74.png)'
  prefs: []
  type: TYPE_IMG
- en: The model parameters consist of six tensors. The first tensor is the parameter
    for the first convolution layer. It consists of `16` kernels, `1` color dimension,
    and a kernel of size `5`. The next tensor is the bias and has a single dimension
    of size `16`. The third tensor in the list is the `32` kernels in the second convolutional
    layer, the `16` input channels, the depth, and the `5 x 5` kernel. In the final
    linear layer, we flattened these dimensions to `10 x 1568`.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch normalization is used widely to improve the performance of neural networks.
    It works by stabilizing the distributions of layer input. This is achieved by
    adjusting the mean and variance of these input. It is fairly indicative of the
    nature of deep learning research that there is uncertainty among the researcher
    community as to why batch normalization is so effective. It was thought that this
    was because it reduces the so called **internal co-variate shift** (**ICS**).
    This refers to the change in distributions as a result of the preceding layers'
    parameter updates. The original motivation for batch normalization was to reduce
    this shift. However, a clear link between ICS and performance has not been conclusively
    found. More recent research has shown that batch normalization works by *smoothing*
    the optimization landscape. Basically, this means that gradient descent will work
    more efficiently. Details of this can be found in *How Does Batch Normalization
    Help Optimization*? by Santurkar et al., which is available at [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604).
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization, implemented with the `nn.BatchNorm2d` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dbe5c56-c922-4935-9031-fd924e20e755.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This model is identical to the previous two-layer CNN with the addition of
    the batch normalization of the output of the convolutional layers. The following
    is a printout of the performance of the three convolutional networks we have built
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f02eb2a-5cce-4b0a-9699-23bbb2857159.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how we could improve the simple linear network developed
    in [Chapter 3](77e1b6da-e5d6-46a4-8a2c-ee1cfa686cc6.xhtml), *Computational Graphs
    and Linear Models*. We can add linear layers, increase the width of the network,
    increase the number of epochs we run the model, and tweak the learning rate. However,
    linear networks will not be able to capture the nonlinear features of datasets,
    and at some point their performance will plateau. Convolutional layers, on the
    other hand, use a kernel to learn nonlinear features. We saw that with two convolutional
    layers, performance on MNIST improved significantly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at some different network architectures, including
    recurrent networks and long short-term networks.
  prefs: []
  type: TYPE_NORMAL
