["```py\nconda create --name bookenv python=3.8\n```", "```py\nconda info --envs\n```", "```py\nconda activate bookenv\n```", "```py\nconda deactivate\n```", "```py\npip install numpy\nconda install pytorch torchvision torchaudio \\\ncudatoolkit=<cuda version> -c pytorch -c nvidia\n```", "```py\npip install -r requirements.txt\n```", "```py\nconda create -n env_1\nconda activate env_1\n# save environment to a file\nconda env export > env.yml\n# clone existing environment \nconda create -n env_2 --clone env_1\n# delete existing environment (env_1)\nconda remove -n env_1 --all\n# create environment (env_1) from the yaml file\nconda env create -f env.yml\n# using conda to install the libraries from requirements.txt\nconda install --force-reinstall -y -q --name py37 -c conda-forge --file requirements.txt\n```", "```py\n# env.yml\nname: env_1\nchannels:\n  - defaults\ndependencies:\n  - appnope=0.1.2=py39hecd8cb5_1001\n  - ipykernel=6.4.1=py39hecd8cb5_1\n  - ipython=7.29.0=py39h01d92e1_0\nprefix: /Users/userA/opt/anaconda3/envs/new_env\n```", "```py\n# url points to the target google scholar page \nresponse = requests.get(url) \nhtml_soup = BeautifulSoup(response.text, 'html.parser')\n```", "```py\n{\n    \"first_name\": \"Ryan\",\n    \"last_name\": \"Smith\",\n    \"phone\": [{\"type\": \"home\",\n               \"number\": \"111 222-3456\"}],\n    \"pets\": [\"ceasor\", \"rocky\"],\n    \"job_location\": null\n}\n```", "```py\nimport pandas as pd\nfrom tabulate import tabulate\nin_file = \"../csv_data/data/cdc-moderna-covid-19-vaccine-distribution-by-state.csv\"\n# read the CSV file and store the returned dataframe to a variable \"df_vacc\"\ndf_vacc = pd.read_csv(in_file)\nprint(tabulate(df_vacc.head(5), headers=\"keys\", tablefmt=\"psql\"))\n```", "```py\ndf = pd.read_csv(in_file)\n# Fill out the empty \"affiliation\" with \"na\"\ndf.affiliation.fillna(value=\"na\", inplace=True)\n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport traceback\nfrom nltk.corpus import stopwords\n# download nltk corpuses\nnltk.download('punkt')\nnltk.download('stopwords')\n# create a set of stop words\nstop_words = set(stopwords.words('english'))\n# read each line in dataframe (i.e., each line of input file)\nfor index, row in df.iterrows():\n   curr_research_interest = str(row['research_interest'])\\\n       .replace(\"##\", \" \")\\\n       .replace(\"_\", \" \")\n   # tokenize text data.\n   curr_res_int_tok = tokenize(curr_research_interest)\n   # remove stop words from the word tokens\n   curr_filtered_research = [w for w in curr_res_int_tok\\\n                             if not w.lower() in stop_words]\n```", "```py\ndef clean_text(in_str):\n   clean_txt = re.sub(r'\\W+', ' ', in_str)\n   return clean_txt\n# remove non alpha-numeric characters for feature \"text\"\ntext = clean_text(text)\n```", "```py\n# replace the new line in the given text with empty string. \ntext = input_text.replace(\"\\n\", \"\")\n```", "```py\n# dictionary mapping the values are commonly used for normalization\ndict_norm = {\"data_science\": \"artificial_intelligence\",\n    \"machine_learning\": \"artificial_intelligence\"}\n# normalize.py\nif curr in dict_norm:\n   return dict_norm[curr]\nelse:\n   return curr\n```", "```py\n# Step 1: calculate state-wise mean number for weekly corora vaccine distribution\ndf = df_in.groupby(\"jurisdiction\")[\"_1st_dose_allocations\"]\\\n   .mean().to_frame(\"mean_vaccine_count\").reset_index()\n# Step 2: calculate normalized mean vaccine count\ndf[\"norm_vaccine_count\"] = df[\"mean_vaccine_count\"] / df[\"mean_vaccine_count\"].max()\n```", "```py\n# word tokenize\ncurr_resh_int_tok = word_tokenize(curr_research_interest)\n# remove stop words from the word tokens\ncurr_filtered_research = [w for w in curr_res_int_tok\\\n                         if not w.lower() in stop_words]\n```", "```py\ndef convert_lowercase(in_str):\n   return str(in_str).lower()\n# convert string to lowercase\ntext = convert_lowercase(text)\n```", "```py\nfrom nltk.stem import PorterStemmer\n# porter stemmer for stemming word tokens\nps = PorterStemmer()\nword = \"information\"\nstemmed_word = ps.stem(word) // \"inform\"\n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ndocument_1 = \"This is a great place to do holiday shopping\"\ndocument_2 = \"This is a good place to eat food\"\ndocument_3 = \"One of the best place to relax is home\"\n# 1-gram (i.e., single word token used for BoW creation)\ncount_vector = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n# transform the sentences\ncount_fit = count_vector.fit_transform([document_1, document_2, document_3])\n# create dataframe\ndf = pd.DataFrame(count_fit.toarray(), columns=count_vector.get_feature_names_out())\nprint(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n```", "```py\ntfidf_vectorizer = TfidfVectorizer(use_idf=True)\n# use the tf-idf instance to fit list of research_interest \ntfidf = tfidf_vectorizer.fit_transform(research_interest_list)\n# tfidf[0].T.todense() provides the tf-idf dense vector \n# calculated for the research_interest\ndf = pd.DataFrame(tfidf[0].T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"tf-idf\"])\n# sort the tf-idf calculated using 'sort_values' of dataframe.\ndf = df.sort_values('tf-idf', ascending=False)\n# top 3 words with highest tf-idf\nprint(df.head(3))\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nencoded_data = labelencoder.fit_transform(df_research ['is_artifical_intelligent'])\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nencoded_data = labelencoder.fit_transform(df_research ['research_interest'])\n```", "```py\nimage = cv2.imread('./images/tiger.jpg')\n# filter to convert color tiger image to gray one\ngray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n# write the gray image to a file\ncv2.imwrite('./images/tiger_gray.jpg', gray_image)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, \n# read the HR data in csv format\ndf_features = pd.read_csv(\"./HR.csv\")\n# Step 1: Standardize features by removing the mean and scaling to unit variance\nscaler = StandardScaler()\n# train = scaler.fit(X)\nX_std = scaler.fit_transform(X)\n# Step 2: Instantiate PCA & choose minimum number of \n# components such that it covers 95% variance\npca = PCA(0.95).fit(X_std)\n```", "```py\nfrom fuzzywuzzy import fuzz\n# compare strings using ratio method\nfuzz.ratio(\"this is a test\", \"this is a test!\") // 91\nfuzz.ratio(\"this is a test!\", \"this is a test!\") // 100\n```", "```py\n# PIE CHART PLOTTING\n# colors for pie chart\ncolors = ['orange', 'green', 'cyan', 'skyblue', 'yellow', 'red', 'blue', 'white', 'black', 'pink']\n# pie chart plot\nplt.pie(list(dict_top10.values()), labels=dict_top10.keys(), colors=colors, autopct='%2.1f%%', shadow=True, startangle=90)\n# show the actual plot\nplt.show()\n# BAR CHART PLOTTING\nx_states = dict_top10.keys()\ny_vaccine_dist_1 = dict_top10.values()\nfig = plt.figure(figsize=(12, 6))  # figure chart with size\nax = fig.add_subplot(111)\n# bar values filling with x-axis/y-axis values\nax.bar(np.arange(len(x_states)), y_vaccine_dist_1, log=1)\nplt.show()\n```", "```py\nimport seaborn as sns \n# top 10 stats by largest mean\ndf_mean_sorted_top10 = ... # top 10 stats by largest mean\n# LINE CHART PLOT\nsns.lineplot(data=df_mean_sorted_top10, x=\"state_names\", y=\"count_vaccine\")\n# show the actual plot\nplt.show()\n# HISTOGRAM CHART PLOTTING\n# plot histogram bars with top 10 states mean distribution count of vaccine\nsns.displot(df_mean_sorted_top10['count_vaccine'], kde=False)\nplt.show()\n```"]