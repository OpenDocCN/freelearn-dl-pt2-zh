- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Experiment Tracking, Model Management, and Dataset Versioning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验跟踪，模型管理和数据集版本控制
- en: In this chapter, we will introduce a set of useful tools for experiment tracking,
    model management, and dataset versioning, which allows you to effectively manage
    **deep learning** (**DL**) projects. The tools we will be discussing in this chapter
    can help us track many experiments and interpret the results more efficiently,
    which naturally leads to a reduction in operational costs and boosts the development
    cycle. By the end of the chapter, you will have hands-on experience with the most
    popular tools and be able to select the right set of tools for your project.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一组用于实验跟踪、模型管理和数据集版本控制的实用工具，这些工具可以帮助您有效管理深度学习（DL）项目。本章讨论的工具可以帮助我们跟踪许多实验并更高效地解释结果，从而自然而然地减少运营成本并加速开发周期。通过本章，您将能够亲自体验最流行的工具，并能够为您的项目选择适当的工具集。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Overview of DL project tracking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL项目跟踪概述
- en: DL project tracking with Weights & Biases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Weights & Biases进行DL项目跟踪
- en: DL project tracking with MLflow and DVC
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLflow和DVC进行DL项目跟踪
- en: Dataset versioning – beyond Weights & Biases, MLflow, and DVC
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集版本控制 – 超越Weights & Biases, MLflow和DVC
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can download the supplemental material for this chapter from this book’s
    GitHub repository at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的GitHub仓库下载本章的补充材料，网址为[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4)。
- en: Overview of DL project tracking
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL项目跟踪概述
- en: Training DL models is an iterative process that consumes a lot of time and resources.
    Therefore, keeping track of all experiments and consistently organizing them can
    prevent us from wasting our time on unnecessary operations such as training similar
    models repeatedly on the same set of data. In other words, having well-documented
    records of all model architectures and their hyperparameter sets, as well as the
    version of data used during experiments, can help us derive the right conclusion
    from the experiments, which naturally leads to the project being successful.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 训练DL模型是一个消耗大量时间和资源的迭代过程。因此，跟踪所有实验并始终有条理地组织它们可以防止我们浪费时间在无需重复训练相似模型的操作上。换句话说，有关所有模型架构及其超参数集合以及实验过程中使用的数据版本的详细记录可以帮助我们从实验中得出正确的结论，这自然而然地有助于项目的成功。
- en: Components of DL project tracking
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DL项目跟踪的组成部分
- en: The essential components of **DL project tracking** are **experiment tracking**,
    **model management**, and **dataset versioning**. Let’s look at each component
    in detail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DL项目跟踪的基本组成部分包括实验跟踪，模型管理和数据集版本控制。让我们详细看看每个组件。
- en: Experiment tracking
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验跟踪
- en: 'The concept behind experiment tracking is simple: store the description and
    the motivations of each experiment so that we don’t run another set of experiments
    for the same purpose. Overall, effective experiment tracking will save us operational
    costs and allows us to derive the right conclusion from a minimal set of experimental
    results. One of the basic approaches for effective experiment tracking is adding
    a unique identifier to each experiment. The information we need to track for each
    experiment includes project dependencies, the definition of the model architecture,
    parameters used, and evaluation metrics. Experiment tracking also includes visualizing
    ongoing experiments in real time and being able to compare a set of experiments
    intuitively. For example, if we can check train and validation losses from every
    epoch as the model gets trained, we can identify overfitting quicker, saving some
    resources. Also, by comparing results and a set of changes made between two experiments,
    we can understand how the changes affect the model performance.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Model management
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model management goes beyond experiment tracking as it covers the full life
    cycle of a model: dataset information, artifacts (any data generated from training
    a model), the implementation of the model, evaluation metrics, and pipeline information
    (such as development, testing, staging, and production). Model management allows
    us to quickly pick up the model of interest and efficiently set up the environment
    in which the model can be used.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Dataset versioning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last component of DL project tracking is dataset versioning. In many projects,
    datasets change over time. Changes can come from data schemas (blueprints of how
    the data is organized), file locations, or even from filters applied to the dataset
    manipulating the meaning of the underlying data. Many datasets found in the industry
    are structured in a complex way and often stored in multiple locations in various
    data formats. Therefore, changes can be more dramatic and harder to track than
    you anticipated. As a result, keeping a record of the changes is critical in reproducing
    consistent results throughout the project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset tracking can be summarized as follows: a set of data stored as an artifact
    should become a new version of the artifact whenever the underlying data is modified.
    Having said that, every artifact should have metadata that consists of important
    information about the dataset: when it is created, who created it, and how it
    is different from the previous version.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a dataset with dataset versioning should be formulated as follows.
    The dataset should have a timestamp in its name:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As mentioned previously, the metadata should contain key information about
    the dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note that the set of information that’s tracked by metadata may be different
    for each project.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Tools for DL project tracking
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL tracking can be achieved in various ways, starting from simple notes in a
    text file, through spreadsheets, keeping the information in GitHub or dedicated
    web pages, to self-built platforms and external tools. Model and data artifacts
    can be stored as is, or more sophisticated methods can be applied to avoid redundancy
    and increase efficiency.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DL跟踪可以通过多种方式实现，从简单的文本文件中的注释、电子表格，到在GitHub或专用网页上保存信息，再到自建平台和外部工具。模型和数据工件可以按原样存储，也可以应用更复杂的方法以避免冗余并提高效率。
- en: The field of DL project tracking is growing fast and is introducing new tools
    continuously. As a result, selecting the right tool for the underlying project
    is not an easy task. We must consider both business and technical constraints.
    While the pricing model is a basic one, the other constraints can possibly be
    introduced by the existing development settings; integrating the existing tools
    should be easy, and the infrastructure must be easy to maintain. It is also important
    to consider the engineering competence of the MLOps team. Having said that, the
    following list would be a good starting point when you’re selecting a tool for
    your project.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DL项目跟踪领域正在快速增长，并不断引入新工具。因此，为底层项目选择合适的工具并不容易。我们必须考虑业务和技术限制。尽管定价模型是基本的，但其他限制可能由现有的开发设置引入；集成现有工具应该很容易，基础设施必须易于维护。还要考虑MLOps团队的工程能力。话虽如此，在选择项目工具时，以下列表将是一个很好的起点。
- en: 'TensorBoard ([https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)):'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard ([https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard))：
- en: An open source visualization tool developed by the TensorFlow team
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow团队开发的一款开源可视化工具。
- en: A standard tool for tracking and visualizing the experimental results
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于跟踪和可视化实验结果的标准工具。
- en: 'Weights & Biases ([https://wandb.ai](https://wandb.ai/site/experiment-tracking)):'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weights & Biases ([https://wandb.ai](https://wandb.ai/site/experiment-tracking))：
- en: A cloud-based service with an effective and interactive dashboard for visualizing
    and organizing the experimental results
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一款云端服务，提供有效的交互式仪表板，用于可视化和组织实验结果。
- en: The server can be run locally or hosted in a private cloud
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器可以在本地运行，也可以托管在私有云中。
- en: It provides an automated hyperparameter-tuning feature called Sweeps
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供名为Sweeps的自动化超参数调整功能。
- en: Free for personal projects. Pricing is based on the tracking hours and storage
    space
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人项目免费。定价基于跟踪小时数和存储空间。
- en: 'Neptune ([https://neptune.ai](https://neptune.ai/product)):'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neptune ([https://neptune.ai](https://neptune.ai/product))：
- en: An online tool for monitoring and storing the artifacts from machine learning
    (ML) experiments
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监视和存储机器学习（ML）实验结果的在线工具。
- en: It can easily be integrated with the other ML tools
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可轻松集成其他ML工具。
- en: It’s known for its powerful dashboard which summarizes the experiments in real
    time
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以其强大的仪表板而闻名，实时总结实验结果。
- en: 'MLflow ([https://mlflow.org](https://mlflow.org/docs/latest/tracking.html)):'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow ([https://mlflow.org](https://mlflow.org/docs/latest/tracking.html))：
- en: An open source platform that offers end-to-end ML life cycle management
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个提供端到端ML生命周期管理的开源平台。
- en: It supports both Python and R-based systems. It is often used in combination
    with **Data Version Control** (**DVC**)
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持基于Python和R的系统。通常与**数据版本控制**（**DVC**）结合使用。
- en: 'SageMaker Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)):'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/))：
- en: A web-based visual interface for managing ML experiments set up with SageMaker
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基于Web的可视化界面，用于管理与SageMaker设置的ML实验。
- en: The tool allows users to efficiently build, train, and deploy models by providing
    simple integrations to the other useful features of AWS
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该工具允许用户通过提供与AWS的其他有用功能简单集成，高效地构建、训练和部署模型。
- en: 'Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org/)):'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org/))：
- en: An open source platform designed by Google for end-to-end ML orchestration and
    management
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Google设计的开源平台，用于端到端ML编排和管理。
- en: It is also designed for deploying ML systems to various development and production
    environments efficiently
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也设计用于将ML系统高效地部署到各种开发和生产环境中。
- en: 'Valohai ([https://valohai.com](https://valohai.com/product/)):'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valohai ([https://valohai.com](https://valohai.com/product/))：
- en: A DL management platform designed for automatic machine orchestration, version
    control, and data pipeline management
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一款专为自动化机器编排、版本控制和数据管道管理而设计的DL管理平台。
- en: It is not free software as it’s designed for an enterprise
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is gaining popularity for being technology agnostic and having a responsive
    support team
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Out of the various tools, we will cover the two most commonly used settings:
    Weights & Biases and MLflow combined with DVC.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: a. The essential components of DL tracking are experiment tracking, model management,
    and dataset versioning. Recent DL tracking tools often have user-friendly dashboards
    that summarize the experimental results.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: b. The field is growing and there are many tools with different advantages.
    Selecting the right tool involves understanding both business and technical constraints.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at DL project tracking with **Weights & Biases** (**W&B**).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: DL project tracking with Weights & Biases
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: W&B is an experiment management platform that provides versioning for models
    and data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'W&B provides an interactive dashboard that can be embedded in Jupyter notebooks
    or used as a standalone web page. The simple Python API opens up the possibility
    for simple integration as well. Furthermore, its features focus on simplifying
    DL experiment management: logging and monitoring model and data versions, hyperparameter
    values, evaluation metrics, artifacts, and other related information.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting feature of W&B is its built-in hyperparameter search feature
    called **Sweeps** ([https://docs.wandb.ai/guides/sweeps](https://docs.wandb.ai/guides/sweeps)).
    Sweeps can easily be set up using the Python API, and the results and models can
    be compared interactively on the W&B web page.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Finally, W&B automatically creates reports for you that summarize and organize
    a set of experiments intuitively ([https://docs.wandb.ai/guides/reports](https://docs.wandb.ai/guides/reports)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the key functionalities of W&B can be summarized as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment tracking and management**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact management**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model optimization**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative analysis**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B is a subscription-based service, but personal accounts are free of charge.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Setting up W&B
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: W&B has a Python API that provides simple integration methods for many DL frameworks,
    including TensorFlow and PyTorch. The logged information, such as projects, teams,
    and the list of runs, is managed and visible online or on a self-hosted server.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of setting up W&B is to install the Python API and log into
    the W&B server. You must create an account beforehand through [https://wandb.ai](https://wandb.ai):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Within your Python code, you can register a single experiment that will be
    called `run-1` through the following line of code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'More precisely, the `wandb.init` function creates a new `wandb.Run` instance
    named `run_1` within a project called `example-DL-Book`. If a name is not provided,
    W&B will generate a random two-word name for you. If the project name is empty,
    W&B will put your run into the `Uncategorized` project. All the parameters of
    `wandb.init` are listed at [https://docs.wandb.ai/ref/python/init](https://docs.wandb.ai/ref/python/init),
    but we would like to introduce the ones that you will mostly interact with:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '`id` sets a unique ID for your run'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume` allows you to resume an experiment without creating a new run'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`job_type` allows you to assign your run to a specific type such as training,
    testing, validation, exploration, or any other name that can be used for grouping
    the runs'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` gives you additional flexibility for organizing your runs'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the `wandb.init` function is triggered, information about the run will
    start appearing on the W&B dashboard. You can monitor the dashboard on the W&B
    web page or directly in the Jupyter notebook environment, as shown in the following
    screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The W&B dashboard inside a Jupyter notebook environment'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The W&B dashboard inside a Jupyter notebook environment
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'When the run is created, you can start logging information; the `wandb.log`
    function allows you to log any data you want. For example, you can log loss during
    training by adding `wandb.log({"custom_loss": custom_loss})` to the training loop.
    Similarly, you can log validation loss and any other details that you want to
    keep track of.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, W&B made this process even simpler by providing built-in logging
    functionalities for DL models. At the time of writing, you can find integrations
    for most frameworks, including Keras, PyTorch, PyTorch Lightning, TensorFlow,
    fast.ai, scikit-learn, SageMaker, Kubeflow, Docker, Databricks, and Ray Tune (for
    details, see [https://docs.wandb.ai/guides/integrations](https://docs.wandb.ai/guides/integrations)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '`wandb.config` is an excellent place to track model hyperparameters. For any
    artifacts from experiments, you can use the `wandb.log_artifact` method (for more
    details, see [https://docs.wandb.ai/guides/artifacts](https://docs.wandb.ai/guides/artifacts)).
    When logging an artifact, you need to define a file path and then assign the name
    and type of your artifact, as shown in the following code snippet:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, you can reuse the artifact that’s been stored, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So far, you have learned how to set up `wandb` for your project and log metrics
    and artifacts of your choice individually throughout training. Interestingly,
    `wandb` provides automatic logging for many DL frameworks. In this chapter, we
    will take a closer look at W&B integration for Keras and **PyTorch Lighting**
    (**PL**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a Keras project
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the case of Keras, integration can be achieved through the `WandbCallback`
    class. The complete version can be found in this book’s GitHub repository:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As described in the previous section, key information about the models gets
    logged and becomes available on the W&B dashboard. You can monitor losses, evaluation
    metrics, and hyperparameters. *Figure 4.2* shows the sample plots that are generated
    automatically by W&B through the preceding code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Sample plots generated by W&B from logged metrics'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_04_02.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Sample plots generated by W&B from logged metrics
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a PL project is similar to integrating W&B into a Keras
    project.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a PyTorch Lightning project
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a project based on PL, W&B provides a custom logger and hides most of the
    boilerplate code. All you need to do is instantiate the `WandbLogger` class and
    pass it to the `Trainer` instance through `logger` parameter:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A detailed explanation of the integration can be found at [https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: a. W&B is an experiment management platform that helps in tracking different
    versions of models and data. It also supports storing configurations, hyperparameters,
    data, and model artifacts while providing experiment tracking in real time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: b. W&B is easy to set up. It provides a built-in integration feature for many
    DL frameworks, including TensorFlow and PyTorch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: c. W&B can be used to perform hyperparameter tuning/model optimization.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: While W&B has been dominating the field of DL project tracking, the combination
    of MLflow and DVC is another popular setup for a DL project.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: DL project tracking with MLflow and DVC
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLflow is a popular framework that supports tracking technical dependencies,
    model parameters, metrics, and artifacts. The key components of MLflow are as
    follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking**: It keeps a track of result changes every time the model runs'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projects**: It packages model code in a reproducible way'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: It organizes model artifacts for future convenient deployments'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry**: It manages a full life cycle of an MLflow model'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plugins**: It can be easily integrated with other DL frameworks as it provides
    flexible plugins'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may have already noticed, there are some similarities between W&B and
    MLflow. However, in the case of MLflow, every experiment is linked with a set
    of Git commits. Git does not prevent us from saving datasets, but it shows many
    limitations when the datasets are large, even with an extension built for large
    files (Git LFS). Thus, MLflow is commonly combined with DVC, an open source version
    control system that solves Git limitations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MLflow can be installed using `pip`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similar to W&B, MLflow also provides a Python API that allows you to track
    hyperparameters (`log_param`), evaluation metrics (`log_metric`), and artifacts
    (`log_artifacts`):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The experiment definition can be initialized and tagged with the following
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'MLflow has provided a set of tutorials that introduce its APIs: [https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html](https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the basic usage of MLflow, we will describe how
    it can be integrated for Keras and PL projects.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow into a Keras project
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s take a look at Keras integration. Logging the details of a Keras
    model using MLflow can be achieved through the `log_model` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `mlflow.keras` and `mlflow.tensorflow` modules provide a set of APIs for
    logging various information about Keras and TensorFlow models, respectively. For
    additional details, please look at [https://www.mlflow.org/docs/latest/python_api/index.html](https://www.mlflow.org/docs/latest/python_api/index.html).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow into a PyTorch Lightning project
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to how W&B supports PL projects, MLflow also provides an `MLFlowLogger`
    class. This can be passed to a `Trainer` instance for logging the model details
    in MLflow:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we have passed an instance of `MLFlowLogger` to replace
    the default logger of PL. The `tracking_uri` argument controls where the logged
    data goes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Other details about PyTorch integration can be found on the official website:
    [https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.mlflow.html](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.mlflow.html).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow with DVC
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use DVC to manage large datasets, you need to install it using a package
    manager such as `pip`, `conda`, or `brew` (for macOS users):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: All the installation options can be found at [https://dvc.org/doc/install](https://dvc.org/doc/install/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Managing datasets using DVC requires a set of commands to be executed in a
    specific order:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to set up a Git repository with DVC:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we need to configure the remote storage for DVC:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s create a sample data directory and fill it with some sample data:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At this stage, we are ready to start tracking the dataset. We just need to
    add our file to DVC. This operation will create an additional file, `example_data.csv.dvc`.
    In addition, the `example_data.csv` file gets added to `.gitignore` automatically
    so that Git no longer tracks the original file:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, you need to commit and upload the `example_data.csv.dvc` and `.gitignore`
    files. We will tag our first dataset as `v1`:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After using the `dvc push` command, our data will be available on remote storage.
    This means we can remove the local version. To restore `example_data.csv`, you
    can simply call `dvc pull`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When `example_data.csv` is modified, we need to add and push again to update
    the version on remote storage. We will tag the modified dataset as `v2`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After executing these commands, you will have two versions of the same dataset
    being tracked by Git and DVC: `v1` and `v2`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at how MLflow can be combined with DVC:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code snippet, `mlflow.log_artifact` was used to save information
    about specific columns for the experiment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we can run multiple experiments through MLflow with different versions
    of the dataset tracked by DVC. Similar to W&B, MLflow also provides a web page
    where we can compare our experiments. All you need is to type the following command
    in the terminal:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This command will start a web server hosting a web page on [http://127.0.0.1:5000](http://127.0.0.1:5000).
    The following screenshot shows the MLflow dashboard:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The MLflow dashboard; new runs will be populated at the bottom
    of the page'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_04_03.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – The MLflow dashboard; new runs will be populated at the bottom
    of the page
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: a. MLflow can track dependencies, model parameters, metrics, and artifacts.
    It is often combined with DVC for efficient dataset versioning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: b. MLflow can easily be integrated with DL frameworks, including Keras, TensorFlow,
    and PyTorch.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: c. MLflow provides an interactive visualization where multiple experiments can
    be analyzed at the same time.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to manage DL projects in W&B and MLflow and DVC.
    In the next section, we will introduce popular tools for dataset versioning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Dataset versioning – beyond Weights & Biases, MLflow, and DVC
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have seen how datasets can be managed by DL project-tracking
    tools. In the case of W&B, we can use artifacts, while in the case of MLflow and
    DVC, DVC runs on top of a Git repository to track different versions of datasets,
    thereby solving the limitations of Git.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Are there any other methods and/or tools that are useful for dataset versioning?
    The simple answer is yes, but again, the more precise answer depends on the context.
    To make the right choice, you must consider various aspects including cost, ease
    of use, and integration difficulty. In this section, we will mention a few tools
    that we believe are worth exploring if dataset versioning is one of the critical
    components of your project:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Neptune** ([https://docs.neptune.ai](https://docs.neptune.ai/)) is a metadata
    store for MLOps. Neptune artifacts allow versioning to be conducted on datasets
    that are stored locally or in cloud.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delta Lake** ([https://delta.io](https://delta.io/)) is an open source storage
    abstraction that runs on top of a data lake. Delta Lake works with Apache Spark
    APIs and uses distributed processing to improve throughput and efficiency.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Things to remember
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: a. There are many data versioning tools on the market. To select the right tool,
    you must consider various aspects including cost, ease of use, and integration
    difficulty.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: b. Tools such as W&B, MLflow, DVC, Neptune, and Delta Lake can help you with
    dataset versioning.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have introduced popular tools for dataset versioning. The right
    tool differs project by project. Therefore, you must evaluate the pros and cons
    of each tool before integrating one into your project.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们介绍了数据集版本控制的流行工具。适合的工具因项目而异。因此，在将任何工具集成到您的项目之前，您必须评估每个工具的利弊。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Since DL projects involve many iterations of training models and evaluation,
    efficiently managing experiments, models, and datasets can help the team reach
    its goal faster. In this chapter, we looked at the two most popular settings for
    DL project tracking: W&B and MLflow integrated with DVC. Both settings provide
    built-in support for Keras and PL, which are the two most popular DL frameworks.
    We have also spent some time describing tools that put more emphasis on dataset
    versioning: Neptune and Delta Lake. Please keep in mind that you must evaluate
    each tool thoroughly to select the right tool for your project.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习项目涉及多次模型训练和评估迭代，有效管理实验、模型和数据集可以帮助团队更快地实现目标。在本章中，我们探讨了两种最流行的深度学习项目跟踪设置：W&B
    和与DVC集成的MLflow。这两种设置都内置支持Keras和PyTorch，这两个最流行的深度学习框架。我们还花了一些时间描述了更加强调数据集版本控制的工具：Neptune
    和 Delta Lake。请记住，您必须仔细评估每个工具，以选择适合您项目的正确工具。
- en: At this point, you are familiar with the frameworks and processes for building
    a proof of concept and training the necessary DL model. Starting from the next
    chapter, we will discuss how to scale up by moving individual components of the
    DL pipeline to the cloud.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，您已经熟悉了构建概念验证和训练必要深度学习模型的框架和流程。从下一章开始，我们将讨论如何通过将深度学习管道的各个组件移至云端来实现规模化。
