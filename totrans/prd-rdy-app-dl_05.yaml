- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experiment Tracking, Model Management, and Dataset Versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce a set of useful tools for experiment tracking,
    model management, and dataset versioning, which allows you to effectively manage
    **deep learning** (**DL**) projects. The tools we will be discussing in this chapter
    can help us track many experiments and interpret the results more efficiently,
    which naturally leads to a reduction in operational costs and boosts the development
    cycle. By the end of the chapter, you will have hands-on experience with the most
    popular tools and be able to select the right set of tools for your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of DL project tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL project tracking with Weights & Biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL project tracking with MLflow and DVC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset versioning – beyond Weights & Biases, MLflow, and DVC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the supplemental material for this chapter from this book’s
    GitHub repository at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_4).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of DL project tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training DL models is an iterative process that consumes a lot of time and resources.
    Therefore, keeping track of all experiments and consistently organizing them can
    prevent us from wasting our time on unnecessary operations such as training similar
    models repeatedly on the same set of data. In other words, having well-documented
    records of all model architectures and their hyperparameter sets, as well as the
    version of data used during experiments, can help us derive the right conclusion
    from the experiments, which naturally leads to the project being successful.
  prefs: []
  type: TYPE_NORMAL
- en: Components of DL project tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The essential components of **DL project tracking** are **experiment tracking**,
    **model management**, and **dataset versioning**. Let’s look at each component
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept behind experiment tracking is simple: store the description and
    the motivations of each experiment so that we don’t run another set of experiments
    for the same purpose. Overall, effective experiment tracking will save us operational
    costs and allows us to derive the right conclusion from a minimal set of experimental
    results. One of the basic approaches for effective experiment tracking is adding
    a unique identifier to each experiment. The information we need to track for each
    experiment includes project dependencies, the definition of the model architecture,
    parameters used, and evaluation metrics. Experiment tracking also includes visualizing
    ongoing experiments in real time and being able to compare a set of experiments
    intuitively. For example, if we can check train and validation losses from every
    epoch as the model gets trained, we can identify overfitting quicker, saving some
    resources. Also, by comparing results and a set of changes made between two experiments,
    we can understand how the changes affect the model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Model management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model management goes beyond experiment tracking as it covers the full life
    cycle of a model: dataset information, artifacts (any data generated from training
    a model), the implementation of the model, evaluation metrics, and pipeline information
    (such as development, testing, staging, and production). Model management allows
    us to quickly pick up the model of interest and efficiently set up the environment
    in which the model can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset versioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last component of DL project tracking is dataset versioning. In many projects,
    datasets change over time. Changes can come from data schemas (blueprints of how
    the data is organized), file locations, or even from filters applied to the dataset
    manipulating the meaning of the underlying data. Many datasets found in the industry
    are structured in a complex way and often stored in multiple locations in various
    data formats. Therefore, changes can be more dramatic and harder to track than
    you anticipated. As a result, keeping a record of the changes is critical in reproducing
    consistent results throughout the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset tracking can be summarized as follows: a set of data stored as an artifact
    should become a new version of the artifact whenever the underlying data is modified.
    Having said that, every artifact should have metadata that consists of important
    information about the dataset: when it is created, who created it, and how it
    is different from the previous version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a dataset with dataset versioning should be formulated as follows.
    The dataset should have a timestamp in its name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned previously, the metadata should contain key information about
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the set of information that’s tracked by metadata may be different
    for each project.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for DL project tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL tracking can be achieved in various ways, starting from simple notes in a
    text file, through spreadsheets, keeping the information in GitHub or dedicated
    web pages, to self-built platforms and external tools. Model and data artifacts
    can be stored as is, or more sophisticated methods can be applied to avoid redundancy
    and increase efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The field of DL project tracking is growing fast and is introducing new tools
    continuously. As a result, selecting the right tool for the underlying project
    is not an easy task. We must consider both business and technical constraints.
    While the pricing model is a basic one, the other constraints can possibly be
    introduced by the existing development settings; integrating the existing tools
    should be easy, and the infrastructure must be easy to maintain. It is also important
    to consider the engineering competence of the MLOps team. Having said that, the
    following list would be a good starting point when you’re selecting a tool for
    your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard ([https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An open source visualization tool developed by the TensorFlow team
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A standard tool for tracking and visualizing the experimental results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weights & Biases ([https://wandb.ai](https://wandb.ai/site/experiment-tracking)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cloud-based service with an effective and interactive dashboard for visualizing
    and organizing the experimental results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The server can be run locally or hosted in a private cloud
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an automated hyperparameter-tuning feature called Sweeps
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Free for personal projects. Pricing is based on the tracking hours and storage
    space
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neptune ([https://neptune.ai](https://neptune.ai/product)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An online tool for monitoring and storing the artifacts from machine learning
    (ML) experiments
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It can easily be integrated with the other ML tools
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s known for its powerful dashboard which summarizes the experiments in real
    time
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow ([https://mlflow.org](https://mlflow.org/docs/latest/tracking.html)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An open source platform that offers end-to-end ML life cycle management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports both Python and R-based systems. It is often used in combination
    with **Data Version Control** (**DVC**)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker Studio ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web-based visual interface for managing ML experiments set up with SageMaker
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The tool allows users to efficiently build, train, and deploy models by providing
    simple integrations to the other useful features of AWS
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org/)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An open source platform designed by Google for end-to-end ML orchestration and
    management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also designed for deploying ML systems to various development and production
    environments efficiently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valohai ([https://valohai.com](https://valohai.com/product/)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DL management platform designed for automatic machine orchestration, version
    control, and data pipeline management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not free software as it’s designed for an enterprise
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is gaining popularity for being technology agnostic and having a responsive
    support team
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Out of the various tools, we will cover the two most commonly used settings:
    Weights & Biases and MLflow combined with DVC.'
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. The essential components of DL tracking are experiment tracking, model management,
    and dataset versioning. Recent DL tracking tools often have user-friendly dashboards
    that summarize the experimental results.
  prefs: []
  type: TYPE_NORMAL
- en: b. The field is growing and there are many tools with different advantages.
    Selecting the right tool involves understanding both business and technical constraints.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at DL project tracking with **Weights & Biases** (**W&B**).
  prefs: []
  type: TYPE_NORMAL
- en: DL project tracking with Weights & Biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: W&B is an experiment management platform that provides versioning for models
    and data.
  prefs: []
  type: TYPE_NORMAL
- en: 'W&B provides an interactive dashboard that can be embedded in Jupyter notebooks
    or used as a standalone web page. The simple Python API opens up the possibility
    for simple integration as well. Furthermore, its features focus on simplifying
    DL experiment management: logging and monitoring model and data versions, hyperparameter
    values, evaluation metrics, artifacts, and other related information.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting feature of W&B is its built-in hyperparameter search feature
    called **Sweeps** ([https://docs.wandb.ai/guides/sweeps](https://docs.wandb.ai/guides/sweeps)).
    Sweeps can easily be set up using the Python API, and the results and models can
    be compared interactively on the W&B web page.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, W&B automatically creates reports for you that summarize and organize
    a set of experiments intuitively ([https://docs.wandb.ai/guides/reports](https://docs.wandb.ai/guides/reports)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the key functionalities of W&B can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment tracking and management**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact management**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model optimization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative analysis**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W&B is a subscription-based service, but personal accounts are free of charge.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up W&B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: W&B has a Python API that provides simple integration methods for many DL frameworks,
    including TensorFlow and PyTorch. The logged information, such as projects, teams,
    and the list of runs, is managed and visible online or on a self-hosted server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of setting up W&B is to install the Python API and log into
    the W&B server. You must create an account beforehand through [https://wandb.ai](https://wandb.ai):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Within your Python code, you can register a single experiment that will be
    called `run-1` through the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'More precisely, the `wandb.init` function creates a new `wandb.Run` instance
    named `run_1` within a project called `example-DL-Book`. If a name is not provided,
    W&B will generate a random two-word name for you. If the project name is empty,
    W&B will put your run into the `Uncategorized` project. All the parameters of
    `wandb.init` are listed at [https://docs.wandb.ai/ref/python/init](https://docs.wandb.ai/ref/python/init),
    but we would like to introduce the ones that you will mostly interact with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id` sets a unique ID for your run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume` allows you to resume an experiment without creating a new run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`job_type` allows you to assign your run to a specific type such as training,
    testing, validation, exploration, or any other name that can be used for grouping
    the runs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` gives you additional flexibility for organizing your runs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the `wandb.init` function is triggered, information about the run will
    start appearing on the W&B dashboard. You can monitor the dashboard on the W&B
    web page or directly in the Jupyter notebook environment, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The W&B dashboard inside a Jupyter notebook environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The W&B dashboard inside a Jupyter notebook environment
  prefs: []
  type: TYPE_NORMAL
- en: 'When the run is created, you can start logging information; the `wandb.log`
    function allows you to log any data you want. For example, you can log loss during
    training by adding `wandb.log({"custom_loss": custom_loss})` to the training loop.
    Similarly, you can log validation loss and any other details that you want to
    keep track of.'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, W&B made this process even simpler by providing built-in logging
    functionalities for DL models. At the time of writing, you can find integrations
    for most frameworks, including Keras, PyTorch, PyTorch Lightning, TensorFlow,
    fast.ai, scikit-learn, SageMaker, Kubeflow, Docker, Databricks, and Ray Tune (for
    details, see [https://docs.wandb.ai/guides/integrations](https://docs.wandb.ai/guides/integrations)).
  prefs: []
  type: TYPE_NORMAL
- en: '`wandb.config` is an excellent place to track model hyperparameters. For any
    artifacts from experiments, you can use the `wandb.log_artifact` method (for more
    details, see [https://docs.wandb.ai/guides/artifacts](https://docs.wandb.ai/guides/artifacts)).
    When logging an artifact, you need to define a file path and then assign the name
    and type of your artifact, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can reuse the artifact that’s been stored, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So far, you have learned how to set up `wandb` for your project and log metrics
    and artifacts of your choice individually throughout training. Interestingly,
    `wandb` provides automatic logging for many DL frameworks. In this chapter, we
    will take a closer look at W&B integration for Keras and **PyTorch Lighting**
    (**PL**).
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a Keras project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the case of Keras, integration can be achieved through the `WandbCallback`
    class. The complete version can be found in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As described in the previous section, key information about the models gets
    logged and becomes available on the W&B dashboard. You can monitor losses, evaluation
    metrics, and hyperparameters. *Figure 4.2* shows the sample plots that are generated
    automatically by W&B through the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Sample plots generated by W&B from logged metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Sample plots generated by W&B from logged metrics
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a PL project is similar to integrating W&B into a Keras
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating W&B into a PyTorch Lightning project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a project based on PL, W&B provides a custom logger and hides most of the
    boilerplate code. All you need to do is instantiate the `WandbLogger` class and
    pass it to the `Trainer` instance through `logger` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A detailed explanation of the integration can be found at [https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html).
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. W&B is an experiment management platform that helps in tracking different
    versions of models and data. It also supports storing configurations, hyperparameters,
    data, and model artifacts while providing experiment tracking in real time.
  prefs: []
  type: TYPE_NORMAL
- en: b. W&B is easy to set up. It provides a built-in integration feature for many
    DL frameworks, including TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: c. W&B can be used to perform hyperparameter tuning/model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: While W&B has been dominating the field of DL project tracking, the combination
    of MLflow and DVC is another popular setup for a DL project.
  prefs: []
  type: TYPE_NORMAL
- en: DL project tracking with MLflow and DVC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLflow is a popular framework that supports tracking technical dependencies,
    model parameters, metrics, and artifacts. The key components of MLflow are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking**: It keeps a track of result changes every time the model runs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projects**: It packages model code in a reproducible way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: It organizes model artifacts for future convenient deployments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry**: It manages a full life cycle of an MLflow model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plugins**: It can be easily integrated with other DL frameworks as it provides
    flexible plugins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may have already noticed, there are some similarities between W&B and
    MLflow. However, in the case of MLflow, every experiment is linked with a set
    of Git commits. Git does not prevent us from saving datasets, but it shows many
    limitations when the datasets are large, even with an extension built for large
    files (Git LFS). Thus, MLflow is commonly combined with DVC, an open source version
    control system that solves Git limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MLflow can be installed using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to W&B, MLflow also provides a Python API that allows you to track
    hyperparameters (`log_param`), evaluation metrics (`log_metric`), and artifacts
    (`log_artifacts`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The experiment definition can be initialized and tagged with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'MLflow has provided a set of tutorials that introduce its APIs: [https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html](https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the basic usage of MLflow, we will describe how
    it can be integrated for Keras and PL projects.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow into a Keras project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s take a look at Keras integration. Logging the details of a Keras
    model using MLflow can be achieved through the `log_model` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `mlflow.keras` and `mlflow.tensorflow` modules provide a set of APIs for
    logging various information about Keras and TensorFlow models, respectively. For
    additional details, please look at [https://www.mlflow.org/docs/latest/python_api/index.html](https://www.mlflow.org/docs/latest/python_api/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow into a PyTorch Lightning project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to how W&B supports PL projects, MLflow also provides an `MLFlowLogger`
    class. This can be passed to a `Trainer` instance for logging the model details
    in MLflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have passed an instance of `MLFlowLogger` to replace
    the default logger of PL. The `tracking_uri` argument controls where the logged
    data goes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other details about PyTorch integration can be found on the official website:
    [https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.mlflow.html](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.mlflow.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow with DVC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use DVC to manage large datasets, you need to install it using a package
    manager such as `pip`, `conda`, or `brew` (for macOS users):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All the installation options can be found at [https://dvc.org/doc/install](https://dvc.org/doc/install/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Managing datasets using DVC requires a set of commands to be executed in a
    specific order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to set up a Git repository with DVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to configure the remote storage for DVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a sample data directory and fill it with some sample data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this stage, we are ready to start tracking the dataset. We just need to
    add our file to DVC. This operation will create an additional file, `example_data.csv.dvc`.
    In addition, the `example_data.csv` file gets added to `.gitignore` automatically
    so that Git no longer tracks the original file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you need to commit and upload the `example_data.csv.dvc` and `.gitignore`
    files. We will tag our first dataset as `v1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After using the `dvc push` command, our data will be available on remote storage.
    This means we can remove the local version. To restore `example_data.csv`, you
    can simply call `dvc pull`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When `example_data.csv` is modified, we need to add and push again to update
    the version on remote storage. We will tag the modified dataset as `v2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing these commands, you will have two versions of the same dataset
    being tracked by Git and DVC: `v1` and `v2`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at how MLflow can be combined with DVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, `mlflow.log_artifact` was used to save information
    about specific columns for the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we can run multiple experiments through MLflow with different versions
    of the dataset tracked by DVC. Similar to W&B, MLflow also provides a web page
    where we can compare our experiments. All you need is to type the following command
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will start a web server hosting a web page on [http://127.0.0.1:5000](http://127.0.0.1:5000).
    The following screenshot shows the MLflow dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The MLflow dashboard; new runs will be populated at the bottom
    of the page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – The MLflow dashboard; new runs will be populated at the bottom
    of the page
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. MLflow can track dependencies, model parameters, metrics, and artifacts.
    It is often combined with DVC for efficient dataset versioning.
  prefs: []
  type: TYPE_NORMAL
- en: b. MLflow can easily be integrated with DL frameworks, including Keras, TensorFlow,
    and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: c. MLflow provides an interactive visualization where multiple experiments can
    be analyzed at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to manage DL projects in W&B and MLflow and DVC.
    In the next section, we will introduce popular tools for dataset versioning.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset versioning – beyond Weights & Biases, MLflow, and DVC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have seen how datasets can be managed by DL project-tracking
    tools. In the case of W&B, we can use artifacts, while in the case of MLflow and
    DVC, DVC runs on top of a Git repository to track different versions of datasets,
    thereby solving the limitations of Git.
  prefs: []
  type: TYPE_NORMAL
- en: 'Are there any other methods and/or tools that are useful for dataset versioning?
    The simple answer is yes, but again, the more precise answer depends on the context.
    To make the right choice, you must consider various aspects including cost, ease
    of use, and integration difficulty. In this section, we will mention a few tools
    that we believe are worth exploring if dataset versioning is one of the critical
    components of your project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neptune** ([https://docs.neptune.ai](https://docs.neptune.ai/)) is a metadata
    store for MLOps. Neptune artifacts allow versioning to be conducted on datasets
    that are stored locally or in cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delta Lake** ([https://delta.io](https://delta.io/)) is an open source storage
    abstraction that runs on top of a data lake. Delta Lake works with Apache Spark
    APIs and uses distributed processing to improve throughput and efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. There are many data versioning tools on the market. To select the right tool,
    you must consider various aspects including cost, ease of use, and integration
    difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: b. Tools such as W&B, MLflow, DVC, Neptune, and Delta Lake can help you with
    dataset versioning.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have introduced popular tools for dataset versioning. The right
    tool differs project by project. Therefore, you must evaluate the pros and cons
    of each tool before integrating one into your project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since DL projects involve many iterations of training models and evaluation,
    efficiently managing experiments, models, and datasets can help the team reach
    its goal faster. In this chapter, we looked at the two most popular settings for
    DL project tracking: W&B and MLflow integrated with DVC. Both settings provide
    built-in support for Keras and PL, which are the two most popular DL frameworks.
    We have also spent some time describing tools that put more emphasis on dataset
    versioning: Neptune and Delta Lake. Please keep in mind that you must evaluate
    each tool thoroughly to select the right tool for your project.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are familiar with the frameworks and processes for building
    a proof of concept and training the necessary DL model. Starting from the next
    chapter, we will discuss how to scale up by moving individual components of the
    DL pipeline to the cloud.
  prefs: []
  type: TYPE_NORMAL
