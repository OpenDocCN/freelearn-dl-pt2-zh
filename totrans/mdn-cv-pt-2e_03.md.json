["```py\n    >>> import torch\n    >>> print(torch.__version__) \n    ```", "```py\n    import torch\n    x = torch.tensor([[1,2]])\n    y = torch.tensor([[1],[2]]) \n    ```", "```py\n    print(x.shape)\n    # torch.Size([1,2]) # one entity of two items\n    print(y.shape)\n    # torch.Size([2,1]) # two entities of one item each\n    print(x.dtype)\n    # torch.int64 \n    ```", "```py\n    x = torch.tensor([False, 1, 2.0])\n    print(x)\n    # tensor([0., 1., 2.]) \n    ```", "```py\n    torch.zeros((3, 4)) \n    ```", "```py\n    torch.ones((3, 4)) \n    ```", "```py\n    torch.randint(low=0, high=10, size=(3,4)) \n    ```", "```py\n    torch.rand(3, 4) \n    ```", "```py\n    torch.randn((3,4)) \n    ```", "```py\n    x = np.array([[10,20,30],[2,3,4]])\n    y = torch.tensor(x)\n    print(type(x), type(y))\n    # <class 'numpy.ndarray'> <class 'torch.Tensor'> \n    ```", "```py\n    import torch\n    x = torch.tensor([[1,2,3,4], [5,6,7,8]]) \n    print(x * 10)\n    # tensor([[10, 20, 30, 40],\n    #        [50, 60, 70, 80]]) \n    ```", "```py\n    x = torch.tensor([[1,2,3,4], [5,6,7,8]]) \n    y = x.add(10)\n    print(y)\n    # tensor([[11, 12, 13, 14],\n    #         [15, 16, 17, 18]]) \n    ```", "```py\n    y = torch.tensor([2, 3, 1, 0]) \n    # y.shape == (4)\n    y = y.view(4,1)                \n    # y.shape == (4, 1) \n    ```", "```py\n    x = torch.randn(10,1,10)\n    z1 = torch.squeeze(x, 1) # similar to np.squeeze()\n    # The same operation can be directly performed on\n    # x by calling squeeze and the dimension to squeeze out\n    z2 = x.squeeze(1)\n    assert torch.all(z1 == z2) \n    # all the elements in both tensors are equal\n    print('Squeeze:\\n', x.shape, z1.shape)\n    # Squeeze: torch.Size([10, 1, 10]) torch.Size([10, 10]) \n    ```", "```py\n    x = torch.randn(10,10)\n    print(x.shape)\n    # torch.size(10,10)\n    z1 = x.unsqueeze(0)\n    print(z1.shape)\n    # torch.size(1,10,10)\n    # The same can be achieved using [None] indexing\n    # Adding None will auto create a fake dim \n    # at the specified axis\n    x = torch.randn(10,10)\n    z2, z3, z4 = x[None], x[:,None], x[:,:,None]\n    print(z2.shape, z3.shape, z4.shape)\n    # torch.Size([1, 10, 10]) \n    # torch.Size([10, 1, 10]) \n    # torch.Size([10, 10, 1]) \n    ```", "```py\n    x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n    print(torch.matmul(x, y))\n    # tensor([[11],\n    #         [35]]) \n    ```", "```py\n    print(x@y)\n    # tensor([[11],\n    #  [35]]) \n    ```", "```py\n    import torch\n    x = torch.randn(10,10,10)\n    z = torch.cat([x,x], axis=0) # np.concatenate()\n    print('Cat axis 0:', x.shape, z.shape)\n    # Cat axis 0:  torch.Size([10, 10, 10]) \n    # torch.Size([20, 10, 10])\n    z = torch.cat([x,x], axis=1) # np.concatenate()\n    print('Cat axis 1:', x.shape, z.shape)\n    # Cat axis 1: torch.Size([10, 10, 10]) \n    # torch.Size([10, 20, 10]) \n    ```", "```py\n    x = torch.arange(25).reshape(5,5)\n    print('Max:', x.shape, x.max()) \n    # Max:  torch.Size([5, 5]) tensor(24) \n    ```", "```py\n    x.max(dim=0)\n    # torch.return_types.max(values=tensor([20, 21, 22, 23, 24]), \n    # indices=tensor([4, 4, 4, 4, 4])) \n    ```", "```py\n    m, argm = x.max(dim=1) \n    print('Max in axis 1:\\n', m, argm) \n    # Max in axis 1: tensor([ 4, 9, 14, 19, 24]) \n    # tensor([4, 4, 4, 4, 4]) \n    ```", "```py\n    x = torch.randn(10,20,30)\n    z = x.permute(2,0,1) # np.permute()\n    print('Permute dimensions:', x.shape, z.shape)\n    # Permute dimensions:  torch.Size([10, 20, 30]) \n    # torch.Size([30, 10, 20]) \n    ```", "```py\n    import torch\n    x = torch.tensor([[2., -1.], [1., 1.]], requires_grad=True)\n    print(x) \n    ```", "```py\n    out = x.pow(2).sum() \n    ```", "```py\n    out.backward() \n    ```", "```py\n    x.grad \n    ```", "```py\n    # tensor([[4., -2.],\n    #         [2., 2.]]) \n    ```", "```py\n    import torch\n    x = torch.rand(1, 6400)\n    y = torch.rand(6400, 5000) \n    ```", "```py\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    x, y = x.to(device), y.to(device) \n    ```", "```py\n    %timeit z=(x@y)\n    # It takes 0.515 milli seconds on an average to \n    # perform matrix multiplication \n    ```", "```py\n    x, y = x.cpu(), y.cpu()\n    %timeit z=(x@y)\n    # It takes 9 milli seconds on an average to \n    # perform matrix multiplication \n    ```", "```py\n    import numpy as np\n    x = np.random.random((1, 6400))\n    y = np.random.random((6400, 5000))\n    %timeit z = np.matmul(x,y)\n    # It takes 19 milli seconds on an average to \n    # perform matrix multiplication \n    ```", "```py\n    import torch\n    x = [[1,2],[3,4],[5,6],[7,8]]\n    y = [[3],[7],[11],[15]] \n    ```", "```py\n    X = torch.tensor(x).float()\n    Y = torch.tensor(y).float() \n    ```", "```py\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    X = X.to(device)\n    Y = Y.to(device) \n    ```", "```py\n    import torch.nn as nn \n    ```", "```py\n    class MyNeuralNet(nn.Module): \n    ```", "```py\n     def __init__(self):\n            super().__init__() \n    ```", "```py\n     self.input_to_hidden_layer = nn.Linear(2,8)\n            self.hidden_layer_activation = nn.ReLU()\n            self.hidden_to_output_layer = nn.Linear(8,1) \n    ```", "```py\n    **# NOTE - This line of code is not a part of model building,** \n    **# this is used only for illustration of Linear method**\n    print(nn.Linear(2, 7))\n    Linear(in_features=2, out_features=7, bias=True) \n    ```", "```py\n     def forward(self, x):\n            x = self.input_to_hidden_layer(x)\n            x = self.hidden_layer_activation(x)\n            x = self.hidden_to_output_layer(x)\n            return x \n    ```", "```py\n    mynet = MyNeuralNet().to(device) \n    ```", "```py\n    # NOTE - This line of code is not a part of model building, \n    # this is used only for illustration of \n    # how to obtain parameters of a given layer\n    mynet.input_to_hidden_layer.weight \n    ```", "```py\n    # NOTE - This line of code is not a part of model building, \n    # this is used only for illustration of \n    # how to obtain parameters of all layers in a model\n    mynet.parameters() \n    ```", "```py\n    # NOTE - This line of code is not a part of model building, \n    # this is used only for illustration of how to \n    # obtain parameters of all layers in a model \n    # by looping through the generator object\n    for par in mynet.parameters():\n        print(par) \n    ```", "```py\n    # for illustration only\n    class MyNeuralNet(nn.Module):\n         def __init__(self):\n            super().__init__()\n            self.input_to_hidden_layer = nn.Parameter(torch.rand(2,8))\n            self.hidden_layer_activation = nn.ReLU()\n            self.hidden_to_output_layer = nn.Parameter(torch.rand(8,1))\n         def forward(self, x):\n            x = x @ self.input_to_hidden_layer\n            x = self.hidden_layer_activation(x)\n            x = x @ self.hidden_to_output_layer\n            return x \n    ```", "```py\n    loss_func = nn.MSELoss() \n    ```", "```py\n    _Y = mynet(X)\n    loss_value = loss_func(_Y,Y)\n    print(loss_value)\n    # tensor(91.5550, grad_fn=<MseLossBackward>)\n    # Note that loss value can differ in your instance \n    # due to a different random weight initialization \n    ```", "```py\n    from torch.optim import SGD\n    opt = SGD(mynet.parameters(), lr = 0.001) \n    ```", "```py\n    **# NOTE - This line of code is not a part of model building,** \n    **# this is used only for illustration of how we perform** \n    opt.zero_grad() # flush the previous epoch's gradients\n    loss_value = loss_func(mynet(X),Y) # compute loss\n    loss_value.backward() # perform backpropagation\n    opt.step() # update the weights according to the #gradients computed \n    ```", "```py\n    loss_history = []\n    for _ in range(50):\n        opt.zero_grad()\n        loss_value = loss_func(mynet(X),Y)\n        loss_value.backward()\n        opt.step()\n        loss_history.append(loss_value.item()) \n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    plt.plot(loss_history)\n    plt.title('Loss variation over increasing epochs')\n    plt.xlabel('epochs')\n    plt.ylabel('loss value') \n    ```", "```py\n    from torch.utils.data import Dataset, DataLoader\n    import torch\n    import torch.nn as nn \n    ```", "```py\n    x = [[1,2],[3,4],[5,6],[7,8]]\n    y = [[3],[7],[11],[15]] \n    ```", "```py\n    X = torch.tensor(x).float()\n    Y = torch.tensor(y).float() \n    ```", "```py\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    X = X.to(device)\n    Y = Y.to(device) \n    ```", "```py\n    class MyDataset(Dataset): \n    ```", "```py\n     def __init__(self,x,y):\n            self.x = torch.tensor(x).float()\n            self.y = torch.tensor(y).float() \n    ```", "```py\n     def __len__(self):\n            return len(self.x) \n    ```", "```py\n     def __getitem__(self, ix):\n            return self.x[ix], self.y[ix] \n    ```", "```py\n    ds = MyDataset(X, Y) \n    ```", "```py\n    dl = DataLoader(ds, batch_size=2, shuffle=True) \n    ```", "```py\n    # NOTE - This line of code is not a part of model building, \n    # this is used only for illustration of \n    # how to print the input and output batches of data\n    for x,y in dl:\n        print(x,y) \n    ```", "```py\n    tensor([[1., 2.],\n            [3., 4.]]) tensor([[3.], [7.]])\n    tensor([[5., 6.],\n            [7., 8.]]) tensor([[1.], [15.]]) \n    ```", "```py\n    class MyNeuralNet(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.input_to_hidden_layer = nn.Linear(2,8)\n            self.hidden_layer_activation = nn.ReLU()\n            self.hidden_to_output_layer = nn.Linear(8,1)\n        def forward(self, x):\n            x = self.input_to_hidden_layer(x)\n            x = self.hidden_layer_activation(x)\n            x = self.hidden_to_output_layer(x)\n            return x \n    ```", "```py\n    mynet = MyNeuralNet().to(device)\n    loss_func = nn.MSELoss()\n    from torch.optim import SGD\n    opt = SGD(mynet.parameters(), lr = 0.001) \n    ```", "```py\n    import time\n    loss_history = []\n    start = time.time()\n    for _ in range(50):\n        for data in dl:\n            x, y = data\n            opt.zero_grad()\n            loss_value = loss_func(mynet(x),y)\n            loss_value.backward()\n            opt.step()\n            loss_history.append(loss_value.item())\n    end = time.time()\n    print(end - start) \n    ```", "```py\n    val_x = [[10,11]] \n    ```", "```py\n    val_x = torch.tensor(val_x).float().to(device) \n    ```", "```py\n    mynet(val_x)\n    # 20.99 \n    ```", "```py\n    x = [[1,2],[3,4],[5,6],[7,8]]\n    y = [[3],[7],[11],[15]]\n    import torch\n    X = torch.tensor(x).float()\n    Y = torch.tensor(y).float()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    X = X.to(device)\n    Y = Y.to(device) \n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n    class MyDataset(Dataset):\n        def __init__(self,x,y):\n            self.x = torch.tensor(x).float()\n            self.y = torch.tensor(y).float()\n        def __len__(self):\n            return len(self.x)\n        def __getitem__(self, ix):\n            return self.x[ix], self.y[ix]\n    ds = MyDataset(X, Y)\n    dl = DataLoader(ds, batch_size=2, shuffle=True)\n    class MyNeuralNet(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.input_to_hidden_layer = nn.Linear(2,8)\n            self.hidden_layer_activation = nn.ReLU()\n            self.hidden_to_output_layer = nn.Linear(8,1)\n        def forward(self, x):\n            x = self.input_to_hidden_layer(x)\n            x = self.hidden_layer_activation(x)\n            x = self.hidden_to_output_layer(x)\n            return x\n    mynet = MyNeuralNet().to(device) \n    ```", "```py\n    def my_mean_squared_error(_y, y):\n        loss = (_y-y)**2\n        loss = loss.mean()\n        return loss \n    ```", "```py\n    loss_func = nn.MSELoss()\n    loss_value = loss_func(mynet(X),Y)\n    print(loss_value)\n    # 92.7534 \n    ```", "```py\n    my_mean_squared_error(mynet(X),Y)\n    # 92.7534 \n    ```", "```py\n    input_to_hidden = mynet.input_to_hidden_layer(X)\n    hidden_activation = mynet.hidden_layer_activation(input_to_hidden)\n    print(hidden_activation) \n    ```", "```py\n    class NeuralNet(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.input_to_hidden_layer = nn.Linear(2,8)\n            self.hidden_layer_activation = nn.ReLU()\n            self.hidden_to_output_layer = nn.Linear(8,1)\n        def forward(self, x):\n            hidden1 = self.input_to_hidden_layer(x)\n            hidden2 = self.hidden_layer_activation(hidden1)\n            output = self.hidden_to_output_layer(hidden2)\n            return output, hidden2 \n    ```", "```py\n    mynet = NeuralNet().to(device)\n    mynet(X)[1] \n    ```", "```py\n    x = [[1,2],[3,4],[5,6],[7,8]]\n    y = [[3],[7],[11],[15]] \n    ```", "```py\n    import torch\n    import torch.nn as nn\n    import numpy as np\n    from torch.utils.data import Dataset, DataLoader\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    class MyDataset(Dataset):\n        def __init__(self, x, y):\n            self.x = torch.tensor(x).float().to(device)\n            self.y = torch.tensor(y).float().to(device)\n        def __getitem__(self, ix):\n            return self.x[ix], self.y[ix]\n        def __len__(self): \n            return len(self.x) \n    ```", "```py\n    ds = MyDataset(x, y)\n    dl = DataLoader(ds, batch_size=2, shuffle=True) \n    ```", "```py\n    model = nn.Sequential(\n                nn.Linear(2, 8),\n                nn.ReLU(),\n                nn.Linear(8, 1)\n            ).to(device) \n    ```", "```py\n    !pip install torch_summary\n    from torchsummary import summary \n    ```", "```py\n    summary(model, torch.zeros(1,2)) \n    ```", "```py\n    loss_func = nn.MSELoss()\n    from torch.optim import SGD\n    opt = SGD(model.parameters(), lr = 0.001)\n    import time\n    loss_history = []\n    start = time.time()\n    for _ in range(50):\n        for ix, iy in dl:\n            opt.zero_grad()\n            loss_value = loss_func(model(ix),iy)\n            loss_value.backward()\n            opt.step()\n            loss_history.append(loss_value.item())\n    end = time.time()\n    print(end - start) \n    ```", "```py\n    val = [[8,9],[10,11],[1.5,2.5]] \n    ```", "```py\n    model(torch.tensor(val).float().to(device))\n    # tensor([[16.9051], [20.8352], [ 4.0773]], \n    # device='cuda:0', grad_fn=<AddmmBackward>) \n    ```", "```py\ntorch.save(model.to('cpu').state_dict(), 'mymodel.pth') \n```", "```py\n    model = nn.Sequential(\n                nn.Linear(2, 8),\n                nn.ReLU(),\n                nn.Linear(8, 1)\n            ).to(device) \n    ```", "```py\n    state_dict = torch.load('mymodel.pth') \n    ```", "```py\n    model.load_state_dict(state_dict)\n    # <All keys matched successfully>\n    model.to(device)\n    model(torch.tensor(val).float().to(device)) \n    ```"]