- en: Appendix I — Terminology of Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The past decades have produced **Convolutional Neural Networks** (**CNNs**),
    **Recurrent Neural Networks** (**RNNs**), and more types of **Artificial Neural
    Networks** (**ANNs**). They all have a certain amount of vocabulary in common.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models introduced some new words and used existing words slightly
    differently. This appendix briefly describes transformer models to clarify the
    usage of deep learning vocabulary when applied to transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of transformer model architecture relies upon an industrial approach
    to deep learning. The geometric nature of transformers boosts parallel processing.
    In addition, the architecture of transformers perfectly fits hardware optimization
    requirements. Google, for example, took advantage of the stack structure of transformers
    to design domain-specific optimized hardware that requires less floating-number
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: Designing transformers models implies taking hardware into account. Therefore,
    the architecture of a transformer combines software and hardware optimization
    from the start.
  prefs: []
  type: TYPE_NORMAL
- en: This appendix defines some of the new usages of neural network language.
  prefs: []
  type: TYPE_NORMAL
- en: Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *stack* contains identically sized layers that differ from classical deep
    learning models, as shown in *Figure I.1*. A stack runs from *bottom to top*.
    A stack can be an encoder or a decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_Appendix_I_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure I.1: Layers form a stack'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer stacks learn and see more as they rise in the stacks. Each layer
    transmits what it learned to the next layer just as our memory does.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that a *stack* is the Empire State Building in New York City. At the
    bottom, you cannot see much. But you will see more and farther as you ascend throught
    the offices on higher floors and look out the windows. Finally, at the top, you
    have a fantastic view of Manhattan!
  prefs: []
  type: TYPE_NORMAL
- en: Sublayer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each layer contains sublayers, as shown in *Figure I.2*. Each sublayer of different
    layers has an identical structure, which boosts hardware optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The original Transformer contains two sublayers that run from *bottom to top:*
  prefs: []
  type: TYPE_NORMAL
- en: A self-attention sublayer, designed specifically for NLP and hardware optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classical feedforward network with some tweaking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_Appendix_I_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure I.2: A layer contains two sublayers'
  prefs: []
  type: TYPE_NORMAL
- en: Attention heads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A self-attention sublayer is divided into n independent and identical layers
    called *heads*. For example, the original Transformer contains eight heads.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure I.3* represents heads as processors to show that transformers’ industrialized
    structure fits hardware design:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, scoreboard  Description automatically generated](img/B17948_Appendix_I_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure I.3: A self-attention sublayer contains heads'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the attention heads are represented by microprocessors in *Figure
    I.3* to stress the parallel processing power of transformer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architectures fit both NLP and hardware-optimization requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
