["```py\n>>> import pandas as pd\n>>> from io import StringIO\n>>> csv_data = \\\n... '''A,B,C,D\n... 1.0,2.0,3.0,4.0\n... 5.0,6.0,,8.0\n... 10.0,11.0,12.0,'''\n>>> # If you are using Python 2.7, you need\n>>> # to convert the string to unicode:\n>>> # csv_data = unicode(csv_data)\n>>> df = pd.read_csv(StringIO(csv_data))\n>>> df\n        A        B        C        D\n0     1.0      2.0      3.0      4.0\n1     5.0      6.0      NaN      8.0\n2    10.0     11.0     12.0      NaN \n```", "```py\n>>> df.isnull().sum()\nA      0\nB      0\nC      1\nD      1\ndtype: int64 \n```", "```py\n>>> df.values\narray([[  1.,   2.,   3.,   4.],\n       [  5.,   6.,  nan,   8.],\n       [ 10.,  11.,  12.,  nan]]) \n```", "```py\n>>> df.dropna(axis=0)\n      A    B    C    D\n0   1.0  2.0  3.0  4.0 \n```", "```py\n>>> df.dropna(axis=1)\n      A      B\n0   1.0    2.0\n1   5.0    6.0\n2  10.0   11.0 \n```", "```py\n>>> # only drop rows where all columns are NaN\n>>> # (returns the whole array here since we don't\n>>> # have a row with all values NaN)\n>>> df.dropna(how='all')\n      A      B      C      D\n0   1.0    2.0    3.0    4.0\n1   5.0    6.0    NaN    8.0\n2  10.0   11.0   12.0    NaN\n>>> # drop rows that have fewer than 4 real values\n>>> df.dropna(thresh=4)\n      A      B      C      D\n0   1.0    2.0    3.0    4.0\n>>> # only drop rows where NaN appear in specific columns (here: 'C')\n>>> df.dropna(subset=['C'])\n      A      B      C      D\n0   1.0    2.0    3.0    4.0\n2  10.0   11.0   12.0    NaN \n```", "```py\n>>> from sklearn.impute import SimpleImputer\n>>> import numpy as np\n>>> imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n>>> imr = imr.fit(df.values)\n>>> imputed_data = imr.transform(df.values)\n>>> imputed_data\narray([[  1.,   2.,   3.,   4.],\n       [  5.,   6.,  7.5,   8.],\n       [ 10.,  11.,  12.,   6.]]) \n```", "```py\n>>> df.fillna(df.mean()) \n```", "```py\n>>> import pandas as pd\n>>> df = pd.DataFrame([\n...            ['green', 'M', 10.1, 'class2'],\n...            ['red', 'L', 13.5, 'class1'],\n...            ['blue', 'XL', 15.3, 'class2']])\n>>> df.columns = ['color', 'size', 'price', 'classlabel']\n>>> df\n    color  size  price  classlabel\n0   green     M   10.1      class2\n1     red     L   13.5      class1\n2    blue    XL   15.3      class2 \n```", "```py\n>>> size_mapping = {'XL': 3,\n...                 'L': 2,\n...                 'M': 1}\n>>> df['size'] = df['size'].map(size_mapping)\n>>> df\n    color  size  price  classlabel\n0   green     1   10.1      class2\n1     red     2   13.5      class1\n2    blue     3   15.3      class2 \n```", "```py\n>>> inv_size_mapping = {v: k for k, v in size_mapping.items()}\n>>> df['size'].map(inv_size_mapping)\n0   M\n1   L\n2   XL\nName: size, dtype: object \n```", "```py\n>>> import numpy as np\n>>> class_mapping = {label: idx for idx, label in\n...                  enumerate(np.unique(df['classlabel']))}\n>>> class_mapping\n{'class1': 0, 'class2': 1} \n```", "```py\n>>> df['classlabel'] = df['classlabel'].map(class_mapping)\n>>> df\n    color  size  price  classlabel\n0   green     1   10.1           1\n1     red     2   13.5           0\n2    blue     3   15.3           1 \n```", "```py\n>>> inv_class_mapping = {v: k for k, v in class_mapping.items()}\n>>> df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n>>> df\n    color  size  price  classlabel\n0   green     1   10.1      class2\n1     red     2   13.5      class1\n2    blue     3   15.3      class2 \n```", "```py\n>>> from sklearn.preprocessing import LabelEncoder\n>>> class_le = LabelEncoder()\n>>> y = class_le.fit_transform(df['classlabel'].values)\n>>> y\narray([1, 0, 1]) \n```", "```py\n>>> class_le.inverse_transform(y)\narray(['class2', 'class1', 'class2'], dtype=object) \n```", "```py\n>>> X = df[['color', 'size', 'price']].values\n>>> color_le = LabelEncoder()\n>>> X[:, 0] = color_le.fit_transform(X[:, 0])\n>>> X\narray([[1, 1, 10.1],\n       [2, 2, 13.5],\n       [0, 3, 15.3]], dtype=object) \n```", "```py\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> X = df[['color', 'size', 'price']].values\n>>> color_ohe = OneHotEncoder()\n>>> color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()\n    array([[0., 1., 0.],\n           [0., 0., 1.],\n           [1., 0., 0.]]) \n```", "```py\n>>> from sklearn.compose import ColumnTransformer\n>>> X = df[['color', 'size', 'price']].values\n>>> c_transf = ColumnTransformer([\n...     ('onehot', OneHotEncoder(), [0]),\n...     ('nothing', 'passthrough', [1, 2])\n... ])\n>>> c_transf.fit_transform(X).astype(float)\n    array([[0.0, 1.0, 0.0, 1, 10.1],\n           [0.0, 0.0, 1.0, 2, 13.5],\n           [1.0, 0.0, 0.0, 3, 15.3]]) \n```", "```py\n>>> pd.get_dummies(df[['price', 'color', 'size']])\n    price  size  color_blue  color_green  color_red\n0    10.1     1           0            1          0\n1    13.5     2           0            0          1\n2    15.3     3           1            0          0 \n```", "```py\n>>> pd.get_dummies(df[['price', 'color', 'size']],\n...                drop_first=True)\n    price  size  color_green  color_red\n0    10.1     1            1          0\n1    13.5     2            0          1\n2    15.3     3            0          0 \n```", "```py\n>>> color_ohe = OneHotEncoder(categories='auto', drop='first')\n>>> c_transf = ColumnTransformer([\n...            ('onehot', color_ohe, [0]),\n...            ('nothing', 'passthrough', [1, 2])\n... ])\n>>> c_transf.fit_transform(X).astype(float)\narray([[  1\\. ,  0\\. ,  1\\. ,  10.1],\n       [  0\\. ,  1\\. ,  2\\. ,  13.5],\n       [  0\\. ,  0\\. ,  3\\. ,  15.3]]) \n```", "```py\n>>> df = pd.DataFrame([['green', 'M', 10.1,\n...                     'class2'],\n...                    ['red', 'L', 13.5,\n...                     'class1'],\n...                    ['blue', 'XL', 15.3,\n...                     'class2']])\n>>> df.columns = ['color', 'size', 'price',\n...               'classlabel']\n>>> df \n```", "```py\n>>> df['x > M'] = df['size'].apply(\n...     lambda x: 1 if x in {'L', 'XL'} else 0)\n>>> df['x > L'] = df['size'].apply(\n...     lambda x: 1 if x == 'XL' else 0)\n>>> del df['size']\n>>> df \n```", "```py\ndf = pd.read_csv(\n    'https://archive.ics.uci.edu/ml/'\n    'machine-learning-databases/wine/wine.data',\n    header=None\n) \n```", "```py\ndf = pd.read_csv(\n    'your/local/path/to/wine.data', header=None\n) \n```", "```py\n>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n...                       'ml/machine-learning-databases/'\n...                       'wine/wine.data', header=None)\n>>> df_wine.columns = ['Class label', 'Alcohol',\n...                    'Malic acid', 'Ash',\n...                    'Alcalinity of ash', 'Magnesium',\n...                    'Total phenols', 'Flavanoids',\n...                    'Nonflavanoid phenols',\n...                    'Proanthocyanins',\n...                    'Color intensity', 'Hue',\n...                    'OD280/OD315 of diluted wines',\n...                    'Proline']\n>>> print('Class labels', np.unique(df_wine['Class label']))\nClass labels [1 2 3]\n>>> df_wine.head() \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n>>> X_train, X_test, y_train, y_test =\\\n...     train_test_split(X, y,\n...                      test_size=0.3,\n...                      random_state=0,\n...                      stratify=y) \n```", "```py\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> mms = MinMaxScaler()\n>>> X_train_norm = mms.fit_transform(X_train)\n>>> X_test_norm = mms.transform(X_test) \n```", "```py\n>>> ex = np.array([0, 1, 2, 3, 4, 5])\n>>> print('standardized:', (ex - ex.mean()) / ex.std())\nstandardized: [-1.46385011  -0.87831007  -0.29277002  0.29277002\n0.87831007  1.46385011]\n>>> print('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))\nnormalized: [ 0\\.  0.2  0.4  0.6  0.8  1\\. ] \n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> stdsc = StandardScaler()\n>>> X_train_std = stdsc.fit_transform(X_train)\n>>> X_test_std = stdsc.transform(X_test) \n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> LogisticRegression(penalty='l1',\n...                    solver='liblinear',\n...                    multi_class='ovr') \n```", "```py\n>>> lr = LogisticRegression(penalty='l1',\n...                         C=1.0,\n...                         solver='liblinear',\n...                         multi_class='ovr')\n>>> # Note that C=1.0 is the default. You can increase\n>>> # or decrease it to make the regularization effect\n>>> # stronger or weaker, respectively.\n>>> lr.fit(X_train_std, y_train)\n>>> print('Training accuracy:', lr.score(X_train_std, y_train))\nTraining accuracy: 1.0\n>>> print('Test accuracy:', lr.score(X_test_std, y_test))\nTest accuracy: 1.0 \n```", "```py\n>>> lr.intercept_\n    array([-1.26317363, -1.21537306, -2.37111954]) \n```", "```py\n>>> lr.coef_\narray([[ 1.24647953,  0.18050894,  0.74540443, -1.16301108,\n         0\\.        ,0\\.        ,  1.16243821,  0\\.        ,\n         0\\.        ,  0\\.        , 0\\.        ,  0.55620267,\n         2.50890638],\n       [-1.53919461, -0.38562247, -0.99565934,  0.36390047,\n        -0.05892612, 0\\.        ,  0.66710883,  0\\.        ,\n         0\\.        , -1.9318798 , 1.23775092,  0\\.        ,\n        -2.23280039],\n       [ 0.13557571,  0.16848763,  0.35710712,  0\\.        ,\n         0\\.        , 0\\.        , -2.43804744,  0\\.        ,\n         0\\.        ,  1.56388787, -0.81881015, -0.49217022,\n         0\\.        ]]) \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> fig = plt.figure()\n>>> ax = plt.subplot(111)\n>>> colors = ['blue', 'green', 'red', 'cyan',\n...           'magenta', 'yellow', 'black',\n...           'pink', 'lightgreen', 'lightblue',\n...           'gray', 'indigo', 'orange']\n>>> weights, params = [], []\n>>> for c in np.arange(-4., 6.):\n...     lr = LogisticRegression(penalty='l1', C=10.**c,\n...                             solver='liblinear',\n...                             multi_class='ovr', random_state=0)\n...     lr.fit(X_train_std, y_train)\n...     weights.append(lr.coef_[1])\n...     params.append(10**c)\n>>> weights = np.array(weights)\n>>> for column, color in zip(range(weights.shape[1]), colors):\n...     plt.plot(params, weights[:, column],\n...              label=df_wine.columns[column + 1],\n...              color=color)\n>>> plt.axhline(0, color='black', linestyle='--', linewidth=3)\n>>> plt.xlim([10**(-5), 10**5])\n>>> plt.ylabel('Weight coefficient')\n>>> plt.xlabel('C (inverse regularization strength)')\n>>> plt.xscale('log')\n>>> plt.legend(loc='upper left')\n>>> ax.legend(loc='upper center',\n...           bbox_to_anchor=(1.38, 1.03),\n...           ncol=1, fancybox=True)\n>>> plt.show() \n```", "```py\nfrom sklearn.base import clone\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nclass SBS:\n    def __init__(self, estimator, k_features,\n                 scoring=accuracy_score,\n                 test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n    def fit(self, X, y):\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=self.test_size,\n                             random_state=self.random_state)\n\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train,\n                                 X_test, y_test, self.indices_)\n        self.scores_ = [score]\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n\n            for p in combinations(self.indices_, r=dim - 1):\n                score = self._calc_score(X_train, y_train,\n                                         X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn = KNeighborsClassifier(n_neighbors=5)\n>>> sbs = SBS(knn, k_features=1)\n>>> sbs.fit(X_train_std, y_train) \n```", "```py\n>>> k_feat = [len(k) for k in sbs.subsets_]\n>>> plt.plot(k_feat, sbs.scores_, marker='o')\n>>> plt.ylim([0.7, 1.02])\n>>> plt.ylabel('Accuracy')\n>>> plt.xlabel('Number of features')\n>>> plt.grid()\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> k3 = list(sbs.subsets_[10])\n>>> print(df_wine.columns[1:][k3])\nIndex(['Alcohol', 'Malic acid', 'OD280/OD315 of diluted wines'], dtype='object') \n```", "```py\n>>> knn.fit(X_train_std, y_train)\n>>> print('Training accuracy:', knn.score(X_train_std, y_train))\nTraining accuracy: 0.967741935484\n>>> print('Test accuracy:', knn.score(X_test_std, y_test))\nTest accuracy: 0.962962962963 \n```", "```py\n>>> knn.fit(X_train_std[:, k3], y_train)\n>>> print('Training accuracy:',\n...       knn.score(X_train_std[:, k3], y_train))\nTraining accuracy: 0.951612903226\n>>> print('Test accuracy:',\n...       knn.score(X_test_std[:, k3], y_test))\nTest accuracy: 0.925925925926 \n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> feat_labels = df_wine.columns[1:]\n>>> forest = RandomForestClassifier(n_estimators=500,\n...                                 random_state=1)\n>>> forest.fit(X_train, y_train)\n>>> importances = forest.feature_importances_\n>>> indices = np.argsort(importances)[::-1]\n>>> for f in range(X_train.shape[1]):\n...     print(\"%2d) %-*s %f\" % (f + 1, 30,\n...                             feat_labels[indices[f]],\n...                             importances[indices[f]]))\n>>> plt.title('Feature importance')\n>>> plt.bar(range(X_train.shape[1]),\n...         importances[indices],\n...         align='center')\n>>> plt.xticks(range(X_train.shape[1]),\n...            feat_labels[indices], rotation=90)\n>>> plt.xlim([-1, X_train.shape[1]])\n>>> plt.tight_layout()\n>>> plt.show()\n 1) Proline                         0.185453\n 2) Flavanoids                      0.174751\n 3) Color intensity                 0.143920\n 4) OD280/OD315 of diluted wines    0.136162\n 5) Alcohol                         0.118529\n 6) Hue                             0.058739\n 7) Total phenols                   0.050872\n 8) Magnesium                       0.031357\n 9) Malic acid                      0.025648\n 10) Proanthocyanins                0.025570\n 11) Alcalinity of ash              0.022366\n 12) Nonflavanoid phenols           0.013354\n 13) Ash                            0.013279 \n```", "```py\n>>> from sklearn.feature_selection import SelectFromModel\n>>> sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n>>> X_selected = sfm.transform(X_train)\n>>> print('Number of features that meet this threshold',\n...       'criterion:', X_selected.shape[1])\nNumber of features that meet this threshold criterion: 5\n>>> for f in range(X_selected.shape[1]):\n...     print(\"%2d) %-*s %f\" % (f + 1, 30,\n...                             feat_labels[indices[f]],\n...                             importances[indices[f]]))\n 1) Proline                         0.185453\n 2) Flavanoids                      0.174751\n 3) Color intensity                 0.143920\n 4) OD280/OD315 of diluted wines    0.136162\n 5) Alcohol                         0.118529 \n```"]