- en: 'Chapter 3: GPT-3 and Programming'
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of GPT-3’s NLP capabilities are created in the Python programming
    language. But to enable wider accessibility, the API comes with pre-built support
    for all the major programming languages, so users can build GPT-3 powered applications
    using the programming language of their choice.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will illustrate how this works by replicating an example
    with different programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a heads-up: In each language-specific chapter, we assume you have a basic
    understanding of the programming language being discussed. If you don’t, you can
    safely skip the section.'
  prefs: []
  type: TYPE_NORMAL
- en: How to use OpenAI API with Python?
  prefs: []
  type: TYPE_NORMAL
- en: Python is the most popular language for data science and machine learning tasks.
    Compared to conventional data-science programming languages like R and Stata,
    Python shines because it’s scalable and integrates well with databases. It is
    widely used and has a flourishing community of developers keeping its ecosystem
    up to date. Python is easy to learn and comes with useful data science libraries
    like Numpy and Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pair GPT-3 with Python using a library called [Chronology](https://github.com/OthersideAI/chronology)
    that provides a simple, intuitive interface. Chronology can mitigate the monotonous
    work of writing all of your code from scratch every time. Its features include:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        It calls the OpenAI API asynchronously, allowing you to generate multiple
    prompt completions at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: ●        You can create and modify training prompts easily; for example, modifying
    a training prompt used by a different example is fairly straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: ●        It allows you to chain prompts together by plugging the output of one
    prompt into another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chronology is hosted on PyPI and supports Python 3.6 and above. To install
    the library, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: (base) PS D:\GPT-3 Python> pip install chronological
  prefs: []
  type: TYPE_NORMAL
- en: After installing the Python library via PyPI, let’s look at an example of how
    to prime GPT-3 to summarize a given text document at a second-grade reading level.
    We’ll show you to call the API, send the training prompt as a request, and get
    the summarized completion as an output. We’ve posted the code for you in a [Github
    repository.](https://github.com/Shubhamsaboo/kairos_gpt3/tree/master/Programming_with_GPT-3/GPT-3_Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the following training prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'My second-grader asked me what this passage means:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: Olive oil is a liquid fat obtained from olives (the fruit of Olea europaea;
    family Oleaceae)...
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: 'I rephrased it for him, in plain language a second grader can understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Importing Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from chronological import read_prompt, cleaned_completion, main
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create a function that reads the training prompt and provides the
    completion output. We have made this function asynchronous, which allows us to
    carry out parallel function calls. We will use the following configuration for
    the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Maximum tokens=100
  prefs: []
  type: TYPE_NORMAL
- en: ●        Execution Engine="Davinci"
  prefs: []
  type: TYPE_NORMAL
- en: ●        Temperature=0.5
  prefs: []
  type: TYPE_NORMAL
- en: ●        Top-p=1
  prefs: []
  type: TYPE_NORMAL
- en: ●        Frequency Penalty = 0.2
  prefs: []
  type: TYPE_NORMAL
- en: ●        Stop Sequence = ["\n\n"]
  prefs: []
  type: TYPE_NORMAL
- en: Takes in the training prompt and returns the completed response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'async def summarization_example():'
  prefs: []
  type: TYPE_NORMAL
- en: '# Takes in a text file(summarize_for_a_2nd_grader) as the input prompt'
  prefs: []
  type: TYPE_NORMAL
- en: prompt_summarize = read_prompt('summarize_for_a_2nd_grader')
  prefs: []
  type: TYPE_NORMAL
- en: '# Calling the completion method along with the specific GPT-3 parameters'
  prefs: []
  type: TYPE_NORMAL
- en: completion_summarize = await cleaned_completion(prompt_summarize, max_tokens=100,
    engine="davinci", temperature=0.5, top_p=1, frequency_penalty=0.2, stop=["\n\n"])
  prefs: []
  type: TYPE_NORMAL
- en: '# Return the completion response'
  prefs: []
  type: TYPE_NORMAL
- en: return completion_summarize
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create an asynchronous workflow, invoke that workflow using the
    ‘main’ function provided by the library, and print the output in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing the end-to-end async workflow, capable of running multiple prompts
    in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'async def workflow():'
  prefs: []
  type: TYPE_NORMAL
- en: '# Making async call to the summarization function'
  prefs: []
  type: TYPE_NORMAL
- en: text_summ_example = await summarization_example()
  prefs: []
  type: TYPE_NORMAL
- en: '# Printing the result in console'
  prefs: []
  type: TYPE_NORMAL
- en: print('-------------------------')
  prefs: []
  type: TYPE_NORMAL
- en: 'print(''Basic Example Response: {0}''.format(text_summ_example))'
  prefs: []
  type: TYPE_NORMAL
- en: print('-------------------------')
  prefs: []
  type: TYPE_NORMAL
- en: invoke Chronology by using the main function to run the async workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: main(workflow)
  prefs: []
  type: TYPE_NORMAL
- en: 'Save it as a Python script with the following name ‘text_summarization.py’
    and run it from the terminal to generate the output. You can run the following
    command from your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: (base) PS D:\GPT-3 Python> python text_summarization.py
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you execute the script, your console should print  the following summary
    of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic Example Response: Olive oil is a liquid fat that comes from olives. Olives
    grow on a tree called an olive tree. The olive tree is the most common tree in
    the Mediterranean. People use the oil to cook with, to put on their salads, and
    as a fuel for lamps.'
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: If you are not well versed in Python and want to chain different prompts without
    writing code, you can use the [no-code interface](https://chronology-ui.vercel.app/)
    built on top of the [Chronology library](https://github.com/OthersideAI/chronology-ui)
    to create the prompt workflow using drag-and-drop. See our GitHub [repository](https://github.com/Shubhamsaboo/kairos_gpt3/tree/master/Programming_with_GPT-3/GPT-3_Python)
    for more examples of how you can use Python programming to interact with GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: How to use OpenAI API with Go?
  prefs: []
  type: TYPE_NORMAL
- en: Go is an open-source programming language that incorporates elements from other
    languages to create a powerful, efficient, and user-friendly tool. Many developers
    refer to it as a modern version of C.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go is the language of preference for building projects that require high security,
    high speed, and high modularity. This makes it an attractive option for many projects
    in the fintech industry. Key features of Go are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Ease of use
  prefs: []
  type: TYPE_NORMAL
- en: ●        State-of-the-art productivity
  prefs: []
  type: TYPE_NORMAL
- en: ●        High-level efficiency Static typing
  prefs: []
  type: TYPE_NORMAL
- en: ●        Advanced performance for networking
  prefs: []
  type: TYPE_NORMAL
- en: ●        Full use of multi-core power
  prefs: []
  type: TYPE_NORMAL
- en: If you are completely new to Go and want to give it a try, you can [follow the
    documentation](https://golang.org/doc/install) to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done with the installation and understand the basics of Go programming,
    you can follow these steps to u use the [Go API wrapper for GPT-3](https://github.com/sashabaranov/go-gpt3).
    To learn more about creating Go modules, see [this tutorial](https://golang.org/doc/tutorial/create-module).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you’ll create a module to track and import code dependencies. Create
    and initialize the “gogpt” module using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: D:\GPT-3 Go> go mod init gogpt
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the “gogpt” module, let’s point it to [this github repository](http://github.com/sashabaranov/go-gpt3)
    to download the necessary dependencies and packages for working with the API.
    Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: D:\GPT-3 Go> go get github.com/sashabaranov/go-gpt3
  prefs: []
  type: TYPE_NORMAL
- en: 'go get: added github.com/sashabaranov/go-gpt3 v0.0.0-20210606183212-2be4a268a894'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the same text summarization example as in the previous section. (You
    can find all the code at the following [repository](https://github.com/Shubhamsaboo/kairos_gpt3/tree/master/Programming_with_GPT-3/GPT-3_Go).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the necessary dependencies and packages for starters:'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the package main
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: package main
  prefs: []
  type: TYPE_NORMAL
- en: Importing Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import (
  prefs: []
  type: TYPE_NORMAL
- en: '"fmt"'
  prefs: []
  type: TYPE_NORMAL
- en: '"io/ioutil"'
  prefs: []
  type: TYPE_NORMAL
- en: '"context"'
  prefs: []
  type: TYPE_NORMAL
- en: gogpt "github.com/sashabaranov/go-gpt3"
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Go programming organizes source files into system directories called packages,
    which make it easier to reuse code across Go applications. In the first line of
    the code we call the package "main" and tell the Go compiler that the package
    should compile as an executable program instead of a shared library.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: In Go, you create a package as a shared library for reusable code, and
    the "main" package for executable programs. The "main" function within the package
    serves as the entry point for the program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’ll create a main function that will host the entire logic of reading
    the training prompt and providing the completion output. Use the following configuration
    for the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Maximum tokens=100
  prefs: []
  type: TYPE_NORMAL
- en: ●        Execution Engine="davinci"
  prefs: []
  type: TYPE_NORMAL
- en: ●        Temperature=0.5
  prefs: []
  type: TYPE_NORMAL
- en: ●        Top-p=1
  prefs: []
  type: TYPE_NORMAL
- en: ●        Frequency Penalty = 0.2
  prefs: []
  type: TYPE_NORMAL
- en: ●        Stop Sequence = ["\n\n"]
  prefs: []
  type: TYPE_NORMAL
- en: func main() {
  prefs: []
  type: TYPE_NORMAL
- en: c := gogpt.NewClient("OPENAI-API-KEY")
  prefs: []
  type: TYPE_NORMAL
- en: ctx := context.Background()
  prefs: []
  type: TYPE_NORMAL
- en: prompt, err := ioutil.ReadFile("prompts/summarize_for_a_2nd_grader.txt")
  prefs: []
  type: TYPE_NORMAL
- en: req := gogpt.CompletionRequest{
  prefs: []
  type: TYPE_NORMAL
- en: 'MaxTokens: 100,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temperature: 0.5,'
  prefs: []
  type: TYPE_NORMAL
- en: 'TopP: 1.0,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop: []string{"\n\n"},'
  prefs: []
  type: TYPE_NORMAL
- en: 'FrequencyPenalty: 0.2,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: string(prompt),'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: resp, err := c.CreateCompletion(ctx, "davinci", req)
  prefs: []
  type: TYPE_NORMAL
- en: if err != nil {
  prefs: []
  type: TYPE_NORMAL
- en: return
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: fmt.Println("-------------------------")
  prefs: []
  type: TYPE_NORMAL
- en: fmt.Println(resp.Choices[0].Text)
  prefs: []
  type: TYPE_NORMAL
- en: fmt.Println("-------------------------")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Sets up a new API client by providing it with the API token and then leaves
    it to run in the background.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reads the prompt “” in the form of a text file from the prompts folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates a completion request by providing the training prompt and specifying
    the value API parameters (like temperature, top-p, stop sequence, and so forth).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls the create completion function and provides it with the API client, completion
    request, and execution engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generates a response in the form of a completion, which prints towards the end
    in the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can then save the code file as ‘text_summarization.go’ and run it from
    the terminal to generate the output. Use the following command to run the file
    from your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: (base) PS D:\GPT-3 Go> go run text_summarization.go
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you execute the file, your console will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: Olive oil is a liquid fat that comes from olives. Olives grow on a tree called
    an olive tree. The olive tree is the most common tree in the Mediterranean. People
    use the oil to cook with, to put on their salads, and as a fuel for lamps.
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: For more examples of how you can use Go programming to interact with GPT-3,
    please visit our GitHub [repository](https://github.com/Shubhamsaboo/kairos_gpt3/tree/master/Programming_with_GPT-3/GPT-3_Go).
  prefs: []
  type: TYPE_NORMAL
- en: How to use OpenAI API with Java?
  prefs: []
  type: TYPE_NORMAL
- en: Java is one of the oldest and most popular programming languages for developing
    conventional software systems; it is also a platform that comes with a runtime
    environment. It was developed by Sun Microsystems (now a subsidiary of Oracle)
    in 1995, and as of today, more than 3 billion devices run on it. It is a general-purpose,
    class-based, object-oriented programming language designed to have fewer implementation
    dependencies. Its syntax is similar to that of C and C++. Two-thirds of the software
    industry still uses Java as its core programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use our olive-oil text summarization example once more. As we did with
    Python and Go, we’ll show you how to call the API, send the training prompt as
    a request, and get the summarized completion as an output using Java.
  prefs: []
  type: TYPE_NORMAL
- en: For a step-by-step code walkthrough on your local machine, clone our GitHub
    [repository](https://github.com/Shubhamsaboo/kairos_gpt3). In the cloned repository
    go to Programming_with_GPT-3 folder and open the GPT-3_Java folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import all the relevant dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: package example;
  prefs: []
  type: TYPE_NORMAL
- en: // Importing Dependencies
  prefs: []
  type: TYPE_NORMAL
- en: import java.util.*;
  prefs: []
  type: TYPE_NORMAL
- en: import java.io.*;
  prefs: []
  type: TYPE_NORMAL
- en: import com.theokanning.openai.OpenAiService;
  prefs: []
  type: TYPE_NORMAL
- en: import com.theokanning.openai.completion.CompletionRequest;
  prefs: []
  type: TYPE_NORMAL
- en: import com.theokanning.openai.engine.Engine;
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’ll create a class named OpenAiApiExample. All of your code will be
    a part of it. Under that class, first create an OpenAiService object using the
    API token:'
  prefs: []
  type: TYPE_NORMAL
- en: class OpenAiApiExample {
  prefs: []
  type: TYPE_NORMAL
- en: public static void main(String... args) throws FileNotFoundException {
  prefs: []
  type: TYPE_NORMAL
- en: String token = "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";
  prefs: []
  type: TYPE_NORMAL
- en: OpenAiService service = new OpenAiService(token);
  prefs: []
  type: TYPE_NORMAL
- en: 'The connection to OpenAI API is now established in the form of a service object.
    Read the training prompt from the prompts folder:'
  prefs: []
  type: TYPE_NORMAL
- en: // Reading the training prompt from the prompts folder
  prefs: []
  type: TYPE_NORMAL
- en: File file = new File("D:\\GPT-3 Book\\Programming with GPT-3\\GPT-3
  prefs: []
  type: TYPE_NORMAL
- en: Java\\example\\src\\main\\java\\example\\prompts\\summarize_for_a_2nd_grader.txt");
  prefs: []
  type: TYPE_NORMAL
- en: Scanner sc = new Scanner(file);
  prefs: []
  type: TYPE_NORMAL
- en: // we just need to use \\Z as delimiter
  prefs: []
  type: TYPE_NORMAL
- en: sc.useDelimiter("\\Z");
  prefs: []
  type: TYPE_NORMAL
- en: // pp is the string consisting of the training prompt
  prefs: []
  type: TYPE_NORMAL
- en: String pp = sc.next();
  prefs: []
  type: TYPE_NORMAL
- en: '​Then  you can create a completion request with the following configuration
    for the API parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Maximum tokens=100
  prefs: []
  type: TYPE_NORMAL
- en: ●        Execution Engine="Davinci"
  prefs: []
  type: TYPE_NORMAL
- en: ●        Temperature=0.5
  prefs: []
  type: TYPE_NORMAL
- en: ●        Top-p=1
  prefs: []
  type: TYPE_NORMAL
- en: ●        Frequency Penalty = 0.2
  prefs: []
  type: TYPE_NORMAL
- en: ●        Stop Sequence = ["\n\n"]
  prefs: []
  type: TYPE_NORMAL
- en: // Creating a list of strings to used as stop sequence
  prefs: []
  type: TYPE_NORMAL
- en: List<String> li = new ArrayList<String>();
  prefs: []
  type: TYPE_NORMAL
- en: li.add("\n\n'''");
  prefs: []
  type: TYPE_NORMAL
- en: // Creating a completion request with the API parameters
  prefs: []
  type: TYPE_NORMAL
- en: CompletionRequest completionRequest = CompletionRequest.builder().prompt(pp).maxTokens(100).temperature(0.5).topP(1.0).frequencyPenalty(0.2).stop(li).echo(true).build();
  prefs: []
  type: TYPE_NORMAL
- en: // Using the service object to fetch the completion response
  prefs: []
  type: TYPE_NORMAL
- en: service.createCompletion("davinci",completionRequest).getChoices().forEach(System.out::println);
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the code file as ‘text_summarization.java’ and run it from the terminal
    to generate the output. You can use the following command to run the file from
    your root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: (base) PS D:\GPT-3 Java> ./gradlew example:run
  prefs: []
  type: TYPE_NORMAL
- en: Your console should print the same summary as it did with the previous examples.For
    more examples of how you can use Java programming to interact with GPT-3, see
    our GitHub [repository](https://github.com/Shubhamsaboo/kairos_gpt3/tree/master/Programming_with_GPT-3/GPT-3_Java).
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 Sandbox Powered by Streamlit
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will walk you through the GPT-3 Sandbox, an open-source tool
    we’ve created to help you turn your ideas into reality with just a few lines of
    Python code. We’ll show you how to use it and how to customize it for your specific
    application.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our sandbox is to empower you to create cool web applications, no
    matter what your technical background. It is built on top of the Streamlit framework.
  prefs: []
  type: TYPE_NORMAL
- en: To accompany this book, we have also created a [video series](https://www.youtube.com/playlist?list=PLHdP3OXYnDmi1m3EQ76IrLoyJj3CHhC4M)
    with a step-by-step instructions for creating and deploying your GPT-3 application,
    which you can access by scanning the QR code in Figure 3-1.  Please follow it
    as you read this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. QR code for GPT-3 Sandbox video series
  prefs: []
  type: TYPE_NORMAL
- en: 'We use VSCode as the IDE for our examples, but feel free to use any IDE. You’ll
    need to install the IDE before you start. Please also make sure you are running
    Python version 3.7 or above. You can confirm which version you have installed
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: python --version
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the code from this [repository](https://github.com/Shubhamsaboo/kairos_gpt3)
    by opening a new terminal in your IDE and using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: git clone [https://github.com/Shubhamsaboo/kairos_gpt3](https://github.com/Shubhamsaboo/kairos_gpt3)
  prefs: []
  type: TYPE_NORMAL
- en: 'After cloning the repository the code structure in your IDE should now look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Sandbox file directory structure
  prefs: []
  type: TYPE_NORMAL
- en: Everything you need to create and deploy a web application is already present
    in the code. You just need to tweak a few files to customize the sandbox for your
    specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Create a [Python virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/),
    which you’ll name env. Then you can install the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the email_generation folder. Your path should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: (env) kairos_gpt3\GPT-3 Sandbox\email_generation>
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: (env) kairos_gpt3\GPT-3 Sandbox\email_generation> pip install -r requirements.txt
  prefs: []
  type: TYPE_NORMAL
- en: Now you can start customizing the sandbox code. The first file that you need
    to look at is training_data.py. Open that file and replace the default prompt
    with the training prompt you want to use. You can use the GPT-3 playground to
    experiment with different training prompts (see chapter 2 and our [video](https://www.youtube.com/watch?v=YGKY9Mc24MA&list=PLHdP3OXYnDmi1m3EQ76IrLoyJj3CHhC4M&index=3)
    for more on customizing the sandbox).
  prefs: []
  type: TYPE_NORMAL
- en: You’re now ready to tweak the API parameters (Maximum tokens, Execution Engine,
    Temperature, Top-p, Frequency Penalty, Stop Sequence) as per the requirements
    of your application use case. We recommend experimenting with different values
    of API parameters for a given training prompt in the playground to determine what
    values will work best for your use case. Once you get satisfactory results then
    you can alter the values in the training_service.py file.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it! Your GPT-3 based web application is now ready. You can run it locally
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: (env) kairos_gpt3\GPT-3 Sandbox\email_generation> streamlit run gpt_app.py
  prefs: []
  type: TYPE_NORMAL
- en: Check to make sure it works, and then you can deploy the application to the
    internet using Streamlit sharing to showcase it to a wider audience. Our [video](https://www.youtube.com/watch?v=IO2ndhOoTfc&list=PLHdP3OXYnDmi1m3EQ76IrLoyJj3CHhC4M&index=4)
    offers a full deployment walkthrough.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: This application follows a simple workflow, where the training prompt
    receives single input from the UI and comes up with the response. If your application
    requires a more complex workflow, where the training prompt takes in multiple
    inputs, customize the UI elements by going through the scripts app1.py, app2.py,
    and gpt_app.py. For details, refer to the [Streamlit documentation](https://docs.streamlit.io).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next few chapters, we will explore different applications of GPT-3 and
    leverage this sandbox to create easily deployable web applications.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use the OpenAI API with the programming languages
    Python, Go, and Java. We also walked through a low-code sandbox environment created
    using Streamlit that will help you to quickly turn your idea into an application.
    Lastly, we looked at the key requirements to go live with a GPT-3 application.
    This chapter provided you with the programming outlook of the API; going forward
    we’ll dive deeper into the burgeoning ecosystem empowered by GPT-3.
  prefs: []
  type: TYPE_NORMAL
