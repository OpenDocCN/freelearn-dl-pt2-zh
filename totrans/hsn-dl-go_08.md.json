["```py\n<1 x label><3072 x pixel>\n<1 x label><3072 x pixel>\n<1 x label><3072 x pixel>\n<1 x label><3072 x pixel>\n<1 x label><3072 x pixel>\n<1 x label><3072 x pixel>\n ...\n<1 x label><3072 x pixel>\n```", "```py\ncase \"train\":\n    arrayFiles = []string{\n        \"data_batch_1.bin\",\n        \"data_batch_2.bin\",\n        \"data_batch_3.bin\",\n        \"data_batch_4.bin\",\n        \"data_batch_5.bin\",\n    }\ncase \"test\":\n    arrayFiles = []string{\n        \"test_batch.bin\",\n    }\n}\n```", "```py\nf, err := os.Open(filepath.Join(loc, targetFile))\nif err != nil {\n    log.Fatal(err)\n}\n\ndefer f.Close()\ncifar, err := ioutil.ReadAll(f)\n\nif err != nil {\n    log.Fatal(err)\n}\n\nfor index, element := range cifar {\n    if index%3073 == 0 {\n        labelSlice = append(labelSlice, float64(element))\n    } else {\n        imageSlice = append(imageSlice, pixelWeight(element))\n    }\n}\n```", "```py\nfunc pixelWeight(px byte) float64 {\n    retVal := float64(px)/pixelRange*0.9 + 0.1\n    if retVal == 1.0 {\n        return 0.999\n    }\n    return retVal\n}\n```", "```py\nlabelBacking := make([]float64, len(labelSlice)*numLabels, len(labelSlice)*numLabels)\nlabelBacking = labelBacking[:0]\nfor i := 0; i < len(labelSlice); i++ {\n    for j := 0; j < numLabels; j++ {\n        if j == int(labelSlice[i]) {\n            labelBacking = append(labelBacking, 0.9)\n        } else {\n            labelBacking = append(labelBacking, 0.1)\n        }\n    }\n}\n```", "```py\nfunc Load(typ, loc string) (inputs, targets tensor.Tensor, err error) {\n\n    ...\n\n    inputs = tensor.New(tensor.WithShape(len(labelSlice), 3, 32, 32),        tensor.WithBacking(imageSlice))\n    targets = tensor.New(tensor.WithShape(len(labelSlice), numLabels), tensor.WithBacking(labelBacking))\n    return\n}\n```", "```py\nif inputs, targets, err = cifar.Load(\"train\", loc); err != nil {\n    log.Fatal(err)\n}\n```", "```py\nvar (\n    epochs = flag.Int(\"epochs\", 10, \"Number of epochs to train for\")\n    dataset = flag.String(\"dataset\", \"train\", \"Which dataset to train on? Valid options are \\\"train\\\" or \\\"test\\\"\")\n    dtype = flag.String(\"dtype\", \"float64\", \"Which dtype to use\")\n    batchsize = flag.Int(\"batchsize\", 100, \"Batch size\")\n    cpuprofile = flag.String(\"cpuprofile\", \"\", \"CPU profiling\")\n)\n```", "```py\n // get label\n    yRowT, _ := yVal.Slice(sli{j, j + 1})\n    yRow := yRowT.Data().([]float64)\n    var rowLabel int\n    var yRowHigh float64\n\n    for k := 0; k < 10; k++ {\n        if k == 0 {\n            rowLabel = 0\n            yRowHigh = yRow[k]\n        } else if yRow[k] > yRowHigh {\n            rowLabel = k\n            yRowHigh = yRow[k]\n        }\n    }\n```", "```py\nyOutput2 := tensor.New(tensor.WithShape(bs, 10), tensor.WithBacking(arrayOutput2))\n\n // get prediction\n    predRowT, _ := yOutput2.Slice(sli{j, j + 1})\n    predRow := predRowT.Data().([]float64)\n    var rowGuess int\n    var predRowHigh float64\n\n    // guess result\n    for k := 0; k < 10; k++ {\n        if k == 0 {\n            rowGuess = 0\n            predRowHigh = predRow[k]\n        } else if predRow[k] > predRowHigh {\n            rowGuess = k\n            predRowHigh = predRow[k]\n        }\n    }\n```", "```py\nif rowLabel == rowGuess {\n    accuracyGuess += 1.0 / float64(numExamples)\n}\n```", "```py\n// Layer 0\nif c0, err = gorgonia.Conv2d(x, m.w0, tensor.Shape{5, 5}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n    return errors.Wrap(err, \"Layer 0 Convolution failed\")\n}\nif a0, err = gorgonia.Rectify(c0); err != nil {\n    return errors.Wrap(err, \"Layer 0 activation failed\")\n}\nif p0, err = gorgonia.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n    return errors.Wrap(err, \"Layer 0 Maxpooling failed\")\n}\nif l0, err = gorgonia.Dropout(p0, m.d0); err != nil {\n    return errors.Wrap(err, \"Unable to apply a dropout\")\n}\n```", "```py\n// Layer 1\nif c1, err = gorgonia.Conv2d(l0, m.w1, tensor.Shape{5, 5}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n    return errors.Wrap(err, \"Layer 1 Convolution failed\")\n}\nif a1, err = gorgonia.Rectify(c1); err != nil {\n    return errors.Wrap(err, \"Layer 1 activation failed\")\n}\nif p1, err = gorgonia.MaxPool2D(a1, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n    return errors.Wrap(err, \"Layer 1 Maxpooling failed\")\n}\nif l1, err = gorgonia.Dropout(p1, m.d1); err != nil {\n    return errors.Wrap(err, \"Unable to apply a dropout to layer 1\")\n}\n```", "```py\n// Layer 2\nif c2, err = gorgonia.Conv2d(l1, m.w2, tensor.Shape{5, 5}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n    return errors.Wrap(err, \"Layer 2 Convolution failed\")\n}\nif a2, err = gorgonia.Rectify(c2); err != nil {\n    return errors.Wrap(err, \"Layer 2 activation failed\")\n}\nif p2, err = gorgonia.MaxPool2D(a2, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n    return errors.Wrap(err, \"Layer 2 Maxpooling failed\")\n}\n\nvar r2 *gorgonia.Node\nb, c, h, w := p2.Shape()[0], p2.Shape()[1], p2.Shape()[2], p2.Shape()[3]\nif r2, err = gorgonia.Reshape(p2, tensor.Shape{b, c * h * w}); err != nil {\n    return errors.Wrap(err, \"Unable to reshape layer 2\")\n}\nif l2, err = gorgonia.Dropout(r2, m.d2); err != nil {\n    return errors.Wrap(err, \"Unable to apply a dropout on layer 2\")\n}\n```", "```py\n// Layer 3\nlog.Printf(\"l2 shape %v\", l2.Shape())\nlog.Printf(\"w3 shape %v\", m.w3.Shape())\nif fc, err = gorgonia.Mul(l2, m.w3); err != nil {\n    return errors.Wrapf(err, \"Unable to multiply l2 and w3\")\n}\nif a3, err = gorgonia.Rectify(fc); err != nil {\n    return errors.Wrapf(err, \"Unable to activate fc\")\n}\nif l3, err = gorgonia.Dropout(a3, m.d3); err != nil {\n    return errors.Wrapf(err, \"Unable to apply a dropout on layer 3\")\n}\n```", "```py\nlosses := gorgonia.Must(gorgonia.HadamardProd(gorgonia.Must(gorgonia.Log(m.out)), y))\ncost := gorgonia.Must(gorgonia.Sum(losses))\ncost = gorgonia.Must(gorgonia.Neg(cost))\n\nif _, err = gorgonia.Grad(cost, m.learnables()...); err != nil {\n    log.Fatal(err)\n}\n```", "```py\nvm := gorgonia.NewTapeMachine(g, gorgonia.WithPrecompiled(prog, locMap), gorgonia.BindDualValues(m.learnables()...))\nsolver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))\n```", "```py\nif inputs, targets, err = cifar.Load(\"test\", loc); err != nil {\n    log.Fatal(err)\n}\n```", "```py\nbatches = inputs.Shape()[0] / bs\nbar = pb.New(batches)\nbar.SetRefreshRate(time.Second)\nbar.SetMaxWidth(80)\n```", "```py\n// slices to store our output\nvar testActual, testPred []int\n\n// store our output into the slices within the loop\ntestActual = append(testActual, rowLabel)\ntestPred = append(testPred, rowGuess)\n```", "```py\nprintIntSlice(\"testActual.txt\", testActual)\nprintIntSlice(\"testPred.txt\", testPred)\n```", "```py\ngo build -tags='cuda'\n```", "```py\nnvidia-smi -l 1\n```", "```py\n2018/12/30 13:23:36 Batches 500\n2018/12/30 13:26:23 Epoch 0 |\n2018/12/30 13:29:15 Epoch 1 |\n2018/12/30 13:32:01 Epoch 2 |\n2018/12/30 13:34:47 Epoch 3 |\n2018/12/30 13:37:33 Epoch 4 |\n2018/12/30 13:40:19 Epoch 5 |\n2018/12/30 13:43:05 Epoch 6 |\n2018/12/30 13:45:50 Epoch 7 |\n2018/12/30 13:48:36 Epoch 8 |\n2018/12/30 13:51:22 Epoch 9 |\n2018/12/30 13:51:55 Epoch Test |\n```", "```py\n2018/12/30 12:57:56 Batches 500\n2018/12/30 13:00:24 Epoch 0\n2018/12/30 13:02:49 Epoch 1\n2018/12/30 13:05:15 Epoch 2\n2018/12/30 13:07:40 Epoch 3\n2018/12/30 13:10:04 Epoch 4\n2018/12/30 13:12:29 Epoch 5\n2018/12/30 13:14:55 Epoch 6\n2018/12/30 13:17:21 Epoch 7\n2018/12/30 13:19:45 Epoch 8\n2018/12/30 13:22:10 Epoch 9\n2018/12/30 13:22:40 Epoch Test\n```", "```py\nif c0, err = nnops.Conv2d(x, m.w0, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n    return errors.Wrap(err, \"Layer 0 Convolution failed\")\n}\nif a0, err = nnops.Rectify(c0); err != nil {\n    return errors.Wrap(err, \"Layer 0 activation failed\")\n}\nif p0, err = nnops.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n    return errors.Wrap(err, \"Layer 0 Maxpooling failed\")\n}\nif l0, err = nnops.Dropout(p0, m.d0); err != nil {\n    return errors.Wrap(err, \"Unable to apply a dropout\")\n}\n```"]