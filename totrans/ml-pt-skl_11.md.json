["```py\n>>> from sklearn.datasets import fetch_openml\n>>> X, y = fetch_openml('mnist_784', version=1,\n...                     return_X_y=True)\n>>> X = X.values\n>>> y = y.astype(int).values \n```", "```py\n>>> print(X.shape)\n(70000, 784)\n>>> print(y.shape)\n(70000,) \n```", "```py\n>>> X = ((X / 255.) - .5) * 2 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots(nrows=2, ncols=5,\n...                        sharex=True, sharey=True)\n>>> ax = ax.flatten()\n>>> for i in range(10):\n...     img = X[y == i][0].reshape(28, 28)\n...     ax[i].imshow(img, cmap='Greys')\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> fig, ax = plt.subplots(nrows=5,\n...                        ncols=5,\n...                        sharex=True,\n...                        sharey=True)\n>>> ax = ax.flatten()\n>>> for i in range(25):\n...     img = X[y == 7][i].reshape(28, 28)\n...     ax[i].imshow(img, cmap='Greys')\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X_temp, X_test, y_temp, y_test = train_test_split(\n...     X, y, test_size=10000, random_state=123, stratify=y\n... )\n>>> X_train, X_valid, y_train, y_valid = train_test_split(\n...     X_temp, y_temp, test_size=5000,\n...     random_state=123, stratify=y_temp\n... ) \n```", "```py\nfrom neuralnet import NeuralNetMLP \n```", "```py\nimport numpy as np\ndef sigmoid(z):\n    return 1\\. / (1\\. + np.exp(-z))\ndef int_to_onehot(y, num_labels):\n    ary = np.zeros((y.shape[0], num_labels))\n    for i, val in enumerate(y):\n        ary[i, val] = 1\n    return ary \n```", "```py\nclass NeuralNetMLP:\n    def __init__(self, num_features, num_hidden,\n                 num_classes, random_seed=123):\n        super().__init__()\n\n        self.num_classes = num_classes\n\n        # hidden\n        rng = np.random.RandomState(random_seed)\n\n        self.weight_h = rng.normal(\n            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n        self.bias_h = np.zeros(num_hidden)\n\n        # output\n        self.weight_out = rng.normal(\n            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n        self.bias_out = np.zeros(num_classes) \n```", "```py\n def forward(self, x):\n        # Hidden layer\n\n        # input dim: [n_hidden, n_features]\n        #        dot [n_features, n_examples] .T\n        # output dim: [n_examples, n_hidden]\n        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n        a_h = sigmoid(z_h)\n        # Output layer\n        # input dim: [n_classes, n_hidden]\n        #        dot [n_hidden, n_examples] .T\n        # output dim: [n_examples, n_classes]\n        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n        a_out = sigmoid(z_out)\n        return a_h, a_out \n```", "```py\n def backward(self, x, a_h, a_out, y):\n\n        #########################\n        ### Output layer weights\n        #########################\n\n        # one-hot encoding\n        y_onehot = int_to_onehot(y, self.num_classes)\n        # Part 1: dLoss/dOutWeights\n        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n        ## for convenient re-use\n\n        # input/output dim: [n_examples, n_classes]\n        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n        # input/output dim: [n_examples, n_classes]\n        d_a_out__d_z_out = a_out * (1\\. - a_out) # sigmoid derivative\n        # output dim: [n_examples, n_classes]\n        delta_out = d_loss__d_a_out * d_a_out__d_z_out\n        # gradient for output weights\n\n        # [n_examples, n_hidden]\n        d_z_out__dw_out = a_h\n\n        # input dim: [n_classes, n_examples]\n        #           dot [n_examples, n_hidden]\n        # output dim: [n_classes, n_hidden]\n        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n        d_loss__db_out = np.sum(delta_out, axis=0)\n\n        #################################\n        # Part 2: dLoss/dHiddenWeights\n        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet\n        #    * dHiddenNet/dWeight\n\n        # [n_classes, n_hidden]\n        d_z_out__a_h = self.weight_out\n\n        # output dim: [n_examples, n_hidden]\n        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n\n        # [n_examples, n_hidden]\n        d_a_h__d_z_h = a_h * (1\\. - a_h) # sigmoid derivative\n\n        # [n_examples, n_features]\n        d_z_h__d_w_h = x\n\n        # output dim: [n_hidden, n_features]\n        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T,\n                                d_z_h__d_w_h)\n        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n        return (d_loss__dw_out, d_loss__db_out,\n                d_loss__d_w_h, d_loss__d_b_h) \n```", "```py\n>>> model = NeuralNetMLP(num_features=28*28,\n...                      num_hidden=50,\n...                      num_classes=10) \n```", "```py\n>>> import numpy as np\n>>> num_epochs = 50\n>>> minibatch_size = 100\n>>> def minibatch_generator(X, y, minibatch_size):\n...     indices = np.arange(X.shape[0])\n...     np.random.shuffle(indices)\n...     for start_idx in range(0, indices.shape[0] - minibatch_size\n...                            + 1, minibatch_size):\n...         batch_idx = indices[start_idx:start_idx + minibatch_size]\n...         yield X[batch_idx], y[batch_idx] \n```", "```py\n>>> # iterate over training epochs\n>>> for i in range(num_epochs):\n...     # iterate over minibatches\n...     minibatch_gen = minibatch_generator(\n...         X_train, y_train, minibatch_size)\n...     for X_train_mini, y_train_mini in minibatch_gen:\n...         break\n...     break\n>>> print(X_train_mini.shape)\n(100, 784)\n>>> print(y_train_mini.shape)\n(100,) \n```", "```py\n>>> def mse_loss(targets, probas, num_labels=10):\n...     onehot_targets = int_to_onehot(\n...         targets, num_labels=num_labels\n...     )\n...     return np.mean((onehot_targets - probas)**2)\n>>> def accuracy(targets, predicted_labels):\n...     return np.mean(predicted_labels == targets) \n```", "```py\n>>> _, probas = model.forward(X_valid)\n>>> mse = mse_loss(y_valid, probas)\n>>> print(f'Initial validation MSE: {mse:.1f}')\nInitial validation MSE: 0.3\n>>> predicted_labels = np.argmax(probas, axis=1)\n>>> acc = accuracy(y_valid, predicted_labels)\n>>> print(f'Initial validation accuracy: {acc*100:.1f}%')\nInitial validation accuracy: 9.4% \n```", "```py\n>>> def compute_mse_and_acc(nnet, X, y, num_labels=10,\n...                         minibatch_size=100):\n...     mse, correct_pred, num_examples = 0., 0, 0\n...     minibatch_gen = minibatch_generator(X, y, minibatch_size)\n...     for i, (features, targets) in enumerate(minibatch_gen):\n...         _, probas = nnet.forward(features)\n...         predicted_labels = np.argmax(probas, axis=1)\n...         onehot_targets = int_to_onehot(\n...             targets, num_labels=num_labels\n...         )\n...         loss = np.mean((onehot_targets - probas)**2)\n...         correct_pred += (predicted_labels == targets).sum()\n...         num_examples += targets.shape[0]\n...         mse += loss\n...     mse = mse/i\n...     acc = correct_pred/num_examples\n...     return mse, acc \n```", "```py\n>>> mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n>>> print(f'Initial valid MSE: {mse:.1f}')\nInitial valid MSE: 0.3\n>>> print(f'Initial valid accuracy: {acc*100:.1f}%')\nInitial valid accuracy: 9.4% \n```", "```py\n>>> def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n...           learning_rate=0.1):\n...     epoch_loss = []\n...     epoch_train_acc = []\n...     epoch_valid_acc = []\n...\n...     for e in range(num_epochs):\n...         # iterate over minibatches\n...         minibatch_gen = minibatch_generator(\n...             X_train, y_train, minibatch_size)\n...         for X_train_mini, y_train_mini in minibatch_gen:\n...             #### Compute outputs ####\n...             a_h, a_out = model.forward(X_train_mini)\n...             #### Compute gradients ####\n...             d_loss__d_w_out, d_loss__d_b_out, \\\n...             d_loss__d_w_h, d_loss__d_b_h = \\\n...                 model.backward(X_train_mini, a_h, a_out,\n...                                y_train_mini)\n...\n...             #### Update weights ####\n...             model.weight_h -= learning_rate * d_loss__d_w_h\n...             model.bias_h -= learning_rate * d_loss__d_b_h\n...             model.weight_out -= learning_rate * d_loss__d_w_out\n...             model.bias_out -= learning_rate * d_loss__d_b_out\n...         \n...         #### Epoch Logging ####\n...         train_mse, train_acc = compute_mse_and_acc(\n...             model, X_train, y_train\n...         )\n...         valid_mse, valid_acc = compute_mse_and_acc(\n...             model, X_valid, y_valid\n...         )\n...         train_acc, valid_acc = train_acc*100, valid_acc*100\n...         epoch_train_acc.append(train_acc)\n...         epoch_valid_acc.append(valid_acc)\n...         epoch_loss.append(train_mse)\n...         print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n...               f'| Train MSE: {train_mse:.2f} '\n...               f'| Train Acc: {train_acc:.2f}% '\n...               f'| Valid Acc: {valid_acc:.2f}%')\n...\n...     return epoch_loss, epoch_train_acc, epoch_valid_acc \n```", "```py\nmodel.weight_h -= learning_rate * d_loss__d_w_h \n```", "```py\n>>> np.random.seed(123) # for the training set shuffling\n>>> epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n...     model, X_train, y_train, X_valid, y_valid,\n...     num_epochs=50, learning_rate=0.1) \n```", "```py\nEpoch: 001/050 | Train MSE: 0.05 | Train Acc: 76.17% | Valid Acc: 76.02%\nEpoch: 002/050 | Train MSE: 0.03 | Train Acc: 85.46% | Valid Acc: 84.94%\nEpoch: 003/050 | Train MSE: 0.02 | Train Acc: 87.89% | Valid Acc: 87.64%\nEpoch: 004/050 | Train MSE: 0.02 | Train Acc: 89.36% | Valid Acc: 89.38%\nEpoch: 005/050 | Train MSE: 0.02 | Train Acc: 90.21% | Valid Acc: 90.16%\n...\nEpoch: 048/050 | Train MSE: 0.01 | Train Acc: 95.57% | Valid Acc: 94.58%\nEpoch: 049/050 | Train MSE: 0.01 | Train Acc: 95.55% | Valid Acc: 94.54%\nEpoch: 050/050 | Train MSE: 0.01 | Train Acc: 95.59% | Valid Acc: 94.74% \n```", "```py\n>>> plt.plot(range(len(epoch_loss)), epoch_loss)\n>>> plt.ylabel('Mean squared error')\n>>> plt.xlabel('Epoch')\n>>> plt.show() \n```", "```py\n>>> plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n...          label='Training')\n>>> plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n...          label='Validation')\n>>> plt.ylabel('Accuracy')\n>>> plt.xlabel('Epochs')\n>>> plt.legend(loc='lower right')\n>>> plt.show() \n```", "```py\n>>> test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)\n>>> print(f'Test accuracy: {test_acc*100:.2f}%')\nTest accuracy: 94.51% \n```", "```py\n>>> X_test_subset = X_test[:1000, :]\n>>> y_test_subset = y_test[:1000]\n>>> _, probas = model.forward(X_test_subset)\n>>> test_pred = np.argmax(probas, axis=1)\n>>> misclassified_images = \\\n...      X_test_subset[y_test_subset != test_pred][:25]\n>>> misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n>>> correct_labels = y_test_subset[y_test_subset != test_pred][:25]\n>>> fig, ax = plt.subplots(nrows=5, ncols=5,\n...                        sharex=True, sharey=True,\n...                        figsize=(8, 8))\n>>> ax = ax.flatten()\n>>> for i in range(25):\n...     img = misclassified_images[i].reshape(28, 28)\n...     ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n...     ax[i].set_title(f'{i+1}) '\n...                     f'True: {correct_labels[i]}\\n'\n...                     f' Predicted: {misclassified_labels[i]}')\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n # Part 1: dLoss/dOutWeights\n        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet for convenient re-use\n\n        # input/output dim: [n_examples, n_classes]\n        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n        # input/output dim: [n_examples, n_classes]\n        d_a_out__d_z_out = a_out * (1\\. - a_out) # sigmoid derivative\n        # output dim: [n_examples, n_classes]\n        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule)\n                                                       # placeholder\"\n        # gradient for output weights\n\n        # [n_examples, n_hidden]\n        d_z_out__dw_out = a_h\n\n        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n        # output dim: [n_classes, n_hidden]\n        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n        d_loss__db_out = np.sum(delta_out, axis=0) \n the following “delta” placeholder variable:\n```"]