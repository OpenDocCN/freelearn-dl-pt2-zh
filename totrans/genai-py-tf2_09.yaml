- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: The Rise of Methods for Text Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[文本生成方法的崛起](https://wiki.example.org/the_rise_of_methods_for_text_generation)'
- en: In the preceding chapters, we discussed different methods and techniques to
    develop and train generative models. Particularly, in *Chapter 6*, *Image Generation
    with GANs*, we discussed the taxonomy of generative models and introduced explicit
    and implicit classes. Throughout this book, our focus has been on developing generative
    models in the vision space, utilizing image and video datasets. The advancements
    in the field of deep learning for computer vision and ease of understanding were
    the major reasons behind such a focused introduction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了不同的方法和技术来开发和训练生成模型。特别是在*第6章*“使用GAN生成图像”中，我们讨论了生成模型的分类以及介绍了显式和隐式类。在整本书中，我们的重点一直是在视觉空间中开发生成模型，利用图像和视频数据集。深度学习在计算机视觉领域的发展以及易于理解性是引入这样一个专注介绍的主要原因。
- en: In the past couple of years though, **Natural Language Processing** (**NLP**)
    or processing of textual data has seen great interest and research. Text is not
    just another unstructured type of data; there's a lot more to it than what meets
    the eye. Textual data is a representation of our thoughts, ideas, knowledge, and
    communication.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在过去几年中，**自然语言处理**（**NLP**）或文本数据处理受到了极大的关注和研究。文本不只是另一种无结构类型的数据；其背后还有更多东西超出了表面所见。文本数据代表了我们的思想、想法、知识和交流。
- en: 'In this chapter and the next, we will focus on understanding concepts related
    to NLP and generative models for textual data. We will cover different concepts,
    architectures, and components associated with generative models for textual data,
    with a focus on the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将专注于理解与NLP和文本数据的生成模型相关的概念。我们将在本章中涵盖与文本数据生成模型相关的不同概念、架构和组件，重点关注以下主题：
- en: A brief overview of traditional ways of representing textual data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统表示文本数据方式的简要概述
- en: Distributed representation methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式表示方法
- en: RNN-based text generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RNN的文本生成
- en: LSTM variants and convolutions for text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM变体和文本卷积
- en: 'We will cover the internal workings of different architectures and key contributions
    that have enabled text generation use cases. We will also build and train these
    architectures to get a better understanding of them. Readers should also note
    that while we will go deep into key contributions and related details across *Chapter
    9*, *The Rise of Methods for Text Generation*, and *Chapter 10*, *NLP 2.0: Using
    Transformers to Generate Text*, some of these models are extremely large to train
    on commodity hardware. We will make use of certain high-level Python packages
    wherever necessary to avoid complexity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍不同架构的内部工作原理和使文本生成用例成为可能的关键贡献。我们还将构建和训练这些架构，以更好地理解它们。读者还应该注意，虽然我们将在*第9章*“文本生成方法的崛起”和*第10章*“NLP
    2.0:使用Transformer生成文本”中深入研究关键贡献和相关细节，但这些模型中的一些非常庞大，无法在常规硬件上进行训练。我们将在必要时利用某些高级Python包，以避免复杂性。
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中呈现的所有代码片段都可以直接在Google Colab中运行。由于篇幅原因，未包含依赖项的导入语句，但读者可以参考GitHub存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)。
- en: Before we get into the modeling aspects, let's get started by understanding
    how to represent textual data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入建模方面之前，让我们先了解如何表示文本数据。
- en: Representing text
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示文本
- en: Language is one of the most complex aspects of our existence. We use language
    to communicate our thoughts and choices. Every language is defined with a list
    of characters called the alphabet, a vocabulary, and a set of rules called grammar.
    Yet it is not a trivial task to understand and learn a language. Languages are
    complex and have fuzzy grammatical rules and structures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是我们存在中最复杂的方面之一。我们使用语言来传达我们的思想和选择。每种语言都有一个叫做字母表的字符列表，一个词汇表和一组叫做语法的规则。然而，理解和学习一门语言并不是一项微不足道的任务。语言是复杂的，拥有模糊的语法规则和结构。
- en: 'Text is a representation of language that helps us communicate and share. This
    makes it a perfect area of research to expand the horizons of what artificial
    intelligence can achieve. Text is a type of unstructured data that cannot directly
    be used by any of the known algorithms. Machine learning and deep learning algorithms
    in general work with numbers, matrices, vectors, and so on. This, in turn, raises
    the question: how can we represent text for different language-related tasks?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Bag of Words
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, every language consists of a defined list of characters
    (alphabet), which are combined to form words (vocabulary). Traditionally, **Bag
    of Words** (**BoW**) has been one of the most popular methods for representing
    textual information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'BoW is a simple and flexible approach to transforming text into vector form.
    This transformation helps not only in extracting features from raw text, but also
    in making it fit for consumption by different algorithms and architectures. As
    the name suggests, the BoW model of representation utilizes each word as a basic
    unit of measurement. A BoW model describes the occurrence of words within a given
    corpus of text. To build a BoW model for representation, we require two major
    things:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '**A vocabulary**: A collection of known words from the corpus of text to be
    analyzed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A measure of occurrence**: Something that we choose based upon the application/task
    at hand. For instance, counting the occurrence of each word, known as term frequency,
    is one such measure.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed discussion related to the BoW model is beyond the scope of this chapter.
    We are presenting a high-level overview as a primer before more complex topics
    are introduced later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The BoW model is called a "bag" to highlight the simplicity and the fact that
    we overlook any ordering of the occurrences. In other words, the BoW model discards
    any order or structure-related information of the words in a given text. This
    might sound like a big issue but until recently, the BoW model remained quite
    a popular and effective choice for representing textual data. Let's have a quick
    look at a few examples to understand how this simple method works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '"Some say the world will end in fire,Some say in ice.From what I have tasted
    of desire I hold with those who favour fire."'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Define a vocabulary**:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first and foremost step is to define a list of known words from our corpus.
    For ease of understanding and practical reasons, we can ignore the case and punctuation
    marks for now. The vocabulary, or unique words, thus are {some, say, the, world,
    will, end, in, fire, ice, from, what, i, have, tasted, of, desire, hold, with,
    those, who, favour}.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This vocabulary is a set of 21 unique words in a corpus of 26 words.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Define a metric of occurrence**:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have the vocabulary set, we need to define how we will measure the
    occurrence of each word from the vocabulary. As we mentioned earlier, there are
    a number of ways to do so. One such metric is simply checking if a specific word
    is present or absent. We use a 0 if the word is absent or a 1 if it is present.
    The sentence "some say ice" can thus be scored as:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'some: 1'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'say: 1'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the: 0'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'world: 0'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'will: 0'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'end: 0'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'in: 0'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fire: 0'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ice: 1'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the overall vector will look like [1, 1, 0, 0, 0, 0, 0, 0, 1].
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are a few other metrics that have been developed over the years. The
    most widely used metrics are:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Term frequency
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF, as seen in *Chapter 7*, *Style Transfer with GANs*
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashing
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These steps provide a high-level glimpse into how the BoW model helps us represent
    textual data as numbers or vectors. The overall vector representation of our excerpt
    from the poem is depicted in the following table:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B16176_09_01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: BoW representation'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Each row in the matrix corresponds to one line from the poem, while the unique
    words from the vocabulary form the columns. Each row thus is simply the vector
    representation of the text under consideration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few additional steps involved in improving the outcome of this
    method. The refinements are related to vocabulary and scoring aspects. Managing
    the vocabulary is very important; often, a corpus of text can increase in size
    quite rapidly. A few common methods of handling vocabularies are:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring punctuation marks
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignoring case
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing frequently occurring words (or stopwords) like a, an, the, this, and
    so on
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods to use the root form of words, such as *stop* in place of *stopping*.
    Stemming and lemmatization are two such methods
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling spelling mistakes
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We already discussed different scoring methods and how they help in capturing
    certain important features. BoW is simple, yet is an effective tool that serves
    as a good starting point for most NLP tasks. Yet there are a few issues which
    can be summarized as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing context**:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned earlier, the BoW model does not consider the ordering or structure
    of the text. By simply discarding information related to ordering, the vectors
    lose out on capturing the context in which the underlying text was used. For instance,
    the sentences "I am sure about it" and "Am I sure about it?" would have identical
    vector representations, yet they express different thoughts. Expanding BoW models
    to include n-grams (contiguous terms) instead of singular terms does help in capturing
    some context, but in a very limited way.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Vocabulary and sparse vectors**:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the corpus size increases, so does the vocabulary. The steps required to
    manage vocabulary size require a lot of oversight and manual effort. Due to the
    way this model works, a large vocabulary leads to very sparse vectors. Sparse
    vectors pose issues with modeling and computation requirements (space and time).
    Aggressive pruning and vocabulary management steps do help to a certain extent
    but can lead to the loss of important features as well.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着语料库的规模增加，词汇量也在增加。管理词汇量大小所需的步骤需要大量的监督和手动工作。由于这种模型的工作方式，大量的词汇导致非常稀疏的向量。稀疏向量对建模和计算需求（空间和时间）造成问题。激进的修剪和词汇管理步骤在一定程度上确实有所帮助，但也可能导致重要特征的丢失。
- en: Here, we discussed how the BoW model helps in transforming text into vector
    form, along with a few issues with this setup. In the next section, we will move
    on to a few more involved representation methods that alleviate some of these
    issues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了词袋模型如何帮助将文本转换为向量形式，以及这种设置中的一些问题。在下一节，我们将转向一些更多涉及的表示方法，这些方法缓解了一些这些问题。
- en: Distributed representation
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式表示
- en: The Bag of Words model is an easy-to-understand way of transforming words into
    vector form. This process is generally termed *vectorization*. While it is a useful
    method, the BoW model has its limitations when it comes to capturing context,
    along with sparsity-related issues. Since deep learning architectures are becoming
    de facto state-of-the-art systems in most spaces, it is obvious that we should
    be leveraging them for NLP tasks as well. Apart from the issues mentioned earlier,
    the sparse and large (wide) vectors from the BoW model are another aspect which
    can be tackled using neural networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是将单词转换为向量形式的易于理解的方法。这个过程通常被称为*向量化*。虽然这是一种有用的方法，但在捕获上下文和与稀疏相关的问题方面，词袋模型也有它的局限性。由于深度学习架构正在成为大多数空间的事实上的最先进系统，显而易见的是我们应该在NLP任务中也利用它们。除了前面提到的问题，词袋模型的稀疏和大（宽）向量是另一个可以使用神经网络解决的方面。
- en: A simple alternative that handles the sparsity issue can be implemented by encoding
    each word as a unique number. Continuing with the example from the previous section,
    "some say ice", we could assign 1 to "some", 2 to "say", 3 to "ice", and so on.
    This would result in a dense vector, [1, 2, 3]. This is an efficient utilization
    of space and we end up with vectors where all the elements are full. However,
    the limitation of missing context still remains. Since the numbers are arbitrary,
    they hardly capture any context on their own. On the contrary, arbitrarily mapping
    numbers to words is not very interpretable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理稀疏问题的简单替代方案是将每个单词编码为唯一的数字。继续上一节的示例，“有人说冰”，我们可以将“有人”赋值为1，“说”赋值为2，“冰”赋值为3，以此类推。这将导致一个密集的向量，[1,
    2, 3]。这是对空间的有效利用，并且我们得到了所有元素都是完整的向量。然而，缺失上下文的限制仍然存在。由于数字是任意的，它们几乎不能单独捕获任何上下文。相反，将数字任意映射到单词并不是非常可解释的。
- en: '**Interpretability** is an important requirement when it comes to NLP tasks.
    For computer vision use cases, visual cues are good enough indicators for understanding
    how a model is perceiving or generating outputs (though quantification is also
    a problem there, but we can skip it for now). For NLP tasks, since the textual
    data is first required to be transformed into a vector, it is important to understand
    what those vectors capture and how they are used by the models.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释性**是NLP任务的重要要求。对于计算机视觉用例，视觉线索足以成为理解模型如何感知或生成输出的良好指标（尽管在那方面的量化也是一个问题，但我们现在可以跳过它）。对于NLP任务，由于文本数据首先需要转换为向量，因此重要的是理解这些向量捕获了什么，以及模型如何使用它们。'
- en: In the coming sections, we will cover some of the popular vectorization techniques
    that try to capture context while limiting the sparsity of the vectors as well.
    Please note that there are a number of other methods (such as SVD-based methods
    and co-occurrence matrices) as well that help in vectorizing textual data. In
    this section, we will be covering only those which are helpful in understanding
    later sections of this chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些流行的向量化技术，尝试捕捉上下文，同时限制向量的稀疏性。请注意，还有许多其他方法（例如基于SVD的方法和共现矩阵）也有助于向量化文本数据。在本节中，我们将只涉及那些有助于理解本章后续内容的方法。
- en: Word2vec
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2vec
- en: The English Oxford dictionary has about 600k unique words and is growing year
    on year. Yet those words are not independent terms; they have some relationship
    to each other. The premise of the word2vec model is to learn high-quality vector
    representations that capture context. This is better summarized by the famous
    quote by J.R. Firth *"you shall know a word by the company it keeps"*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 英国牛津词典大约有 60 万个独特的单词，并且每年都在增长。然而，这些单词并非独立的术语；它们彼此之间存在一些关系。word2vec 模型的假设是学习高质量的向量表示，以捕获上下文。这更好地总结了
    J.R. 菲斯的著名引文：“*你可以通过它搭配的伙伴来认识一个词*”。
- en: In their work titled *Efficient Estimation of Word Representations in Vector
    Space*, Mikolov et al.¹ present two different models that learn vector representations
    of words from a large corpus. Word2Vec is a software implementation of these models
    which is classified as an iterative approach to learning such embeddings. Instead
    of taking the whole corpus into account in one go, this approach tries to iteratively
    learn to encode each word's representation, along with its context. This idea
    of learning word representations as dense context vectors is not a new one. It
    was first proposed by Rumelhart et al. in 1990². They presented how a neural network
    is able to learn representations, with similar words ending up in the same clusters.
    The ability to have vector forms of words that capture some notion of similarity
    is quite a powerful one. Let's see in detail how the word2vec models achieve this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们名为“Vector Space 中单词表示的高效估计”的工作中，Mikolov 等人¹介绍了两种学习大型语料库中单词向量表示的模型。Word2Vec
    是这些模型的软件实现，属于学习这些嵌入的迭代方法。与一次性考虑整个语料库不同，这种方法尝试迭代地学习编码每个单词的表示及其上下文。学习词表示作为密集上下文向量的这一概念并不新鲜。这最初是由
    Rumelhart 等人于 1990²年提出的。他们展示了神经网络如何学习表示，使类似的单词最终处于相同的聚类中。拥有捕获某种相似性概念的单词向量形式的能力是非常强大的。让我们详细看看
    word2vec 模型是如何实现这一点的。
- en: Continuous Bag of Words (CBOW) Model
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续词袋 (CBOW) 模型
- en: The Continuous Bag of Words model is an extension of the Bag of Words model
    we discussed in the previous section. The key aspect of this model is the context
    window. A context window is defined as a sliding window of a fixed size moving
    along a sentence. The word in the middle is termed the *target*, and the terms
    to its left and right within the window are the *context terms*. The CBOW model
    works by predicting the target term, given its context terms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 连续词袋模型是我们在上一节讨论的词袋模型的扩展。该模型的关键方面是上下文窗口。上下文窗口被定义为沿着句子移动的固定大小的滑动窗口。中间的词称为*目标*，窗口内的左右术语称为*上下文术语*。CBOW
    模型通过给定其上下文术语来预测目标术语。
- en: For instance, let's consider a reference sentence, "some say the *world* will
    end in fire". If we have a window size of 4 and a target term of *world*, the
    context terms would be {say, the} and {will, end}. The model inputs are tuples
    of the form (context terms, target term), which are then passed through a neural
    network to learn the embeddings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一句参考句子，“some say the *world* will end in fire”。如果我们的窗口大小为 4，目标术语为*world*，那么上下文术语将会是{say,
    the}和{will, end}。模型的输入是形式为（上下文术语，目标术语）的元组，然后将其通过神经网络学习嵌入向量。
- en: 'This process is depicted in the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程如下图所示：
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_02.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图表，图解 自动生成描述](img/B16176_09_02.png)'
- en: 'Figure 9.2: Continuous Bag of Words model setup'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：连续词袋模型设置
- en: 'As shown in the preceding diagram, the context terms, denoted as ![](img/B16176_09_001.png),
    are passed as input to the model to predict the target term, denoted as *w*[t].
    The overall working of the CBOW model can be explained as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图表所示，上下文术语，表示为![](img/B16176_09_001.png)，被作为输入传递给模型，以预测目标术语，表示为*w*[t]。CBOW
    模型的整体工作可以解释如下：
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小为*V*的词汇表，定义了大小为*C*的上下文窗口。*C*可以是 4、6 或任何其他大小。我们还定义了两个矩阵*W*和*W'*来生成输入和输出向量。矩阵*W*是*VxN*，而*W'*是*NxV*的维度。*N*是嵌入向量的大小。
- en: 'The context terms (![](img/B16176_09_001.png)) and the target term (*y*) are
    transformed into one-hot encodings (or label-encodings) and training data is prepared
    in the form of tuples: (![](img/B16176_09_001.png), *y*).'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文术语（![](img/B16176_09_001.png)）和目标术语（*y*）被转化为独热编码（或标签编码），并且训练数据以元组的形式准备：（![](img/B16176_09_001.png)，*y*）。
- en: We average the context vectors to get ![](img/B16176_09_004.png).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对上下文向量进行平均以得到 ![](img/B16176_09_004.png)。
- en: The final output scoring vector *z* is calculated as a dot product between the
    average vector *v'* and the output matrix *W'*.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的输出评分向量 *z* 是平均向量 *v'* 和输出矩阵 *W'* 的点积。
- en: The output scoring vector is transformed into a probability using a softmax
    function; that is, *y' = softmax(z)*, where *y'* should correspond to one of the
    terms in the vocabulary.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出评分向量经过 softmax 函数转换为概率值；也就是说，*y' = softmax(z)*，其中 *y'* 应该对应词汇表中的一个术语。
- en: The final aim would be to train the neural network such that *y'* and the actual
    target *y* become as close as possible.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的目标是训练神经网络，使得 *y'* 和实际目标 *y* 尽可能接近。
- en: The authors proposed using a cost function such as cross-entropy to train the
    network and learn such embeddings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议使用诸如交叉熵之类的成本函数来训练网络并学习这样的嵌入。
- en: Skip-gram model
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: skip-gram 模型
- en: The skip-gram model is the second variant presented in the paper for learning
    word embeddings. In essence, this model works in exactly the opposite way to the
    CBOW model. In other words, in the case of skip-gram, we input a word (center/target
    word) and predict the context terms as the model output. Let's use the same example
    as before, "some say the *world* will end in fire". Here, we will start with *world*
    as our input term and train a model to predict {say, the, will, end} as context
    terms with high probability.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型是该论文中用于学习词嵌入的第二个变体。本质上，该模型的工作方式与 CBOW 模型完全相反。换句话说，在 skip-gram 的情况下，我们输入一个词（中心/目标词），预测上下文术语作为模型的输出。让我们用之前的例子进行说明，“some
    say the *world* will end in fire”。在这里，我们将用 *world* 作为输入术语，并训练一个模型以高概率预测 {say,
    the, will, end}，作为上下文术语。
- en: 'The following diagram depicts the skip-gram model; as expected, this is a mirror
    image of the CBOW setup we discussed in *Figure 9.2*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 skip-gram 模型；如预期的那样，这是我们在 *图 9.2* 中讨论的 CBOW 设置的镜像：
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图表，图解  自动生成描述](img/B16176_09_03.png)'
- en: 'Figure 9.3: Skip-gram model setup'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：skip-gram 模型设置
- en: 'The step by step working skip-gram model can be explained as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型的逐步工作可以解释如下：
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个大小为 *V* 的词汇表，定义一个大小为 *C* 的上下文窗口。*C* 可以是 4、6 或其他任意大小。我们还定义了两个矩阵 *W* 和 *W'*，分别用于生成输入向量和输出向量。矩阵
    *W* 是 *VxN* 的，而 *W'* 的维度是 *NxV*。*N* 是嵌入向量的大小。
- en: Generate the one-hot encoded representation of the center word *x*.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成中心词 *x* 的独热编码表示。
- en: We get the word embedding representation of *x* by taking a dot product of *x*
    and *W*. The embedded representation is given as *v = W.x*.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 *x* 和 *W* 的点积来获取 *x* 的词嵌入表示。嵌入表示为 *v = W.x*。
- en: We generate the output score vector *z* by taking a dot product of *W'* and
    *v*; that is, *z = W'.v*.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将 *W'* 和 *v* 的点积得到输出评分向量 *z*；也就是说，*z = W'.v*。
- en: The scoring vector is transformed into output probabilities using a softmax
    layer to generate *y'*.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评分向量通过 softmax 层转换为输出概率，生成 *y'*。
- en: The final aim would be to train the neural network such that *y'* and the actual
    context *y* become as close as possible.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的目标是训练神经网络，使得 *y'* 和实际上的上下文 *y* 尽可能接近。
- en: In the case of skip-gram, we have multiple input-output training pairs for any
    given center word. This model treats all context terms equally, irrespective of
    their distance from the center word in the context window. This allows us to use
    cross-entropy as the cost function with a strong conditional independence assumption.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 skip-gram 的情况下，对于任何给定的中心词，我们有多个输入-输出训练对。该模型将所有上下文术语都视为同等重要，无论它们与上下文窗口中的中心词之间的距离如何。这使我们能够使用交叉熵作为成本函数，并假设具有强条件独立性。
- en: In order to improve the outcomes and speed up the training process, the authors
    introduced some simple yet effective tricks. Concepts such as *negative sampling,
    noise contrastive estimation* and *hierarchical softmax* are a few such techniques
    which have been leveraged. For a detailed understanding of CBOW and skip-gram,
    readers are requested to go through the cited paper by Mikolov et al.,¹ where
    the authors have given detailed explanations of each of the steps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善结果并加快训练过程，作者们引入了一些简单但有效的技巧。*负采样、噪声对比估计*和*分层softmax*等概念是一些被利用的技术。要详细了解CBOW和skip-gram，请读者阅读Mikolov等人引用的文献¹，作者在其中详细解释了每个步骤。
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Just a few lines of code and we have our word2vec representations of our vocabulary
    ready. Upon checking, we find that there are 19,000 unique words in our vocabulary,
    and that we have a vector representation for each. The following snippet shows
    how we can get the vector representation of any word. We will also demonstrate
    how to get words that are most similar to a given word:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几行代码，我们就可以获得我们词汇表的word2vec表示。检查后，我们发现我们的词汇表中有19,000个独特单词，并且我们为每个单词都有一个向量表示。以下代码片段显示了如何获得任何单词的向量表示。我们还将演示如何获取与给定单词最相似的单词：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding outputs show a 32-dimensional vector for the word *sun*. We also
    display words that are most similar to the word *god*. We can clearly see that
    words such as believe, existence, and so on seem to be the most similar, which
    makes sense given the dataset we used. For interested readers, we have a 3-dimensional
    vector space representation using TensorBoard showcased in the corresponding notebook.
    The TensorBoard representation helps us visually understand the embedding space
    and see how these vectors interact.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前述输出展示了*sun*这个单词的32维向量。我们还展示了与单词*god*最相似的单词。我们可以清楚地看到，诸如believe、existence等单词似乎是最相似的，这是合乎逻辑的，考虑到我们使用的数据集。对于感兴趣的读者，我们在对应的笔记本中展示了使用TensorBoard的3维向量空间表示。TensorBoard表示帮助我们直观地理解嵌入空间，以及这些向量是如何相互作用的。
- en: GloVe
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GloVe
- en: The word2vec models helped in improving performance for various NLP tasks. Continuing
    with the same momentum, another important implementation called GloVe came into
    the picture. GloVe or *Global Vectors for Word Representation* was published by
    Pennington et al. in 2014 to improve upon the known word representation techniques.³
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型有助于改进各种自然语言处理任务的性能。在相同的动力下，另一个重要的实现叫做GloVe也出现了。GloVe或*全局词向量表示*于2014年由Pennington等人发表，旨在改进已知的单词表示技术。³
- en: As we've seen, word2vec models work by considering the local context (a defined
    window) of the words in the vocabulary. Even though this works remarkably well,
    it is a bit rough around the edges. The fact that words may mean different things
    in different contexts requires us to understand not just the local but the global
    context as well. GloVe tries to work upon the global context while learning the
    word vectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，word2vec模型通过考虑词汇表中单词的局部上下文（定义的窗口）来工作。即使这是非常有效的，但还不够完善。单词在不同上下文中可能意味着不同的东西，这要求我们不仅要理解局部上下文，还要理解全局上下文。GloVe试图在学习单词向量的同时考虑全局上下文。
- en: There are classical techniques for this, such as **Latent Semantic Analysis**
    (**LSA**), which are based on matrix factorization and do a good job at capturing
    global context, but are not so good at things such as vector math.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些经典的技术，例如**潜在语义分析**（**LSA**），这些技术基于矩阵分解，在捕获全局上下文方面做得很好，但在向量数学等方面做得不太好。
- en: 'GloVe is a method which tries to get the best of both worlds in order to learn
    better word representations. The GloVe algorithm consists of the following steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是一种旨在学习更好的词表示的方法。GloVe算法包括以下步骤：
- en: Prepare a word co-occurrence matrix *X*, such that each element, *x*[i][j],
    represents how often the word *j* appears in the context of the word *i*. GloVe
    makes use of two fixed size windows that help in capturing context before the
    word and context after the word.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个词共现矩阵*X*，使得每个元素*x*[i][j]表示单词*i*在单词*j*上下文中出现的频率。GloVe使用了两个固定尺寸的窗口，这有助于捕捉单词之前和之后的上下文。
- en: The co-occurrence matrix *X* is updated with a decay factor to penalize terms
    that are farther apart in the context. The decay factor is defined as ![](img/B16176_09_005.png),
    where offset is the distance from the word under consideration.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共现矩阵*X*使用衰减因子进行更新，以惩罚上下文中距离较远的术语。衰减因子定义如下：![](img/B16176_09_005.png)，其中offset是考虑的单词的距离。
- en: Then, we prepare the GloVe equation as the following soft constraint:![](img/B16176_09_006.png)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将准备GloVe方程如下软约束条件：![](img/B16176_09_006.png)
- en: Here, *w*[i] is the vector for the main word, *w*[j] is the vector for the context
    word, and *b*[i], *b*[j] are the corresponding bias terms.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，*w*[i]是主要单词的向量，*w*[j]是上下文单词的向量，*b*[i]，*b*[j]是相应的偏差项。
- en: The final step is to use the preceding constraint to define the cost function,
    which is given as:![](img/B16176_09_007.png)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是使用前述约束条件来定义成本函数，其定义如下：![](img/B16176_09_007.png)
- en: 'Here, *f* is a weighting function, defined as:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，*f*是一个加权函数，定义如下：
- en: '![](img/B16176_09_008.png)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16176_09_008.png)'
- en: The authors of the paper achieved the best results with ![](img/B16176_09_009.png).
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该论文的作者使用![](img/B16176_09_009.png)获得了最佳结果。
- en: Similar to word2vec models, GloVe embeddings also achieve good results and the
    authors present results where they show GloVe outperforming word2vec. They attribute
    this to better problem formulation and the global context being captured.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于word2vec模型，GloVe嵌入也取得了良好的结果，作者展示了GloVe胜过word2vec的结果。他们将此归因于更好的问题表述和全局上下文的捕捉。
- en: In practice, both models perform more or less similarly. As larger vocabularies
    are required to get better embeddings (for both word2vec and GloVe), for most
    practical use cases, pretrained embeddings are available and used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这两种模型的性能差不多。由于需要更大的词汇表来获得更好的嵌入（对于word2vec和GloVe），对于大多数实际应用情况，预训练的嵌入是可用且被使用的。
- en: Pretrained GloVe vectors are available through a number of packages, such as
    `spacy`. Interested readers may wish to explore the `spacy` package for more details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的GloVe向量可以通过多个软件包获得，例如`spacy`。感兴趣的读者可以探索`spacy`软件包以获得更多详情。
- en: FastText
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastText
- en: Word2Vec and GloVe are powerful methods which have nice properties when it comes
    to encoding words in the vector space. Both techniques work nicely when it comes
    to getting vector representation of words that are in the vocabulary, but they
    do not have clear answers for terms that are outside of the vocabulary.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec和GloVe是强大的方法，在将单词编码为向量空间时具有很好的特性。当涉及到获取在词汇表中的单词的向量表示时，这两种技术都能很好地工作，但对于词汇表之外的术语，它们没有明确的答案。
- en: The word is the fundamental unit in the case of the word2vec and GloVe methods.
    This assumption is challenged and improved upon in the FastText implementation.
    The word representation aspect of FastText is based on the paper, *Enriching Word
    Vectors with Subword Information* by Bojanowski et al. in 2017.⁴ This work decomposes
    each word into a set of n-grams. This helps in capturing and learning vector representations
    of different combinations of characters, as opposed to the whole word in earlier
    techniques.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在word2vec和GloVe方法中，单词是基本单位。这一假设在FastText实现中受到挑战和改进。FastText的单词表示方面基于2017年Bojanowski等人的论文*使用子词信息丰富化词向量*。⁴
    该工作将每个单词分解为一组n-grams。这有助于捕捉和学习字符组合的不同向量表示，与早期技术中的整个单词不同。
- en: For instance, if we consider the word "India" and *n=3* for the n-gram setup,
    it will decompose the word into {<india>, <in, ind, ndi, dia, ia>}. The symbols
    "<" and ">" are special characters to denote the start and end of the original
    word. This helps in differentiating between <in>, which represents the whole word,
    and <in, which is an n-gram. This approach helps FastText generate embeddings
    for *out of vocabulary* terms as well. This can be done by adding and averaging
    the vector representation of required n-grams.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑单词“India”和*n=3*用于n-gram设置，则它将将单词分解为{<india>, <in, ind, ndi, dia, ia>}。符号“<”和“>”是特殊字符，用于表示原始单词的开始和结束。这有助于区分<in>，它代表整个单词，和<in，它代表一个n-gram。这种方法有助于FastText生成*超出词汇表*的术语的嵌入。这可以通过添加和平均所需n-gram的向量表示来实现。
- en: FastText is shown to drastically improve performance when it comes to use cases
    where there is a high chance of new/out of vocabulary terms. FastText was developed
    by researchers at **Facebook AI Research** (**FAIR**), which shouldn't come as
    a surprise as the kind of content generated on social media platforms such as
    Facebook is huge and ever-changing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: With its added improvements come a few disadvantages as well. Since the basic
    unit in this case is an n-gram, the amount of time required to train/learn such
    representations is higher than previous techniques. The n-gram approach also increases
    the amount of memory required to train such a model. However, the authors of the
    paper point out that a hashing trick helps in controlling the memory requirements
    to a certain extent.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease of understanding, let''s again make use of our well-known Python library,
    `gensim`. We will extend upon the same dataset and pre-processing steps that we
    performed for the word2vec model exercise in the previous section. The following
    snippet prepares the FastText model object:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The word2vec model fails to return a vector representation of the word "sunny"
    as it is not in the trained vocabulary. The following snippet shows how FastText
    is still able to generate a vector representation:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This showcases how FastText improves upon word2vec- and GloVe-based representation
    techniques. We can easily handle out of vocabulary terms, all the while ensuring
    context-based dense representations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Let's now use this understanding to develop a text generation model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Text generation and the magic of LSTMs
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed different ways of representing textual
    data in order to make it fit for consumption by different NLP algorithms. In this
    section, we will leverage this understanding of text representation to work our
    way toward building text generation models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have built models using feedforward networks consisting of different
    kinds and combinations of layers. These networks work with one training example
    at a time, which is independent of other training samples. We say that the samples
    are **independent and identically distributed**, or **IID**. Language, or text,
    is a bit different.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous sections, words change their meaning based on
    the context they are being used in. In other words, if we were to develop and
    train a language generation model, we would have to ensure the model understands
    the context of its input.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are a class of neural networks that
    allow previous outputs to be used as inputs, along with memory or hidden units.
    This awareness of previous inputs helps in capturing context, and provides us
    with the ability to handle variable-length input sequences (sentences are hardly
    ever of the same length). A typical RNN is depicted in the following diagram,
    in both actual and unrolled form:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_04.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: A typical RNN'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.4*, at time *t*[1], input *x*[1] generates output *y*[1].
    At time *t*[2], *x*[2] along with *y*[1] (the previous output) generate output
    *y*[2], and so on. Unlike typical feedforward networks where every input is independent
    of the others, RNN introduces the notion of previous outputs impacting the current
    and upcoming ones.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have a few different variants to them, namely, **Gated Recurrent Units**
    (**GRUs**) and **Long Short-Term Memory** (**LSTMs**). The vanilla RNN described
    previously works in auto-regressive settings well. Yet it has issues with longer
    context windows (vanishing gradients). GRUs and LSTMs try to overcome such issues
    by using different gates and memory units. Introduced by Hochreiter and Schmidhuber
    in 1997, LSTMs can remember information from really long sequence-based data.
    LSTMs consist of three gates called input, output, and forget gates. This is depicted
    in the following diagram.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_09_05.png)Figure 9.5: Different gates of an LSTM cell'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed understanding of LSTMs, you may refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We will now focus on defining the task of text generation more formally.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP-based solutions are quite effective and can be seen all around us. The most
    prominent example is the autocomplete feature on most smartphone keyboards, search
    engines (Google, Bing, and so on), and even word processors (like MS Word).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Autocomplete is a common name for a formal concept called *language modeling*.
    In simple words, a language model takes certain text as the input context to generate
    the next set of words as the output. This is interesting because a language model
    tries to understand the input context, the language structure, and rules to predict
    the next word(s). We use it in the form of text completion utilities on search
    engines, chat platforms, emails, and more all the time. Language models are a
    perfect real-life application of NLP and showcase the power of RNNs. In this section,
    we will work toward building an understanding, as well as training an RNN-based
    language model for text generation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by understanding the process of generating a training dataset.
    We can do this with the help of the following image. This image depicts a word-level
    language model; that is, a model for which a word is the basic unit. Along the
    same lines, we can develop character-level, phrase-level, or even document-level
    models:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B16176_09_06.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Training data generation process for a language model'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, a language model looks at the context to generate the
    next set of words. This context is also called a sliding window, which moves across
    the input sentence from left to right (right to left for languages that are written
    from right to left). The sliding window depicted in *Figure 9.6* spans three words,
    which act as input. The corresponding output for each training data point is the
    immediate next word after the window (or a set of words if the aim is to predict
    the next phrase). We thus prepare our training dataset, which consists of tuples
    of the form ({context terms}, next_word). The sliding window helps us to generate
    a good number of training samples from every sentence in the training dataset
    without explicit labeling.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset is then used to train an RNN-based language model. In
    practice, we typically use LSTMs or GRU units in place of vanilla RNN units. We
    discussed earlier that RNNs have the ability to auto-regress on previous timestep
    values. In the context of language models, we auto-regress on the context terms
    and the model generates the corresponding next word. We then make use of **backpropagation
    through time** (**BPTT**) to update model weights through gradient descent until
    the required performance is achieved. We discussed BPTT in detail in *Chapter
    3*, *Building Blocks of Deep Neural Networks*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: We now have a fair understanding of what a language model is and what steps
    are involved in preparing the training dataset, along with the model setup. Let's
    now implement some of these concepts using TensorFlow and Keras.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Character-level language model'
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed the basics of language modeling in the previous section. In this
    section, we will build and train our own language model, but with a twist. In
    contrast to the discussion in the previous section, here, we will work at the
    character level, not at the word level. In simple words, we will work toward building
    a model that takes a few characters as input (context) to generate the next set
    of characters. This choice of a more granular language model is for the ease of
    training such a model. A character-level language model needs to worry about a
    much smaller vocabulary, or number of unique characters, compared to a word-level
    language model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To build our language model, the first step is to get a dataset to use as a
    training source. Project Gutenberg is a volunteer effort to digitize historical
    works and make them available as free downloads. Since we need lots of data to
    train a language model, we will pick one of the biggest available books, *War
    and Peace* by Leo Tolstoy. This book is available for download at the following
    URL:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.gutenberg.org/ebooks/2600](https://www.gutenberg.org/ebooks/2600)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet loads the book''s content for use as our source dataset:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The next step is to prepare our dataset for the model. As we discussed in the
    *Representing text* section, textual data is transformed into vectors using word
    representation models. One way to do so is to first transform them into one-hot
    encoded vectors, which are then transformed into dense representations using models
    such as word2vec. The other way is to transform them into an arbitrary numerical
    representation first and then train an embedding layer along with the rest of
    the RNN-based language model. In this case, we are using the latter approach of
    training an embedding layer alongside the rest of the model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet prepares a mapping of individual characters to their
    integer mapping:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, each unique character is mapped to an integer; for instance,
    `\n` is mapped to 0, `!` is mapped to 3, and so on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'For optimal memory utilization, we can make use of the `tf.data` API to slice
    our data into manageable slices. We restrict our input sequences to 100 characters
    long, and this API helps us create contiguous slices of this dataset. This is
    showcased in the following code snippet:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have created the model object. As is apparent from the snippet, the model
    is a stack of embedding, LSTM, and dense layers. The embedding layer helps transform
    raw text into vector form, and is followed by the LSTM and dense layers, which
    learn context and language semantics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'The next set of steps involve defining a loss function and compiling the model.
    We will be using sparse categorical cross-entropy as our loss function. The following
    snippet defines the loss function and compiles the model; we are using the Adam
    optimizer for minimization:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since we are using TensorFlow with the high-level Keras API, training the model
    is as simple as calling the `fit` function. We train the model for just 10 epochs,
    using the `ModelCheckpoint` callback to save the model''s weights every epoch,
    as shown in the following snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Congratulations, you've trained your very first language model. Now, we'll use
    it to generate some fake text. Before we do that, though, we need to understand
    how we can decode the output generated by our model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Decoding strategies
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier on, we transformed all the textual data into suitable vector forms
    for training and inference purposes. Now that we have a trained model, the next
    step is to input some context words and generate the next word as output. This
    output generation step is formally known as the **decoding step**. It is termed
    "decoding" because the model outputs a vector which has to be processed to get
    the actual word as output. There are a few different decoding techniques; let''s
    briefly discuss the popular ones: greedy decoding, beam search, and sampling.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Greedy decoding
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest and fastest decoding strategy. As the name suggests, greedy
    decoding is a method which picks up the highest probability term at every prediction
    step.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: While this is fast and efficient, being greedy does create a few issues while
    generating text. By focusing on only the highest probability outputs, the model
    may generate inconsistent or incoherent outputs. In the case of character-language
    models, this may even result in outputs that are non-dictionary words. Greedy
    decoding also limits the variance of outputs, which may result in repetitive content
    as well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Beam search
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Beam search is a widely used alternative to greedy decoding. This decoding
    strategy, instead of picking the highest probability term, keeps track of *n*
    possible outputs at every timestep. The following diagram illustrates the beam
    search decoding strategy. It shows multiple beams forming from step 0, creating
    a tree-like structure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_07.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Beam search-based decoding strategy'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.7*, the beam search strategy works by keeping track of
    *n* predictions at every timestep and finally selects the path with the **overall**
    highest probability, highlighted with bold lines in the figure. Let's analyze
    the beam search decoding example used in the preceding diagram step by step, assuming
    a beam size of 2.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'At time step *t*[0]:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The model predicts the following three words (with probabilities) as (**the**,
    0.3), (**when**, 0.6), and (**and**, 0.1).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of greedy decoding, we would have selected "when" as it has the
    highest probability.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we will keep track of the top two outputs as our beam size is
    2.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At time step *t*[2]:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the same steps; that is, we keep track of the top two outputs from
    each of the two beams.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The beam-wise scores are calculated by multiplying the probabilities along
    the branches, like so:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(when, 0.6) –> (the, 0.4) = 0.6*0.4 = 0.24*'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(the, 0.3) –> (war, 0.9) = 0.3*0.9 = 0.27*'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the above discussion, the final output generated is "It was July, 1805
    *the war*". This output had a final probability of 0.27 in comparison to an output
    like "It was July, 1805 *when the*", which had a score of 0.24, and is what greedy
    decoding would have given us.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: This decoding strategy drastically improves upon the naïve greedy decoding strategy
    we discussed in the previous section. This, in a way, provides the language model
    with additional capabilities to pick the best possible outcome.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sampling is a process wherein a predefined a number of observations are selected
    from a larger population. As an improvement over greedy decoding, a random sampling
    decoding method can be employed to address the variation/repetition issue. In
    general, a sampling-based decoding strategy helps in selecting the next word conditioned
    on the context so far, that is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_09_010.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[t] is the output at time step *t* that''s been conditioned on words
    that are generated until time step *t-1*. Continuing with the example from our
    previous decoding strategies, the following image highlights how a sampling-based
    decoding strategy would select the next word:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, funnel chart  Description automatically generated](img/B16176_09_08.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Sampling-based decoding strategy'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.8*, this method picks a random word at every timestep
    from the given conditional probability. In the case of our example, the model
    ended by randomly selecting **in** and then **Paris** as subsequent outputs. If
    you notice carefully, at timestep *t*[1], the model ends up selecting the word
    with the least probability. This brings in a much-required randomness associated
    with the way humans use language. Holtzman et al. in their work titled *The Curious
    Case of Neural Text Degeneration*⁵ present this exact argument by stating that
    humans do not always simply use the words with the highest probability. They present
    different scenarios and examples to highlight how language is a random choice
    of words and not a typical high probability curve formed by beam search or greedy
    decoding.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to an important parameter called *temperature*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed earlier, a sampling-based decoding strategy helps with improving
    the randomness of the output. However, too much randomness is also not ideal,
    as it can lead to gibberish and incoherent results. To control this amount of
    randomness, we can introduce a tunable parameter called temperature. This parameter
    helps to increase the likelihood of high probability terms while reducing the
    likelihood of low probability ones, which leads to sharper distributions. High
    temperature leads to more randomness, while lower temperature brings in predictability.
    An important point to note is that this can be applied to any decoding strategy.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Top-k sampling
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beam search and sampling-based decoding strategies both have their own set of
    advantages and disadvantages. Top-*k* sampling is a hybrid strategy which takes
    the best of both worlds to provide an even more sophisticated decoding method.
    In simple terms, at every timestep, instead of selecting a random word, we keep
    track of the *top k terms* (similar to beam search) and redistribute the probabilities
    among them. This gives the model an additional chance of generating coherent samples.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Decoding strategies'
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a decent enough understanding of some of the most widely used
    decoding strategies, it's time to see them in action.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to prepare a utility function `generate_text` to generate
    the next word based on a given decoding strategy, as shown in the following code
    snippet:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The code first transforms raw input text into integer indices. We then use
    the model to make predictions which are manipulated based on the mode selected,
    greedy or sampling. We already have a character-language model trained from the
    previous exercise, along with a utility to help us generate the next word based
    on a decoding strategy of choice. We use both of these in the following snippet
    to understand the different outputs that are generated using different strategies:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The results of using the same seed with different decoding strategies are showcased
    in the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, letter, email  Description automatically
    generated](img/B16176_09_09.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Text generation based on different decoding strategies. The text
    in bold is the seed text, followed by the output text generated by the model.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: This output highlights some of the issues as well as the salient features of
    all the decoding strategies we've discussed so far. We can see how the increase
    in temperature makes the model more expressive. We can also observe that the model
    has learned to pair up quotation marks and even use punctuation. The model also
    seems to have learned how to use capitalization. The added expressiveness of the
    temperature parameter comes at the cost of the stability of the model. Thus, there
    is usually a trade-off between expressiveness and stability.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our first method for generating text; we leveraged RNNs (LSTMs
    in particular) to generate text using different decoding strategies. Next, we
    will look at some variations of the LSTM model, as well as convolutions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: LSTM variants and convolutions for text
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are extremely useful when it comes to handling sequential datasets. We
    saw in the previous section how a simple model effectively learned to generate
    text based on what it learned from the training dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the years, there have been a number of enhancements in the way we model
    and use RNNs. In this section, we will discuss two widely used variants of the
    single-layer LSTM network we discussed in the previous section: stacked and bidirectional
    LSTMs.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Stacked LSTMs
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are well aware of how the depth of a neural network helps it learn complex
    and abstract concepts when it comes to computer vision tasks. Along the same lines,
    a stacked LSTM architecture, which has multiple layers of LSTMs stacked one after
    the other, has been shown to give considerable improvements. Stacked LSTMs were
    first presented by Graves et al. in their work *Speech Recognition with Deep Recurrent
    Neural Networks*.⁶ They highlight the fact that depth – multiple layers of RNNs
    – has a greater impact on performance compared to the number of units per layer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Though there isn't any theoretical proof to explain this performance gain, empirical
    results help us understand the impact. These enhancements can be attributed to
    the model's capacity to learn complex features and even abstract representations
    of inputs. Since there is a time component associated with LSTMs and RNNs in general,
    deeper networks learn the ability to operate at different time scales as well.⁷
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The dataset, training loop, and even the inference utilities remain as-is. For
    brevity, we have skipped presenting those code snippets again. We will discuss
    the bidirectional argument that we introduce here shortly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how the results look for this deeper LSTM-based language model.
    The following screenshot demonstrates the results from this model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a flower  Description automatically generated](img/B16176_09_10.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Text generation based on different decoding strategies for the
    stacked-LSTM based language model'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see how the generated text is picking up the writing style of
    the book, capitalization, punctuation, and other aspects better than the outputs
    shown in *Figure 9.9*. This highlights some of the advantages we discussed regarding
    deeper RNN architectures.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional LSTMs
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second variant that''s very widely used nowadays is the bidirectional LSTM.
    We have already discussed how LSTMs, and RNNs in general, condition their outputs
    by making use of previous timesteps. When it comes to text or any sequence data,
    this means that the LSTM is able to make use of past context to predict future
    timesteps. While this is a very useful property, this is not the best we can achieve.
    Let''s illustrate why this is a limitation through an example:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, PowerPoint  Description automatically
    generated](img/B16176_09_11.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Looking at both past and future context windows for a given word'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'As is evident from this example, without looking at what is to the right of
    the target word "Teddy", the model would not pick up the context properly. To
    handle such scenarios, bidirectional LSTMs were introduced. The idea behind them
    is pretty simple and straightforward. A bidirectional LSTM (or biLSTM) is a combination
    of two LSTM layers that work simultaneously. The first is the usual forward LSTM,
    which takes the input sequence in its original order. The second one is called
    the backward LSTM, which takes **a reversed copy** of the sequence as input. The
    following diagram showcases a typical biLSTM setup:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_12.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Bidirectional LSTM setup'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in *Figure 9.12*, the forward and backward LSTMs work in tandem
    to process the original and reversed copy of the input sequences. Since we have
    two LSTM cells working on different contexts at any given time step, we need a
    way of defining the output that will be used by the downstream layers in the network.
    The outputs can be combined via summation, multiplication, concatenation, or even
    averaging of hidden states. Different deep learning frameworks might set different
    defaults, but the most widely used method is concatenation of the biLSTM outputs.
    Please note that, similar to biLSTM, we can make use of bi-RNNs or even bi-GRUs
    (**Gated Recurrent Units**).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The biLSTM setup has advantages compared to a normal LSTM, as the former can
    look at the future context as well. This advantage also becomes a limitation when
    it is not possible to peek into the future. For the current use case of text generation,
    biLSTMs are leveraged in an encoder-decoder type of architecture. We make use
    of biLSTMs to learn better embeddings of the inputs, but the decoding stage (where
    we use these embeddings to guess the next word) only uses the normal LSTMs. Similar
    to earlier hands-on exercises, we can train this network using the same set of
    utilities. We leave this as an exercise for you; for now, we will move on to convolutions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions and text
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RNNs are extremely powerful and expressive when it comes to *sequence-to-sequence*
    tasks such as text generation. Yet they meet a few challenges:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: RNNs suffer from vanishing gradients when the context window is very wide. Though
    LSTMs and GRUs overcome that to a certain extent, the context windows are still
    fairly small compared to the typical non-local interaction of words we see in
    normal usage.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The recurrence aspect of RNNs makes them sequential and eventually slow for
    training as well as inference.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The architecture we covered in the previous section tries to encode the whole
    input context (or seed text) into a single vector, which is then used by the decoder
    to generate the next set of words. This creates limitations when the seed/context
    is pretty long, as does the fact that the RNN pays a lot more attention to the
    last set of inputs in the context.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs have a larger memory footprint compared to other types of neural network
    architectures; that is, they require more parameters and hence more memory during
    their implementation.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the other hand, we have convolutional networks, which are battle-tested in
    the field of computer vision. State-of-the-art architectures make use of CNNs
    to extract features and perform well on different vision tasks. The success of
    CNNs led researchers to explore their application to NLP tasks as well.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind using CNNs for text is to first try to create vector representations
    of **a set of words** rather than individual words. More formally, the idea is
    to generate a vector representation of every sub-sequence of words in a given
    sentence.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a sample sentence, "Flu outbreak forces schools to close". The
    aim would be to first break down this sentence into all possible sub-sequences,
    such as "Flu outbreak forces", "outbreak forces schools",…, "schools to close",
    and then generate a vector representation of each of these sub-sequences. Though
    such sub-sequences may or may not carry much meaning, they provide us with a way
    to understand words in different contexts, as well as their usage. Since we already
    understand how to prepare dense vector representation of words (see the *Distributed
    representation* section), let's build on top of that to understand how CNNs can
    be leveraged.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the preceding example, *Figure 9.13 (A)* depicts each of the
    words in their vector form. The vectors are only 4-dimensional for ease of understanding:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![Calendar  Description automatically generated](img/B16176_09_13.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: (A) Vector representation (1x4) of each word in sample sentence.
    (B) Two kernels/filters of size 3 each. (C) Phrase vectors of dimension 1x2 each
    after taking the Hadamard product, followed by the sum for each kernel with stride
    1.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The two kernels, each of size 3, are depicted in *Figure 9.13 (B)*. The kernels
    in the case of text/NLP use cases are chosen to be as wide as the word vector
    dimension. The size of 3 signifies the context window each kernel is focusing
    on. Since the kernel width is the same as the word-vector width, we move the kernel
    along the words in the sentence. This constraint on size and movement in one direction
    only is the reason these convolutional filters are termed 1-D convolutions. The
    output phrase vectors are depicted in *Figure 9.13 (C)*.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Similar to deep convolutional neural networks for computer vision use cases,
    the above setup enables us to stack 1-D convolutional layers for NLP use cases
    as well. The greater depth allows the models to capture not just more complex
    representations but also a wider context window (this is analogous to an increase
    in the receptive field for a vision model with depth).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Using CNNs for NLP use cases also improves computation speed, as well as reducing
    the memory and time requirements to train such networks. In fact, these are some
    of the advantages that are explored by the following works for NLP tasks using
    1-D CNNs:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '*Natural Language Processing (almost) from Scratch*, Collobert et al.⁸'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Character-level Convolutional Networks for Text Classification*, Zhang et
    al.⁹'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convolutional Neural Networks for Sentence Classification*, Kim^(10)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recurrent Convolutional Neural Networks for Text Classification*, Lai and
    Xu et al.^(11)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we've discussed how CNNs can be used to extract features and capture
    a larger context for NLP use cases. Language-related tasks, especially text generation,
    have a certain temporal aspect associated with them. Hence, the next obvious question
    is, can we leverage CNNs for understanding temporal features, just like RNNs do?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have been exploring the use of CNNs for temporal or sequential processing
    for quite some time. While we discussed how CNNs are a good choice for capturing
    the context of a given word, this presents a problem for certain use cases. For
    instance, tasks such as language modeling/text generation require models to understand
    context, but only from one side. In simple words, a language model works by looking
    at words that have already been processed (past context) to generate future words.
    But a CNN can span to future timesteps as well.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Digressing a bit from the NLP domain, the works by Van den Oord et al. on PixelCNNs^(12)
    and WaveNets^(13) are particularly important to understand the use of CNNs in
    a temporal setting. They present the concept of **causal convolutions** to ensure
    CNNs only utilize past and not future context. This concept is highlighted in
    the following diagram:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![A chain link fence  Description automatically generated](img/B16176_09_14.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Causal padding for CNNs Based on Van den Oord et al.^(13) Figure
    2'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Causal convolutions ensure that the model, at any given time step *t*, makes
    predictions of the type *p*(*x*[t+1] *|* *x*[1:][t]) and doesn't depend on future
    timesteps *x*[t+1], *x*[t+2] … *x*[t+][T], as depicted in *Figure 9.14*. During
    training, conditional predictions for all timesteps can be made in parallel; the
    generation/inference step is sequential though; the output at every timestep is
    fed back into the model for the next timestep's prediction.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Since this setup does not have any recurrent connections, the model trains faster,
    even for longer sequences. The setup for causal convolutions originated for image
    and audio generation use cases but has been extended to NLP use cases as well.
    The authors of the WaveNet paper additionally made use of a concept called *dilated
    convolutions* to give the model larger receptive fields without requiring very
    deep architectures.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: This idea of using CNNs to capture and use temporal components has opened up
    doors for further exploration.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the more involved concepts of attention and transformer
    architectures in the next chapter, it is important to highlight some important
    works which preceded them:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '*Neural Machine Translation in Time* by Kalchbrenner et al.^(14) presents the
    ByteNet neural translation model based on encoder-decoder architecture. The overall
    setup makes use of 1-D causal convolutions, along with dilated kernels, to provide
    state-of-the-art performance on English to German translation tasks.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dauphin et al. presented a language model based on Gated Convolutions in their
    work titled *Language Modeling with Gated Convolutional Networks*.^(15) They observed
    that their Gated Convolutions provide remarkable training speedup and lower memory
    footprint.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works by Gehring et al.^(16) and Lea et al.^(17) explored these ideas further
    and provided even better results.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested readers may also explore the paper titled *An Empirical Evaluation
    of Generic Convolutional and Recurrent Networks for Sequence Modeling* by Bai
    et al.^(18) This paper provides a nice overview of RNN- and CNN-based architectures
    for sequence modeling tasks.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our discussion of the building blocks of older architectures
    for language modeling.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on completing a complex chapter involving a large number of
    concepts. In this chapter, we covered various concepts associated with handling
    textual data for the task of text generation. We started off by developing an
    understanding of different text representation models. We covered most of the
    widely used representation models, from Bag of Words to word2vec and even FastText.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The next section of the chapter focused on developing an understanding of RNN-based
    text generation models. We briefly discussed what comprises a language model and
    how we can prepare a dataset for such a task. We then trained a character-based
    language model to generate synthetic text samples. We touched upon different decoding
    strategies and used them to understand different outputs from our RNN based-language
    model. We also delved into a few variants, such as stacked LSTMs and bidirectional
    LSTM-based language models. Finally, we discussed the usage of convolutional networks
    in the NLP space.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the building blocks of some of the most
    recent and powerful architectures in the NLP domain, including attention and transformers
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation
    of Word Representations in Vector Space*. arXiv. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rumelhart, D.E., & McClelland, J.L. (1987). *Distributed Representations*,
    in Parallel Distributed Processing: Explorations in the Microstructure of Cognition:
    Foundations, pp.77-109\. MIT Press. [https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf)'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pennington, J., Socher, R., & Manning, C.D. (2014). *GloVe: Global Vectors
    for Word Representation*. Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP). [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *Enriching Word
    Vectors with Subword Information*. arXiv. [https://arxiv.org/abs/1607.04606](https://arxiv.org/abs/1607.04606)
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). *The Curious
    Case of Neural Text Degeneration*. arXiv. [https://arxiv.org/abs/1904.09751](https://arxiv.org/abs/1904.09751)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Graves, A., Mohamed, A., & Hinton, G. (2013). *Speech Recognition with Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pascanu, R., Gulcehre, C., Cho, K., & Bengio, Y. (2013). *How to Construct Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026)
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collobert, R., Weston, J., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
    *Natural Language Processing (almost) from Scratch*. arXiv. [https://arxiv.org/abs/1103.0398](https://arxiv.org/abs/1103.0398)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhang, X., Zhao, J., & LeCun, Y. (2015). *Character-level Convolutional Networks
    for Text Classification*. arXiv. [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kim, Y. (2014). *Convolutional Neural Networks for Sentence Classification*.
    arXiv. [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lai, S., Xu, L., Liu, K., & Zhao, J. (2015). *Recurrent Convolutional Neural
    Networks for Text Classification*. Proceedings of the Twenty-Ninth AAAI Conference
    on Artifical Intelligence. [http://zhengyima.com/my/pdfs/Textrcnn.pdf](http://zhengyima.com/my/pdfs/Textrcnn.pdf)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &
    Kavukcuoglu, K. (2016). *Conditional Image Generation with PixelCNN Decoders*.
    arXiv. [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'van den Oord, A., Dieleman, S., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,
    N., Senior, A., Kavukcuoglu, K. (2016). *WaveNet: A Generative Model for Raw Audio*.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kalchbrenner, N., Espeholt, L., Simonyan, K., van den Oord, A., Graves, A.,
    & Kavukcuoglu, K. (2016). *Neural Machine Translation in Linear Time*. arXiv.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dauphin, Y.N., Fan, A., Auli, M., & Grangier, D. (2016). *Language Modeling
    with Gated Convolutional Networks*. arXiv. [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y.N. (2017). *Convolutional
    Sequence to Sequence Learning*. arXiv. [https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lea, C., Flynn, M.D., Vidal, R., Reiter, A., & Hager, G.D. (2016). *Temporal
    Convolutional Networks for Action Segmentation and Detection*. arXiv. [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bai, S., Kolter, J.Z., & Koltun, V. (2018). *An Empirical Evaluation of Generic
    Convolutional and Recurrent Networks for Sequence Modeling*. arXiv. [https://arxiv.org/abs/1803.01271](https://arxiv.org/abs/1803.01271)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
