- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rise of Methods for Text Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we discussed different methods and techniques to
    develop and train generative models. Particularly, in *Chapter 6*, *Image Generation
    with GANs*, we discussed the taxonomy of generative models and introduced explicit
    and implicit classes. Throughout this book, our focus has been on developing generative
    models in the vision space, utilizing image and video datasets. The advancements
    in the field of deep learning for computer vision and ease of understanding were
    the major reasons behind such a focused introduction.
  prefs: []
  type: TYPE_NORMAL
- en: In the past couple of years though, **Natural Language Processing** (**NLP**)
    or processing of textual data has seen great interest and research. Text is not
    just another unstructured type of data; there's a lot more to it than what meets
    the eye. Textual data is a representation of our thoughts, ideas, knowledge, and
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter and the next, we will focus on understanding concepts related
    to NLP and generative models for textual data. We will cover different concepts,
    architectures, and components associated with generative models for textual data,
    with a focus on the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of traditional ways of representing textual data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed representation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN-based text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM variants and convolutions for text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will cover the internal workings of different architectures and key contributions
    that have enabled text generation use cases. We will also build and train these
    architectures to get a better understanding of them. Readers should also note
    that while we will go deep into key contributions and related details across *Chapter
    9*, *The Rise of Methods for Text Generation*, and *Chapter 10*, *NLP 2.0: Using
    Transformers to Generate Text*, some of these models are extremely large to train
    on commodity hardware. We will make use of certain high-level Python packages
    wherever necessary to avoid complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the modeling aspects, let's get started by understanding
    how to represent textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Representing text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language is one of the most complex aspects of our existence. We use language
    to communicate our thoughts and choices. Every language is defined with a list
    of characters called the alphabet, a vocabulary, and a set of rules called grammar.
    Yet it is not a trivial task to understand and learn a language. Languages are
    complex and have fuzzy grammatical rules and structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text is a representation of language that helps us communicate and share. This
    makes it a perfect area of research to expand the horizons of what artificial
    intelligence can achieve. Text is a type of unstructured data that cannot directly
    be used by any of the known algorithms. Machine learning and deep learning algorithms
    in general work with numbers, matrices, vectors, and so on. This, in turn, raises
    the question: how can we represent text for different language-related tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: Bag of Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, every language consists of a defined list of characters
    (alphabet), which are combined to form words (vocabulary). Traditionally, **Bag
    of Words** (**BoW**) has been one of the most popular methods for representing
    textual information.
  prefs: []
  type: TYPE_NORMAL
- en: 'BoW is a simple and flexible approach to transforming text into vector form.
    This transformation helps not only in extracting features from raw text, but also
    in making it fit for consumption by different algorithms and architectures. As
    the name suggests, the BoW model of representation utilizes each word as a basic
    unit of measurement. A BoW model describes the occurrence of words within a given
    corpus of text. To build a BoW model for representation, we require two major
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A vocabulary**: A collection of known words from the corpus of text to be
    analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A measure of occurrence**: Something that we choose based upon the application/task
    at hand. For instance, counting the occurrence of each word, known as term frequency,
    is one such measure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed discussion related to the BoW model is beyond the scope of this chapter.
    We are presenting a high-level overview as a primer before more complex topics
    are introduced later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The BoW model is called a "bag" to highlight the simplicity and the fact that
    we overlook any ordering of the occurrences. In other words, the BoW model discards
    any order or structure-related information of the words in a given text. This
    might sound like a big issue but until recently, the BoW model remained quite
    a popular and effective choice for representing textual data. Let's have a quick
    look at a few examples to understand how this simple method works.
  prefs: []
  type: TYPE_NORMAL
- en: '"Some say the world will end in fire,Some say in ice.From what I have tasted
    of desire I hold with those who favour fire."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Define a vocabulary**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first and foremost step is to define a list of known words from our corpus.
    For ease of understanding and practical reasons, we can ignore the case and punctuation
    marks for now. The vocabulary, or unique words, thus are {some, say, the, world,
    will, end, in, fire, ice, from, what, i, have, tasted, of, desire, hold, with,
    those, who, favour}.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This vocabulary is a set of 21 unique words in a corpus of 26 words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Define a metric of occurrence**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have the vocabulary set, we need to define how we will measure the
    occurrence of each word from the vocabulary. As we mentioned earlier, there are
    a number of ways to do so. One such metric is simply checking if a specific word
    is present or absent. We use a 0 if the word is absent or a 1 if it is present.
    The sentence "some say ice" can thus be scored as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'some: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'say: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'world: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'will: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'end: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'in: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fire: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ice: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the overall vector will look like [1, 1, 0, 0, 0, 0, 0, 0, 1].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are a few other metrics that have been developed over the years. The
    most widely used metrics are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Term frequency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF, as seen in *Chapter 7*, *Style Transfer with GANs*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These steps provide a high-level glimpse into how the BoW model helps us represent
    textual data as numbers or vectors. The overall vector representation of our excerpt
    from the poem is depicted in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B16176_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: BoW representation'
  prefs: []
  type: TYPE_NORMAL
- en: Each row in the matrix corresponds to one line from the poem, while the unique
    words from the vocabulary form the columns. Each row thus is simply the vector
    representation of the text under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few additional steps involved in improving the outcome of this
    method. The refinements are related to vocabulary and scoring aspects. Managing
    the vocabulary is very important; often, a corpus of text can increase in size
    quite rapidly. A few common methods of handling vocabularies are:'
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring punctuation marks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignoring case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing frequently occurring words (or stopwords) like a, an, the, this, and
    so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods to use the root form of words, such as *stop* in place of *stopping*.
    Stemming and lemmatization are two such methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling spelling mistakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We already discussed different scoring methods and how they help in capturing
    certain important features. BoW is simple, yet is an effective tool that serves
    as a good starting point for most NLP tasks. Yet there are a few issues which
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing context**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned earlier, the BoW model does not consider the ordering or structure
    of the text. By simply discarding information related to ordering, the vectors
    lose out on capturing the context in which the underlying text was used. For instance,
    the sentences "I am sure about it" and "Am I sure about it?" would have identical
    vector representations, yet they express different thoughts. Expanding BoW models
    to include n-grams (contiguous terms) instead of singular terms does help in capturing
    some context, but in a very limited way.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Vocabulary and sparse vectors**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the corpus size increases, so does the vocabulary. The steps required to
    manage vocabulary size require a lot of oversight and manual effort. Due to the
    way this model works, a large vocabulary leads to very sparse vectors. Sparse
    vectors pose issues with modeling and computation requirements (space and time).
    Aggressive pruning and vocabulary management steps do help to a certain extent
    but can lead to the loss of important features as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we discussed how the BoW model helps in transforming text into vector
    form, along with a few issues with this setup. In the next section, we will move
    on to a few more involved representation methods that alleviate some of these
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Bag of Words model is an easy-to-understand way of transforming words into
    vector form. This process is generally termed *vectorization*. While it is a useful
    method, the BoW model has its limitations when it comes to capturing context,
    along with sparsity-related issues. Since deep learning architectures are becoming
    de facto state-of-the-art systems in most spaces, it is obvious that we should
    be leveraging them for NLP tasks as well. Apart from the issues mentioned earlier,
    the sparse and large (wide) vectors from the BoW model are another aspect which
    can be tackled using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: A simple alternative that handles the sparsity issue can be implemented by encoding
    each word as a unique number. Continuing with the example from the previous section,
    "some say ice", we could assign 1 to "some", 2 to "say", 3 to "ice", and so on.
    This would result in a dense vector, [1, 2, 3]. This is an efficient utilization
    of space and we end up with vectors where all the elements are full. However,
    the limitation of missing context still remains. Since the numbers are arbitrary,
    they hardly capture any context on their own. On the contrary, arbitrarily mapping
    numbers to words is not very interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretability** is an important requirement when it comes to NLP tasks.
    For computer vision use cases, visual cues are good enough indicators for understanding
    how a model is perceiving or generating outputs (though quantification is also
    a problem there, but we can skip it for now). For NLP tasks, since the textual
    data is first required to be transformed into a vector, it is important to understand
    what those vectors capture and how they are used by the models.'
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections, we will cover some of the popular vectorization techniques
    that try to capture context while limiting the sparsity of the vectors as well.
    Please note that there are a number of other methods (such as SVD-based methods
    and co-occurrence matrices) as well that help in vectorizing textual data. In
    this section, we will be covering only those which are helpful in understanding
    later sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The English Oxford dictionary has about 600k unique words and is growing year
    on year. Yet those words are not independent terms; they have some relationship
    to each other. The premise of the word2vec model is to learn high-quality vector
    representations that capture context. This is better summarized by the famous
    quote by J.R. Firth *"you shall know a word by the company it keeps"*.
  prefs: []
  type: TYPE_NORMAL
- en: In their work titled *Efficient Estimation of Word Representations in Vector
    Space*, Mikolov et al.¹ present two different models that learn vector representations
    of words from a large corpus. Word2Vec is a software implementation of these models
    which is classified as an iterative approach to learning such embeddings. Instead
    of taking the whole corpus into account in one go, this approach tries to iteratively
    learn to encode each word's representation, along with its context. This idea
    of learning word representations as dense context vectors is not a new one. It
    was first proposed by Rumelhart et al. in 1990². They presented how a neural network
    is able to learn representations, with similar words ending up in the same clusters.
    The ability to have vector forms of words that capture some notion of similarity
    is quite a powerful one. Let's see in detail how the word2vec models achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Bag of Words (CBOW) Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Continuous Bag of Words model is an extension of the Bag of Words model
    we discussed in the previous section. The key aspect of this model is the context
    window. A context window is defined as a sliding window of a fixed size moving
    along a sentence. The word in the middle is termed the *target*, and the terms
    to its left and right within the window are the *context terms*. The CBOW model
    works by predicting the target term, given its context terms.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's consider a reference sentence, "some say the *world* will
    end in fire". If we have a window size of 4 and a target term of *world*, the
    context terms would be {say, the} and {will, end}. The model inputs are tuples
    of the form (context terms, target term), which are then passed through a neural
    network to learn the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Continuous Bag of Words model setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, the context terms, denoted as ![](img/B16176_09_001.png),
    are passed as input to the model to predict the target term, denoted as *w*[t].
    The overall working of the CBOW model can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The context terms (![](img/B16176_09_001.png)) and the target term (*y*) are
    transformed into one-hot encodings (or label-encodings) and training data is prepared
    in the form of tuples: (![](img/B16176_09_001.png), *y*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We average the context vectors to get ![](img/B16176_09_004.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final output scoring vector *z* is calculated as a dot product between the
    average vector *v'* and the output matrix *W'*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output scoring vector is transformed into a probability using a softmax
    function; that is, *y' = softmax(z)*, where *y'* should correspond to one of the
    terms in the vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final aim would be to train the neural network such that *y'* and the actual
    target *y* become as close as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors proposed using a cost function such as cross-entropy to train the
    network and learn such embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The skip-gram model is the second variant presented in the paper for learning
    word embeddings. In essence, this model works in exactly the opposite way to the
    CBOW model. In other words, in the case of skip-gram, we input a word (center/target
    word) and predict the context terms as the model output. Let's use the same example
    as before, "some say the *world* will end in fire". Here, we will start with *world*
    as our input term and train a model to predict {say, the, will, end} as context
    terms with high probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the skip-gram model; as expected, this is a mirror
    image of the CBOW setup we discussed in *Figure 9.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Skip-gram model setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'The step by step working skip-gram model can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the one-hot encoded representation of the center word *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get the word embedding representation of *x* by taking a dot product of *x*
    and *W*. The embedded representation is given as *v = W.x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate the output score vector *z* by taking a dot product of *W'* and
    *v*; that is, *z = W'.v*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scoring vector is transformed into output probabilities using a softmax
    layer to generate *y'*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final aim would be to train the neural network such that *y'* and the actual
    context *y* become as close as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of skip-gram, we have multiple input-output training pairs for any
    given center word. This model treats all context terms equally, irrespective of
    their distance from the center word in the context window. This allows us to use
    cross-entropy as the cost function with a strong conditional independence assumption.
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve the outcomes and speed up the training process, the authors
    introduced some simple yet effective tricks. Concepts such as *negative sampling,
    noise contrastive estimation* and *hierarchical softmax* are a few such techniques
    which have been leveraged. For a detailed understanding of CBOW and skip-gram,
    readers are requested to go through the cited paper by Mikolov et al.,¹ where
    the authors have given detailed explanations of each of the steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Just a few lines of code and we have our word2vec representations of our vocabulary
    ready. Upon checking, we find that there are 19,000 unique words in our vocabulary,
    and that we have a vector representation for each. The following snippet shows
    how we can get the vector representation of any word. We will also demonstrate
    how to get words that are most similar to a given word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding outputs show a 32-dimensional vector for the word *sun*. We also
    display words that are most similar to the word *god*. We can clearly see that
    words such as believe, existence, and so on seem to be the most similar, which
    makes sense given the dataset we used. For interested readers, we have a 3-dimensional
    vector space representation using TensorBoard showcased in the corresponding notebook.
    The TensorBoard representation helps us visually understand the embedding space
    and see how these vectors interact.
  prefs: []
  type: TYPE_NORMAL
- en: GloVe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The word2vec models helped in improving performance for various NLP tasks. Continuing
    with the same momentum, another important implementation called GloVe came into
    the picture. GloVe or *Global Vectors for Word Representation* was published by
    Pennington et al. in 2014 to improve upon the known word representation techniques.³
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen, word2vec models work by considering the local context (a defined
    window) of the words in the vocabulary. Even though this works remarkably well,
    it is a bit rough around the edges. The fact that words may mean different things
    in different contexts requires us to understand not just the local but the global
    context as well. GloVe tries to work upon the global context while learning the
    word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: There are classical techniques for this, such as **Latent Semantic Analysis**
    (**LSA**), which are based on matrix factorization and do a good job at capturing
    global context, but are not so good at things such as vector math.
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe is a method which tries to get the best of both worlds in order to learn
    better word representations. The GloVe algorithm consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare a word co-occurrence matrix *X*, such that each element, *x*[i][j],
    represents how often the word *j* appears in the context of the word *i*. GloVe
    makes use of two fixed size windows that help in capturing context before the
    word and context after the word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The co-occurrence matrix *X* is updated with a decay factor to penalize terms
    that are farther apart in the context. The decay factor is defined as ![](img/B16176_09_005.png),
    where offset is the distance from the word under consideration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we prepare the GloVe equation as the following soft constraint:![](img/B16176_09_006.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, *w*[i] is the vector for the main word, *w*[j] is the vector for the context
    word, and *b*[i], *b*[j] are the corresponding bias terms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final step is to use the preceding constraint to define the cost function,
    which is given as:![](img/B16176_09_007.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, *f* is a weighting function, defined as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16176_09_008.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The authors of the paper achieved the best results with ![](img/B16176_09_009.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to word2vec models, GloVe embeddings also achieve good results and the
    authors present results where they show GloVe outperforming word2vec. They attribute
    this to better problem formulation and the global context being captured.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, both models perform more or less similarly. As larger vocabularies
    are required to get better embeddings (for both word2vec and GloVe), for most
    practical use cases, pretrained embeddings are available and used.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained GloVe vectors are available through a number of packages, such as
    `spacy`. Interested readers may wish to explore the `spacy` package for more details.
  prefs: []
  type: TYPE_NORMAL
- en: FastText
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word2Vec and GloVe are powerful methods which have nice properties when it comes
    to encoding words in the vector space. Both techniques work nicely when it comes
    to getting vector representation of words that are in the vocabulary, but they
    do not have clear answers for terms that are outside of the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The word is the fundamental unit in the case of the word2vec and GloVe methods.
    This assumption is challenged and improved upon in the FastText implementation.
    The word representation aspect of FastText is based on the paper, *Enriching Word
    Vectors with Subword Information* by Bojanowski et al. in 2017.⁴ This work decomposes
    each word into a set of n-grams. This helps in capturing and learning vector representations
    of different combinations of characters, as opposed to the whole word in earlier
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we consider the word "India" and *n=3* for the n-gram setup,
    it will decompose the word into {<india>, <in, ind, ndi, dia, ia>}. The symbols
    "<" and ">" are special characters to denote the start and end of the original
    word. This helps in differentiating between <in>, which represents the whole word,
    and <in, which is an n-gram. This approach helps FastText generate embeddings
    for *out of vocabulary* terms as well. This can be done by adding and averaging
    the vector representation of required n-grams.
  prefs: []
  type: TYPE_NORMAL
- en: FastText is shown to drastically improve performance when it comes to use cases
    where there is a high chance of new/out of vocabulary terms. FastText was developed
    by researchers at **Facebook AI Research** (**FAIR**), which shouldn't come as
    a surprise as the kind of content generated on social media platforms such as
    Facebook is huge and ever-changing.
  prefs: []
  type: TYPE_NORMAL
- en: With its added improvements come a few disadvantages as well. Since the basic
    unit in this case is an n-gram, the amount of time required to train/learn such
    representations is higher than previous techniques. The n-gram approach also increases
    the amount of memory required to train such a model. However, the authors of the
    paper point out that a hashing trick helps in controlling the memory requirements
    to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease of understanding, let''s again make use of our well-known Python library,
    `gensim`. We will extend upon the same dataset and pre-processing steps that we
    performed for the word2vec model exercise in the previous section. The following
    snippet prepares the FastText model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The word2vec model fails to return a vector representation of the word "sunny"
    as it is not in the trained vocabulary. The following snippet shows how FastText
    is still able to generate a vector representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This showcases how FastText improves upon word2vec- and GloVe-based representation
    techniques. We can easily handle out of vocabulary terms, all the while ensuring
    context-based dense representations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now use this understanding to develop a text generation model.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation and the magic of LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed different ways of representing textual
    data in order to make it fit for consumption by different NLP algorithms. In this
    section, we will leverage this understanding of text representation to work our
    way toward building text generation models.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have built models using feedforward networks consisting of different
    kinds and combinations of layers. These networks work with one training example
    at a time, which is independent of other training samples. We say that the samples
    are **independent and identically distributed**, or **IID**. Language, or text,
    is a bit different.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous sections, words change their meaning based on
    the context they are being used in. In other words, if we were to develop and
    train a language generation model, we would have to ensure the model understands
    the context of its input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are a class of neural networks that
    allow previous outputs to be used as inputs, along with memory or hidden units.
    This awareness of previous inputs helps in capturing context, and provides us
    with the ability to handle variable-length input sequences (sentences are hardly
    ever of the same length). A typical RNN is depicted in the following diagram,
    in both actual and unrolled form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: A typical RNN'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.4*, at time *t*[1], input *x*[1] generates output *y*[1].
    At time *t*[2], *x*[2] along with *y*[1] (the previous output) generate output
    *y*[2], and so on. Unlike typical feedforward networks where every input is independent
    of the others, RNN introduces the notion of previous outputs impacting the current
    and upcoming ones.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have a few different variants to them, namely, **Gated Recurrent Units**
    (**GRUs**) and **Long Short-Term Memory** (**LSTMs**). The vanilla RNN described
    previously works in auto-regressive settings well. Yet it has issues with longer
    context windows (vanishing gradients). GRUs and LSTMs try to overcome such issues
    by using different gates and memory units. Introduced by Hochreiter and Schmidhuber
    in 1997, LSTMs can remember information from really long sequence-based data.
    LSTMs consist of three gates called input, output, and forget gates. This is depicted
    in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_09_05.png)Figure 9.5: Different gates of an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed understanding of LSTMs, you may refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: We will now focus on defining the task of text generation more formally.
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP-based solutions are quite effective and can be seen all around us. The most
    prominent example is the autocomplete feature on most smartphone keyboards, search
    engines (Google, Bing, and so on), and even word processors (like MS Word).
  prefs: []
  type: TYPE_NORMAL
- en: Autocomplete is a common name for a formal concept called *language modeling*.
    In simple words, a language model takes certain text as the input context to generate
    the next set of words as the output. This is interesting because a language model
    tries to understand the input context, the language structure, and rules to predict
    the next word(s). We use it in the form of text completion utilities on search
    engines, chat platforms, emails, and more all the time. Language models are a
    perfect real-life application of NLP and showcase the power of RNNs. In this section,
    we will work toward building an understanding, as well as training an RNN-based
    language model for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by understanding the process of generating a training dataset.
    We can do this with the help of the following image. This image depicts a word-level
    language model; that is, a model for which a word is the basic unit. Along the
    same lines, we can develop character-level, phrase-level, or even document-level
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B16176_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Training data generation process for a language model'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, a language model looks at the context to generate the
    next set of words. This context is also called a sliding window, which moves across
    the input sentence from left to right (right to left for languages that are written
    from right to left). The sliding window depicted in *Figure 9.6* spans three words,
    which act as input. The corresponding output for each training data point is the
    immediate next word after the window (or a set of words if the aim is to predict
    the next phrase). We thus prepare our training dataset, which consists of tuples
    of the form ({context terms}, next_word). The sliding window helps us to generate
    a good number of training samples from every sentence in the training dataset
    without explicit labeling.
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset is then used to train an RNN-based language model. In
    practice, we typically use LSTMs or GRU units in place of vanilla RNN units. We
    discussed earlier that RNNs have the ability to auto-regress on previous timestep
    values. In the context of language models, we auto-regress on the context terms
    and the model generates the corresponding next word. We then make use of **backpropagation
    through time** (**BPTT**) to update model weights through gradient descent until
    the required performance is achieved. We discussed BPTT in detail in *Chapter
    3*, *Building Blocks of Deep Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a fair understanding of what a language model is and what steps
    are involved in preparing the training dataset, along with the model setup. Let's
    now implement some of these concepts using TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Character-level language model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed the basics of language modeling in the previous section. In this
    section, we will build and train our own language model, but with a twist. In
    contrast to the discussion in the previous section, here, we will work at the
    character level, not at the word level. In simple words, we will work toward building
    a model that takes a few characters as input (context) to generate the next set
    of characters. This choice of a more granular language model is for the ease of
    training such a model. A character-level language model needs to worry about a
    much smaller vocabulary, or number of unique characters, compared to a word-level
    language model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build our language model, the first step is to get a dataset to use as a
    training source. Project Gutenberg is a volunteer effort to digitize historical
    works and make them available as free downloads. Since we need lots of data to
    train a language model, we will pick one of the biggest available books, *War
    and Peace* by Leo Tolstoy. This book is available for download at the following
    URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.gutenberg.org/ebooks/2600](https://www.gutenberg.org/ebooks/2600)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet loads the book''s content for use as our source dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to prepare our dataset for the model. As we discussed in the
    *Representing text* section, textual data is transformed into vectors using word
    representation models. One way to do so is to first transform them into one-hot
    encoded vectors, which are then transformed into dense representations using models
    such as word2vec. The other way is to transform them into an arbitrary numerical
    representation first and then train an embedding layer along with the rest of
    the RNN-based language model. In this case, we are using the latter approach of
    training an embedding layer alongside the rest of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet prepares a mapping of individual characters to their
    integer mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each unique character is mapped to an integer; for instance,
    `\n` is mapped to 0, `!` is mapped to 3, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For optimal memory utilization, we can make use of the `tf.data` API to slice
    our data into manageable slices. We restrict our input sequences to 100 characters
    long, and this API helps us create contiguous slices of this dataset. This is
    showcased in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have created the model object. As is apparent from the snippet, the model
    is a stack of embedding, LSTM, and dense layers. The embedding layer helps transform
    raw text into vector form, and is followed by the LSTM and dense layers, which
    learn context and language semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next set of steps involve defining a loss function and compiling the model.
    We will be using sparse categorical cross-entropy as our loss function. The following
    snippet defines the loss function and compiles the model; we are using the Adam
    optimizer for minimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are using TensorFlow with the high-level Keras API, training the model
    is as simple as calling the `fit` function. We train the model for just 10 epochs,
    using the `ModelCheckpoint` callback to save the model''s weights every epoch,
    as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you've trained your very first language model. Now, we'll use
    it to generate some fake text. Before we do that, though, we need to understand
    how we can decode the output generated by our model.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier on, we transformed all the textual data into suitable vector forms
    for training and inference purposes. Now that we have a trained model, the next
    step is to input some context words and generate the next word as output. This
    output generation step is formally known as the **decoding step**. It is termed
    "decoding" because the model outputs a vector which has to be processed to get
    the actual word as output. There are a few different decoding techniques; let''s
    briefly discuss the popular ones: greedy decoding, beam search, and sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: Greedy decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest and fastest decoding strategy. As the name suggests, greedy
    decoding is a method which picks up the highest probability term at every prediction
    step.
  prefs: []
  type: TYPE_NORMAL
- en: While this is fast and efficient, being greedy does create a few issues while
    generating text. By focusing on only the highest probability outputs, the model
    may generate inconsistent or incoherent outputs. In the case of character-language
    models, this may even result in outputs that are non-dictionary words. Greedy
    decoding also limits the variance of outputs, which may result in repetitive content
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Beam search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Beam search is a widely used alternative to greedy decoding. This decoding
    strategy, instead of picking the highest probability term, keeps track of *n*
    possible outputs at every timestep. The following diagram illustrates the beam
    search decoding strategy. It shows multiple beams forming from step 0, creating
    a tree-like structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Beam search-based decoding strategy'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.7*, the beam search strategy works by keeping track of
    *n* predictions at every timestep and finally selects the path with the **overall**
    highest probability, highlighted with bold lines in the figure. Let's analyze
    the beam search decoding example used in the preceding diagram step by step, assuming
    a beam size of 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'At time step *t*[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: The model predicts the following three words (with probabilities) as (**the**,
    0.3), (**when**, 0.6), and (**and**, 0.1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of greedy decoding, we would have selected "when" as it has the
    highest probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we will keep track of the top two outputs as our beam size is
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At time step *t*[2]:'
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the same steps; that is, we keep track of the top two outputs from
    each of the two beams.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The beam-wise scores are calculated by multiplying the probabilities along
    the branches, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(when, 0.6) –> (the, 0.4) = 0.6*0.4 = 0.24*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(the, 0.3) –> (war, 0.9) = 0.3*0.9 = 0.27*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the above discussion, the final output generated is "It was July, 1805
    *the war*". This output had a final probability of 0.27 in comparison to an output
    like "It was July, 1805 *when the*", which had a score of 0.24, and is what greedy
    decoding would have given us.
  prefs: []
  type: TYPE_NORMAL
- en: This decoding strategy drastically improves upon the naïve greedy decoding strategy
    we discussed in the previous section. This, in a way, provides the language model
    with additional capabilities to pick the best possible outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sampling is a process wherein a predefined a number of observations are selected
    from a larger population. As an improvement over greedy decoding, a random sampling
    decoding method can be employed to address the variation/repetition issue. In
    general, a sampling-based decoding strategy helps in selecting the next word conditioned
    on the context so far, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_09_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[t] is the output at time step *t* that''s been conditioned on words
    that are generated until time step *t-1*. Continuing with the example from our
    previous decoding strategies, the following image highlights how a sampling-based
    decoding strategy would select the next word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, funnel chart  Description automatically generated](img/B16176_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Sampling-based decoding strategy'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9.8*, this method picks a random word at every timestep
    from the given conditional probability. In the case of our example, the model
    ended by randomly selecting **in** and then **Paris** as subsequent outputs. If
    you notice carefully, at timestep *t*[1], the model ends up selecting the word
    with the least probability. This brings in a much-required randomness associated
    with the way humans use language. Holtzman et al. in their work titled *The Curious
    Case of Neural Text Degeneration*⁵ present this exact argument by stating that
    humans do not always simply use the words with the highest probability. They present
    different scenarios and examples to highlight how language is a random choice
    of words and not a typical high probability curve formed by beam search or greedy
    decoding.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to an important parameter called *temperature*.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed earlier, a sampling-based decoding strategy helps with improving
    the randomness of the output. However, too much randomness is also not ideal,
    as it can lead to gibberish and incoherent results. To control this amount of
    randomness, we can introduce a tunable parameter called temperature. This parameter
    helps to increase the likelihood of high probability terms while reducing the
    likelihood of low probability ones, which leads to sharper distributions. High
    temperature leads to more randomness, while lower temperature brings in predictability.
    An important point to note is that this can be applied to any decoding strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Top-k sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beam search and sampling-based decoding strategies both have their own set of
    advantages and disadvantages. Top-*k* sampling is a hybrid strategy which takes
    the best of both worlds to provide an even more sophisticated decoding method.
    In simple terms, at every timestep, instead of selecting a random word, we keep
    track of the *top k terms* (similar to beam search) and redistribute the probabilities
    among them. This gives the model an additional chance of generating coherent samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Decoding strategies'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a decent enough understanding of some of the most widely used
    decoding strategies, it's time to see them in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to prepare a utility function `generate_text` to generate
    the next word based on a given decoding strategy, as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The code first transforms raw input text into integer indices. We then use
    the model to make predictions which are manipulated based on the mode selected,
    greedy or sampling. We already have a character-language model trained from the
    previous exercise, along with a utility to help us generate the next word based
    on a decoding strategy of choice. We use both of these in the following snippet
    to understand the different outputs that are generated using different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of using the same seed with different decoding strategies are showcased
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, letter, email  Description automatically
    generated](img/B16176_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Text generation based on different decoding strategies. The text
    in bold is the seed text, followed by the output text generated by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: This output highlights some of the issues as well as the salient features of
    all the decoding strategies we've discussed so far. We can see how the increase
    in temperature makes the model more expressive. We can also observe that the model
    has learned to pair up quotation marks and even use punctuation. The model also
    seems to have learned how to use capitalization. The added expressiveness of the
    temperature parameter comes at the cost of the stability of the model. Thus, there
    is usually a trade-off between expressiveness and stability.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our first method for generating text; we leveraged RNNs (LSTMs
    in particular) to generate text using different decoding strategies. Next, we
    will look at some variations of the LSTM model, as well as convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM variants and convolutions for text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are extremely useful when it comes to handling sequential datasets. We
    saw in the previous section how a simple model effectively learned to generate
    text based on what it learned from the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the years, there have been a number of enhancements in the way we model
    and use RNNs. In this section, we will discuss two widely used variants of the
    single-layer LSTM network we discussed in the previous section: stacked and bidirectional
    LSTMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacked LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are well aware of how the depth of a neural network helps it learn complex
    and abstract concepts when it comes to computer vision tasks. Along the same lines,
    a stacked LSTM architecture, which has multiple layers of LSTMs stacked one after
    the other, has been shown to give considerable improvements. Stacked LSTMs were
    first presented by Graves et al. in their work *Speech Recognition with Deep Recurrent
    Neural Networks*.⁶ They highlight the fact that depth – multiple layers of RNNs
    – has a greater impact on performance compared to the number of units per layer.
  prefs: []
  type: TYPE_NORMAL
- en: Though there isn't any theoretical proof to explain this performance gain, empirical
    results help us understand the impact. These enhancements can be attributed to
    the model's capacity to learn complex features and even abstract representations
    of inputs. Since there is a time component associated with LSTMs and RNNs in general,
    deeper networks learn the ability to operate at different time scales as well.⁷
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The dataset, training loop, and even the inference utilities remain as-is. For
    brevity, we have skipped presenting those code snippets again. We will discuss
    the bidirectional argument that we introduce here shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how the results look for this deeper LSTM-based language model.
    The following screenshot demonstrates the results from this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a flower  Description automatically generated](img/B16176_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Text generation based on different decoding strategies for the
    stacked-LSTM based language model'
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see how the generated text is picking up the writing style of
    the book, capitalization, punctuation, and other aspects better than the outputs
    shown in *Figure 9.9*. This highlights some of the advantages we discussed regarding
    deeper RNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second variant that''s very widely used nowadays is the bidirectional LSTM.
    We have already discussed how LSTMs, and RNNs in general, condition their outputs
    by making use of previous timesteps. When it comes to text or any sequence data,
    this means that the LSTM is able to make use of past context to predict future
    timesteps. While this is a very useful property, this is not the best we can achieve.
    Let''s illustrate why this is a limitation through an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, PowerPoint  Description automatically
    generated](img/B16176_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Looking at both past and future context windows for a given word'
  prefs: []
  type: TYPE_NORMAL
- en: 'As is evident from this example, without looking at what is to the right of
    the target word "Teddy", the model would not pick up the context properly. To
    handle such scenarios, bidirectional LSTMs were introduced. The idea behind them
    is pretty simple and straightforward. A bidirectional LSTM (or biLSTM) is a combination
    of two LSTM layers that work simultaneously. The first is the usual forward LSTM,
    which takes the input sequence in its original order. The second one is called
    the backward LSTM, which takes **a reversed copy** of the sequence as input. The
    following diagram showcases a typical biLSTM setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B16176_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Bidirectional LSTM setup'
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in *Figure 9.12*, the forward and backward LSTMs work in tandem
    to process the original and reversed copy of the input sequences. Since we have
    two LSTM cells working on different contexts at any given time step, we need a
    way of defining the output that will be used by the downstream layers in the network.
    The outputs can be combined via summation, multiplication, concatenation, or even
    averaging of hidden states. Different deep learning frameworks might set different
    defaults, but the most widely used method is concatenation of the biLSTM outputs.
    Please note that, similar to biLSTM, we can make use of bi-RNNs or even bi-GRUs
    (**Gated Recurrent Units**).
  prefs: []
  type: TYPE_NORMAL
- en: The biLSTM setup has advantages compared to a normal LSTM, as the former can
    look at the future context as well. This advantage also becomes a limitation when
    it is not possible to peek into the future. For the current use case of text generation,
    biLSTMs are leveraged in an encoder-decoder type of architecture. We make use
    of biLSTMs to learn better embeddings of the inputs, but the decoding stage (where
    we use these embeddings to guess the next word) only uses the normal LSTMs. Similar
    to earlier hands-on exercises, we can train this network using the same set of
    utilities. We leave this as an exercise for you; for now, we will move on to convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions and text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RNNs are extremely powerful and expressive when it comes to *sequence-to-sequence*
    tasks such as text generation. Yet they meet a few challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs suffer from vanishing gradients when the context window is very wide. Though
    LSTMs and GRUs overcome that to a certain extent, the context windows are still
    fairly small compared to the typical non-local interaction of words we see in
    normal usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The recurrence aspect of RNNs makes them sequential and eventually slow for
    training as well as inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The architecture we covered in the previous section tries to encode the whole
    input context (or seed text) into a single vector, which is then used by the decoder
    to generate the next set of words. This creates limitations when the seed/context
    is pretty long, as does the fact that the RNN pays a lot more attention to the
    last set of inputs in the context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs have a larger memory footprint compared to other types of neural network
    architectures; that is, they require more parameters and hence more memory during
    their implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the other hand, we have convolutional networks, which are battle-tested in
    the field of computer vision. State-of-the-art architectures make use of CNNs
    to extract features and perform well on different vision tasks. The success of
    CNNs led researchers to explore their application to NLP tasks as well.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind using CNNs for text is to first try to create vector representations
    of **a set of words** rather than individual words. More formally, the idea is
    to generate a vector representation of every sub-sequence of words in a given
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a sample sentence, "Flu outbreak forces schools to close". The
    aim would be to first break down this sentence into all possible sub-sequences,
    such as "Flu outbreak forces", "outbreak forces schools",…, "schools to close",
    and then generate a vector representation of each of these sub-sequences. Though
    such sub-sequences may or may not carry much meaning, they provide us with a way
    to understand words in different contexts, as well as their usage. Since we already
    understand how to prepare dense vector representation of words (see the *Distributed
    representation* section), let's build on top of that to understand how CNNs can
    be leveraged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the preceding example, *Figure 9.13 (A)* depicts each of the
    words in their vector form. The vectors are only 4-dimensional for ease of understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calendar  Description automatically generated](img/B16176_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: (A) Vector representation (1x4) of each word in sample sentence.
    (B) Two kernels/filters of size 3 each. (C) Phrase vectors of dimension 1x2 each
    after taking the Hadamard product, followed by the sum for each kernel with stride
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: The two kernels, each of size 3, are depicted in *Figure 9.13 (B)*. The kernels
    in the case of text/NLP use cases are chosen to be as wide as the word vector
    dimension. The size of 3 signifies the context window each kernel is focusing
    on. Since the kernel width is the same as the word-vector width, we move the kernel
    along the words in the sentence. This constraint on size and movement in one direction
    only is the reason these convolutional filters are termed 1-D convolutions. The
    output phrase vectors are depicted in *Figure 9.13 (C)*.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to deep convolutional neural networks for computer vision use cases,
    the above setup enables us to stack 1-D convolutional layers for NLP use cases
    as well. The greater depth allows the models to capture not just more complex
    representations but also a wider context window (this is analogous to an increase
    in the receptive field for a vision model with depth).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using CNNs for NLP use cases also improves computation speed, as well as reducing
    the memory and time requirements to train such networks. In fact, these are some
    of the advantages that are explored by the following works for NLP tasks using
    1-D CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Natural Language Processing (almost) from Scratch*, Collobert et al.⁸'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Character-level Convolutional Networks for Text Classification*, Zhang et
    al.⁹'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convolutional Neural Networks for Sentence Classification*, Kim^(10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recurrent Convolutional Neural Networks for Text Classification*, Lai and
    Xu et al.^(11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we've discussed how CNNs can be used to extract features and capture
    a larger context for NLP use cases. Language-related tasks, especially text generation,
    have a certain temporal aspect associated with them. Hence, the next obvious question
    is, can we leverage CNNs for understanding temporal features, just like RNNs do?
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have been exploring the use of CNNs for temporal or sequential processing
    for quite some time. While we discussed how CNNs are a good choice for capturing
    the context of a given word, this presents a problem for certain use cases. For
    instance, tasks such as language modeling/text generation require models to understand
    context, but only from one side. In simple words, a language model works by looking
    at words that have already been processed (past context) to generate future words.
    But a CNN can span to future timesteps as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Digressing a bit from the NLP domain, the works by Van den Oord et al. on PixelCNNs^(12)
    and WaveNets^(13) are particularly important to understand the use of CNNs in
    a temporal setting. They present the concept of **causal convolutions** to ensure
    CNNs only utilize past and not future context. This concept is highlighted in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A chain link fence  Description automatically generated](img/B16176_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Causal padding for CNNs Based on Van den Oord et al.^(13) Figure
    2'
  prefs: []
  type: TYPE_NORMAL
- en: Causal convolutions ensure that the model, at any given time step *t*, makes
    predictions of the type *p*(*x*[t+1] *|* *x*[1:][t]) and doesn't depend on future
    timesteps *x*[t+1], *x*[t+2] … *x*[t+][T], as depicted in *Figure 9.14*. During
    training, conditional predictions for all timesteps can be made in parallel; the
    generation/inference step is sequential though; the output at every timestep is
    fed back into the model for the next timestep's prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Since this setup does not have any recurrent connections, the model trains faster,
    even for longer sequences. The setup for causal convolutions originated for image
    and audio generation use cases but has been extended to NLP use cases as well.
    The authors of the WaveNet paper additionally made use of a concept called *dilated
    convolutions* to give the model larger receptive fields without requiring very
    deep architectures.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of using CNNs to capture and use temporal components has opened up
    doors for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the more involved concepts of attention and transformer
    architectures in the next chapter, it is important to highlight some important
    works which preceded them:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Neural Machine Translation in Time* by Kalchbrenner et al.^(14) presents the
    ByteNet neural translation model based on encoder-decoder architecture. The overall
    setup makes use of 1-D causal convolutions, along with dilated kernels, to provide
    state-of-the-art performance on English to German translation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dauphin et al. presented a language model based on Gated Convolutions in their
    work titled *Language Modeling with Gated Convolutional Networks*.^(15) They observed
    that their Gated Convolutions provide remarkable training speedup and lower memory
    footprint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works by Gehring et al.^(16) and Lea et al.^(17) explored these ideas further
    and provided even better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested readers may also explore the paper titled *An Empirical Evaluation
    of Generic Convolutional and Recurrent Networks for Sequence Modeling* by Bai
    et al.^(18) This paper provides a nice overview of RNN- and CNN-based architectures
    for sequence modeling tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our discussion of the building blocks of older architectures
    for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on completing a complex chapter involving a large number of
    concepts. In this chapter, we covered various concepts associated with handling
    textual data for the task of text generation. We started off by developing an
    understanding of different text representation models. We covered most of the
    widely used representation models, from Bag of Words to word2vec and even FastText.
  prefs: []
  type: TYPE_NORMAL
- en: The next section of the chapter focused on developing an understanding of RNN-based
    text generation models. We briefly discussed what comprises a language model and
    how we can prepare a dataset for such a task. We then trained a character-based
    language model to generate synthetic text samples. We touched upon different decoding
    strategies and used them to understand different outputs from our RNN based-language
    model. We also delved into a few variants, such as stacked LSTMs and bidirectional
    LSTM-based language models. Finally, we discussed the usage of convolutional networks
    in the NLP space.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the building blocks of some of the most
    recent and powerful architectures in the NLP domain, including attention and transformers
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation
    of Word Representations in Vector Space*. arXiv. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rumelhart, D.E., & McClelland, J.L. (1987). *Distributed Representations*,
    in Parallel Distributed Processing: Explorations in the Microstructure of Cognition:
    Foundations, pp.77-109\. MIT Press. [https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pennington, J., Socher, R., & Manning, C.D. (2014). *GloVe: Global Vectors
    for Word Representation*. Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP). [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *Enriching Word
    Vectors with Subword Information*. arXiv. [https://arxiv.org/abs/1607.04606](https://arxiv.org/abs/1607.04606)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). *The Curious
    Case of Neural Text Degeneration*. arXiv. [https://arxiv.org/abs/1904.09751](https://arxiv.org/abs/1904.09751)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Graves, A., Mohamed, A., & Hinton, G. (2013). *Speech Recognition with Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pascanu, R., Gulcehre, C., Cho, K., & Bengio, Y. (2013). *How to Construct Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collobert, R., Weston, J., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
    *Natural Language Processing (almost) from Scratch*. arXiv. [https://arxiv.org/abs/1103.0398](https://arxiv.org/abs/1103.0398)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhang, X., Zhao, J., & LeCun, Y. (2015). *Character-level Convolutional Networks
    for Text Classification*. arXiv. [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kim, Y. (2014). *Convolutional Neural Networks for Sentence Classification*.
    arXiv. [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lai, S., Xu, L., Liu, K., & Zhao, J. (2015). *Recurrent Convolutional Neural
    Networks for Text Classification*. Proceedings of the Twenty-Ninth AAAI Conference
    on Artifical Intelligence. [http://zhengyima.com/my/pdfs/Textrcnn.pdf](http://zhengyima.com/my/pdfs/Textrcnn.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &
    Kavukcuoglu, K. (2016). *Conditional Image Generation with PixelCNN Decoders*.
    arXiv. [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'van den Oord, A., Dieleman, S., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,
    N., Senior, A., Kavukcuoglu, K. (2016). *WaveNet: A Generative Model for Raw Audio*.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kalchbrenner, N., Espeholt, L., Simonyan, K., van den Oord, A., Graves, A.,
    & Kavukcuoglu, K. (2016). *Neural Machine Translation in Linear Time*. arXiv.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dauphin, Y.N., Fan, A., Auli, M., & Grangier, D. (2016). *Language Modeling
    with Gated Convolutional Networks*. arXiv. [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y.N. (2017). *Convolutional
    Sequence to Sequence Learning*. arXiv. [https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lea, C., Flynn, M.D., Vidal, R., Reiter, A., & Hager, G.D. (2016). *Temporal
    Convolutional Networks for Action Segmentation and Detection*. arXiv. [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bai, S., Kolter, J.Z., & Koltun, V. (2018). *An Empirical Evaluation of Generic
    Convolutional and Recurrent Networks for Sequence Modeling*. arXiv. [https://arxiv.org/abs/1803.01271](https://arxiv.org/abs/1803.01271)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
