- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: The Rise of Methods for Text Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[文本生成方法的崛起](https://wiki.example.org/the_rise_of_methods_for_text_generation)'
- en: In the preceding chapters, we discussed different methods and techniques to
    develop and train generative models. Particularly, in *Chapter 6*, *Image Generation
    with GANs*, we discussed the taxonomy of generative models and introduced explicit
    and implicit classes. Throughout this book, our focus has been on developing generative
    models in the vision space, utilizing image and video datasets. The advancements
    in the field of deep learning for computer vision and ease of understanding were
    the major reasons behind such a focused introduction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了不同的方法和技术来开发和训练生成模型。特别是在*第6章*“使用GAN生成图像”中，我们讨论了生成模型的分类以及介绍了显式和隐式类。在整本书中，我们的重点一直是在视觉空间中开发生成模型，利用图像和视频数据集。深度学习在计算机视觉领域的发展以及易于理解性是引入这样一个专注介绍的主要原因。
- en: In the past couple of years though, **Natural Language Processing** (**NLP**)
    or processing of textual data has seen great interest and research. Text is not
    just another unstructured type of data; there's a lot more to it than what meets
    the eye. Textual data is a representation of our thoughts, ideas, knowledge, and
    communication.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在过去几年中，**自然语言处理**（**NLP**）或文本数据处理受到了极大的关注和研究。文本不只是另一种无结构类型的数据；其背后还有更多东西超出了表面所见。文本数据代表了我们的思想、想法、知识和交流。
- en: 'In this chapter and the next, we will focus on understanding concepts related
    to NLP and generative models for textual data. We will cover different concepts,
    architectures, and components associated with generative models for textual data,
    with a focus on the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将专注于理解与NLP和文本数据的生成模型相关的概念。我们将在本章中涵盖与文本数据生成模型相关的不同概念、架构和组件，重点关注以下主题：
- en: A brief overview of traditional ways of representing textual data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统表示文本数据方式的简要概述
- en: Distributed representation methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式表示方法
- en: RNN-based text generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RNN的文本生成
- en: LSTM variants and convolutions for text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM变体和文本卷积
- en: 'We will cover the internal workings of different architectures and key contributions
    that have enabled text generation use cases. We will also build and train these
    architectures to get a better understanding of them. Readers should also note
    that while we will go deep into key contributions and related details across *Chapter
    9*, *The Rise of Methods for Text Generation*, and *Chapter 10*, *NLP 2.0: Using
    Transformers to Generate Text*, some of these models are extremely large to train
    on commodity hardware. We will make use of certain high-level Python packages
    wherever necessary to avoid complexity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍不同架构的内部工作原理和使文本生成用例成为可能的关键贡献。我们还将构建和训练这些架构，以更好地理解它们。读者还应该注意，虽然我们将在*第9章*“文本生成方法的崛起”和*第10章*“NLP
    2.0:使用Transformer生成文本”中深入研究关键贡献和相关细节，但这些模型中的一些非常庞大，无法在常规硬件上进行训练。我们将在必要时利用某些高级Python包，以避免复杂性。
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中呈现的所有代码片段都可以直接在Google Colab中运行。由于篇幅原因，未包含依赖项的导入语句，但读者可以参考GitHub存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)。
- en: Before we get into the modeling aspects, let's get started by understanding
    how to represent textual data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入建模方面之前，让我们先了解如何表示文本数据。
- en: Representing text
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示文本
- en: Language is one of the most complex aspects of our existence. We use language
    to communicate our thoughts and choices. Every language is defined with a list
    of characters called the alphabet, a vocabulary, and a set of rules called grammar.
    Yet it is not a trivial task to understand and learn a language. Languages are
    complex and have fuzzy grammatical rules and structures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是我们存在中最复杂的方面之一。我们使用语言来传达我们的思想和选择。每种语言都有一个叫做字母表的字符列表，一个词汇表和一组叫做语法的规则。然而，理解和学习一门语言并不是一项微不足道的任务。语言是复杂的，拥有模糊的语法规则和结构。
- en: 'Text is a representation of language that helps us communicate and share. This
    makes it a perfect area of research to expand the horizons of what artificial
    intelligence can achieve. Text is a type of unstructured data that cannot directly
    be used by any of the known algorithms. Machine learning and deep learning algorithms
    in general work with numbers, matrices, vectors, and so on. This, in turn, raises
    the question: how can we represent text for different language-related tasks?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是语言的一种表达形式，帮助我们交流和分享。这使得它成为研究的完美领域，以扩展人工智能可以实现的范围。文本是一种无结构数据，不能直接被任何已知算法使用。机器学习和深度学习算法通常使用数字、矩阵、向量等进行工作。这又引出了一个问题：我们如何为不同的与语言相关的任务表示文本？
- en: Bag of Words
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋
- en: As we mentioned earlier, every language consists of a defined list of characters
    (alphabet), which are combined to form words (vocabulary). Traditionally, **Bag
    of Words** (**BoW**) has been one of the most popular methods for representing
    textual information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，每种语言都包括一个定义的字符列表（字母表），这些字符组合在一起形成单词（词汇表）。传统上，**词袋**（**BoW**）一直是表示文本信息的最流行方法之一。
- en: 'BoW is a simple and flexible approach to transforming text into vector form.
    This transformation helps not only in extracting features from raw text, but also
    in making it fit for consumption by different algorithms and architectures. As
    the name suggests, the BoW model of representation utilizes each word as a basic
    unit of measurement. A BoW model describes the occurrence of words within a given
    corpus of text. To build a BoW model for representation, we require two major
    things:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋是将文本转换为向量形式的一种简单灵活的方法。这种转换不仅有助于从原始文本中提取特征，还使其适用于不同的算法和架构。正如其名称所示，词袋表示模型将每个单词作为一种基本的度量单位。词袋模型描述了在给定文本语料库中单词的出现情况。为了构建一个用于表示的词袋模型，我们需要两个主要的东西：
- en: '**A vocabulary**: A collection of known words from the corpus of text to be
    analyzed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇表**：从要分析的文本语料库中已知单词的集合。'
- en: '**A measure of occurrence**: Something that we choose based upon the application/task
    at hand. For instance, counting the occurrence of each word, known as term frequency,
    is one such measure.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**出现的度量**：根据手头的应用/任务选择的东西。例如，计算每个单词的出现次数，称为词频，就是一种度量。'
- en: A detailed discussion related to the BoW model is beyond the scope of this chapter.
    We are presenting a high-level overview as a primer before more complex topics
    are introduced later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与词袋模型相关的详细讨论超出了本章的范围。我们正在呈现一个高级概述，作为在本章后面引入更复杂主题之前的入门。
- en: The BoW model is called a "bag" to highlight the simplicity and the fact that
    we overlook any ordering of the occurrences. In other words, the BoW model discards
    any order or structure-related information of the words in a given text. This
    might sound like a big issue but until recently, the BoW model remained quite
    a popular and effective choice for representing textual data. Let's have a quick
    look at a few examples to understand how this simple method works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型被称为“袋子”，以突显其简单性和我们忽略出现次数的任何排序的事实。换句话说，词袋模型舍弃了给定文本中单词的任何顺序或结构相关信息。这听起来可能是一个很大的问题，但直到最近，词袋模型仍然是表示文本数据的一种流行和有效的选择。让我们快速看几个例子，了解这种简单方法是如何工作的。
- en: '"Some say the world will end in fire,Some say in ice.From what I have tasted
    of desire I hold with those who favour fire."'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"有人说世界将在火中终结，有人说在冰中。从我尝到的欲望中，我同意那些赞成火的人。"'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Define a vocabulary**:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义词汇表**：'
- en: The first and foremost step is to define a list of known words from our corpus.
    For ease of understanding and practical reasons, we can ignore the case and punctuation
    marks for now. The vocabulary, or unique words, thus are {some, say, the, world,
    will, end, in, fire, ice, from, what, i, have, tasted, of, desire, hold, with,
    those, who, favour}.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先且最重要的步骤是从我们的语料库中定义一个已知单词列表。为了便于理解和实际原因，我们现在可以忽略大小写和标点符号。因此，词汇或唯一单词为 {some,
    say, the, world, will, end, in, fire, ice, from, what, i, have, tasted, of, desire,
    hold, with, those, who, favour}。
- en: This vocabulary is a set of 21 unique words in a corpus of 26 words.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个词汇表是一个包含 26 个词中的 21 个唯一单词的语料库。
- en: '**Define a metric of occurrence**:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义出现的度量**：'
- en: 'Once we have the vocabulary set, we need to define how we will measure the
    occurrence of each word from the vocabulary. As we mentioned earlier, there are
    a number of ways to do so. One such metric is simply checking if a specific word
    is present or absent. We use a 0 if the word is absent or a 1 if it is present.
    The sentence "some say ice" can thus be scored as:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们有了词汇集，我们需要定义如何衡量词汇中每个单词的出现次数。正如我们之前提到的，有很多种方法可以做到这一点。这样的一个指标就是简单地检查特定单词是否存在。如果单词不存在，则使用0，如果存在则使用1。因此，句子“some
    say ice”可以得分为：
- en: 'some: 1'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'some: 1'
- en: 'say: 1'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'say: 1'
- en: 'the: 0'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'the: 0'
- en: 'world: 0'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'world: 0'
- en: 'will: 0'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'will: 0'
- en: 'end: 0'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'end: 0'
- en: 'in: 0'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'in: 0'
- en: 'fire: 0'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'fire: 0'
- en: 'ice: 1'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ice: 1'
- en: Hence, the overall vector will look like [1, 1, 0, 0, 0, 0, 0, 0, 1].
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，总向量看起来像[1, 1, 0, 0, 0, 0, 0, 0, 1]。
- en: 'There are a few other metrics that have been developed over the years. The
    most widely used metrics are:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多年来已经发展了一些其他指标。最常用的指标是：
- en: Term frequency
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频
- en: TF-IDF, as seen in *Chapter 7*, *Style Transfer with GANs*
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF，如*第7章*，*使用GAN进行风格转移*
- en: Hashing
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希化
- en: 'These steps provide a high-level glimpse into how the BoW model helps us represent
    textual data as numbers or vectors. The overall vector representation of our excerpt
    from the poem is depicted in the following table:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤提供了词袋模型如何帮助我们将文本数据表示为数字或向量的高层次概述。诗歌摘录的总体向量表示如下表所示：
- en: '![Table  Description automatically generated](img/B16176_09_01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![表的描述是自动生成的](img/B16176_09_01.png)'
- en: 'Figure 9.1: BoW representation'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：词袋表示
- en: Each row in the matrix corresponds to one line from the poem, while the unique
    words from the vocabulary form the columns. Each row thus is simply the vector
    representation of the text under consideration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每一行对应诗歌中的一行，而词汇表中的唯一单词构成了列。因此，每一行就是所考虑文本的向量表示。
- en: 'There are a few additional steps involved in improving the outcome of this
    method. The refinements are related to vocabulary and scoring aspects. Managing
    the vocabulary is very important; often, a corpus of text can increase in size
    quite rapidly. A few common methods of handling vocabularies are:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 改进此方法的结果涉及一些额外的步骤。这些优化与词汇和评分方面有关。管理词汇非常重要；通常，文本语料库的规模会迅速增大。处理词汇的一些常见方法包括：
- en: Ignoring punctuation marks
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略标点符号
- en: Ignoring case
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略大小写
- en: Removing frequently occurring words (or stopwords) like a, an, the, this, and
    so on
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除常见单词（或停用词）如a, an, the, this等
- en: Methods to use the root form of words, such as *stop* in place of *stopping*.
    Stemming and lemmatization are two such methods
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单词的词根形式的方法，如*stop*代替*stopping*。词干提取和词形还原是两种这样的方法
- en: Handling spelling mistakes
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理拼写错误
- en: 'We already discussed different scoring methods and how they help in capturing
    certain important features. BoW is simple, yet is an effective tool that serves
    as a good starting point for most NLP tasks. Yet there are a few issues which
    can be summarized as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了不同的评分方法以及它们如何帮助捕捉某些重要特征。词袋模型简单而有效，是大多数自然语言处理任务的良好起点。然而，它存在一些问题，可以总结如下：
- en: '**Missing context**:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失的上下文**：'
- en: As we mentioned earlier, the BoW model does not consider the ordering or structure
    of the text. By simply discarding information related to ordering, the vectors
    lose out on capturing the context in which the underlying text was used. For instance,
    the sentences "I am sure about it" and "Am I sure about it?" would have identical
    vector representations, yet they express different thoughts. Expanding BoW models
    to include n-grams (contiguous terms) instead of singular terms does help in capturing
    some context, but in a very limited way.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，词袋模型不考虑文本的排序或结构。通过简单地丢弃与排序相关的信息，向量失去了捕捉基础文本使用上下文的机会。例如，“我肯定”和“我怀疑我肯定”这两句话将具有相同的向量表示，然而它们表达了不同的思想。扩展词袋模型以包括n-gram（连续词组）而不是单个词确实有助于捕捉一些上下文，但在非常有限的范围内。
- en: '**Vocabulary and sparse vectors**:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇和稀疏向量**：'
- en: As the corpus size increases, so does the vocabulary. The steps required to
    manage vocabulary size require a lot of oversight and manual effort. Due to the
    way this model works, a large vocabulary leads to very sparse vectors. Sparse
    vectors pose issues with modeling and computation requirements (space and time).
    Aggressive pruning and vocabulary management steps do help to a certain extent
    but can lead to the loss of important features as well.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着语料库的规模增加，词汇量也在增加。管理词汇量大小所需的步骤需要大量的监督和手动工作。由于这种模型的工作方式，大量的词汇导致非常稀疏的向量。稀疏向量对建模和计算需求（空间和时间）造成问题。激进的修剪和词汇管理步骤在一定程度上确实有所帮助，但也可能导致重要特征的丢失。
- en: Here, we discussed how the BoW model helps in transforming text into vector
    form, along with a few issues with this setup. In the next section, we will move
    on to a few more involved representation methods that alleviate some of these
    issues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了词袋模型如何帮助将文本转换为向量形式，以及这种设置中的一些问题。在下一节，我们将转向一些更多涉及的表示方法，这些方法缓解了一些这些问题。
- en: Distributed representation
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式表示
- en: The Bag of Words model is an easy-to-understand way of transforming words into
    vector form. This process is generally termed *vectorization*. While it is a useful
    method, the BoW model has its limitations when it comes to capturing context,
    along with sparsity-related issues. Since deep learning architectures are becoming
    de facto state-of-the-art systems in most spaces, it is obvious that we should
    be leveraging them for NLP tasks as well. Apart from the issues mentioned earlier,
    the sparse and large (wide) vectors from the BoW model are another aspect which
    can be tackled using neural networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是将单词转换为向量形式的易于理解的方法。这个过程通常被称为*向量化*。虽然这是一种有用的方法，但在捕获上下文和与稀疏相关的问题方面，词袋模型也有它的局限性。由于深度学习架构正在成为大多数空间的事实上的最先进系统，显而易见的是我们应该在NLP任务中也利用它们。除了前面提到的问题，词袋模型的稀疏和大（宽）向量是另一个可以使用神经网络解决的方面。
- en: A simple alternative that handles the sparsity issue can be implemented by encoding
    each word as a unique number. Continuing with the example from the previous section,
    "some say ice", we could assign 1 to "some", 2 to "say", 3 to "ice", and so on.
    This would result in a dense vector, [1, 2, 3]. This is an efficient utilization
    of space and we end up with vectors where all the elements are full. However,
    the limitation of missing context still remains. Since the numbers are arbitrary,
    they hardly capture any context on their own. On the contrary, arbitrarily mapping
    numbers to words is not very interpretable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理稀疏问题的简单替代方案是将每个单词编码为唯一的数字。继续上一节的示例，“有人说冰”，我们可以将“有人”赋值为1，“说”赋值为2，“冰”赋值为3，以此类推。这将导致一个密集的向量，[1,
    2, 3]。这是对空间的有效利用，并且我们得到了所有元素都是完整的向量。然而，缺失上下文的限制仍然存在。由于数字是任意的，它们几乎不能单独捕获任何上下文。相反，将数字任意映射到单词并不是非常可解释的。
- en: '**Interpretability** is an important requirement when it comes to NLP tasks.
    For computer vision use cases, visual cues are good enough indicators for understanding
    how a model is perceiving or generating outputs (though quantification is also
    a problem there, but we can skip it for now). For NLP tasks, since the textual
    data is first required to be transformed into a vector, it is important to understand
    what those vectors capture and how they are used by the models.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释性**是NLP任务的重要要求。对于计算机视觉用例，视觉线索足以成为理解模型如何感知或生成输出的良好指标（尽管在那方面的量化也是一个问题，但我们现在可以跳过它）。对于NLP任务，由于文本数据首先需要转换为向量，因此重要的是理解这些向量捕获了什么，以及模型如何使用它们。'
- en: In the coming sections, we will cover some of the popular vectorization techniques
    that try to capture context while limiting the sparsity of the vectors as well.
    Please note that there are a number of other methods (such as SVD-based methods
    and co-occurrence matrices) as well that help in vectorizing textual data. In
    this section, we will be covering only those which are helpful in understanding
    later sections of this chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些流行的向量化技术，尝试捕捉上下文，同时限制向量的稀疏性。请注意，还有许多其他方法（例如基于SVD的方法和共现矩阵）也有助于向量化文本数据。在本节中，我们将只涉及那些有助于理解本章后续内容的方法。
- en: Word2vec
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2vec
- en: The English Oxford dictionary has about 600k unique words and is growing year
    on year. Yet those words are not independent terms; they have some relationship
    to each other. The premise of the word2vec model is to learn high-quality vector
    representations that capture context. This is better summarized by the famous
    quote by J.R. Firth *"you shall know a word by the company it keeps"*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 英国牛津词典大约有 60 万个独特的单词，并且每年都在增长。然而，这些单词并非独立的术语；它们彼此之间存在一些关系。word2vec 模型的假设是学习高质量的向量表示，以捕获上下文。这更好地总结了
    J.R. 菲斯的著名引文：“*你可以通过它搭配的伙伴来认识一个词*”。
- en: In their work titled *Efficient Estimation of Word Representations in Vector
    Space*, Mikolov et al.¹ present two different models that learn vector representations
    of words from a large corpus. Word2Vec is a software implementation of these models
    which is classified as an iterative approach to learning such embeddings. Instead
    of taking the whole corpus into account in one go, this approach tries to iteratively
    learn to encode each word's representation, along with its context. This idea
    of learning word representations as dense context vectors is not a new one. It
    was first proposed by Rumelhart et al. in 1990². They presented how a neural network
    is able to learn representations, with similar words ending up in the same clusters.
    The ability to have vector forms of words that capture some notion of similarity
    is quite a powerful one. Let's see in detail how the word2vec models achieve this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们名为“Vector Space 中单词表示的高效估计”的工作中，Mikolov 等人¹介绍了两种学习大型语料库中单词向量表示的模型。Word2Vec
    是这些模型的软件实现，属于学习这些嵌入的迭代方法。与一次性考虑整个语料库不同，这种方法尝试迭代地学习编码每个单词的表示及其上下文。学习词表示作为密集上下文向量的这一概念并不新鲜。这最初是由
    Rumelhart 等人于 1990²年提出的。他们展示了神经网络如何学习表示，使类似的单词最终处于相同的聚类中。拥有捕获某种相似性概念的单词向量形式的能力是非常强大的。让我们详细看看
    word2vec 模型是如何实现这一点的。
- en: Continuous Bag of Words (CBOW) Model
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续词袋 (CBOW) 模型
- en: The Continuous Bag of Words model is an extension of the Bag of Words model
    we discussed in the previous section. The key aspect of this model is the context
    window. A context window is defined as a sliding window of a fixed size moving
    along a sentence. The word in the middle is termed the *target*, and the terms
    to its left and right within the window are the *context terms*. The CBOW model
    works by predicting the target term, given its context terms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 连续词袋模型是我们在上一节讨论的词袋模型的扩展。该模型的关键方面是上下文窗口。上下文窗口被定义为沿着句子移动的固定大小的滑动窗口。中间的词称为*目标*，窗口内的左右术语称为*上下文术语*。CBOW
    模型通过给定其上下文术语来预测目标术语。
- en: For instance, let's consider a reference sentence, "some say the *world* will
    end in fire". If we have a window size of 4 and a target term of *world*, the
    context terms would be {say, the} and {will, end}. The model inputs are tuples
    of the form (context terms, target term), which are then passed through a neural
    network to learn the embeddings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一句参考句子，“some say the *world* will end in fire”。如果我们的窗口大小为 4，目标术语为*world*，那么上下文术语将会是{say,
    the}和{will, end}。模型的输入是形式为（上下文术语，目标术语）的元组，然后将其通过神经网络学习嵌入向量。
- en: 'This process is depicted in the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程如下图所示：
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_02.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图表，图解 自动生成描述](img/B16176_09_02.png)'
- en: 'Figure 9.2: Continuous Bag of Words model setup'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：连续词袋模型设置
- en: 'As shown in the preceding diagram, the context terms, denoted as ![](img/B16176_09_001.png),
    are passed as input to the model to predict the target term, denoted as *w*[t].
    The overall working of the CBOW model can be explained as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图表所示，上下文术语，表示为![](img/B16176_09_001.png)，被作为输入传递给模型，以预测目标术语，表示为*w*[t]。CBOW
    模型的整体工作可以解释如下：
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小为*V*的词汇表，定义了大小为*C*的上下文窗口。*C*可以是 4、6 或任何其他大小。我们还定义了两个矩阵*W*和*W'*来生成输入和输出向量。矩阵*W*是*VxN*，而*W'*是*NxV*的维度。*N*是嵌入向量的大小。
- en: 'The context terms (![](img/B16176_09_001.png)) and the target term (*y*) are
    transformed into one-hot encodings (or label-encodings) and training data is prepared
    in the form of tuples: (![](img/B16176_09_001.png), *y*).'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文术语（![](img/B16176_09_001.png)）和目标术语（*y*）被转化为独热编码（或标签编码），并且训练数据以元组的形式准备：（![](img/B16176_09_001.png)，*y*）。
- en: We average the context vectors to get ![](img/B16176_09_004.png).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对上下文向量进行平均以得到 ![](img/B16176_09_004.png)。
- en: The final output scoring vector *z* is calculated as a dot product between the
    average vector *v'* and the output matrix *W'*.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的输出评分向量 *z* 是平均向量 *v'* 和输出矩阵 *W'* 的点积。
- en: The output scoring vector is transformed into a probability using a softmax
    function; that is, *y' = softmax(z)*, where *y'* should correspond to one of the
    terms in the vocabulary.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出评分向量经过 softmax 函数转换为概率值；也就是说，*y' = softmax(z)*，其中 *y'* 应该对应词汇表中的一个术语。
- en: The final aim would be to train the neural network such that *y'* and the actual
    target *y* become as close as possible.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的目标是训练神经网络，使得 *y'* 和实际目标 *y* 尽可能接近。
- en: The authors proposed using a cost function such as cross-entropy to train the
    network and learn such embeddings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议使用诸如交叉熵之类的成本函数来训练网络并学习这样的嵌入。
- en: Skip-gram model
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: skip-gram 模型
- en: The skip-gram model is the second variant presented in the paper for learning
    word embeddings. In essence, this model works in exactly the opposite way to the
    CBOW model. In other words, in the case of skip-gram, we input a word (center/target
    word) and predict the context terms as the model output. Let's use the same example
    as before, "some say the *world* will end in fire". Here, we will start with *world*
    as our input term and train a model to predict {say, the, will, end} as context
    terms with high probability.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型是该论文中用于学习词嵌入的第二个变体。本质上，该模型的工作方式与 CBOW 模型完全相反。换句话说，在 skip-gram 的情况下，我们输入一个词（中心/目标词），预测上下文术语作为模型的输出。让我们用之前的例子进行说明，“some
    say the *world* will end in fire”。在这里，我们将用 *world* 作为输入术语，并训练一个模型以高概率预测 {say,
    the, will, end}，作为上下文术语。
- en: 'The following diagram depicts the skip-gram model; as expected, this is a mirror
    image of the CBOW setup we discussed in *Figure 9.2*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 skip-gram 模型；如预期的那样，这是我们在 *图 9.2* 中讨论的 CBOW 设置的镜像：
- en: '![Chart, diagram  Description automatically generated](img/B16176_09_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图表，图解  自动生成描述](img/B16176_09_03.png)'
- en: 'Figure 9.3: Skip-gram model setup'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：skip-gram 模型设置
- en: 'The step by step working skip-gram model can be explained as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型的逐步工作可以解释如下：
- en: For a vocabulary of size *V*, a context window of size *C* is defined. *C* could
    be 4, 6, or any other size. We also define two matrices *W* and *W'* to generate
    input and output vectors, respectively. The matrix *W* is *VxN*, while *W'* is
    *NxV* in dimensions. *N* is the size of the embedding vector.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个大小为 *V* 的词汇表，定义一个大小为 *C* 的上下文窗口。*C* 可以是 4、6 或其他任意大小。我们还定义了两个矩阵 *W* 和 *W'*，分别用于生成输入向量和输出向量。矩阵
    *W* 是 *VxN* 的，而 *W'* 的维度是 *NxV*。*N* 是嵌入向量的大小。
- en: Generate the one-hot encoded representation of the center word *x*.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成中心词 *x* 的独热编码表示。
- en: We get the word embedding representation of *x* by taking a dot product of *x*
    and *W*. The embedded representation is given as *v = W.x*.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 *x* 和 *W* 的点积来获取 *x* 的词嵌入表示。嵌入表示为 *v = W.x*。
- en: We generate the output score vector *z* by taking a dot product of *W'* and
    *v*; that is, *z = W'.v*.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将 *W'* 和 *v* 的点积得到输出评分向量 *z*；也就是说，*z = W'.v*。
- en: The scoring vector is transformed into output probabilities using a softmax
    layer to generate *y'*.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评分向量通过 softmax 层转换为输出概率，生成 *y'*。
- en: The final aim would be to train the neural network such that *y'* and the actual
    context *y* become as close as possible.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的目标是训练神经网络，使得 *y'* 和实际上的上下文 *y* 尽可能接近。
- en: In the case of skip-gram, we have multiple input-output training pairs for any
    given center word. This model treats all context terms equally, irrespective of
    their distance from the center word in the context window. This allows us to use
    cross-entropy as the cost function with a strong conditional independence assumption.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 skip-gram 的情况下，对于任何给定的中心词，我们有多个输入-输出训练对。该模型将所有上下文术语都视为同等重要，无论它们与上下文窗口中的中心词之间的距离如何。这使我们能够使用交叉熵作为成本函数，并假设具有强条件独立性。
- en: In order to improve the outcomes and speed up the training process, the authors
    introduced some simple yet effective tricks. Concepts such as *negative sampling,
    noise contrastive estimation* and *hierarchical softmax* are a few such techniques
    which have been leveraged. For a detailed understanding of CBOW and skip-gram,
    readers are requested to go through the cited paper by Mikolov et al.,¹ where
    the authors have given detailed explanations of each of the steps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善结果并加快训练过程，作者们引入了一些简单但有效的技巧。*负采样、噪声对比估计*和*分层softmax*等概念是一些被利用的技术。要详细了解CBOW和skip-gram，请读者阅读Mikolov等人引用的文献¹，作者在其中详细解释了每个步骤。
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Just a few lines of code and we have our word2vec representations of our vocabulary
    ready. Upon checking, we find that there are 19,000 unique words in our vocabulary,
    and that we have a vector representation for each. The following snippet shows
    how we can get the vector representation of any word. We will also demonstrate
    how to get words that are most similar to a given word:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几行代码，我们就可以获得我们词汇表的word2vec表示。检查后，我们发现我们的词汇表中有19,000个独特单词，并且我们为每个单词都有一个向量表示。以下代码片段显示了如何获得任何单词的向量表示。我们还将演示如何获取与给定单词最相似的单词：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding outputs show a 32-dimensional vector for the word *sun*. We also
    display words that are most similar to the word *god*. We can clearly see that
    words such as believe, existence, and so on seem to be the most similar, which
    makes sense given the dataset we used. For interested readers, we have a 3-dimensional
    vector space representation using TensorBoard showcased in the corresponding notebook.
    The TensorBoard representation helps us visually understand the embedding space
    and see how these vectors interact.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前述输出展示了*sun*这个单词的32维向量。我们还展示了与单词*god*最相似的单词。我们可以清楚地看到，诸如believe、existence等单词似乎是最相似的，这是合乎逻辑的，考虑到我们使用的数据集。对于感兴趣的读者，我们在对应的笔记本中展示了使用TensorBoard的3维向量空间表示。TensorBoard表示帮助我们直观地理解嵌入空间，以及这些向量是如何相互作用的。
- en: GloVe
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GloVe
- en: The word2vec models helped in improving performance for various NLP tasks. Continuing
    with the same momentum, another important implementation called GloVe came into
    the picture. GloVe or *Global Vectors for Word Representation* was published by
    Pennington et al. in 2014 to improve upon the known word representation techniques.³
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型有助于改进各种自然语言处理任务的性能。在相同的动力下，另一个重要的实现叫做GloVe也出现了。GloVe或*全局词向量表示*于2014年由Pennington等人发表，旨在改进已知的单词表示技术。³
- en: As we've seen, word2vec models work by considering the local context (a defined
    window) of the words in the vocabulary. Even though this works remarkably well,
    it is a bit rough around the edges. The fact that words may mean different things
    in different contexts requires us to understand not just the local but the global
    context as well. GloVe tries to work upon the global context while learning the
    word vectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，word2vec模型通过考虑词汇表中单词的局部上下文（定义的窗口）来工作。即使这是非常有效的，但还不够完善。单词在不同上下文中可能意味着不同的东西，这要求我们不仅要理解局部上下文，还要理解全局上下文。GloVe试图在学习单词向量的同时考虑全局上下文。
- en: There are classical techniques for this, such as **Latent Semantic Analysis**
    (**LSA**), which are based on matrix factorization and do a good job at capturing
    global context, but are not so good at things such as vector math.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些经典的技术，例如**潜在语义分析**（**LSA**），这些技术基于矩阵分解，在捕获全局上下文方面做得很好，但在向量数学等方面做得不太好。
- en: 'GloVe is a method which tries to get the best of both worlds in order to learn
    better word representations. The GloVe algorithm consists of the following steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是一种旨在学习更好的词表示的方法。GloVe算法包括以下步骤：
- en: Prepare a word co-occurrence matrix *X*, such that each element, *x*[i][j],
    represents how often the word *j* appears in the context of the word *i*. GloVe
    makes use of two fixed size windows that help in capturing context before the
    word and context after the word.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个词共现矩阵*X*，使得每个元素*x*[i][j]表示单词*i*在单词*j*上下文中出现的频率。GloVe使用了两个固定尺寸的窗口，这有助于捕捉单词之前和之后的上下文。
- en: The co-occurrence matrix *X* is updated with a decay factor to penalize terms
    that are farther apart in the context. The decay factor is defined as ![](img/B16176_09_005.png),
    where offset is the distance from the word under consideration.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共现矩阵*X*使用衰减因子进行更新，以惩罚上下文中距离较远的术语。衰减因子定义如下：![](img/B16176_09_005.png)，其中offset是考虑的单词的距离。
- en: Then, we prepare the GloVe equation as the following soft constraint:![](img/B16176_09_006.png)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将准备GloVe方程如下软约束条件：![](img/B16176_09_006.png)
- en: Here, *w*[i] is the vector for the main word, *w*[j] is the vector for the context
    word, and *b*[i], *b*[j] are the corresponding bias terms.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，*w*[i]是主要单词的向量，*w*[j]是上下文单词的向量，*b*[i]，*b*[j]是相应的偏差项。
- en: The final step is to use the preceding constraint to define the cost function,
    which is given as:![](img/B16176_09_007.png)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是使用前述约束条件来定义成本函数，其定义如下：![](img/B16176_09_007.png)
- en: 'Here, *f* is a weighting function, defined as:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，*f*是一个加权函数，定义如下：
- en: '![](img/B16176_09_008.png)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16176_09_008.png)'
- en: The authors of the paper achieved the best results with ![](img/B16176_09_009.png).
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该论文的作者使用![](img/B16176_09_009.png)获得了最佳结果。
- en: Similar to word2vec models, GloVe embeddings also achieve good results and the
    authors present results where they show GloVe outperforming word2vec. They attribute
    this to better problem formulation and the global context being captured.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于word2vec模型，GloVe嵌入也取得了良好的结果，作者展示了GloVe胜过word2vec的结果。他们将此归因于更好的问题表述和全局上下文的捕捉。
- en: In practice, both models perform more or less similarly. As larger vocabularies
    are required to get better embeddings (for both word2vec and GloVe), for most
    practical use cases, pretrained embeddings are available and used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这两种模型的性能差不多。由于需要更大的词汇表来获得更好的嵌入（对于word2vec和GloVe），对于大多数实际应用情况，预训练的嵌入是可用且被使用的。
- en: Pretrained GloVe vectors are available through a number of packages, such as
    `spacy`. Interested readers may wish to explore the `spacy` package for more details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的GloVe向量可以通过多个软件包获得，例如`spacy`。感兴趣的读者可以探索`spacy`软件包以获得更多详情。
- en: FastText
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastText
- en: Word2Vec and GloVe are powerful methods which have nice properties when it comes
    to encoding words in the vector space. Both techniques work nicely when it comes
    to getting vector representation of words that are in the vocabulary, but they
    do not have clear answers for terms that are outside of the vocabulary.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec和GloVe是强大的方法，在将单词编码为向量空间时具有很好的特性。当涉及到获取在词汇表中的单词的向量表示时，这两种技术都能很好地工作，但对于词汇表之外的术语，它们没有明确的答案。
- en: The word is the fundamental unit in the case of the word2vec and GloVe methods.
    This assumption is challenged and improved upon in the FastText implementation.
    The word representation aspect of FastText is based on the paper, *Enriching Word
    Vectors with Subword Information* by Bojanowski et al. in 2017.⁴ This work decomposes
    each word into a set of n-grams. This helps in capturing and learning vector representations
    of different combinations of characters, as opposed to the whole word in earlier
    techniques.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在word2vec和GloVe方法中，单词是基本单位。这一假设在FastText实现中受到挑战和改进。FastText的单词表示方面基于2017年Bojanowski等人的论文*使用子词信息丰富化词向量*。⁴
    该工作将每个单词分解为一组n-grams。这有助于捕捉和学习字符组合的不同向量表示，与早期技术中的整个单词不同。
- en: For instance, if we consider the word "India" and *n=3* for the n-gram setup,
    it will decompose the word into {<india>, <in, ind, ndi, dia, ia>}. The symbols
    "<" and ">" are special characters to denote the start and end of the original
    word. This helps in differentiating between <in>, which represents the whole word,
    and <in, which is an n-gram. This approach helps FastText generate embeddings
    for *out of vocabulary* terms as well. This can be done by adding and averaging
    the vector representation of required n-grams.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑单词“India”和*n=3*用于n-gram设置，则它将将单词分解为{<india>, <in, ind, ndi, dia, ia>}。符号“<”和“>”是特殊字符，用于表示原始单词的开始和结束。这有助于区分<in>，它代表整个单词，和<in，它代表一个n-gram。这种方法有助于FastText生成*超出词汇表*的术语的嵌入。这可以通过添加和平均所需n-gram的向量表示来实现。
- en: FastText is shown to drastically improve performance when it comes to use cases
    where there is a high chance of new/out of vocabulary terms. FastText was developed
    by researchers at **Facebook AI Research** (**FAIR**), which shouldn't come as
    a surprise as the kind of content generated on social media platforms such as
    Facebook is huge and ever-changing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: FastText在处理可能有大量新/词汇表之外术语的用例时，被显示明显提高性能。FastText是由**Facebook AI Research**（**FAIR**）的研究人员开发的，这应该不足为奇，因为在Facebook等社交媒体平台上生成的内容是巨大且不断变化的。
- en: With its added improvements come a few disadvantages as well. Since the basic
    unit in this case is an n-gram, the amount of time required to train/learn such
    representations is higher than previous techniques. The n-gram approach also increases
    the amount of memory required to train such a model. However, the authors of the
    paper point out that a hashing trick helps in controlling the memory requirements
    to a certain extent.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随着它的改进，也有一些缺点。由于这种情况下的基本单位是一个n-gram，因此训练/学习这种表示所需的时间比以前的技术更长。 n-gram方法还增加了训练这种模型所需的内存量。然而，论文的作者指出，散列技巧在一定程度上有助于控制内存需求。
- en: 'For ease of understanding, let''s again make use of our well-known Python library,
    `gensim`. We will extend upon the same dataset and pre-processing steps that we
    performed for the word2vec model exercise in the previous section. The following
    snippet prepares the FastText model object:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解，让我们再次利用我们熟悉的Python库`gensim`。我们将扩展上一节中word2vec模型练习所使用的相同数据集和预处理步骤。以下片段准备了FastText模型对象：
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The word2vec model fails to return a vector representation of the word "sunny"
    as it is not in the trained vocabulary. The following snippet shows how FastText
    is still able to generate a vector representation:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型无法返回单词"sunny"的矢量表示，因为它不在训练词汇表中。以下片段显示了FastText仍能生成矢量表示的方法：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This showcases how FastText improves upon word2vec- and GloVe-based representation
    techniques. We can easily handle out of vocabulary terms, all the while ensuring
    context-based dense representations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了FastText如何改进基于word2vec和GloVe的表示技术。我们可以轻松处理词汇表之外的术语，同时确保基于上下文的密集表示。
- en: Let's now use this understanding to develop a text generation model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们利用这个理解来开发文本生成模型。
- en: Text generation and the magic of LSTMs
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成和LSTM的魔法
- en: In the previous sections, we discussed different ways of representing textual
    data in order to make it fit for consumption by different NLP algorithms. In this
    section, we will leverage this understanding of text representation to work our
    way toward building text generation models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们讨论了不同的表示文本数据的方法，以使其适合不同的自然语言处理算法使用。在本节中，我们将利用这种对文本表示的理解来构建文本生成模型。
- en: So far, we have built models using feedforward networks consisting of different
    kinds and combinations of layers. These networks work with one training example
    at a time, which is independent of other training samples. We say that the samples
    are **independent and identically distributed**, or **IID**. Language, or text,
    is a bit different.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用由不同种类和组合的层组成的前馈网络构建了模型。这些网络一次处理一个训练示例，这与其他训练样本是独立的。我们说这些样本是**独立同分布**的，或**IID**。语言，或文本，有点不同。
- en: As we discussed in the previous sections, words change their meaning based on
    the context they are being used in. In other words, if we were to develop and
    train a language generation model, we would have to ensure the model understands
    the context of its input.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几节中讨论的，单词根据它们被使用的上下文而改变它们的含义。换句话说，如果我们要开发和训练一种语言生成模型，我们必须确保模型理解其输入的上下文。
- en: '**Recurrent Neural Networks** (**RNNs**) are a class of neural networks that
    allow previous outputs to be used as inputs, along with memory or hidden units.
    This awareness of previous inputs helps in capturing context, and provides us
    with the ability to handle variable-length input sequences (sentences are hardly
    ever of the same length). A typical RNN is depicted in the following diagram,
    in both actual and unrolled form:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNNs**）是一类允许先前输出用作输入的神经网络，同时具有记忆或隐藏单元。对先前输入的意识有助于捕捉上下文，并使我们能够处理可变长度的输入序列（句子很少长度相同）。下图显示了典型的RNN，既实际形式又展开形式：'
- en: '![Diagram  Description automatically generated](img/B16176_09_04.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B16176_09_04.png)'
- en: 'Figure 9.4: A typical RNN'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：一个典型的RNN
- en: As shown in *Figure 9.4*, at time *t*[1], input *x*[1] generates output *y*[1].
    At time *t*[2], *x*[2] along with *y*[1] (the previous output) generate output
    *y*[2], and so on. Unlike typical feedforward networks where every input is independent
    of the others, RNN introduces the notion of previous outputs impacting the current
    and upcoming ones.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.4*所示，在时间 *t*[1]，输入 *x*[1] 生成输出 *y*[1]。在时间 *t*[2]，*x*[2] 和 *y*[1]（前一个输出）一起生成输出
    *y*[2]，以此类推。与 typcial feedforward网络不同，其中的每个输入都独立于其他输入，RNN引入了前面的输出对当前和将来的输出的影响。
- en: RNNs have a few different variants to them, namely, **Gated Recurrent Units**
    (**GRUs**) and **Long Short-Term Memory** (**LSTMs**). The vanilla RNN described
    previously works in auto-regressive settings well. Yet it has issues with longer
    context windows (vanishing gradients). GRUs and LSTMs try to overcome such issues
    by using different gates and memory units. Introduced by Hochreiter and Schmidhuber
    in 1997, LSTMs can remember information from really long sequence-based data.
    LSTMs consist of three gates called input, output, and forget gates. This is depicted
    in the following diagram.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: RNN还有一些不同的变体，即**门控循环单元**（**GRUs**）和**长短期记忆**（**LSTMs**）。之前描述的原始RNN在自回归环境中工作良好。但是，它在处理更长上下文窗口（梯度消失）时存在问题。GRUs和LSTMs通过使用不同的门和记忆单元来尝试克服此类问题。LSTMs由Hochreiter和Schmidhuber于1997年引入，可以记住非常长的序列数据中的信息。LSTMs由称为输入、输出和遗忘门的三个门组成。以下图表显示了这一点。
- en: '![](img/B16176_09_05.png)Figure 9.5: Different gates of an LSTM cell'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B16176_09_05.png)图9.5：LSTM单元的不同门'
- en: For a detailed understanding of LSTMs, you may refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有关LSTMs的详细了解，请参阅[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: We will now focus on defining the task of text generation more formally.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重点介绍更正式地定义文本生成任务。
- en: Language modeling
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模
- en: NLP-based solutions are quite effective and can be seen all around us. The most
    prominent example is the autocomplete feature on most smartphone keyboards, search
    engines (Google, Bing, and so on), and even word processors (like MS Word).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NLP的解决方案非常有效，我们可以在我们周围看到它们的应用。最突出的例子是手机键盘上的自动完成功能，搜索引擎（Google，Bing等）甚至文字处理软件（如MS
    Word）。
- en: Autocomplete is a common name for a formal concept called *language modeling*.
    In simple words, a language model takes certain text as the input context to generate
    the next set of words as the output. This is interesting because a language model
    tries to understand the input context, the language structure, and rules to predict
    the next word(s). We use it in the form of text completion utilities on search
    engines, chat platforms, emails, and more all the time. Language models are a
    perfect real-life application of NLP and showcase the power of RNNs. In this section,
    we will work toward building an understanding, as well as training an RNN-based
    language model for text generation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 自动完成是一个正式概念称为*语言建模*的常见名称。简单来说，语言模型以某些文本作为输入上下文，以生成下一组单词作为输出。这很有趣，因为语言模型试图理解输入的上下文，语言结构和规则，以预测下一个单词。我们经常在搜索引擎，聊天平台，电子邮件等上使用它作为文本完成工具。语言模型是NLP的一个完美实际应用，并展示了RNN的威力。在本节中，我们将致力于建立对RNN基于语言模型的文本生成的理解以及训练。
- en: 'Let''s get started by understanding the process of generating a training dataset.
    We can do this with the help of the following image. This image depicts a word-level
    language model; that is, a model for which a word is the basic unit. Along the
    same lines, we can develop character-level, phrase-level, or even document-level
    models:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解生成训练数据集的过程开始。我们可以使用下面的图像来做到这一点。该图像描绘了一个基于单词的语言模型，即以单词为基本单位的模型。在同样的思路下，我们可以开发基于字符，基于短语甚至基于文档的模型：
- en: '![Timeline  Description automatically generated](img/B16176_09_06.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![Timeline  Description automatically generated](img/B16176_09_06.png)'
- en: 'Figure 9.6: Training data generation process for a language model'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：用于语言模型的训练数据生成过程
- en: As we mentioned earlier, a language model looks at the context to generate the
    next set of words. This context is also called a sliding window, which moves across
    the input sentence from left to right (right to left for languages that are written
    from right to left). The sliding window depicted in *Figure 9.6* spans three words,
    which act as input. The corresponding output for each training data point is the
    immediate next word after the window (or a set of words if the aim is to predict
    the next phrase). We thus prepare our training dataset, which consists of tuples
    of the form ({context terms}, next_word). The sliding window helps us to generate
    a good number of training samples from every sentence in the training dataset
    without explicit labeling.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，语言模型通过上下文来生成接下来的单词。这个上下文也被称为一个滑动窗口，它在输入的句子中从左到右（从右到左对于从右往左书写的语言）移动。
    *图9.6*中的滑动窗口跨越三个单词，作为输入。每个训练数据点的对应输出是窗口后面紧跟的下一个单词（或一组单词，如果目标是预测下一个短语）。因此，我们准备我们的训练数据集，其中包括({上下文词汇},
    下一个单词)这种形式的元组。滑动窗口帮助我们从训练数据集中的每个句子中生成大量的训练样本，而无需显式标记。
- en: This training dataset is then used to train an RNN-based language model. In
    practice, we typically use LSTMs or GRU units in place of vanilla RNN units. We
    discussed earlier that RNNs have the ability to auto-regress on previous timestep
    values. In the context of language models, we auto-regress on the context terms
    and the model generates the corresponding next word. We then make use of **backpropagation
    through time** (**BPTT**) to update model weights through gradient descent until
    the required performance is achieved. We discussed BPTT in detail in *Chapter
    3*, *Building Blocks of Deep Neural Networks*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这个训练数据集来训练基于RNN的语言模型。在实践中，我们通常使用LSTM或GRU单元来代替普通的RNN单元。我们之前讨论过RNN具有自动回归到先前时间步的数值的能力。在语言模型的上下文中，我们自动回归到上下文词汇，模型产生相应的下一个单词。然后我们利用**时间反向传播**（**BPTT**）通过梯度下降来更新模型权重，直到达到所需的性能。我们在
    *第3章*，*深度神经网络的构建模块*中详细讨论了BPTT。
- en: We now have a fair understanding of what a language model is and what steps
    are involved in preparing the training dataset, along with the model setup. Let's
    now implement some of these concepts using TensorFlow and Keras.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对语言模型以及准备训练数据集和模型设置所涉及的步骤有了一定的了解。现在让我们利用TensorFlow和Keras来实现其中一些概念。
- en: 'Hands-on: Character-level language model'
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践操作：字符级语言模型
- en: We discussed the basics of language modeling in the previous section. In this
    section, we will build and train our own language model, but with a twist. In
    contrast to the discussion in the previous section, here, we will work at the
    character level, not at the word level. In simple words, we will work toward building
    a model that takes a few characters as input (context) to generate the next set
    of characters. This choice of a more granular language model is for the ease of
    training such a model. A character-level language model needs to worry about a
    much smaller vocabulary, or number of unique characters, compared to a word-level
    language model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的部分中讨论了语言建模的基础知识。在这一部分中，我们将构建并训练自己的语言模型，但是有一点不同。与之前部分的讨论相反，在这里，我们将在字符级别而不是词级别工作。简单来说，我们将着手构建一个模型，将少量字符作为输入（上下文）来生成接下来的一组字符。选择更细粒度的语言模型是为了便于训练这样的模型。字符级语言模型需要考虑的词汇量（或独特字符的数量）要比词级语言模型少得多。
- en: 'To build our language model, the first step is to get a dataset to use as a
    training source. Project Gutenberg is a volunteer effort to digitize historical
    works and make them available as free downloads. Since we need lots of data to
    train a language model, we will pick one of the biggest available books, *War
    and Peace* by Leo Tolstoy. This book is available for download at the following
    URL:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的语言模型，第一步是获取一个数据集用作训练的来源。古腾堡计划是一项志愿者工作，旨在数字化历史著作并提供免费下载。由于我们需要大量数据来训练语言模型，我们将选择其中最大的一本书，列夫·托尔斯泰的
    *战争与和平*。该书可在以下网址下载：
- en: '[https://www.gutenberg.org/ebooks/2600](https://www.gutenberg.org/ebooks/2600)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.gutenberg.org/ebooks/2600](https://www.gutenberg.org/ebooks/2600)'
- en: 'The following snippet loads the book''s content for use as our source dataset:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段载入了作为我们源数据集的书籍内容：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The next step is to prepare our dataset for the model. As we discussed in the
    *Representing text* section, textual data is transformed into vectors using word
    representation models. One way to do so is to first transform them into one-hot
    encoded vectors, which are then transformed into dense representations using models
    such as word2vec. The other way is to transform them into an arbitrary numerical
    representation first and then train an embedding layer along with the rest of
    the RNN-based language model. In this case, we are using the latter approach of
    training an embedding layer alongside the rest of the model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是准备我们的数据集用于模型。正如我们在*表示文本*部分讨论的那样，文本数据被转换为向量，使用词表示模型。一种方法是首先将它们转换为独热编码向量，然后使用诸如word2vec之类的模型将其转换为密集表示。另一种方法是首先将它们转换为任意数值表示，然后在RNN-based语言模型的其余部分中训练嵌入层。在这种情况下，我们使用了后一种方法，即在模型的其余部分一起训练一个嵌入层。
- en: 'The following snippet prepares a mapping of individual characters to their
    integer mapping:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段准备了单个字符到整数映射的映射：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, each unique character is mapped to an integer; for instance,
    `\n` is mapped to 0, `!` is mapped to 3, and so on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个唯一的字符都映射到一个整数；例如，`\n`映射为0，`!`映射为3，依此类推。
- en: 'For optimal memory utilization, we can make use of the `tf.data` API to slice
    our data into manageable slices. We restrict our input sequences to 100 characters
    long, and this API helps us create contiguous slices of this dataset. This is
    showcased in the following code snippet:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最佳的内存利用，我们可以利用`tf.data` API将我们的数据切片为可管理的片段。我们将我们的输入序列限制在100个字符长度，并且这个API帮助我们创建这个数据集的连续片段。这在下面的代码片段中展示：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have created the model object. As is apparent from the snippet, the model
    is a stack of embedding, LSTM, and dense layers. The embedding layer helps transform
    raw text into vector form, and is followed by the LSTM and dense layers, which
    learn context and language semantics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了模型对象。从代码片段中可以看出，模型是一堆嵌入层、LSTM层和稠密层。嵌入层有助于将原始文本转换为向量形式，然后是LSTM和稠密层，它们学习上下文和语言语义。
- en: 'The next set of steps involve defining a loss function and compiling the model.
    We will be using sparse categorical cross-entropy as our loss function. The following
    snippet defines the loss function and compiles the model; we are using the Adam
    optimizer for minimization:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组步骤涉及定义损失函数和编译模型。我们将使用稀疏分类交叉熵作为我们的损失函数。下面的代码片段定义了损失函数和编译模型；我们使用Adam优化器进行最小化：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since we are using TensorFlow with the high-level Keras API, training the model
    is as simple as calling the `fit` function. We train the model for just 10 epochs,
    using the `ModelCheckpoint` callback to save the model''s weights every epoch,
    as shown in the following snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用TensorFlow和高级Keras API，训练模型就像调用`fit`函数一样简单。我们只训练了10个时代，使用`ModelCheckpoint`回调来保存模型的权重，如下面的代码片段所示：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Congratulations, you've trained your very first language model. Now, we'll use
    it to generate some fake text. Before we do that, though, we need to understand
    how we can decode the output generated by our model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你已经训练了你的第一个语言模型。现在，我们将使用它来生成一些假文本。在我们做到这一点之前，我们需要了解如何解码我们模型生成的输出。
- en: Decoding strategies
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码策略
- en: 'Earlier on, we transformed all the textual data into suitable vector forms
    for training and inference purposes. Now that we have a trained model, the next
    step is to input some context words and generate the next word as output. This
    output generation step is formally known as the **decoding step**. It is termed
    "decoding" because the model outputs a vector which has to be processed to get
    the actual word as output. There are a few different decoding techniques; let''s
    briefly discuss the popular ones: greedy decoding, beam search, and sampling.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们将所有文本数据转换为适合训练和推理的向量形式。现在我们有了一个训练好的模型，下一步是输入一些上下文词，以及生成下一个词作为输出。这个输出生成步骤正式被称为**解码步骤**。它被称为"解码"，因为模型输出一个向量，必须经过处理才能得到实际的词作为输出。有一些不同的解码技术；让我们简要讨论一下流行的：贪婪解码、束搜索和抽样。
- en: Greedy decoding
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贪婪解码
- en: This is the simplest and fastest decoding strategy. As the name suggests, greedy
    decoding is a method which picks up the highest probability term at every prediction
    step.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单和最快的解码策略。正如其名，贪婪解码是一种在每次预测步骤中选择最高概率项的方法。
- en: While this is fast and efficient, being greedy does create a few issues while
    generating text. By focusing on only the highest probability outputs, the model
    may generate inconsistent or incoherent outputs. In the case of character-language
    models, this may even result in outputs that are non-dictionary words. Greedy
    decoding also limits the variance of outputs, which may result in repetitive content
    as well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这样快速高效，但贪婪会在生成文本时产生一些问题。通过仅关注最高概率的输出，模型可能会生成不一致或不连贯的输出。在字符语言模型的情况下，这甚至可能导致非词典词的输出。贪婪解码还限制了输出的差异性，这也可能导致重复的内容。
- en: Beam search
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 波束搜索
- en: 'Beam search is a widely used alternative to greedy decoding. This decoding
    strategy, instead of picking the highest probability term, keeps track of *n*
    possible outputs at every timestep. The following diagram illustrates the beam
    search decoding strategy. It shows multiple beams forming from step 0, creating
    a tree-like structure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 波束搜索是广泛使用的贪婪解码的替代方法。该解码策略不是选择最高概率的术语，而是在每个时间步长跟踪*n*个可能的输出。下图说明了波束搜索解码策略。它展示了从步骤0开始形成的多个波束，创建了一个树状结构：
- en: '![Diagram  Description automatically generated](img/B16176_09_07.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B16176_09_07.png)'
- en: 'Figure 9.7: Beam search-based decoding strategy'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：基于波束搜索的解码策略
- en: As shown in *Figure 9.7*, the beam search strategy works by keeping track of
    *n* predictions at every timestep and finally selects the path with the **overall**
    highest probability, highlighted with bold lines in the figure. Let's analyze
    the beam search decoding example used in the preceding diagram step by step, assuming
    a beam size of 2.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.7*所示，波束搜索策略通过在每个时间步长跟踪*n*个预测，并最终选择具有**总体**最高概率的路径来工作，在图中用粗线突出显示。让我们逐步分析在上述图中使用的波束搜索解码示例，假设波束大小为2。
- en: 'At time step *t*[0]:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步骤*t*[0]：
- en: The model predicts the following three words (with probabilities) as (**the**,
    0.3), (**when**, 0.6), and (**and**, 0.1).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型预测以下三个单词（带概率）为(**the**, 0.3)，(**when**, 0.6)和(**and**, 0.1)。
- en: In the case of greedy decoding, we would have selected "when" as it has the
    highest probability.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在贪婪解码的情况下，我们将选择"when"，因为它的概率最高。
- en: In this case, we will keep track of the top two outputs as our beam size is
    2.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们的波束大小为2，我们将跟踪前两个输出。
- en: 'At time step *t*[2]:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步骤*t*[2]：
- en: We repeat the same steps; that is, we keep track of the top two outputs from
    each of the two beams.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重复相同的步骤；即，我们跟踪两个波束中的前两个输出。
- en: 'The beam-wise scores are calculated by multiplying the probabilities along
    the branches, like so:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过沿着分支计算分支的概率，计算波束分数如下：
- en: '*(when, 0.6) –> (the, 0.4) = 0.6*0.4 = 0.24*'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(when, 0.6) –> (the, 0.4) = 0.6*0.4 = 0.24*'
- en: '*(the, 0.3) –> (war, 0.9) = 0.3*0.9 = 0.27*'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(the, 0.3) –> (war, 0.9) = 0.3*0.9 = 0.27*'
- en: Based on the above discussion, the final output generated is "It was July, 1805
    *the war*". This output had a final probability of 0.27 in comparison to an output
    like "It was July, 1805 *when the*", which had a score of 0.24, and is what greedy
    decoding would have given us.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述讨论，生成的最终输出是"It was July, 1805 *the war*"。与"它是1805年7月，*当时的*战争"这样的输出相比，它的最终概率为0.27，而"它是1805年7月，*当时*的"这样的输出的分数为0.24，这是贪婪解码给我们的结果。
- en: This decoding strategy drastically improves upon the naïve greedy decoding strategy
    we discussed in the previous section. This, in a way, provides the language model
    with additional capabilities to pick the best possible outcome.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解码策略大大改进了我们在前一节讨论的天真贪婪解码策略。这在某种程度上为语言模型提供了额外的能力，以选择最佳的可能结果。
- en: Sampling
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽样
- en: 'Sampling is a process wherein a predefined a number of observations are selected
    from a larger population. As an improvement over greedy decoding, a random sampling
    decoding method can be employed to address the variation/repetition issue. In
    general, a sampling-based decoding strategy helps in selecting the next word conditioned
    on the context so far, that is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样是一个过程，在此过程中从更大的总体中选择了预定义数量的观察结果。作为对贪婪解码的改进，可以采用随机抽样解码方法来解决变化/重复问题。一般来说，基于抽样的解码策略有助于根据迄今为止的上下文选择下一个词，即：
- en: '![](img/B16176_09_010.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_09_010.png)'
- en: 'Here, *w*[t] is the output at time step *t* that''s been conditioned on words
    that are generated until time step *t-1*. Continuing with the example from our
    previous decoding strategies, the following image highlights how a sampling-based
    decoding strategy would select the next word:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*[t]是在时间步*t*上的输出，已经根据在时间步*t-1*之前生成的单词进行了条件化。延续我们之前解码策略的示例，以下图像突出显示了基于采样的解码策略将如何选择下一个单词：
- en: '![Chart, funnel chart  Description automatically generated](img/B16176_09_08.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图，漏斗图 描述自动生成](img/B16176_09_08.png)'
- en: 'Figure 9.8: Sampling-based decoding strategy'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：基于采样的解码策略
- en: As shown in *Figure 9.8*, this method picks a random word at every timestep
    from the given conditional probability. In the case of our example, the model
    ended by randomly selecting **in** and then **Paris** as subsequent outputs. If
    you notice carefully, at timestep *t*[1], the model ends up selecting the word
    with the least probability. This brings in a much-required randomness associated
    with the way humans use language. Holtzman et al. in their work titled *The Curious
    Case of Neural Text Degeneration*⁵ present this exact argument by stating that
    humans do not always simply use the words with the highest probability. They present
    different scenarios and examples to highlight how language is a random choice
    of words and not a typical high probability curve formed by beam search or greedy
    decoding.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图 9.8*所示，该方法在每个时间步从给定的条件概率中随机选择一个单词。在我们的示例中，模型最终通过随机选择**in**，然后选择**Paris**作为随后的输出。如果你仔细观察，在时间步*t*[1]，模型最终选择了概率最低的单词。这带来了与人类使用语言方式相关的非常必要的随机性。
    Holtzman 等人在其题为*神经文本退化的奇特案例*的作品⁵中通过陈述人类并不总是简单地使用概率最高的单词来提出了这个确切的论点。他们提出了不同的场景和示例，以突显语言是单词的随机选择，而不是由波束搜索或贪婪解码形成的典型高概率曲线。
- en: This brings us to an important parameter called *temperature*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引入一个重要的参数称为*温度*。
- en: Temperature
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 温度
- en: As we discussed earlier, a sampling-based decoding strategy helps with improving
    the randomness of the output. However, too much randomness is also not ideal,
    as it can lead to gibberish and incoherent results. To control this amount of
    randomness, we can introduce a tunable parameter called temperature. This parameter
    helps to increase the likelihood of high probability terms while reducing the
    likelihood of low probability ones, which leads to sharper distributions. High
    temperature leads to more randomness, while lower temperature brings in predictability.
    An important point to note is that this can be applied to any decoding strategy.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，基于采样的解码策略有助于改善输出的随机性。然而，过多的随机性也不理想，因为它可能导致无意义和不连贯的结果。为了控制这种随机性的程度，我们可以引入一个可调参数称为温度。该参数有助于增加高概率项的概率，同时减少低概率项的概率，从而产生更尖锐的分布。高温度导致更多的随机性，而较低的温度则带来可预测性。值得注意的是，这可以应用于任何解码策略。
- en: Top-k sampling
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Top-k 采样
- en: Beam search and sampling-based decoding strategies both have their own set of
    advantages and disadvantages. Top-*k* sampling is a hybrid strategy which takes
    the best of both worlds to provide an even more sophisticated decoding method.
    In simple terms, at every timestep, instead of selecting a random word, we keep
    track of the *top k terms* (similar to beam search) and redistribute the probabilities
    among them. This gives the model an additional chance of generating coherent samples.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 波束搜索和基于采样的解码策略都有各自的优缺点。Top-*k* 采样是一种混合策略，它兼具两者的优点，提供了更复杂的解码方法。简单来说，在每个时间步，我们不是选择一个随机单词，而是跟踪*前k个条目*（类似于波束搜索），并在它们之间重新分配概率。这给了模型生成连贯样本的额外机会。
- en: 'Hands-on: Decoding strategies'
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实践操作：解码策略
- en: Now that we have a decent enough understanding of some of the most widely used
    decoding strategies, it's time to see them in action.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对一些最广泛使用的解码策略有了足够的理解，是时候看看它们的实际效果了。
- en: 'The first step is to prepare a utility function `generate_text` to generate
    the next word based on a given decoding strategy, as shown in the following code
    snippet:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是准备一个实用函数`generate_text`，根据给定的解码策略生成下一个单词，如下面的代码片段所示：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The code first transforms raw input text into integer indices. We then use
    the model to make predictions which are manipulated based on the mode selected,
    greedy or sampling. We already have a character-language model trained from the
    previous exercise, along with a utility to help us generate the next word based
    on a decoding strategy of choice. We use both of these in the following snippet
    to understand the different outputs that are generated using different strategies:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先将原始输入文本转换为整数索引。然后我们使用模型进行预测，根据所选择的模式进行操作，贪婪或采样。我们已经从前面的练习中训练了一个字符语言模型，以及一个辅助工具，帮助我们根据所选择的解码策略生成下一个词。我们在以下片段中使用了这两者来理解使用不同策略生成的不同输出：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The results of using the same seed with different decoding strategies are showcased
    in the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同种子与不同解码策略的结果在以下屏幕截图中展示：
- en: '![Graphical user interface, text, application, letter, email  Description automatically
    generated](img/B16176_09_09.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，信件，电子邮件 说明自动生成](img/B16176_09_09.png)'
- en: 'Figure 9.9: Text generation based on different decoding strategies. The text
    in bold is the seed text, followed by the output text generated by the model.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：基于不同解码策略的文本生成。粗体文本是种子文本，后面是模型生成的输出文本。
- en: This output highlights some of the issues as well as the salient features of
    all the decoding strategies we've discussed so far. We can see how the increase
    in temperature makes the model more expressive. We can also observe that the model
    has learned to pair up quotation marks and even use punctuation. The model also
    seems to have learned how to use capitalization. The added expressiveness of the
    temperature parameter comes at the cost of the stability of the model. Thus, there
    is usually a trade-off between expressiveness and stability.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出突显了我们迄今讨论的所有解码策略的一些问题以及显著特征。我们可以看到温度的增加如何使模型更具表现力。我们还可以观察到模型已经学会了配对引号甚至使用标点符号。模型似乎还学会了如何使用大写字母。温度参数的增加表现力是以牺牲模型稳定性为代价的。因此，通常在表现力和稳定性之间存在权衡。
- en: This concludes our first method for generating text; we leveraged RNNs (LSTMs
    in particular) to generate text using different decoding strategies. Next, we
    will look at some variations of the LSTM model, as well as convolutions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们生成文本的第一种方法的总结；我们利用了RNNs（特别是LSTMs）来使用不同的解码策略生成文本。接下来，我们将看一些LSTM模型的变体，以及卷积。
- en: LSTM variants and convolutions for text
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM变体和文本的卷积
- en: RNNs are extremely useful when it comes to handling sequential datasets. We
    saw in the previous section how a simple model effectively learned to generate
    text based on what it learned from the training dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理序列数据集时，RNNs非常有用。我们在前一节中看到，一个简单的模型有效地学会了根据训练数据集学到的内容生成文本。
- en: 'Over the years, there have been a number of enhancements in the way we model
    and use RNNs. In this section, we will discuss two widely used variants of the
    single-layer LSTM network we discussed in the previous section: stacked and bidirectional
    LSTMs.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我们在对RNNs建模和使用的方式方面取得了许多改进。在本节中，我们将讨论前一节中讨论的单层LSTM网络的两种广泛使用的变体：堆叠和双向LSTMs。
- en: Stacked LSTMs
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠的LSTMs
- en: We are well aware of how the depth of a neural network helps it learn complex
    and abstract concepts when it comes to computer vision tasks. Along the same lines,
    a stacked LSTM architecture, which has multiple layers of LSTMs stacked one after
    the other, has been shown to give considerable improvements. Stacked LSTMs were
    first presented by Graves et al. in their work *Speech Recognition with Deep Recurrent
    Neural Networks*.⁶ They highlight the fact that depth – multiple layers of RNNs
    – has a greater impact on performance compared to the number of units per layer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常清楚神经网络的深度在处理计算机视觉任务时如何帮助其学习复杂和抽象的概念。同样，一个堆叠的LSTM架构，它有多个LSTMs层依次堆叠在一起，已经被证明能够带来相当大的改进。堆叠的LSTMs首次由格雷夫斯等人在他们的工作中提出*使用深度递归神经网络进行语音识别*⁶。他们强调了深度-多层RNNs-与每层单位数目相比，对性能的影响更大。
- en: Though there isn't any theoretical proof to explain this performance gain, empirical
    results help us understand the impact. These enhancements can be attributed to
    the model's capacity to learn complex features and even abstract representations
    of inputs. Since there is a time component associated with LSTMs and RNNs in general,
    deeper networks learn the ability to operate at different time scales as well.⁷
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有理论证明可以解释这种性能提升，经验结果帮助我们理解影响。这些增强可以归因于模型学习复杂特征甚至输入的抽象表示的能力。由于 LSTM 和 RNNs
    一般具有时间成分，更深的网络学习在不同时间尺度上运行的能力。⁷
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The dataset, training loop, and even the inference utilities remain as-is. For
    brevity, we have skipped presenting those code snippets again. We will discuss
    the bidirectional argument that we introduce here shortly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集、训练循环，甚至推理工具保持不变。为了简洁起见，我们跳过了再次展示这些代码摘录。我们不久将讨论我们在这里引入的双向参数。
- en: 'Now, let''s see how the results look for this deeper LSTM-based language model.
    The following screenshot demonstrates the results from this model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个更深的基于 LSTM 的语言模型的结果是如何的。下面的截图展示了这个模型的结果：
- en: '![A close up of a flower  Description automatically generated](img/B16176_09_10.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![花朵的近照  自动生成描述](img/B16176_09_10.png)'
- en: 'Figure 9.10: Text generation based on different decoding strategies for the
    stacked-LSTM based language model'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.10: 基于不同解码策略的堆叠 LSTM 的语言模型的文本生成'
- en: We can clearly see how the generated text is picking up the writing style of
    the book, capitalization, punctuation, and other aspects better than the outputs
    shown in *Figure 9.9*. This highlights some of the advantages we discussed regarding
    deeper RNN architectures.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，生成的文本如何更好地捕捉到书中的书写风格、大写、标点等方面，比*图 9.9*中显示的结果更好。这突出了我们讨论过的关于更深的 RNN
    结构的一些优势。
- en: Bidirectional LSTMs
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向 LSTM
- en: 'The second variant that''s very widely used nowadays is the bidirectional LSTM.
    We have already discussed how LSTMs, and RNNs in general, condition their outputs
    by making use of previous timesteps. When it comes to text or any sequence data,
    this means that the LSTM is able to make use of past context to predict future
    timesteps. While this is a very useful property, this is not the best we can achieve.
    Let''s illustrate why this is a limitation through an example:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在非常广泛使用的第二个变体是双向 LSTM。我们已经讨论过 LSTMs 和 RNNs 生成它们的输出通过利用之前的时间步。当涉及到文本或任何序列数据时，这意味着
    LSTM 能够利用过去的上下文来预测未来的时间步。虽然这是一个非常有用的特性，但这并不是我们可以达到的最好水平。让我们通过一个例子来说明为什么这是一个限制：
- en: '![Graphical user interface, text, application, PowerPoint  Description automatically
    generated](img/B16176_09_11.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用、PowerPoint  自动生成描述](img/B16176_09_11.png)'
- en: 'Figure 9.11: Looking at both past and future context windows for a given word'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.11: 查看给定单词的过去和未来上下文窗口'
- en: 'As is evident from this example, without looking at what is to the right of
    the target word "Teddy", the model would not pick up the context properly. To
    handle such scenarios, bidirectional LSTMs were introduced. The idea behind them
    is pretty simple and straightforward. A bidirectional LSTM (or biLSTM) is a combination
    of two LSTM layers that work simultaneously. The first is the usual forward LSTM,
    which takes the input sequence in its original order. The second one is called
    the backward LSTM, which takes **a reversed copy** of the sequence as input. The
    following diagram showcases a typical biLSTM setup:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子可以明显看出，不看目标单词“Teddy”右侧的内容，模型无法正确地获取上下文。为了处理这种情况，引入了双向 LSTM。它们的背后的想法非常简单和直接。双向
    LSTM（或者 biLSTM）是两个 LSTM 层同时工作的组合。第一个是通常的前向 LSTM，它按照原始顺序接受输入序列。第二个被称为后向 LSTM，它接受**倒置的一份复制**作为输入序列。下面的图表展示了一个典型的双向
    LSTM 设置：
- en: '![Diagram  Description automatically generated](img/B16176_09_12.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成描述](img/B16176_09_12.png)'
- en: 'Figure 9.12: Bidirectional LSTM setup'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.12: 双向 LSTM 设置'
- en: As depicted in *Figure 9.12*, the forward and backward LSTMs work in tandem
    to process the original and reversed copy of the input sequences. Since we have
    two LSTM cells working on different contexts at any given time step, we need a
    way of defining the output that will be used by the downstream layers in the network.
    The outputs can be combined via summation, multiplication, concatenation, or even
    averaging of hidden states. Different deep learning frameworks might set different
    defaults, but the most widely used method is concatenation of the biLSTM outputs.
    Please note that, similar to biLSTM, we can make use of bi-RNNs or even bi-GRUs
    (**Gated Recurrent Units**).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.12*所示，前向和后向LSTM协作处理输入序列的原始和反转副本。由于在任何给定的时间步上有两个LSTM单元在不同的上下文上工作，我们需要一种定义输出的方式，以供网络中的下游层使用。输出可以通过求和、乘积、连接，甚至隐藏状态的平均值来组合。不同的深度学习框架可能会设置不同的默认值，但最常用的方法是双向LSTM输出的连接。请注意，与双向LSTM类似，我们可以使用双向RNN或甚至双向GRU（**门控循环单元**）。
- en: The biLSTM setup has advantages compared to a normal LSTM, as the former can
    look at the future context as well. This advantage also becomes a limitation when
    it is not possible to peek into the future. For the current use case of text generation,
    biLSTMs are leveraged in an encoder-decoder type of architecture. We make use
    of biLSTMs to learn better embeddings of the inputs, but the decoding stage (where
    we use these embeddings to guess the next word) only uses the normal LSTMs. Similar
    to earlier hands-on exercises, we can train this network using the same set of
    utilities. We leave this as an exercise for you; for now, we will move on to convolutions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通LSTM相比，双向LSTM设置具有优势，因为它可以查看未来的上下文。当无法窥视未来时，这种优势也变成了限制。对于当前的文本生成用例，我们利用双向LSTM在编码器-解码器类型的架构中。我们利用双向LSTM来学习更好地嵌入输入，但解码阶段（我们使用这些嵌入去猜测下一个单词）只使用普通的LSTM。与早期的实践一样，我们可以使用相同的一套工具来训练这个网络。我们把这留给你来练习；现在我们将继续讨论卷积。
- en: Convolutions and text
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积和文本
- en: 'RNNs are extremely powerful and expressive when it comes to *sequence-to-sequence*
    tasks such as text generation. Yet they meet a few challenges:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在*序列对序列*任务（例如文本生成）方面非常强大和表现出色。但它们也面临一些挑战：
- en: RNNs suffer from vanishing gradients when the context window is very wide. Though
    LSTMs and GRUs overcome that to a certain extent, the context windows are still
    fairly small compared to the typical non-local interaction of words we see in
    normal usage.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当上下文窗口非常宽时，RNN会受到消失梯度的影响。虽然LSTM和GRU在一定程度上克服了这一问题，但是与我们在正常用法中看到的非局部交互的典型情况相比，上下文窗口仍然非常小。
- en: The recurrence aspect of RNNs makes them sequential and eventually slow for
    training as well as inference.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN的反复出现使其变得顺序且最终在训练和推断时变得缓慢。
- en: The architecture we covered in the previous section tries to encode the whole
    input context (or seed text) into a single vector, which is then used by the decoder
    to generate the next set of words. This creates limitations when the seed/context
    is pretty long, as does the fact that the RNN pays a lot more attention to the
    last set of inputs in the context.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在上一节介绍的架构试图将整个输入语境（或种子文本）编码成单个向量，然后由解码器用于生成下一组单词。这在种子/语境非常长时会受到限制，正如RNN更多地关注上下文中最后一组输入的事实一样。
- en: RNNs have a larger memory footprint compared to other types of neural network
    architectures; that is, they require more parameters and hence more memory during
    their implementation.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其他类型的神经网络架构相比，RNN具有更大的内存占用空间；也就是说，在实现过程中需要更多的参数和更多的内存。
- en: On the other hand, we have convolutional networks, which are battle-tested in
    the field of computer vision. State-of-the-art architectures make use of CNNs
    to extract features and perform well on different vision tasks. The success of
    CNNs led researchers to explore their application to NLP tasks as well.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们有卷积网络，在计算机视觉领域经过了战斗的检验。最先进的架构利用CNN提取特征，在不同的视觉任务上表现良好。CNN的成功使研究人员开始探索它们在自然语言处理任务中的应用。
- en: The main idea behind using CNNs for text is to first try to create vector representations
    of **a set of words** rather than individual words. More formally, the idea is
    to generate a vector representation of every sub-sequence of words in a given
    sentence.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNN处理文本的主要思想是首先尝试创建**一组单词**的向量表示，而不是单个单词。更正式地说，这个想法是在给定句子中生成每个单词子序列的向量表示。
- en: Let's consider a sample sentence, "Flu outbreak forces schools to close". The
    aim would be to first break down this sentence into all possible sub-sequences,
    such as "Flu outbreak forces", "outbreak forces schools",…, "schools to close",
    and then generate a vector representation of each of these sub-sequences. Though
    such sub-sequences may or may not carry much meaning, they provide us with a way
    to understand words in different contexts, as well as their usage. Since we already
    understand how to prepare dense vector representation of words (see the *Distributed
    representation* section), let's build on top of that to understand how CNNs can
    be leveraged.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个示例句子，“流感爆发迫使学校关闭”。首先的目标将是将这个句子分解为所有可能的子序列，比如“流感爆发迫使”，“爆发迫使学校”，…，“学校关闭”，然后为每个子序列生成一个向量表示。虽然这样的子序列可能或可能不带有太多含义，但它们为我们提供了一种理解不同上下文中的单词以及它们的用法的方式。由于我们已经了解如何准备单词的密集向量表示（参见*Distributed
    representation*一节），让我们在此基础上了解如何利用CNNs。
- en: 'Continuing with the preceding example, *Figure 9.13 (A)* depicts each of the
    words in their vector form. The vectors are only 4-dimensional for ease of understanding:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前面的例子，*图 9.13（A）* 描述了每个单词的向量形式。为了方便理解，向量仅为4维：
- en: '![Calendar  Description automatically generated](img/B16176_09_13.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的日历描述](img/B16176_09_13.png)'
- en: 'Figure 9.13: (A) Vector representation (1x4) of each word in sample sentence.
    (B) Two kernels/filters of size 3 each. (C) Phrase vectors of dimension 1x2 each
    after taking the Hadamard product, followed by the sum for each kernel with stride
    1.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：（A）示例句子中每个单词的向量表示（1x4）。 （B）两个大小为3的内核/过滤器。 （C）取Hadamard乘积后的每个内核的维度为1x2的短语向量，然后进行步幅为1的求和。
- en: The two kernels, each of size 3, are depicted in *Figure 9.13 (B)*. The kernels
    in the case of text/NLP use cases are chosen to be as wide as the word vector
    dimension. The size of 3 signifies the context window each kernel is focusing
    on. Since the kernel width is the same as the word-vector width, we move the kernel
    along the words in the sentence. This constraint on size and movement in one direction
    only is the reason these convolutional filters are termed 1-D convolutions. The
    output phrase vectors are depicted in *Figure 9.13 (C)*.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 两个大小为3的内核分别显示在*图 9.13（B）* 中。在文本/NLP用例中，内核的选择为单词向量维度的宽度。大小为3表示每个内核关注的上下文窗口。由于内核宽度与单词向量宽度相同，我们将内核沿着句子中的单词移动。这种尺寸和单向移动的约束是这些卷积滤波器被称为1-D卷积的原因。输出短语向量显示在*图
    9.13（C）* 中。
- en: Similar to deep convolutional neural networks for computer vision use cases,
    the above setup enables us to stack 1-D convolutional layers for NLP use cases
    as well. The greater depth allows the models to capture not just more complex
    representations but also a wider context window (this is analogous to an increase
    in the receptive field for a vision model with depth).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于用于计算机视觉用例的深度卷积神经网络，上述设置也使我们能够为自然语言处理用例堆叠1-D卷积层。更大的深度不仅允许模型捕获更复杂的表示，还允许捕获更广泛的上下文窗口（这类似于增加视觉模型的感受野随深度增加）。
- en: 'Using CNNs for NLP use cases also improves computation speed, as well as reducing
    the memory and time requirements to train such networks. In fact, these are some
    of the advantages that are explored by the following works for NLP tasks using
    1-D CNNs:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNNs用于自然语言处理用例还提高了计算速度，同时减少了训练这种网络所需的内存和时间。事实上，这些都是以下研究中探索的一些使用1-D CNNs的自然语言处理任务的优势：
- en: '*Natural Language Processing (almost) from Scratch*, Collobert et al.⁸'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言处理（几乎）从零开始*，Collobert 等⁸'
- en: '*Character-level Convolutional Networks for Text Classification*, Zhang et
    al.⁹'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于文本分类的字符级卷积网络*，Zhang 等⁹'
- en: '*Convolutional Neural Networks for Sentence Classification*, Kim^(10)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于句子分类的卷积神经网络*，Kim^(10)'
- en: '*Recurrent Convolutional Neural Networks for Text Classification*, Lai and
    Xu et al.^(11)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于文本分类的循环卷积神经网络*，Lai 和 Xu 等^(11)'
- en: So far, we've discussed how CNNs can be used to extract features and capture
    a larger context for NLP use cases. Language-related tasks, especially text generation,
    have a certain temporal aspect associated with them. Hence, the next obvious question
    is, can we leverage CNNs for understanding temporal features, just like RNNs do?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了CNNs如何用于提取特征并为自然语言处理用例捕获更大的上下文。与语言相关的任务，特别是文本生成，与之相关的有一定的时间方面。因此，下一个显而易见的问题是，我们是否可以利用CNNs来理解时间特征，就像RNNs一样？
- en: Researchers have been exploring the use of CNNs for temporal or sequential processing
    for quite some time. While we discussed how CNNs are a good choice for capturing
    the context of a given word, this presents a problem for certain use cases. For
    instance, tasks such as language modeling/text generation require models to understand
    context, but only from one side. In simple words, a language model works by looking
    at words that have already been processed (past context) to generate future words.
    But a CNN can span to future timesteps as well.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经探索使用CNN进行时间或序贯处理已经有一段时间了。虽然我们讨论了CNN如何是捕捉给定单词上下文的好选择，但这对于某些用例来说存在问题。例如，像语言建模/文本生成这样的任务需要模型理解上下文，但只需来自一侧。简单来说，语言模型通过查看已处理的单词（过去上下文）来生成未来单词。但CNN也可以覆盖未来的时间步。
- en: 'Digressing a bit from the NLP domain, the works by Van den Oord et al. on PixelCNNs^(12)
    and WaveNets^(13) are particularly important to understand the use of CNNs in
    a temporal setting. They present the concept of **causal convolutions** to ensure
    CNNs only utilize past and not future context. This concept is highlighted in
    the following diagram:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 从NLP领域稍微偏离，Van den Oord等人关于PixelCNNs^(12)和WaveNets^(13)的作品对于理解CNN在时间设置中的应用特别重要。他们提出了因果卷积的概念，以确保CNN只利用过去而不是未来上下文。这一概念在下图中得到了突出：
- en: '![A chain link fence  Description automatically generated](img/B16176_09_14.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![一个链条围栏  自动生成的描述](img/B16176_09_14.png)'
- en: 'Figure 9.14: Causal padding for CNNs Based on Van den Oord et al.^(13) Figure
    2'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：基于Van den Oord等人^(13)的CNN的因果填充。 图2
- en: Causal convolutions ensure that the model, at any given time step *t*, makes
    predictions of the type *p*(*x*[t+1] *|* *x*[1:][t]) and doesn't depend on future
    timesteps *x*[t+1], *x*[t+2] … *x*[t+][T], as depicted in *Figure 9.14*. During
    training, conditional predictions for all timesteps can be made in parallel; the
    generation/inference step is sequential though; the output at every timestep is
    fed back into the model for the next timestep's prediction.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因果卷积确保模型在任何给定的时间步*t*都会进行*p*(*x*[t+1] *|* *x*[1:][t])类型的预测，并且不依赖于未来的时间步*x*[t+1]，*x*[t+2]
    … *x*[t+][T]，正如 *图9.14*所示。在训练过程中，可以并行地为所有时间步进行条件预测；但生成/推理步骤是顺序的；每个时间步的输出都会反馈给模型以进行下一个时间步的预测。
- en: Since this setup does not have any recurrent connections, the model trains faster,
    even for longer sequences. The setup for causal convolutions originated for image
    and audio generation use cases but has been extended to NLP use cases as well.
    The authors of the WaveNet paper additionally made use of a concept called *dilated
    convolutions* to give the model larger receptive fields without requiring very
    deep architectures.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个设置没有任何循环连接，模型训练速度更快，哪怕是更长序列也一样。因果卷积的设置最初来源于图像和音频生成用例，但也已扩展到NLP用例。WaveNet论文的作者此外利用了一个称为*dilated
    convolutions*的概念，以提供更大的感知域，而不需要非常深的架构。
- en: This idea of using CNNs to capture and use temporal components has opened up
    doors for further exploration.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这种利用CNN捕捉和使用时间组件的想法已经为进一步探索打开了大门。
- en: 'Before we move on to the more involved concepts of attention and transformer
    architectures in the next chapter, it is important to highlight some important
    works which preceded them:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一章涉及更复杂的注意力和变换器架构之前，有必要强调一些先前的重要作品：
- en: '*Neural Machine Translation in Time* by Kalchbrenner et al.^(14) presents the
    ByteNet neural translation model based on encoder-decoder architecture. The overall
    setup makes use of 1-D causal convolutions, along with dilated kernels, to provide
    state-of-the-art performance on English to German translation tasks.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalchbrenner等人^(14)的*时间内神经机器翻译*介绍了基于编码器-解码器架构的ByteNet神经翻译模型。总体设置利用1-D因果卷积，以及扩张核，以在英德翻译任务上提供最先进的性能。
- en: Dauphin et al. presented a language model based on Gated Convolutions in their
    work titled *Language Modeling with Gated Convolutional Networks*.^(15) They observed
    that their Gated Convolutions provide remarkable training speedup and lower memory
    footprint.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dauphin等人在他们名为*具有门控卷积的语言建模*的作品中提出了一个基于门控卷积的语言模型。(15) 他们观察到他们的门控卷积提供了显着的训练加速和更低的内存占用。
- en: Works by Gehring et al.^(16) and Lea et al.^(17) explored these ideas further
    and provided even better results.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring等人^(16)和Lea等人^(17)的作品进一步探讨了这些想法，并提供了更好的结果。
- en: Interested readers may also explore the paper titled *An Empirical Evaluation
    of Generic Convolutional and Recurrent Networks for Sequence Modeling* by Bai
    et al.^(18) This paper provides a nice overview of RNN- and CNN-based architectures
    for sequence modeling tasks.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感兴趣的读者还可以探索 Bai 等人的一篇题为*基于序列建模的通用卷积和循环网络的实证评估*的论文^(18)。该论文为基于循环神经网络（RNN）和卷积神经网络（CNN）的架构提供了一个很好的概述，用于序列建模任务。
- en: This concludes our discussion of the building blocks of older architectures
    for language modeling.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对语言建模旧架构的基本要素的讨论。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations on completing a complex chapter involving a large number of
    concepts. In this chapter, we covered various concepts associated with handling
    textual data for the task of text generation. We started off by developing an
    understanding of different text representation models. We covered most of the
    widely used representation models, from Bag of Words to word2vec and even FastText.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺你完成了涉及大量概念的复杂章节。在本章中，我们涵盖了处理文本数据以进行文本生成任务的各种概念。我们首先了解了不同的文本表示模型。我们涵盖了大多数广泛使用的表示模型，从词袋到word2vec甚至FastText。
- en: The next section of the chapter focused on developing an understanding of RNN-based
    text generation models. We briefly discussed what comprises a language model and
    how we can prepare a dataset for such a task. We then trained a character-based
    language model to generate synthetic text samples. We touched upon different decoding
    strategies and used them to understand different outputs from our RNN based-language
    model. We also delved into a few variants, such as stacked LSTMs and bidirectional
    LSTM-based language models. Finally, we discussed the usage of convolutional networks
    in the NLP space.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的下一部分重点讨论了发展对基于循环神经网络（RNN）的文本生成模型的理解。我们简要讨论了什么构成了语言模型以及我们如何为这样的任务准备数据集。之后我们训练了一个基于字符的语言模型，生成了一些合成文本样本。我们涉及了不同的解码策略，并用它们来理解我们RNN模型的不同输出。我们还深入探讨了一些变种，比如堆叠LSTM和双向LSTM的语言模型。最后，我们讨论了在NLP领域使用卷积网络的情况。
- en: In the next chapter, we will focus on the building blocks of some of the most
    recent and powerful architectures in the NLP domain, including attention and transformers
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注NLP领域一些最新和最强大架构的基本构件，包括注意力和变压器。
- en: References
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation
    of Word Representations in Vector Space*. arXiv. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *词向量的高效估计*. arXiv. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)
- en: 'Rumelhart, D.E., & McClelland, J.L. (1987). *Distributed Representations*,
    in Parallel Distributed Processing: Explorations in the Microstructure of Cognition:
    Foundations, pp.77-109\. MIT Press. [https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf)'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rumelhart, D.E., & McClelland, J.L. (1987). *分布表示*, in 并行分布式处理: 认知微结构探索：基础,
    pp.77-109\. MIT Press. [https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf)'
- en: 'Pennington, J., Socher, R., & Manning, C.D. (2014). *GloVe: Global Vectors
    for Word Representation*. Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP). [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pennington, J., Socher, R., & Manning, C.D. (2014). *GloVe: 全局词向量表示*. Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
    [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
- en: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *Enriching Word
    Vectors with Subword Information*. arXiv. [https://arxiv.org/abs/1607.04606](https://arxiv.org/abs/1607.04606)
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *使用子词信息丰富词向量*.
    arXiv. [https://arxiv.org/abs/1607.04606](https://arxiv.org/abs/1607.04606)
- en: Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). *The Curious
    Case of Neural Text Degeneration*. arXiv. [https://arxiv.org/abs/1904.09751](https://arxiv.org/abs/1904.09751)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). *神经文本退化的好奇案例*.
    arXiv. [https://arxiv.org/abs/1904.09751](https://arxiv.org/abs/1904.09751)
- en: Graves, A., Mohamed, A., & Hinton, G. (2013). *Speech Recognition with Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Graves, A., Mohamed, A., & Hinton, G. (2013). *深度递归神经网络语音识别*. arXiv. [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)
- en: Pascanu, R., Gulcehre, C., Cho, K., & Bengio, Y. (2013). *How to Construct Deep
    Recurrent Neural Networks*. arXiv. [https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026)
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pascanu, R., Gulcehre, C., Cho, K., & Bengio, Y. (2013). *如何构建深度循环神经网络*. arXiv.
    [https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026)
- en: Collobert, R., Weston, J., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
    *Natural Language Processing (almost) from Scratch*. arXiv. [https://arxiv.org/abs/1103.0398](https://arxiv.org/abs/1103.0398)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Collobert, R., Weston, J., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
    *几乎从零开始的自然语言处理*. arXiv. [https://arxiv.org/abs/1103.0398](https://arxiv.org/abs/1103.0398)
- en: Zhang, X., Zhao, J., & LeCun, Y. (2015). *Character-level Convolutional Networks
    for Text Classification*. arXiv. [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, X., Zhao, J., & LeCun, Y. (2015). *用于文本分类的字符级卷积网络*. arXiv. [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)
- en: Kim, Y. (2014). *Convolutional Neural Networks for Sentence Classification*.
    arXiv. [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kim, Y. (2014). *用于句子分类的卷积神经网络*. arXiv. [https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)
- en: Lai, S., Xu, L., Liu, K., & Zhao, J. (2015). *Recurrent Convolutional Neural
    Networks for Text Classification*. Proceedings of the Twenty-Ninth AAAI Conference
    on Artifical Intelligence. [http://zhengyima.com/my/pdfs/Textrcnn.pdf](http://zhengyima.com/my/pdfs/Textrcnn.pdf)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lai, S., Xu, L., Liu, K., & Zhao, J. (2015). *用于文本分类的循环卷积神经网络*. 第二十九届AAAI人工智能大会论文集。[http://zhengyima.com/my/pdfs/Textrcnn.pdf](http://zhengyima.com/my/pdfs/Textrcnn.pdf)
- en: van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &
    Kavukcuoglu, K. (2016). *Conditional Image Generation with PixelCNN Decoders*.
    arXiv. [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &
    Kavukcuoglu, K. (2016). *具有 PixelCNN 解码器的条件图像生成*. arXiv. [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328)
- en: 'van den Oord, A., Dieleman, S., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,
    N., Senior, A., Kavukcuoglu, K. (2016). *WaveNet: A Generative Model for Raw Audio*.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: van den Oord, A., Dieleman, S., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,
    N., Senior, A., Kavukcuoglu, K. (2016). *WaveNet：用于原始音频的生成模型*. [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
- en: Kalchbrenner, N., Espeholt, L., Simonyan, K., van den Oord, A., Graves, A.,
    & Kavukcuoglu, K. (2016). *Neural Machine Translation in Linear Time*. arXiv.
    [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kalchbrenner, N., Espeholt, L., Simonyan, K., van den Oord, A., Graves, A.,
    & Kavukcuoglu, K. (2016). *线性时间内的神经机器翻译*. arXiv. [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
- en: Dauphin, Y.N., Fan, A., Auli, M., & Grangier, D. (2016). *Language Modeling
    with Gated Convolutional Networks*. arXiv. [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dauphin, Y.N., Fan, A., Auli, M., & Grangier, D. (2016). *带门卷积网络的语言建模*. arXiv.
    [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)
- en: Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y.N. (2017). *Convolutional
    Sequence to Sequence Learning*. arXiv. [https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y.N. (2017). *卷积序列到序列学习*.
    arXiv. [https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)
- en: Lea, C., Flynn, M.D., Vidal, R., Reiter, A., & Hager, G.D. (2016). *Temporal
    Convolutional Networks for Action Segmentation and Detection*. arXiv. [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lea, C., Flynn, M.D., Vidal, R., Reiter, A., & Hager, G.D. (2016). *用于动作分割和检测的时态卷积网络*.
    arXiv. [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)
- en: Bai, S., Kolter, J.Z., & Koltun, V. (2018). *An Empirical Evaluation of Generic
    Convolutional and Recurrent Networks for Sequence Modeling*. arXiv. [https://arxiv.org/abs/1803.01271](https://arxiv.org/abs/1803.01271)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bai, S., Kolter, J.Z., & Koltun, V. (2018). *对序列建模的通用卷积和循环网络的经验评估*. arXiv. [https://arxiv.org/abs/1803.01271](https://arxiv.org/abs/1803.01271)
