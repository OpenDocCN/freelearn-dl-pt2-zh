- en: '*Chapter 6*: Fine-Tuning Language Models for Token Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about fine-tuning language models for token classification.
    Tasks such as **Named Entity Recognition** (**NER**), **Part-of-Speech** (**POS**)
    tagging, and **Question Answering** (**QA**) are explored in this chapter. We
    will learn how a specific language model can be fine-tuned on such tasks. We will
    focus on BERT more than other language models. You will learn how to apply POS,
    NER, and QA using BERT. You will get familiar with the theoretical details of
    these tasks such as their respective datasets and how to perform them. After finishing
    this chapter, you will be able to perform any token classification using Transformers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will fine-tune BERT for the following tasks: fine-tuning
    BERT for token classification problems such as NER and POS, fine-tuning a language
    model for an NER problem, and thinking of the QA problem as a start/stop token
    classification.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to token classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning language models for NER
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering using token classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using Jupyter Notebook to run our coding exercises and Python 3.6+
    and the following packages need to be installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers 4.0+`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Datasets`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seqeval`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All notebooks with coding exercises will be available at the following GitHub
    link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/2UGMQP2](https://bit.ly/2UGMQP2)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to token classification
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task of classifying each token in a token sequence is called **token classification**.
    This task says that a specific model must be able to classify each token into
    a class. POS and NER are two of the most well-known tasks in this criterion. However,
    QA is also another major NLP task that fits in this category. We will discuss
    the basics of these three tasks in the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NER
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the well-known tasks in the category of token classification is NER
    – the recognition of each token as an entity or not and identifying the type of
    each detected entity. For example, a text can contain multiple entities at the
    same time – person names, locations, organizations, and other types of entities.
    The following text is a clear example of NER:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '`George Washington is one the presidents of the United States of America.`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '*George Washington* is a person name while *the* *United States of America*
    is a location name. A sequence tagging model is expected to tag each word in the
    form of tags, each containing information about the tag. BIO''s tags are the ones
    that are universally used for standard NER tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table is a list of tags and their descriptions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1 – Table of BIOS tags and their descriptions ](img/B17123_06_Table_1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Table 1 – Table of BIOS tags and their descriptions
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'From this table, **B** indicates the beginning of a tag, and **I** denotes
    the inside of a tag, while **O** is the outside of the entity. This is the reason
    that this type of annotation is called **BIO**. For example, the sentence shown
    earlier can be annotated using BIO:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Accordingly, the sequence must be tagged in BIO format. A sample dataset can
    be in the format shown as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – CONLL2003 dataset ](img/B17123_06_002.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – CONLL2003 dataset
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the NER tags we have seen, there are POS tags available in this
    dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Understanding POS tagging
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: POS tagging, or grammar tagging, is annotating a word in a given text according
    to its respective part of speech. As a simple example, in a given text, identification
    of each word's role in the categories of noun, adjective, adverb, and verb is
    considered to be POS. However, from a linguistic perspective, there are many roles
    other than these four.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of POS tags, there are variations, but the Penn Treebank POS tagset
    is one of the most well-known ones. The following screenshot shows a summary and
    respective description of these roles:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Penn Treebank POS tags ](img/B17123_06_003.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Penn Treebank POS tags
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Datasets for POS tasks are annotated like the example shown in *Figure 6.1*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The annotation of these tags is very useful in specific NLP applications and
    is one of the building blocks of many other methods. Transformers and many advanced
    models can somehow understand the relation of words in their complex architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Understanding QA
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A QA or reading comprehension task comprises a set of reading comprehension
    texts with respective questions on them. An exemplary dataset from this scope
    is **SQUAD** or **Stanford Question Answering Dataset**. This dataset consists
    of Wikipedia texts and respective questions asked about them. The answers are
    in the form of segments of the original Wikipedia text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – SQUAD dataset example ](img/B17123_06_004.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – SQUAD dataset example
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The highlighted red segments are the answers and important parts of each question
    are highlighted in blue. It is required for a good NLP model to segment text according
    to the question, and this segmentation can be done in the form of sequence labeling.
    The model labels the start and the end of the segment as answer start and end
    segments.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you have learned the basics of modern NLP sequence tagging
    tasks such as QA, NER, and POS. In the next section, you will learn how it is
    possible to fine-tune BERT for these specific tasks and use the related datasets
    from the `datasets` library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning language models for NER
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to fine-tune BERT for an NER task. We first
    start with the `datasets` library and by loading the `conll2003` dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset card is accessible at [https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003).
    The following screenshot shows this model card from the HuggingFace website:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – CONLL2003 dataset card from HuggingFace ](img/B17123_06_005.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – CONLL2003 dataset card from HuggingFace
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'From this screenshot, it can be seen that the model is trained on this dataset
    and is currently available and listed in the right panel. However, there are also
    descriptions of the dataset such as its size and its characteristics:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the dataset, the following commands are used:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A download progress bar will appear and after finishing the downloading and
    caching, the dataset will be ready to use. The following screenshot shows the
    progress bars:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Downloading and preparing the dataset ](img/B17123_06_006.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.5 – Downloading and preparing the dataset
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can easily double-check the dataset by accessing the train samples using
    the following command:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot shows the result:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6 – CONLL2003 train samples from the datasets library ](img/B17123_06_007.jpg)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.6 – CONLL2003 train samples from the datasets library
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The respective tags for POS and NER are shown in the preceding screenshot.
    We will use only NER tags for this part. You can use the following command to
    get the NER tags available in this dataset:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is also shown in *Figure 6.7*. All the BIO tags are shown and there
    are nine tags in total:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to load the BERT tokenizer:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `tokenizer` class can work with white-space tokenized sentences also. We
    need to enable our tokenizer for working with white-space tokenized sentences,
    because the NER task has a token-based label for each token. Tokens in this task
    are usually the white-space tokenized words rather than BPE or any other tokenizer
    tokens. According to what is said, let''s see how `tokenizer` can be used with
    a white-space tokenized sentence:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, by just setting `is_split_into_words` to `True`, the problem
    is solved.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is required to preprocess the data before using it for training. To do so,
    we must use the following function and map into the entire dataset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This function will make sure that our tokens and labels are aligned properly.
    This alignment is required because the tokens are tokenized in pieces, but the
    words must be of one piece. To test and see how this function works, you can run
    it by giving a single sample to it:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the result is shown as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'But this result is not readable, so you can run the following code to have
    a readable version:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result is shown as follows:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Result of the tokenize and align functions](img/B17123_06_008.jpg)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.7 – Result of the tokenize and align functions
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The mapping of this function to the dataset can be done by using the `map`
    function of the `datasets` library:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the next step, it is required to load the BERT model with the respective
    number of labels:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model will be loaded and ready to be trained. In the next step, we must
    prepare the trainer and training parameters:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is required to prepare the data collator. It will apply batch operations
    on the training dataset to use less memory and perform faster. You can do so as
    follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To be able to evaluate model performance, there are many metrics available
    for many tasks in HuggingFace''s `datasets` library. We will be using the sequence
    evaluation metric for NER. seqeval is a good Python framework to evaluate sequence
    tagging algorithms and models. It is necessary to install the `seqeval` library:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Afterward, you can load the metric:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It is easily possible to see how the metric works by using the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result is as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Output of the seqeval metric ](img/B17123_06_009.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.8 – Output of the seqeval metric
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Various metrics such as accuracy, F1-score, precision, and recall are computed
    for the sample input.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following function is used to compute the metrics:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The last steps are to make a trainer and train it accordingly:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After running the `train` function of `trainer`, the result will be as follows:![Figure
    6.9 – Trainer results after running train ](img/B17123_06_010.jpg)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6.9 – Trainer results after running train
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is necessary to save the model and tokenizer after training:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you wish to use the model with the pipeline, you must read the config file
    and assign `label2id` and `id2label` correctly according to the labels you have
    used in the `label_list` object:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Afterward, it is easy to use the model as in the following example:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And the result will appear as seen here:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Up to this point, you have learned how to apply POS using BERT. You learned
    how to train your own POS tagging model using Transformers and you also tested
    the model. In the next section, we will focus on QA.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Question answering using token classification
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **QA** problem is generally defined as an NLP problem with a given text and
    a question for AI, and getting an answer back. Usually, this answer can be found
    in the original text but there are different approaches to this problem. In the
    case of **Visual Question Answering** (**VQA**), the question is about a visual
    entity or visual concept rather than text but the question itself is in the form
    of text.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of VQA are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – VQA examples ](img/B17123_06_011.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – VQA examples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the models that are intended to be used in VQA are multimodal models
    that can understand the visual context along with the question and generate the
    answer properly. However, unimodal fully textual QA or just QA is based on textual
    context and textual questions with respective textual answers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'SQUAD is one of the most well-known datasets in the field of QA. To see examples
    of SQUAD and examine them, you can use the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SQUAD是问答领域中最知名的数据集之一。要查看SQUAD的示例并对其进行检查，您可以使用以下代码：
- en: '[PRE24]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the result:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE25]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: However, there is version 2 of the SQUAD dataset, which has more training samples,
    and it is highly recommended to use it. To have an overall understanding of how
    it is possible to train a model for a QA problem, we will focus on the current
    part of this problem.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但是，SQUAD数据集还有第2版，其中有更多的训练样本，并且强烈建议使用它。为了全面了解如何为QA问题训练模型的可能性，我们将重点放在解决这个问题的当前部分上。
- en: 'To start, load SQUAD version 2 using the following code:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，使用以下代码加载SQUAD第2版：
- en: '[PRE26]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After loading the SQUAD dataset, you can see the details of this dataset by
    using the following code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加载SQUAD数据集之后，您可以通过使用以下代码查看此数据集的详细信息：
- en: '[PRE27]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result is as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.11 – SQUAD dataset (version 2) details ](img/B17123_06_012.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.11 – SQUAD数据集（第2版）详细信息](img/B17123_06_012.jpg)'
- en: Figure 6.11 – SQUAD dataset (version 2) details
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11 – SQUAD数据集（第2版）详细信息
- en: The details of the SQUAD dataset will be shown as seen in *Figure 6.11*. As
    you can see, there are more than 130,000 training samples with more than 11,000
    validation samples.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SQUAD数据集的详细信息将显示在*图6.11*中。正如您所看到的，有超过130,000个训练样本和超过11,000个验证样本。
- en: 'As we did for NER, we must preprocess the data to have the right form to be
    used by the model. To do so, you must first load your tokenizer, which is a pretrained
    tokenizer as long as you are using a pretrained model and want to fine-tune it
    for a QA problem:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们对NER所做的那样，我们必须预处理数据，使其具有适合模型使用的正确形式。为此，您必须首先加载您的分词器，只要您使用预训练模型并希望为QA问题进行微调：
- en: '[PRE28]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you have seen, we are going to use the `distillBERT` model.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们将使用`distillBERT`模型。
- en: According to our SQUAD example, we need to give more than one text to the model,
    one for the question and one for the context. Accordingly, we need our tokenizer
    to put these two side by side and separate them with the special `[SEP]` token
    because `distillBERT` is a BERT-based model.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们的SQUAD示例，我们需要向模型提供不止一个文本，一个用于问题，一个用于上下文。因此，我们的分词器需要将这两个文本并排放在一起，并使用特殊的`[SEP]`标记将它们分开，因为`distillBERT`是基于BERT的模型。
- en: There is another problem in the scope of QA, and it is the size of the context.
    The context size can be longer than the model input size, but we cannot reduce
    it to the size the model accepts. With some problems, we can do so but in QA,
    it is possible that the answer could be in the truncated part. We will show you
    an example where we tackle this problem using document stride.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在QA范围内还有另一个问题，即上下文的大小。上下文的大小可以比模型输入大小长，但我们不能将其缩减到模型接受的大小。对于某些问题，我们可能可以这样做，但在QA中，答案可能在被截断的部分中。我们将向您展示一个示例，展示我们如何使用文档步幅来解决此问题。
- en: 'The following is an example to show how it works using `tokenizer`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示了如何使用`tokenizer`：
- en: '[PRE29]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The stride is the document stride used to return the stride for the second
    part, like a window, while the `return_overflowing_tokens` flag gives the model
    information on whether it should return the extra tokens. The result of `tokenized_example`
    is more than a single tokenized output, instead having two input IDs. In the following,
    you can see the result:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步幅是用于返回第二部分的窗口的文档步幅，而`return_overflowing_tokens`标志向模型提供有关是否应返回额外标记的信息。`tokenized_example`的结果不止一个标记化输出，而是有两个输入ID。在以下，您可以看到结果：
- en: '[PRE30]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Accordingly, you can see the full result by running the following `for` loop:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，您可以通过运行以下`for`循环看到完整的结果：
- en: '[PRE31]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE32]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see from the preceding output, with a window of 128 tokens, the rest
    of the context is replicated again in the second output of input IDs.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您可以从前面的输出中看到的那样，使用128个标记的窗口，剩余的上下文再次复制到了第二个输出的输入ID中。
- en: Another problem is the end span, which is not available in the dataset, but
    instead, the start span or the start character for the answer is given. It is
    easy to find the length of the answer and add it to the start span, which would
    automatically yield the end span.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个问题是结束跨度，在数据集中不可用，而是给出了答案的开始跨度或开始字符。很容易找到答案的长度并将其添加到起始跨度，这将自动产生结束跨度。
- en: 'Now that we know all the details of this dataset and how to deal with them,
    we can easily put them together to make a preprocessing function (link: [https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py)):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Mapping this function to the dataset would apply all the required changes:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Just like other examples, you can now load a pretrained model to be fine-tuned:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next step is to create training arguments:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we are not going to use a data collator, we will give a default data collator
    to the model trainer:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, everything is ready to make the trainer:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And the trainer can be used with the `train` function:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The result will be something like the following:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Training results ](img/B17123_06_013.jpg)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.12 – Training results
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the model is trained with three epochs and the outputs for loss
    in validation and training are reported.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Like any other model, you can easily save this model by using the following
    function:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If you want to use your saved model or any other model that is trained on QA,
    the `transformers` library provides a pipeline that's easy to use and implement
    with no extra effort.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By using this pipeline functionality, you can use any model. The following
    is an example given for using a model with the QA pipeline:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The pipeline just requires two inputs to make the model ready for usage, the
    model and the tokenizer. Although, you are also required to give it a pipeline
    type, which is QA in the given example.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to give it the inputs it requires, `context` and `question`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The model can be used by the following example:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And the result can be seen as follows:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Up to this point, you have learned how you can train on the dataset you want.
    You have also learned how you can use the trained model using pipelines.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to fine-tune a pretrained model to any token
    classification task. Fine-tuning models on NER and QA problems were explored.
    Using the pretrained and fine-tuned models on specific tasks with pipelines was
    detailed with examples. We also learned about various preprocessing steps for
    these two tasks. Saving pretrained models that are fine-tuned on specific tasks
    was another major learning point of this chapter. We also saw how it is possible
    to train models with a limited input size on tasks such as QA that have longer
    sequence sizes than the model input. Using tokenizers more efficiently to have
    document splitting with document stride was another important item in this chapter
    too.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss text representation methods using Transformers.
    By studying the chapter, you will learn how to perform zero-/few-shot learning
    and semantic text clustering.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
