- en: '*Chapter 6*: Fine-Tuning Language Models for Token Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about fine-tuning language models for token classification.
    Tasks such as **Named Entity Recognition** (**NER**), **Part-of-Speech** (**POS**)
    tagging, and **Question Answering** (**QA**) are explored in this chapter. We
    will learn how a specific language model can be fine-tuned on such tasks. We will
    focus on BERT more than other language models. You will learn how to apply POS,
    NER, and QA using BERT. You will get familiar with the theoretical details of
    these tasks such as their respective datasets and how to perform them. After finishing
    this chapter, you will be able to perform any token classification using Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will fine-tune BERT for the following tasks: fine-tuning
    BERT for token classification problems such as NER and POS, fine-tuning a language
    model for an NER problem, and thinking of the QA problem as a start/stop token
    classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to token classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning language models for NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering using token classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using Jupyter Notebook to run our coding exercises and Python 3.6+
    and the following packages need to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers 4.0+`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Datasets`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seqeval`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All notebooks with coding exercises will be available at the following GitHub
    link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/2UGMQP2](https://bit.ly/2UGMQP2)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to token classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task of classifying each token in a token sequence is called **token classification**.
    This task says that a specific model must be able to classify each token into
    a class. POS and NER are two of the most well-known tasks in this criterion. However,
    QA is also another major NLP task that fits in this category. We will discuss
    the basics of these three tasks in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the well-known tasks in the category of token classification is NER
    – the recognition of each token as an entity or not and identifying the type of
    each detected entity. For example, a text can contain multiple entities at the
    same time – person names, locations, organizations, and other types of entities.
    The following text is a clear example of NER:'
  prefs: []
  type: TYPE_NORMAL
- en: '`George Washington is one the presidents of the United States of America.`'
  prefs: []
  type: TYPE_NORMAL
- en: '*George Washington* is a person name while *the* *United States of America*
    is a location name. A sequence tagging model is expected to tag each word in the
    form of tags, each containing information about the tag. BIO''s tags are the ones
    that are universally used for standard NER tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table is a list of tags and their descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1 – Table of BIOS tags and their descriptions ](img/B17123_06_Table_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 – Table of BIOS tags and their descriptions
  prefs: []
  type: TYPE_NORMAL
- en: 'From this table, **B** indicates the beginning of a tag, and **I** denotes
    the inside of a tag, while **O** is the outside of the entity. This is the reason
    that this type of annotation is called **BIO**. For example, the sentence shown
    earlier can be annotated using BIO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Accordingly, the sequence must be tagged in BIO format. A sample dataset can
    be in the format shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – CONLL2003 dataset ](img/B17123_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – CONLL2003 dataset
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the NER tags we have seen, there are POS tags available in this
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: Understanding POS tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: POS tagging, or grammar tagging, is annotating a word in a given text according
    to its respective part of speech. As a simple example, in a given text, identification
    of each word's role in the categories of noun, adjective, adverb, and verb is
    considered to be POS. However, from a linguistic perspective, there are many roles
    other than these four.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of POS tags, there are variations, but the Penn Treebank POS tagset
    is one of the most well-known ones. The following screenshot shows a summary and
    respective description of these roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Penn Treebank POS tags ](img/B17123_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Penn Treebank POS tags
  prefs: []
  type: TYPE_NORMAL
- en: Datasets for POS tasks are annotated like the example shown in *Figure 6.1*.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation of these tags is very useful in specific NLP applications and
    is one of the building blocks of many other methods. Transformers and many advanced
    models can somehow understand the relation of words in their complex architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding QA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A QA or reading comprehension task comprises a set of reading comprehension
    texts with respective questions on them. An exemplary dataset from this scope
    is **SQUAD** or **Stanford Question Answering Dataset**. This dataset consists
    of Wikipedia texts and respective questions asked about them. The answers are
    in the form of segments of the original Wikipedia text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – SQUAD dataset example ](img/B17123_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – SQUAD dataset example
  prefs: []
  type: TYPE_NORMAL
- en: The highlighted red segments are the answers and important parts of each question
    are highlighted in blue. It is required for a good NLP model to segment text according
    to the question, and this segmentation can be done in the form of sequence labeling.
    The model labels the start and the end of the segment as answer start and end
    segments.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you have learned the basics of modern NLP sequence tagging
    tasks such as QA, NER, and POS. In the next section, you will learn how it is
    possible to fine-tune BERT for these specific tasks and use the related datasets
    from the `datasets` library.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning language models for NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to fine-tune BERT for an NER task. We first
    start with the `datasets` library and by loading the `conll2003` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset card is accessible at [https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003).
    The following screenshot shows this model card from the HuggingFace website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – CONLL2003 dataset card from HuggingFace ](img/B17123_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – CONLL2003 dataset card from HuggingFace
  prefs: []
  type: TYPE_NORMAL
- en: 'From this screenshot, it can be seen that the model is trained on this dataset
    and is currently available and listed in the right panel. However, there are also
    descriptions of the dataset such as its size and its characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the dataset, the following commands are used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A download progress bar will appear and after finishing the downloading and
    caching, the dataset will be ready to use. The following screenshot shows the
    progress bars:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Downloading and preparing the dataset ](img/B17123_06_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.5 – Downloading and preparing the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can easily double-check the dataset by accessing the train samples using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6 – CONLL2003 train samples from the datasets library ](img/B17123_06_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.6 – CONLL2003 train samples from the datasets library
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The respective tags for POS and NER are shown in the preceding screenshot.
    We will use only NER tags for this part. You can use the following command to
    get the NER tags available in this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is also shown in *Figure 6.7*. All the BIO tags are shown and there
    are nine tags in total:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to load the BERT tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `tokenizer` class can work with white-space tokenized sentences also. We
    need to enable our tokenizer for working with white-space tokenized sentences,
    because the NER task has a token-based label for each token. Tokens in this task
    are usually the white-space tokenized words rather than BPE or any other tokenizer
    tokens. According to what is said, let''s see how `tokenizer` can be used with
    a white-space tokenized sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, by just setting `is_split_into_words` to `True`, the problem
    is solved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is required to preprocess the data before using it for training. To do so,
    we must use the following function and map into the entire dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function will make sure that our tokens and labels are aligned properly.
    This alignment is required because the tokens are tokenized in pieces, but the
    words must be of one piece. To test and see how this function works, you can run
    it by giving a single sample to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the result is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'But this result is not readable, so you can run the following code to have
    a readable version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Result of the tokenize and align functions](img/B17123_06_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.7 – Result of the tokenize and align functions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The mapping of this function to the dataset can be done by using the `map`
    function of the `datasets` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next step, it is required to load the BERT model with the respective
    number of labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model will be loaded and ready to be trained. In the next step, we must
    prepare the trainer and training parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is required to prepare the data collator. It will apply batch operations
    on the training dataset to use less memory and perform faster. You can do so as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To be able to evaluate model performance, there are many metrics available
    for many tasks in HuggingFace''s `datasets` library. We will be using the sequence
    evaluation metric for NER. seqeval is a good Python framework to evaluate sequence
    tagging algorithms and models. It is necessary to install the `seqeval` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Afterward, you can load the metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is easily possible to see how the metric works by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Output of the seqeval metric ](img/B17123_06_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.8 – Output of the seqeval metric
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Various metrics such as accuracy, F1-score, precision, and recall are computed
    for the sample input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following function is used to compute the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last steps are to make a trainer and train it accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After running the `train` function of `trainer`, the result will be as follows:![Figure
    6.9 – Trainer results after running train ](img/B17123_06_010.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6.9 – Trainer results after running train
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is necessary to save the model and tokenizer after training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you wish to use the model with the pipeline, you must read the config file
    and assign `label2id` and `id2label` correctly according to the labels you have
    used in the `label_list` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Afterward, it is easy to use the model as in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the result will appear as seen here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Up to this point, you have learned how to apply POS using BERT. You learned
    how to train your own POS tagging model using Transformers and you also tested
    the model. In the next section, we will focus on QA.
  prefs: []
  type: TYPE_NORMAL
- en: Question answering using token classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **QA** problem is generally defined as an NLP problem with a given text and
    a question for AI, and getting an answer back. Usually, this answer can be found
    in the original text but there are different approaches to this problem. In the
    case of **Visual Question Answering** (**VQA**), the question is about a visual
    entity or visual concept rather than text but the question itself is in the form
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of VQA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – VQA examples ](img/B17123_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – VQA examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the models that are intended to be used in VQA are multimodal models
    that can understand the visual context along with the question and generate the
    answer properly. However, unimodal fully textual QA or just QA is based on textual
    context and textual questions with respective textual answers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SQUAD is one of the most well-known datasets in the field of QA. To see examples
    of SQUAD and examine them, you can use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, there is version 2 of the SQUAD dataset, which has more training samples,
    and it is highly recommended to use it. To have an overall understanding of how
    it is possible to train a model for a QA problem, we will focus on the current
    part of this problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To start, load SQUAD version 2 using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After loading the SQUAD dataset, you can see the details of this dataset by
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – SQUAD dataset (version 2) details ](img/B17123_06_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.11 – SQUAD dataset (version 2) details
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The details of the SQUAD dataset will be shown as seen in *Figure 6.11*. As
    you can see, there are more than 130,000 training samples with more than 11,000
    validation samples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As we did for NER, we must preprocess the data to have the right form to be
    used by the model. To do so, you must first load your tokenizer, which is a pretrained
    tokenizer as long as you are using a pretrained model and want to fine-tune it
    for a QA problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you have seen, we are going to use the `distillBERT` model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: According to our SQUAD example, we need to give more than one text to the model,
    one for the question and one for the context. Accordingly, we need our tokenizer
    to put these two side by side and separate them with the special `[SEP]` token
    because `distillBERT` is a BERT-based model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is another problem in the scope of QA, and it is the size of the context.
    The context size can be longer than the model input size, but we cannot reduce
    it to the size the model accepts. With some problems, we can do so but in QA,
    it is possible that the answer could be in the truncated part. We will show you
    an example where we tackle this problem using document stride.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is an example to show how it works using `tokenizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The stride is the document stride used to return the stride for the second
    part, like a window, while the `return_overflowing_tokens` flag gives the model
    information on whether it should return the extra tokens. The result of `tokenized_example`
    is more than a single tokenized output, instead having two input IDs. In the following,
    you can see the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Accordingly, you can see the full result by running the following `for` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from the preceding output, with a window of 128 tokens, the rest
    of the context is replicated again in the second output of input IDs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Another problem is the end span, which is not available in the dataset, but
    instead, the start span or the start character for the answer is given. It is
    easy to find the length of the answer and add it to the start span, which would
    automatically yield the end span.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we know all the details of this dataset and how to deal with them,
    we can easily put them together to make a preprocessing function (link: [https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Mapping this function to the dataset would apply all the required changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Just like other examples, you can now load a pretrained model to be fine-tuned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to create training arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we are not going to use a data collator, we will give a default data collator
    to the model trainer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, everything is ready to make the trainer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the trainer can be used with the `train` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Training results ](img/B17123_06_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.12 – Training results
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the model is trained with three epochs and the outputs for loss
    in validation and training are reported.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Like any other model, you can easily save this model by using the following
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you want to use your saved model or any other model that is trained on QA,
    the `transformers` library provides a pipeline that's easy to use and implement
    with no extra effort.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By using this pipeline functionality, you can use any model. The following
    is an example given for using a model with the QA pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pipeline just requires two inputs to make the model ready for usage, the
    model and the tokenizer. Although, you are also required to give it a pipeline
    type, which is QA in the given example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to give it the inputs it requires, `context` and `question`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model can be used by the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the result can be seen as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Up to this point, you have learned how you can train on the dataset you want.
    You have also learned how you can use the trained model using pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to fine-tune a pretrained model to any token
    classification task. Fine-tuning models on NER and QA problems were explored.
    Using the pretrained and fine-tuned models on specific tasks with pipelines was
    detailed with examples. We also learned about various preprocessing steps for
    these two tasks. Saving pretrained models that are fine-tuned on specific tasks
    was another major learning point of this chapter. We also saw how it is possible
    to train models with a limited input size on tasks such as QA that have longer
    sequence sizes than the model input. Using tokenizers more efficiently to have
    document splitting with document stride was another important item in this chapter
    too.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss text representation methods using Transformers.
    By studying the chapter, you will learn how to perform zero-/few-shot learning
    and semantic text clustering.
  prefs: []
  type: TYPE_NORMAL
