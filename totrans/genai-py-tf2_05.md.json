["```py\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\ncifar10_builder = tfds.builder(\"cifar10\")\ncifar10_builder.download_and_prepare() \n```", "```py\ncifar10_train = cifar10_builder.as_dataset(split=\"train\")\ncifar10_test = cifar10_builder.as_dataset(split=\"test\") \n```", "```py\ncifar10_train.take(1) \n```", "```py\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfor sample in cifar10_train.map(lambda x: flatten_image(x, label=True)).take(1):\n    plt.imshow(sample[0].numpy().reshape(32,32,3).astype(np.float32), \n               cmap=plt.get_cmap(\"gray\")\n              )\n    print(\"Label: %d\" % sample[1].numpy()) \n```", "```py\ndef flatten_image(x, label=False):\n    if label:\n        return (tf.divide(\n            tf.dtypes.cast(\n                tf.reshape(x[\"image\"], (1, 32*32*3)), tf.float32), \n                    256.0),\n                x[\"label\"])\n    else:\n        return (\n            tf.divide(tf.dtypes.cast(\n                tf.reshape(x[\"image\"], (1, 32*32*3)), tf.float32), \n                    256.0)) \n```", "```py\nclass BernoulliMLP(tf.keras.Model):\n    def __init__(self, input_shape, name='BernoulliMLP', hidden_dim=10, latent_dim=10, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._h = tf.keras.layers.Dense(hidden_dim, \n                                        activation='tanh')\n        self._y = tf.keras.layers.Dense(latent_dim, \n                                        activation='sigmoid')\n    def call(self, x):\n        return self._y(self._h(x)), None, None \n```", "```py\nclass GaussianMLP(tf.keras.Model):\n    def __init__(self, input_shape, name='GaussianMLP', hidden_dim=10, latent_dim=10, iaf=False, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._h = tf.keras.layers.Dense(hidden_dim, \n                                        activation='tanh')\n        self._mean = tf.keras.layers.Dense(latent_dim)\n        self._logvar = tf.keras.layers.Dense(latent_dim)\n        self._iaf_output = None\n        if iaf:\n            self._iaf_output = tf.keras.layers.Dense(latent_dim)\n    def call(self, x):\n        if self._iaf_output:\n            return self._mean(self._h(x)), self._logvar(self._h(x)), \n                self._iaf_output(self._h(x))\n        else:\n            return self._mean(self._h(x)), self._logvar(self._h(x)), \n                None \n```", "```py\nclass VAE(tf.keras.Model):    \n    def __init__(self, input_shape, name='variational_autoencoder',\n                 latent_dim=10, hidden_dim=10, encoder='GaussianMLP', \n                 decoder='BernoulliMLP', iaf_model=None,\n                 number_iaf_networks=0,\n                 iaf_params={},\n                 num_samples=100, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._latent_dim = latent_dim\n        self._num_samples = num_samples\n        self._iaf = []\n        if encoder == 'GaussianMLP':\n            self._encoder = GaussianMLP(input_shape=input_shape, \n                                        latent_dim=latent_dim, \n                                        iaf=(iaf_model is not None), \n                                        hidden_dim=hidden_dim)\n        else:\n            raise ValueError(\"Unknown encoder type: {}\".format(encoder))\n        if decoder == 'BernoulliMLP':\n            self._decoder = BernoulliMLP(input_shape=(1,latent_dim),\n                                         latent_dim=input_shape[1], \n                                         hidden_dim=hidden_dim)\n        elif decoder == 'GaussianMLP':\n            self._encoder = GaussianMLP(input_shape=(1,latent_dim), \n                                        latent_dim=input_shape[1], \n                                        iaf=(iaf_model is not None), \n                                        hidden_dim=hidden_dim)\n        else:\n            raise ValueError(\"Unknown decoder type: {}\".format(decoder))\n        if iaf_model:\n            self._iaf = []\n            for t in range(number_iaf_networks):\n                self._iaf.append(\n                    iaf_model(input_shape==(1,latent_dim*2), \n                              **iaf_params)) \n```", "```py\ndef encode(self, x):\n        return self._encoder.call(x)\n    def decode(self, z, apply_sigmoid=False):\n        logits, _, _ = self._decoder.call(z)\n        if apply_sigmoid:\n            probs = tf.sigmoid(logits)\n            return probs\n        return logits \n```", "```py\n@tf.function\n    def sample(self, eps=None):\n        if eps is None:\n            eps = tf.random.normal(shape=(self._num_samples, \n                                          self.latent_dim))\n        return self._decoder.call(eps, apply_sigmoid=False) \n```", "```py\ndef reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean \n```", "```py\n@property\n    def iaf(self):\n        return self._iaf \n```", "```py\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n    log2pi = tf.math.log(2\\. * np.pi)\n    return tf.reduce_sum(\n          -.5 * ((sample - mean) ** 2\\. * tf.exp(-logvar) + \\\n            logvar + log2pi), axis=raxis) \n```", "```py\n@tf.function\ndef compute_loss(model, x):\n    mean, logvar, h = model.encode(x)\n    z = model.reparameterize(mean, logvar)\n    logqz_x = log_normal_pdf(z, mean, logvar)\n    for iaf_model in model.iaf:\n        mean, logvar, _ = iaf_model.call(tf.concat([z, h], 2))\n        s = tf.sigmoid(logvar)\n        z = tf.add(tf.math.multiply(z,s), tf.math.multiply(mean,(1-s)))\n        logqz_x -= tf.reduce_sum(tf.math.log(s))\n\n    x_logit = model.decode(z)\n    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n    logpx_z = -tf.reduce_sum(cross_ent, axis=[2])\n    logpz = log_normal_pdf(z, 0., 0.)\n    return -tf.reduce_mean(logpx_z + logpz - logqz_x) \n```", "```py\n@tf.function\ndef compute_apply_gradients(model, x, optimizer):\n    with tf.GradientTape() as tape:\n        loss = compute_loss(model, x)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) \n```", "```py\nmodel = VAE(input_shape=(1,3072), hidden_dim=500, latent_dim=500) \n```", "```py\nmodel = VAE(input_shape=(1,3072), hidden_dim=500, latent_dim=500, \n    iaf_model=GaussianMLP, number_iaf_networks=3, \n    iaf_params={'latent_dim': 500, 'hidden_dim': 500, 'iaf': False}) \n```", "```py\nimport time as time\nepochs = 100\noptimizer = tf.keras.optimizers.Adam(1e-4)\nfor epoch in range(1, epochs + 1):\n    start_time = time.time()\n    for train_x in cifar10_train.map(\n            lambda x: flatten_image(x, label=False)).batch(32):\n        compute_apply_gradients(model, train_x, optimizer)\n    end_time = time.time()\n    if epoch % 1 == 0:\n        loss = tf.keras.metrics.Mean()\n        for test_x in cifar10_test.map(\n            lambda x: flatten_image(x, label=False)).batch(32):\n            loss(compute_loss(model, test_x))\n    elbo = -loss.result()\n    print('Epoch: {}, Test set ELBO: {}, '\n          'time elapse for current epoch {}'.format(epoch,\n                                                elbo,\n                                                end_time - start_time)) \n```", "```py\nfor sample in cifar10_train.map(lambda x: flatten_image(x, label=False)).batch(1).take(10):\n    mean, logvar, h = model.encode(sample)\n    z = model.reparameterize(mean, logvar)\n    for iaf_model in model.iaf:\n        mean, logvar, _ = iaf_model.call(tf.concat([z, h], 2))\n        s = tf.sigmoid(logvar)\n        z = tf.add(tf.math.multiply(z,s), tf.math.multiply(mean,(1-s)))    \n\n    plt.figure(0)\n    plt.imshow((sample.numpy().reshape(32,32,3)).astype(np.float32), \n               cmap=plt.get_cmap(\"gray\"))\n    plt.figure(1)\n    plt.imshow((model.decode(z).numpy().reshape(32,32,3)).astype(np.float32), cmap=plt.get_cmap(\"gray\")) \n```", "```py\nplt.imshow((model.sample(10)).numpy().reshape(32,32,3)).astype(np.float32), cmap=plt.get_cmap(\"gray\")) \n```"]