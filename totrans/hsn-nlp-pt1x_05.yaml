- en: '*Chapter 3*: NLP and Text Embeddings'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 3 章*：自然语言处理与文本嵌入'
- en: There are many different ways of representing text in deep learning. While we
    have covered basic **bag-of-words** (**BoW**) representations, unsurprisingly,
    there is a far more sophisticated way of representing text data known as embeddings.
    While a BoW vector acts only as a count of words within a sentence, embeddings
    help to numerically define the actual meaning of certain words.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中有许多不同的文本表示方式。尽管我们已经涵盖了基本的**词袋（bag-of-words）**（**BoW**）表示法，但毫不奇怪，还有一种更复杂的文本表示方式，称为嵌入。虽然词袋向量仅作为句子中单词的计数，嵌入则帮助数值化定义了某些单词的实际含义。
- en: In this chapter, we will explore text embeddings and learn how to create embeddings
    using a continuous BoW model. We will then move on to discuss n-grams and how
    they can be used within models. We will also cover various ways in which tagging,
    chunking, and tokenization can be used to split up NLP into its various constituent
    parts. Finally, we will look at TF-IDF language models and how they can be useful
    in weighting our models toward infrequently occurring words.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨文本嵌入，并学习如何使用连续词袋模型创建嵌入。然后我们将讨论 n-gram，以及它们如何在模型中使用。我们还将涵盖各种标注、分块和分词方法，以将自然语言处理拆分为其各个组成部分。最后，我们将看看
    TF-IDF 语言模型及其在加权模型中对不经常出现的单词的有用性。
- en: 'The following topics will be covered in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Word embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Exploring CBOW
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 CBOW
- en: Exploring n-grams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 n-gram
- en: Tokenization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Tagging and chunking for parts of speech
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注和分块
- en: TF-IDF
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: GLoVe vectors can be downloaded from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    . It is recommended to use the `glove.6B.50d.txt` file as it is much smaller than
    the other files and will be much faster to process. NLTK will be required for
    later parts of this chapter. All the code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GLoVe 向量可以从 [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    下载。建议使用`glove.6B.50d.txt`文件，因为它比其他文件要小得多，并且处理起来更快。后续章节将需要 NLTK。本章的所有代码可以在 [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x)
    找到。
- en: Embeddings for NLP
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中的嵌入
- en: 'Words do not have a natural way of representing their meaning. In images, we
    already have representations in rich vectors (containing the values of each pixel
    within the image), so it would clearly be beneficial to have a similarly rich
    vector representation of words. When parts of language are represented in a high-dimensional
    vector format, they are known as **embeddings**. Through analysis of a corpus
    of words, and by determining which words appear frequently together, we can obtain
    an *n*-length vector for each word, which better represents the semantic relationship
    of each word to all other words. We saw previously that we can easily represent
    words as one-hot encoded vectors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 单词没有自然的方式来表示它们的含义。在图像中，我们已经有了富向量表示（包含图像中每个像素的值），所以显然将单词表示为类似富向量的表示是有益的。当语言部分以高维向量格式表示时，它们被称为**嵌入**。通过对单词语料库的分析，并确定哪些单词经常在一起出现，我们可以为每个单词获取一个*n*长度的向量，这更好地表示了每个单词与所有其他单词的语义关系。我们之前看到，我们可以轻松地将单词表示为单热编码向量：
- en: '![Figure 3.1 – One-hot encoded vectors'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 单热编码向量'
- en: '](img/B12365_03_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_1.jpg)'
- en: Figure 3.1 – One-hot encoded vectors
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 单热编码向量
- en: 'On the other hand, embeddings are vectors of length *n* (in the following example,
    *n* = 3) that can take any value:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，嵌入是长度为*n*的向量（在以下示例中，*n*=3），可以取任何值：
- en: '![Figure 3.2 – Vectors with n=3'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – n=3 的向量'
- en: '](img/B12365_03_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_2.jpg)'
- en: Figure 3.2 – Vectors with n=3
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – n=3 的向量
- en: These embeddings represent the word's vector in *n*-dimensional space (where
    *n* is the length of the embedding vectors), and words with similar vectors within
    this space are considered to be more similar in meaning. While these embeddings
    can be of any size, they are generally of much lower dimensionality than the BoW
    representation. The BOW representation requires vectors that are of the length
    of the entire corpus, which, when looking at a whole language, could become very
    large very quickly. Although embeddings are of a high enough dimensionality to
    represent the individual words, they are generally not much larger than a few
    hundred dimensions. Furthermore, the BOW vectors are generally very sparse, consisting
    mostly of zeros, whereas embeddings are rich in data and every dimension contributes
    to the overall representation of the word. The lower dimensionality and the fact
    that they are not sparse makes performing deep learning on embeddings much more
    efficient than performing it on BOW representations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入代表了*n*维空间中单词的向量（其中*n*是嵌入向量的长度），在这个空间中具有相似向量的单词被认为在意义上更相似。虽然这些嵌入可以是任何尺寸，但它们通常比BoW表示的尺寸要低得多。
    BoW表示需要的向量长度是整个语料库的长度，当考虑整个语言时，可能会非常快速地变得非常大。尽管嵌入足够高维度以表示单词，但它们通常不比几百维大。此外，BoW向量通常非常稀疏，大部分由零组成，而嵌入富含数据，每个维度都有助于单词的整体表示。低维度和非稀疏性使得在嵌入上执行深度学习比在BoW表示上执行更加高效。
- en: GLoVe
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GLoVe
- en: 'We can download a set of pre-calculated word embeddings to demonstrate how
    they work. For this, we will use **Global Vectors for Word Representation** (**GLoVe**)
    embeddings, which can be downloaded from here: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    . These embeddings are calculated on a very large corpus of NLP data and are trained
    on a word co-occurrence matrix. This is based on the notion that words that appear
    together are more likely to have similar meaning. For instance, the word *sun*
    is likely to appear more frequently with the word *hot* as opposed to the word
    *cold*, so it is more likely that *sun* and *hot* are considered more similar.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以下载一组预先计算的单词嵌入来演示它们的工作原理。为此，我们将使用**全球词向量表示**（**GLoVe**）嵌入，可以从这里下载：[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    。这些嵌入是在一个非常大的NLP数据语料库上计算的，并且是基于单词共现矩阵进行训练的。这是基于这样的概念：一起出现的单词更有可能具有相似的含义。例如，单词*sun*更可能与单词*hot*一起频繁出现，而不是与单词*cold*一起，因此*sun*和*hot*更可能被认为是更相似的。
- en: 'We can validate that this is true by examining the individual GLoVe vectors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查单个GLoVe向量来验证这一点：
- en: 'We first create a simple function to load our GLoVe vectors from a text file.
    This just builds a dictionary where the index is each word in the corpus and the
    values are the embedding vector:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个简单的函数来从文本文件中加载我们的GLoVe向量。这只是构建一个字典，其中索引是语料库中的每个单词，值是嵌入向量：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This means we can access a single vector by just calling it from the dictionary:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着我们可以通过从字典中调用来访问单个向量：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in the following output:'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.3 – Vector output'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.3 – 向量输出'
- en: '](img/B12365_03_3.jpg)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_3.jpg)'
- en: Figure 3.3 – Vector output
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.3 – 向量输出
- en: 'We can see that this returns a 50-dimensional vector embedding for the word
    Python. We will now introduce the concept of **cosine similarity** to compare
    how similar two vectors are. Vectors will have a similarity of 1 if the angle
    in the *n*-dimensional space between them is 0 degrees. Values with high cosine
    similarity can be considered similar, even if they are not equal. This can be
    calculated using the following formula, where A and B are the two embedding vectors
    being compared:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，这返回了Python这个词的50维向量嵌入。现在我们将介绍余弦相似度的概念，以比较两个向量的相似度。如果*n*维空间中它们之间的角度为0度，则向量将具有相似度为1。具有高余弦相似度的值可以被认为是相似的，即使它们不相等。可以使用以下公式计算这一点，其中A和B是要比较的两个嵌入向量：
- en: '![](img/Formula_03_001.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Formula_03_001.jpg)'
- en: 'We can calculate this easily in Python using the `cosine_similarity()` function
    from `Sklearn`. We can see that `cat` and `dog` have similar vectors as they are
    both animals:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在Python中使用`Sklearn`的`cosine_similarity()`函数轻松计算这个。我们可以看到`cat`和`dog`作为动物具有相似的向量：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following output:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.4 – Cosine similarity output for cat and dog'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.4 – cat和dog的余弦相似度输出'
- en: '](img/B12365_03_4.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_4.jpg)'
- en: Figure 3.4 – Cosine similarity output for cat and dog
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4 – cat和dog的余弦相似度输出
- en: 'However, `cat` and `piano` are quite dissimilar as they are two seemingly unrelated
    items:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，`cat`和`piano`是非常不同的，因为它们是两个看似不相关的物品：
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.5 – Cosine similarity output for cat and piano'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5 – cat和piano的余弦相似度输出'
- en: '](img/B12365_03_5.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_5.jpg)'
- en: Figure 3.5 – Cosine similarity output for cat and piano
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – cat和piano的余弦相似度输出
- en: Embedding operations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入操作
- en: 'Since embeddings are vectors, we can perform operations on them. For example,
    let''s say we take the embeddings for the following sorts and we calculate the
    following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于嵌入是向量，我们可以对它们执行操作。例如，假设我们取以下类型的嵌入并计算以下内容：
- en: '*Queen-Woman+Man*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*Queen-Woman+Man*'
- en: 'With this, we can approximate the embedding for *king*. This essentially replaces
    the *Woman* vector component from *Queen* with the *Man* vector to arrive at this
    approximation. We can graphically illustrate this as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们可以近似计算*king*的嵌入。这实质上是将*Queen*的*Woman*向量部分替换为*Man*向量，以获得这个近似。我们可以用图形方式说明如下：
- en: '![Figure 3.6 – Graphical representation of the example'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6 – 示例的图形表示'
- en: '](img/B12365_03_6.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_6.jpg)'
- en: Figure 3.6 – Graphical representation of the example
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 示例的图形表示
- en: 'Note that in this example, we illustrate this graphically in two dimensions.
    In the case of our embeddings, this is happening in a 50-dimensional space. While
    this is not exact, we can verify that our calculated vector is indeed similar
    to the GLoVe vector for **King**:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，我们以二维图形方式进行了说明。在我们的嵌入中，这发生在一个50维空间中。虽然这不是精确的，我们可以验证我们计算的向量确实与**King**的GLoVe向量相似：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.7 – Output for the GloVe vector'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.7 – GLoVe向量输出'
- en: '](img/B12365_03_7.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_7.jpg)'
- en: Figure 3.7 – Output for the GLoVe vector
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – GLoVe向量输出
- en: While GLoVe embeddings are very useful pre-calculated embeddings, it is actually
    possible for us to calculate our own embeddings. This may be useful when we are
    analyzing a particularly unique corpus. For example, the language used on Twitter
    may differ from the language used on Wikipedia, so embeddings trained on one may
    not be useful for the other. We will now demonstrate how we can calculate our
    own embeddings using a continuous bag-of-words.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GLoVe嵌入非常有用且预先计算的嵌入，但我们实际上可以计算自己的嵌入。当我们分析特别独特的语料库时，这可能非常有用。例如，Twitter上使用的语言可能与维基百科上使用的语言不同，因此在一个语料库上训练的嵌入可能对另一个语料库无效。我们将展示如何使用连续词袋来计算自己的嵌入。
- en: Exploring CBOW
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索CBOW
- en: 'The **continuous** **bag-of-words (CBOW)** model forms part of Word2Vec – a
    model created by Google in order to obtain vector representations of words. By
    running these models over a very large corpus, we are able to obtain detailed
    representations of words that represent their semantic and contextual similarity
    to one another. The Word2Vec model consists of two main components:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续词袋模型（CBOW）**：这是Word2Vec的一部分，由Google创建，用于获取单词的向量表示。通过在非常大的语料库上运行这些模型，我们能够获得详细的单词表示，这些表示代表它们在语义和上下文上的相似性。Word2Vec模型包含两个主要组成部分：'
- en: '**CBOW**: This model attempts to predict the target word in a document, given
    the surrounding words.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW**：这个模型试图在文档中预测目标词，给定周围的单词。'
- en: '**Skip-gram**: This is the opposite of CBOW; this model attempts to predict
    the surrounding words, given the target word.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳字模型（Skip-gram）**：这是CBOW的相反，这个模型试图根据目标词来预测周围的单词。'
- en: 'Since these models perform similar tasks, we will focus on just one for now,
    specifically CBOW. This model aims to predict a word (the **target word**), given
    the other words around it (known as the **context** words). One way of accounting
    for context words could be as simple as using the word directly before the target
    word in the sentence to predict the target word, whereas more complex models could
    use several words before and after the target word. Consider the following sentence:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些模型执行类似的任务，我们现在只关注其中一个，具体来说是CBOW模型。这个模型旨在预测一个词（**目标词**），给定其周围的其他单词（称为**上下文**单词）。一种考虑上下文单词的方法可以简单到只使用目标词前面的单词来预测目标词，而更复杂的模型可以使用目标词前后的几个单词。考虑以下句子：
- en: '*PyTorch is a deep learning framework*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*PyTorch是一个深度学习框架*'
- en: 'Let''s say we want to predict the word *deep*, given the context words:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想预测*deep*这个词，给定上下文单词：
- en: '*PyTorch is a {target_word} learning framework*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*PyTorch is a {target_word} learning framework*'
- en: 'We could look at this in a number of ways:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从多个角度来看待这个问题：
- en: '![Figure 3.8 – Table of context and representations'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – 上下文和表示表'
- en: '](img/B12365_03_8.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_8.jpg)'
- en: Figure 3.8 – Table of context and representations
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 上下文和表示表
- en: For our CBOW model, we will use a window of length 2, which means for our model's
    (*X, y*) input/output pairs, we use *([n-2, n-1, n+1, n+2, n])*, where *n* is
    our target word being predicted.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 CBOW 模型，我们将使用长度为 2 的窗口，这意味着对于我们模型的 (*X, y*) 输入/输出对，我们使用 *([n-2, n-1, n+1,
    n+2, n])*，其中 *n* 是我们要预测的目标单词。
- en: Using these as our model inputs, we will train a model that includes an embedding
    layer. This embedding layer automatically forms an *n*-dimensional representation
    of the words in our corpus. However, to begin with, this layer is initialized
    with random weights. These parameters are what will be learned using our model
    so that after our model has finished training, this embedding layer can be used
    can be used to encode our corpus in an embedded vector representation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些作为我们模型的输入，我们将训练一个包括嵌入层的模型。这个嵌入层会自动形成我们语料库中单词的 n 维表示。然而，起初，这一层会用随机权重进行初始化。这些参数是我们模型学习的内容，以便在模型训练完成后，这个嵌入层可以被用来将我们的语料库编码成嵌入向量表示。
- en: CBOW architecture
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CBOW 架构
- en: 'We will now design the architecture of our model in order to learn our embeddings.
    Here, our model takes an input of four words (two before our target word and two
    after) and trains it against an output (our target word). The following representation
    is an illustration of how this might look:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将设计我们模型的架构，以便学习我们的嵌入。在这里，我们的模型输入四个单词（目标单词之前两个和之后两个），并将其与输出（我们的目标单词）进行训练。以下是这个过程可能看起来的一个示例：
- en: '![Figure 3.9 – CBOW architecture'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – CBOW 架构'
- en: '](img/B12365_03_9.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_9.jpg)'
- en: Figure 3.9 – CBOW architecture
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – CBOW 架构
- en: Our input words are first fed through an embedding layer, represented as a tensor
    of size (n,l), where n is the specified length of our embeddings and l is the
    number of words in our corpus. This is because every word within the corpus has
    its own unique tensor representation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入单词首先通过一个嵌入层进行处理，表示为大小为 (n,l) 的张量，其中 n 是我们嵌入的指定长度，l 是语料库中的单词数。这是因为语料库中的每个单词都有其独特的张量表示。
- en: Using our combined (summed) embeddings from our four context words, this is
    then fed into a fully connected layer in order to learn the final classification
    of our target word against our embedded representation of our context words. Note
    that our predicted/target word is encoded as a vector that's the length of our
    corpus. This is because our model effectively predicts the probability of each
    word in the corpus to be the target word, and the final classification is the
    one with the highest probability. We then obtain a loss, backpropagate this through
    our network, and update the parameters on the fully connected layer, as well as
    the embeddings themselves.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们四个上下文单词的组合（求和）嵌入，然后将其馈送到全连接层，以便学习目标单词的最终分类，根据我们上下文单词的嵌入表示。请注意，我们预测的/目标单词被编码为与我们语料库长度相同的向量。这是因为我们的模型有效地预测每个单词成为目标单词的概率，而最终分类是具有最高概率的那个单词。然后，我们计算损失，通过网络反向传播，更新全连接层的参数以及嵌入本身。
- en: 'The reason this methodology works is because our learned embeddings represent
    semantic similarity. Let''s say we train our model on the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效的原因是，我们学习到的嵌入表示语义相似性。假设我们在以下内容上训练我们的模型：
- en: '*X = ["is", "a", "learning", "framework"]; y = "deep"*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*X = ["is", "a", "learning", "framework"]; y = "deep"*'
- en: What our model is essentially learning is that the combined embedding representation
    of our target words is semantically similar to our target word. If we repeat this
    over a large enough corpus of words, we will find that our word embeddings begin
    to resemble our previously seen GLoVe embeddings, where semantically similar words
    appear to one another within the embedding space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型本质上学习的是，目标单词的组合嵌入表示在语义上与我们的目标单词相似。如果我们在足够大的单词语料库上重复这个过程，我们会发现我们的单词嵌入开始类似于我们之前见过的
    GLoVe 嵌入，即语义相似的单词在嵌入空间中彼此接近。
- en: Building CBOW
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 CBOW
- en: 'We will now run through building a CBOW model from scratch, thereby demonstrating
    how our embedding vectors can be learned:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将展示如何从头开始构建一个 CBOW 模型，从而演示如何学习我们的嵌入向量：
- en: 'We first define some text and perform some basic text cleaning, removing basic
    punctuation and converting it all into lowercase:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一些文本并执行一些基本的文本清理，删除基本的标点并将其全部转换为小写：
- en: '[PRE5]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We start by defining our corpus and its length:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义我们的语料库及其长度：
- en: '[PRE6]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that we use a set instead of a list as we are only concerned with the
    unique words within our text. We then build our corpus index and our inverse corpus
    index. Our corpus index will allow us to obtain the index of a word given the
    word itself, which will be useful when encoding our words for entry into our network.
    Our inverse corpus index allows us to obtain a word, given the index value, which
    will be used to convert our predictions back into words:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们使用集合而不是列表，因为我们只关注文本中的唯一单词。然后，我们构建我们的语料库索引和逆语料库索引。我们的语料库索引将允许我们获取给定单词本身时的单词索引，这在将我们的单词编码输入到我们的网络时将会很有用。我们的逆语料库索引允许我们根据索引值获取单词，这将用于将我们的预测转换回单词：
- en: '[PRE7]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we encode our data. We loop through our corpus and for each target word,
    we capture the context words (the two words before and the two words after). We
    append this with the target word itself to our dataset. Note how we begin this
    process from the third word in our corpus (index = `2`) and stop it two steps
    before the end of the corpus. This is because the two words at the beginning won''t
    have two words before them and, similarly, the two words at the end won''t have
    two words after them:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对数据进行编码。我们遍历我们的语料库，对于每个目标单词，我们捕获上下文单词（前两个单词和后两个单词）。我们将目标单词本身追加到我们的数据集中。请注意，我们从我们的语料库的第三个单词（索引=`2`）开始此过程，并在语料库末尾停止两步。这是因为开头的两个单词不会有两个单词在它们前面，类似地，结尾的两个单词也不会有两个单词在它们后面：
- en: '[PRE8]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.10 – Encoding the data'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.10 – 编码数据'
- en: '](img/B12365_03_10.jpg)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_10.jpg)'
- en: Figure 3.10 – Encoding the data
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.10 – 编码数据
- en: 'We then define the length of our embeddings. While this can technically be
    any number you wish, there are some tradeoffs to consider. While higher-dimensional
    embeddings can lead to a more detailed representation of the words, the feature
    space also becomes sparser, which means high-dimensional embeddings are only appropriate
    for large corpuses. Furthermore, larger embeddings mean more parameters to learn,
    so increasing the embedding size can increase training time significantly. We
    are only training on a very small dataset, so we have opted to use embeddings
    of size `20`:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们定义我们的嵌入长度。虽然这个长度在技术上可以是任意你想要的数字，但是有一些需要考虑的权衡。虽然更高维度的嵌入可以导致单词更详细的表示，但特征空间也会变得更稀疏，这意味着高维度的嵌入只适用于大型语料库。此外，更大的嵌入意味着更多的参数需要学习，因此增加嵌入大小可能会显著增加训练时间。由于我们只在一个非常小的数据集上进行训练，因此我们选择使用大小为`20`的嵌入：
- en: '[PRE9]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, we define our `CBOW` model in PyTorch. We define our embeddings layer
    so that it takes a vector of corpus length in and outputs a single embedding.
    We define our linear layer as a fully connected layer that takes an embedding
    in and outputs a vector of `64`. We define our final layer as a classification
    layer that is the same length as our text corpus.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们在PyTorch中定义我们的`CBOW`模型。我们定义我们的嵌入层，以便它接受一个语料库长度的向量并输出一个单一的嵌入。我们将我们的线性层定义为一个全连接层，它接受一个嵌入并输出一个`64`维的向量。我们将我们的最终层定义为一个与我们的文本语料库长度相同的分类层。
- en: 'We define our forward pass by obtaining and summing the embeddings for all
    input context words. This then passes through the fully connected layer with ReLU
    activation functions and finally into the classification layer, which predicts
    which word in the corpus corresponds to the summed embeddings of the context words
    the most:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过获取和汇总所有输入上下文单词的嵌入来定义我们的前向传播。然后，这些嵌入通过具有ReLU激活函数的全连接层，并最终进入分类层，该层预测在语料库中哪个单词与上下文单词的汇总嵌入最匹配：
- en: '[PRE10]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also define a `get_word_embedding()` function, which will allow us to
    extract embeddings for a given word after our model has been trained:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以定义一个`get_word_embedding()`函数，这将允许我们在模型训练后提取给定单词的嵌入：
- en: '[PRE11]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we are ready to train our model. We first create an instance of our model
    and define the loss function and optimizer:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备训练我们的模型。我们首先创建我们模型的一个实例，并定义损失函数和优化器：
- en: '[PRE12]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then create a helper function that takes our input context words, gets the
    word indexes for each of these, and transforms them into a tensor of length 4,
    which forms the input to our neural network:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个帮助函数，它接受我们的输入上下文词，并为每个词获取单词索引，并将它们转换为长度为 4 的张量，这将成为我们神经网络的输入：
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.11 – Tensor value'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.11 – 张量值'
- en: '](img/B12365_03_11.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_11.jpg)'
- en: Figure 3.11 – Tensor value
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.11 – 张量值
- en: 'Now, we train our network. We loop through 100 epochs and for each pass, we
    loop through all our context words, that is, target word pairs. For each of these
    pairs, we load the context sentence using `make_sentence_vector()` and use our
    current model state to obtain predictions. We evaluate these predictions against
    our actual target in order to obtain our loss. We backpropagate to calculate the
    gradients and step through our optimizer to update the weights. Finally, we sum
    all our losses for the epoch and print this out. Here, we can see that our loss
    is decreasing, showing that our model is learning:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开始训练我们的网络。我们循环执行 100 个 epochs，每次通过所有上下文词（即目标词对）。对于每一个这样的对，我们使用 `make_sentence_vector()`
    加载上下文句子，并使用当前模型状态进行预测。我们将这些预测与实际目标进行评估，以获得损失。我们进行反向传播以计算梯度，并通过优化器更新权重。最后，我们将整个
    epoch 的所有损失求和并打印出来。在这里，我们可以看到我们的损失正在减少，显示出我们的模型正在学习：
- en: '[PRE14]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.12 – Training our network'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.12 – 训练我们的网络'
- en: '](img/B12365_03_12.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_12.jpg)'
- en: Figure 3.12 – Training our network
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.12 – 训练我们的网络
- en: Now that our model has been trained, we can make predictions. We define a couple
    of functions to allow us to do so. `get_predicted_result()` returns the predicted
    word from the array of predictions, while our `predict_sentence()` function makes
    a prediction based on the context words.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们的模型已经训练好了，我们可以进行预测了。我们定义了几个函数来实现这一点。`get_predicted_result()` 从预测数组中返回预测的单词，而我们的
    `predict_sentence()` 函数则基于上下文词进行预测。
- en: 'We split our sentences into individual words and transform them into an input
    vector. We then create our prediction array by feeding this into our model and
    get our final predicted word by using the `get_predicted_result()` function. We
    also print the two words before and after the predicted target word for context.
    We can run a couple of predictions to validate our model is working correctly:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将我们的句子拆分为单词，并将它们转换为输入向量。然后，通过将其输入模型并使用 `get_predicted_result()` 函数，我们创建我们的预测数组，并通过使用上下文获得我们最终预测的单词。我们还打印预测目标单词前后的两个单词以提供上下文。我们可以运行一些预测来验证我们的模型是否工作正常：
- en: '[PRE15]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in the following output:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.13 – Predicted values'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.13 – 预测值'
- en: '](img/B12365_03_13.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_13.jpg)'
- en: Figure 3.13 – Predicted values
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.13 – 预测值
- en: 'Now that we have a trained model, we are able to use the `get_word_embedding()`
    function in order to return the 20 dimensions word embedding for any word in our
    corpus. If we needed our embeddings for another NLP task, we could actually extract
    the weights from the whole embedding layer and use this in our new model:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练好的模型，我们可以使用 `get_word_embedding()` 函数来返回语料库中任何单词的 20 维词嵌入。如果我们需要为另一个
    NLP 任务提取嵌入，我们实际上可以从整个嵌入层提取权重，并在我们的新模型中使用它们：
- en: '[PRE16]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.14 – Tensor value after editing the model'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14 – 编辑模型后的张量值'
- en: '](img/B12365_03_14.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_14.jpg)'
- en: Figure 3.14 – Tensor value after editing the model
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – 编辑模型后的张量值
- en: Here, we have demonstrated how to train a CBOW model for creating word embeddings.
    In reality, to create reliable embeddings for a corpus, we would require a very
    large dataset to be able to truly capture the semantic relationship between all
    the words. Because of this, it may be preferable to use pre-trained embeddings
    such as GLoVe, which have been trained on a very large corpus of data, for your
    models, but there may be cases where it would be preferable to train a brand new
    set of embeddings from scratch; for example, when analyzing a corpus of data that
    doesn't resemble normal NLP (for example, Twitter data where users may speak in
    short abbreviations and not use full sentences).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何训练CBOW模型来创建词嵌入。实际上，要为语料库创建可靠的嵌入，我们需要一个非常大的数据集，才能真正捕捉所有单词之间的语义关系。因此，使用已经在非常大的数据语料库上训练过的预训练嵌入（如GLoVe）可能更可取，但也可能存在某些情况，例如分析不符合正常自然语言处理的数据语料库时（例如，用户可能使用简短缩写而不是完整句子的Twitter数据），最好从头开始训练全新的嵌入。
- en: Exploring n-grams
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索n-gram
- en: 'In our CBOW model, we successfully showed that the meaning of the words is
    related to the context of the words around it. It is not only our context words
    that influence the meaning of words in a sentence, but the order of those words
    as well. Consider the following sentences:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的CBOW模型中，我们成功地展示了单词的意义与其周围上下文的关系。不仅是上下文单词影响了句子中单词的含义，而且单词的顺序也很重要。考虑以下句子：
- en: '*The cat sat on the dog*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*猫坐在狗上*'
- en: '*The dog sat on the cat*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*狗坐在猫上*'
- en: If you were to transform these two sentences into a bag-of-words representation,
    we would see that they are identical. However, by reading the sentences, we know
    they have completely different meanings (in fact, they are the complete opposite!).
    This clearly demonstrates that the meaning of a sentence is not just the words
    it contains, but the order in which they occur. One simple way of attempting to
    capture the order of words within a sentence is by using n-grams.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这两个句子转换成词袋表示法，我们会发现它们是相同的。然而，通过阅读句子，我们知道它们有完全不同的含义（事实上，它们是完全相反的！）。这清楚地表明，一个句子的含义不仅仅是它包含的单词，而是它们出现的顺序。试图捕捉句子中单词顺序的一种简单方法是使用n-gram。
- en: 'If we perform a count on our sentences, but instead of counting individual
    words, we now count the distinct two-word pairings that occur within the sentences,
    this is known as using **bi-grams**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对句子进行计数，但不是计算单个单词，而是计算句子内出现的不同的两个词组，这被称为使用**二元组**：
- en: '![Figure 3.15 – Tabular representation of bi-grams'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.15 – 二元组的表格表示'
- en: '](img/B12365_03_15.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_15.jpg)'
- en: Figure 3.15 – Tabular representation of bi-grams
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 – 二元组的表格表示
- en: 'We can represent this as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下表示这一点：
- en: '*The cat sat on the dog -> [1,1,1,0,1,1]*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*猫坐在狗上 -> [1,1,1,0,1,1]*'
- en: '*The dog sat on the cat -> [1,1,0,1,1,1]*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*狗坐在猫上 -> [1,1,0,1,1,1]*'
- en: These pairs of words attempt to capture the order the words appear in within
    a sentence, not just their frequency. Our first sentence contains the bi-gram
    *cat sat*, while the other one contains *dog sat*. These bigrams clearly help
    add much more context to our sentence than just using raw word counts.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单词对试图捕捉单词在句子中出现的顺序，而不仅仅是它们的频率。我们的第一个句子包含二元组*猫坐*，而另一个句子包含*狗坐*。这些二元组显然比仅使用原始词频更能为我们的句子增加更多上下文。
- en: We are not limited to pairs of words. We can also look at distinct word triplets,
    known as **trigrams**, or indeed any distinct number of words. We can use n-grams
    as inputs into our deep learning models instead of just a singular word, but when
    using n-gram models, it is worth noting that your feature space can become very
    large very quickly and may make machine learning very slow. If a dictionary contains
    all the words in the English language, a dictionary containing all distinct pairs
    of words would be several orders of magnitude larger!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于单词对。我们还可以看不同的三个单词组成的三元组，称为**三元组**，或者任何不同数量的单词组。我们可以将n-gram作为深度学习模型的输入，而不仅仅是一个单词，但是当使用n-gram模型时，值得注意的是，您的特征空间可能会迅速变得非常大，并且可能会使机器学习变得非常缓慢。如果字典包含英语中所有单词，那么包含所有不同的单词对的字典将大几个数量级！
- en: N-gram language modeling
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n-gram语言建模
- en: One thing that n-grams help us do is understand how natural language is formed.
    If we think of a language as being represented by parts of smaller word pairs
    (bigrams) instead of single words, we can begin to model language as a probabilistic
    model where the probability that a word appears in a sentence depends on the words
    that appeared before it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram帮助我们理解自然语言是如何形成的一件事。如果我们将语言表示为较小单词对（二元组）的部分，而不是单个单词，我们可以开始将语言建模为一个概率模型，其中单词出现在句子中的概率取决于它之前出现的单词。
- en: 'In a **unigram** model, we assume that all the words have a finite probability
    of appearing based on the distribution of the words in a corpus or document. Let''s
    take a document consisting of one sentence:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在**一元模型**中，我们假设所有单词都有出现的有限概率，基于语料库或文档中单词的分布。让我们以一个只包含一句话的文档为例：
- en: '*My name is my name*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*My name is my name*'
- en: 'Based on this sentence, we can generate a distribution of words whereby each
    word has a given probability of occurring based on its frequency within the document:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个句子，我们可以生成单词的分布，其中每个单词出现的概率取决于它在文档中的频率：
- en: '![Figure 3.16 – Tabular representation of a unigram'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.16 – 一元表达的表现'
- en: '](img/B12365_03_16.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_16.jpg)'
- en: Figure 3.16 – Tabular representation of a unigram
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 一元表达的表现
- en: 'We could then draw words randomly from this distribution in order to generate
    new sentences:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以从这个分布中随机抽取单词以生成新的句子：
- en: '*Name is Name my my*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*Name is Name my my*'
- en: But as we can see, this sentence doesn't make any sense, illustrating the problems
    of using a unigram model. Because the probability of each word occurring is independent
    of all the other words in the sentence, there is no consideration given to the
    order or context of the words appearing. This is where n-gram models are useful.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但是正如我们所见，这个句子毫无意义，说明了使用一元模型的问题。因为每个单词出现的概率独立于句子中所有其他单词，所以对单词出现的顺序或上下文没有考虑。这就是n-gram模型有用的地方。
- en: 'We will now consider using a **bigram** language model. This calculation takes
    the probability of a word occurring, given the word that appears before it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将考虑使用**二元语言模型**。这种计算考虑到一个单词出现的概率，给定它前面出现的单词：
- en: '![](img/Formula_03_002.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_002.png)'
- en: 'This means that the probability of a word occurring, given the previous word,
    is the probability of the word n-gram occurring divided by the probability of
    the previous word occurring. Let''s say we are trying to predict the next word
    in the following sentence:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一个单词出现的概率，给定前一个单词的概率，就是单词n-gram出现的概率除以前一个单词出现的概率。假设我们试图预测以下句子中的下一个单词：
- en: '*My favourite language is ___*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*My favourite language is ___*'
- en: 'Along with this, we''re given the following n-gram and word probabilities:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们还给出了以下n-gram和单词概率：
- en: '![Figure 3.17 – Tabular representation of the probabilities'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.17 – 概率的表格表示'
- en: '](img/B12365_03_17.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_17.jpg)'
- en: Figure 3.17 – Tabular representation of the probabilities
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 概率的表格表示
- en: With this, we could calculate the probability of Python occurring, given the
    probability of the previous word *is* occurring is only 20%, whereas the probability
    of *English* occurring is only 10%. We could expand this model further to use
    a trigram or any n-gram representation of words as we deem appropriate. We have
    demonstrated that n-gram language modeling can be used to introduce further information
    about word's relationships to one another into our models, rather than naively
    assuming that words are independently distributed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 借此，我们可以计算出Python出现的概率，假设前一个单词*is*出现的概率仅为20%，而*English*出现的概率仅为10%。我们可以进一步扩展这个模型，使用三元组或任何我们认为适当的n-gram单词表示。我们已经证明了n-gram语言建模可以用来引入关于单词彼此关系的更多信息到我们的模型中，而不是天真地假设单词是独立分布的。
- en: Tokenization
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: 'Next, we will learn about tokenization for NLP, a way of pre-processing text
    for entry into our models. Tokenization splits our sentences up into smaller parts.
    This could involve splitting a sentence up into its individual words or splitting
    a whole document up into individual sentences. This is an essential pre-processing
    step for NLP that can be done fairly simply in Python:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习用于NLP的标记化，这是对文本进行预处理以输入到我们的模型中的一种方式。标记化将我们的句子分解成更小的部分。这可能涉及将句子分割成单独的单词，或将整个文档分割成单独的句子。这是NLP的一个基本预处理步骤，可以在Python中相对简单地完成：
- en: 'We first take a basic sentence and split this up into individual words using
    the **word tokenizer** in NLTK:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用NLTK中的**词分词器**将基本句子分割成单独的单词：
- en: '[PRE17]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following output:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.18 – Splitting the sentence'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.18 – 分割句子'
- en: '](img/B12365_03_18.jpg)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_18.jpg)'
- en: Figure 3.18 – Splitting the sentence
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.18 – 分割句子
- en: 'Note how a period (`.`) is considered a token as it is a part of natural language.
    Depending on what we want to do with the text, we may wish to keep or dispose
    of the punctuation:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意句号（`.`）如何被视为一个标记，因为它是自然语言的一部分。根据我们想要对文本进行的处理，我们可能希望保留或丢弃标点符号：
- en: '[PRE18]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.19 – Removing the punctuation'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.19 – 移除标点符号'
- en: '](img/B12365_03_19.jpg)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_19.jpg)'
- en: Figure 3.19 – Removing the punctuation
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.19 – 移除标点符号
- en: 'We can also tokenize documents into individual sentences using the **sentence**
    **tokenizer**:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用**句子分词器**将文档分割成单独的句子：
- en: '[PRE19]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in the following output:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.20 – Splitting multiple sentences into single sentences'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.20 – 将多个句子分割为单个句子'
- en: '](img/B12365_03_20.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_20.jpg)'
- en: Figure 3.20 – Splitting multiple sentences into single sentences
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.20 – 将多个句子分割成单个句子
- en: 'Alternatively, we can combine the two to split into individual sentences of
    words:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以将两者结合起来，将其分割成单词的单个句子：
- en: '[PRE20]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This results in the following output:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.21 – Splitting multiple sentences into words'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.21 – 将多个句子分割成单词'
- en: '](img/B12365_03_21.jpg)'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_21.jpg)'
- en: Figure 3.21 – Splitting multiple sentences into words
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.21 – 将多个句子分割成单词
- en: 'One other optional step in the process of tokenization, which is the removal
    of stopwords. Stopwords are very common words that do not contribute to the overall
    meaning of a sentence. These include words such as *a*, `I`, and `or`. We can
    print a complete list from NLTK using the following code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词过程中的另一个可选步骤是移除停用词。停用词是一些非常常见的词，它们不会对句子的整体含义做出贡献。这些词包括像*a*、`I`和`or`等词。我们可以使用以下代码从NLTK中打印出完整的停用词列表：
- en: '[PRE21]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following output:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.22 – Displaying stopwords'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.22 – 显示停用词'
- en: '](img/B12365_03_22.jpg)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_22.jpg)'
- en: Figure 3.22 – Displaying stopwords
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.22 – 显示停用词
- en: 'We can easily remove these stopwords from our words using basic list comprehension:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用基本的列表推导来轻松地从我们的单词中移除这些停用词：
- en: '[PRE22]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This results in the following output:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.23 – Removing stopwords'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.23 – 移除停用词'
- en: '](img/B12365_03_23.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_23.jpg)'
- en: Figure 3.23 – Removing stopwords
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 – 移除停用词
- en: While some NLP tasks (such as predicting the next word in the sentence) require
    stopwords, others (such as judging the sentiment of a film review) do not as the
    stopwords do not contribute much toward the overall meaning of the document. Removing
    stopwords may be preferable in these circumstances as the frequency of these common
    words means they can increase our feature space unnecessarily, which will increase
    the time it takes for our models to train.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然某些NLP任务（如预测句子中的下一个单词）需要使用停用词，而其他任务（如判断电影评论的情感）则不需要，因为停用词对文档的整体含义贡献不大。在这些情况下，移除停用词可能更为可取，因为这些常见词的频率意味着它们可能会不必要地增加我们的特征空间，这会增加模型训练的时间。
- en: Tagging and chunking for parts of speech
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注和块句法分析
- en: 'So far, we have covered several approaches for representing words and sentences,
    including bag-of-words, embeddings, and n-grams. However, these representations
    fail to capture the structure of any given sentence. Within natural language,
    different words can have different functions within a sentence. Consider the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了几种表示单词和句子的方法，包括词袋模型、嵌入和n-gram。然而，这些表示方法未能捕捉到任何给定句子的结构。在自然语言中，不同的单词在句子中可能具有不同的功能。考虑以下例子：
- en: '*The big dog is sleeping on the bed*'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*大狗正在床上睡觉*'
- en: 'We can "tag" the various words of this text, depending on the function of each
    word in the sentence. So, the preceding sentence becomes as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以“标记”文本中的各个单词，具体取决于每个单词在句子中的功能。因此，前述句子变成如下所示：
- en: '*The -> big -> dog -> is -> sleeping -> on -> the -> bed*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*The -> big -> dog -> is -> sleeping -> on -> the -> bed*'
- en: '*Determiner -> Adjective -> Noun -> Verb -> Verb -> Preposition -> Determiner->
    Noun*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*限定词 -> 形容词 -> 名词 -> 动词 -> 动词 -> 介词 -> 限定词 -> 名词*'
- en: 'These **parts of speech** include, but are not limited to, the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些**词性**包括但不限于以下内容：
- en: '![Figure 3.24 – Parts of speech'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24 – 词性'
- en: '](img/B12365_03_24.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_24.jpg)'
- en: Figure 3.24 – Parts of speech
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 词性
- en: These different parts of speech can be used to better understand the structure
    of sentences. For example, adjectives often precede nouns in English. We can use
    these parts of speech and their relationships to one another in our models. For
    example, if we are predicting the next word in the sentence and the context word
    is an adjective, we know the probability of the next word being a noun is high.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的词性可以帮助我们更好地理解句子的结构。例如，形容词在英语中通常在名词前面。我们可以在模型中使用这些词性及其彼此之间的关系。例如，如果我们正在预测句子中的下一个词，而上下文词是形容词，我们就知道下一个词很可能是名词。
- en: Tagging
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记
- en: 'Part of speech **tagging** is the act of assigning these part of speech tags
    to the various words within the sentence. Fortunately, NTLK has a built-in tagging
    functionality, so there is no need for us to train our own classifier to be able
    to do so:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 词性**标注**是将这些词性标签分配给句子中各个单词的行为。幸运的是，NLTK具有内置的标注功能，因此我们无需训练自己的分类器即可执行此操作：
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 3.25 – Classifying parts of speech'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25 – 词性分类'
- en: '](img/B12365_03_25.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_25.jpg)'
- en: Figure 3.25 – Classifying parts of speech
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – 词性分类
- en: 'Here, we simply tokenize our text and call the `pos_tag()` function to tag
    each of the words in the sentence. This returns a tag for each of the words in
    the sentence. We can decode the meaning of this tag by calling `upenn_tagset()`
    on the code. In this case, we can see that "`VBG`" corresponds to a verb:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只需对文本进行分词，并调用`pos_tag()`函数对句子中的每个单词进行标记。这将返回每个单词的标记。我们可以通过在代码中调用`upenn_tagset()`来解码该标记的含义。在这种情况下，我们可以看到"`VBG`"对应于动词：
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 3.26 – Explanation of VBG'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.26 – VBG 的解释'
- en: '](img/B12365_03_26.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_26.jpg)'
- en: Figure 3.26 – Explanation of VBG
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – VBG 的解释
- en: 'Using pre-trained part of speech taggers is beneficial because they don''t
    just act as a dictionary that looks up the individual words in the sentence; they
    also use the context of the word within the sentence to allocate its meaning.
    Consider the following sentences:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的词性标注工具是有益的，因为它们不仅充当查找句子中各个单词的字典，还利用单词在句子中的上下文来确定其含义。考虑以下句子：
- en: '*He drinks the water*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*他喝水*'
- en: '*I will buy us some drinks*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*我会给我们买一些饮料*'
- en: The word *drinks* within these sentences represents two different parts of speech.
    In the first sentence, *drinks* refers to the verb; the present tense of the verb
    *to drink.* In the second sentence, *drinks* refers to the noun; the plural of
    a singular *drink*. Our pre-trained tagger is able to determine the context of
    these individual words and perform accurate part of speech tagging.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些句子中，单词*drinks*代表两种不同的词性。在第一个句子中，*drinks*指的是动词；动词*drink*的现在时。在第二个句子中，*drinks*指的是名词；单数*drink*的复数形式。我们的预训练标注器能够确定这些单词的上下文并进行准确的词性标注。
- en: Chunking
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切块
- en: '**Chunking** expands upon our initial parts of speech tagging and aims to structure
    our sentences in small chunks, where each of these chunks represent a small part
    of speech.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**切块**扩展了我们最初的词性标注，旨在将句子结构化成小块，其中每个块代表一个小词性。'
- en: 'We may wish to split our text up into **entities**, where each entity is a
    separate object or thing. For example, *the red book* refers not to three separate
    entities, but to a single entity described by three words. We can easily implement
    chunking using NLTK again. We must first define a grammar pattern to match using
    regular expressions. The pattern in question looks for **noun phrases** (**NP**),
    where a noun phrase is defined as a **determiner** (**DT**), followed by an **optional
    adjective** (**JJ**), followed by a **noun** (**NN**):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望将文本分成**实体**，其中每个实体是一个单独的对象或物品。例如，*红色的书*不是指三个单独的实体，而是指由三个单词描述的一个实体。我们可以再次使用NLTK轻松实现切块。我们首先必须定义一个使用正则表达式匹配的语法模式。所考虑的模式查找**名词短语**（**NP**），其中名词短语被定义为**限定词**（**DT**），后跟**可选形容词**（**JJ**），再跟一个**名词**（**NN**）：
- en: '[PRE25]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using the `RegexpParser()` function, we can match occurrences of this expression
    and tag them as noun phrases. We are then able to print the resulting tree, showing
    the tagged phrases. In our example sentence, we can see that *the big dog* and
    *the bed* are tagged as two separate noun phrases. We are able to match any chunk
    of text that we define using the regular expression as we see fit:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RegexpParser()`函数，我们可以匹配此表达式的出现并将其标记为名词短语。然后，我们能够打印出生成的树，显示标记的短语。在我们的例句中，我们可以看到*大狗*和*床*被标记为两个单独的名词短语。我们能够根据需要使用正则表达式定义任何文本块进行匹配：
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This results in the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.27 – Tree representation'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.27 – 树表示'
- en: '](img/B12365_03_27.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_27.jpg)'
- en: Figure 3.27 – Tree representation
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 – 树表示
- en: TF-IDF
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: '**TF-IDF** is yet another technique we can learn about to better represent
    natural language. It is often used in text mining and information retrieval to
    match documents based on search terms, but can also be used in combination with
    embeddings to better represent sentences in embedding form. Let''s take the following
    phrase:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF**是另一种我们可以学习的技术，用于更好地表示自然语言。它经常用于文本挖掘和信息检索，以基于搜索术语匹配文档，但也可以与嵌入结合使用，以更好地以嵌入形式表示句子。让我们看看以下短语：'
- en: '*This is a small giraffe*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一只小长颈鹿*'
- en: 'Let''s say we want a single embedding to represent the meaning of this sentence.
    One thing we could do is simply average the individual embeddings of each of the
    five words in this sentence:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要一个单一的嵌入来表示这个句子的含义。我们可以做的一件事是简单地平均每个单词的个体嵌入：
- en: '![Figure 3.28 – Word embeddings'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.28 – 词嵌入'
- en: '](img/B12365_03_28.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_28.jpg)'
- en: Figure 3.28 – Word embeddings
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28 – 词嵌入
- en: However, this methodology assigns equal weight to all the words in the sentence.
    Do you think that all the words contribute equally to the meaning of the sentence?
    **This** and **a** are very common words in the English language, but **giraffe**
    is very rarely seen. Therefore, we might want to assign more weight to the rarer
    words. This methodology is known as **Term Frequency – Inverse Document Frequency**
    (**TD-IDF**). We will now demonstrate how we can calculate TF-IDF weightings for
    our documents.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法将所有单词分配相等的权重。您认为所有单词对句子的含义贡献相等吗？**This**和**a**是英语中非常常见的词，但**giraffe**非常少见。因此，我们可能希望给较稀有的单词分配更高的权重。这种方法被称为**词频
    - 逆文档频率**（**TF-IDF**）。接下来我们将展示如何计算我们文档的TF-IDF权重。
- en: Calculating TF-IDF
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算TF-IDF
- en: 'As the name suggests, TF-IDF consists of two separate parts: term frequency
    and inverse document frequency. Term frequency is a document-specific measure
    counting the frequency of a given word within the document being analyzed:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，TF-IDF包括两个单独的部分：词频和逆文档频率。词频是一个文档特定的度量，计算在正在分析的文档中给定单词的频率：
- en: '![](img/Formula_03_003.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_003.png)'
- en: Note that we divide this measure by the total number of words in the document
    as a longer document is more likely to contain any given word. If a word appears
    many times in a document, it will receive a higher term frequency. However, this
    is the opposite of what we wish our TF-IDF weighting to do as we want to give
    a higher weight to occurrences of rare words within our document. This is where
    IDF comes into play.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将此度量值除以文档中的总字数，因为长文档更有可能包含任何给定单词。如果一个单词在文档中出现多次，它将获得较高的词频。然而，这与我们希望TF-IDF加权的相反，因为我们希望给予文档中稀有单词出现的更高权重。这就是IDF发挥作用的地方。
- en: 'Document frequency measures the number of documents within the entire corpus
    of documents where the word is being analyzed, and inverse document frequency
    calculates the ratio of the total documents to the document frequency:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 文档频率衡量的是在整个文档语料库中分析的单词出现的文档数量，而逆文档频率计算的是总文档数与文档频率的比率：
- en: '![](img/Formula_03_004.jpg)![](img/Formula_03_005.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_004.jpg)![](img/Formula_03_005.jpg)'
- en: 'If we have a corpus of 100 documents and our word appears five times across
    them, we will have an inverse document frequency of 20\. This means that a higher
    weight is given to words with lower occurrences across all documents. Now, consider
    a corpus of 100,000 documents. If a word appears just once, it will have an IDF
    of 100,000, whereas a word occurring twice would have an IDF of 50,000\. These
    very large and volatile IDFs aren''t ideal for our calculations, so we must first
    normalize them with logs. Note how we add 1 within our calculations to prevent
    division by 0 if we calculate TF-IDF for a word that doesn''t appear in our corpus:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个包含 100 个文档的语料库，并且我们的单词在这些文档中出现了五次，那么我们的逆文档频率将是 20。这意味着对于在所有文档中出现次数较少的单词，给予了更高的权重。现在，考虑一个包含
    100,000 个文档的语料库。如果一个单词只出现一次，它的 IDF 将是 100,000，而出现两次的单词的 IDF 将是 50,000。这些非常大且不稳定的
    IDF 对于我们的计算不是理想的，因此我们必须首先通过对数对其进行归一化。请注意，在我们的计算中添加 1 是为了防止在我们的语料库中计算 TF-IDF 时出现除以零的情况：
- en: '![](img/Formula_03_006.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_006.jpg)'
- en: 'This makes our final TF-IDF equation look as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们最终的 TF-IDF 方程如下所示：
- en: '![](img/Formula_03_007.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_007.jpg)'
- en: We can now demonstrate how to implement this in Python and apply TF-IDF weightings
    to our embeddings.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以演示如何在 Python 中实现这一点，并将 TF-IDF 权重应用于我们的嵌入向量。
- en: Implementing TF-IDF
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 TF-IDF
- en: 'Here, we will implement TF-IDF on a dataset using the Emma corpus from NLTK
    datasets. This dataset consists of a selection of sentences from the book *Emma*
    by Jane Austen, and we wish to calculate an embedded vector representation for
    each of these sentences:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 NLTK 数据集中的 Emma 语料库来实现 TF-IDF。该数据集由简·奥斯汀的书 *Emma* 中的若干句子组成，我们希望为每个句子计算嵌入向量表示：
- en: 'We start by importing our dataset and looping through each of the sentences,
    removing any punctuation and non-alphanumeric characters (such as astericks).
    We choose to leave stopwords in our dataset to demonstrate how TF-IDF accounts
    for these as these words appear in many documents and so have a very low IDF.
    We create a list of parsed sentences and a set of the distinct words in our corpus:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入我们的数据集，并循环遍历每个句子，删除任何标点符号和非字母数字字符（如星号）。我们选择保留数据集中的停用词，以演示 TF-IDF 如何考虑这些词，因为这些词出现在许多文档中，因此其
    IDF 非常低。我们创建一个解析后的句子列表和我们语料库中不同单词的集合：
- en: '[PRE27]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we create a function that will return our Term Frequencies for a given
    word in a given document. We take the length of the document to give us the number
    of words and count the occurrences of this word in the document before returning
    the ratio. Here, we can see that the word `ago` appears in the sentence once and
    that the sentence is 41 words long, giving us a Term Frequency of 0.024:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数，该函数将返回给定文档中给定单词的词频。我们获取文档的长度以获取单词数，并计算文档中该单词的出现次数，然后返回比率。在这里，我们可以看到单词
    `ago` 在句子中出现了一次，而该句子共有 41 个单词，因此我们得到了词频为 0.024：
- en: '[PRE28]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following output:'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.29 – TF-IDF score'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.29 – TF-IDF 分数'
- en: '](img/B12365_03_29.jpg)'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_29.jpg)'
- en: Figure 3.29 – TF-IDF score
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.29 – TF-IDF 分数
- en: 'Next, we calculate our Document Frequency. In order to do this efficiently,
    we first need to pre-compute a Document Frequency dictionary. This loops through
    all the data and counts the number of documents each word in our corpus appears
    in. We pre-compute this so we that do not have to perform this loop every time
    we wish to calculate Document Frequency for a given word:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算我们的文档频率。为了有效地做到这一点，我们首先需要预先计算一个文档频率字典。这会遍历所有数据，并计算我们语料库中每个单词出现在多少个文档中。我们预先计算这个字典，这样我们就不必每次想要计算给定单词的文档频率时都进行循环：
- en: '[PRE29]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we can see that the word `ago` appears within our document 32 times.
    Using this dictionary, we can very easily calculate our Inverse Document Frequency
    by dividing the total number of documents by our Document Frequency and taking
    the logarithm of this value. Note how we add one to the Document Frequency to
    avoid a divide by zero error when the word doesn''t appear in the corpus:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到单词 `ago` 在我们的文档中出现了 32 次。利用这个字典，我们可以非常容易地通过将文档总数除以文档频率并取其对数来计算逆文档频率。请注意，在文档频率为零时，我们将文档频率加一以避免除以零错误：
- en: '[PRE30]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we simply combine the Term Frequency and Inverse Document Frequency
    to get the TF-IDF weighting for each word/document pair:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们只需将词频和逆文档频率结合起来，即可获得每个单词/文档对的 TF-IDF 权重：
- en: '[PRE31]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This results in the following output:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.30 – TF-IDF score for ago and indistinct'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.30 – ago 和 indistinct 的 TF-IDF 分数'
- en: '](img/B12365_03_30.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_30.jpg)'
- en: Figure 3.30 – TF-IDF score for ago and indistinct
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.30 – ago 和 indistinct 的 TF-IDF 分数
- en: Here, we can see that although the words `ago` and `indistinct` appear only
    once in the given document, `indistinct` occurs less frequently throughout the
    whole corpus, meaning it receives a higher TF-IDF weighting.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管单词 `ago` 和 `indistinct` 在给定文档中只出现一次，但 `indistinct` 在整个语料库中出现的频率较低，意味着它获得了较高的
    TF-IDF 加权。
- en: Calculating TF-IDF weighted embeddings
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算 TF-IDF 加权嵌入
- en: 'Next, we can show how these TF-IDF weightings can be applied to embeddings:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以展示如何将这些 TF-IDF 加权应用到嵌入中：
- en: 'We first load our pre-computed GLoVe embeddings to provide the initial embedding
    representation of words in our corpus:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载我们预先计算的 GLoVe 嵌入，为我们语料库中单词提供初始的嵌入表示：
- en: '[PRE32]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then calculate an unweighted mean average of all the individual embeddings
    in our document to get a vector representation of the sentence as a whole. We
    simply loop through all the words in our document, extract the embedding from
    the GLoVe dictionary, and calculate the average over all these vectors:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接着计算文档中所有单个嵌入的无权平均值，以得到整个句子的向量表示。我们简单地遍历文档中的所有单词，从 GLoVe 字典中提取嵌入，并计算所有这些向量的平均值：
- en: '[PRE33]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following output:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.31 – Mean embedding'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.31 – 平均嵌入'
- en: '](img/B12365_03_31.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_31.jpg)'
- en: Figure 3.31 – Mean embedding
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.31 – 平均嵌入
- en: 'We repeat this process to calculate our TF-IDF weighted document vector, but
    this time, we multiply our vectors by their TF-IDF weighting before we average
    them:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重复这个过程来计算我们的 TF-IDF 加权文档向量，但这次，在我们对它们求平均之前，我们将我们的向量乘以它们的 TF-IDF 加权：
- en: '[PRE34]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This results in the following output:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.32 – TF-IDF embedding'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.32 – TF-IDF 嵌入'
- en: '](img/B12365_03_32.jpg)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_03_32.jpg)'
- en: Figure 3.32 – TF-IDF embedding
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.32 – TF-IDF 嵌入
- en: 'We can then compare the TF-IDF weighted embedding with our average embedding
    to see how similar they are. We can do this using cosine similarity, as follows:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以比较 TF-IDF 加权嵌入和我们的平均嵌入，看它们有多相似。我们可以使用余弦相似度来进行此操作，如下所示：
- en: '[PRE35]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This results in the following output:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 3.33 – Cosine similarity between TF-IDF and average embedding'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.33 – TF-IDF 和平均嵌入的余弦相似度'
- en: '](img/B12365_03_33.png)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_03_33.png)'
- en: Figure 3.33 – Cosine similarity between TF-IDF and average embedding
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.33 – TF-IDF 和平均嵌入的余弦相似度
- en: Here, we can see that our two different representations are very similar. Therefore,
    while using TF-IDF may not dramatically change our representation of a given sentence
    or document, it may weigh it in favor of words of interest, thus providing a more
    useful representation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的两种不同表示非常相似。因此，虽然使用 TF-IDF 可能不会显著改变我们对给定句子或文档的表示，但它可能会偏向于感兴趣的词语，从而提供更有用的表示。
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have taken a deeper dive into word embeddings and their
    applications. We have demonstrated how they can be trained using a continuous
    bag-of-words model and how we can incorporate n-gram language modeling to better
    understand the relationship between words in a sentence. We then looked at splitting
    documents into individual tokens for easy processing and how to use tagging and
    chunking to identify parts of speech. Finally, we showed how TF-IDF weightings
    can be used to better represent documents in embedding form.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了词嵌入及其应用。我们展示了如何使用连续词袋模型训练它们，并如何结合 n-gram 语言建模以更好地理解句子中词语之间的关系。然后，我们查看了如何将文档拆分为个别标记以便于处理，以及如何使用标记和块分析来识别词性。最后，我们展示了如何使用
    TF-IDF 权重来更好地表示文档的嵌入形式。
- en: In the next chapter, we will see how to use NLP for text preprocessing, stemming,
    and lemmatization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用 NLP 进行文本预处理、词干化和词形还原。
