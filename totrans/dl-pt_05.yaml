- en: Deep Learning for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习用于计算机视觉
- en: 'In [Chapter 3](1de63bb8-4140-4d2d-b93c-5b6bdc42ec03.xhtml), *Diving Deep into
    Neural Networks,* we built an image classifier using a popular **Convolutional
    Neural Network** (**CNN**) architecture called **ResNet**, but we used this model
    as a black box. In this chapter, we will cover the important building blocks of
    convolutional networks. Some of the important topics that we will be covering
    in this chapter are:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](1de63bb8-4140-4d2d-b93c-5b6bdc42ec03.xhtml)中，*深入探讨神经网络*，我们使用了一个名为**ResNet**的流行**卷积神经网络**（**CNN**）架构构建了一个图像分类器，但我们将这个模型当作一个黑盒子使用。在本章中，我们将介绍卷积网络的重要构建模块。本章中我们将涵盖的一些重要主题包括：
- en: Introduction to neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: Building a CNN model from scratch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头构建CNN模型
- en: Creating and exploring a VGG16 model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和探索VGG16模型
- en: Calculating pre-convoluted features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算预卷积特征
- en: Understanding what a CNN model learns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CNN模型学习的内容
- en: Visualizing weights of the CNN layer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化CNN层的权重
- en: We will explore how we can build an architecture from scratch for solving image
    classification problems, which are the most common use cases. We will also learn
    how to use transfer learning, which will help us in building image classifiers
    using a very small dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨如何从头开始构建架构来解决图像分类问题，这是最常见的用例。我们还将学习如何使用迁移学习，这将帮助我们使用非常小的数据集构建图像分类器。
- en: Apart from learning how to use CNNs, we will also explore what these convolutional
    networks learn.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习如何使用CNN，我们还将探索这些卷积网络学习了什么。
- en: Introduction to neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: 'In the last few years, CNNs have become popular in the areas of image recognition,
    object detection, segmentation, and many other tasks in the field of computer
    vision. They are also becoming popular in the field of **natural language processing**
    (**NLP**), though they are not commonly used yet. The fundamental difference between
    fully connected layers and convolution layers is the way the weights are connected
    to each other in the intermediate layers. Let''s take a look at an image where
    we depict how fully connected, or linear, layers work:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，CNN在计算机视觉领域的图像识别、物体检测、分割等任务中变得非常流行。它们也在**自然语言处理**（**NLP**）领域变得流行起来，尽管目前还不常用。完全连接层和卷积层之间的基本区别在于中间层中权重连接的方式。让我们看一下一幅图像，展示了完全连接或线性层的工作原理：
- en: '![](img/b11c5a02-626f-4e4c-a4be-d4404fd31f15.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b11c5a02-626f-4e4c-a4be-d4404fd31f15.png)'
- en: 'One of the biggest challenges of using a linear layer or fully connected layers
    for computer vision is that they lose all spatial information, and the complexity
    in terms of the number of weights used by fully connected layers is too big. For
    example, when we represent a 224 pixel image as a flat array, we would end up
    with 150, 528 (224 x 224 x 3 channels). When the image is flattened, we lose all
    the spatial information. Let''s look at what a simplified version of a CNN looks
    like:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中使用线性层或完全连接层的最大挑战之一是它们会丢失所有空间信息，而完全连接层在权重数量上的复杂性太大。例如，当我们将一个224像素的图像表示为一个平坦的数组时，我们最终会得到150,528个元素（224
    x 224 x 3个通道）。当图像被展平时，我们失去了所有的空间信息。让我们看看简化版CNN的样子：
- en: '![](img/612b9675-1581-47c6-8b47-b07d1e20eef2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/612b9675-1581-47c6-8b47-b07d1e20eef2.png)'
- en: 'All the convolution layer is doing is applying a window of weights called **filters**
    across the image. Before we try to understand convolutions and other building
    blocks in detail, let''s build a simple yet powerful image classifier for the
    `MNIST` dataset. Once we build this, we will walk through each component of the
    network. We will break down building our image classifier into the following steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有卷积层所做的就是在图像上应用称为**滤波器**的权重窗口。在我们试图深入理解卷积和其他构建模块之前，让我们为`MNIST`数据集构建一个简单而强大的图像分类器。一旦我们构建了这个模型，我们将逐步分析网络的每个组成部分。我们将将构建图像分类器分解为以下步骤：
- en: Getting data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Creating a validation dataset
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建验证数据集
- en: Building our CNN model from scratch
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头构建我们的CNN模型
- en: Training and validating the model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: MNIST – getting data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST – 获取数据
- en: 'The `MNIST` dataset contains 60,000 handwritten digits from 0 to 9 for training,
    and 10,000 images for a test set. The PyTorch `torchvision` library provides us
    with an `MNIST` dataset, which downloads the data and provides it in a readily-usable
    format. Let''s use the dataset `MNIST` function to pull the dataset to our local
    machine, and then wrap it around a `DataLoader`. We will use torchvision transformations
    to convert the data into PyTorch tensors and do data normalization. The following
    code takes care of downloading, wrapping around the `DataLoader` and normalizing
    the data:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`数据集包含了60000个手写数字（0到9）用于训练，以及10000张图片用作测试集。PyTorch的`torchvision`库为我们提供了一个`MNIST`数据集，它可以下载数据并以易于使用的格式提供。让我们使用`MNIST`函数将数据集拉到我们的本地机器上，然后将其包装在一个`DataLoader`中。我们将使用torchvision的变换来将数据转换为PyTorch张量，并进行数据归一化。以下代码负责下载数据、包装在`DataLoader`中并进行数据归一化：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So, the previous code provides us with a `DataLoader` for the `train` and `test`
    datasets. Let''s visualize a few images to get an understanding of what we are
    dealing with. The following code will help us in visualizing the MNIST images:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的代码为我们提供了`train`和`test`数据集的`DataLoader`。让我们可视化几个图像，以了解我们正在处理的内容。以下代码将帮助我们可视化MNIST图像：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we can pass the `plot_img` method to visualize our dataset. We will pull
    a batch of records from the `DataLoader` using the following code, and plot the
    images:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过`plot_img`方法来可视化我们的数据集。我们将使用以下代码从`DataLoader`中提取一批记录，并绘制这些图像：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The images are visualized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可视化如下所示：
- en: '![](img/66ba4835-005a-43e1-9a0c-283ed4b18c65.png)   ![](img/92b0f189-a014-4814-96d8-395e79fe00ce.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66ba4835-005a-43e1-9a0c-283ed4b18c65.png)   ![](img/92b0f189-a014-4814-96d8-395e79fe00ce.png)'
- en: Building a CNN model from scratch
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建CNN模型
- en: 'For this example, let''s build our own architecture from scratch. Our network
    architecture will contain a combination of different layers, namely:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们从头开始构建我们自己的架构。我们的网络架构将包含不同的层组合，具体来说是：
- en: '`Conv2d`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conv2d`'
- en: '`MaxPool2d`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxPool2d`'
- en: '**Rectified linear unit** (**ReLU**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）'
- en: View
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视图
- en: Linear layer
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性层
- en: 'Let''s look at a pictorial representation of the architecture we are going
    to implement:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们将要实现的架构的图示表示：
- en: '![](img/813aed09-39f2-46c2-bfd4-7ce8fd92227c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/813aed09-39f2-46c2-bfd4-7ce8fd92227c.png)'
- en: 'Let''s implement this architecture in PyTorch and then walk through what each
    individual layer does:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在PyTorch中实现这个架构，然后逐步分析每个单独的层的作用：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's understand in detail what each layer does.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每一层的作用。
- en: Conv2d
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Conv2d
- en: '`Conv2d` takes care of applying a convolutional filter on our MNIST images.
    Let''s try to understand how convolution is applied on a one-dimensional array,
    and then move to how a two-dimensional convolution is applied to an image. We
    will look at the following image, to which we will apply a `Conv1d` of a filter
    (or kernel) size `3` to a tensor of length `7`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv2d`负责在我们的MNIST图像上应用卷积滤波器。让我们尝试理解如何在一维数组上应用卷积，然后再转向如何在图像上应用二维卷积。我们将查看以下图像，我们将在长度为`7`的张量上应用一个滤波器（或内核）大小为`3`的`Conv1d`：'
- en: '![](img/e4495ddb-9bab-465e-ae9b-0d95e1834f13.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4495ddb-9bab-465e-ae9b-0d95e1834f13.png)'
- en: 'The bottom boxes represent our input tensor of seven values, and the connected
    boxes represent the output after we apply our convolution filter of size three.
    At the top-right corner of the image, the three boxes represent the weights and
    parameters of the `Conv1d` layer. The convolution filter is applied like a window
    and it moves to the next values by skipping one value. The number of values to
    be skipped is called the **stride,** and is set to `1` by default. Let''s understand
    how the output values are being calculated by writing down the calculation for
    the first and last outputs:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的方框表示我们的输入张量有七个值，连接的方框表示我们应用卷积滤波器后的输出，滤波器大小为三。在图像的右上角，三个方框表示`Conv1d`层的权重和参数。卷积滤波器像窗口一样应用，并通过跳过一个值移动到下一个值。要跳过的值的数量称为**步幅**，默认设置为`1`。让我们通过书写第一个和最后一个输出的计算来了解输出值是如何计算的：
- en: Output 1 –> (*-0.5209 x 0.2286*) + (*-0.0147 x 2.4488*) + (*-0.4281 x -0.9498*)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出1 –> (*-0.5209 x 0.2286*) + (*-0.0147 x 2.4488*) + (*-0.4281 x -0.9498*)
- en: Output 5 –> (*-0.5209 x -0.6791*) + (*-0.0147 x -0.6535*) + (*-0.4281 x 0.6437*)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出5 –> (*-0.5209 x -0.6791*) + (*-0.0147 x -0.6535*) + (*-0.4281 x 0.6437*)
- en: 'So, by now, it should be clear what a convolution does. It applies a filter
    (or kernel), which is a bunch of weights, on the input by moving it based on the
    value of the stride. In the previous  example, we are moving our filter one at
    a time. If the stride value is `2`, then we would move two points at a time. Let''s
    look at a PyTorch implementation to understand how it works:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，到目前为止，应该清楚卷积是做什么的了。它将一个滤波器（或内核），即一堆权重，应用在输入上，并根据步幅的值移动它。在前面的例子中，我们每次移动我们的滤波器一次。如果步幅值为
    `2`，那么我们将一次移动两个点。让我们看一个 PyTorch 实现来理解它是如何工作的：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There is another important parameter, called **padding,** which is often used
    with convolutions. If we keenly observe the previous example, we may realize that
    if the filter is not applied until the end of the data, when there are not enough
    elements for the data to stride, it stops. Padding prevents this by adding zeros
    to both ends of a tensor. Let''s again look at a one-dimensional example of how
    padding works:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个重要的参数，称为**填充**（padding），通常与卷积一起使用。如果我们仔细观察前面的例子，我们可能会意识到，如果在数据的末尾没有足够的元素供数据进行跨步时，滤波器将停止。填充通过在张量的两端添加零来防止这种情况。让我们再次看一个关于填充如何工作的一维示例：
- en: '![](img/17c7f3b3-c505-4b6d-ad7f-2f0981453a3e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![image](img/17c7f3b3-c505-4b6d-ad7f-2f0981453a3e.png)'
- en: In the preceding image, we applied a `Conv1d` layer with padding `2` and stride
    `1`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，我们应用了带有填充 `2` 和步幅 `1` 的 `Conv1d` 层。
- en: 'Let''s look at how Conv2d works on an image:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Conv2d 在图像上是如何工作的：
- en: Before we understand how Conv2d works, I would strongly recommend you to check
    the amazing blog ([http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/))
    where it contains a live demo of how a convolution works. After you have spent
    few minutes playing with the demo, read the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解 Conv2d 如何工作之前，我强烈建议你查看这篇精彩的博客（[http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/)），其中包含一个卷积演示的实时示例。在你花几分钟玩弄演示后，再阅读下一节。
- en: Let's understand what happened in the demo. In the center box of the image,
    we have two different sets of numbers; one represented in the boxes and the other
    beneath the boxes. The ones represented in the boxes are pixel values, as highlighted
    by the white box on the left-hand photo. The numbers denoted beneath the boxes
    are the filter (or kernel) values that are being used to sharpen the image. The
    numbers are handpicked to do a particular job. In this case, it is sharpening
    the image. Just like in our previous example, we are doing an element-to-element
    multiplication and summing up all the values to generate the value of the pixel
    in the right-hand image. The generated value is highlighted by the white box on
    the right-hand side of the image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解演示中发生了什么。在图像的中心框中，我们有两组不同的数字；一组在框中表示，另一组在框下面。框中表示的是像素值，如左手边的白色框所示。框下面标记的数字是用于锐化图像的滤波器（或内核）值。这些数字是手动选择的以执行特定的任务。在这种情况下，它是用来锐化图像的。就像在我们之前的例子中一样，我们进行元素对元素的乘法，并将所有值相加以生成右侧图像像素的值。生成的值由图像右侧的白色框突出显示。
- en: 'Though the values in the kernel are handpicked in this example, in CNNs we
    do not handpick the values, but rather we initialize them randomly and let the
    gradient descent and backpropagation tune the values of the kernels. The learned
    kernels will be responsible for identifying different features such as lines,
    curves, and eyes. Let''s look at another image where we look at it as a matrix
    of numbers and see how convolution works:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个例子中内核中的值是手动选择的，在 CNN 中我们不会手动选择这些值，而是随机初始化它们，并让梯度下降和反向传播调整内核的值。学习到的内核将负责识别不同的特征，如线条、曲线和眼睛。让我们看看另一张图像，我们把它看作是一个数字矩阵，并看看卷积是如何工作的：
- en: '![](img/e96abcad-3542-4a9e-9d85-92ae5770157a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![image](img/e96abcad-3542-4a9e-9d85-92ae5770157a.png)'
- en: 'In the preceding screenshot, we assume that the 6 x 6 matrix represents an
    image and we apply the convolution filter of size 3 x 3, then we show how the
    output is generated. To keep it simple, we are just calculating for the highlighted
    portion of the matrix. The output is generated by doing the following calculation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一张屏幕截图中，我们假设 6 x 6 矩阵表示一张图像，我们应用大小为 3 x 3 的卷积滤波器，然后展示生成输出的方式。为了简单起见，我们只计算矩阵的突出部分。输出是通过进行以下计算生成的：
- en: Output –> *0.86 x 0 + -0.92 x 0 + -0.61 x 1 + -0.32 x -1 + -1.69 x -1 + ........*
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 –> *0.86 x 0 + -0.92 x 0 + -0.61 x 1 + -0.32 x -1 + -1.69 x -1 + ........*
- en: The other important parameter used in the `Conv2d` function is `kernel_size`,
    which decides the size of the kernel. Some of the commonly used kernel sizes are
    *1*, *3*, *5*, and *7*. The larger the kernel size, the larger the area that a
    filter can cover becomes huge, so it is common to observe filters of *7* or *9*
    being applied to the input data in the early layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv2d`函数中使用的另一个重要参数是`kernel_size`，它决定了核的大小。一些常用的核大小包括*1*、*3*、*5*和*7*。核大小越大，滤波器能覆盖的区域就越大，因此在早期层中观察到使用*7*或*9*的滤波器是常见的。'
- en: Pooling
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: It is a common practice to add pooling layers after convolution layers, as they
    reduce the size of feature maps and the outcomes of convolution layers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层后添加池化层是一种常见做法，因为它们可以减小特征图的大小和卷积层的输出。
- en: 'Pooling offers two different features: one is reducing the size of data to
    process, and the other is forcing the algorithm to not focus on small changes
    in the position of an image. For example, a face-detecting algorithm should be
    able to detect a face in the picture, irrespective of the position of the face
    in the photo.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 池化提供两种不同的特性：一是减少要处理的数据大小，另一种是强制算法不要关注图像中位置的微小变化。例如，一个人脸检测算法应该能够在图片中检测到人脸，而不管人脸在照片中的位置如何。
- en: 'Let''s look at how MaxPool2d works. It also has the same concept of kernel
    size and strides. It differs from convolutions as it does not have any weights,
    and just acts on the data generated by each filter from the previous layer. If
    the kernel size is *2 x 2,* then it considers that size in the image and picks
    the max of that area. Let''s look at the following image, which will make it clear
    how MaxPool2d works:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看MaxPool2d是如何工作的。它也有与卷积相同的核大小和步长概念。它与卷积不同的地方在于它没有任何权重，只是作用于上一层每个滤波器生成的数据。如果核大小是*2
    x 2*，则它在图像中考虑该大小并选择该区域的最大值。让我们看一下接下来的图像，这将清楚地展示MaxPool2d的工作原理：
- en: '![](img/eeda68e5-57bb-4c9e-87f3-41da57355140.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeda68e5-57bb-4c9e-87f3-41da57355140.png)'
- en: 'The box on the left-hand side contains the values of feature maps. After applying
    max-pooling, the output is stored on the right-hand side of the box. Let''s look
    at how the output is calculated, by writing down the calculation for the values
    in the first row of the output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧框中包含特征图的值。应用最大池化后，输出存储在框的右侧。让我们看一下如何计算输出的第一行值的计算方式：
- en: '![](img/8f3e3104-a958-4656-958f-6c4f271a4836.png)![](img/8527235d-3368-410b-9830-5719fc95d708.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f3e3104-a958-4656-958f-6c4f271a4836.png)![](img/8527235d-3368-410b-9830-5719fc95d708.png)'
- en: 'The other commonly used pooling technique is **average pooling**. The `maximum`
    function is replaced by the `average` function. The following image explains how
    average pooling works:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的池化技术是**平均池化**。`maximum`函数被`average`函数替换。下图解释了平均池化的工作原理：
- en: '![](img/4bfbbbc6-3420-4fab-8ff7-787313489ecb.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bfbbbc6-3420-4fab-8ff7-787313489ecb.png)'
- en: 'In this example, instead of taking a maximum of four values, we are taking
    the average four values. Let''s write down the calculation to make it easier to
    understand:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们不是取四个值的最大值，而是取这四个值的平均值。让我们写下计算方式，以便更容易理解：
- en: '![](img/3fa94b1a-b075-4415-8484-5444b6cae1c4.png)![](img/564447b0-5d9f-432d-abe0-7bf68bbe1c54.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fa94b1a-b075-4415-8484-5444b6cae1c4.png)![](img/564447b0-5d9f-432d-abe0-7bf68bbe1c54.png)'
- en: Nonlinear activation – ReLU
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性激活 – ReLU
- en: 'It is a common and a best practice to have a nonlinear layer after max-pooling,
    or after convolution is applied. Most of the network architectures tend to use ReLU or
    different flavors of ReLU. Whatever nonlinear function we choose, it gets applied
    to each element of the feature maps. To make it more intuitive, let''s look at
    an example where we apply ReLU for the same feature map to which we applied max-pooling
    and average pooling:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用最大池化或卷积后，添加非线性层是一种常见且最佳的实践。大多数网络架构倾向于使用ReLU或不同变体的ReLU。无论我们选择哪种非线性函数，它都应用于特征图的每个元素。为了更直观地理解，让我们看一个例子，在这个例子中，我们对应用了最大池化和平均池化的相同特征图应用ReLU：
- en: '![](img/bd13c8fa-adf1-436f-b1ce-83027ebc2df4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd13c8fa-adf1-436f-b1ce-83027ebc2df4.png)'
- en: View
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视图
- en: 'It is a common practice to use a fully connected, or linear, layer at the end
    of most networks for an image classification problem. We are using a two-dimensional
    convolution that takes a matrix of numbers as input and outputs another matrix
    of numbers. To apply a linear layer, we need to flatten the matrix which is a
    tensor of two-dimensions to a vector of one-dimension. The following example will
    show you how `view` works:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网络在图像分类问题的最后都会使用全连接或线性层。我们使用二维卷积，它将一个数字矩阵作为输入并输出另一个数字矩阵。要应用线性层，我们需要将这个二维张量展平为一个一维向量。接下来的示例将展示`view`函数的工作方式：
- en: '![](img/b74f354c-6690-4b8a-96fc-fad0537ac50d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b74f354c-6690-4b8a-96fc-fad0537ac50d.png)'
- en: 'Let''s look at the code used in our network that does the same:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们网络中使用的代码，它执行相同的操作：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw earlier, the `view` method will flatten an *n*-dimension tensor to
    a one-dimensional tensor. In our network, the first dimension is of each image.
    The input data after batching will have a dimension of *32 x 1 x 28 x 28,* where
    the first number, *32,* will denote that there are *32* images of size *28* height,
    *28* width, and *1* channel since it is a black-and-white image. When we flatten,
    we do not want to flatten or mix the data for different images. So, the first
    argument that we pass to the `view` function will instruct PyTorch to avoid flattening
    the data on the first dimension. Let''s take a look at how this works in the following
    image:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，`view`方法将把一个*n*维张量展平为一个一维张量。在我们的网络中，第一维度是每个图像的尺寸。批处理后的输入数据将具有*32 x
    1 x 28 x 28*的维度，其中第一个数字*32*表示有*32*张大小为*28*高度、*28*宽度和*1*通道的图像，因为它是一张黑白图像。当我们展平时，我们不希望展平或混合不同图像的数据。因此，我们传递给`view`函数的第一个参数将指导PyTorch避免在第一维上展平数据。让我们看看这在下面的图像中是如何工作的：
- en: '![](img/f0a8f523-4a29-4ee8-8ef0-a10d54b777df.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0a8f523-4a29-4ee8-8ef0-a10d54b777df.png)'
- en: 'In the preceding example, we have data of size *2 x 1 x 2 x 2*; after we apply
    the `view` function, it converts to a tensor of size *2 x 1 x 4*. Let''s also
    look at another example where we don''t mention the *- 1*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们有大小为*2 x 1 x 2 x 2*的数据；在应用`view`函数后，它转换为大小为*2 x 1 x 4*的张量。让我们再看另一个例子，这次我们不提到*-
    1*：
- en: '![](img/73b8a6ca-1268-4c68-a0c2-8a27606a991e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73b8a6ca-1268-4c68-a0c2-8a27606a991e.png)'
- en: If we ever forget to mention which dimension to flatten, we may end up with
    unexpected results. So be extra careful at this step.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忘记提到要展平的维度，可能会导致意外的结果。所以在这一步要特别小心。
- en: Linear layer
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性层
- en: After we convert the data from a two-dimensional tensor to a one-dimensional
    tensor, we pass the data through a linear layer, followed by a nonlinear activation
    layer. In our architecture, we have two linear layers; one followed by ReLU, and
    the other followed by a `log_softmax`, which predicts what digit is contained
    in the given image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据从二维张量转换为一维张量后，我们通过一个线性层，然后是一个非线性激活层。在我们的架构中，我们有两个线性层；一个后面跟着ReLU，另一个后面跟着`log_softmax`，用于预测给定图像中包含的数字。
- en: Training the model
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Training the model is the same process as for our previous dogs and cats image
    classification problems. The following code snippet does the training of our model
    on the provided dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的过程与我们之前猫狗图像分类问题的过程相同。以下代码片段展示了如何在提供的数据集上训练我们的模型：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This method has different logic for `training` and `validation`. There are
    primarily two reasons for using different modes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在`training`和`validation`中有不同的逻辑。使用不同模式主要有两个原因：
- en: In `train` mode, dropout removes a percentage of values, which should not happen
    in the validation or testing phase
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`train`模式下，dropout会删除一定百分比的值，在验证或测试阶段不应该发生这种情况。
- en: For `training` mode, we calculate gradients and change the model's parameters
    value, but backpropagation is not required during the testing or validation phases
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`training`模式，我们计算梯度并改变模型的参数值，但在测试或验证阶段不需要反向传播。
- en: Most of the code in the previous function is self-explanatory, as discussed
    in previous chapters. At the end of the function, we return the `loss` and `accuracy`
    of the model for that particular epoch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个函数中的大部分代码是不言自明的，正如前几章中讨论的那样。在函数的最后，我们返回该特定时期模型的`loss`和`accuracy`。
- en: 'Let''s run the model through the preceding function for 20 iterations and plot
    the `loss` and `accuracy` of `train` and `validation`, to understand how our network
    performed. The following code runs the `fit` method for the `train` and `test`
    dataset for `20` iterations:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行模型通过前述函数进行20次迭代，并绘制`训练`和`验证`的`损失`和`准确率`，以了解我们的网络表现如何。以下代码运行`fit`方法用于`训练`和`测试`数据集，迭代`20`次：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code plots the `training` and `test loss`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制`训练`和`测试损失`：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code generates the following graph:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成如下图表：
- en: '![](img/9a1a319b-9b66-4479-a4d2-014ded3303c5.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a1a319b-9b66-4479-a4d2-014ded3303c5.png)'
- en: 'The following code plots the training and test accuracy:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制训练和测试的准确率：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates the following graph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成如下图表：
- en: '![](img/3a360060-04a5-41f8-8dcf-bffc2a6229c0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a360060-04a5-41f8-8dcf-bffc2a6229c0.png)'
- en: At the end of the 20^(th) epoch, we achieve a `test` accuracy of 98.9%. We have
    got our simple convolutional model working and almost achieving state-of-the-art
    results. Let's take a look at what happens when we try the same network architecture
    on the previously-used `Dogs vs. Cats` dataset. We will use the data from our
    previous chapter, [Chapter 3](5a68470a-8fdb-4dc8-9613-e38a9b4354d5.xhtml), *Building
    Blocks of Neural Networks,* and architecture from the MNIST example with some
    minor changes. Once we train the model, let's evaluate it to understand how well
    our simple architecture performs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第20个迭代周期结束时，我们实现了`测试`准确率达到了98.9%。我们的简单卷积模型已经可以工作，并且几乎达到了最先进的结果。让我们看看当我们尝试在之前使用的`狗与猫`数据集上使用相同网络架构时会发生什么。我们将使用我们前一章节中的数据，[第3章](5a68470a-8fdb-4dc8-9613-e38a9b4354d5.xhtml)，*神经网络的构建块*，以及来自MNIST示例的架构，并进行一些微小的更改。一旦我们训练模型，让我们评估它，以了解我们的简单架构的表现如何。
- en: Classifying dogs and cats – CNN from scratch
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独自从头构建的CNN对狗和猫进行分类
- en: 'We will use the same architecture with a few minor changes, as listed here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与少许更改的相同架构，如下所列：
- en: The input dimensions for the first linear layer changes, as the dimensions for
    our cats and dogs images are *256, 256*
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个线性层的输入维度发生变化，因为我们猫和狗图像的维度为*256, 256*
- en: We add another linear layer to give more flexibility for the model to learn
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们增加另一层线性层以提供模型更多的灵活性学习
- en: 'Let''s look at the code that implements the network architecture:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下实现网络架构的代码：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will use the same `training` function which was used for the MNIST example.
    So, I am not including the code here. But let's look at the plots generated when
    the model is trained for *20* iterations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与MNIST示例相同的`训练`函数。所以，我这里不包括代码。但让我们看一下在训练*20*次迭代时生成的图表。
- en: 'Loss of `training` and `validation` datasets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`训练`和`验证`数据集的损失：'
- en: '![](img/68c556c7-c801-401d-b600-3f07b15e9436.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68c556c7-c801-401d-b600-3f07b15e9436.png)'
- en: 'Accuracy for `training` and `validation` datasets:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`训练`和`验证`数据集的准确率：'
- en: '![](img/25496cf6-3032-4ba7-8a33-22d3e6202e79.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25496cf6-3032-4ba7-8a33-22d3e6202e79.png)'
- en: It's clear from the plots that the training loss is decreasing for every iteration,
    but the validation loss gets worse. Accuracy also increases during the training,
    but almost saturates at 75%. It is a clear example where the model is not generalizing.
    We will look at another technique called **transfer learning**, which helps us
    in training more accurate models, along with providing tricks to make the training
    faster.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表可以清楚地看出，训练损失在每次迭代中都在减少，但验证损失却变得更糟。准确率在训练过程中也在增加，但几乎在75%时饱和。这是一个明显的例子，显示模型没有泛化。我们将看一下另一种称为**迁移学习**的技术，它帮助我们训练更精确的模型，同时提供使训练更快的技巧。
- en: Classifying dogs and cats using transfer learning
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习对狗和猫进行分类
- en: Transfer learning is the ability to reuse a trained algorithm on a similar dataset
    without training it from scratch. We humans do not learn to recognize new images
    by analyzing thousands of similar images. We, as humans, just understand the different
    features that actually differentiate a particular animal, say a fox from a dog.
    We do not need to learn what a fox is from understanding what lines, eyes, and
    other smaller features are like. So we will learn how to use a pre-trained model
    to build state-of-the-art image classifiers with very little data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是在不从头训练算法的情况下，重新使用已训练算法处理类似数据集的能力。我们人类并不通过分析成千上万张类似图像来学习识别新图像。作为人类，我们只是理解实际区分特定动物（比如狐狸和狗）的不同特征。我们无需从理解线条、眼睛及其他较小特征是什么开始学习什么是狐狸。因此，我们将学习如何利用预训练模型来仅用少量数据构建最先进的图像分类器。
- en: 'The first few layers of a CNN architecture focus on smaller features, such
    as how a line or curve looks. The filters in the later layers of a CNN learn higher-level
    features, such as eyes and fingers, and the last few layers learn to identify
    the exact category. A pre-trained model is an algorithm that is trained on a similar
    dataset. Most of the popular algorithms are pre-trained on the popular `ImageNet`
    dataset to identify 1,000 different categories. Such a pre-trained model will
    have filter weights tuned to identify various patterns. So, let''s understand
    how can we take advantage of these pre-trained weights. We will look into an algorithm
    called **VGG16**, which was one of the earliest algorithms to find success in
    ImageNet competitions. Though there are more modern algorithms, this algorithm
    is still popular as it is simple to understand and use for transfer learning.
    Let''s take a look at the architecture of the VGG16 model, and then try to understand
    the architecture and how we can use it to train our image classifier:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构的前几层专注于较小的特征，例如线条或曲线的外观。CNN的后几层中的滤波器学习更高级的特征，例如眼睛和手指，最后几层学习识别确切的类别。预训练模型是在类似数据集上训练的算法。大多数流行的算法都是在流行的`ImageNet`数据集上预训练，以识别1,000个不同的类别。这样一个预训练模型将具有调整后的滤波器权重，用于识别各种模式。因此，让我们了解如何利用这些预训练权重。我们将研究一种名为**VGG16**的算法，这是最早在ImageNet竞赛中取得成功的算法之一。尽管现代有更多的算法，但由于其简单易懂且适用于迁移学习，这个算法仍然很受欢迎。让我们先看一下VGG16模型的架构，然后尝试理解这个架构以及如何用它来训练我们的图像分类器：
- en: '![](img/f76292fc-f90a-4a8e-92c0-328236a9d09d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f76292fc-f90a-4a8e-92c0-328236a9d09d.png)'
- en: Architecture of the VGG16 model
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16模型的架构
- en: The VGG16 architecture contains five VGG blocks. A block is a set of convolution
    layers, a nonlinear activation function, and a max-pooling function. All the algorithm
    parameters are tuned to achieve state-of-the-art results for classifying 1,000
    categories. The algorithm takes input data in the form of batches, which are normalized
    by the mean and standard deviation of the `ImageNet` dataset. In transfer learning,
    we try to capture what the algorithm learns by freezing the learned parameters
    of most of the layers of the architecture. It is often a common practice to fine-tune
    only the last layers of the network. In this example, let's train only the last
    few linear layers and leave the convolutional layers intact, as the features learned
    by the convolutional features are mostly used for all kinds of image problems
    where the images share similar properties. Let's train a VGG16 model using transfer
    learning to classify dogs and cats. Let's walk through the different steps required
    to implement this.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16的架构包含五个VGG块。一个块包括一组卷积层、非线性激活函数和最大池化函数。所有的算法参数都调整到达到分类1,000个类别的最先进结果。该算法接受批处理形式的输入数据，这些数据是通过`ImageNet`数据集的均值和标准差进行标准化的。在迁移学习中，我们尝试通过冻结大部分层的学习参数来捕捉算法学到的内容。通常的做法是仅微调网络的最后几个线性层，并保持卷积层不变，因为卷积层学到的特征对所有具有相似属性的图像问题都是有效的。在这个例子中，让我们仅训练最后几个线性层，保持卷积层不变。让我们使用迁移学习训练一个VGG16模型来分类狗和猫。让我们逐步实施这些不同步骤。
- en: Creating and exploring a VGG16 model
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和探索一个VGG16模型
- en: 'PyTorch provides a set of trained models in its `torchvision` library. Most
    of them accept an argument called `pretrained` when `True`, which downloads the
    weights tuned for the **ImageNet** classification problem. Let''s look at the
    code snippet that creates a VGG16 model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在其`torchvision`库中提供了一组经过训练的模型。当参数`pretrained`为`True`时，大多数模型会下载针对**ImageNet**分类问题调整过的权重。让我们看一下创建VGG16模型的代码片段：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we have our VGG16 model with all the pre-trained weights ready to be used.
    When the code is run for the first time, it could take several minutes, depending
    on your internet speed. The size of the weights could be around 500 MB. We can
    take a quick look at the VGG16 model by printing it. Understanding how these networks
    are implemented turns out to be very useful when we use modern architectures.
    Let''s take a look at the model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了准备好使用的所有预训练权重的VGG16模型。当第一次运行代码时，根据您的网络速度，可能需要几分钟的时间。权重的大小大约为500 MB。我们可以通过打印来快速查看VGG16模型。了解这些网络是如何实现的，在使用现代架构时非常有用。让我们看看这个模型：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The model summary contains two sequential model `features` and `classifiers`.
    The `features sequential` model has the layers that we are going to freeze.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要包含两个顺序模型 `特征` 和 `分类器`。 `特征顺序` 模型具有我们将要冻结的层。
- en: Freezing the layers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结层
- en: 'Let''s freeze all the layers of the `features` model, which contains the convolutional
    block. Freezing the weights in the layers will prevent the weights of these convolutional
    blocks. As the weights of the model are trained to recognize a lot of important
    features, our algorithm would be able to do the same from the very first iteration.
    The ability to use models weights, which were initially trained for a different
    use case, is called **transfer learning**. Now let''s look at how we can freeze
    the weights, or parameters, of layers:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们冻结 `特征` 模型的所有层，其中包含卷积块。冻结这些卷积块的权重将防止模型权重。由于模型的权重是训练用于识别许多重要特征，我们的算法将能够从第一个迭代开始做同样的事情。使用最初针对不同用例训练的模型权重的能力称为**迁移学习**。现在让我们看看如何冻结层的权重或参数：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code prevents the optimizer from updating the weights.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码防止优化器更新权重。
- en: Fine-tuning VGG16
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 VGG16
- en: 'The VGG16 model is trained to classify 1,000 categories, but not trained to
    classify dogs and cats. So, we need to change the output features of the last
    layer to `2` from `1000`. The following code snippet does it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型经过训练，用于分类 1000 个类别，但没有经过狗和猫的分类训练。因此，我们需要将最后一层的输出特征从 `1000` 更改为 `2`。以下代码片段执行此操作：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `vgg.classifier` gives access to all the layers inside the sequential model,
    and the sixth element will contain the last layer. When we train the VGG16 model,
    we only need the classifier parameters to be trained. So, we pass only the `classifier.parameters`
    to the optimizer as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`vgg.classifier` 提供了顺序模型中所有层的访问权限，第六个元素将包含最后一层。当我们训练 VGG16 模型时，我们只需要训练分类器参数。因此，我们将仅将
    `classifier.parameters` 传递给优化器，如下所示：'
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the VGG16 model
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 VGG16 模型
- en: 'We have created the model and optimizer. Since we are using the `Dogs vs. Cats`
    dataset, we can use the same data loaders and the `train` function to train our
    model. Remember, when we train the model, only the parameters inside the classifier
    change. The following code snippet will train the model for `20` epochs, reaching
    a validation accuracy of 98.45%:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了模型和优化器。由于我们使用的是 `狗与猫` 数据集，我们可以使用相同的数据加载器和 `train` 函数来训练我们的模型。请记住，当我们训练模型时，只有分类器内部的参数会发生变化。以下代码片段将训练模型
    `20` 个 epoch，达到 98.45% 的验证准确率。
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s visualize the training and validation loss:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化训练和验证损失：
- en: '![](img/f8d58fda-2db4-4327-9ebc-f4a1a7a02416.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8d58fda-2db4-4327-9ebc-f4a1a7a02416.png)'
- en: 'Let''s visualize the training and validation accuracy:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化训练和验证准确率：
- en: '![](img/914a6a81-d15d-45db-8e35-bebf67f2ccb8.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/914a6a81-d15d-45db-8e35-bebf67f2ccb8.png)'
- en: 'We can apply a couple of tricks, such as data augmentation and playing with
    different values of the dropout to improve the model''s generalization capabilities.
    The following code snippet changes the dropout, value in the classifier module
    of VGG to `0.2` from `0.5` and trains the model:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用一些技巧，例如数据增强和尝试不同的丢弃值来改进模型的泛化能力。下面的代码片段将在 VGG 的分类器模块中将丢弃值从 `0.5` 更改为 `0.2`
    并训练模型：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Training this for a few epochs gave me a slight improvement; you can try playing
    with different dropout values. Another important trick to improve model generalization
    is to add more data or do data augmentation. We will do data augmentation, by
    randomly flipping the image horizontally or rotating the image by a small angle.
    The `torchvision` transforms provide different functionalities for doing data
    augmentation and they do it dynamically, changing for every epoch. We implement
    data augmentation using the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对此进行几个 epoch 的训练稍微改进了我的模型；您可以尝试不同的丢弃值。改进模型泛化的另一个重要技巧是增加更多的数据或进行数据增强。我们将进行数据增强，随机水平翻转图像或将图像旋转一个小角度。
    `torchvision` 转换提供不同的功能来执行数据增强，它们在每个 epoch 动态地进行更改。我们使用以下代码实现数据增强：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding code is generated as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出生成如下：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training the model with augmented data improved the model accuracy by 0.1% by
    running just two epochs; we can run it for a few more epochs to improve further.
    If you have been training these models while reading the book, you will have realized
    that training each epoch could take more than a couple of minutes, depending on
    the GPU you are running. Let's look at a technique where we can train each epoch
    in a few seconds.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用增强数据训练模型提高了0.1%的模型准确性，仅运行两个epoch即可。如果继续运行更多epoch，可以进一步提高准确性。如果你在阅读书籍时一直在训练这些模型，你会意识到每个epoch的训练可能会超过几分钟，具体取决于你使用的GPU。让我们看一下一种技术，可以在几秒钟内完成每个epoch的训练。
- en: Calculating pre-convoluted features
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算预卷积特征
- en: 'When we freeze the convolution layers and the train model, the input to the
    fully connected layers, or dense layers, (`vgg.classifier`) is always the same.
    To understand better, let''s treat the convolution block, in our example the `vgg.features`
    block, as a function that has learned weights and does not change during training.
    So, calculating the convolution features and storing them will help us to improve
    the training speed. The time to train the model decreases, as we calculate these
    features only once instead of calculating for each epoch. Let''s visually understand
    and implement the same:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们冻结卷积层并训练模型时，完全连接层（或称为密集层）的输入（`vgg.classifier`）始终保持不变。为了更好地理解，让我们将卷积块，在我们的例子中是`vgg.features`块，视为一个已经学习权重且在训练过程中不会改变的函数。因此，计算卷积特征并存储它们将有助于提高训练速度。由于我们只需计算这些特征一次，而不是每个epoch都计算，因此模型训练的时间大大缩短。让我们通过可视化的方式理解并实现相同的过程：
- en: '![](img/d1cb4fa0-d566-42fb-8b27-f45abadeca02.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1cb4fa0-d566-42fb-8b27-f45abadeca02.png)'
- en: 'The first box depicts how training is done in general, which could be slow,
    as we calculate the convolutional features for every epoch, though the values
    do not change. In the bottom box, we calculate the convolutional features once
    and train only the linear layers. To calculate the pre-convoluted features, we
    will pass all the training data through the convolution blocks and store them.
    To perform this, we need to select the convolution blocks of the VGG model. Fortunately,
    the PyTorch implementation of VGG16 has two sequential models, so just picking
    the first sequential model''s features is enough. The following code does that:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个框显示了通常的训练方式，这可能会很慢，因为我们为每个epoch计算卷积特征，尽管这些值不会改变。在底部框中，我们只计算一次卷积特征并仅训练线性层。为了计算预卷积特征，我们将所有训练数据通过卷积块，并将它们存储起来。为此，我们需要选择VGG模型的卷积块。幸运的是，VGG16的PyTorch实现有两个连续模型，因此仅选择第一个连续模型的特征就足够了。以下代码实现了这一点：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the previous code, the `preconvfeat` method takes in the dataset and `vgg`
    model and returns the convoluted features along with the labels associated with
    it. The rest of the code is similar to what we have used in the other examples
    for creating data loaders and datasets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`preconvfeat`方法接收数据集和`vgg`模型，并返回了卷积特征以及与之相关联的标签。其余的代码与我们在其他示例中用于创建数据加载器和数据集的代码类似。
- en: 'Once we have the convolutional features for the `train` and `validation` sets,
    let''s create a PyTorch dataset and `DataLoader` classes, which will ease up our
    training process. The following code creates the `Dataset` and `DataLoader` for
    our convolution features:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对`train`和`validation`集合有了卷积特征，让我们创建PyTorch数据集和`DataLoader`类，这将简化我们的训练过程。以下代码创建了用于我们卷积特征的`Dataset`和`DataLoader`：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we have our new data loaders that generate batches of convoluted features
    along with the labels, we can use the same `train` function that we have been
    using in the other examples. Now we will use `vgg.classifier` as the model for
    creating the `optimizer` and `fit` methods. The following code trains the classifier
    module to identify dogs and cats. On a Titan X GPU, each epoch takes less than
    five seconds, which would otherwise take a few minutes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有新的数据加载器，可以生成带有标签的卷积特征批次，我们可以使用在其他示例中使用过的相同的`train`函数。现在我们将使用`vgg.classifier`作为模型，用于创建`optimizer`和`fit`方法。以下代码训练分类器模块以识别狗和猫。在Titan
    X GPU上，每个epoch的时间少于五秒，这相比于原本可能需要几分钟：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Understanding what a CNN model learns
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNN模型学习到了什么
- en: Deep learning models are often said to be not interpretable. But there are different
    techniques that are being explored to interpret what happens inside these models.
    For images, the features learned by convents are interpretable. We will explore
    two popular techniques to understand convents.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常被认为是不可解释的。但是，正在探索不同的技术来解释这些模型内部发生的事情。对于图像，卷积网络学习到的特征是可解释的。我们将探索两种流行的技术来理解卷积网络。
- en: Visualizing outputs from intermediate layers
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化中间层的输出
- en: Visualizing the outputs from intermediate layers will help us in understanding
    how the input image is being transformed across different layers. Often, the output
    from each layer is called an **activation**. To do this, we should extract output
    from intermediate layers, which can be done in different ways. PyTorch provides
    a method called `register_forward_hook`, which allows us to pass a function which
    can extract outputs of a particular layer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间层的输出将帮助我们理解输入图像如何在不同层次中进行转换。通常，每层的输出称为**激活**。为此，我们应该从中间层提取输出，可以通过不同的方式实现。PyTorch
    提供了一个称为 `register_forward_hook` 的方法，允许我们传递一个函数来提取特定层的输出。
- en: 'By default, PyTorch models only store the output of the last layer, to use
    memory optimally. So, before we inspect what the activations from the intermediate
    layers look like, let''s understand how to extract outputs from the model. Let''s
    look at the following code snippet, which extracts, and we will walk through it
    to understand what happens:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch 模型仅存储最后一层的输出，以便最优化内存使用。因此，在我们检查中间层激活之前，让我们先了解如何从模型中提取输出。让我们看下面的代码片段，该片段提取了输出，我们将逐步分析其过程：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We start with the creation of a pre-trained VGG model, from which we extract
    the outputs of a particular layer. The `LayerActivations` class instructs PyTorch
    to store the output of a layer to the `features` variable. Let's walk through
    each function inside the `LayerActivations` class.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个预训练的 VGG 模型开始，从中提取特定层的输出。`LayerActivations` 类指示 PyTorch 将层的输出存储到 `features`
    变量中。让我们逐个检查 `LayerActivations` 类中的每个函数。
- en: The `_init_` function takes a model and the layer number for which the outputs
    need to be extracted as arguments. We call the `register_forward_hook` method
    on the layer and pass in a function. PyTorch, when doing a forward pass—that is,
    when the images are passed through the layers—calls the function that is passed
    to the `register_forward_hook` method. This method returns a handle, which can
    be used to deregister the function that is passed to the `register_forward_hook`
    method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`_init_` 函数接受模型和需要提取输出的层号作为参数。我们在该层上调用 `register_forward_hook` 方法并传递一个函数。当
    PyTorch 进行前向传播——即图像通过各层时——会调用传递给 `register_forward_hook` 方法的函数。该方法返回一个句柄，可用于注销传递给
    `register_forward_hook` 方法的函数。'
- en: The `register_forward_hook` method passes three values to the function that
    we pass to it. The `module` parameter allows us access to the layer itself. The
    second parameter is `input`, which refers to data that is flowing through the
    layer. The third parameter is `output`, which allows access to the transformed
    inputs, or activation, of the layer. We store the output to the features variable
    in the `LayerActivations` class.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`register_forward_hook` 方法向我们传递了三个值，这些值会传递给我们传递给它的函数。`module` 参数允许我们访问层本身。第二个参数是
    `input`，指的是流经该层的数据。第三个参数是 `output`，允许访问转换后的输入或层的激活。我们将输出存储到 `LayerActivations`
    类的 `features` 变量中。'
- en: 'The third function takes the `hook` from the `_init_` function and deregisters
    the function. Now we can pass the model and the layer number for which we are
    looking for activations. Let''s look at the activations created for the following
    image for different layers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个函数从 `_init_` 函数获取 `hook` 并注销函数。现在我们可以传递模型和我们希望查找激活的层号。让我们看看以下图像不同层级的激活：
- en: '![](img/4869b190-ad00-4e28-b4d9-2f6c06447aff.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4869b190-ad00-4e28-b4d9-2f6c06447aff.png)'
- en: 'Let''s visualize some of the activations created by the first convolution layer
    and the code used for it:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化第一个卷积层创建的一些激活及其使用的代码：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s visualize some of the activations created by the fifth convolution layer:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化第五个卷积层创建的一些激活：
- en: '![](img/612e41be-0616-4e24-b63e-f875afe02f56.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/612e41be-0616-4e24-b63e-f875afe02f56.png)'
- en: 'Let''s look at the last CNN layer:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最后一个 CNN 层：
- en: '![](img/50a60d05-6b3a-4b58-a811-716c608ffe5c.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50a60d05-6b3a-4b58-a811-716c608ffe5c.png)'
- en: From looking at what different layers generate, we can see that the early layers
    detect lines and edges, and the last layers tend to learn higher-level features
    and are less interpretable. Before we move on to visualizing weights, let's see
    how the features maps or activations represents itself after the ReLU layer. So,
    let's visualize the outputs of the second layer.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从观察不同层生成的内容可以看出，早期层检测线条和边缘，而最后的层倾向于学习更高级的特征，并且不那么可解释。在我们继续可视化权重之前，让我们看看在ReLU层之后特征映射或激活是如何呈现的。因此，让我们来可视化第二层的输出。
- en: If you take a quick look at the fifth image in the second row of the preceding
    image, it looks like the filter is detecting the eyes in the image. When the models
    do not perform, these tricks to visualize can help us understand why the model
    may not be working.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您快速查看前一图像的第二行中的第五幅图像，它看起来像是在检测图像中的眼睛。当模型性能不佳时，这些可视化技巧可以帮助我们理解模型可能出现问题的原因。
- en: Visualizing weights of the CNN layer
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化CNN层的权重
- en: 'Getting model weights for a particular layer is straightforward. All the model
    weights can be accessed through the `state_dict` function. The `state_dict` function
    returns a dictionary, with keys as its layers and weights as its values. The following
    code demonstrates how to pull weights for a particular layer and visualize them:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 获得特定层的模型权重非常简单。所有模型权重可以通过`state_dict`函数访问。`state_dict`函数返回一个字典，其中键是层，值是其权重。以下代码演示了如何提取特定层的权重并进行可视化：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code provides us with the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码给出了以下输出：
- en: '![](img/bf5cc2da-6e75-49fa-9e3a-ee8f83c1ad2c.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf5cc2da-6e75-49fa-9e3a-ee8f83c1ad2c.png)'
- en: Each box represents weights of a filter that is of size *3 x 3*. Each filter
    is trained to identify certain patterns in the images.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框表示大小为*3 x 3*的过滤器权重。每个过滤器都经过训练来识别图像中的某些模式。
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to build an image classifier using convents,
    and also how to use a pre-trained model. We covered tricks on how to speed up
    the process of training by using these pre-convoluted features. Also, we understood
    different techniques that can be used to understand what goes on inside a CNN.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用卷积神经网络构建图像分类器，以及如何使用预训练模型。我们介绍了通过使用这些预卷积特征加快训练过程的技巧。此外，我们了解了可以用来理解CNN内部运作过程的不同技术。
- en: In the next chapter, we will learn how to handle sequential data using recurrent
    neural networks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用循环神经网络处理序列数据。
