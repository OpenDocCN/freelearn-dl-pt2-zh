- en: 2 Combining CNNs and LSTMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 结合 CNN 和 LSTM
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区Discord
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file0.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file0.png)'
- en: '**Convolutional Neural Networks** (**CNNs**) are a type of deep learning model
    known to solve machine learning problems related to images and video, such as
    image classification, object detection, segmentation, and more. This is because
    CNNs use a special type of layer called **convolutional layers**, which have shared
    learnable parameters. The weight or parameter sharing works because the patterns
    to be learned in an image (such as edges or contours) are assumed to be independent
    of the location of the pixels in the image. Just as CNNs are applied to images,
    **Long Short-Term Memory** (**LSTM**) networks – which are a type of **Recurrent
    Neural Network** (**RNN**) – prove to be extremely effective at solving machine
    learning problems related to **sequential data**. An example of sequential data
    could be text. For example, in a sentence, each word is dependent on the previous
    word(s). LSTM models are meant to model such sequential dependencies.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是一种深度学习模型，已知可以解决与图像和视频相关的机器学习问题，如图像分类、目标检测、分割等。这是因为 CNN
    使用一种称为**卷积层**的特殊层，它具有共享的可学习参数。权重或参数共享之所以有效，是因为图像中要学习的模式（如边缘或轮廓）被假定独立于图像中像素的位置。正如
    CNN 适用于图像一样，**长短期记忆网络**（**LSTM**）——它们是**循环神经网络**（**RNN**）的一种类型——在解决**顺序数据**相关的机器学习问题时非常有效。顺序数据的一个例子可以是文本。例如，在一个句子中，每个单词依赖于前面的单词。LSTM
    模型旨在建模这种顺序依赖关系。'
- en: These two different types of networks – CNNs and LSTMs – can be cascaded to
    form a hybrid model that takes in images or video and outputs text. One well-known
    application of such a hybrid model is image captioning, where the model takes
    in an image and outputs a plausible textual description of the image. Since 2010,
    machine learning has been used to perform the task of image captioning [2.1].
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种不同类型的网络——CNN 和 LSTM——可以级联以形成一个混合模型，接受图像或视频并输出文本。这种混合模型的一个众所周知的应用是图像字幕生成，模型接收图像并输出图像的一个合理的文本描述。自
    2010 年以来，机器学习已被用于执行图像字幕生成任务[2.1]。
- en: However, neural networks were first successfully used for this task in around
    2014/2015 [2.2]. Ever since, image captioning has been actively researched. With
    significant improvements each year, this deep learning application can become
    useful for real-world applications such as auto-generating alt-text in websites
    to make them more accessible for the visually impaired.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络最初成功地用于这项任务大约是在 2014/2015 年[2.2]。自那时以来，图像字幕生成一直在积极研究中。每年都有显著改进，这种深度学习应用可以成为现实世界应用的一部分，例如在网站上自动生成视觉障碍者友好的替代文本。
- en: 'This chapter first discusses the architecture of such a hybrid model, along
    with the related implementational details in PyTorch, and at the end of the chapter,
    we will build an image captioning system from scratch using PyTorch. This chapter
    covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先讨论这种混合模型的架构，以及在 PyTorch 中相关的实现细节，最后我们将使用 PyTorch 从头开始构建一个图像字幕系统。本章涵盖以下主题：
- en: Building a neural network with CNNs and LSTMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 和 LSTM 构建神经网络
- en: Building an image caption generator using PyTorch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建图像标题生成器
- en: Building a neural network with CNNs and LSTMs
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CNN 和 LSTM 构建神经网络
- en: A CNN-LSTM network architecture consists of a convolutional layer(s) for extracting
    features from the input data (image), followed by an LSTM layer(s) to perform
    sequential predictions. This kind of model is both spatially and temporally deep.
    The convolutional part of the model is often used as an **encoder** that takes
    in an input image and outputs high-dimensional features or embeddings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN-LSTM 网络架构包括一个或多个卷积层，用于从输入数据（图像）中提取特征，然后是一个或多个 LSTM 层，用于执行顺序预测。这种模型在空间和时间上都很深。模型的卷积部分通常被用作一个**编码器**，它接受输入图像并输出高维特征或嵌入。
- en: In practice, the CNN used for these hybrid networks is often pre-trained on,
    say, an image classification task. The last hidden layer of the pre-trained CNN
    model is then used as an input to the LSTM component, which is used as a **decoder**
    to generate text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，用于这些混合网络的 CNN 通常是在图像分类任务上预训练的。预训练 CNN 模型的最后一个隐藏层然后作为 LSTM 组件的输入，LSTM 作为一个**解码器**用于生成文本。
- en: When we are dealing with textual data, we need to transform the words and other
    symbols (punctuation, identifiers, and more) – together referred to as **tokens**
    – into numbers. We do so by representing each token in the text with a unique
    corresponding number. In the following sub-section, we will demonstrate an example
    of text encoding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本数据时，我们需要将单词和其他符号（标点符号、标识符等）一起转换为数字，统称为**标记**。我们通过用唯一对应的数字表示文本中的每个标记来实现这一点。在下一小节中，我们将演示文本编码的示例。
- en: Text encoding demo
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本编码演示
- en: 'Let''s assume we''re building a machine learning model with textual data; say,
    for example, that our text is as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们正在构建一个使用文本数据的机器学习模型；例如，我们的文本如下：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we would map each of these words/tokens to numbers, as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将每个单词/标记映射为数字，如下所示：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have the mapping, we can represent this sentence numerically as a list
    of numbers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了映射，我们可以将这个句子表示为一个数字列表：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Also, for example, `<start> PyTorch is deep. <end>` would be encoded as `->
    [0, 1, 2, 4, 7, 8]` and so on. This mapping, in general, is referred to as **vocabulary**,
    and building a vocabulary is a crucial part of most text-related machine learning
    problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，例如 `<start> PyTorch is deep. <end>` 将被编码为 `-> [0, 1, 2, 4, 7, 8]` 等等。这种映射通常被称为**词汇表**，构建词汇表是大多数文本相关的机器学习问题的关键部分。
- en: 'The LSTM model, which acts as the decoder, takes in a CNN embedding as input
    at `t=0`. Then, each LSTM cell makes a token prediction at each time-step, which
    is fed as the input to the next LSTM cell. The overall architecture thus generated
    can be visualized as shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解码器的 LSTM 模型在 `t=0` 时以 CNN 嵌入作为输入。然后，每个 LSTM 单元在每个时间步进行标记预测，这些预测作为下一个 LSTM
    单元的输入。因此生成的整体架构可以如下图所示：
- en: '![Figure 2.1 – Example CNN-LSTM architecture](img/file1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 示例 CNN-LSTM 架构](img/file1.jpg)'
- en: Figure 2.1 – Example CNN-LSTM architecture
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 示例 CNN-LSTM 架构
- en: The demonstrated architecture is suitable for the image captioning task. If
    instead of just having a single image we had a sequence of images (say, in a video)
    as the input to the CNN layer, then we would include the CNN embedding as the
    LSTM cell input at each time-step, not just at `t=0`. This kind of architecture
    would be useful for applications such as activity recognition or video description.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所演示的架构适用于图像字幕任务。如果我们不仅仅有单个图像，而是有一个图像序列（比如视频）作为 CNN 层的输入，那么我们将在每个时间步将 CNN 嵌入作为
    LSTM 单元的输入，而不仅仅是在 `t=0`。这种架构对于诸如活动识别或视频描述等应用非常有用。
- en: In the next section, we will implement an image captioning system in PyTorch
    that includes building a hybrid model architecture as well as data loading, preprocessing,
    model training, and model evaluation pipelines.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将在 PyTorch 中实现一个图像字幕系统，包括构建混合模型架构、数据加载、预处理、模型训练和模型评估流程。
- en: Building an image caption generator using PyTorch
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建图像字幕生成器
- en: For this exercise, we will be using the **Common Objects in Context** (**COCO**)
    dataset [2.3] , which is a large-scale object detection, segmentation, and captioning
    dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用**通用物体上下文**（**COCO**）数据集 [2.3]，这是一个大规模的对象检测、分割和字幕数据集。
- en: This dataset consists of over 200,000 labeled images with five captions for
    each image. The COCO dataset emerged in 2014 and has helped significantly in the
    advancement of object recognition-related computer vision tasks. It stands as
    one of the most commonly used datasets for benchmarking tasks such as object detection,
    object segmentation, instance segmentation, and image captioning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含超过 20 万张带有每张图像五个标题的标注图像。COCO 数据集于 2014 年出现，并显著促进了与对象识别相关的计算机视觉任务的进展。它是最常用于基准测试任务的数据集之一，例如对象检测、对象分割、实例分割和图像字幕。
- en: In this exercise, we will use PyTorch to train a CNN-LSTM model on this dataset
    and use the trained model to generate captions for unseen samples. Before we do
    that, though, there are a few pre-requisites that we need to take care of .
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 PyTorch 在这个数据集上训练一个 CNN-LSTM 模型，并使用训练好的模型为未见样本生成标题。在此之前，我们需要处理一些先决条件。
- en: Note
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will be referring to only the important snippets of code for illustration
    purposes. The full exercise code can be found in our github repository [2.4]
  id: totrans-33
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将仅参考一些重要的代码片段来进行说明。完整的练习代码可以在我们的 GitHub 仓库 [2.4] 中找到。
- en: Downloading the image captioning datasets
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载图像字幕数据集
- en: Before we begin building the image captioning system, we need to download the
    required datasets. If you do not have the datasets downloaded, then run the following
    script with the help of Jupyter Notebook. This should help with downloading the
    datasets locally.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建图像字幕系统之前，我们需要下载所需的数据集。如果您尚未下载数据集，请在 Jupyter Notebook 的帮助下运行以下脚本。这应该可以帮助您在本地下载数据集。
- en: Note
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We are using a slightly older version of the dataset as it is slightly smaller
    in size, enabling us to get the results faster.
  id: totrans-38
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用稍旧版本的数据集，因为它的大小稍小，这样可以更快地得到结果。
- en: 'The training and validation datasets are 13 GB and 6 GB in size, respectively.
    Downloading and extracting the dataset files, as well as cleaning and processing
    them, might take a while. A good idea is to execute these steps as follows and
    let them finish overnight:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证数据集分别为13 GB和6 GB。下载和提取数据集文件以及清理和处理它们可能需要一些时间。一个好主意是按照以下步骤执行，并让它们在夜间完成：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '![Figure 2.2 – Data download and extraction](img/file2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 数据下载和提取](img/file2.jpg)'
- en: Figure 2.2 – Data download and extraction
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 数据下载和提取
- en: This step basically creates a data folder (`./data_dir`), downloads the zipped
    images and annotation files, and extracts them inside the data folder.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤基本上创建了一个数据文件夹（`./data_dir`），下载了压缩的图像和注释文件，并将它们提取到数据文件夹中。
- en: Preprocessing caption (text) data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理字幕（文本）数据
- en: 'The downloaded image captioning datasets consist of both text (captions) and
    images. In this section, we will preprocess the text data to make it usable for
    our CNN-LSTM model. The exercise is laid out as a sequence of steps. The first
    three steps are focused on processing the text data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的图像字幕数据集包含文本（字幕）和图像。在本节中，我们将预处理文本数据，使其可用于我们的 CNN-LSTM 模型。这项练习按步骤进行。前三个步骤专注于处理文本数据：
- en: 'For this exercise, we will need to import a few dependencies. Some of the crucial
    modules we will import for this chapter are as follows:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们需要导入一些依赖项。本章的一些关键模块如下：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`nltk` is the natural language toolkit, which will be helpful in building our
    vocabulary, while `pycocotools` is a helper tool to work with the COCO dataset.
    The various Torch modules we have imported here have already been discussed in
    the previous chapter, except the last one – that is, `pack_padded_sequence`. This
    function will be useful to transform sentences with variable lengths (number of
    words) into fixed-length sentences by applying padding.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 是自然语言工具包，将有助于构建我们的词汇表，而 `pycocotools` 是与 COCO 数据集一起工作的辅助工具。我们在这里导入的各种
    Torch 模块已在前一章中讨论过，除了最后一个 - `pack_padded_sequence`。此函数将有助于通过应用填充，将具有不同长度（单词数）的句子转换为固定长度的句子。'
- en: 'Besides importing the `nltk` library, we will also need to download its `punkt`
    tokenizer model, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了导入`nltk`库之外，我们还需要下载其`punkt`分词模型，如下所示：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will enable us to tokenize given text into constituent words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够将给定文本标记为组成单词。
- en: 'Next, we build the vocabulary – that is, a dictionary that can convert actual
    textual tokens (such as words) into numeric tokens. This step is essential for
    most text-related tasks: :'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建词汇表 - 即可以将实际文本标记（如单词）转换为数值标记的字典。这一步在大多数与文本相关的任务中都是必不可少的：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First, inside the vocabulary builder function, JSON text annotations are loaded,
    and individual words in the annotation/caption are tokenized or converted into
    numbers and stored in a counter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在词汇构建器函数内部，加载了 JSON 文本注释，并将注释/字幕中的个别单词进行了标记化或转换为数字并存储在计数器中。
- en: 'Then, tokens with fewer than a certain number of occurrences are discarded,
    and the remaining tokens are added to a vocabulary object beside some wildcard
    tokens – `start` (of the sentence), `end`, `unknown_word`, and padding tokens,
    as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，丢弃少于某个数量出现次数的标记，并将剩余的标记添加到词汇对象中，同时添加一些通配符标记 - `start`（句子的开头）、`end`、`unknown_word`和填充标记，如下所示：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, using the vocabulary builder function, a vocabulary object vocab is
    created and saved locally for further reuse, as shown in the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用词汇构建器函数创建并保存了一个词汇对象 vocab，以便进一步重用，如下所示：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output for this is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作的输出如下：
- en: '![Figure 2.3 – Vocabulary creation](img/file3.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 词汇表创建](img/file3.jpg)'
- en: Figure 2.3 – Vocabulary creation
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 词汇表创建
- en: Once we have built the vocabulary, we can deal with the textual data by transforming
    it into numbers at runtime.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了词汇表，我们可以在运行时将文本数据转换为数字。
- en: Preprocessing image data
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据预处理
- en: After downloading the data and building the vocabulary for the text captions,
    we need to perform some preprocessing for the image data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据并为文本标题构建词汇表后，我们需要对图像数据进行一些预处理。
- en: 'Because the images in the dataset can come in various sizes or shapes, we need
    to reshape all the images to a fixed shape so that they can be inputted to the
    first layer of our CNN model, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据集中的图像可能有不同的尺寸或形状，我们需要将所有图像重塑为固定的形状，以便它们可以输入到我们 CNN 模型的第一层，如下所示：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output for this will be as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 2.4 – Image preprocessing (reshaping)](img/file4.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 图像预处理（重塑）](img/file4.jpg)'
- en: Figure 2.4 – Image preprocessing (reshaping)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 图像预处理（重塑）
- en: We have reshaped all the images to 256 X 256 pixels, which makes them compatible
    with our CNN model architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将所有图像重塑为 256 x 256 像素，使其与我们的 CNN 模型架构兼容。
- en: Defining the image captioning data loader
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义图像字幕数据加载器
- en: 'We have already downloaded and preprocessed the image captioning data. Now
    it is time to cast this data as a PyTorch dataset object. This dataset object
    can subsequently be used to define a PyTorch data loader object, which we will
    use in our training loop to fetch batches of data as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经下载并预处理了图像字幕数据。现在是将此数据转换为 PyTorch 数据集对象的时候了。这个数据集对象随后可以用来定义一个 PyTorch 数据加载器对象，在训练循环中使用以获取数据批次，如下所示：
- en: 'Now, we will implement our own custom `Dataset` module and a custom data loader:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实现自己的定制 `Dataset` 模块和一个自定义的数据加载器：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: First, in order to define our custom PyTorch `Dataset` object, we have defined
    our own `__init__`, `__get_item__`, and `__len__` methods for instantiation, fetching
    items, and returning the size of the dataset, respectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了定义我们自定义的 PyTorch `Dataset` 对象，我们已经为实例化、获取项目和返回数据集大小定义了自己的 `__init__`、`__get_item__`
    和 `__len__` 方法。
- en: 'Next, we define `collate_function`, which returns mini batches of data in the
    form of `X`, `y`, as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `collate_function`，它以 `X`、`y` 的形式返回数据的小批量，如下所示：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Usually, we would not need to write our own `collate` function, but we do so
    to deal with variable-length sentences so that when the length of a sentence (say,
    `k`) is less than the fixed length, `n`, then we need to pad the `n-k` tokens
    with padding tokens using the `pack_padded_sequence` function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不需要编写自己的 `collate` 函数，但我们需要处理变长句子，以便当句子的长度（例如 `k`）小于固定长度 `n` 时，使用 `pack_padded_sequence`
    函数填充 `n-k` 个标记。
- en: 'Finally, we will implement the `get_loader` function, which returns a custom
    data loader for the `COCO` dataset in the following code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将实现 `get_loader` 函数，该函数返回一个用于 `COCO` 数据集的自定义数据加载器，代码如下：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: During the training loop, this function will be extremely useful and efficient
    in fetching mini batches of data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中，此函数将非常有用且高效，用于获取数据的小批量。
- en: This completes the work needed to set up the data pipeline for model training.
    We will now work toward the actual model itself.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了为模型训练设置数据流水线所需的工作。现在我们将朝着实际模型本身迈进。
- en: Defining the CNN-LSTM model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 CNN-LSTM 模型
- en: 'Now that we have set up our data pipeline, we will define the model architecture
    as per the description in *Figure 2.1*, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了数据流水线，我们将按照 *图 2.1* 中的描述定义模型架构，如下所示：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have defined two sub-models – that is, a CNN model and an RNN model. For
    the CNN part, we use a pre-trained CNN model available under the PyTorch models
    repository: the ResNet 152 architecture. While we will learn more about ResNet
    in detail in the next chapter, this deep CNN model with 152 layers is pre-trained
    on the ImageNet dataset [2.5] . The ImageNet dataset contains over 1.4 million
    RGB images labeled over 1,000 classes. These 1,000 classes belong to categories
    such as plants, animals, food, sports, and more.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个子模型，即 CNN 模型和 RNN 模型。对于 CNN 部分，我们使用了 PyTorch 模型库中可用的预训练 CNN 模型：ResNet
    152 架构。在下一章节中，我们将详细学习 ResNet，这个具有 152 层的深度 CNN 模型已在 ImageNet 数据集上进行了预训练 [2.5]
    。ImageNet 数据集包含超过 140 万张 RGB 图像，标注了超过 1000 个类别。这些 1000 个类别包括植物、动物、食物、运动等多个类别。
- en: We remove the last layer of this pre-trained ResNet model and replace it with
    a fully- connected layer followed by a batch normalization layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了预训练的 ResNet 模型的最后一层，并替换为一个全连接层，接着是一个批归一化层。
- en: FAQ - Why are we able to replace the fully-connected layer?
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The neural network can be seen as a sequence of weight matrices starting from
    the weight matrix between the input layer and the first hidden layer straight
    up to the weight matrix between the penultimate layer and the output layer. A
    pre-trained model can then be seen as a sequence of nicely tuned weight matrices.
  id: totrans-91
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By replacing the final layer, we are essentially replacing the final weight
    matrix (K x 1000-dimensional, assuming K number of neurons in the penultimate
    layer) with a new randomly initialized weight matrix (K x 256-dimensional, where
    256 is the new output size).
  id: totrans-93
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: The batch normalization layer normalizes the fully connected layer outputs with
    a mean of `0` and a standard deviation of `1` across the entire batch. This is
    similar to the standard input data normalization that we perform using `torch.transforms`.
    Performing batch normalization helps limit the extent to which the hidden layer
    output values fluctuate. It also generally helps with faster learning. We can
    use higher learning rates because of a more uniform (`0` mean, `1` standard deviation)
    optimization hyperplane.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Since this is the final layer of the CNN sub-model, batch normalization helps
    insulate the LSTM sub-model against any data shifts that the CNN might introduce.
    If we do not use batch-norm, then in the worst-case scenario, the CNN final layer
    could output values with, say, mean > 0.5 and standard deviation = 1 during training.
    But during inference, if for a certain image the CNN outputs values with mean
    < 0.5 and standard deviation = 1, then the LSTM sub-model would struggle to operate
    on this unforeseen data distribution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the fully connected layer, we introduce our own layer because
    we do not need the 1,000 class probabilities of the ResNet model. Instead, we
    want to use this model to generate an embedding vector for each image. This embedding
    can be thought of as a one-dimensional, numerically encoded version of a given
    input image. This embedding is then fed to the LSTM model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore LSTMs in detail in *Chapter 4*, *Deep Recurrent Model Architectures*.
    But, as we have seen in *Figure 2.1*, the LSTM layer takes in the embedding vectors
    as input and outputs a sequence of words that should ideally describe the image
    from which the embedding was generated:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The LSTM model consists of an LSTM layer followed by a fully connected linear
    layer. The LSTM layer is a recurrent layer, which can be imagined as LSTM cells
    unfolded along the time dimension, forming a temporal sequence of LSTM cells.
    For our use case, these cells will output word prediction probabilities at each
    time-step and the word with the highest probability is appended to the output
    sentence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM cell at each time-step also generates an internal cell state, which
    is passed on as input to the LSTM cell of the next time-step. The process continues
    until an LSTM cell outputs an `<end>` token/word. The `<end>` token is appended
    to the output sentence. The completed sentence is our predicted caption for the
    image.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步的LSTM单元还会生成一个内部单元状态，这将作为下一个时间步LSTM单元的输入。这个过程持续进行，直到一个LSTM单元输出一个`<end>`标记/词。`<end>`标记将附加到输出句子中。完成的句子就是我们对图像的预测标题。
- en: Note that we also specify the maximum allowed sequence length as `20` under
    the `max_seq_len` variable. This will essentially mean that any sentence shorter
    than 20 words will have empty word tokens padded at the end and sentences longer
    than 20 words will be curtailed to just the first 20 words.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还在`max_seq_len`变量下指定了允许的最大序列长度为`20`。这基本上意味着任何少于20个词的句子将在末尾填充空词标记，而超过20个词的句子将被截断为前20个词。
- en: Why do we do it and why 20? If we truly want our LSTM to handle sentences of
    any length, we might want to set this variable to an extremely large value, say,
    9,999 words. However, (a) not many image captions come with that many words, and
    (b), more importantly, if there were ever such extra-long outlier sentences, the
    LSTM would struggle with learning temporal patterns across such a huge number
    of time-steps.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们这样做，以及为什么选择20？如果我们真的希望我们的LSTM处理任意长度的句子，我们可能会将这个变量设置为一个非常大的值，比如9999个词。然而，(a)不多的图像标题包含那么多词，而且(b)更重要的是，如果有这样的超长异常句子，LSTM将在学习跨如此多时间步的时间模式时遇到困难。
- en: We know that LSTMs are better than RNNs at dealing with longer sequences; however,
    it is difficult to retain memory across such sequence lengths. We choose `20`
    as a reasonable number given the usual image caption lengths and the maximum length
    of captions we would like our model to generate.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道LSTM在处理较长序列时比RNN更好；然而，跨这种序列长度保持记忆是困难的。考虑到通常的图像标题长度和我们希望模型生成的最大标题长度，我们选择`20`作为一个合理的数字。
- en: 'Both the LSTM layer and the linear layer objects in the previous code are derived
    from `nn.module` and we define the `__init__` and `forward` methods to construct
    the model and run a forward pass through the model, respectively. For the LSTM
    model, we additionally implement a `sample` method, as shown in the following
    code, which will be useful for generating captions for a given image:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中LSTM层和线性层对象都是从`nn.module`派生的，我们定义了`__init__`和`forward`方法来构建模型并通过模型运行前向传播。对于LSTM模型，我们还额外实现了一个`sample`方法，如下所示，用于为给定图像生成标题：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `sample` method makes use of greedy search to generate sentences; that is,
    it chooses the sequence with the highest overall probability.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample`方法利用贪婪搜索生成句子；也就是说，它选择具有最高总体概率的序列。'
- en: This brings us to the end of the image captioning model definition step. We
    are now all set to train this model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到图像标注模型定义步骤的最后。现在我们已经准备好训练这个模型了。
- en: Training the CNN-LSTM model
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练CNN-LSTM模型
- en: 'As we have already defined the model architecture in the previous section,
    we will now train the CNN-LSTM model. Let''s examine the details of this step
    one by one:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在前一节中定义了模型架构，现在我们将训练CNN-LSTM模型。让我们一步一步地检查这一步骤的详细信息：
- en: 'First, we define the device. If there is a GPU available, use it for training;
    otherwise, use the CPU:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义设备。如果有GPU可用，则用于训练；否则，使用CPU：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Although we have already reshaped all the images to a fixed shape, `(256,` `256)`,
    that is not enough. We still need to normalize the data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经将所有图像重塑为固定形状`(256, 256)`，但这还不够。我们仍然需要对数据进行归一化。
- en: FAQ - Why do we need to normalize the data?
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: FAQ - 为什么我们需要对数据进行归一化？
- en: ''
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Normalization is important because different data dimensions might have different
    distributions, which might skew the overall optimization space and lead to inefficient
    gradient descent (think of an ellipse versus a circle).
  id: totrans-115
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据归一化很重要，因为不同的数据维度可能具有不同的分布，这可能会偏移整体优化空间并导致梯度下降效率低下（想象椭圆与圆的区别）。
- en: 'We will use PyTorch''s `transform` module to normalize the input image pixel
    values:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch的`transform`模块来归一化输入图像像素值：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Furthermore, we augment the available dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们增加了可用数据集。
- en: FAQ - Why do we need data augmentation?
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: FAQ - 为什么我们需要数据增强？
- en: ''
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Augmentation helps not only in generating larger volumes of training data but
    also in making the model robust against potential variations in input data.
  id: totrans-121
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 增强不仅有助于生成更大量的训练数据，还有助于使模型在输入数据的潜在变化中变得更加健壮。
- en: 'Using PyTorch''s `transform` module, we implement two data augmentation techniques
    here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用PyTorch的`transform`模块实现了两种数据增强技术：
- en: i) Random cropping, resulting in the reduction of the image size from `(256,`
    `256)` to `(224,` `224)`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: i) 随机裁剪，将图像大小从`(256, 256)`减小为`(224, 224)`。
- en: ii) Horizontal flipping of the images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 图像的水平翻转。
- en: 'Next, we load the vocabulary that we built in the *Preprocessing caption (text)
    data* section. We also initialize the data loader using the `get_loader` function
    defined in the *Defining the image captioning data loader* section:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载在*预处理字幕（文本）数据*部分中构建的词汇表。我们还使用在*定义图像字幕数据加载器*部分中定义的`get_loader`函数初始化数据加载器：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we arrive at the main section of this step, where we instantiate the
    CNN and LSTM models in the form of encoder and decoder models. Furthermore, we
    also define the loss function – **cross entropy loss** – and the optimization
    schedule – the **Adam optimizer** – as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进入本步骤的主要部分，在这里，我们以编码器和解码器模型的形式实例化CNN和LSTM模型。此外，我们还定义损失函数 – **交叉熵损失** –
    和优化调度 – **Adam优化器** – 如下所示：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As discussed in *Chapter 1*, *Overview of Deep Learning Using PyTorch*, Adam
    is possibly the best choice for an optimization schedule when dealing with sparse
    data. Here, we are dealing with both images and text – perfect examples of sparse
    data because not all pixels contain useful information and numericized/vectorized
    text is a sparse matrix in itself.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第1章*中讨论的，使用PyTorch进行深度学习的概述中，Adam可能是处理稀疏数据时最佳选择的优化调度。在这里，我们处理图像和文本 – 这两者都是稀疏数据的完美例子，因为并非所有像素包含有用信息，而数值化/向量化的文本本身就是一个稀疏矩阵。
- en: 'Finally, we run the training loop (for five epochs) where we use the data loader
    to fetch a mini batch of the COCO dataset, run a forward pass with the mini batch
    through the encoder and decoder networks, and finally, tune the parameters of
    the CNN-LSTM model using backpropagation (backpropagation through time, for the
    LSTM network):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们运行训练循环（五个epoch），使用数据加载器获取一个CO​​CO数据集的小批量数据，通过编码器和解码器网络对小批量进行前向传递，最后使用反向传播调整CNN-LSTM模型的参数（LSTM网络的时间反向传播）：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Every 1,000 iterations into the training loop, we save a model checkpoint.
    For demonstration purposes, we have run the training for just two epochs, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每1,000次迭代进入训练循环时，我们保存一个模型检查点。为了演示目的，我们只运行了两个epoch的训练，如下所示：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will be as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.5 – Model training loop](img/file5.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 模型训练循环](img/file5.jpg)'
- en: Figure 2.5 – Model training loop
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 模型训练循环
- en: Generating image captions using the trained model
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用训练模型生成图像标题
- en: 'We have trained an image captioning model in the previous section. In this
    section, we will use the trained model to generate captions for images previously
    unseen by the model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们训练了一个图像字幕模型。在本节中，我们将使用训练好的模型为模型以前未见的图像生成字幕：
- en: 'We have stored a sample image, `sample.jpg`, to run inference on. We define
    a function to load the image and reshape it to `(224, 224)` pixels. Then , we
    define the transformation module to normalize the image pixels, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经存储了一个样本图像，`sample.jpg`，用于运行推理。我们定义一个函数来加载图像并将其重塑为`(224, 224)`像素。然后，我们定义转换模块来规范化图像像素，如下所示：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we load the vocabulary and instantiate the encoder and decoder models:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载词汇表并实例化编码器和解码器模型：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have the model scaffold ready, we will use the latest saved checkpoint
    from the two epochs of training to set the model parameters:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了模型框架，我们将使用训练的两个epoch中最新保存的检查点来设置模型参数：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After this point, the model is ready to use for inference.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，模型已准备好用于推理。
- en: 'Next, we load the image and run model-inference – that is, first we use the
    encoder model to generate an embedding from the image, and then we feed the embedding
    to the decoder network to generate sequences, as follows:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载图像并进行模型推理 – 首先使用编码器模型从图像生成嵌入，然后将嵌入传递给解码器网络以生成序列，如下所示：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'At this stage, the caption predictions are still in the form of numeric tokens.
    We need to convert the numeric tokens into actual text using the vocabulary in
    reverse:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，字幕预测仍以数字标记的形式存在。我们需要使用反向词汇表将数字标记转换为实际文本：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once we have transformed our output into text, we can visualize both the image
    as well as the generated caption:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将输出转换为文本，我们可以同时可视化图像及生成的标题：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.6 – Model inference on a sample image](img/file6.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 在样本图像上的模型推理](img/file6.jpg)'
- en: Figure 2.6 – Model inference on a sample image
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 在样本图像上的模型推理
- en: It seems that although the model is not absolutely perfect, within two epochs,
    it is already trained well enough to generate sensible captions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来虽然模型并非完美无缺，但在两个epochs内，已经训练得足够好以生成合理的标题。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed the concept of combining a CNN model and an LSTM model
    in an encoder-decoder framework, jointly training them, and using the combined
    model to generate captions for an image.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在编码器-解码器框架中结合CNN模型和LSTM模型的概念，联合训练它们，并使用组合模型为图像生成标题。
- en: We have used CNNs both in this and the previous chapter's exercises.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章和上一章的练习中都使用了CNN。
- en: In the next chapter, we will take a deeper look at the gamut of different CNN
    architectures developed over the years, how each of them is uniquely useful, and
    how they can be easily implemented using PyTorch.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨多年来开发的不同CNN架构的全貌，它们各自的独特用途，以及如何使用PyTorch轻松实现它们。
