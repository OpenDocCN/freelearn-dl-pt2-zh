["```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom transformers import TextDataset,DataCollatorForLanguageModeling\n# Get dataset\nnews = pd.read_csv('abcnews-date-text.csv')\nX_train, X_test= train_test_split(news.headline_text.tolist(),test_size=0.33, random_state=42)\n# Write the headlines from training dataset\nwith open('train_dataset.txt','w') as f:\n  for line in X_train:\n    f.write(line)\n    f.write(\"\\n\")\n# Write the headlines from testing dataset\nwith open('test_dataset.txt','w') as f:\n  for line in X_test:\n    f.write(line)\n    f.write(\"\\n\")\n# Prepare tokenizer object\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\",pad_token='<pad>')\ntrain_path = 'train_dataset.txt'\ntest_path = 'test_dataset.txt'\n# Utility method to prepare DataSet objects\ndef load_dataset(train_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=4)\n\n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=4)   \n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,test_dataset,data_collator\ntrain_dataset,test_dataset,data_collator = load_dataset(train_path, test_path, tokenizer) \nsklearn to split our dataset into training and test segments, which are then transformed into usable form using the TextDataset class. The train_dataset and test_dataset objects are simple generator objects that will beÂ used by the Trainer class to fine-tune our model. The following snippet prepares the setup for training the model:\n```", "```py\nfrom transformers import Trainer,TrainingArguments,AutoModelWithLMHead\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2\")\ntraining_args = TrainingArguments(\n    output_dir=\"./headliner\",  # The output directory\n    overwrite_output_dir=True, # overwrite the content of \n                               # the output directory\n    num_train_epochs=1,        # number of training epochs\n    per_device_train_batch_size=4, # batch size for training\n    per_device_eval_batch_size=2,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps \n                      # between two evaluations.\n    save_steps=800,   # after # steps model is saved \n    warmup_steps=500, # number of warmup steps for \n                      # learning rate scheduler\n    )\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    prediction_loss_only=True,\n) \n```", "```py\ntrainer.train() \n```", "```py\n{'loss': 6.99887060546875, 'learning_rate': 5e-05, 'epoch': 0.0010584004182798454, 'total_flos': 5973110784000, 'step': 500}\n{'loss': 6.54750146484375, 'learning_rate': 4.994702390916932e-05, 'epoch': 0.0021168008365596907, 'total_flos': 11946221568000, 'step': 1000}\n{'loss': 6.5059072265625, 'learning_rate': 4.989404781833863e-05, 'epoch': 0.003175201254839536, 'total_flos': 17919332352000, 'step': 1500}\n{'loss': 6.46778125, 'learning_rate': 4.9841071727507945e-05, 'epoch': 0.0042336016731193814, 'total_flos': 23892443136000, 'step': 2000}\n{'loss': 6.339587890625, 'learning_rate': 4.978809563667726e-05, 'epoch': 0.005292002091399226, 'total_flos': 29865553920000, 'step': 2500}\n{'loss': 6.3247421875, 'learning_rate': 4.973511954584657e-05, 'epoch': 0.006350402509679072, 'total_flos': 35838664704000, 'step': 3000} \npipeline object along with the utility function get_headline, which we need to generate headlines using this fine-tuned model:\n```", "```py\nfrom transformers import pipeline\nheadliner = pipeline('text-generation',\n                model='./headliner', \n                tokenizer='gpt2',\n                config={'max_length':8})\n# Utility method\ndef get_headline(headliner_pipeline, seed_text=\"News\"):\n  return headliner_pipeline(seed_text)[0]['generated_text'].split('\\n')[0] \n```"]