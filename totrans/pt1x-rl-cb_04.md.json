["```py\n>>> import gym\n>>> env = gym.make(\"CliffWalking-v0\")\n>>> n_state = env.observation_space.n\n>>> print(n_state)\n48\n>>> n_action = env.action_space.n\n>>> print(n_action)\n4\n```", "```py\n>>> env.reset()\n 0\n```", "```py\n>>> env.render()\n```", "```py\n>>> new_state, reward, is_done, info = env.step(2)\n>>> env.render()\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n x  C  C  C  C  C  C  C  C  C  C  T\n```", "```py\n>>> print(new_state)\n36\n>>> print(reward)\n-1\n```", "```py\n>>> print(is_done)\n False\n```", "```py\n>>> print(info)\n {'prob': 1.0}\n```", "```py\n>>> new_state, reward, is_done, info = env.step(0)\n>>> env.render()\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n x  o  o  o  o  o  o  o  o  o  o  o\n o  C  C  C  C  C  C  C  C  C  C  T\n```", "```py\n>>> print(new_state)\n 24\n```", "```py\n>>> print(reward)\n -1\n```", "```py\n>>> new_state, reward, is_done, info = env.step(1)\n>>> new_state, reward, is_done, info = env.step(2)\n>>> env.render()\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n x  C  C  C  C  C  C  C  C  C  C  T\n```", "```py\n>>> print(new_state)\n36\n>>> print(reward)\n-100\n>>> print(is_done)\nFalse\n```", "```py\n >>> new_state, reward, is_done, info = env.step(0)\n >>> for _ in range(11):\n ...     env.step(1)\n >>> new_state, reward, is_done, info = env.step(2)\n >>> env.render()\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o  o  o\n o  C  C  C  C  C  C  C  C  C  C  x\n >>> print(new_state)\n 47\n >>> print(reward)\n -1\n >>> print(is_done)\n True\n```", "```py\n>>> import torch\n>>> import gym >>> env = gym.make(\"CliffWalking-v0\")\n>>> from collections import defaultdict\n```", "```py\n>>> def gen_epsilon_greedy_policy(n_action, epsilon):\n ...     def policy_function(state, Q):\n ...         probs = torch.ones(n_action) * epsilon / n_action\n ...         best_action = torch.argmax(Q[state]).item()\n ...         probs[best_action] += 1.0 - epsilon\n ...         action = torch.multinomial(probs, 1).item()\n ...         return action\n ...     return policy_function\n```", "```py\n>>> def q_learning(env, gamma, n_episode, alpha):\n ...     \"\"\"\n ...     Obtain the optimal policy with off-policy Q-learning method\n ...     @param env: OpenAI Gym environment\n ...     @param gamma: discount factor\n ...     @param n_episode: number of episodes\n ...     @return: the optimal Q-function, and the optimal policy\n ...     \"\"\"\n ...     n_action = env.action_space.n\n ...     Q = defaultdict(lambda: torch.zeros(n_action))\n ...     for episode in range(n_episode):\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = epsilon_greedy_policy(state, Q)\n ...             next_state, reward, is_done, info = \n                                         env.step(action)\n ...             td_delta = reward + \n                             gamma * torch.max(Q[next_state])\n                             - Q[state][action]\n ...             Q[state][action] += alpha * td_delta\n ...             if is_done:\n ...                 break\n ...             state = next_state\n ...     policy = {}\n ...     for state, actions in Q.items():\n ...         policy[state] = torch.argmax(actions).item()\n ...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> n_episode = 500\n>>> alpha = 0.4\n>>> epsilon = 0.1\n```", "```py\n>>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n```", "```py\n>>> optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha) >>> print('The optimal policy:\\n', optimal_policy)\n The optimal policy:\n {36: 0, 24: 1, 25: 1, 13: 1, 12: 2, 0: 3, 1: 1, 14: 2, 2: 1, 26: 1, 15: 1, 27: 1, 28: 1, 16: 2, 4: 2, 3: 1, 29: 1, 17: 1, 5: 0, 30: 1, 18: 1, 6: 1, 19: 1, 7: 1, 31: 1, 32: 1, 20: 2, 8: 1, 33: 1, 21: 1, 9: 1, 34: 1, 22: 2, 10: 2, 23: 2, 11: 2, 35: 2, 47: 3}\n```", "```py\n>>> length_episode = [0] * n_episode\n>>> total_reward_episode = [0] * n_episode\n```", "```py\n>>> def q_learning(env, gamma, n_episode, alpha):\n ...     n_action = env.action_space.n\n ...     Q = defaultdict(lambda: torch.zeros(n_action))\n ...     for episode in range(n_episode):\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = epsilon_greedy_policy(state, Q)\n ...             next_state, reward, is_done, info = \n                                        env.step(action)\n ...             td_delta = reward + \n                        gamma * torch.max(Q[next_state])\n                        - Q[state][action]\n ...             Q[state][action] += alpha * td_delta\n ...             length_episode[episode] += 1\n ...             total_reward_episode[episode] += reward\n ...             if is_done:\n ...                 break\n ...             state = next_state\n ...      policy = {}\n ...      for state, actions in Q.items():\n ...          policy[state] = torch.argmax(actions).item()\n ...      return Q, policy\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(length_episode)\n>>> plt.title('Episode length over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Length')\n>>> plt.show()\n```", "```py\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import numpy as np\n>>> import sys\n>>> from gym.envs.toy_text import discrete\n```", "```py\n>>> UP = 0\n>>> RIGHT = 1\n>>> DOWN = 2\n>>> LEFT = 3\n```", "```py\n>>> class WindyGridworldEnv(discrete.DiscreteEnv):\n ...     def __init__(self):\n ...         self.shape = (7, 10)\n ...         nS = self.shape[0] * self.shape[1]\n ...         nA = 4\n ...         # Wind locations\n ...         winds = np.zeros(self.shape)\n ...         winds[:,[3,4,5,8]] = 1\n ...         winds[:,[6,7]] = 2\n ...         self.goal = (3, 7)\n ...         # Calculate transition probabilities and rewards\n ...         P = {}\n ...         for s in range(nS):\n ...             position = np.unravel_index(s, self.shape)\n ...             P[s] = {a: [] for a in range(nA)}\n ...             P[s][UP] = self._calculate_transition_prob(\n                                       position, [-1, 0], winds)\n ...             P[s][RIGHT] = self._calculate_transition_prob(\n                                       position, [0, 1], winds)\n ...             P[s][DOWN] = self._calculate_transition_prob(\n                                       position, [1, 0], winds)\n ...             P[s][LEFT] = self._calculate_transition_prob(\n                                       position, [0, -1], winds)\n ...         # Calculate initial state distribution\n ...         # We always start in state (3, 0)\n ...         isd = np.zeros(nS)\n ...         isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\n ...         super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)\n```", "```py\n...     def _calculate_transition_prob(self, current, \n                                                delta, winds):\n ...         \"\"\"\n ...         Determine the outcome for an action. Transition \n                                             Prob is always 1.0.\n ...         @param current: (row, col), current position \n                                                 on the grid\n ...         @param delta: Change in position for transition\n ...         @param winds: Wind effect\n ...         @return: (1.0, new_state, reward, is_done)\n ...         \"\"\"\n ...         new_position = np.array(current) + np.array(delta) \n                     + np.array([-1, 0]) * winds[tuple(current)]\n ...         new_position = self._limit_coordinates( new_position).astype(int)\n ...         new_state = np.ravel_multi_index( tuple(new_position), self.shape)\n ...         is_done = tuple(new_position) == self.goal\n ...         return [(1.0, new_state, -1.0, is_done)]\n```", "```py\n...     def _limit_coordinates(self, coord):\n ...         coord[0] = min(coord[0], self.shape[0] - 1)\n ...         coord[0] = max(coord[0], 0)\n ...         coord[1] = min(coord[1], self.shape[1] - 1)\n ...         coord[1] = max(coord[1], 0)\n ...         return coord\n```", "```py\n...     def render(self):\n ...         outfile = sys.stdout\n ...         for s in range(self.nS):\n ...             position = np.unravel_index(s, self.shape)\n ...             if self.s == s:\n ...                 output = \" x \"\n ...             elif position == self.goal:\n ...                 output = \" T \"\n ...             else:\n ...                 output = \" o \"\n ...             if position[1] == 0:\n ...                 output = output.lstrip()\n ...             if position[1] == self.shape[1] - 1:\n ...                 output = output.rstrip()\n ...                 output += \"\\n\"\n ...             outfile.write(output)\n ...         outfile.write(\"\\n\")\n```", "```py\n>>> env = WindyGridworldEnv()\n```", "```py\n>>> env.reset()\n >>> env.render()\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n x  o  o  o  o  o  o  T  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n```", "```py\n>>> print(env.step(1))\n >>> env.render()\n (31, -1.0, False, {'prob': 1.0})\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  x  o  o  o  o  o  T  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n```", "```py\n>>> print(env.step(1))\n>>> print(env.step(1))\n>>> env.render()\n (32, -1.0, False, {'prob': 1.0})\n (33, -1.0, False, {'prob': 1.0})\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  x  o  o  o  T  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n```", "```py\n>>> print(env.step(1))\n >>> env.render()\n (24, -1.0, False, {'prob': 1.0})\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  x  o  o  o  o  o\n o  o  o  o  o  o  o  T  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n o  o  o  o  o  o  o  o  o  o\n```", "```py\n>>> import torch\n>>> from windy_gridworld import WindyGridworldEnv >>> env = WindyGridworldEnv()\n```", "```py\n>>> def gen_epsilon_greedy_policy(n_action, epsilon):\n ...     def policy_function(state, Q):\n ...         probs = torch.ones(n_action) * epsilon / n_action\n ...         best_action = torch.argmax(Q[state]).item()\n ...         probs[best_action] += 1.0 - epsilon\n ...         action = torch.multinomial(probs, 1).item()\n ...         return action\n ...     return policy_function\n```", "```py\n>>> n_episode = 500\n>>> length_episode = [0] * n_episode\n>>> total_reward_episode = [0] * n_episode\n```", "```py\n>>> from collections import defaultdict\n>>> def sarsa(env, gamma, n_episode, alpha):\n ...     \"\"\"\n ...     Obtain the optimal policy with on-policy SARSA algorithm\n ...     @param env: OpenAI Gym environment\n ...     @param gamma: discount factor\n ...     @param n_episode: number of episodes\n ...     @return: the optimal Q-function, and the optimal policy\n ...     \"\"\"\n ...     n_action = env.action_space.n\n ...     Q = defaultdict(lambda: torch.zeros(n_action))\n ...     for episode in range(n_episode):\n ...         state = env.reset()\n ...         is_done = False\n ...         action = epsilon_greedy_policy(state, Q)\n ...         while not is_done:\n ...             next_state, reward, is_done, info \n                                            = env.step(action)\n ...             next_action = epsilon_greedy_policy(next_state, Q)\n ...             td_delta = reward + \n                          gamma * Q[next_state][next_action]\n                          - Q[state][action]\n ...             Q[state][action] += alpha * td_delta\n ...             length_episode[episode] += 1\n ...             total_reward_episode[episode] += reward\n ...             if is_done:\n ...                 break\n ...             state = next_state\n ...             action = next_action\n ...     policy = {}\n ...     for state, actions in Q.items():\n ...         policy[state] = torch.argmax(actions).item()\n ...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> alpha = 0.4\n>>> epsilon = 0.1\n```", "```py\n>>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n```", "```py\n>>> optimal_Q, optimal_policy = sarsa(env, gamma, n_episode, alpha) >>> print('The optimal policy:\\n', optimal_policy)\n The optimal policy:\n {30: 2, 31: 1, 32: 1, 40: 1, 50: 2, 60: 1, 61: 1, 51: 1, 41: 1, 42: 1, 20: 1, 21: 1, 62: 1, 63: 2, 52: 1, 53: 1, 43: 1, 22: 1, 11: 1, 10: 1, 0: 1, 33: 1, 23: 1, 12: 1, 13: 1, 2: 1, 1: 1, 3: 1, 24: 1, 4: 1, 5: 1, 6: 1, 14: 1, 7: 1, 8: 1, 9: 2, 19: 2, 18: 2, 29: 2, 28: 1, 17: 2, 39: 2, 38: 1, 27: 0, 49: 3, 48: 3, 37: 3, 34: 1, 59: 2, 58: 3, 47: 2, 26: 1, 44: 1, 15: 1, 69: 3, 68: 1, 57: 2, 36: 1, 25: 1, 54: 2, 16: 1, 35: 1, 45: 1}\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(length_episode)\n>>> plt.title('Episode length over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Length')\n>>> plt.show()\n```", "```py\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import gym\n>>> env = gym.make('Taxi-v2')\n>>> n_state = env.observation_space.n\n>>> print(n_state)\n 500\n>>> n_action = env.action_space.n\n>>> print(n_action)\n 6\n```", "```py\n>>> env.reset()\n 262\n```", "```py\n>>> env.render()\n```", "```py\n>>> print(env.step(3))\n (242, -1, False, {'prob': 1.0})\n>>> print(env.step(3))\n (222, -1, False, {'prob': 1.0})\n>>> print(env.step(3))\n (202, -1, False, {'prob': 1.0})\n>>> print(env.step(1))\n (102, -1, False, {'prob': 1.0})\n>>> print(env.step(1))\n (2, -1, False, {'prob': 1.0})\n>>> print(env.step(4))\n (18, -1, False, {'prob': 1.0})\n Render the environment:\n>>> env.render()\n```", "```py\n>>> print(env.step(0))\n (118, -1, False, {'prob': 1.0})\n>>> print(env.step(0))\n (218, -1, False, {'prob': 1.0})\n>>> print(env.step(0))\n (318, -1, False, {'prob': 1.0})\n>>> print(env.step(0))\n (418, -1, False, {'prob': 1.0})\n>>> print(env.step(5))\n (410, 20, True, {'prob': 1.0})\n```", "```py\n>>> env.render()\n```", "```py\n>>> import torch\n```", "```py\n>>> n_episode = 1000\n>>> length_episode = [0] * n_episode\n>>> total_reward_episode = [0] * n_episode\n```", "```py\n>>> gamma = 1\n>>> alpha = 0.4\n>>> epsilon = 0.1 >>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n```", "```py\n>>> optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha)\n```", "```py\n>>> import torch\n>>> import gym >>> env = gym.make('Taxi-v2')\n```", "```py\n>>> n_episode = 1000 >>> length_episode = [0] * n_episode\n>>> total_reward_episode = [0] * n_episode\n```", "```py\n>>> gamma = 1\n>>> alpha = 0.4\n>>> epsilon = 0.01\n```", "```py\n>>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n```", "```py\n>>> optimal_Q, optimal_policy = sarsa(env, gamma, n_episode, alpha)\n```", "```py\n>>> alpha_options = [0.4, 0.5, 0.6]\n>>> epsilon_options = [0.1, 0.03, 0.01]\n>>> n_episode = 500\n```", "```py\n>>> for alpha in alpha_options:\n ...     for epsilon in epsilon_options:\n ...         length_episode = [0] * n_episode\n ...         total_reward_episode = [0] * n_episode\n ...         sarsa(env, gamma, n_episode, alpha)\n ...         reward_per_step = [reward/float(step) for \n                               reward, step in zip(\n                            total_reward_episode, length_episode)]\n ...         print('alpha: {}, epsilon: {}'.format(alpha, epsilon))\n ...         print('Average reward over {} episodes: {}'.format( n_episode, sum(total_reward_episode) / n_episode))\n ...         print('Average length over {} episodes: {}'.format( n_episode, sum(length_episode) / n_episode))\n ...         print('Average reward per step over {} episodes: \n        {}\\n'.format(n_episode, sum(reward_per_step) / n_episode))\n```", "```py\nalpha: 0.4, epsilon: 0.1\n Average reward over 500 episodes: -75.442\n Average length over 500 episodes: 57.682\n Average reward per step over 500 episodes: -0.32510755063660324\n alpha: 0.4, epsilon: 0.03\n Average reward over 500 episodes: -73.378\n Average length over 500 episodes: 56.53\n Average reward per step over 500 episodes: -0.2761201410280632\n alpha: 0.4, epsilon: 0.01\n Average reward over 500 episodes: -78.722\n Average length over 500 episodes: 59.366\n Average reward per step over 500 episodes: -0.3561815084186654\n alpha: 0.5, epsilon: 0.1\n Average reward over 500 episodes: -72.026\n Average length over 500 episodes: 55.592\n Average reward per step over 500 episodes: -0.25355404831497264\n alpha: 0.5, epsilon: 0.03\n Average reward over 500 episodes: -67.562\n Average length over 500 episodes: 52.706\n Average reward per step over 500 episodes: -0.20602525679639022\n alpha: 0.5, epsilon: 0.01\n Average reward over 500 episodes: -75.252\n Average length over 500 episodes: 56.73\n Average reward per step over 500 episodes: -0.2588407558703358\n alpha: 0.6, epsilon: 0.1\n Average reward over 500 episodes: -62.568\n Average length over 500 episodes: 49.488\n Average reward per step over 500 episodes: -0.1700284221229244\n alpha: 0.6, epsilon: 0.03\n Average reward over 500 episodes: -68.56\n Average length over 500 episodes: 52.804\n Average reward per step over 500 episodes: -0.24794191768600077\n alpha: 0.6, epsilon: 0.01\n Average reward over 500 episodes: -63.468\n Average length over 500 episodes: 49.752\n Average reward per step over 500 episodes: -0.14350124172091722\n```", "```py\n>>> import torch >>> import gym\n>>> env = gym.make('Taxi-v2')\n```", "```py\n>>> n_episode = 3000\n>>> length_episode = [0] * n_episode\n>>> total_reward_episode = [0] * n_episode\n```", "```py\n>>> def double_q_learning(env, gamma, n_episode, alpha):\n ...     \"\"\"\n ...     Obtain the optimal policy with off-policy double \n         Q-learning method\n ...     @param env: OpenAI Gym environment\n ...     @param gamma: discount factor\n ...     @param n_episode: number of episodes\n ...     @return: the optimal Q-function, and the optimal policy\n ...     \"\"\"\n ...     n_action = env.action_space.n\n ...     n_state = env.observation_space.n\n ...     Q1 = torch.zeros(n_state, n_action)\n ...     Q2 = torch.zeros(n_state, n_action)\n ...     for episode in range(n_episode):\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = epsilon_greedy_policy(state, Q1 + Q2)\n ...             next_state, reward, is_done, info \n                                     = env.step(action)\n ...             if (torch.rand(1).item() < 0.5):\n ...                 best_next_action = torch.argmax(Q1[next_state])\n ...                 td_delta = reward + \n                           gamma * Q2[next_state][best_next_action]\n                           - Q1[state][action]\n ...                 Q1[state][action] += alpha * td_delta\n ...             else:\n ...                 best_next_action = torch.argmax(Q2[next_state])\n ...                 td_delta = reward + \n                           gamma * Q1[next_state][best_next_action]\n                           - Q2[state][action]\n ...                 Q2[state][action] += alpha * td_delta\n ...             length_episode[episode] += 1\n ...             total_reward_episode[episode] += reward\n ...             if is_done:\n ...                 break\n ...             state = next_state\n ...     policy = {}\n ...     Q = Q1 + Q2\n ...     for state in range(n_state):\n ...         policy[state] = torch.argmax(Q[state]).item()\n ...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> alpha = 0.4\n>>> epsilon = 0.1 >>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n```", "```py\n>>> optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)\n```"]