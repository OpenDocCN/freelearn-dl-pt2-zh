- en: NLP and Bots for Self-Ordering Kiosks
  prefs: []
  type: TYPE_NORMAL
- en: Language understanding has improved dramatically over the past few years. New
    algorithms and hardware have come out that have dramatically changed the effectiveness
    of the feasibility of voice-activated systems. In addition, the ability for computers
    to accurately sound like a human has achieved near perfection. Another area where
    machine learning has made large strides in the past few years is **natural language
    processing** (**NLP**), or as some would call it, language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: When you combine computer voice with language understanding, then new markets
    open up for voice-activated technologies such as smart kiosks and smart devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Wake word detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech-to-text using Microsoft Speech API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with LUIS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing smart bots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom voice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing bots with QnA Maker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wake word detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wake word detection is used to make sure that your voice-activated system does
    not behave in unexpected ways. Achieving high accuracy rates for audio is challenging.
    Background noises interfere with the main vocal commands. One way to achieve higher
    accuracy is to use an array microphone. Array microphones are used for background
    noise canceling. In this recipe, we are using the ROOBO array microphone and the
    Microsoft Speech Devices SDK. The ROOBO array microphone is ideal for voice kiosks
    because its form factor allows it to be put flat on a kiosk face.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROOBO has an Android-based compute module attached to it. Android is a
    very common platform for kiosks because it is inexpensive and has a touch-first
    interface. For this recipe, we will be using the Android version of the Microsoft
    Speech Devices SDK. The Speech Devices SDK is different from the Speech SDK. The
    Speech Devices SDK will work with both array and circular microphones, while the
    Speech SDK is for using a single microphone. The following is a photo of a ROOBO
    array microphone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e7f6646-cb71-49c5-9f6c-178901796d01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, you are going to need an Azure subscription and a ROOBO linear
    array or circular microphone. On your PC, you will also need to download and install
    Android Studio and **Vysor** for working with the ROOBO. To set up the device,
    take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and install Android Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download and install Vysor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Power on the device and connect it to your computer. There are two USB connectors:
    one labeled power and one labeled debug. Connect the power connector to a power
    source and the debug USB cable to your computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/99522e34-a31e-471c-ac73-b91daf52dff0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Open Vysor and select the device to view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8722eb4f-e4dc-4db9-97b1-b7eb554c466f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/962c9594-1e42-4f0f-abbf-7d4733f956d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have completed the device setup, let us generate a wake word. To
    generate a wake word, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [https://speech.microsoft.com/](https://speech.microsoft.com/) and click
    on Get started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7f0b4907-85b7-4156-9655-c53f2c05265b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select **New Project** and fill out the custom speech form, and then click
    **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/59ebca06-8686-4233-9ac9-70b6db84ea5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Create Model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the form with the wake word you wish to train. Then, click Next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/24bbf39f-8a67-45da-9a05-6e73371365d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Listen to and approve the pronunciations, then click Train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32e05dea-e84e-456f-9f9b-6a242f3e6491.png)'
  prefs: []
  type: TYPE_IMG
- en: The model will take 20 minutes to train. When it is done, click Download.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Android Studio, create a new project using Java:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/870b985f-823b-424f-ba8b-007192b6c9a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Gradle scripts section, change the `Gradle Voice Projects` folder and
    add the reference to the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Gradle scripts section, in the Gradle build app section, add this line
    to the dependencies section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the libraries needed for this project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main activity class, add the keys and location of the trained model.
    Also, add the microphone type; in this case, we are using a linear microphone:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the method that will display the result to the UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set audio input by using the default microphone:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the task listener for the on-complete event:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the speech settings, such as device geometry, speech region, and
    language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Set an on-complete task listener:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set on-click buttons and a keyword listener:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Microsoft Speech Devices SDK is designed to work with linear and circular
    microphone arrays. In this recipe, we created an Android application to give the
    user a UI associated with the voice. Android's touch-first interface is a common
    form factor for kiosks. We also created a wake word file in Azure's Speech Studio.
    We then retrieved the keys from our service.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Speech Devices SDK does more than create a wake word. It does speech recognition,
    language understanding, and translation. If your kiosk is going to be put in a
    place with background noise that could interfere with the main subject of your
    voice recognition, then an array microphone will be your best option.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this recipe, we mentioned that the Speech Devices SDK also
    supports circular microphones. While array microphones are designed to point directly
    at the person talking, circular microphones are designed to be put perpendicular
    to the people talking. They can help determine the direction of the person speaking
    and are often used in a multi-speaker scenario, such as diarization.
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-text using the Microsoft Speech API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft Speech Services is an ecosystem of speech-to-text, text-to-speech,
    and translation features, among others. It supports multiple languages and has
    advanced features such as customizing the speech recognition to support accents,
    proper names (such as product names), background noise, and microphone quality.
    In this recipe, we will implement the Microsoft Speech SDK using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you will need to go into the Azure portal and create a speech service.
    You will then go to the Quick start section and copy down the key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, install the Azure Speech SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the key that was generated in the *Getting ready* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the speech service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, perform continuous speech recognition by using an infinite loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, clean up and disconnect the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cognitive Services takes individual words and uses machine learning to piece
    them together into meaningful sentences. The SDK takes care of finding the microphone,
    sending the audio to Cognitive Services, and returning the results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, we are going to use language understanding to determine
    the meaning of the speech. After that, we are going to make a smart bot using
    Bot Framework, which builds upon the language understanding to give state and
    logic to the ordering kiosk. You can use speech as an input to that system.
  prefs: []
  type: TYPE_NORMAL
- en: The Microsoft Speech SDK allows you to account for accents, pronunciations,
    and sound quality through its custom speech service. You can also use Docker containers
    for environments with limited connectivity to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with LUIS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Language Understanding**, or **LUIS**, from Microsoft, is a service that
    takes text and extracts out of the text the entities, the things the sentence
    was about, the intents, and the actions of a sentence. Because having a narrow-focused
    domain helps reduce the error rate, the LUIS authorizing service helps users to
    create a pre-defined list of entities and intents for LUIS to parse.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LUIS is a product of Azure's Cognitive Services. You will need to log in to
    the Azure portal and create a LUIS resource. Then, go to [https://preview.luis.ai](https://preview.luis.ai)
    and click on New App for Conversation. Then, fill out the form for the name, language,
    and prediction resource you set up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on Entities in the side menu and add, as in our restaurant ordering
    kiosk, `Cheese burger`, `French Fries`, `Diet Pepsi`, `Milk Shake`, `Chocolate`,
    `Vanilla`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/493f15ee-e9f7-46c2-8cbf-0b2ff081c5e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you have enough entities added, you will need to add intents. Click on
    the intents, then add an intent. In our recipe, we are going to add a `Menu.Add
    item` intent. Then, we add some example sentences of how someone would order at
    a kiosk. We then click on the entities in the sentences and tag them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b69a802-ca77-4185-b4eb-80f0c138fc4f.png)'
  prefs: []
  type: TYPE_IMG
- en: When there is enough to represent the entire menu, click on the Train button
    at the upper right of the window. After it has completed training, click on the
    Publish button. After the publishing is completed, a notification will appear
    on the screen giving you the keys, endpoints, and a sample query you can put in
    your browser's URL bar to get a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Then, create a new intent for other actions someone would take in an ordering
    kiosk, such as removing items from an order or changing orders. Copy that query
    string because we are going to use it later.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `requests` library to allow us to use a web service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter your order text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Send a message to LUIS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the intents and entities from the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LUIS is a system that breaks down sentences and extracts their objects (entities)
    and actions (intents). In our recipe, we created a set of entities and intents.
    LUIS is able to extract these entities and intents from sentences even if they
    are similar to the sample phrases we typed in but not exactly the same. For example,
    the phrase *A vanilla shake would be lovely* is not something that we trained
    our model on and yet LUIS is still able to understand that this is an order for
    a vanilla milkshake.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sending text to LUIS and getting a JSON payload back is the tip of the iceberg
    for LUIS. LUIS is integrated with the Microsoft Speech SDK, meaning you can get
    entities and intents from using a microphone. You can use on-board speech recognition
    with devices such as smartphones and send just the text to LUIS. As is our *Wake
    word detection* recipe, you can use an array microphone to filter background noise
    or understand the directionality of the sound and have that integrated with LUIS.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing smart bots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to use Microsoft Bot Framework to create smart
    bots. Smart bots implement a conversation between the user and the bot. These
    conversations trigger a series of actions. Bots keep track of the conversation
    state so that it knows where it is in the conversation. Bots also keep track of
    the user state, or to be more precise, they keep track of the variables the user
    has inputted.
  prefs: []
  type: TYPE_NORMAL
- en: Bots have been used to input complex forms such as legal forms or financial
    documents. For our self-ordering kiosk scenario, we will be implementing a simple
    bot that allows someone to add food to their order. We will build upon the LUIS
    model we implemented in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test bots locally, you will need to download and install the Bot Framework
    Emulator from Microsoft. Installation instructions and links to documentation
    can be found on the GitHub page at [https://github.com/microsoft/BotFramework-Emulator](https://github.com/microsoft/BotFramework-Emulator).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will need to install the dependencies. For this project, we are using
    Python and we have a requirements file. To install the requirements, clone the
    GitHub repo for this book and navigate to the `Ch7/SmartBot` folder. Then, enter
    the following `pip install` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will install the Bot Framework components in addition to Flask, the web
    server platform that our bot will use, and `async.io`, an asynchronous library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `app.py` file and import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the Flask web server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the event loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the bot memory and conversation state as well as the user state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the URL routing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop through the LUIS and Bot Framework logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `luisbot.py` file and in the `luisbot.py` file, import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `Order` data store. This will serve as a place to hold our information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a conversation state data class. This will hold the conversation state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `LuisBot` class and initialize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'On each turn, record the current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `on_message_activity` to get the state and entities from LUIS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the step logic. This will be the set of steps we need to take to complete
    an order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bot Framework is a bot building framework developed by Microsoft. It consists
    of activities and states. There are many different types of activities, such as
    messaging, events, and end of the conversation. For keeping track of the state,
    there are two variables, which are `UserState` and `ConversationState`. The user
    state captures information the user inputted. In our example, this is the food
    order. The conversation state allows the bot to ask questions in sequential order.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bot Framework keeps track of the conversation state and user data but that is
    not limited to one conversation. You can, for example, use LUIS to determine that
    the intent may be of a different conversation. In our ordering scenario, you can
    allow users to start ordering and then allow them to ask for nutritional information
    or the current cost of their order. In addition, you can add text-to-speech to
    add a voice output for the kiosk.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom voice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The state of voice technology has come a long way in recent years. A few years
    ago, synthetic voices were easy to recognize. They all had the same voice font,
    had a robotic sound, and were monotone, so they had trouble expressing emotion.
    Today, we can create custom voice fonts and add emphasis, speed, and sentiment
    to them. In this recipe, we will go over creating a custom voice font from your
    voice or some actor's voice.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a custom voice font, we are going to use Microsoft''s Custom Voice
    service. To get started, go to [https://speech.microsoft.com/portal](https://speech.microsoft.com/portal)
    and click on Custom Voice. When on the Custom Voice page, click on New project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad50528e-6245-4426-ad58-0ac4b6ac02e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, after giving your project a name and description, it is time to upload
    some audio files for training. As of the time of writing this book, the best voice
    system, **Neural Voice**, is in private preview. This means you will have to request
    access to use it. If you can access the Neural Voice feature, you will need 1
    hour of voice data. To achieve a slightly less high-fidelity voice font, you can
    use the standard voice training system. You can provide it with as low as 1 hour
    of audio samples but to achieve high quality, you will need 8 hours of audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a new project, you will be in Microsoft Speech Studio. First,
    click on Data, and then Upload data. Then, select audio only, unless you have
    some pre-transcribed audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e28b620d-819e-42d4-81b8-26596ee41db1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, upload all of your `.mp3` files zipped into one file. Depending on the
    amount of audio you have, it may take several hours to process the audio. Then,
    select the Training tab and click on Train Model. You will have the option of
    three different training methods: Statistical parametric, Concatenative, and Neural:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7235709-b2ce-42d8-b5b6-80aef5fad1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Choose the method that is the best one available to you. Statistical parametric
    is the lowest-quality option. It also requires the least amount of data. The next
    method, Concatenative, requires several hours of audio. Finally, the highest-quality
    option is Neural, for which training can take several hours.
  prefs: []
  type: TYPE_NORMAL
- en: After training is complete, head over to the Testing tab and test your new voice.
    In the Testing tab, you can hear and download audio. You can use text to produce
    audio or **Speech Synthesis Markup Language** (**SSML**), which is an XML-based
    voice markup language. SSML allows you, if you are working with neural voices,
    to add sentiment such as cheerful and empathetic. In addition, it allows you to
    fine-tune pronunciation, emphasis, and speed.
  prefs: []
  type: TYPE_NORMAL
- en: After you have tested your custom voice, go over to the Deployment tab and deploy
    your voice. This can also take a while to process. Once it is done, go to the
    deployment information. You will need this information to send the request to
    Cognitive Services.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the variables. These are the keys and variables that we retrieved in the
    *Getting ready* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Send a request to the custom voice with the words we want it to create and
    return the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'From the response, we save the `.wav` file and then play it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used Cognitive Service's custom speech-to-text feature. Custom
    speech-to-text both has pretrained voice fonts and allows you to create your own
    custom voice fonts. Behind the scenes, it takes the voice input, then uses speech-to-text
    to parse words from the text, and then uses the set of words and voice to create
    a custom voice. After the training is complete, you can expose an endpoint to
    retrieve the audio from the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing bots with QnA Maker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft's QnA Maker is a tool that can take **frequently asked questions**
    (**FAQs**) and turn them into a set of questions and answers using language understanding,
    allowing users to ask questions differently to get an answer that matches up to
    the question. **QnA Maker** can take in a list of **tab-separated values** (**TSVs**),
    an FAQ web page, and a PDF, to name a few. In this recipe, we will use a TSV with
    questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: QnA Maker solves the fuzzy logic of interpreting speech and determining the
    user's question. As part of the Cognitive Service speech ecosystem, it can be
    incorporated easily with Bot Framework and voice to give customers a rich interactive
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before using QnA Maker, you need a series of questions and answers. You can
    either point it at a website and have it parse the questions and answers or upload
    a TSV. For this recipe, we will use a TSV. There is a sample in the Git repo for
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: To create a QnA Maker project, go to the QnA Maker portal at [https://www.qnamaker.ai/](https://www.qnamaker.ai/) and
    click on Create a Knowledge Base. It will take you through a five-step wizard
    to create a QnA bot. The first step deploys the resources in Azure. The second
    step has you choose a language and associate the bot with the new service you
    just created. You will then give your project a name and upload the files with
    questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way of adding questions and answers is to use a TSV.
    There are a few fields you will need. These are `question`, `answer`, `source`,
    and `meta`. `meta` and `source` are fields you can use to query data. For example,
    in our nutrition FAQ, we may have several different ways of understanding and
    responding to a query about the calories in a burger.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we upload and create the service, we can review what the system uploaded
    and add both questions and answers to existing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21109acb-2faa-4ebc-b232-69e7108f51c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we are going to click on the viewing options and select Show meta data.
    We are going to add audio we created with Speech Studio''s content creator. We
    introduced Speech Studio in the *Creating a* *custom voice* recipe. In the meta
    tag section, we are going to add the audio files we created with the content creator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c5f0102-20cf-4ac2-91d6-b45b6ae76e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: The next step is to select the **Save and Train** button and after your model
    has been saved, select the Test button and chat with your QnA Maker bot. Once
    you are satisfied with your QnA Maker bot, select the Publish button. After the
    training completes, QnA Maker will display `curl` commands to send a question
    to QnA Maker. From here, we will extract the keys needed to turn the request into
    a Python string.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries needed to send web requests and play sounds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the variables. The key and project URL can be found in the *Getting ready*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the data in the correct format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Send a request to the speech services at the project URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the audio from the response and play it on the speakers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the hood, QnA Maker uses machine learning to train a model based on the
    question-answer pairs. It then parses the incoming text to determine which of
    the questions the customer was asking. In our kiosk example, QnA Maker is used
    to answer simple questions such as the nutritional value of the food and the location
    of the restaurant's information.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are using the QnA Maker service to access the trained model.
    QnA Maker is accessed via `http post`. The results from QnA Maker are converted
    to sound files and played on the speakers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chit-chat is incorporated in QnA Maker. To enable it, when you create a QnA
    Maker project, there is an option for chit-chat. Chit-chat allows users to enter
    a larger set of questions and have the bot make casual conversation. There are
    several personalities for chit-chat, such as professional and conversational.
  prefs: []
  type: TYPE_NORMAL
