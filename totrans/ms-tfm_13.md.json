["```py\n    $ pip install pydantic\n    $ pip install fastapi\n    ```", "```py\n    from pydantic import BaseModel\n    class QADataModel(BaseModel):\n         question: str\n         context: str\n    ```", "```py\n    from transformers import pipeline\n    model_name = 'distilbert-base-cased-distilled-squad'\n    model = pipeline(model=model_name, tokenizer=model_name,   \n                              task='question-answering')\n    ```", "```py\n    from fastapi import FastAPI\n    app = FastAPI()\n    ```", "```py\n    @app.post(\"/question_answering\")\n    async def qa(input_data: QADataModel):\n         result = model(question = input_data.question, context=input_data.context)\n         return {\"result\": result[\"answer\"]}\n    ```", "```py\n    if __name__ == '__main__':\n               uvicorn.run('main:app', workers=1)\n    ```", "```py\n    $ python main.py\n    ```", "```py\n    $ curl --location --request POST 'http://127.0.0.1:8000/question_answering' \\\n    --header 'Content-Type: application/json' \\\n    --data-raw '{\n        \"question\":\"What is extractive question answering?\",\n        \"context\":\"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the `run_squad.py`.\"\n    }'\n    ```", "```py\n    {\"answer\":\"the task of extracting an answer from a text given a question\"}\n    ```", "```py\n    if __name__ == '__main__':\n         uvicorn.run('main:app', workers=1)\n    ```", "```py\n    FROM python:3.7\n    RUN pip install torch\n    RUN pip install fastapi uvicorn transformers\n    EXPOSE 80\n    COPY ./app /app\n    CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n    ```", "```py\n    $ docker build -t qaapi .\n    And easily start it:\n    $ docker run -p 8000:8000 qaapi\n    ```", "```py\n    from transformers import TFBertForSequenceClassification\n    model = \\ TFBertForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-imdb\", from_pt=True)\n    model.save_pretrained(\"tfx_model\", saved_model=True)\n    ```", "```py\n    $ docker pull tensorflow/serving\n    ```", "```py\n    $ docker run -d --name serving_base tensorflow/serving\n    ```", "```py\n    $ docker cp tfx_model/saved_model tfx:/models/bert\n    ```", "```py\n    $ docker commit --change \"ENV MODEL_NAME bert\" tfx my_bert_model\n    ```", "```py\n    $ docker kill tfx\n    ```", "```py\n    $ docker run -p 8501:8501 -p 8500:8500 --name bert my_bert_model\n    ```", "```py\n    import uvicorn\n    from fastapi import FastAPI\n    from pydantic import BaseModel\n    from transformers import BertTokenizerFast, BertConfig\n    import requests\n    import json\n    import numpy as np\n    tokenizer =\\\n     BertTokenizerFast.from_pretrained(\"nateraw/bert-base-uncased-imdb\")\n    config = BertConfig.from_pretrained(\"nateraw/bert-base-uncased-imdb\")\n    class DataModel(BaseModel):\n        text: str\n    app = FastAPI()\n    @app.post(\"/sentiment\")\n    async def sentiment_analysis(input_data: DataModel):\n        print(input_data.text)\n        tokenized_sentence = [dict(tokenizer(input_data.text))]\n        data_send = {\"instances\": tokenized_sentence}\n        response = \\    requests.post(\"http://localhost:8501/v1/models/bert:predict\", data=json.dumps(data_send))\n        result = np.abs(json.loads(response.text)[\"predictions\"][0])\n        return {\"sentiment\": config.id2label[np.argmax(result)]}\n    if __name__ == '__main__': \n         uvicorn.run('main:app', workers=1)\n    ```", "```py\n    $ python main.py\n    ```", "```py\n    $ pip install locust\n    ```", "```py\n    from locust import HttpUser, task\n    from random import choice\n    from string import ascii_uppercase\n    class User(HttpUser):\n        @task\n        def predict(self):\n            payload = {\"text\": ''.join(choice(ascii_uppercase) for i in range(20))}\n            self.client.post(\"/sentiment\", json=payload)\n    ```", "```py\n    $ locust -f locust_file.py\n    ```"]