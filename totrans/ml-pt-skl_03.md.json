["```py\n>>> from sklearn import datasets\n>>> import numpy as np\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, [2, 3]]\n>>> y = iris.target\n>>> print('Class labels:', np.unique(y))\nClass labels: [0 1 2] \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=1, stratify=y\n... ) \n```", "```py\n>>> print('Labels counts in y:', np.bincount(y))\nLabels counts in y: [50 50 50]\n>>> print('Labels counts in y_train:', np.bincount(y_train))\nLabels counts in y_train: [35 35 35]\n>>> print('Labels counts in y_test:', np.bincount(y_test))\nLabels counts in y_test: [15 15 15] \n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> sc = StandardScaler()\n>>> sc.fit(X_train)\n>>> X_train_std = sc.transform(X_train)\n>>> X_test_std = sc.transform(X_test) \n```", "```py\n>>> from sklearn.linear_model import Perceptron\n>>> ppn = Perceptron(eta0=0.1, random_state=1)\n>>> ppn.fit(X_train_std, y_train) \n```", "```py\n>>> y_pred = ppn.predict(X_test_std)\n>>> print('Misclassified examples: %d' % (y_test != y_pred).sum())\nMisclassified examples: 1 \n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\nAccuracy: 0.978 \n```", "```py\n>>> print('Accuracy: %.3f' % ppn.score(X_test_std, y_test))\nAccuracy: 0.978 \n```", "```py\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\ndef plot_decision_regions(X, y, classifier, test_idx=None,\n                          resolution=0.02):\n    # setup marker generator and color map\n    markers = ('o', 's', '^', 'v', '<')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    lab = lab.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class examples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0],\n                    y=X[y == cl, 1],\n                    alpha=0.8,\n                    c=colors[idx],\n                    marker=markers[idx],\n                    label=f'Class {cl}',\n                    edgecolor='black')\n    # highlight test examples\n    if test_idx:\n        # plot all examples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                    c='none', edgecolor='black', alpha=1.0,\n                    linewidth=1, marker='o',\n                    s=100, label='Test set') \n```", "```py\n>>> X_combined_std = np.vstack((X_train_std, X_test_std))\n>>> y_combined = np.hstack((y_train, y_test))\n>>> plot_decision_regions(X=X_combined_std,\n...                       y=y_combined,\n...                       classifier=ppn,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> def sigmoid(z):\n...     return 1.0 / (1.0 + np.exp(-z))\n>>> z = np.arange(-7, 7, 0.1)\n>>> sigma_z = sigmoid(z)\n>>> plt.plot(z, sigma_z)\n>>> plt.axvline(0.0, color='k')\n>>> plt.ylim(-0.1, 1.1)\n>>> plt.xlabel('z')\n>>> plt.ylabel('$\\sigma (z)$')\n>>> # y axis ticks and gridline\n>>> plt.yticks([0.0, 0.5, 1.0])\n>>> ax = plt.gca()\n>>> ax.yaxis.grid(True)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n:\n```", "```py\n>>> def loss_1(z):\n...     return - np.log(sigmoid(z))\n>>> def loss_0(z):\n...     return - np.log(1 - sigmoid(z))\n>>> z = np.arange(-10, 10, 0.1)\n>>> sigma_z = sigmoid(z)\n>>> c1 = [loss_1(x) for x in z]\n>>> plt.plot(sigma_z, c1, label='L(w, b) if y=1')\n>>> c0 = [loss_0(x) for x in z]\n>>> plt.plot(sigma_z, c0, linestyle='--', label='L(w, b) if y=0')\n>>> plt.ylim(0.0, 5.1)\n>>> plt.xlim([0, 1])\n>>> plt.xlabel('$\\sigma(z)$')\n>>> plt.ylabel('L(w, b)')\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nclass LogisticRegressionGD:\n    \"\"\"Gradient descent-based logistic regression classifier.\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after training.\n    b_ : Scalar\n      Bias unit after fitting.\n    losses_ : list\n      Mean squared error loss function values in each epoch.\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the \n          number of examples and n_features is the \n          number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n        Returns\n        -------\n        self : Instance of LogisticRegressionGD\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = np.float_(0.)\n        self.losses_ = []\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean()\n            loss = (-y.dot(np.log(output))\n                   - ((1 - y).dot(np.log(1 - output)))\n                    / X.shape[0])\n            self.losses_.append(loss)\n        return self\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_\n    def activation(self, z):\n        \"\"\"Compute logistic sigmoid activation\"\"\"\n        return 1\\. / (1\\. + np.exp(-np.clip(z, -250, 250)))\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0) \n```", "```py\n>>> X_train_01_subset = X_train_std[(y_train == 0) | (y_train == 1)]\n>>> y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n>>> lrgd = LogisticRegressionGD(eta=0.3,\n...                             n_iter=1000,\n...                             random_state=1)\n>>> lrgd.fit(X_train_01_subset,\n...          y_train_01_subset)\n>>> plot_decision_regions(X=X_train_01_subset,\n...                       y=y_train_01_subset,\n...                       classifier=lrgd)\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> lr = LogisticRegression(C=100.0, solver='lbfgs',\n...                         multi_class='ovr')\n>>> lr.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined,\n...                       classifier=lr,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> lr.predict_proba(X_test_std[:3, :]) \n```", "```py\narray([[3.81527885e-09, 1.44792866e-01, 8.55207131e-01],\n       [8.34020679e-01, 1.65979321e-01, 3.25737138e-13],\n       [8.48831425e-01, 1.51168575e-01, 2.62277619e-14]]) \n```", "```py\n>>> lr.predict_proba(X_test_std[:3, :]).argmax(axis=1) \n```", "```py\narray([2, 0, 0]) \n```", "```py\n>>> lr.predict(X_test_std[:3, :])\narray([2, 0, 0]) \n```", "```py\n>>> lr.predict(X_test_std[0, :].reshape(1, -1))\narray([2]) \n```", "```py\n>>> weights, params = [], []\n>>> for c in np.arange(-5, 5):\n...     lr = LogisticRegression(C=10.**c,\n...                             multi_class='ovr')\n...     lr.fit(X_train_std, y_train)\n...     weights.append(lr.coef_[1])\n...     params.append(10.**c)\n>>> weights = np.array(weights)\n>>> plt.plot(params, weights[:, 0],\n...          label='Petal length')\n>>> plt.plot(params, weights[:, 1], linestyle='--',\n...          label='Petal width')\n>>> plt.ylabel('Weight coefficient')\n>>> plt.xlabel('C')\n>>> plt.legend(loc='upper left')\n>>> plt.xscale('log')\n>>> plt.show() \n```", "```py\n>>> from sklearn.svm import SVC\n>>> svm = SVC(kernel='linear', C=1.0, random_state=1)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined,\n...                       classifier=svm,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.linear_model import SGDClassifier\n>>> ppn = SGDClassifier(loss='perceptron')\n>>> lr = SGDClassifier(loss='log')\n>>> svm = SGDClassifier(loss='hinge') \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> np.random.seed(1)\n>>> X_xor = np.random.randn(200, 2)\n>>> y_xor = np.logical_xor(X_xor[:, 0] > 0,\n...                        X_xor[:, 1] > 0)\n>>> y_xor = np.where(y_xor, 1, 0)\n>>> plt.scatter(X_xor[y_xor == 1, 0],\n...             X_xor[y_xor == 1, 1],\n...             c='royalblue', marker='s',\n...             label='Class 1')\n>>> plt.scatter(X_xor[y_xor == 0, 0],\n...             X_xor[y_xor == 0, 1],\n...             c='tomato', marker='o',\n...             label='Class 0')\n>>> plt.xlim([-3, 3])\n>>> plt.ylim([-3, 3])\n>>> plt.xlabel('Feature 1')\n>>> plt.ylabel('Feature 2')\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\n>>> svm.fit(X_xor, y_xor)\n>>> plot_decision_regions(X_xor, y_xor, classifier=svm)\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined, classifier=svm,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined, classifier=svm,\n...                       test_idx=range(105,150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> def entropy(p):\n...     return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n>>> x = np.arange(0.0, 1.0, 0.01)\n>>> ent = [entropy(p) if p != 0 else None for p in x]\n>>> plt.ylabel('Entropy')\n>>> plt.xlabel('Class-membership probability p(i=1)')\n>>> plt.plot(x, ent)\n>>> plt.show() \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> def gini(p):\n...     return p*(1 - p) + (1 - p)*(1 - (1-p))\n>>> def entropy(p):\n...     return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n>>> def error(p):\n...     return 1 - np.max([p, 1 - p])\n>>> x = np.arange(0.0, 1.0, 0.01)\n>>> ent = [entropy(p) if p != 0 else None for p in x]\n>>> sc_ent = [e*0.5 if e else None for e in ent]\n>>> err = [error(i) for i in x]\n>>> fig = plt.figure()\n>>> ax = plt.subplot(111)\n>>> for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err],\n...                           ['Entropy', 'Entropy (scaled)',\n...                            'Gini impurity',\n...                            'Misclassification error'],\n...                           ['-', '-', '--', '-.'],\n...                           ['black', 'lightgray',\n...                            'red', 'green', 'cyan']):\n...     line = ax.plot(x, i, label=lab,\n...                   linestyle=ls, lw=2, color=c)\n>>> ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n...           ncol=5, fancybox=True, shadow=False)\n>>> ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n>>> ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n>>> plt.ylim([0, 1.1])\n>>> plt.xlabel('p(i=1)')\n>>> plt.ylabel('impurity index')\n>>> plt.show() \n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> tree_model = DecisionTreeClassifier(criterion='gini',\n...                                     max_depth=4,\n...                                     random_state=1)\n>>> tree_model.fit(X_train, y_train)\n>>> X_combined = np.vstack((X_train, X_test))\n>>> y_combined = np.hstack((y_train, y_test))\n>>> plot_decision_regions(X_combined,\n...                       y_combined,\n...                       classifier=tree_model,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('Petal length [cm]')\n>>> plt.ylabel('Petal width [cm]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn import tree\n>>> feature_names = ['Sepal length', 'Sepal width',\n...                  'Petal length', 'Petal width']\n>>> tree.plot_tree(tree_model,\n...                feature_names=feature_names,\n...                filled=True)\n>>> plt.show() \n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> forest = RandomForestClassifier(n_estimators=25,\n...                                 random_state=1,\n...                                 n_jobs=2)\n>>> forest.fit(X_train, y_train)\n>>> plot_decision_regions(X_combined, y_combined,\n...                       classifier=forest, test_idx=range(105,150))\n>>> plt.xlabel('Petal length [cm]')\n>>> plt.ylabel('Petal width [cm]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn = KNeighborsClassifier(n_neighbors=5, p=2,\n...                            metric='minkowski')\n>>> knn.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std, y_combined,\n...                       classifier=knn, test_idx=range(105,150))\n>>> plt.xlabel('Petal length [standardized]')\n>>> plt.ylabel('Petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```"]