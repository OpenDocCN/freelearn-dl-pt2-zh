- en: Natural Language Processing with Sequence Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be looking at different representations of text data
    that are useful for building deep learning models. We will be helping you to understand
    **recurrent neural networks** (**RNNs**). This chapter will cover different implementations
    of RNNs, such as **long short-term memory** (**LSTM**) and **gated recurrent unit**
    (**GRU**), which power most of the deep learning models for text and sequential
    data. We will be looking at different representations of text data and how they
    are useful for building deep learning models. In addition, this chapter will address
    one-dimensional convolutions that can be used for sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the applications that can be built using RNNs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document classifiers**: Identifying the sentiment of a tweet or review, classifying
    news articles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence-to-sequence learning**: For tasks such as language translations,
    converting English to French'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-series forecasting**: Predicting the sales of a store when given details
    about previous days'' sales records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training embedding by building a sentiment classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pretrained word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving text classification problem using LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional network on sequence data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text is one of the most commonly used sequential data types. Text data can
    be seen as either a sequence of characters or a sequence of words. It is common
    to see text as a sequence of words for most problems. Deep learning sequential
    models such as RNNs and its variants are able to learn important patterns from
    text data that can solve problems in areas such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These sequential models also act as important building blocks for various systems,
    such as **question and answerÂ **(**QA**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Though these models are highly useful in building these applications, they do
    not have an understanding of human language, due to its inherent complexities.
    These sequential models are able to successfully find useful patterns that are
    then used for performing different tasks. Applying deep learning to text is a
    fast-growing field, and a lot of new techniques arrive every month. We will cover
    the fundamental components that power most modern-day deep learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models, like any other machine learning model, do not understand
    text, so we need to convert text into numerical representations. The process of
    converting text into numerical representations is called **vectorization** and
    can be done in different ways, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert text into words and represent each word as a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert text into characters and represent each character as a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create n-grams of words and represent them as vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text data can be broken down into one of these representations. Each smaller
    unit of text is called a token, and the process of breaking text into tokens is
    called **tokenization**. There are a lot of powerful libraries available in Python
    that can help us in tokenization. Once we convert the text data into tokens, we
    then need to map each token to a vector. One-hot encoding and word embedding are
    the two most popular approaches for mapping tokens to vectors. The following diagram
    summarizes the steps for converting text into vector representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e7d9d7-448e-467e-864f-20f9f0fd4e44.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look in more detail at tokenization, n-gram representation, and vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a sentence, splitting it into either characters or words is called tokenization.
    There are libraries, such as spaCy, that offer complex solutions to tokenization.
    Let's use simple Python functions such as `split` and `list` to convert the text
    into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how tokenization works on characters and words, let''s consider
    a small review of the movie *Toy Story*. We will work with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Converting text into characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python `list` function takes a string and converts it into a list of individual
    characters. This does the job of converting the text into characters. The following
    code block shows the code used and the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This result shows how our simple Python function has converted text into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text into words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `split` function available in the Python string object to break
    the text into words. The `split` function takes an argument, and based on this,
    it splits the text into tokens. For our example, we will use spaces as delimiters.
    The following code block demonstrates how we can convert text into words using
    the Python `split` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we did not use any separator; by default, the `split`
    function splits on white spaces.
  prefs: []
  type: TYPE_NORMAL
- en: N-gram representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen how text can be represented as characters and words. Sometimes,
    it is useful to look at two, three, or more words together. **N-grams** are groups
    of words extracted from the given text. In an n-gram, *n* represents the number
    of words that can be used together. Let''s look at an example of what a bigram
    (*n=2*) looks like. We used the Python `nltk` package to generate a bigram for
    `toy_story_review`. The following code block shows the result of the bigram and
    the code used to generate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ngrams` function accepts a sequence of words as its first argument and
    the number of words to be grouped as the second argument. The following code block
    shows how a trigram representation would look, and the code used for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The only thing that changed in the preceding code is the *n* value, the second
    argument to the function.
  prefs: []
  type: TYPE_NORMAL
- en: Many supervised machine learning models, for example, Naive Bayes, use n-grams
    to improve their feature space. N-grams are also used for spelling correction
    and text summarization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge with n-gram representation is that it loses the sequential nature
    of text. It is often used with shallow machine learning models. This technique
    is rarely used in deep learning, as architectures such as RNN and Conv1D, learn
    these representations automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two popular approaches to mapping the generated tokens to vectors
    of numbers, called one-hot encoding and word embedding. Let's understand how tokens
    can be converted to these vector representations by writing a simple Python program.
    We will also discuss the various pros and cons of each method.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In one-hot encoding, each token is represented by a vector of length *N*, where
    *N* is the size of the vocabulary. The vocabulary is the total number of unique
    words in the document. Let''s take a simple sentence and observe how each token
    would be represented as one-hot encoded vectors. The following is the sentence
    and its associated token representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An apple a day keeps doctor away said the doctor.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'One-hot encoding for the preceding sentence can be represented in a tabular
    format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| An | 100000000 |'
  prefs: []
  type: TYPE_TB
- en: '| apple | 10000000 |'
  prefs: []
  type: TYPE_TB
- en: '| a | 1000000 |'
  prefs: []
  type: TYPE_TB
- en: '| day | 100000 |'
  prefs: []
  type: TYPE_TB
- en: '| keeps | 10000 |'
  prefs: []
  type: TYPE_TB
- en: '| doctor | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '| away | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| said | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'This table describes the tokens and their one-hot encoded representation. The
    vector length is nine, as there are nine unique words in the sentence. A lot of
    machine learning libraries have eased the process of creating one-hot encoding
    variables. We will write our own implementation to make it easier to understand,
    and we can use the same implementation to build other features required for later
    examples. The following code contains a `Dictionary` class, which contains functionality
    to create a dictionary of unique words along with a function to return a one-hot
    encoded vector for a particular word. Let''s take a look at the code and then
    walk through each functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides three important functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: The initialization,Â `init`, function creates a `word2index` dictionary, which
    will store all unique words along with the index. The `index2word` list stores
    all the unique words and the `length` variable contains the total number of unique
    words in our document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `add_word` function takes a word and adds it to `word2index` and `index2word`,
    and increases the length of the vocabulary, provided the word is unique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `onehot_encoded` function takes a word and returns a vector of length *N*
    with zeros throughout, except at the index of the word. If the index of the passed
    word is two, then the value of the vector at index two will be one, and all the
    remaining values will be zeros.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we have defined our `Dictionary` class, let''s use it on our `toy_story_review`
    data. The following code demonstrates how the `word2index` dictionary is built
    and how we can call our `onehot_encoded` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One of the challenges with one-hot representation is that the data is too sparse,
    and the size of the vector quickly grows as the number of unique words in the
    vocabulary increases. Another limitation is that one-hot has no representation
    of internal relations between words. For these reasons, one-hot is rarely used
    with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embedding is a very popular way of representing text data in problems that
    are solved by deep learning algorithms. Word embedding provides a dense representation
    of a word filled with floating numbers. The vector dimension varies according
    to the vocabulary size. It is common to use a word embedding of dimension size
    50, 100, 256, 300, and sometimes 1,000\. The dimension size is a hyperparameter
    that we need to play with during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: If we are trying to represent a vocabulary of size 20,000 in one-hot representation,
    then we will end up with 20,000 x 20,000 numbers, most of which will be zero.
    The same vocabulary can be represented in a word embedding as 20,000 x dimension
    size, where the dimension size could be 10, 50, 300, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to create word embeddings is to start with dense vectors for each token
    containing random numbers and then train a model, such as a document classifier,
    for sentiment classification. The floating point numbers, which represent the
    tokens, will get adjusted in such a way that semantically closer words will have
    similar representation. To understand it, let''s look at the following diagram,
    where we plotted the word embedding vectors on a two-dimensional plot of five
    movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd68a722-b8a3-4dfb-bd5e-d753a5d2c3e4.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows how the dense vectors are tuned in order to have
    smaller distances for words that are semantically similar. Since movie titles
    such as *Finding Nemo*, *Toy Story*, and *The Incredibles* are fictional movies
    with cartoons, the embedding for such words is closer. On the other hand, the
    embedding for the movie *Titanic* is far from the cartoons and closer to the movie
    title *Notebook*, since they are romantic movies.
  prefs: []
  type: TYPE_NORMAL
- en: Learning word embedding may not be feasible when you have too little data, and
    in those cases, we can use word embeddings that are trained by some other machine
    learning algorithm. An embedding generated from another task is called a pretrained
    word embedding. We will learn how to build our own word embeddings and use pretrained
    word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Training word embedding by building a sentiment classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we briefly learned about word embedding without implementing
    it. In this section, we will download a dataset called IMDb, which contains reviews,
    and build a sentiment classifier that calculates whether a review's sentiment
    is positive, negative, or unknown. In the process of building, we will also train
    word embedding for the words present in the IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a library called `torchtext`, which makes a lot of processes such
    as downloading, text vectorization, and batching, much easier. Training a sentiment
    classifier will involve the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading IMDb data and performing text tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a vocabulary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating batches of vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a network model with embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We shall see these steps in more detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading IMDb data and performing text tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For applications related to computer vision, we used the `torchvision` library,
    which provides us with a lot of utility functions, helping to build computer vision
    applications. In the same way, there is a library called `torchtext`, which is
    built to work with PyTorch and eases a lot of activities related to **natural
    language processing** (**NLP**) by providing different data loaders and abstractions
    for text. At the time of writing, `torchtext` does not come with the standard
    PyTorch installation and requires a separate installation. You can run the following
    code in the command line of your machine to get `torchtext` installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once it is installed, we will be able to use it. The `torchtext` download provides
    two important modules called `torchtext.data` and `torchtext.datasets`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download the IMDb Movies dataset from the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/).'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing with torchtext.data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext.data` instance defines a `Field` class, which helps us to define
    how the data has to be read and tokenized. Let''s look at the following example,
    which we will use for preparing our IMDb dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define two `Field` objects, one for actual text and
    another for the label data. For actual text, we expect `torchtext` to lowercase
    all the text, tokenize the text, and trim it to a maximum length of 20\. If we
    are building an application for a production environment, we may fix the length
    to a much larger number. However, for the toy example, this works well. The `Field`
    constructor also accepts another argument called `tokenize`, which by default
    uses the `str.split` function. We can also specify `spaCy` as the argument, or
    any other tokenizer. For our example, we will stick with `str.split`.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing with torchtext.datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext.datasets` instance provides wrappers for using different datasets
    such as IMDb, TREC (question classification), language modeling (WikiText-2),
    and a few other datasets. We will use `torch.datasets` to download the IMDb dataset
    and split it into train and test datasets. The following code does that and when
    you run it for the first time, it could take several minutes (depending on your
    broadband connection) as it downloads the IMDb dataset from the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous dataset''s `IMDB` class abstracts away all the complexity involved
    in downloading, tokenizing, and splitting the database into train and test datasets.
    The `train.fields` download contains a dictionary where `TEXT` is the key and
    `LABEL` is the value. Let''s look at `train.fields` and what each element of `train`
    contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the variance for the training dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see from these results that a single element contains a field and text,
    along with all the tokens representing the text, and a `label` field that contains
    the label of the text. Now we have the IMDb dataset ready for batching.
  prefs: []
  type: TYPE_NORMAL
- en: Building vocabulary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we created one-hot encoding for `toy_story_review`, we created a `word2index`
    dictionary, which is referred to as the vocabulary since it contains all the details
    of the unique words in the documents. The `torchtext` instance makes that easier
    for us. Once the data is loaded, we can call `build_vocab` and pass the necessary
    arguments that will take care of building the vocabulary for the data. The following
    code shows how the vocabulary is built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we pass in the `train` object on which we need to build
    the vocabulary, and we also ask it to initialize vectors with pretrained embeddings
    of dimensions `300`. The `build_vocab` object just downloads and creates the dimension
    that will be used later when we train the sentiment classifier using pretrained
    weights. The `max_size` instance limits the number of words in the vocabulary,
    and `min_freq` removes any word that has not occurred more than 10 times, where
    `10` is configurable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the vocabulary is built, we can obtain different values such as frequency,
    word index, and the vector representation for each word. The following code demonstrates
    how to access these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code demonstrates how to access the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will print the values for a dictionary containing words and their
    indexes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `stoi` value gives access to a dictionary containing words and their indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Generating batches of vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext` download provides `BucketIterator`, which helps in batching
    all the text and replacing the words with the index number of the words. The `BucketIterator`
    instance comes with a lot of useful parameters such as `batch_size`, `device`
    (GPU or CPU), and `shuffle` (whether data has to be shuffled). The following code
    demonstrates how to create iterators that generate batches for the train and test
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives a `BucketIterator` object for both train and test
    datasets. The following code shows how to create a batch and display the results
    of the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will print the labels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: From the results in the preceding code block, we can see how the text data is
    converted into a matrix of size `(batch_size` * `fix_len`), which is (128 x 20).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a network model with embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed word embeddings briefly earlier. In this section, we create word
    embeddings as part of our network architecture and train the entire model to predict
    the sentiment of each review. At the end of the training, we will have a sentiment
    classifier model and also the word embeddings for the IMDB datasets. The following
    code demonstrates how to create a network architecture to predict the sentiment
    using word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `EmbeddingNetwork` creates the model for sentiment classification.
    Inside the `_init_` function, we initialize an object of the `nn.Embedding` class,
    which takes two arguments, namely, the size of the vocabulary and the dimensions
    that we wish to create for each word. As we have limited the number of unique
    words, the vocabulary size will be 10,000 and we can start with a small embedding
    size of 10\. For running the program quickly, a small embedding size is useful,
    but when you are trying to build applications for production systems, use embeddings
    of a large size. We also have a linear layer that maps the word embeddings to
    the category (positive, negative, or unknown).
  prefs: []
  type: TYPE_NORMAL
- en: The forward function determines how the input data is processed. For a batch
    size of 32 and sentences of a maximum length of 20 words, we will have inputs
    of the shape 32 x 20\. The first embedding layer acts as a lookup table, replacing
    each word with the corresponding embedding vector. For an embedding dimension
    of 10, the output becomes 32 x 20 x 10 as each word is replaced with its corresponding
    embedding. The `view()` function will flatten the result from the embedding layer.
    The first argument passed to view will keep that dimension intact.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we do not want to combine data from different batches, so we preserve
    the first dimension and flatten the rest of the values in the tensor. After the
    `view()` function is applied, the tensor shape changes to 32 x 200\. A dense layer
    maps the flattened embeddings to the number of categories. Once the network is
    defined, we can train the network as usual.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in this network, we lose the sequential nature of the text and
    we just use the text as a bag of words.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training the model is very similar to what we saw for building image classifiers,
    so we will be using the same functions. We pass batches of data through the model,
    calculate the outputs and losses, and then optimize the model weights, which includes
    the embedding weights. The following code does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we iterate over the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can train the model over each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we call the `fit` method by passing the `BucketIterator`
    object that we created for batching the data. The iterator, by default, does not
    stop generating batches, so we have to set the repeat variable of the `BucketIterator`
    object to `False`. If we don't set the repeat variable to `False`, then the `fit`
    function will run indefinitely. Training the model for around 10 epochs gives
    a validation accuracy of approximately 70%. Now that you've learned how to train
    word embedding by building sentiment classifier, let us learn how to use pretrained
    word embeddings in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pretrained word embeddings are useful when we are working in specific domains,
    such as medicine and manufacturing, where we have a lot of data to train the embeddings.
    When we have little data and we cannot meaningfully train the embeddings, we can
    use embeddings that are trained on different data corpuses such as Wikipedia,
    Google News, and Twitter tweets. A lot of teams have open source word embeddings
    trained using different approaches. In this section, we will explore how `torchtext`
    makes it easier to use different word embeddings, and how to use them in our PyTorch
    models. It is similar to transfer learning, which we use in computer vision applications.
    Typically, using pretrained embedding would involve the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the embeddings in the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Freezing the embedding layer weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's explore in detail how each step is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext` library abstracts away a lot of complexity involved in downloading
    the embeddings and mapping them to the right word. The `torchtext` library provides
    three classes in the `vocab` module, namely, GloVe, FastText, CharNGram, which
    ease the process of downloading embeddings and mapping them to our vocabulary.
    Each of these classes provides different embeddings trained on different datasets
    and using different techniques. Let''s look at some of the different embeddings
    provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '`charngram.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fasttext.en.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fasttext.simple.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.42B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.840B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.25d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.50d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `glove.twitter.27B.200d` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.50d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.200d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `build_vocab` method of the `Field` object takes in an argument for the
    embeddings. The following code explains how we download the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The value to the argument vector denotes what embedding class is to be used.
    The `name` and `dim` arguments determine which embeddings can be used. We can
    easily access the embeddings from the `vocab` object. The following code demonstrates
    it, along with a view of how the results will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now we have downloaded and mapped the embeddings to our vocabulary. Let's understand
    how we can use them with a PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the embeddings in the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `vectors` variable returns a torch tensor of shape `vocab_size` x `dimensions`
    containing the pretrained embeddings. We have to store the embeddings to the weights
    of our embedding layer. We can assign the weights of the embeddings by accessing
    the weights of the embeddings layer as demonstrated by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model` download represents the object of our network, and `embedding`
    represents the embedding layer. As we are using the embedding layer with new dimensions,
    there will be a small change in the input to the linear layer that comes after
    the embedding layer. The following code has the new architecture, which is similar
    to the previously used architecture where we trained our embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Once the embeddings are loaded, we have to ensure that, during training, we
    do not change the embedding weights. Let's discuss how to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: Freezing the embedding layer weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a two-step process to tell PyTorch not to change the weights of the embedding
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the `requires_grad` attribute to `False`, which instructs PyTorch that it
    does not need gradients for these weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the passing of the embedding layer parameters to the optimizer. If this
    step is not done, then the optimizer throws an error, as it expects all the parameters
    to have gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates how easy it is to freeze the embedding layer
    weights and instruct the optimizer not to use those parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We generally pass all the model parameters to the optimizer, but in the previous
    code, we passed parameters that have `requires_grad` as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: We can train the model using this exact code and should achieve similar accuracy.
    All these model architectures fail to take advantage of the sequential nature
    of the text. In the next section, we explore two popular techniques, namely, RNN
    and Conv1D, which take advantage of the sequential nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are among the most powerful models that enable us to take on applications
    such as classification, labeling of sequential data, generating sequences of text
    (such as with the SwiftKey Keyboard app, which predicts the next word), and converting
    one sequence to another, such as when translating a language (for example, from
    French to English). Most of the model architectures, such as feedforward neural
    networks, do not take advantage of the sequential nature of data. For example,
    we need the data to present the features of each example in a vector, say all
    the tokens that represent a sentence, paragraph, or documents. Feedforward networks
    are designed just to look at all the features once and map them to output. Let's
    look at a text example that shows why the order, or sequential nature of text,
    is important. *I had cleaned my car* and *I had my car cleaned* are two English
    sentences with the same set of words, but they mean different things when we consider
    the order of the words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most modern languages, humans make sense of text data by reading words from
    left to right and building a powerful model that kind of understands all the different
    things the text says. RNN works similarly by looking at one word in text at a
    time. RNN is also a neural network that has a special layer in it, which loops
    over the data instead of processing all at once. As RNNs can process data in sequence,
    we can use vectors of different lengths and generate outputs of different lengths.
    Some of the different representations are provided in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5536b92-c0fc-446f-bdbc-b1e77ee93f23.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram is from one of the famous blogs on RNN ([http://karpathy.github.
    io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness)),
    in which the author, Andrej Karpathy, writes about how to build an RNN from scratch
    with Python and using it as a sequence generator.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how RNN works with an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with the assumption that we have an RNN model already built and
    let's try to understand what functionality it provides. Once we understand what
    an RNN does, let's then explore what happens inside an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the *Toy Story* review as input to the RNN model. The example
    text we are looking at is *Just perfect. Script, character, animation....this
    manages to break free....*. We start by passing the first word, *just* to our
    model, and the model generates two different things: a **State Vector** and an
    **Output** vector. The **State Vector** is passed to the model when it processes
    the next word in the review, and a new **State Vector** is generated. We just
    consider the **Output** of the model generated during the last sequence. The following
    diagram summarizes it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f19b5d5-4e41-410e-bf89-0d4057d2b74c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram demonstrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How RNN works by unfolding the text input and the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the state is recursively passed to the same model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By now, you will have an idea of what RNN does, but not how it works. Before
    we get into how it works, let''s look at a code snippet that showcases in more
    detail what we have learned. We will still view RNN as a black box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `hidden` variable represents the state vector, sometimes
    called the **hidden state**. By now, we should have an idea of how RNN is used.
    Now, let''s look at the code that implements RNN and understand what happens inside
    the RNN. The following code contains the `RNN` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Except for the word RNN in the preceding code, everything else would sound pretty
    similar to what we have used in the previous chapters, as PyTorch hides a lot
    of complexity of backpropagation. Let's walk through the `__init__` function and
    the `forward` function to understand what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` function initializes two linear layers, one for calculating the
    output and the other for calculating the state or hidden vector.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward` function combines the input vector and the hidden vector and passes
    it through the two linear layers, which generates an output vector and a hidden
    state. For the output layer, we apply a `log_softmax` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `initHidden` function helps in creating hidden vectors with no state for
    calling RNN the very first time. Let''s take a visual look into what the RNN class
    does in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/488bd1e1-0128-4c9d-8375-0ca89bb15457.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows how an RNN works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concepts of RNN are sometimes tricky to understand when you meet them for
    the first time, so I would strongly recommend some of the amazing blogs provided
    in the following links: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    and [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to use a variant of RNN called LSTM to
    build a sentiment classifier on the IMDB dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Solving text classification problem using LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are quite popular in building real-world applications, such as language
    translation, text classification, and many more sequential problems. However,
    in reality, we would rarely use a vanilla version of RNN, such as the one we saw
    in the previous section. The vanilla version of RNN has problems, such as vanishing
    gradients and gradient explosion when dealing with large sequences. In most of
    the real-world problems, variants of RNN such as LSTM or GRU are used, which solve
    the limitations of plain RNN and also have the ability to handle sequential data
    better. We will try to understand what happens in LSTM and build a network based
    on LSTM to solve the text classification problem on the IMDB datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term dependency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RNNs, in theory, should learn all the dependency required from the historical
    data to build a context of what happens next. Say, for example, we are trying
    to predict the last word in the sentence *The clouds are in the sky*. RNN would
    be able to predict it, as the information (clouds) is just a few words behind.
    Let''s take another long paragraph where the dependency need not be that close,
    and we want to predict the last word in it. The sentence is: *I am born in Chennai
    a city in Tamilnadu. Did schooling in different states of India and I speak...*
    The vanilla version of RNN, in practice, finds it difficult to remember the context
    that happened in the earlier parts of sequences. LSTMs, and other different variants
    of RNN, solve this problem by adding different neural networks inside the LSTM,
    which later decide how much, or what data can be remembered.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs are a special kind of RNN, capable of learning long-term dependency. They
    were introduced in 1997 and got popular in the last few years with advancements
    in available data and hardware. They work tremendously well on a large variety
    of problems and are widely used.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are designed to avoid long-term dependency problems by having a design
    by which it is natural to remember information for a long period of time. In RNNs,
    we saw how they repeat themselves over each element of the sequence. In standard
    RNNs, the repeating module will have a simple structure like a single linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how a simple RNN repeats itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c492b55a-f731-4b2e-a0a0-015848b3a8cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inside LSTM, instead of using a simple linear layer, we have smaller networks
    inside the LSTM, which do an independent job. The following diagram showcases
    what happens inside an LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5122dbe6-0dd9-4680-91a4-6d898baa8669.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    (diagram by Christopher Olah)'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the small rectangular (yellow) boxes in the second box in the preceding
    diagram represent a PyTorch layer, the circles represent an element matrix or
    vector addition, and the merging lines represent that the two vectors are being
    concatenated. The good part is, we need not implement all of this manually. Most
    of the modern deep learning frameworks provide an abstraction that will take care
    of what happens inside an LSTM. PyTorch provides abstraction of all the functionality
    inside the `nn.LSTM` layer, which we can use like any other layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important thing in the LSTM is the cell state that passes through
    all the iterations, represented by the horizontal line across the cells in the
    preceding diagram. Multiple networks inside LSTM control what information travels
    across the cell state. The first step in LSTM (a small network represented by
    the symbol Ï) is to decide what information is going to be thrown away from the
    cell state. This network is called a **forget gate** and has a sigmoid as an activation
    function, which outputs values between 0 and 1 for each element in the cell state.
    The network (PyTorch layer) is represented using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39818084-8346-4d7a-8d4a-0eb898c16dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values from the network decide which values are to be held in the cell
    state and which are to be thrown away. The next step is to decide what information
    we are going to add to the cell state. This has two parts; a sigmoid layer, called
    the input gate, which decides what values to be updated, and a *tanh* layer, which
    creates new values to be added to the cell state. The mathematical representation
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70fea098-0372-4393-a059-4c4b8a510564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we combine the two values generated by the input gate and
    *tanh*. Now we can update the cell state by doing an element-wise multiplication
    between the forget gate and the sum of the product of it and *Ct*, as represented
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e447e5e-118b-4001-9a99-d5140f016bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we need to decide on the output, which will be a filtered version of
    the cell state. There are different versions of LSTM available and most of them
    work on similar principles. As developers or data scientists, we rarely need to
    worry about what goes on inside LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about them, go through the following blog links, which
    cover a lot of theory in a very intuitive way.
  prefs: []
  type: TYPE_NORMAL
- en: Look at Christopher Olah's amazing blog on LSTM ([http://colah.github.io/posts/2015-
    08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)),
    and another blog from Brandon Rohrer ([https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html))
    where he explains LSTM in a nice video.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we understand LSTM, let''s implement a PyTorch network that we can use
    to build a sentiment classifier. As usual, we will follow these steps for creating
    the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the batches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will see the steps in detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the same `torchtext` library for downloading, tokenizing, and building
    vocabulary for the IMDB dataset. When creating the `Field` object, we leave the
    `batch_first` argument at `False`. RNNs expect the data to be in the form of `sequence_length`,
    `batch_size`, and `features.` The following is used for preparing the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Creating batches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use the `torchtext BucketIterator` function for creating batches, and the
    size of the batches will be sequence length and batches. For our case, the size
    will be [200, 32], where 200 is the sequence length and 32 is the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code used for batching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Creating the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the code and then walk through it. You may be surprised at how
    familiar the code looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `init` method creates an embedding layer of the size of the vocabulary and
    `hidden_size`. It also creates an LSTM and a linear layer. The last layer is a
    `LogSoftmax` layer for converting the results from the linear layer to probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward` function, we pass the input data of size [200, 32], which gets
    passed through the embedding layer and each token in the batch gets replaced by
    embedding and the size turns to [200, 32, 100], where 100 is the embedding dimensions.
    The LSTM layer takes the output of the embedding layer along with two hidden variables.
    The hidden variables should be the same type of the embeddings output, and their
    size should be [`num_layers`, `batch_size`, `hidden_size`]. The LSTM processes
    the data in a sequence and generates the output of the shape [`Sequence_length`,
    `batch_size`, `hidden_size`], where each sequence index represents the output
    of that sequence. In this case, we just take the output of the last sequence,
    which is of the shape [`batch_size`, `hidden_dim`], and pass it on to a linear
    layer to map it to the output categories. Since the model tends to overfit, add
    a dropout layer. You can play with the dropout probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the network is created, we can train the model using the same code as
    seen in the previous examples. The following is the code for training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of the training model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Training the model for four epochs gave an accuracy of 84%. Training for more
    epochs resulted in an overfitted model, as the loss started increasing. We can
    try some of the techniques that we tried such as decreasing the hidden dimensions,
    increasing sequence length, and training with smaller learning rates to further
    improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We will also explore how we can use one-dimensional convolutions for training
    on sequence data.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional network on sequence data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how CNNs solve problems in computer vision by learning features from
    the images in [Chapter 4](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml), *Deep Learning
    for Computer Vision*. In images, CNNs work by convolving across height and width.
    In the same way, time can be treated as a convolutional feature. One-dimensional
    convolutions sometimes perform better than RNNs and are computationally cheaper.
    In the last few years, companies such as Facebook have shown success in audio
    generation and machine translation. In this section, we will learn how CNNs can
    be used to build a text classification solution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding one-dimensional convolution for sequence data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml), *Deep Learning
    for Computer Vision*, we have seen how two-dimensional weights are learned from
    the training data. These weights move across the image to generate different activations.
    In the same way, one-dimensional convolution activations are learned during the
    training of our text classifier, where these weights learn patterns by moving
    across the data. The following diagram explains how one-dimensional convolutions
    will work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/712620f7-e5b3-4826-b33e-8fce255942d6.png)'
  prefs: []
  type: TYPE_IMG
- en: For training a text classifier on the IMDB dataset, we will follow the same
    steps as we followed for building the classifier using LSTMs. The only thing that
    changes is that we use `batch_first =` `True`, unlike in our LSTM network. So,
    let's look at the network, the training code, and its results.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the network architecture and then walk through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, instead of an LSTM layer, we have a `Conv1d` layer and
    an `AdaptiveAvgPool1d` layer. The convolution layer accepts the sequence length
    as its input size, and the output size to the hidden size, and the kernel size
    as three. Since we have to change the dimensions of the linear layer, every time
    we try to run it with different lengths, we use an `AdaptiveAvgpool1d` layer,
    which takes input of any size and generates an output of the given size. So, we
    can use a linear layer whose size is fixed. The rest of the code is similar to
    what we have seen in most of the network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training steps for the model are the same as the previous example. Let''s
    just look at the code to call the `fit` method and the results it generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We ran the model for four epochs, which gave approximately 83% accuracy. Here
    are the results of running the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Since the validation loss started increasing after three epochs, I stopped running
    the model. A couple of things that we could try to improve the results are using
    pretrained weights, adding another convolution layer, and trying a `MaxPool1d`
    layer between the convolutions. I leave it to you to try this and see if that
    helps improve the accuracy. Now that we have learned all about the various neural
    networks to process sequence data, let us see language modeling in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Language modeling is the task of predicting the next word in a piece of text
    given the previous words. The ability to generate this sequential data has applications
    in many different areas such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic email reply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing stories, news articles, poems, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus in this area was initially held by RNNs, in particular, LSTMs. However,
    after the introduction of the Transformer architecture in 2017 ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    it became prevalent in NLP tasks. A number of modifications of the transformer
    have since appeared, some of which we will cover in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent times there has been a lot of interest in the use of pretrained models
    for NLP tasks. A key benefit to using pretrained language models is that they
    are able to learn with significantly less data. These models are particularly
    beneficial for languages where labeled data is scarce as they only require labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretrained models for sequence learning were first proposed in 2015 by A. M.
    Dai and Q. V. Le, in a paper titled *Semi-supervised Sequence Learning* ([http://arxiv.org/abs/1511.01432](http://arxiv.org/abs/1511.01432)).
    However, it is only recently that they have shown to be beneficial across a broad
    range of tasks. We will now consider some of the most noteworthy advances in this
    field in recent times, which include, but are by no means limited to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embeddings from Language Models** (**ELMo**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative Pretrained Transformer 2** (**GPT**-**2**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings from language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In February 2018, the *Deep contextualized word representations* paper by M.
    Peters and others ([https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365))
    introduced ELMo. It essentially demonstrated that language model embeddings can
    be used as features in a target model as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03d95256-f743-459d-8efb-63b6e8df13b5.png)'
  prefs: []
  type: TYPE_IMG
- en: ELMo uses a bidirectional language model to learn both word and context. The
    internal states of both the forward and the backward pass are concatenated at
    each word to produce an intermediate vector. It is the bidirectional nature of
    the model that gives it information about both the next word in the sentence and
    the words before it.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Encoder Representations from Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a later paper published by Google in November 2018 ([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)),
    **Bidirectional Encoder Representations from Transformers** (**BERT**) was proposed,
    which incorporates an attention mechanism that learns the contextual relations
    between words and text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0368534-50c8-4c93-b718-3080f02282c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike ELMo where the text input is read sequentially (left to right or right
    to left), with BERT, the entire sequence of words is read at once. Essentially,
    BERT is a trained transformer encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Pretrained Transformer 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, OpenAI''s GTP-2 is one of the most state-of-the-art
    language models designed to improve on the realism and coherence of generated
    text. It was introduced in the paper *Language Models are Unsupervised Multi-task
    Learners* ([https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))
    in February 2019\. It was trained to predict the next word on 8 million web pages
    (which totalled 40 GB of text) with 1.5 billion parameters, which is four times
    as many as BERT. Here is what OpenAI has to say about GPT-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '*GPT-2 g**enerates coherent paragraphs of text, achieves state-of-the-art*
    *performance on many language modeling benchmarks,* *and performs rudimentary
    reading comprehension, machine translation,* *question answering, and summarizationâall
    without task-specific training.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, OpenAI said they would not release the dataset, code, or the full
    GPT-2 model weights. The reason for this was due to concerns about them being
    used to generate deceptive, biased, or abusive language at scale. Examples of
    some of the applications of these models for malicious purposes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Realistic fake news articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistically impersonate others online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abusive or faked content that could be posted on social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate the production of spam or phishing content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team then decided to do a staged release of the model in order to give people
    time to assess the societal implications and evaluate the impacts of release after
    each stage.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a popular GitHub repository from the developer Hugging Face that has
    implemented BERT and GPT-2 in PyTorch. It can be found at the following web link:
    [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT).
    The repository was first made available in November 2018 and permits the user
    to generate sentences from their own data. It also includes a variety of classes
    that can be used to test different models when applied to different tasks, such
    as question answering, token classification, and sequence classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how the code from the GitHub repository
    can be used to generate text from the GPT-2 model. Firstly, we import the associated
    libraries and initialize the pretrained information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we feed the model the `''We like unicorns because they''`
    sentence and it generates words from there like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/287e51be-3416-478c-91a1-83ef17291737.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-2 playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is another useful GitHub repository fromÂ developer ilopezfr, which can
    be found at the following link: [https://github.com/ilopezfr/gpt-2](https://github.com/ilopezfr/gpt-2).
    It also provides a notebook in Google Colab that allows the user to play around
    and experiment with the OpenAI GPT-2 model ([https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb](https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some examples from different sections of the playground:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Text Completion** section:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/13cc1515-9720-464e-8a64-a3ad0d5a4a74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Question-Answering** section:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b0350b09-fe81-4365-9650-abe2f0181f09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Translation** section:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/49c7641e-839d-4f53-8aef-8af2f70b7107.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned different techniques to represent text data in deep
    learning. We learned how to use pretrained word embeddings and our own trained
    embeddings when working on a different domain. We built a text classifier using
    LSTMs and one-dimensional convolutions. We also learned about how to generate
    text using state-of-the-art language modeling architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train deep learning algorithms to
    generate stylish images, and new images, and to generate text.
  prefs: []
  type: TYPE_NORMAL
