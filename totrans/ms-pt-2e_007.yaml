- en: 6 Deep Convolutional GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 Deep Convolutional GANs
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区Discord
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file80.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file80.png)'
- en: Generative neural networks have become a popular and active area of research
    and development. A huge amount of credit for this trend goes to a class of models
    that we are going to discuss in this chapter. These models are called **generative
    adversarial networks** (**GANs**) and were introduced in 2014\. Ever since the
    introduction of the basic GAN model, various types of GANs have been, and are
    being, invented for different applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生成神经网络已成为一个流行且活跃的研究和开发领域。这种趋势的巨大推动归功于我们将在本章讨论的一类模型。这些模型被称为**生成对抗网络**（**GANs**），并于2014年提出。自基本GAN模型提出以来，各种类型的GAN已被发明并被用于不同的应用场景。
- en: Essentially, a GAN is composed of two neural networks – a **generator** and
    a **discriminator**. Let's look at an example of the GAN that is used to generate
    images. For such a GAN, the task of the generator would be to generate realistic-looking
    fake images, and the task of the discriminator would be to tell the real images
    apart from the fake images.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，GAN由两个神经网络组成 - 一个**生成器**和一个**判别器**。让我们看一个用于生成图像的GAN的示例。对于这样的GAN，生成器的任务是生成看起来逼真的假图像，而判别器的任务是区分真实图像和假图像。
- en: In a joint optimization procedure, the generator would ultimately learn to generate
    such good fake images that the discriminator will essentially be unable to tell
    them apart from real images. Once such a model is trained, the generator part
    of it can then be used as a reliable data generator. Besides being used as a generative
    model for unsupervised learning, GANs have also proven useful in semi-supervised
    learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合优化过程中，生成器最终将学会生成如此逼真的假图像，以至于判别器基本无法将其与真实图像区分开来。一旦训练了这样的模型，其生成器部分就可以作为可靠的数据生成器使用。除了用作无监督学习的生成模型外，GAN在半监督学习中也被证明是有用的。
- en: In the image example, for instance, the features learned by the discriminator
    model could be used to improve the performance of classification models trained
    on the image data. Besides semi-supervised learning, GANs have also proven to
    be useful in reinforcement learning, which is a topic that we will discuss in
    *Chapter 10, Deep Reinforcement Learning*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像示例中，例如，判别器模型学习到的特征可以用来提高基于图像数据训练的分类模型的性能。除了半监督学习，GAN在强化学习中也被证明是有用的，这是我们将在《深度强化学习第10章》中讨论的一个主题。
- en: A particular type of GAN that we will focus on in this chapter is the **deep
    convolutional GAN** (**DCGAN**). A DCGAN is essentially an unsupervised **convolution
    neural network** (**CNN**) model. Both the generator and the discriminator in
    a DCGAN are purely *CNNs with no fully connected layers*. DCGANs have performed
    well in generating realistic images, and they can be a good starting point for
    learning how to build, train, and run GANs from scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍的一种特定类型的GAN是**深度卷积GAN**（**DCGAN**）。DCGAN本质上是一个无监督**卷积神经网络**（**CNN**）模型。DCGAN中的生成器和判别器都是纯粹的*CNN，没有全连接层*。DCGAN在生成逼真图像方面表现良好，可以作为学习如何从头开始构建、训练和运行GAN的良好起点。
- en: In this chapter, we will first understand the various components within a GAN
    – the generator and the discriminator models and the joint optimization schedule.
    We will then focus on building a DCGAN model using PyTorch. Next, we will use
    an image dataset to train and test the performance of the DCGAN model. We will
    conclude this chapter by revisiting the concept of style transfer on images and
    exploring the Pix2Pix GAN model, which can efficiently perform a style transfer
    on any given pair of images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先会了解GAN内部的各个组件 - 生成器和判别器模型以及联合优化计划。然后，我们将专注于使用PyTorch构建DCGAN模型。接下来，我们将使用图像数据集来训练和测试DCGAN模型的性能。最后，我们将回顾图像的风格转移概念，并探索Pix2Pix
    GAN模型，该模型可以高效地在任意给定的图像对上执行风格转移。
- en: 'We will also learn how the various components of a Pix2Pix GAN model relate
    to that of a DCGAN model. After finishing this chapter, we will truly understand
    how GANs work and will be able to build any type of GAN model using PyTorch. This
    chapter is broken down into the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习Pix2Pix GAN模型的各个组件与DCGAN模型的关系。完成本章后，我们将真正理解GAN的工作原理，并能够使用PyTorch构建任何类型的GAN模型。本章分为以下几个主题：
- en: Defining the generator and discriminator networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器网络
- en: Training a DCGAN using PyTorch
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch训练DCGAN
- en: Using GANs for style transfer
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GAN进行风格转移
- en: Defining the generator and discriminator networks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器网络
- en: As mentioned earlier, GANs are composed of two components – the generator and
    the discriminator. Both of these are essentially neural networks. Generators and
    discriminators with different neural architectures produce different types of
    GANs. For example, DCGANs purely have CNNs as the generator and discriminator.
    You can find a list of different types of GANs along with their PyTorch implementations
    at [9.1] .
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GAN由两个组件组成 – 生成器和鉴别器。这两者本质上都是神经网络。具有不同神经架构的生成器和鉴别器会产生不同类型的GAN。例如，DCGAN纯粹将CNN作为生成器和鉴别器。您可以在
    [9.1] 处找到包含它们PyTorch实现的不同类型的GAN清单。
- en: 'For any GAN that is used to generate some kind of real data, the generator
    usually takes random noise as input and produces an output with the same dimensions
    as the real data. We call this generated output **fake data**. The discriminator,
    on the other hand, works as a **binary classifier**. It takes in the generated
    fake data and the real data (one at a time) as input and predicts whether the
    input data is real or fake. *Figure 9* *.1* shows a diagram of the overall GAN
    model schematic:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何用于生成某种真实数据的GAN，生成器通常以随机噪声作为输入，并生成与真实数据相同维度的输出。我们称这个生成的输出为**假数据**。鉴别器则作为**二元分类器**运作。它接受生成的假数据和真实数据（一个接一个地）作为输入，并预测输入数据是真实的还是假的。*图9*
    *.1* 显示了整体GAN模型框图：
- en: '![Figure 9 .1 – A GAN schematic](img/file81.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图9 .1 – GAN框图](img/file81.jpg)'
- en: Figure 9 .1 – A GAN schematic
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9 .1 – GAN框图
- en: The discriminator network is optimized like any binary classifier, that is,
    using the binary cross-entropy function. Therefore, the discriminator model's
    motivation is to correctly classify real images as real and fake images as fake.
    The generator network has quite the opposite motivation. The generator loss is
    mathematically expressed as *-log(D(G(x)))*, where *x* is random noise inputted
    into the generator model, *G*; *G(x)* is the generated fake image by the generator
    model; and *D(G(x))* is the output probability of the discriminator model, *D*,
    that is, the probability of the image being real.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络像任何二元分类器一样进行优化，即使用二元交叉熵函数。因此，鉴别器模型的目标是正确地将真实图像分类为真实的，将假图像分类为假的。生成器网络具有相反的动机。生成器损失在数学上表示为*-log(D(G(x)))*，其中*x*是输入到生成器模型*G*中的随机噪声，*G(x)*是生成的假图像，*D(G(x))*是鉴别器模型*D*的输出概率，即图像为真实的概率。
- en: Therefore, the generator loss is minimized when the discriminator thinks that
    the generated fake image is real. Essentially, the generator is trying to fool
    the discriminator in this joint optimization problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当鉴别器认为生成的假图像是真实的时，生成器损失最小化。本质上，在这个联合优化问题中，生成器试图欺骗鉴别器。
- en: In execution, these two loss functions are backpropagated alternatively. That
    is, at every iteration of training, first, the discriminator is frozen, and the
    parameters of the generator networks are optimized by backpropagating the gradients
    from the generator loss.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，这两个损失函数是交替反向传播的。也就是说，在训练的每个迭代中，首先冻结鉴别器，然后通过反向传播生成器损失的梯度来优化生成器网络的参数。
- en: Then, the tuned generator is frozen while the discriminator is optimized by
    backpropagating the gradients from the discriminator loss. This is what we call
    joint optimization. It has also been referred to as being equivalent to a two-player
    Minimax game in the original GAN paper [9.2] .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，调整好的生成器被冻结，同时通过反向传播鉴别器损失来优化鉴别器。这就是我们所说的联合优化。在原始GAN论文中也被称为等效于双人最小最大游戏 [9.2]
    。
- en: Understanding the DCGAN generator and discriminator
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解DCGAN生成器和鉴别器
- en: 'For the particular case of DCGANs, let''s consider what the generator and discriminator
    model architectures look like. As already mentioned, both are purely convolutional
    models. *Figure 9* *.2* shows the generator model architecture for a DCGAN:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的DCGAN情况，让我们看看生成器和鉴别器模型架构是什么样的。如前所述，它们都是纯卷积模型。*图9* *.2* 展示了DCGAN的生成器模型架构：
- en: '![Figure 9 .2 – The DCGAN generator model architecture](img/file82.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图9 .2 – DCGAN生成器模型架构](img/file82.jpg)'
- en: Figure 9 .2 – The DCGAN generator model architecture
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图9 .2 – DCGAN生成器模型架构
- en: First, the random noise input vector of size **64** is reshaped and projected
    into **128** feature maps of size **16x16** each. This projection is achieved
    using a linear layer. From there on, a series of upsampling and convolutional
    layers follow. The first upsampling layer simply transforms the **16x16** feature
    maps into **32x32** feature maps using the nearest neighbor upsampling strategy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，大小为**64**的随机噪声输入向量被重塑并投影到大小为**16x16**的**128**个特征图中。这个投影是通过线性层实现的。然后，一系列上采样和卷积层接连而来。第一个上采样层简单地使用最近邻上采样策略将**16x16**特征图转换为**32x32**特征图。
- en: This is followed by a 2D convolutional layer with a **3x3** kernel size and
    **128** output feature maps. The **128** **32x32** feature maps outputted by this
    convolutional layer are further upsampled to **64x64**-sized feature maps, which
    is followed by two **2D** convolutional layers resulting in the generated (fake)
    RGB image of size **64x64**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是一个2D卷积层，卷积核大小为**3x3**，输出**128**个特征图。这个卷积层输出的**128**个**32x32**特征图进一步上采样为**64x64**大小的特征图，然后是两个2D卷积层，生成（伪造的）RGB图像大小为**64x64**。
- en: Note
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We have omitted the batch normalization and leaky ReLU layers to avoid clutter
    in the preceding architectural representation. The PyTorch code in the next section
    will have these details mentioned and explained.
  id: totrans-31
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们省略了批量归一化和泄漏ReLU层，以避免在前面的架构表示中混乱。下一节的PyTorch代码将详细说明和解释这些细节。
- en: 'Now that we know what the generator model looks like, let''s examine what the
    discriminator model looks like. *Figure 9* *.3* shows the discriminator model
    architecture:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道生成器模型的样子，让我们看看鉴别器模型的样子。*图9*.3展示了鉴别器模型的架构：
- en: '![Figure 9 .3 – The DCGAN discriminator model architecture](img/file83.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图9 .3 – DCGAN鉴别器模型架构](img/file83.jpg)'
- en: Figure 9 .3 – The DCGAN discriminator model architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图9 .3 – DCGAN鉴别器模型架构
- en: As you can see, a stride of **2** at every convolutional layer in this architecture
    helps to reduce the spatial dimension, while the depth (that is, the number of
    feature maps) keeps growing. This is a classic CNN-based binary classification
    architecture being used here to classify between real images and generated fake
    images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，在这个架构中，每个卷积层的步幅设置为**2**有助于减少空间维度，而深度（即特征图的数量）不断增加。这是一种经典的基于CNN的二进制分类架构，用于区分真实图像和生成的伪造图像。
- en: Having understood the architectures of the generator and the discriminator network,
    we can now build the entire DCGAN model based on the schematic in *Figure 9* *.1*
    and train the DCGAN model on an image dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 理解了生成器和鉴别器网络的架构之后，我们现在可以根据*图9*.1的示意图构建整个DCGAN模型，并在图像数据集上训练DCGAN模型。
- en: In the next section, we will use PyTorch for this task. We will discuss, in
    detail, the DCGAN model instantiation, loading the image dataset, jointly training
    the DCGAN generator and discriminator, and generating sample fake images from
    the trained DCGAN generator.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用PyTorch完成这个任务。我们将详细讨论DCGAN模型的实例化，加载图像数据集，联合训练DCGAN生成器和鉴别器，并从训练后的DCGAN生成器生成样本假图像。
- en: Training a DCGAN using PyTorch
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch训练DCGAN
- en: In this section, we will build, train, and test a DCGAN model using PyTorch
    in the form of an exercise. We will use an image dataset to train the model and
    test how well the generator of the trained DCGAN model performs when producing
    fake images.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一个练习构建、训练和测试一个PyTorch中的DCGAN模型。我们将使用一个图像数据集来训练模型，并测试训练后的DCGAN模型的生成器在生成伪造图像时的性能。
- en: Defining the generator
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义生成器
- en: 'In the following exercise, we will only show the important parts of the code
    for demonstration purposes. In order to access the full code, you can refer to
    our github repository [9.3] :'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们只展示代码的重要部分以进行演示。要访问完整的代码，您可以参考我们的github仓库 [9.3] ：
- en: 'First, we need to `import` the required libraries, as follows:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要`import`所需的库，如下所示：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this exercise, we only need `torch` and `torchvision` to build the DCGAN
    model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们只需要`torch`和`torchvision`来构建DCGAN模型。
- en: 'After importing the libraries, we specify some model hyperparameters, as shown
    in the following code:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库后，我们指定了一些模型超参数，如下所示的代码：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will be training the model for `10` epochs with a batch size of `32` and
    a learning rate of `0.001`. The expected image size is *64x64x3*. `lat_dimension`
    is the length of the random noise vector, which essentially means that we will
    draw the random noise from a *64*-dimensional latent space as input to the generator
    model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we define the generator model object. The following code is in direct accordance
    with the architecture shown in *Figure 9* *.2*:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After defining the `_init_` method, we define the `forward` method, which is
    essentially just calling the layers in a sequential manner:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We have used the explicit layer-by-layer definition in this exercise as opposed
    to the `nn.Sequential` method; this is because it makes it easier to debug the
    model if something goes wrong.We can also see the batch normalization and leaky
    ReLU layers in the code, which are not mentioned in *Figure 9* *.2*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: FAQ - Why are we using batch normalisation?
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Batch normalization is used after the linear or convolutional layers to both
    fasten the training process and reduce sensitivity to the initial network weights.
  id: totrans-55
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: FAQ - Why are we using leaky ReLU?
  id: totrans-57
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU might lose all the information for inputs with negative values. A leaky
    ReLU set with a 0.2 negative slope gives 20% weightage to incoming negative information,
    which might help us to avoid vanishing gradients during the training of a GAN
    model.
  id: totrans-59
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Next, we will take a look at the PyTorch code to define the discriminator network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Defining the discriminator
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the generator, we will now define the discriminator model as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, the following code is the PyTorch equivalent for the model architecture
    shown in *Figure 9* *.3*:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First, we have defined a general discriminator module, which is a cascade of
    a convolutional layer, an optional batch normalization layer, a leaky ReLU layer,
    and a dropout layer. In order to build the discriminator model, we repeat this
    module sequentially four times – each time with a different set of parameters
    for the convolutional layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to input a 64x64x3 RGB image and to increase the depth (that is,
    the number of channels) and decrease the height and width of the image as it is
    passed through the convolutional layers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The final discriminator module's output is flattened and passed through the
    adversarial layer. Essentially, the adversarial layer fully connects the flattened
    representation to the final model output (that is, a binary output ). This model
    output is then passed through a sigmoid activation function to give us the probability
    of the image being real (or not fake).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `forward` method for the discriminator, which takes in
    a 64x64 RGB image as input and produces the probability of it being a real image:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Having defined the generator and discriminator models, we can now instantiate
    one of each. We also define our adversarial loss function as the binary cross-entropy
    loss function in the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The adversarial loss function will be used to define the generator and discriminator
    loss functions later in the training loop. Conceptually, we are using binary cross-entropy
    as the loss function because the targets are essentially binary – that is, either
    real images or fake images. And, binary cross-entropy loss is a well-suited loss
    function for binary classification tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失函数将用于定义后续训练循环中生成器和鉴别器损失函数。从概念上讲，我们使用二元交叉熵作为损失函数，因为目标基本上是二进制的——即真实图像或假图像。而二元交叉熵损失是二元分类任务的合适损失函数。
- en: Loading the image dataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载图像数据集
- en: 'For the task of training a DCGAN to generate realistic-looking fake images,
    we are going to use the well-known `MNIST` dataset which contains images of handwritten
    digits from 0 to 9\. By using `torchvision.datasets`, we can directly download
    the `MNIST` dataset and create a `dataset` and a `dataloader` instance out of
    it:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练 DCGAN 以生成看起来逼真的假图像的任务，我们将使用著名的 `MNIST` 数据集，该数据集包含从 0 到 9 的手写数字图像。通过使用 `torchvision.datasets`，我们可以直接下载
    `MNIST` 数据集，并创建一个 `dataset` 和一个 `dataloader` 实例：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is an example of a real image from the `MNIST` dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 MNIST 数据集中真实图像的示例：
- en: '![Figure 9\. 4 – A real image from the MNIST dataset](img/file84.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 9\. 4 – MNIST 数据集中的真实图像](img/file84.jpg)'
- en: Figure 9\. 4 – A real image from the MNIST dataset
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 4 – MNIST 数据集中的真实图像
- en: Dataset citation
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据集引用
- en: ''
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[LeCun et al., 1998a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. "Gradient-based
    learning applied to document recognition." Proceedings of the IEEE, 86(11):2278-2324,
    November 1999.'
  id: totrans-81
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[LeCun et al., 1998a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. "基于梯度的学习应用于文档识别."
    《IEEE 会议录》，86(11):2278-2324，1999 年 11 月。'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yann LeCun (Courant Institute, NYU) and Corinna Cortes (Google Labs, New York)
    hold the copyright of the MNIST dataset, which is a derivative work from the original
    NIST datasets. The MNIST dataset is made available under the terms of the Creative
    Commons Attribution-Share Alike 3.0 license.
  id: totrans-83
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yann LeCun（纽约大学库兰特研究所）和 Corinna Cortes（Google 实验室，纽约）拥有 MNIST 数据集的版权，这是原始 NIST
    数据集的一个衍生作品。MNIST 数据集根据知识共享署名-相同方式共享 3.0 许可证的条款提供。
- en: So far, we have defined the model architecture and the data pipeline. Now it
    is time for us to actually write the DCGAN model training routine, which we will
    do in the following section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了模型架构和数据管道。现在是时候实际编写 DCGAN 模型训练程序了，我们将在下一节中进行。
- en: Training loops for DCGANs
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DCGAN 的训练循环
- en: 'In this section, we will train the DCGAN model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练 DCGAN 模型：
- en: '**Defining the optimization schedule**: Before starting the training loop,
    we will define the optimization schedule for both the generator and the discriminator.
    We will use the `Adam` optimizer for our model. In the original DCGAN paper [9\.
    4] , the *beta1* and *beta2* parameters of the Adam optimizer are set to *0.5*
    and *0.999*, as opposed to the usual *0.9* and *0.999*.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义优化计划**：在开始训练循环之前，我们将为生成器和鉴别器定义优化计划。我们将使用 `Adam` 优化器来优化我们的模型。在原始 DCGAN 论文
    [9\. 4] 中，Adam 优化器的 *beta1* 和 *beta2* 参数设置为 *0.5* 和 *0.999*，而不是通常的 *0.9* 和 *0.999*。'
- en: 'We have retained the default values of *0.9* and *0.999* in our exercise. However,
    you are welcome to use the exact same values mentioned in the paper for similar
    results:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的练习中，我们保留了 *0.9* 和 *0.999* 的默认值。但是，您可以使用论文中提到的确切数值以获得类似的结果：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Training the generator**: Finally, we can now run the training loop to train
    the DCGAN. As we will be jointly training the generator and the discriminator,
    the training routine will consist of both these steps – training the generator
    model and training the discriminator model – in an alternate fashion. We will
    begin with training the generator in the following code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练生成器**：最后，我们现在可以运行训练循环来训练 DCGAN。由于我们将联合训练生成器和鉴别器，训练过程将包括以下两个步骤——训练生成器模型和训练鉴别器模型——交替进行。我们将从训练生成器开始，如下代码所示：'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, we first generate the ground truth labels for real and
    fake images. Real images are labeled as `1`, and fake images are labeled as `0`.
    These labels will serve as the target outputs for the discriminator model, which
    is a binary classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们首先生成了真实和假图像的地面真值标签。真实图像标记为 `1`，假图像标记为 `0`。这些标签将作为鉴别器模型的目标输出，该模型是一个二元分类器。
- en: Next, we load a batch of real images from the MINST dataset loader, and we also
    use the generator to generate a batch of fake images using random noise as input.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从MINST数据集加载器中加载一批真实图像，并使用生成器生成一批使用随机噪声作为输入的假图像。
- en: 'Finally, we define the generator loss as the adversarial loss between the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将生成器的损失定义为以下两者之间的对抗损失：
- en: i) The probability of realness of the fake images (produced by the generator
    model) as predicted by the discriminator model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: i) 生成器模型生成的假图像被鉴别器模型预测为真实的概率。
- en: ii) The ground truth value of `1`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 目标值为`1`。
- en: Essentially, if the discriminator is fooled to perceive the fake generated image
    as a real image, then the generator has succeeded in its role, and the generator
    loss will be low. Once we have formulated the generator loss, we can use it to
    backpropagate gradients along the generator model in order to tune its parameters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，如果鉴别器被愚弄成将生成的假图像视为真实图像，那么生成器在其角色上就成功了，生成器损失将很低。一旦我们制定了生成器损失，我们就可以使用它来沿着生成器模型反向传播梯度，以调整其参数。
- en: In the preceding optimization step of the generator model, we left the discriminator
    model parameters unchanged and simply used the discriminator model for a forward
    pass.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器模型的前述优化步骤中，我们保持鉴别器模型参数不变，并且仅仅使用鉴别器模型进行一次前向传播。
- en: '**Training the discriminator**: Next, we will do the opposite, that is, we
    will retain the parameters of the generator model and train the discriminator
    model:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练鉴别器**：接下来，我们将执行相反操作，即保留生成器模型的参数并训练鉴别器模型：'
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remember that we have a batch of both real and fake images. In order to train
    the discriminator model, we will need both. We define the discriminator loss simply
    to be the adversarial loss or the binary cross entropy loss as we do for any binary
    classifier.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们有一批真实和假图像。为了训练鉴别器模型，我们将需要这两者。我们简单地将鉴别器损失定义为对抗损失或二元交叉熵损失，就像我们对任何二元分类器一样。
- en: We compute the discriminator loss for the batches of both real and fake images,
    keeping the target values at `1` for the batch of real images and at `0` for the
    batch of fake images. We then use the mean of these two losses as the final discriminator
    loss, and use it to backpropagate gradients to tune the discriminator model parameters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算真实图像和假图像批次的鉴别器损失，保持真实图像批次的目标值为`1`，假图像批次的目标值为`0`。然后我们使用这两个损失的均值作为最终的鉴别器损失，并用它来反向传播梯度以调整鉴别器模型参数。
- en: 'After every few epochs and batches, we log the model''s performance results,
    that is, the generator loss and the discriminator loss. For the preceding code,
    we should get an output similar to the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每隔几个epoch和批次后，我们记录模型的性能结果，即生成器损失和鉴别器损失。对于前述代码，我们应该得到类似以下的输出：
- en: '![Figure 9\. 5 – DCGAN training logs](img/file85.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 9\. 5 – DCGAN 训练日志](img/file85.jpg)'
- en: Figure 9\. 5 – DCGAN training logs
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 5 – DCGAN 训练日志
- en: 'Notice how the losses are fluctuating a bit; that generally tends to happen
    during the training of GAN models due to the adversarial nature of the joint training
    mechanism. Besides outputting logs, we also save some network-generated images
    at regular intervals. *Figure 9.* *6* shows the progression of those generated
    images along the first few epochs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意损失如何波动一点；这通常在训练 GAN 模型期间由于联合训练机制的对抗性质而发生。除了输出日志外，我们还定期保存一些网络生成的图像。*图 9.* *6*
    展示了这些生成图像在前几个epoch中的进展：
- en: '![Figure 9\. 6 – DCGAN epoch-wise image generation](img/file86.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 9\. 6 – DCGAN 逐epoch生成图像](img/file86.jpg)'
- en: Figure 9\. 6 – DCGAN epoch-wise image generation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 6 – DCGAN 逐epoch生成图像
- en: If we compare the results from the later epochs to the original MNIST images
    in *Figure 9.* *4*, it looks like the DCGAN has learned reasonably well how to
    generate realistic-looking fake images of handwritten digits.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将后续epoch的结果与*图 9.* *4* 中的原始MNIST图像进行比较，可以看出DCGAN已经相当好地学会如何生成看起来逼真的手写数字的假图像。
- en: That is it. We have learned how to use PyTorch to build a DCGAN model from scratch.
    The original DCGAN paper has a few nuanced details, such as the normal initialization
    of the layer parameters of the generator and discriminator models, using specific
    *beta1* and *beta2* values for the Adam optimizers, and more. We have omitted
    some of those details in the interest of focusing on the main parts of the GAN
    code. You are encouraged to incorporate those details and see how that changes
    the results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样了。我们已经学会了如何使用 PyTorch 从头开始构建 DCGAN 模型。原始 DCGAN 论文中有一些微妙的细节，比如生成器和鉴别器模型的层参数的正常初始化，使用
    Adam 优化器的特定 *beta1* 和 *beta2* 值，等等。出于关注 GAN 代码主要部分的兴趣，我们省略了其中一些细节。鼓励你包含这些细节并查看其如何改变结果。
- en: Additionally, we have only used the `MNIST` database in our exercise. However,
    we can use any image dataset to train the DCGAN model. You are encouraged to try
    out this model on other image datasets. One popular image dataset that is used
    for DCGAN training is the celebrity faces dataset [9\. 5] .
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在练习中只使用了 *MNIST* 数据库。然而，我们可以使用任何图像数据集来训练 DCGAN 模型。鼓励你尝试在其他图像数据集上使用这个模型。用于
    DCGAN 训练的一种流行的图像数据集是名人面孔数据集 [9\. 5] 。
- en: A DCGAN trained with this model can then be used to generate the faces of celebrities
    who do not exist. *ThisPersonDoesntExist* [9\. 6] is one such project that generates
    the faces of humans that do not exist. Spooky? Yes. That is how powerful DCGANs
    and GANs, in general, are. Also, thanks to PyTorch, we can now build our own GANs
    in a few lines of code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种模型训练的 DCGAN 可以生成不存在的名人脸部。*ThisPersonDoesntExist* [9\. 6] 就是这样一个项目，它生成了不存在的人类面孔。鬼魅吗？是的。这就是
    DCGAN 和总体上的 GAN 的强大之处。还要感谢 PyTorch，现在我们可以用几行代码构建自己的 GAN。
- en: In the next and final section of this chapter, we will go beyond DCGANs and
    take a brief look at another type of GAN – the `pix2pix` model. The `pix2pix`
    model can be used to generalize the task of style transfer in images and, more
    generally, the task of image-to-image translation. We will discuss the architecture
    of the `pix2pix` model, its generator and discriminator, and use PyTorch to define
    the generator and discriminator models. We will also contrast Pix2Pix with a DCGAN
    in terms of their architecture and implementation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一节，我们将超越 DCGAN，简要介绍另一种 GAN 类型——*pix2pix* 模型。*pix2pix* 模型可以用于在图像中推广风格转移的任务，更一般地说，是图像到图像的翻译任务。我们将讨论
    *pix2pix* 模型的架构、其生成器和鉴别器，并使用 PyTorch 定义生成器和鉴别器模型。我们还将比较 Pix2Pix 与 DCGAN 在架构和实现方面的不同。
- en: Using GANs for style transfer
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GAN 进行风格转移
- en: 'So far, we have only looked at DCGANs in detail. Although there exist hundreds
    of different types of GAN models already, and many more are in the making, some
    of the well-known GAN models include the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只详细讨论了 DCGAN。虽然已经存在数百种不同类型的 GAN 模型，并且还有更多正在开发中，但一些著名的 GAN 模型包括以下几种：
- en: GAN
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN
- en: DCGAN
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCGAN
- en: Pix2Pix
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pix2Pix
- en: CycleGAN
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN
- en: SuperResolutionGAN (SRGAN)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超分辨率 GAN（SRGAN）
- en: Context encoders
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文编码器
- en: Text-2-Image
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像
- en: LeastSquaresGAN (LSGAN)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小二乘 GAN（LSGAN）
- en: SoftmaxGAN
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SoftmaxGAN
- en: WassersteinGAN
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein GAN
- en: Each of these GAN variants differ by either the application they are catering
    to, their underlying model architecture, or due to some tweaks in their optimization
    strategy, such as modifying the loss function. For example, SRGANs are used to
    enhance the resolution of a low-resolution image. The CycleGAN uses two generators
    instead of one, and the generators consist of ResNet-like blocks. The LSGAN uses
    the mean square error as the discriminator loss function instead of the usual
    cross-entropy loss used in most GANs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每种 GAN 变体的不同之处在于它们服务的应用程序，它们的基础模型架构，或者由于一些优化策略的调整，例如修改损失函数。例如，SRGAN 用于增强低分辨率图像的分辨率。CycleGAN
    使用两个生成器而不是一个，并且生成器由类似 ResNet 的块组成。LSGAN 使用均方误差作为鉴别器损失函数，而不是大多数 GAN 中使用的交叉熵损失。
- en: It is impossible to discuss all of these GAN variants in a single chapter or
    even a book. However, in this section, we will explore one more type of GAN model
    that relates to both the DCGAN model discussed in the previous section and the
    neural style transfer model discussed in *Chapter 8 , Neural Style Transfer* .
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能在一章甚至一本书中讨论所有这些 GAN 变体。然而，在本节中，我们将探索另一种与前一节讨论的 DCGAN 模型和第八章“神经风格转移”中讨论的神经风格转移模型相关的
    GAN 模型。
- en: This special type of GAN generalizes the task of style transfer between images
    and, furthermore, provides a general image-to-image translation framework. It
    is called **Pix2Pix**, and we will briefly explore its architecture and the PyTorch
    implementation of its generator and discriminator components.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特殊类型的GAN推广了图像之间的风格转移任务，并提供了一个通用的图像到图像的翻译框架。它被称为**Pix2Pix**，我们将简要探讨其架构以及其生成器和鉴别器组件的PyTorch实现。
- en: Understanding the pix2pix architecture
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解pix2pix的架构
- en: In *Chapter 8* *, Neural Style Transfer*, you may recall that a fully trained
    neural style transfer model only works on a given pair of images. Pix2Pix is a
    more general model that can transfer style between any pair of images once trained
    successfully. In fact, the model goes beyond just style transfer and can be used
    for any image-to-image translation application, such as background masking, color
    palette completion, and more.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*，*神经风格转移*中，您可能会记得一个完全训练好的神经风格转移模型只能在给定的一对图像上工作。Pix2Pix是一个更通用的模型，一旦成功训练，可以在任意一对图像之间进行风格转移。事实上，该模型不仅限于风格转移，还可以用于任何图像到图像的翻译应用，如背景掩蔽、调色板补充等。
- en: Essentially, Pix2Pix works like any GAN model. There is a generator and a discriminator
    involved. Instead of taking in random noise as input and generating an image,
    as shown in *Figure 9.* *1*, the generator in a `pix2pix` model takes in a real
    image as input and tries to generate a translated version of that image. If the
    task at hand is style transfer, then the generator will try to generate a style-transferred
    image.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，Pix2Pix的工作原理与任何GAN模型相似。涉及到一个生成器和一个鉴别器。与接收随机噪声并生成图像不同的是，如*图9.* *1*所示，`pix2pix`模型中的生成器接收真实图像作为输入，并尝试生成该图像的翻译版本。如果任务是风格转移，那么生成器将尝试生成风格转移后的图像。
- en: 'Subsequently, the discriminator now looks at a pair of images instead of just
    a single image, as was the case in *Figure 9.* *1*. A real image and its equivalent
    translated image is fed as input to the discriminator. If the translated image
    is a genuine one, then the discriminator is supposed to output *1*, and if the
    translated image is generated by the generator, then the discriminator is supposed
    to output *0*. *Figure 9.* *7* shows the schematic for a `pix2pix` model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '随后，鉴别器现在查看一对图像而不是仅仅单个图像，就像*图9.* *1*中的情况一样。真实图像及其等效的翻译图像被作为输入馈送到鉴别器。如果翻译图像是真实的，那么鉴别器应该输出*1*，如果翻译图像是由生成器生成的，则鉴别器应该输出*0*。*图9.*
    *7*显示了`pix2pix`模型的示意图： '
- en: '![Figure 9\. 7 – A Pix2Pix model schematic](img/file87.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图9\. 7 – Pix2Pix模型示意图](img/file87.jpg)'
- en: Figure 9\. 7 – A Pix2Pix model schematic
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 7 – Pix2Pix模型示意图
- en: '*Figure 9.* *7* shows significant similarities to *Figure 9.* *1*, which implies
    that the underlying idea is the same as a regular GAN. The only difference is
    that the real or fake question to the discriminator is posed on a pair of images
    as opposed to a single image.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.* *7*显示与*图9.* *1*有显著相似之处，这意味着其基本思想与常规GAN相同。唯一的区别在于，鉴别器的真假问题是针对一对图像而不是单个图像提出的。'
- en: Exploring the Pix2Pix generator
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索Pix2Pix生成器
- en: 'The generator sub-model used in the `pix2pix` model is a well-known CNN used
    for image segmentation – the **UNet**. *Figure 9.* *8* shows the architecture
    of the UNet, which is used as a generator for the `pix2pix` model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pix2pix`模型中使用的生成器子模型是用于图像分割的著名CNN——**UNet**。*图9.* *8*展示了UNet的架构，它被用作`pix2pix`模型的生成器：
- en: '![Figure 9\. 8 – The Pix2Pix generator model architecture](img/file88.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图9\. 8 – Pix2Pix生成器模型架构](img/file88.jpg)'
- en: Figure 9\. 8 – The Pix2Pix generator model architecture
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 8 – Pix2Pix生成器模型架构
- en: 'Firstly, the name, UNet, comes from the *U* shape of the network, as is made
    evident from the preceding diagram. There are two main components in this network,
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，UNet的名称来源于网络的*U*形状，正如前面的图表所显示的。该网络有两个主要组成部分，如下所示：
- en: From the upper-left corner to the bottom lies the encoder part of the network,
    which encodes the **256x256** RGB input image into a **512**-sized feature vector.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左上角到底部是网络的编码器部分，它将**256x256**的RGB输入图像编码成大小为**512**的特征向量。
- en: From the upper-right corner to the bottom lies the decoder part of the network,
    which generates an image from the embedding vector of size **512**.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从右上角到底部是网络的解码器部分，它从大小为**512**的嵌入向量生成图像。
- en: A key property of UNet is the **skip connections**, that is, the concatenation
    of features (along the depth dimension)from the encoder section to the decoder
    section, as shown by the dotted arrows in *Figure 9.8*
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: UNet 的一个关键特性是**跳跃连接**，即来自编码器部分到解码器部分的特征串联（沿深度维度），如 *图 9.8* 中的虚线箭头所示。
- en: FAQ - Why do we have encoder-decoder skip-connections in U-Net?
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: FAQ - 为什么 U-Net 中有编码器-解码器跳跃连接？
- en: ''
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using features from the encoder section helps the decoder to better localize
    the high-resolution information at each upsampling step.
  id: totrans-146
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用编码器部分的特征帮助解码器在每个上采样步骤中更好地定位高分辨率信息。
- en: Essentially, the encoder section is a sequence of down-convolutional blocks,
    where each down-convolutional block is itself a sequence of a 2D convolutional
    layer, an instance normalization layer, and a leaky ReLU activation. Similarly,
    the decoder section consists of a sequence of up-convolutional blocks, where each
    block is a sequence of a 2D-transposed convolutional layer, an instance normalization
    layer, and a ReLU activation layer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，编码器部分是一系列下卷积块，其中每个下卷积块本身是一系列 2D 卷积层、实例归一化层和渗漏的 ReLU 激活。类似地，解码器部分包括一系列上卷积块，其中每个块是一系列
    2D 转置卷积层、实例归一化层和 ReLU 激活层。
- en: 'The final part of this UNet generator architecture is a nearest neighbor-based
    upsampling layer, followed by a 2D convolutional layer, and, finally, a `tanh`
    activation. Let''s now look at the PyTorch code for the UNet generator:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 UNet 生成器架构的最后部分是一个基于最近邻的上采样层，随后是一个 2D 卷积层，最后是一个 `tanh` 激活。现在让我们来看看 UNet 生成器的
    PyTorch 代码：
- en: 'Here is the equivalent PyTorch code for defining the UNet-based generator architecture:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是定义基于 UNet 的生成器架构的等效 PyTorch 代码：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, there are 8 down-convolutional layers and 7 up-convolutional
    layers. The up-convolutional layers have two inputs, one from the previous up-convolutional
    layer output and another from the equivalent down-convolutional layer output,
    as shown by the dotted lines in *Figure 9.* *7*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，有 8 个下卷积层和 7 个上卷积层。上卷积层有两个输入，一个来自前一个上卷积层的输出，另一个来自等效的下卷积层的输出，如 *图 9.*
    *7* 中所示的虚线所示。
- en: 'We have used the `UpConvBlock` and `DownConvBlock` classes to define the layers
    of the UNet model. The following is the definition of these blocks, starting with
    the `UpConvBlock` class:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了 `UpConvBlock` 和 `DownConvBlock` 类来定义 UNet 模型的层。以下是这些块的定义，从 `UpConvBlock`
    类开始：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The transpose convolutional layer in this up-convolutional block consists of
    a 4x4 kernel with a stride of `2` steps, which essentially doubles the spatial
    dimensions of its output compared to the input.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上卷积块中的转置卷积层由一个步幅为 `2` 步的 4x4 核组成，这在输出空间维度上将其输出与输入相比几乎增加了一倍。
- en: In this transpose convolution layer, the 4x4 kernel is passed through every
    other pixel (due to a stride of `2`) in the input image. At each pixel, the pixel
    value is multiplied with each of the 16 values in the 4x4 kernel.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个转置卷积层中，4x4 核通过输入图像的每隔一个像素（由于步幅为 `2`）传递。在每个像素处，像素值与 4x4 核中的每个 16 个值相乘。
- en: The overlapping values of the kernel multiplication results across the image
    are then summed up, resulting in an output twice the length and twice the breadth
    of the input image. Also, in the preceding `forward` method, the concatenation
    operation is performed after the forward pass is done via the up-convolutional
    block.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个图像中，核乘法结果的重叠值然后相加，导致输出长度和宽度是输入图像的两倍。此外，在前述的 `forward` 方法中，拼接操作是在通过上卷积块的前向传递完成之后执行的。
- en: 'Next, here is the PyTorch code for defining the `DownConvBlock` class:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，这里是定义 `DownConvBlock` 类的 PyTorch 代码：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The convolutional layer inside the down-convolutional block has a kernel of
    size 4x4, a stride of `2`, and the padding is activated. Because the stride value
    is `2`, the output of this layer is half the spatial dimensions of its input.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 下卷积块内的卷积层具有 4x4 大小的核，步幅为 `2`，并且激活了填充。因为步幅值为 `2`，所以此层的输出是其输入的空间尺寸的一半。
- en: A leaky ReLU activation is also used for similar reasons as DCGANs – the ability
    to deal with negative inputs, which also helps with alleviating the vanishing
    gradients problem.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理类似 DCGANs 中的负输入问题，还使用了一个渗漏的 ReLU 激活，这也有助于缓解消失梯度问题。
- en: 'So far, we have seen the `__init__` method of our UNet-based generator. The
    `forward` method is pretty straightforward hereafter:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了基于 UNet 的生成器的 `__init__` 方法。接下来的 `forward` 方法非常简单：
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Having discussed the generator part of the `pix2pix` model, let's take a look
    at the discriminator model as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了 `pix2pix` 模型的生成器部分之后，让我们也来看看判别器模型。
- en: Exploring the Pix2Pix discriminator
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 Pix2Pix 判别器
- en: 'The discriminator model, in this case, is also a binary classifier – just as
    it was for the DCGAN. The only difference is that this binary classifier takes
    in two images as inputs. The two inputs are concatenated along the depth dimension.
    *Figure 9.* *9* shows the discriminator model''s high-level architecture:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，判别器模型也是一个二元分类器，就像 DCGAN 一样。唯一的区别是，这个二元分类器接受两个图像作为输入。两个输入沿深度维度连接。*图 9.*
    *9* 展示了判别器模型的高级架构：
- en: '![Figure 9\. 9 – The Pix2Pix discriminator model architecture](img/file89.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 9\. 9 – Pix2Pix 判别器模型架构](img/file89.jpg)'
- en: Figure 9\. 9 – The Pix2Pix discriminator model architecture
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 9 – Pix2Pix 判别器模型架构
- en: 'It is a CNN where the last 3 convolutional layers are followed by a normalization
    layer as well as a leaky ReLU activation. The PyTorch code to define this discriminator
    model will be as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 CNN 模型，最后的 3 个卷积层后跟一个归一化层以及一个泄漏 ReLU 激活函数。定义这个判别器模型的 PyTorch 代码如下：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, the `4` convolutional layers subsequently double the depth
    of the spatial representation at each step. Layers `2`, `3`, and `4` have added
    normalization layers after the convolutional layer, and a leaky ReLU activation
    with a negative slope of 20% is applied at the end of every convolutional block.
    Finally, here is the `forward` method of the discriminator model class in PyTorch:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，第 `4` 个卷积层在每个步骤后会加倍空间表示的深度。第 `2`、`3` 和 `4` 层在卷积层后添加了归一化层，并且每个卷积块的末尾应用了泄漏
    ReLU 激活，负斜率为 20%。最后，这是判别器模型在 PyTorch 中的 `forward` 方法：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: First, the input images are concatenated and passed through the four convolutional
    blocks and finally led into a single binary output that tells us the probability
    of the pair of images being genuine or fake (that is, generated by the generator
    model). In this way, the `pix2pix` model is trained at runtime so that the generator
    of the `pix2pix` model can take in any image as input and apply the image translation
    function that it has learned during training.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输入图像被连接并通过四个卷积块传递，最终进入一个单一的二进制输出，告诉我们图像对的真假概率（即由生成器模型生成的）。这样，在运行时训练 `pix2pix`
    模型，使得生成器可以接受任何图像作为输入，并应用其在训练期间学到的图像翻译函数。
- en: The `pix2pix` model will be considered successful if the generated fake-translated
    image is difficult to tell apart from a genuine translated version of the original
    image.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `pix2pix` 模型生成的伪翻译图像很难与原始图像的真实翻译版本区分开来，则认为 `pix2pix` 模型是成功的。
- en: This concludes our exploration of the `pix2pix` model. In principle, the overall
    model schematic for Pix2Pix is quite similar to that of the DCGAN model. The discriminator
    network for both of these models is a CNN-based binary classifier. The generator
    network for the `pix2pix` model is a slightly more complex architecture inspired
    by the UNet image segmentation model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对 `pix2pix` 模型的探索。原则上，Pix2Pix 的整体模型框图与 DCGAN 模型非常相似。这两个模型的判别器网络都是基于 CNN
    的二元分类器。而 `pix2pix` 模型的生成器网络则是受 UNet 图像分割模型启发的稍微复杂一些的架构。
- en: Overall, we have been able to both successfully define the generator and discriminator
    models for DCGAN and Pix2Pix using PyTorch, and understand the inner workings
    of these two GAN variants.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们已经成功地使用 PyTorch 定义了 DCGAN 和 Pix2Pix 的生成器和判别器模型，并理解了这两个 GAN 变体的内部工作原理。
- en: After finishing this section, you should be able to get started with writing
    PyTorch code for the many other GAN variants out there. Building and training
    various GAN models using PyTorch can be a good learning experience and certainly
    is a fun exercise. We encourage you to use the information from this chapter to
    work on your own GAN projects using PyTorch.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本节后，您应该能够开始编写其他许多 GAN 变体的 PyTorch 代码。使用 PyTorch 构建和训练各种 GAN 模型可以是一次很好的学习经验，而且肯定是一个有趣的练习。我们鼓励您使用本章的信息来开展自己的
    GAN 项目。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: GANs have been an active area of research and development in recent years, ever
    since their inception in 2014\. This chapter was an exploration of the concepts
    behind GANs, including the components of GANs, namely, the generator and the discriminator.
    We discussed the architectures of each of these components and the overall schematic
    of a GAN model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，生成对抗网络（GANs）已成为研究和开发的活跃领域，自其2014年问世以来如此。本章探讨了GANs背后的概念，包括GANs的组成部分，即生成器和鉴别器。我们讨论了每个组件的架构以及GAN模型的整体框图。
- en: In the next chapter, we will go a step further in our pursuit of generative
    models. We will explore how to generate image from text using cutting-edge deep
    learning techniques.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进一步探索生成模型的研究。我们将探讨如何使用尖端深度学习技术从文本生成图像。
