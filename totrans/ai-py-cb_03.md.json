["```py\nimport pandas as pd\n!wget http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\nnames = ['existingchecking', 'duration', 'credithistory',\n         'purpose', 'creditamount', 'savings', \n         'employmentsince', 'installmentrate', \n         'statussex', 'otherdebtors', 'residencesince', \n         'property', 'age', 'otherinstallmentplans', \n         'housing', 'existingcredits', 'job', \n         'peopleliable', 'telephone', 'foreignworker', \n         'classification']\n\ncustomers = pd.read_csv('german.data', names=names, delimiter=' ')\n```", "```py\n!pip install dython\n```", "```py\nfrom dython.nominal import associations\n\nassociations(customers, clustering=True, figsize=(16, 16), cmap='YlOrBr');\n```", "```py\ncatvars = ['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince',\n 'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', \n 'telephone', 'foreignworker']\nnumvars = ['creditamount', 'duration', 'installmentrate', 'residencesince', 'age', \n 'existingcredits', 'peopleliable', 'classification']\n\ndummyvars = pd.get_dummies(customers[catvars])\ntransactions = pd.concat([customers[numvars], dummyvars], axis=1)\n```", "```py\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\n\nsse = {}\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k).fit(transactions)\n    sse[k] = kmeans.inertia_ \nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()\n```", "```py\nkmeans = KMeans(n_clusters=4).fit(transactions)\ny = kmeans.labels_\n```", "```py\nclusters = transactions.join(\n    pd.DataFrame(data=y, columns=['cluster'])\n).groupby(by='cluster').agg(\n    age_mean=pd.NamedAgg(column='age', aggfunc='mean'),\n    age_std=pd.NamedAgg(column='age', aggfunc='std'),\n    creditamount=pd.NamedAgg(column='creditamount', aggfunc='mean'),\n    duration=pd.NamedAgg(column='duration', aggfunc='mean'),\n    count=pd.NamedAgg(column='age', aggfunc='count'),\n    class_mean=pd.NamedAgg(column='classification', aggfunc='mean'),\n    class_std=pd.NamedAgg(column='classification', aggfunc='std'),\n).sort_values(by='class_mean')\nclusters\n```", "```py\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\n\ndistances = squareform(pdist(\n    StandardScaler().fit_transform(\n        transactions[['classification', 'creditamount', 'duration']]\n   )\n))\nclustering = AgglomerativeClustering(\n    n_clusters=5, affinity='precomputed', linkage='average'\n).fit(distances)\ny = clustering.labels_\n```", "```py\nclusters = transactions.join(\n    pd.DataFrame(data=y, columns=['cluster'])\n).groupby(by='cluster').agg(\n    age_mean=pd.NamedAgg(column='age', aggfunc='mean'),\n    age_std=pd.NamedAgg(column='age', aggfunc='std'),\n    creditamount=pd.NamedAgg(column='creditamount', aggfunc='mean'),\n    duration=pd.NamedAgg(column='duration', aggfunc='mean'),\n    count=pd.NamedAgg(column='age', aggfunc='count'),\n    class_mean=pd.NamedAgg(column='classification', aggfunc='mean'),\n    class_std=pd.NamedAgg(column='classification', aggfunc='std'),\n).sort_values(by='class_mean')\nclusters\n```", "```py\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit, vmap\nfrom sklearn.base import ClassifierMixin\nimport jax\nimport random\nfrom scipy.stats import hmean\n\nclass KMeans(ClassifierMixin):\n    def __init__(self, k, n_iter=100):\n      self.k = k\n      self.n_iter = n_iter\n      self.euclidean = jit(vmap(\n          lambda x, y: jnp.linalg.norm(\n              x - y, ord=2, axis=-1, keepdims=False\n          ), in_axes=(0, None), out_axes=0\n      ))\n\n    def adjust_centers(self, X):\n        jnp.row_stack([X[self.clusters == c].mean(axis=0)\n          for c in self.clusters\n        ])\n\n    def initialize_centers(self):\n        '''roughly the kmeans++ initialization\n        '''\n        key = jax.random.PRNGKey(0)\n        # jax doesn't have uniform_multivariate\n        self.centers = jax.random.multivariate_normal(\n            key, jnp.mean(X, axis=0), jnp.cov(X, rowvar=False), shape=(1,)\n        )\n        for c in range(1, self.k):\n            weights = self.euclidean(X, self.centers)\n            if c>1:\n              weights = hmean(weights ,axis=-1)\n              print(weights.shape)\n\n            new_center = jnp.array(\n                random.choices(X, weights=weights, k=1)[0],\n                ndmin=2\n            )\n            self.centers = jnp.row_stack(\n                (self.centers, new_center)\n            )\n            print(self.centers.shape)\n\n    def fit(self, X, y=None):\n        self.initialize_centers()\n        for iter in range(self.n_iter):\n            dists = self.euclidean(X, self.centers)\n            self.clusters = jnp.argmin(dists, axis=-1)\n            self.adjust_centers(X)\n        return self.clusters\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\n\nkmeans = KMeans(k=3)\nkmeans.fit(X)\n```", "```py\nimport pandas as pd\n\n!wget https://raw.githubusercontent.com/haowen-xu/donut/master/sample_data/cpu4.csv\ncpu_data = pd.read_csv('cpu4.csv')\n```", "```py\n!pip install pyOD\n```", "```py\n!pip install keras\n```", "```py\ncpu_data.head()\n```", "```py\nfrom datetime import datetime\nimport seaborn as sns\n\ncpu_data['datetime'] = cpu_data.timestamp.astype(int).apply(\n    datetime.fromtimestamp\n)\n# Use seaborn style defaults and set the default figure size\nsns.set(rc={'figure.figsize':(11, 4)})\n\ntime_data = cpu_data.set_index('datetime')\ntime_data.loc[time_data['label'] == 1.0, 'value'].plot(linewidth=0.5, marker='o', linestyle='')\ntime_data.loc[time_data['label'] == 0.0, 'value'].plot(linewidth=0.5)\n```", "```py\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmarkers = ['r--', 'b-^']\n\ndef hist2d(X, by_col, n_bins=10, title=None):\n  bins = np.linspace(X.min(), X.max(), n_bins)\n\n  vals = np.unique(by_col)\n  for marker, val in zip(markers, vals):\n    n, edges = np.histogram(X[by_col==val], bins=bins)\n    n = n / np.linalg.norm(n)\n    bin_centers = 0.5 * (edges[1:] + edges[:-1])\n    plt.plot(bin_centers, n, marker, alpha=0.8, label=val)\n\n  plt.legend(loc='upper right')\n  if title is not None:\n    plt.title(title)\n  plt.show()\n\nhist2d(cpu_data.value, cpu_data.label, n_bins=50, title='Values by label')\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cpu_data[['value']].values, cpu_data.label.values\n)\n```", "```py\nfrom pyod.utils.data import evaluate_print\nfrom pyod.models.knn import KNN\n\ndef test_outlier_detector(X_train, y_train,\n                          X_test, y_test, only_neg=True,\n                          basemethod=KNN()):\n  clf = basemethod\n  if only_neg:\n    clf.fit(X_train[y_train==0.0], np.zeros(shape=((y_train==0.0).sum(), 1)))\n  else:\n    clf.fit(X_train, y_train) # most algorithms ignore y\n\n  y_train_pred = clf.predict(X_train) # labels_\n  y_train_scores = clf.decision_scores_\n\n  y_test_pred = clf.predict(X_test)\n  y_test_scores = clf.decision_function(X_test)\n\n  print(\"\\nOn Test Data:\")\n  evaluate_print(type(clf).__name__, y_test, y_test_scores)\n  hist2d(X_test, y_test_pred, title='Predicted values by label')\n```", "```py\nfrom pyod.models.iforest import IForest\n\ntest_outlier_detector(\n    X_train, y_train, X_test, y_test, \n    only_neg=True, basemethod=IForest(contamination=0.01),\n)\n#On Test Data:\n#IForest ROC:0.867, precision @ rank n:0.1\n```", "```py\nfrom pyod.models.auto_encoder import AutoEncoder\n\ntest_outlier_detector(\n    X_train, y_train, X_test, y_test, \n    only_neg=False, \n    basemethod=AutoEncoder(hidden_neurons=[1], epochs=10)\n)\n```", "```py\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_39 (Dense)             (None, 1)                 2         \n_________________________________________________________________\ndropout_30 (Dropout)         (None, 1)                 0         \n_________________________________________________________________\ndense_40 (Dense)             (None, 1)                 2         \n_________________________________________________________________\ndropout_31 (Dropout)         (None, 1)                 0         \n_________________________________________________________________\ndense_41 (Dense)             (None, 1)                 2         \n_________________________________________________________________\ndropout_32 (Dropout)         (None, 1)                 0         \n_________________________________________________________________\ndense_42 (Dense)             (None, 1)                 2         \n=================================================================\nTotal params: 8\nTrainable params: 8\nNon-trainable params: 0\n\n... \nOn Test Data:\nAutoEncoder ROC:0.8174, precision @ rank n:0.1\n```", "```py\n!wget https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_train.csv\n```", "```py\nimport pandas as pd\n\ndata = pd.read_csv(\n    'forbes_freebase_goldstandard_train.csv',\n    names=['string1', 'string2', 'matched']\n)\n```", "```py\n!wget https://raw.githubusercontent.com/ofrendo/WebDataIntegration/7db877abadd2be94d5373f5f47c8ccd1d179bea6/data/goldstandard/forbes_freebase_goldstandard_test.csv\n```", "```py\ntest = pd.read_csv(\n    'forbes_freebase_goldstandard_test.csv',\n    names=['string1', 'string2', 'matched']\n)\n```", "```py\n!pip install python-Levenshtein annoy\n```", "```py\ndef clean_string(string):\n    return ''.join(map(lambda x: x.lower() if str.isalnum(x) else ' ', string)).strip()\n```", "```py\nimport Levenshtein\n\ndef levenstein_distance(s1_, s2_):\n    s1, s2 = clean_string(s1_), clean_string(s2_)\n    len_s1, len_s2 = len(s1), len(s2)\n    return Levenshtein.distance(\n        s1, s2\n    ) / max([len_s1, len_s2])\n```", "```py\ndef jaro_winkler_distance(s1_, s2_):\n    s1, s2 = clean_string(s1_), clean_string(s2_)\n    return 1 - Levenshtein.jaro_winkler(s1, s2)\n```", "```py\nfrom difflib import SequenceMatcher\n\ndef common_substring_distance(s1_, s2_):\n    s1, s2 = clean_string(s1_), clean_string(s2_)\n    len_s1, len_s2 = len(s1), len(s2)\n    match = SequenceMatcher(\n        None, s1, s2\n    ).find_longest_match(0, len_s1, 0, len_s2)\n    len_s1, len_s2 = len(s1), len(s2)\n    norm = max([len_s1, len_s2])\n    return 1 - min([1, match.size / norm])\n```", "```py\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\ndists = np.zeros(shape=(len(data), 3))\nfor algo_i, algo in enumerate(\n    [levenstein_distance, jaro_winkler_distance, common_substring_distance]\n):\n    for i, string_pair in data.iterrows():\n        dists[i, algo_i] = algo(string_pair['string1'], string_pair['string2'])\n\n    print('AUC for {}: {}'.format(\n        algo.__name__, \n        roc_auc_score(data['matched'].astype(float), 1 - dists[:, algo_i])\n    ))\n#AUC for levenstein_distance: 0.9508904955034385\n#AUC for jaro_winkler_distance: 0.9470992770234525\n#AUC for common_substring_distance: 0.9560042320578381\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# We clean the strings as before and we take ngrams.\nngram_featurizer = CountVectorizer(\n    min_df=1,\n    analyzer='char',\n    ngram_range=(1, 1), # this is the range of ngrams that are to be extracted!\n    preprocessor=clean_string\n).fit(\n    np.concatenate(\n        [data['string1'], data['string2']],\n        axis=0\n    )\n)\n```", "```py\nstring1cv = ngram_featurizer.transform(data['string1'])\nstring2cv = ngram_featurizer.transform(data['string2'])\n\ndef norm(string1cv):\n    return string1cv / string1cv.sum(axis=1)\n\nsimilarities = 1 - np.sum(np.abs(norm(string1cv) - norm(string2cv)), axis=1) / 2\nroc_auc_score(data['matched'].astype(float), similarities)\n#0.9298183741844471\n```", "```py\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Lambda, Input\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef create_string_featurization_model(\n    feature_dimensionality, output_dim=50):\n    preprocessing_model = Sequential()\n    preprocessing_model.add(\n        Dense(output_dim, activation='linear', input_dim=feature_dimensionality)\n    )\n    preprocessing_model.summary()\n    return preprocessing_model\n```", "```py\ndef euclidean_distance(vects):\n    x, y = vects\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    sum_square = K.sum(\n        K.square(x - y),\n        axis=1,\n        keepdims=True\n    )\n    return K.sqrt(K.maximum(\n        sum_square,\n        K.epsilon()\n    ))\n```", "```py\ndef create_siamese_model(preprocessing_models, #initial_bias =\n                          input_shapes=(10,)):    \n    if not isinstance(preprocessing_models, (list, tuple)):\n        raise ValueError('preprocessing models needs to be a list or tuple of models')\n    print('{} models to be trained against each other'.format(len(preprocessing_models)))\n    if not isinstance(input_shapes, list):\n        input_shapes = [input_shapes] * len(preprocessing_models)\n\n    inputs = []\n    intermediate_layers = []\n    for preprocessing_model, input_shape in zip(preprocessing_models, input_shapes):\n        inputs.append(Input(shape=input_shape))\n        intermediate_layers.append(preprocessing_model(inputs[-1]))\n\n    layer_diffs = []\n    for i in range(len(intermediate_layers)-1): \n        layer_diffs.append(\n            Lambda(euclidean_distance)([intermediate_layers[i], intermediate_layers[i+1]])\n        ) \n    siamese_model = Model(inputs=inputs, outputs=layer_diffs)\n    siamese_model.summary()\n    return siamese_model\n```", "```py\ndef compile_model(model):\n    model.compile(\n        optimizer='rmsprop',\n        loss='mse',\n    )\n\nfeature_dims = len(ngram_featurizer.get_feature_names())\nstring_featurization_model = create_string_featurization_model(feature_dims, output_dim=10)\n\nsiamese_model = create_siamese_model(\n    preprocessing_models=[string_featurization_model, string_featurization_model],\n    input_shapes=[(feature_dims,), (feature_dims,)],\n)\ncompile_model(siamese_model)\nsiamese_model.fit(\n    [string1cv, string2cv],\n    1 - data['matched'].astype(float),\n    epochs=1000\n)\n```", "```py\nfrom scipy.spatial.distance import euclidean\n\nstring_rep1 = string_featurization_model.predict(\n    ngram_featurizer.transform(data['string1'])\n)\nstring_rep2 = string_featurization_model.predict(\n    ngram_featurizer.transform(data['string2'])\n)\ndists = np.zeros(shape=(len(data), 1))\nfor i, (v1, v2) in enumerate(zip(string_rep1, string_rep2)):\n    dists[i] = euclidean(v1, v2)\n\nroc_auc_score(data['matched'].astype(float), 1 - dists)\n0.9802944806912361\n```", "```py\n!pip install git+https://github.com/maciejkula/spotlight.git lightfm\n\n```", "```py\nfrom spotlight.datasets.goodbooks import get_goodbooks_dataset\nfrom spotlight.cross_validation import random_train_test_split \n\nimport numpy as np\n\ninteractions = get_goodbooks_dataset()\ntrain, test = random_train_test_split(\n interactions, random_state=np.random.RandomState(42)\n)\n```", "```py\n<Interactions dataset (53425 users x 10001 items x 4781183 interactions)>\n<Interactions dataset (53425 users x 10001 items x 1195296 interactions)>\n```", "```py\n!wget https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv\n```", "```py\nimport pandas as pd \nbooks = pd.read_csv('books.csv', index_col=0)\n\ndef get_book_titles(book_ids):\n    '''Get book titles by book ids \n    ''' \n    if isinstance(book_ids, int): \n        book_ids = [book_ids]\n    titles = []\n    for book_id in book_ids:\n        titles.append(books.loc[book_id, 'title'])\n    return titles\n\nbook_labels = get_book_titles(list(train.item_ids))\n```", "```py\nget_book_titles(1) \n['The Hunger Games (The Hunger Games, #1)'] \n```", "```py\nimport torch\nfrom spotlight.factorization.explicit import ExplicitFactorizationModel\nfrom spotlight.evaluation import (\n    rmse_score,\n    precision_recall_score\n)\n\nmodel = ExplicitFactorizationModel(\n    loss='regression',\n    embedding_dim=128,\n    n_iter=10,\n    batch_size=1024,\n    l2=1e-9,\n    learning_rate=1e-3,\n    use_cuda=torch.cuda.is_available()\n)\nmodel.fit(train, verbose=True)\ntrain_rmse = rmse_score(model, train)\ntest_rmse = rmse_score(model, test)\nprint('Train RMSE {:.3f}, test RMSE {:.3f}'.format(\n    train_rmse, test_rmse\n))\n```", "```py\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\n\n# Instantiate and train the model\nmodel = LightFM(loss='warp')\nmodel.fit(train.tocoo(), epochs=30, num_threads=2)\ntest_precision = precision_at_k(model, test.tocoo(), k=5)\nprint(\n    'mean test precision at 5: {:.3f}'.format(\n        test_precision.mean()\n))\nmean test precision at 5: 0.114\n```", "```py\n!pip install networkx annoy tqdm python-louvain\n```", "```py\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nimport random\n\nX, y = fetch_openml(data_id=1597, return_X_y=True)\n\nsamples = random.choices(\n    list(range(X.shape[0])), k=int(X.shape[0] * 0.33)\n)\nX_train = X[(samples), :]\n```", "```py\nfrom annoy import AnnoyIndex\nt = AnnoyIndex(X_train.shape[1], 'euclidean') # Length of item vector that will be indexed\nfor i, v in enumerate(X_train):\n    t.add_item(i, v)\n\nt.build(10) # 10 trees\n```", "```py\nfrom tqdm import trange\nfrom scipy.sparse import lil_matrix\n\nMAX_NEIGHBORS = 10000 # Careful: this parameter determines the run-time of the loop!\nTHRESHOLD = 6.0\n\ndef get_neighbors(i):\n  neighbors, distances = t.get_nns_by_item(i, MAX_NEIGHBORS, include_distances=True)\n  return [n for n, d in zip(neighbors, distances) if d < THRESHOLD]\n\nn_rows = X_train.shape[0]\nA = lil_matrix((n_rows, n_rows), dtype=np.bool_)\nfor i in trange(n_rows):\n  neighborhood = get_neighbors(i)\n  for n in neighborhood:\n      A[i, n] = 1\n      A[n, i] = 1\n```", "```py\nfrom scipy.sparse.csgraph import connected_components\n\nn_components, labels = connected_components(\n    A,\n    directed=False,\n    return_labels=True\n)\n```", "```py\nimport networkx as nx\n\nclass ThinGraph(nx.Graph):\n    all_edge_dict = {'weight': 1}\n\n    def single_edge_dict(self):\n        return self.all_edge_dict\n\n    edge_attr_dict_factory = single_edge_dict\n\nG = ThinGraph(A)\n```", "```py\nimport community  # this is the python-louvain package\n\npartition = community.best_partition(G)\n```", "```py\nfrom scipy.sparse.csgraph import connected_components\n\nn_components, labels = connected_components(\n    A,\n    directed=False,\n    return_labels=True\n)\n```", "```py\nfrom sklearn.cluster import AffinityPropagation\n\nap = AffinityPropagation(\n    affinity='precomputed'\n).fit(A)\n```"]