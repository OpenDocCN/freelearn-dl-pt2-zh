- en: Appendix IV — Custom Text Completion with GPT-2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录IV — 使用GPT-2进行自定义文本完成
- en: This appendix, relating to *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*, describes how to customize text completion with a GPT-2 model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这个与*第7章*相关的附录，*GPT-3引擎崛起的超人类变形车*，描述了如何使用GPT-2模型定制文本完成。
- en: This appendix shows how to build a GPT-2 model, train it, and interact with
    custom text in 12 steps.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录展示了如何构建、训练GPT-2模型，并在12个步骤中与自定义文本进行交互。
- en: Open `Training_OpenAI_GPT_2.ipynb`, which is in the GitHub repository of this
    appendix. You will notice that the notebook is also divided into the same 12 steps
    and cells as this appendix.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 打开本附录的GitHub仓库中的`Training_OpenAI_GPT_2.ipynb`。您会注意到笔记本也被分成了与本附录相同的12个步骤和单元格。
- en: Run the notebook cell by cell. The process is tedious, but *the result produced
    by the cloned OpenAI GPT-2 repository is gratifying*. We are not using the GPT-3
    API or a Hugging Face wrapper.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步运行笔记本中的每个单元格。这个过程是单调乏味的，但*克隆OpenAI GPT-2仓库产生的结果是令人满意的*。我们不会使用GPT-3 API或Hugging
    Face包装器。
- en: We are getting our hands dirty to see how the model is built and trained. You
    will see some deprecation messages, but we need to get inside the model, not the
    wrappers or the API. However, the effort is worthwhile.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会忙于了解模型是如何构建和训练的。您会看到一些弃用消息，但我们需要进入模型内部，而不是包装器或API。然而，这个努力是值得的。
- en: Let’s begin by activating the GPU.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始激活GPU。
- en: Training a GPT-2 language model
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GPT-2语言模型
- en: In this section, we will train a GPT-2 model on a custom dataset that we will
    encode. We will then interact with our customized model. We will be using the
    same `kant.txt` dataset as in *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在一个自定义数据集上训练一个GPT-2模型，然后与我们定制的模型进行交互。我们将使用与*第4章*中相同的`kant.txt`数据集，*从头开始预训练RoBERTa模型*。
- en: We will open the notebook and run it cell by cell.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步打开笔记本并运行每个单元格。
- en: 'Step 1: Prerequisites'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：先决条件
- en: 'The files referred to in this section are available in the `AppendixIV` directory
    of this book’s GitHub repository:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中提到的文件可以在本书的GitHub仓库的`AppendixIV`目录中找到：
- en: 'Activate the GPU in the Google Colab’s notebook runtime menu if you are running
    it on Google Colab, as explained in *Step 1: Activating the GPU* in *Appendix
    III*, *Generic Text Completion with GPT-2*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您在Google Colab上运行它，请在Google Colab的笔记本运行时菜单中启用GPU，就像*附录III*中 *第1步：激活GPU*中所解释的那样。
- en: 'Upload the following Python files to Google Colaboratory with the built-in
    file manager: `train.py`, `load_dataset.py`, `encode.py`, `accumulate.py`, `memory_saving_gradients.py`.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置文件管理器将以下Python文件上传到Google Colaboratory：`train.py`、`load_dataset.py`、`encode.py`、`accumulate.py`、`memory_saving_gradients.py`。
- en: 'These files originally come from *N Shepperd’s* GitHub repository: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2).
    However, you can download these files from the `AppendixIV\``gpt-2-train_files`
    directory in this book’s GitHub repository.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些文件最初来自*N Shepperd*的GitHub仓库：[https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)。但是，您可以从本书的GitHub仓库的`AppendixIV\``gpt-2-train_files`目录中下载这些文件。
- en: The *N Shepperd’s* GitHub repository provides the necessary files to train our
    GPT-2 model. We will not clone *N Shepperd’s* repository. Instead, we will be
    cloning OpenAI’s repository and adding the five training files we need from *N
    Shepperd’s* repository.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N Shepperd*的GitHub仓库提供了训练我们的GPT-2模型所需的文件。我们不会克隆*N Shepperd*的仓库，而是将从*N Shepperd*的仓库中获取的五个训练文件添加到OpenAI的仓库中。'
- en: Upload `dset.txt` to Google Colaboratory with the built-in file manager. The
    dataset is named `dset.txt` so that you can replace its content without modifying
    the program with your customized inputs after reading this appendix.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用内置文件管理器将`dset.txt`上传到Google Colaboratory。数据集被命名为`dset.txt`，这样在阅读本附录后，您可以用自定义输入替换其内容而无需修改程序。
- en: This dataset is in the `gpt-2-train_files` directory in the GitHub repository
    of this appendix. It is the `kant.txt` dataset used in *Chapter 4*, *Pretraining
    a RoBERTa Model from Scratch*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个数据集位于本附录的GitHub仓库中的`gpt-2-train_files`目录中。这是*第4章*中使用的`kant.txt`数据集，*从头开始预训练RoBERTa模型*。
- en: We will now go through the initial steps of the training process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐步进行训练过程的初始步骤。
- en: 'Steps 2 to 6: Initial steps of the training process'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2至第6步：训练过程的初始步骤
- en: This subsection will only briefly go through *Steps 2 to 6* since we described
    them in detail in *Appendix III*, *Generic Text Completion with GPT-2*. We will
    then copy the dataset and the model to the project directory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将仅简要介绍*步骤2到6*，因为我们在*附录III*，*使用GPT-2进行通用文本补全*中对其进行了详细描述。然后我们将把数据集和模型复制到项目目录中。
- en: 'The program now clones OpenAI’s GPT-2 repository and not *N Shepperd’s* repository:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序现在克隆OpenAI的GPT-2仓库，而不是*N Shepperd*的仓库：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We have already uploaded the files we need to train the GPT-2 model from *N
    Shepperd’s* directory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从*N Shepperd*的目录中上传了训练GPT-2模型所需的文件。
- en: 'The program now installs the requirements:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在安装所需软件：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This notebook requires `toposort`, which is a topological sort algorithm:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本需要`toposort`，这是一种拓扑排序算法：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Do not restart the notebook after installing the requirements. Instead, wait
    until you have checked the TensorFlow version to restart the VM only once during
    your session. After that, restart it if necessary. It is tedious but worthwhile
    to get inside the code beyond just wrappers and APIs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完所需软件后，请不要重新启动笔记本。相反，请等待直到您检查了TensorFlow版本，在您的会话期间只重新启动虚拟机一次。之后，如果有必要，则重新启动。深入了解代码而不仅仅是包装和API是繁琐但值得的。
- en: 'We now check the TensorFlow version to make sure we are running version `tf
    1.x`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查TensorFlow版本，以确保我们正在运行`tf 1.x`版本：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Whether the `tf 1.x` version is displayed or not, rerun the cell to make sure,
    restart the VM, and rerun this cell. That way, you are sure you are running the
    VM with `tf 1.x`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论显示了`tf 1.x`版还是没有，请重新运行该单元格以确保，重新启动虚拟机，并重新运行该单元格。这样，您可以确保您正在运行带有`tf 1.x`的虚拟机。
- en: 'The program now downloads the 117M parameter GPT-2 model we will train with
    our dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序现在会下载我们将与我们的数据集训练的117M参数的GPT-2模型：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will copy the dataset and the 117M parameter GPT-2 model into the `src`
    directory:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将复制数据集和117M参数的GPT-2模型到`src`目录中：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The goal is to group all the resources we need to train the model in the `src`
    project directory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将我们训练模型所需的所有资源分组到`src`项目目录中。
- en: We will now go through the N Shepperd training files.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将浏览N Shepperd的训练文件。
- en: 'Step 7: The N Shepperd training files'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：N Shepperd训练文件
- en: 'The training files we will use come from *N Shepperd* ‘s GitHub repository.
    We uploaded them in *Step 1: Prerequisites* of this appendix. We will now copy
    them into our project directory:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的训练文件来自*N Shepperd*的GitHub仓库。我们在本附录的*第1步：先决条件*中上传了它们。现在我们将把它们复制到我们的项目目录中：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The training files are now ready to be activated. Let’s now explore them, starting
    with `encode.py`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练文件已经准备好激活。让我们开始探索它们，首先是`encode.py`。
- en: 'Step 8: Encoding the dataset'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8步：对数据集进行编码
- en: The dataset must be encoded before training it. You can double-click on `encode.py`
    to display the file in Google Colaboratory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，数据集必须被编码。您可以双击`encode.py`在Google Colaboratory中查看文件。
- en: '`encode.py` loads `dset.txt` by calling the `load_dataset` function that is
    in `load_dataset.py`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode.py`通过调用`load_dataset.py`中的`load_dataset`函数加载`dset.txt`：'
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`encode.py` also loads OpenAI’s encoding program, `encode.py`, to encode the
    dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode.py`还加载OpenAI的编码程序`encode.py`来对数据集进行编码：'
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The encoded dataset is saved in a `NumPy` array and stored in `out.npz`. Now,
    `npz` is a `NumPy` zip archive of the array generated by the encoder:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 编码的数据集以`NumPy`数组的形式保存，并存储在`out.npz`中。现在，`npz`是由编码器生成的数组的`NumPy`压缩存档：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The dataset is loaded, encoded, and saved in `out.npz` when we run the cell:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行该单元格时，数据集将被加载、编码并保存在`out.npz`中：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Our GPT-2 117M model is ready to be trained.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPT-2 117M模型已经准备好进行训练。
- en: 'Step 9: Training a GPT-2 model'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9步：训练GPT-2模型
- en: 'We will now train the GPT-2 117M model on our dataset. We send the name of
    our encoded dataset to the program:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将对我们的数据集训练GPT-2 117M模型。我们将数据集的编码名称发送给程序：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When you run the cell, it will train until you stop it. The trained model is
    saved after 1,000 steps. When the training exceeds 1,000 steps, stop it. The saved
    model checkpoints are in `/content/gpt-2/src/checkpoint/run1`. You can check the
    list of these files in the *Step 10A: Copying Training Files* cell of the notebook.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行该单元格时，它将一直训练，直到您停止它。训练在1,000步后保存模型。当训练超过1,000步时，请停止它。保存的模型检查点位于`/content/gpt-2/src/checkpoint/run1`中。您可以在笔记本的*第10A步：复制训练文件*单元格中检查这些文件的列表。
- en: You can stop the training by double-clicking on the run button of the cell.
    The training will end, and the trained parameters will be saved.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过双击单元格的运行按钮来停止训练。训练将结束，并且训练参数将被保存。
- en: 'You can also stop training the model after 1,000 steps with *Ctrl* + *M*. The
    program will stop and save the trained parameters. It will convert the code into
    text (you will have to copy it back into a code cell) and display the following
    message:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 1,000 步后停止训练模型，方法是使用*Ctrl* + *M*。程序将停止并保存训练参数。它会将代码转换为文本（您需要将其复制回代码单元格）并显示以下消息：
- en: '![](img/B17948_Appendix_IV_01.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_IV_01.png)'
- en: 'Figure IV.1: Saving a trained GPT-2 model automatically'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 IV.1：自动保存训练好的 GPT-2 模型
- en: The program manages the optimizer and gradients with the `/content/gpt-2/src/memory_saving_gradients.py`
    and `/content/gpt-2/src/accumulate.py` programs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序使用`/content/gpt-2/src/memory_saving_gradients.py`和`/content/gpt-2/src/accumulate.py`程序来管理优化器和梯度。
- en: '`train.py` contains a complete list of parameters that can be tweaked to modify
    the training process. Run the notebook without changing them first. Then, if you
    wish, you can experiment with the training parameters and see if you can obtain
    better results.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.py`包含了可以调整以修改训练过程的完整参数列表。首先不要改变它们运行笔记本。然后，如果你愿意，可以尝试修改训练参数，看看能否获得更好的结果。'
- en: 'The GPT-3 model generates samples that you can read during its training. At
    one point during my GPT-2 training run, the system generated a sample I found
    enlightening:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型会在训练过程中生成一些样本供您阅读。在我训练 GPT-2 的过程中，系统生成了一个让我感到启迪的样本：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A representation of the world is what we humans create and what AI learns. Interesting!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 世界的形象是我们人类创造的，也是 AI 学到的。有趣！
- en: Let’s continue our experiment by creating a directory for our training model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的实验，为我们的训练模型创建一个目录。
- en: 'Step 10: Creating a training model directory'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 10：创建训练模型目录
- en: This section will create a temporary directory for our model, store the information
    we need, and rename it to replace the directory of the GPT-2 117M model we downloaded.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将为我们的模型创建一个临时目录，存储我们需要的信息，并将其重命名以替换我们下载的 GPT-2 117M 模型目录。
- en: 'We start by creating a temporary directory named `tgmodel`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个名为`tgmodel`的临时目录：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then copy the checkpoint files that contain the trained parameters we saved
    when we trained our model in the *Step 9: Training the model* subsection of this
    section:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们复制包含我们训练模型时保存的训练参数的检查点文件，这是在本节的*步骤 9：训练模型*中进行的。
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Our `tgmodel` directory now contains the trained parameters of our GPT-2 model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`tgmodel`目录现在包含我们的 GPT-2 模型的训练参数。
- en: 'We described these files’ content in *Step 5: Downloading the 345M parameter
    GPT-2 model* in *Appendix III*, *Generic Text Completion with GPT-2*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*附录 III*的*使用 GPT-2 进行通用文本补全*的*步骤 5：下载 345M 参数 GPT-2 模型*中描述了这些文件的内容。
- en: 'We will now retrieve the hyperparameters and vocabulary files from the GPT-2
    117M model we downloaded:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将从我们下载的 GPT-2 117M 模型中检索超参数和词汇文件：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our `tgmodel` directory now contains our complete customized GPT-2 117M model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`tgmodel`目录现在包含我们完整定制的 GPT-2 117M 模型。
- en: 'Our last step is to rename the original GPT-2 model we downloaded and set the
    name of our model to `117M`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是将我们下载的原始 GPT-2 模型重命名，并将我们的模型名称设置为`117M`：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our trained model is now the one the cloned OpenAI GPT-2 repository will run.
    Let’s interact with our model!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练好的模型现在是克隆的 OpenAI GPT-2 代码库将要运行的模型。让我们与我们的模型交互吧！
- en: 'Step 11: Generating unconditional samples'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 11：生成无条件样本
- en: In this section, we will interact with a GPT-2 117M model trained on our dataset.
    We will first generate an unconditional sample that requires no input on our part.
    Then we will enter a context paragraph to obtain a conditional text completion
    response from our trained model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将与在我们的数据集上训练的 GPT-2 117M 模型进行互动。我们将首先生成一个无条件样本，不需要我们输入任何内容。然后，我们将输入一个上下文段落，以从我们训练好的模型获得一个条件文本补全响应。
- en: 'Let’s first run an unconditional sample:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先运行一个无条件样本：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You will not be prompted to enter context sentences since this is an unconditional
    sample generator.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个无条件的样本生成器，你不需要输入上下文句子。
- en: To stop the cell, double-click on the run button of the cell or press *Ctrl*
    + *M*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止该单元格，双击单元格的运行按钮，或按*Ctrl* + *M*。
- en: The result is random but makes sense from a grammatical perspective. From a
    semantic point of view, the result is not so interesting because we provided no
    context. But still, the process is remarkable. It invents posts, writes a title,
    dates it, invents organizations and addresses, produces a topic, and even imagines
    web links!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是随机的，但从语法的角度来看是合理的。从语义的角度来看，结果并不那么有趣，因为我们没有提供上下文。但仍然，这个过程是了不起的。它创造了帖子，写了一个标题，日期，想象了组织和地址，提出了一个主题，甚至想象了网页链接！
- en: 'The first few lines are rather incredible:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 开头的几行令人难以置信：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The result of an unconditional random text generator is interesting but not
    convincing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无条件的随机文本生成器的结果很有趣，但并不令人信服。
- en: 'Step 12: Interactive context and completion examples'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12步：交互式上下文和补全示例
- en: We will now run a conditional sample. The context we enter will condition the
    model to think as we want it to, to complete the text by generating tailor-made
    paragraphs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行一个有条件的示例。我们输入的上下文将使模型以我们想要的方式进行思考，通过生成定制段落来完成文本。
- en: 'Run the cell and explore the magic:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格并探索魔法：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If necessary, take a few minutes to go back through the parameters in *Step
    9, Interacting with GPT-2* of *Appendix III*, *Generic Text Completion with GPT-2*.
    The program prompts us to enter the context:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，花几分钟回顾附录III的*第9步：与GPT-2交互*中的参数。程序提示我们输入上下文：
- en: '![](img/B17948_Appendix_IV_02.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_IV_02.png)'
- en: 'Figure IV.2: Context input for text completion'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图IV.2：文本补全的上下文输入
- en: 'Let’s enter the same paragraph written by Emmanuel Kant as we did in *Step
    9: Interacting with GPT-2* in *Appendix III*, *Generic Text Completion with GPT-2*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看埃曼纽尔·康德在附录III的*第9步：与GPT-2交互*中写的同一段落，*使用GPT-2进行通用文本补全*。
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Press *Enter* to generate text as we did previously. Again, the outputs might
    change from one run to another, though it is structured and logical, making transformers
    attractive. This time, the result is not random and is impressive.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 按下*Enter*键来生成文本，就像之前一样。尽管输出可能会从一个运行到另一个运行而变化，但它是有结构和逻辑的，这使得变压器具有吸引力。这次，结果不是随机的，而且令人印象深刻。
- en: 'Let’s look at the first few lines the GPT-2 model produced:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看GPT-2模型生成的开头几行：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To stop the cell, double-click on the run button of the cell or enter *Ctrl*
    + *M*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单元格，双击单元格的运行按钮，或输入*Ctrl* + *M*。
- en: 'Wow! I doubt anybody can see the difference between the text completion produced
    by our trained GPT-2 model and humans. It might also generate different outputs
    at each run. This could be the output, for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我怀疑有谁能看出我们训练过的GPT-2模型产生的文本补全与人类之间的区别。它也可能在每次运行时生成不同的输出。这可能是输出的样子，例如：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: I think our model could outperform many humans in this abstract exercise in
    philosophy, reason, and logic!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们的模型在这个哲学、推理和逻辑的抽象练习中可能会胜过许多人类！
- en: The limit is that the text will vary from one run to another. So, although it
    seems excellent, it will not fit every single need we have in everyday life.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 限制在于文本会随着每次运行而变化。因此，尽管看起来很出色，但它并不会满足我们在日常生活中的每一个需求。
- en: 'We can draw some conclusions from our experiment:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从我们的实验中得出一些结论：
- en: A well-trained transformer model can produce text completion that is at a human
    level.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练良好的变压器模型可以产生接近人类水平的文本补全。
- en: A GPT-2 model can almost reach human level in text generation on complex and
    abstract reasoning.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在复杂和抽象推理方面，GPT-2模型几乎可以达到人类水平的文本生成。
- en: Text context is an efficient way of conditioning a model by demonstrating what
    is expected.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本上下文是通过展示预期的内容来有效地对模型进行条件化的一种方法。
- en: Text completion is text generation based on text conditioning if context sentences
    are provided.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了上下文句子，文本补全就是基于文本条件生成文本。
- en: Although the text is at human level, it does not mean that it will fit every
    need we have. It is locally interesting but not globally effective at this point.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管文本处于人类水平，但这并不意味着它会满足我们所有的需求。目前，它在局部是有趣的，但在整体上并不有效。
- en: You can try to enter conditioning text context examples to experiment with text
    completion. You can also train our model on your own data. Just replace the content
    of the `dset.txt` file with yours and see what happens!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试输入一些条件化的文本上下文示例来实验文本补全。你也可以用自己的数据训练我们的模型。只需用你自己的内容替换`dset.txt`文件的内容，然后看看会发生什么！
- en: Bear in mind that our trained GPT-2 model will react like a human. If you enter
    a short, incomplete, uninteresting, or tricky context, you will obtain puzzled
    or bad results. This is because GPT-2 expects the best out of us, as in real life!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们训练的 GPT-2 模型会像人类一样做出反应。如果你输入一个简短的、不完整的、不引人注意的或者棘手的上下文，你将会得到困惑或者糟糕的结果。这是因为
    GPT-2 期望我们做到最好，就像在现实生活中一样！
- en: References
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'OpenAI GPT-2 GitHub Repository: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI GPT-2 GitHub 仓库: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
- en: '*N Shepperd’s* GitHub Repository: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N Shepperd* 的 GitHub 仓库: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
- en: Join our book’s Discord space
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的 *问我任何问题* 会议：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
