- en: Deep Learning for Predictive Maintenance
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 预测性维护的深度学习
- en: Predictive maintenance is one of the most sought after machine learning solutions
    for IoT. It is also one of the most elusive machine learning solutions for IoT.
    Other areas of machine learning can easily be solved, implementing Computer Vision,
    for example, can be done in hours using tools such as OpenCV or Keras. To be successful
    with predictive maintenance you first need the right sensors. The *Data collection
    design* recipe in [Chapter 2](f3f42fb7-fd75-4607-8b92-3970c18ae234.xhtml), *Handling
    Data*, can be used to help determine proper sensor placement. The *Exploratory
    factor analysts* recipe in [Chapter 2](f3f42fb7-fd75-4607-8b92-3970c18ae234.xhtml),
    *Handling Data* can help determine the cadence with which the data needs to be
    stored. One of the biggest hurdles to implementing predictive maintenance is that
    there needs to be a sufficient amount of device failures. For rugged industrial
    devices, this can take a long time. Linking repair records with device telemetry
    is also a critical step.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测性维护是物联网中最受追捧的机器学习解决方案之一。但它也是最难以捉摸的机器学习解决方案之一。机器学习的其他领域可以轻松解决，例如使用 OpenCV 或
    Keras 等工具实现计算机视觉，可以在几小时内完成。要成功实施预测性维护，首先需要合适的传感器。[第2章](f3f42fb7-fd75-4607-8b92-3970c18ae234.xhtml)中的*数据采集设计*部分可以帮助确定适当的传感器位置。[第2章](f3f42fb7-fd75-4607-8b92-3970c18ae234.xhtml)中的*探索性因子分析*部分可以帮助确定数据存储的频率。实施预测性维护的最大难题之一是需要有足够数量的设备故障。对于坚固的工业设备来说，这可能需要很长时间。将维修记录与设备遥测数据进行关联也是关键步骤。
- en: Even though the challenge is daunting the rewards are great. A properly implemented
    predictive maintenance solution can save lives by helping to ensure critical devices
    are ready when needed. They can also increase customer loyalty because they help
    companies have less downtime than similar products on the market. Finally, they
    can reduce costs and improve efficiency by giving service technicians the information
    they need before servicing the device. This can help them diagnose the device
    and ensure that they have the right parts with them when they are servicing the
    device.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管挑战艰巨，回报是巨大的。一个正确实施的预测性维护解决方案可以通过确保关键设备在需要时准备就绪来拯救生命。它们还可以增加客户忠诚度，因为它们有助于公司比市场上类似产品少停机。最后，它们可以通过在服务设备之前提供服务技术人员所需的信息来降低成本并提高效率。这可以帮助他们诊断设备，并确保他们在服务设备时携带正确的零件。
- en: 'In this chapter, we will continue to use the NASA Turbofan dataset for predictive
    maintenance and cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用 NASA Turbofan 数据集进行预测性维护，并涵盖以下配方：
- en: Enhancing data using feature engineering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用特征工程增强数据
- en: Using Keras for fall detection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 进行跌倒检测
- en: Implementing LSTM to predict device failure
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施 LSTM 进行设备故障预测
- en: Deploying models to web services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型到 Web 服务
- en: Enhancing data using feature engineering
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用特征工程增强数据
- en: One of the best use of time in improving models is feature engineering. The
    ecosystem of IoT has many tools that can make it easier. Devices can be geographically
    connected or hierarchically connected with digital twins, graph frames, and GraphX.
    This can add features such as showing the degree of contentedness to other failing
    devices. Windowing can show how the current reading differs over a period of time.
    Streaming tools such as Kafka can combine different data streams allowing you
    to combine data from other sources. Machines that are outdoor may be negatively
    affected by high temperatures or moisture as opposed to machines that are in a
    climate-controlled building.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进模型中，最好的时间利用是特征工程。物联网生态系统有许多工具可以简化这一过程。设备可以通过数字孪生、图帧和 GraphX 进行地理连接或层次连接。这些工具可以增加一些特征，如显示与其他故障设备的联系程度。窗口化可以显示当前读数在一段时间内的差异。流式处理工具如
    Kafka 可以合并不同的数据流，允许你将来自其他来源的数据进行整合。户外的机器可能会受到高温或潮湿的负面影响，而在气候控制建筑物内的机器则不会。
- en: In this recipe, we are going to look at enhancing our data by looking at time-series
    data such as deltas, seasonality, and windowing. One of the most valuable uses
    of time for a data scientist is feature engineering. Being able to slice the data
    into meaningful features can greatly increase the accuracy of our models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将通过分析时间序列数据（如变化量、季节性和窗口化）来增强我们的数据。对于数据科学家来说，时间最宝贵的用途之一是进行特征工程。能够将数据切分为有意义的特征可以极大地提高我们模型的准确性。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In the *Predictive maintenance with XGBoost* recipe in the previous chapter,
    we used XGBoost to predict whether or not a machine needed maintenance. We have
    imported the NASA *Turbofan engine degradation simulation* dataset which can be
    found at [https://data.nasa.gov/dataset/Turbofan-engine-degradation-simulation-data-set/vrks-gjie](https://data.nasa.gov/dataset/Turbofan-engine-degradation-simulation-data-set/vrks-gjie).
    In the rest of this chapter, we will continue to use that dataset. To get ready
    you will need the dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节的*使用XGBoost进行预测性维护*配方中，我们使用XGBoost来预测机器是否需要维护。我们导入了NASA的*涡轮风扇引擎退化模拟*数据集，可以在[https://data.nasa.gov/dataset/Turbofan-engine-degradation-simulation-data-set/vrks-gjie](https://data.nasa.gov/dataset/Turbofan-engine-degradation-simulation-data-set/vrks-gjie)找到。在本章的其余部分，我们将继续使用该数据集。为了做好准备，您将需要该数据集。
- en: Then if you have not already imported `numpy`, `pandas`, `matplotlib`, and `seaborn`
    into Databricks do so now.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果您还没有将`numpy`、`pandas`、`matplotlib`和`seaborn`导入到Databricks中，请立即这样做。
- en: How to do it...
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The following steps need to be observed to follow this recipe:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 需要遵循此配方的以下步骤：
- en: 'Firstly, import the required libraries. We will be using `pyspark.sql`, `numpy`, and
    `pandas` for data manipulation and `matplotlib` and `seaborn` for visualization:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入所需的库。我们将使用`pyspark.sql`、`numpy`和`pandas`进行数据操作，使用`matplotlib`和`seaborn`进行可视化：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we''re going to import the data and apply a schema to it so that the
    data types can be correctly used. To do this we import the data file through the
    wizard and then apply our schema to it:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导入数据并为其应用模式，以便正确使用数据类型。为此，我们通过向导导入数据文件，然后将我们的模式应用于它：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we put it into a Spark DataFrame:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将其放入一个Spark DataFrame中：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then create a temporary view so that we can run a Spark SQL job on it:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个临时视图，以便我们可以在其上运行一个Spark SQL作业：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we calculate **remaining useful life** (**RUL**). Using the SQL magics,
    we create a table named `engine` from the `raw_engine` temp view we just created.
    We then use SQL to calculate the RUL:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算**剩余使用寿命** (**RUL**)。使用SQL魔法，我们从刚刚创建的`raw_engine`临时视图中创建一个名为`engine`的表。然后，我们使用SQL来计算RUL：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then import the data into a Spark DataFrame:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据导入到一个Spark DataFrame中：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we calculate the r**ate of change** (**ROC**).In the ROC calculation, we
    are looking at the ROC based on the current record compared to the previous record. 
    The ROC calculation gets the percent of change between the current cycle and the
    previous one:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们计算**变化率** (**ROC**)。在ROC计算中，我们查看当前记录与上一记录之间的变化百分比。ROC计算获取当前周期与前一个周期之间的变化百分比：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we review static columns. In order to do that, we''re going to convert
    the Spark DataFrame to Pandas so that we can view summary statistics on the data
    such as mean quartiles and​ standard deviation:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们审查静态列。为了做到这一点，我们将Spark DataFrame转换为Pandas，以便查看数据的汇总统计信息，如均值、四分位数和标准差：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will get the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到以下输出：
- en: '![](img/f5e81791-47d8-409d-889d-d7dce9d1933a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5e81791-47d8-409d-889d-d7dce9d1933a.png)'
- en: 'Now we drop the columns that are not valuable to us in this exercise. For example,
    we are going to drop `settings3` and `s1` columns because the values never change:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们删除在这个练习中对我们没有价值的列。例如，我们将删除`settings3`和`s1`列，因为这些值从不改变：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we are going to review the correlation between values. We are looking
    for columns that are exactly the same. First, we perform a correlation function
    on the DataFrame. Then we use `np.zeros_like` to mask the upper triangle. We are
    then going to set the figure size. Next, we are going to use `diverging_palette`
    to define a custom color map, then we are going to use the `heatmap` function
    do draw the heat map:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将审查值之间的相关性。我们寻找完全相同的列。首先，在DataFrame上执行相关性函数。然后，使用`np.zeros_like`掩盖上三角。接下来，设置图表大小。然后，使用`diverging_palette`定义自定义颜色映射，接着使用`heatmap`函数绘制热力图：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following heat map shows values with a high degree of correlation. The
    values that are `1` show that they are perfectly correlated and therefore can
    be dropped from the analysis:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的热力图显示了具有高度相关性的值。数值为`1`表示它们完全相关，因此可以从分析中删除：
- en: '![](img/d4faf71c-ccdf-4a10-9ffd-43fc492f6403.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4faf71c-ccdf-4a10-9ffd-43fc492f6403.png)'
- en: 'Remove similar columns. We found that `S14` is exactly the same as `S9` so
    we are removing that column:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除相似的列。我们发现`S14`与`S9`完全相同，因此我们将删除该列：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we take the DataFrame and express it visually. A histogram or distribution
    table is used to show potential issues with our data such as outliers, skew data,
    random data and data that would not affect the model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们拿到 DataFrame 并将其可视化。使用直方图或分布表显示数据的潜在问题，如异常值、偏斜数据、随机数据和不影响模型的数据：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following histogram screenshots are the results:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下直方图截图展示了结果：
- en: '![](img/4c9b992c-c00f-4fc0-823d-cf325eb3d7a7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c9b992c-c00f-4fc0-823d-cf325eb3d7a7.png)'
- en: 'We then review the noise of our model to make sure that it is not unduly affected
    by fluctuation:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们检查模型的噪声，以确保它不会过度受到波动的影响：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/c9c1d93d-82d5-4054-b97f-e88d4ebb5f6e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9c1d93d-82d5-4054-b97f-e88d4ebb5f6e.png)'
- en: 'Based on the previous step, it is clear that the data is noisy. This can lead
    to false readings. A rolling average can help smooth the data. Using a 7 cycle
    rolling average we denoise the data as shown:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前一步骤，很明显数据是嘈杂的。这可能导致错误的读数。滚动平均可以帮助平滑数据。使用7个周期的滚动平均，我们去噪了数据，如下所示：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following screenshot is a chart of `rolling_average_s4` versus `s4`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 `rolling_average_s4` 相对于 `s4` 的图表：
- en: '![](img/a06cb431-58c3-48ee-8b3f-899137feb76c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a06cb431-58c3-48ee-8b3f-899137feb76c.png)'
- en: 'Since we want this data to be accessible to other notebooks, we''re going to
    save it as an ML ready table:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们希望其他笔记本能访问这些数据，我们将其保存为一个 ML 准备好的表格：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we have performed feature engineering so that we could make
    our data more usable by our ML algorithms. We removed the columns with no variation,
    high correlation, and we denoised the dataset. In *step 8* we removed the columns
    with no variation. The method describes the data in several ways. Reviewing the
    chart showed that many variables do not change at all. Next, we used a heat map
    to find sensors that had the same data. Finally, we used a rolling average to
    smooth the data from our original dataset into a new one.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们进行了特征工程，使我们的数据更适用于我们的机器学习算法。我们移除了没有变化、高相关性的列，并对数据集进行了去噪。在 *步骤 8* 中，我们移除了没有变化的列。该方法用多种方式描述数据。审查图表显示许多变量根本不发生变化。接下来，我们使用热力图找到具有相同数据的传感器。最后，我们使用滚动平均值将原始数据集的数据平滑成新数据集。
- en: There's more...
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'So far we have just looked at training data. But we will also need to look
    at testing the data. There is a test dataset and a RUL dataset. These datasets
    will help us test our models. To import them you would run 2 additional import
    steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看过训练数据。但是我们还需要查看测试数据。有一个测试数据集和一个 RUL 数据集。这些数据集将帮助我们测试我们的模型。要导入它们，您需要运行额外的
    2 个导入步骤：
- en: '**Importing test data**: Relying on the schema from the training set the test
    set is imported and put in a table called `engine_test`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入测试数据**：依赖于训练集的架构，导入测试集并放入名为 `engine_test` 的表格中：'
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Importing the RUL Dataset**: The next step is to import the remaining useful
    life dataset and save that to a table as well:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入 RUL 数据集**：接下来是导入剩余寿命数据集并将其保存到一个表格中：'
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using keras for fall detection
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 keras 进行摔倒检测
- en: One strategy for predictive maintenance is to look at patterns of device failures
    for a given record. In this recipe, we will classify the data that exhibits a
    pattern that happens before the device fails.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一种预测性维护的策略是查看给定记录的设备故障模式。在这个配方中，我们将分类表现出在设备故障之前发生的模式的数据。
- en: We will be using `keras`, which is a fairly powerful machine learning library.
    Keras strips away some of the complexity of TensorFlow and PyTorch. Keras is a
    great framework for beginners in machine learning as it is easy to get started
    on and the concepts learned in Keras transfer to more expressive machine learning
    libraries such as TensorFlow and PyTorch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `keras`，这是一个非常强大的机器学习库。Keras简化了TensorFlow和PyTorch的一些复杂性。对于机器学习初学者来说，Keras是一个很好的框架，因为它易于入门，并且在Keras中学到的概念可以转移到更具表现力的机器学习库，如TensorFlow和PyTorch。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe expands on the predictive maintenance dataset we feature engineered
    in the previous recipe. If you have not already done so you will need to import
    the `keras`, `tensorflow`, `sklearn`, `pandas`, and `numpy` libraries into your
    Databricks cluster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方扩展了我们在之前配方中进行的预测性维护数据集的特征工程。如果你还没有这样做，你需要将 `keras`、`tensorflow`、`sklearn`、`pandas`
    和 `numpy` 库导入到你的 Databricks 集群中。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Please observe the following steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下步骤：
- en: 'Firstly, import the required libraries. We import `pandas`, `pyspark.sql`,
    and `numpy` for data manipulation, `keras` for machine learning, and `sklearn`
    for evaluating the model. After evaluating the model we use `io`, `pickle`, and
    `mlflow` to save the model and results so that it can be evaluated against other
    models:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入所需的库。我们导入`pandas`，`pyspark.sql`和`numpy`用于数据操作，`keras`用于机器学习，以及`sklearn`用于评估模型。在评估模型后，我们使用`io`，`pickle`和`mlflow`保存模型和结果，以便与其他模型进行比较：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we import training and testing data. Out training data will be used to
    train our models and our testing data will be used to evaluate the models:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们导入训练和测试数据。我们的训练数据将用于训练模型，而测试数据将用于评估模型：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we scale the data. Each sensor of the dataset has a different scale. For
    example, the maximum value of `S1` is `518` while the maximum value of `S16` is
    `0.03`. For that reason, we convert all of the values to a range between `0` and
    `1`. Allowing each metric affect the model in a similar way. We will make use
    of the `MinMaxScaler` function from the `sklearn` library to adjust the scale:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们对数据进行缩放。数据集的每个传感器具有不同的比例。例如，`S1`的最大值是`518`，而`S16`的最大值是`0.03`。因此，我们将所有值转换为`0`到`1`的范围。使每个度量影响模型的方式类似。我们将使用`sklearn`库中的`MinMaxScaler`函数调整比例：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The first layer, the input layer, has 32 nodes. The activation function, `LeakyReLU`,
    defines the output node when given the input. To prevent overfitting, 25% of the
    layers both hidden and visible are dropped when training:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一层，即输入层，有32个节点。激活函数`LeakyReLU`在给定输入时定义输出节点。为了防止过拟合，在训练时将25%的隐藏层和可见层丢弃：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Similar to the input layer, the hidden layer, uses 32 nodes as the input layer
    and `LeakyReLU` as its output layer. It also uses a 25% drop out to prevent overfitting:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于输入层，隐藏层使用32个节点作为输入层，`LeakyReLU`作为输出层。它还使用25%的dropout来防止过拟合：
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we add an output layer. We give it one layer so that we can have an
    output between `0` and `1`. `sigmoid`, our activation function, helps predict
    the probability of the output. Our optimizer, `rmsprop`, along with the loss function
    helps optimize the data pattern and reduce the error rate:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们添加一个输出层。我们给它一层，以便可以得到在`0`到`1`之间的输出。`sigmoid`作为我们的激活函数，有助于预测输出的概率。我们的优化器`rmsprop`和损失函数帮助优化数据模式并减少误差率：
- en: '[PRE22]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we train the Model. We use the `model.fit` function to specify our training
    and test data.  The batch size is used to set the number of training records used
    in 1 iteration of the algorithm.  The epoch of `5` means that it will pass through
    the data set 5 times:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们训练模型。我们使用`model.fit`函数指定我们的训练和测试数据。批量大小用于设置算法迭代中使用的训练记录数量。`5`个epoch意味着它将通过数据集5次：
- en: '[PRE23]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to evaluate the results. We use the trained model and our `X_test` dataset
    to get the predictions (`y_pred`). We then compare the predictions with the real
    results and review how accurate it is:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是评估结果。我们使用训练好的模型和我们的`X_test`数据集来获取预测值`y_pred`。然后，我们将预测结果与实际结果进行比较，并查看其准确性：
- en: '[PRE24]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we save the results to `mlflow`. The results will be compared against
    the other ML algorithms for predictive maintenance we are using in this book:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将结果保存到`mlflow`中。结果将与本书中使用的其他预测维护ML算法进行比较：
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'There are typically three tasks that neural networks does:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通常神经网络有三个任务：
- en: Import data
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入数据
- en: Recognize the patterns of the data by training
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过训练识别数据的模式
- en: Predicting the outcomes of new data
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测新数据的结果
- en: Neural networks take in data, trains themselves to recognize the patterns of
    the data, and then are used to predict the outcomes of new data. This recipe uses
    the cleaned and feature engineered dataset saved in the previous recipe. The `X_train` dataset
    is pulled in from the `spark` data table into a Panda DataFrame. The training
    DataFrames, `X_train`, and `y_train` are used for training. `X_test` gives us
    a list of devices that have failed and `y_test` gives us the real-time failure
    of those machines. Those datasets are used to train models and test the results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络接收数据，训练自己识别数据的模式，然后用于预测新数据的结果。这个流程使用上一个流程中保存的清理和特征工程后的数据集。`X_train`数据集从`spark`数据表中取出并转换为Panda
    DataFrame。训练数据集`X_train`和`y_train`用于训练模型。`X_test`提供了设备列表，这些设备已经发生了故障，而`y_test`提供了这些机器的实时故障。这些数据集用于训练模型和测试结果。
- en: First, we have the input layer. The data is fed to each of our 32 input neurons.
    The neurons are connected through channels. The channel is assigned a numerical
    value known as **weight**. The inputs are multiplied by the corresponding weight
    and their sum is sent as input to the neurons in the hidden layer. Each of these
    neurons is associated with a numerical value called the **bias**, which is added
    to the input sum. This value is then passed to a threshold function called the
    **activation function**. The activation function determines if a neuron will get
    activated or not. We used Leaky ReLU as our activation function for our first
    2 layers. **ReLU** or **Rectified Linear Unit** is a popular activation function
    because it solves the vanishing gradient problem. In this recipe, we used the
    Leaky ReLU. Leaky ReLU solves a problem that ReLU has where big gradients can
    cause the neuron to never fire. The activated neuron passes its data to the next
    layer over the channels. This method allows the data to be propagated through
    the network. This is called **forward propagation**. In the output layer, the
    neuron with the highest layer fires and determines the output.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有输入层。数据被馈送到我们的32个输入神经元中的每一个。神经元通过通道连接。通道被分配一个称为**权重**的数值。输入被相应的权重乘以，并且它们的总和被发送为输入到隐藏层中的神经元。每个这些神经元与一个称为**偏置**的数值相关联，该偏置被加到输入总和中。然后，这个值被传递到称为**激活函数**的阈值函数。激活函数决定神经元是否会被激活。在我们的前两层中，我们使用了Leaky
    ReLU作为我们的激活函数。**ReLU**或**修正线性单元**是一种流行的激活函数，因为它解决了梯度消失问题。在这个示例中，我们使用了Leaky ReLU。Leaky
    ReLU解决了ReLU存在的问题，即大梯度可能导致神经元永远不会被激活。激活的神经元通过通道将其数据传递到下一层。这种方法允许数据通过网络传播。这称为**前向传播**。在输出层中，具有最高层的神经元被激活并确定输出。
- en: 'When we first start running data through our network, the data usually has
    a high degree of error. Our error and optimizer functions use backpropagation
    to update the weights. The cycle of forward propagation and backpropagation is
    repeated to achieve a lower error rate. The following diagram shows how the input,
    hidden, and output layers are linked together:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们首次将数据传入网络时，数据通常具有较高的误差程度。我们的误差和优化器函数使用反向传播来更新权重。前向传播和反向传播的循环重复进行，以达到更低的误差率。下图显示了输入、隐藏和输出层如何连接在一起：
- en: '![](img/5d5bf0ad-fd08-43b0-969e-c5222122e358.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d5bf0ad-fd08-43b0-969e-c5222122e358.png)'
- en: There's more...
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we used `LeakyReLU` as our activation function, `rmsprop` as
    our optimizer, and `binary_crossentropy` as our loss function. We then saved the
    results to `mlflow`. We can tune parameters in this experiment by trying different
    combinations such as the number of neurons or the number of layers. We could also
    change the activation function to use ReLU or TanH. We could also use `Adam` as
    our optimizer. Saving those results to `mlflow` allows us to improve our model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将`LeakyReLU`作为激活函数，`rmsprop`作为优化器，`binary_crossentropy`作为损失函数。然后我们将结果保存到`mlflow`。我们可以通过尝试不同的组合来调整这个实验的参数，比如神经元的数量或层数。我们也可以改变激活函数，使用ReLU或者TanH。我们也可以将`Adam`作为优化器。将这些结果保存到`mlflow`可以帮助我们改进模型。
- en: Implementing LSTM to predict device failure
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施LSTM以预测设备故障
- en: Recurrent neural networks predict sequences of data. In the previous recipe,
    we looked at 1 point in time and determined to determine if maintenance was needed.
    As we saw in the first recipe when we did the data analysis the turbofan run to
    failure dataset is highly variable. The data reading at any point in time might
    indicate a need for maintenance while the next indicates that there is no need
    for maintenance. When determining whether or not to send a technician out having
    an oscillating signal can be problematic. **Long Short Term Memory** (**LSTM**)
    is often used with time-series data such as the turbofan run to failure dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络预测数据序列。在前一个示例中，我们查看了某一时刻并确定是否需要维护。正如我们在第一个示例中看到的那样，涡轮风扇失效数据集具有高度的变异性。在任何时间点读取的数据可能表明需要维护，而下一个时间点可能表明不需要维护。在确定是否派出技术员时，有一个振荡信号可能会导致问题。**长短期记忆网络**（**LSTM**）经常与时间序列数据一起使用，例如涡轮风扇失效数据集。
- en: With the LSTM, we look at a series of data, similar to windowing. LSTM uses
    an ordered sequence to help determine, in our case, if a turbofan engine is about
    to fail based on the previous sequence of data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM，我们查看一系列数据，类似于窗口。LSTM使用有序序列来帮助确定，例如，基于先前数据序列，涡轮风扇引擎是否即将故障。
- en: Getting ready
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe we will use the NASA *Turbofan run to failure* dataset. For
    this recipe we will be using a Databricks notebook. This recipe requires a few
    libraries to be installed. For data processing we need to install `numpy` and
    `pandas`, `keras` for creating a LSTM model, and `sklearn` and `mlflow` for evaluating
    and saving the results of our model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将使用NASA的*涡轮风扇运行到故障*数据集。在这个配方中，我们将使用Databricks笔记本。此配方需要安装几个库。对于数据处理，我们需要安装`numpy`和`pandas`，用于创建LSTM模型的`keras`，以及用于评估和保存模型结果的`sklearn`和`mlflow`。
- en: Even though in previous recipes we added windowing and preprocessed the data,
    in this recipe we will use the raw data. LSTMs window the data and also have a
    good deal of extraction and transformation that is unique to this type of ML Algorithm.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在先前的配方中我们添加了窗口处理和预处理数据，但在这个配方中我们将使用原始数据。LSTM窗口化数据，并且还有大量与这种类型ML算法独特的提取和转换。
- en: How to do it...
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We will execute the following steps for this recipe:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个配方执行以下步骤：
- en: 'First, we will import all of the libraries which we will need later. We will
    import `pandas` and `numpy` for data processing, `keras` for the ML models, `sklearn`
    for evaluations, and `pickel` and `mlflow` for storing the results:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入后续需要使用的所有库。我们将导入`pandas`和`numpy`进行数据处理，`keras`用于ML模型，`sklearn`用于评估，以及`pickel`和`mlflow`用于存储结果：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next we will set the variables. We will set 2 cycles periods. In addition we
    use a sequence length variable. The sequence length allows the LSTM to look back
    over 5 cycles. This is similar to windowing that was discussed in [Chapter 1](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml),
    *Setting Up the IoT and AI Environment*. We are also going to get a list of data
    columns:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置变量。我们将设置2个周期期间。此外，我们使用一个序列长度变量。序列长度允许LSTM回溯5个周期。这类似于[第1章](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml)中讨论的窗口处理，*设置IoT和AI环境*。我们还将获取数据列的列表：
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next we import data from the `spark` data tables we created in the *Simple
    predictive maintenance with XGBoost* recipe in [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*.  We also drop the `label` column because we are going
    to recalculate the labels. We are going to import three DataFrames. The `train`
    DataFrame is used to train the model. The `test` DataFrame is used to test the
    accuracy of the model and the `truth` DataFrame is the actual failures for the
    `test` DataFrame:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从[第3章](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml)的*IoT机器学习中的简单预测性维护与XGBoost*配方中导入`spark`数据表的数据。我们还删除`label`列，因为我们将重新计算标签。我们将导入三个数据框。`train`数据框用于训练模型。`test`数据框用于测试模型的准确性，而`truth`数据框则是`test`数据框的实际故障：
- en: '[PRE29]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we generate labels that show if a device needs maintenance. `label1` shows
    when a device will fail in 14 cycles and `label2` shows when a device will fail
    in 7 cycles. First we create a DataFrame that shows the RUL based on the maximum
    cycle number for each engine. Next we use that the RUL DataFrame create a RUL
    column in our train DataFrame. We do this by subtracting the maximum life from
    the current cycle. We then drop our `max` column. Next we create a new column
    `label1`. `label1` has a `1` value if the RUL is less than the 14 cycles. Then
    copy that over to `label2` and add a `2` value if the RUL is less than 1 week:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们生成标签，显示设备是否需要维护。`label1`显示设备在14个周期内将会故障，而`label2`显示设备将在7个周期内故障。首先，我们创建一个DataFrame，显示每台发动机的最大周期数基于RUL。接下来，我们使用RUL
    DataFrame在我们的train DataFrame中创建一个RUL列。通过从当前周期中减去最大寿命来完成此操作。然后，我们放弃我们的`max`列。接下来，我们创建一个新列`label1`。如果RUL小于14个周期，则`label1`的值为`1`。然后将其复制到`label2`中，并且如果RUL小于1周，则添加值`2`：
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In addition to generating labels for training data we also need to do so for
    test data. The training and test data are slightly different. The training data
    had an end date that signified when the machine broke. The training set does not.
    Instead we have a `truth` DataFrame that shows when the machine actually failed.
    To add the label columns we need to combine the `test` and `truth` dataset before
    we can calculate the labels:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了为训练数据生成标签外，我们还需要为测试数据做同样的事情。训练数据和测试数据有所不同。训练数据有一个表示机器故障时间的结束日期。训练集没有。相反，我们有一个`truth`数据框，显示了机器实际故障的时间。在我们计算标签之前，需要将`test`和`truth`数据集结合起来添加标签列：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, because the columns have different mins and maxes, we will normalize
    the data so that one variable does not overshadow the rest. To do this we are
    going to use the `sklearn` library''s `MinMaxScaler` function. This function transforms
    the values between `0` and `1`. It is a great scalier to use when, as in our case,
    there is not a lot of outlier values. We are going to do the same normalization
    step for both the training and test set:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于列具有不同的最小值和最大值，我们将对数据进行归一化，以防止一个变量掩盖其他变量。为此，我们将使用`sklearn`库的`MinMaxScaler`函数。该函数将值转换为`0`到`1`之间的范围。在我们的情况下，由于没有很多异常值，这是一个很好的缩放器。我们将对训练集和测试集执行相同的归一化步骤：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The LSTM algorithm in Keras requires the data to be in a sequence. In our variables
    section, we chose to have `sequence_length` equal to `100`. This is one of the
    hyperparameters that can be tuned during experimentation. As this is a look at
    data over a sequential period of time the sequence length is the length of the
    sequence of data we are training the model on. There is no real rule of thumb
    on what is the optimal length for a sequence. But from experimentation, it became
    clear that small sequences were less accurate. To aid in generating our sequence
    we use the function to return the sequential data in a way that the LSTM algorithm
    expects:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 中的 LSTM 算法要求数据以序列形式呈现。在我们的变量部分中，我们选择将`sequence_length`设为`100`。这是在实验过程中可以调整的超参数之一。由于这是对一段时间内数据的顺序观察，序列长度是我们训练模型所需数据序列的长度。关于序列的最佳长度没有明确的规则。但通过实验，小序列的准确性较低已经显而易见。为了生成我们的序列，我们使用函数以符合
    LSTM 算法的预期方式返回顺序数据：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The next step is to build a neural network. We build the first layer of our
    LSTM. We start off with a sequential model. Then give it the input shape and length
    of the sequence. The units tell us the dimensionality of the output shape which
    it will pass to the next layer. Next, it returns either `true` or `false`. We
    then add `Dropout` to add the randomness to our training that prevents overfitting:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是构建神经网络。我们构建 LSTM 的第一层。我们从一个序列模型开始。然后给它输入形状和序列的长度。单元告诉我们输出形状的维度，它将传递给下一层。接下来，它返回`true`或`false`。然后我们添加`Dropout`以增加训练中的随机性，防止过拟合：
- en: '[PRE34]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then build the network''s hidden layer. Similar to the first layer the hidden
    layer is an LSTM layer. If, however, instead of passing the entire sequence state
    to the output just passes the last nodes'' values:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们构建网络的隐藏层。与第一层类似，隐藏层是一个 LSTM 层。然而，与将整个序列状态传递给输出不同，它只传递最后节点的值：
- en: '[PRE35]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we build the network''s output layer. The output layer specifies the
    output dimensions and the `activation` function. With this we have built the shape
    of our neural network:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们构建网络的输出层。输出层指定输出维度和`activation`函数。通过这样，我们已经建立了神经网络的形状：
- en: '[PRE36]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we run the `compile` method which configures the model for training.
    In it we put the metric we are evaluating against. In this case, we are measuring against
    `accuracy`. We then define our measure of error or loss. In this example, we are
    using `binary_crossentropy` as our measure. Finally, we specify the optimizer that
    will reduce error every iteration:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们运行`compile`方法来配置模型进行训练。在其中，我们设置我们正在评估的度量标准。在这种情况下，我们正在使用`accuracy`作为我们的度量标准。然后，我们定义我们的误差或损失度量。在这个例子中，我们使用`binary_crossentropy`作为我们的度量标准。最后，我们指定将减少每次迭代中的错误的优化器：
- en: '[PRE37]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We then use our `fit` function to train the model. Our `epochs` parameters
    means that the data will be run through 10 times. Because of the random dropout,
    the extra runs will increase accuracy. We are using `batch_size` of `200`. This
    means that model will train through 200 samples before it updates the gradients.
    Next, we use `validation_split` to put 95% of the data to training the model and
    5% to validating the model. Finally, we use an `EarlyStopping` callback to stop
    the model from training when it stops improving accuracy:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`fit`函数来训练模型。我们的`epochs`参数意味着数据将通过 10 次。由于随机丢弃，额外的运行将提高准确性。我们使用`batch_size`为`200`。这意味着模型在更新梯度之前将通过
    200 个样本进行训练。接下来，我们使用`validation_split`将 95% 的数据用于训练模型，将 5% 用于验证模型。最后，我们使用`EarlyStopping`回调函数在模型停止提高准确性时停止训练：
- en: '[PRE38]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we evaluate our model based on the 95%/5% split we performed on the training
    data. The results show our model is evaluating the 5% data that we held back at
    an 87% accuracy:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们根据我们在训练数据上进行的 95%/5% 分割来评估我们的模型。结果显示我们的模型在评估我们保留的 5% 数据时达到了 87% 的准确性：
- en: '[PRE39]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we look at the confusion matrix which shows us a matrix of correct or
    wrong assessments of whether the engine needed maintenance or not:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们看一下混淆矩阵，它显示了引擎是否需要维护的正确或错误评估的矩阵：
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our confusion matrix looks like the following grid:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混淆矩阵看起来像下面的网格：
- en: '|  | **Actually Did not need maintenance** | **Predicted Needed Maintenance**
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际不需要维护** | **预测需要维护** |'
- en: '| **Actually Did not need maintenance** | 13911 | 220 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **实际不需要维护** | 13911 | 220 |'
- en: '| **Actually  Needed Maintenance** | 201 | 1299 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **实际需要维护** | 201 | 1299 |'
- en: 'We then compute the precision and recall. Because the dataset is unbalanced,
    meaning there are far more values that do not need maintenance than they do, precision
    and recall are the most appropriate measures for evaluating this algorithm:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算精度和召回率。由于数据集不平衡，即不需要维护的值远远超过需要维护的值，精度和召回率是评估此算法的最合适的指标：
- en: '[PRE41]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we need to transform the data so that the testing data is the same type
    of sequential data that the training data is. To do this we perform a data transformation
    step similar to the one we did for the training data:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要转换数据，使得测试数据与训练数据的顺序数据类型相同。为此，我们执行类似于我们对训练数据所做的数据转换步骤：
- en: '[PRE42]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we evaluate the model generated with the training dataset against the
    test dataset to see how accurately the model predicts when an engine needs maintenance:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们评估使用训练数据集生成的模型对测试数据集的准确性，以查看模型预测引擎何时需要维护的准确程度：
- en: '[PRE43]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that we have our results we store those along with the model in our MLflow
    database:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了我们的结果，我们将这些结果与我们的模型一起存储在我们的 MLflow 数据库中：
- en: '[PRE44]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How it works...
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: A LSTM is a special type of **recurrent neural network** (**RNN**). A RNN is
    a neural network architecture that deal with sequenced data by keeping the sequence
    in memory. Conversely, a typical feed-forward neural does not keep the information
    about the sequences and do not allow for flexible inputs and outputs. A recursive
    neural network uses recursion to call from one output back to its input thereby
    generating a sequence. It passes a copy of the state of the network at any given
    time. In our case we are using two layers for our RNN. This additional layer helps
    with accuracy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种特殊类型的**循环神经网络**（**RNN**）。RNN 是一种神经网络架构，通过保持序列在内存中来处理序列数据。相比之下，典型的前馈神经网络不保持序列信息，不允许灵活的输入和输出。递归神经网络使用递归从一个输出调用到其输入，从而生成一个序列。它传递网络在任何给定时间的状态的副本。在我们的案例中，我们使用了两层
    RNN。这一额外的层有助于提高准确性。
- en: LSTMs solve a problem of vanilla RNNs by dropping out data to solve the vanishing
    gradient problem. The vanishing gradient problem is when the neural network stops
    training early but is inaccurate. By using dropout data we can help solve that
    problem. The LSTM does this by using gating functions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 通过使用门控函数解决了传统 RNN 存在的数据消失问题。数据消失问题是指神经网络过早停止训练但不准确的情况。通过使用丢失数据，我们可以帮助解决这个问题。LSTM
    通过使用门控函数来实现这一点。
- en: Deploying models to web services
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到网络服务
- en: Deployment of the model can be different depending on the capabilities of the
    device. Some devices with extra compute can handle having the machine learning
    models run directly on the device. While others require assistance. In this chapter,
    we are going to deploy the model to a simple web service. With modern cloud web
    apps or Kubernetes these web services can scale to meet the needs of the fleet
    of devices. In the next chapter, we will show how to run the model on the device.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的部署可以因设备能力而异。一些带有额外计算能力的设备可以直接运行机器学习模型。而其他设备则需要帮助。在本章中，我们将把模型部署到一个简单的网络服务中。使用现代化的云网络应用或
    Kubernetes，这些网络服务可以扩展以满足设备群的需求。在下一章中，我们将展示如何在设备上运行模型。
- en: Getting ready
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: So far in this book, we have looked at three different machine learning algorithms
    to solve the predictive maintenance problem with the NASA Turbofan run to failure
    dataset. We recorded the results to MLflow. We can see that our XGBoost notebook
    outperformed the more complex neural networks. The following screenshot shows
    the MLflow result set showing the parameters and their associated scores.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经研究了三种不同的机器学习算法，用于解决 NASA Turbofan 故障预测数据集的预测性维护问题。我们记录了结果到 MLflow。我们可以看到，我们的
    XGBoost 笔记本在性能上超过了更复杂的神经网络。以下截图显示了 MLflow 结果集，显示了参数及其相关分数。
- en: '![](img/46fbd902-91fa-438d-9978-be9a8f860bef.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46fbd902-91fa-438d-9978-be9a8f860bef.png)'
- en: From here we can download our model and put it in our web service. To do this
    we are going to use a Python Flask web service and Docker to make the service
    portable. Before we start, `pip install` the python `Flask` package. Also install
    Docker onto your local computer. Docker is a tool that allows you to build out
    complex deployments.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以下载我们的模型并将其放入我们的 web 服务中。为此，我们将使用 Python Flask web 服务和 Docker 使服务可移植。在开始之前，使用
    `pip install` 安装 python 的 `Flask` 包。还要在本地计算机上安装 Docker。Docker 是一个工具，允许您构建复杂的部署。
- en: How to do it...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this project, you will need to create three files for testing the predictor
    web service and one file to scale it to production. First create `app.py` for
    our web server, `requirements.txt` for the dependencies, and the XGBoost model
    you downloaded from `mlflow`. These files will allow you to test the web service.
    Next, to put it into production you will need to dockerize the application. Dockerizing
    the file allow you to deploy it to services such as cloud-based web application
    or Kubernetes services. These services scale easily making onboarding new IoT
    devices seamless. Then execute the following steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，您需要创建三个文件来测试预测器 web 服务和一个文件来扩展到生产环境。首先创建 `app.py` 作为我们的 web 服务器，`requirements.txt`
    作为依赖项，以及从 `mlflow` 下载的 XGBoost 模型。这些文件将允许您测试 web 服务。接下来，要投入生产，您需要将应用程序 docker
    化。将文件 docker 化后，可以将其部署到诸如基于云的 web 应用程序或 Kubernetes 服务等服务中。这些服务易于扩展，使新的 IoT 设备接入无缝进行。
- en: 'The `app.py` file is the Flask application. Import `Flask` for the web service,
    `os` and `pickle` for reading the model into memory, `pandas` for data manipulation,
    and `xgboost` to run our model:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件 `app.py` 是 Flask 应用程序。为了 web 服务，导入 `Flask`，用于内存中读取模型的 `os` 和 `pickle`，数据操作的
    `pandas`，以及运行我们的模型的 `xgboost`：
- en: '[PRE45]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next is to initialize our variables. By loading the Flask application and XGBoost
    model into memory outside a function we ensure that it only loads once rather
    than on every web service call.  By doing this we greatly increase the speed and
    efficiency of the web service. We use `pickle` to re-hydrate our model. `pickle`
    can take almost any Python object and write it to disk. It can also, as in our
    case, read if from disk and put it back into memory:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是初始化我们的变量。通过将 Flask 应用程序和 XGBoost 模型加载到内存中的函数外部，我们确保它仅在每次 web 服务调用时加载一次，而不是每次。通过这样做，我们极大地提高了
    web 服务的速度和效率。我们使用 `pickle` 来重新加载我们的模型。`pickle` 可以接受几乎任何 Python 对象并将其写入磁盘。它还可以像我们的情况一样，从磁盘中读取并将其放回内存中：
- en: '[PRE46]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then create `@application.route` to give us an `http` endpoint. The `POST`
    methods section specifies that it will only accept post web request. We also specify
    that the URL will route to `/predict`. For example, when we run this locally we
    could use the `http://localhost:8000/precict` URL to post our JSON string. We
    then convert it into a `pandas` DataFrame and then an XGBoost data matrix becomes
    calling `predict`. We then determine if it is above `.5` or below and return the
    results:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建 `@application.route` 来提供一个 `http` 端点。`POST` 方法部分指定它将仅接受 post web 请求。我们还指定
    URL 将路由到 `/predict`。例如，当我们在本地运行时，可以使用 `http://localhost:8000/precict` URL 来发布我们的
    JSON 字符串。然后我们将其转换为 `pandas` DataFrame，然后 XGBoost 数据矩阵变为调用 `predict`。然后我们确定它是否大于
    `.5` 或小于并返回结果：
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, the last thing to do in any Flask app is to call the `application.run`
    method.  This method allows us to specify a host. In this case, we are specifying
    a special host of `0.0.0.0` which tells flask to accept requests from other computers.
    Next, we specify a port. The port can be any number. It does however need to match
    the port in the Dockerfile:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在任何 Flask 应用程序中执行的最后一件事是调用 `application.run` 方法。此方法允许我们指定一个主机。在本例中，我们指定了一个特殊的主机
    `0.0.0.0`，告诉 Flask 接受来自其他计算机的请求。接下来，我们指定一个端口。端口可以是任何数字。但是它确实需要与 Dockerfile 中的端口匹配：
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We then create a requirements file. The `requirements.txt` file will install
    all of the python dependencies for the project. The docker will use this to install
    the dependencies:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个要求文件。`requirements.txt` 文件将安装项目的所有 python 依赖项。docker 将使用此文件安装依赖项：
- en: '[PRE49]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, we create the Dockerfile. The `docker` file allows the deployment of
    the predictor to a web endpoint. The first line of the docker file will pull in
    the official python 3.7.5 image from Docker Hub. Next, we copy the local folder
    to a new folder in the docker container in a folder named `app`. Next, we set
    the working directory to the `app` folder. Then we use `pip install` to install
    the requirements from the file we created in *step 5*. Then we expose port `8000`.
    Finally, we run the `gunicorn` command that starts the Gunicorn server:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们创建Dockerfile。`docker`文件允许将预测器部署到Web端点。docker文件的第一行将从Docker Hub拉取官方的Python
    3.7.5镜像。接下来，我们将本地文件夹复制到名为`app`的docker容器中的新文件夹中。然后，我们设置工作目录为`app`文件夹。接着，我们使用`pip
    install`来安装我们在*步骤5*中创建的要求文件中的要求。然后，我们暴露端口`8000`。最后，我们运行启动Gunicorn服务器的`gunicorn`命令：
- en: '[PRE50]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works...
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理……
- en: Flask is a lightweight web server. We pull in the model that we saved to disk
    using `pickle` to rehydrate the model. We then create an `http` endpoint to call
    into.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Flask是一个轻量级的Web服务器。我们使用`pickle`从磁盘上保存的模型来恢复模型。然后，我们创建一个`http`端点进行调用。
- en: There's more...
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这还不是全部……
- en: 'Modern cloud-based web applications such as **Azure Web Apps** can automatically
    pull new Docker images into production. There is also a great deal of DevOps tools
    that can pull images and run them through various tests before deploying them
    with Docker container instances or docker orchestration tools such as Kubernetes.
    But for them to do this one must first put them into a container registry such
    as **Azure Container Registry** or **Docker Hub**. To do this we will need to
    do a few steps. First, we will build our container. Next, we can run our container
    to ensure that it works. Then we log into our container registry service and push
    the container to it. The detailed steps are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现代基于云的Web应用程序，例如**Azure Web Apps**，可以自动将新的Docker镜像拉入生产环境。还有许多DevOps工具可以拉取镜像，并通过各种测试来运行它们，然后使用Docker容器实例或诸如Kubernetes之类的Docker编排工具部署它们。但要让它们这样做，首先必须将它们放入诸如**Azure
    Container Registry**或**Docker Hub**之类的容器注册表中。为此，我们需要执行几个步骤。首先，我们将构建我们的容器。接下来，我们可以运行我们的容器以确保它工作正常。然后，我们登录到我们的容器注册表服务并将容器推送到其中。详细步骤如下：
- en: 'First, we build the container. To do it we navigate to the folder with the
    docker file and run docker build. We are going to tag it with the `-t` command
    to `ch4`.  We then specify that the docker file is in the local folder with the
    period `.`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们构建容器。为此，我们导航到包含docker文件的文件夹，并运行docker build。我们将使用`-t`命令标记它为`ch4`。然后，我们使用句点`.`指定docker文件位于本地文件夹中：
- en: '[PRE51]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now that we have a docker image built, we are going to run the container based
    on the image with the `docker run` command. We are going to use the `-it` interactive
    command so we can see any output from the server. We are also going to use the
    `-p` or `port` command to specify that we are mapping the docker containers internal
    port `8000` to the external port `8000`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个Docker镜像，接下来我们将基于该镜像运行容器，使用`docker run`命令。我们将使用`-it`交互命令，以便能够查看服务器的任何输出。我们还将使用`-p`或`port`命令指定将Docker容器的内部端口`8000`映射到外部端口`8000`：
- en: '[PRE52]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We then need to put the container into something that can be accessible by our
    compute resource. To do this, first register for a Docker Registry service such
    as Docker Hub or Azure Container Registry. Then create a new repository. The repository
    provider will give you a path for that repository.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要将容器放入能够被我们计算资源访问的地方。为此，首先注册Docker Registry服务，如Docker Hub或Azure Container
    Registry。然后创建一个新的仓库。仓库提供者将为该仓库提供一个路径。
- en: 'Next is to log in to your container registry service, tag the container, and
    push the container.  Remember to replace `[Your container path]` with the registry
    name or path provided by the registry service:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是登录到您的容器注册表服务，标记容器并推送容器。请记住用注册表服务提供的注册名或路径替换`[Your container path]`：
- en: '[PRE53]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: You can then use docker enabled cloud technology to push that predictor service
    into production. Your device can then send its sensor reading to the web service
    and receive through a cloud to device message whether the device needs maintenance
    or not.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用启用了docker的云技术将该预测器服务推送到生产环境。然后，您的设备可以将其传感器读数发送到Web服务，并通过云到设备消息接收设备是否需要维护的反馈。
