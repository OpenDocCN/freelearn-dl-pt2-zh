- en: Reconstructing Inputs Using Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autoencoders** are unsupervised learning algorithm. Unlike other algorithms,
    autoencoders learn to reconstruct the input, that is, an autoencoder takes the
    input and learns to reproduce the input as an output. We start the chapter by
    understanding what are autoencoders and how exactly they reconstruct the input.
    Then, we will learn how autoencoders reconstruct MNIST images.'
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we will learn about the different variants of autoencoders; first,
    we will learn about **convolutional autoencoders** (**CAEs**), which use convolutional
    layers; then, we will learn about how **denoising autoencoders** (**DAEs**) which
    learn to remove noise in the input. After this, we will understand sparse autoencoders
    and how they learn from sparse inputs. At the end of the chapter, we will learn
    about an interesting generative type of autoencoders called **variational autoencoders**.
    We will understand how variational autoencoders learn to generate new inputs and
    how they differ from other autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders and their architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing MNIST images using autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building convolutional autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing noise in the image using denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contractive autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an autoencoder?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **autoencoder** is an interesting unsupervised learning algorithm. Unlike
    other neural networks, the objective of the autoencoder is to reconstruct the
    given input; that is, the output of the autoencoders is the same as the input.
    It consists of two important components called the **encoder** and the **decoder**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The role of the encoder is to encode the input by learning the latent representation
    of the input, and the role of the decoder is to reconstruct the input from the
    latent representation produced by the encoder. The latent representation is also
    called **bottleneck** or **code**. As shown in the following diagram, an image
    is passed as an input to the autoencoder. An encoder takes the image and learns
    the latent representation of the image. The decoder takes the latent representation
    and tries to reconstruct the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8c5f6fa-9522-4fc2-b1a8-f10d4e451148.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A simple vanilla autoencoder with two layers is shown in the following diagram;
    as you may notice, it consists of an input layer, a hidden layer, and an output
    layer. First, we feed the input to the input layer, and then the encoder learns
    the representation of the input and maps it to the bottleneck. From the bottleneck,
    the decoder reconstructs the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08400dbf-eb6e-4d1d-b6b2-8189fc9b2542.png)'
  prefs: []
  type: TYPE_IMG
- en: We might wonder what the use of this is. Why do we need to encode and decode
    the inputs? Why do we just have to reconstruct the input? Well, there are various
    applications such as dimensionality reduction, data compression, image denoising,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Since the autoencoder reconstructs the inputs, the number of nodes in the input
    and output layers are always the same. Let's assume we have a dataset that contains
    100 input features, and we have a neural network with an input layer of 100 units,
    a hidden layer of 50 units, and an output layer with 100 units. When we feed the
    dataset to the autoencoder, the encoder tries to learn the important features
    in the dataset and reduces the number of features to 50 and forms the bottleneck.
    The bottleneck holds the representations of the data, that is, the embeddings
    of the data, and encompasses only the necessary information. Then, the bottleneck
    is fed to the decoder to reconstruct the original input. If the decoder reconstructs
    the original input successfully, then it means that the encoder has successfully
    learned the encodings or representations of the given input. That is, the encoder
    has successfully encoded or compressed the dataset of 100 features into a representation
    with only 50 features by capturing the necessary information.
  prefs: []
  type: TYPE_NORMAL
- en: So, essentially the encoder tries to learn to reduce the dimensionality of the
    data without losing the useful information. We can think of autoencoders as similar
    to dimensionality reduction techniques such as **Principal Component Analysis**
    (**PCA**). In PCA, we project the data into a low dimension using linear transformation
    and remove the features that are not required. The difference between PCA and
    an autoencoder is that PCA uses linear transformation for dimensionality reduction
    while the autoencoder uses a nonlinear transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from dimensionality reduction, autoencoders are also widely used for denoising
    noise in the images, audio, and so on. We know that the encoder in the autoencoder
    reduces the dimensionality of the dataset by learning only the necessary information
    and forms the bottleneck or code. Thus, when the noisy image is fed as an input
    to the autoencoder, the encoder learns only the necessary information of the image
    and forms the bottleneck. Since the encoder learns only the important and necessary
    information to represent the image, it learns that noise is unwanted information
    and removes the representations of noise from the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, now we will have a bottleneck, that is, a representation of the image
    without any noise information. When this learned representation of the encoder,
    that is, the bottleneck, is fed to the decoder, the decoder reconstructs the input
    image from the encodings produced by the encoder. Since the encodings have no
    noise, the reconstructed image will not contain any noise.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, autoencoders map our data of a high dimension data to a low-level
    representation. This low-level data representation of data is called as **latent
    representations** or **bottleneck** which have only meaningful and important features
    that represent the input.
  prefs: []
  type: TYPE_NORMAL
- en: Since the role of our autoencoder is to reconstruct its input, we use a reconstruction
    error as our loss function, which implies we try to understand how much of the
    input is properly reconstructed by the decoder. So, we can use mean squared error
    loss as our loss function to quantify the performance of autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood what autoencoders are, we will explore the architecture
    of autoencoders in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have just learned, autoencoders consist of two important components:
    an encoder ![](img/c65fad3f-6838-4bf8-b679-003d28baa3a7.png) and a decoder ![](img/cf3ce104-f87f-4ab7-835c-c995932034e4.png).
    Let''s look at each one of them closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: The encoder ![](img/9a47e5e0-17ff-4efe-b17e-b17526fa5cb4.png)learns
    the input and returns the latent representation of the input. Let''s assume we
    have an input, ![](img/4470ee88-2391-41ec-9171-68702209ff03.png). When we feed
    the input to the encoder, it returns a low-dimensional latent representation of
    the input called code or a bottleneck, ![](img/91e148bc-9386-4668-9944-9e6df178308c.png).
    We represent the parameter of the encoder by ![](img/321cd6ec-64b6-4d9f-b925-56f95d03b72d.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a82daa05-5fb1-43df-84c2-c2a4b7f2c50e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4bfa12c0-9d86-429e-b58f-a354d5f6a92b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Decoder**: The decoder ![](img/d1fb2440-9b44-452c-878f-3b0e32ac279b.png)tries
    to reconstruct the original input ![](img/cff89e6e-fe17-4a4c-a132-24984ddd4d06.png)using
    the output of the encoder that is code ![](img/ff91f3f9-49f1-4b43-abc4-176737c14fc1.png)as
    an input. The reconstructed image is represented by ![](img/8166f7f5-d832-4f16-a73d-ccf5e48e894f.png).
    We represent the parameters of the decoder by ![](img/3d8f2b54-3177-4537-98d4-c0a90116f100.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f30df43a-7e30-4226-8a47-1d1ca038d699.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/8a27d14b-69f3-46fb-8045-597e058b1932.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to learn the optimal parameters, ![](img/bfdff208-233d-410f-9bf8-a9d9b8ba6aaa.png)
    and ![](img/3ab8fb2d-f6e3-404b-85e5-1410304272ef.png), of our encoder and decoder
    respectively so that we can minimize the reconstruction loss. We can define our
    loss function as the mean squared error between the actual input and reconstructed
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fda0da7-1495-4aed-a959-687fcf7eb9fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/aa817e65-f1ca-498f-9e6f-147c514a640c.png) is the number of training
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: When the latent representation has a lesser dimension than the input, then it
    is called an **undercomplete autoencoder.** Since the dimensions are less, undercomplete
    autoencoders try to learn and retain the only useful distinguishing and important
    features of the input and remove the rest. When the latent representation has
    a dimension greater than or the same as the input, the autoencoders will just
    copy the input without learning any useful features, and such a type of autoencoder
    is called **overcomplete autoencoders**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Undercomplete and overcomplete autoencoders are shown in the following diagram.
    Undercomplete autoencoders have fewer neurons in the hidden layer (code) than
    the number of neurons in the input layer; while in the overcomplete autoencoders,
    the number of neurons in the hidden layer (code) is greater than the number of
    units in the input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fceabe6-527b-4d7b-853b-680a094ec75d.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, by limiting the neurons in the hidden layer (code), we can learn the useful
    representations of the input. Autoencoders can also have any number of hidden
    layers. Autoencoders with multiple hidden layers are called **multilayer autoencoders**
    or **deep autoencoders**. What we have learned so far is just the **vanilla**
    or the **shallow autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing the MNIST images using an autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will learn how autoencoders reconstruct handwritten digits using the
    MNIST dataset. First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s load the MNIST dataset. Since we are reconstructing the given input,
    we don''t need the labels. So, we just load `x_train` for training and `x_test`
    for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the data by dividing by the max pixel value, which is `255`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the `shape` of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the images as a 2D array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the shape of the data would become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Defining the encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we define the encoder layer, which takes the images as an input and returns
    the encodings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the size of the encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholders for the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the encoder which takes `input_image` and returns the encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Defining the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s define the decoder which takes the encoded values from the encoder and
    returns the reconstructed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we defined encoder and decoder, we define the model which takes images
    as input and returns the output of the decoder which is the reconstructed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at a summary of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model with `loss` as a binary cross-entropy and minimize the loss
    using the `adadelta` optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now let's train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, we train the model as `model.fit(x,y)` where `x` is the input and
    `y` is the label. But since autoencoders reconstruct their inputs, the input and
    output to the model should be the same. So, here, we train the model as `model.fit(x_train,
    x_train)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Reconstructing images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have trained the model, we see how the model is reconstructing
    the images of the test set. Feed the test images to the model and get the reconstructed
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Plotting reconstructed images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let us plot the actual images, that is, input images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the actual images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37773424-afde-4562-bd8c-7320360ad538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the reconstructed image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the reconstructed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f12fa78-b120-43e2-b4a0-8fb789bf913a.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the autoencoder has learned better representations of the input
    images and reconstructed them.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders with convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned what autoencoders are in the previous section. We learned about
    a vanilla autoencoder, which is basically the feedforward shallow network with
    one hidden layer. Instead of keeping them as a feedforward network, can we use
    them as a convolutional network? Since we know that a convolutional network is
    good at classifying and recognizing images (provided that we use convolutional
    layers instead of feedforward layers in the autoencoders), it will learn to reconstruct
    the inputs better when the inputs are images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we introduce a new type of autoencoders called CAEs that use a convolutional
    network instead of a vanilla neural network. In the vanilla autoencoders, encoders
    and decoders are basically a feedforward network. But in CAEs, they are basically
    convolutional networks. This means the encoder consists of convolutional layers
    and the decoder consists of transposed convolutional layers, instead of a feedforward
    network. A CAE is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5250f8e-2b95-4554-9fb5-7f84ac083f25.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown, we feed the input image to the encoder that consists of a convolutional
    layer, and the convolutional layer performs the convolution operation and extracts
    important features from the image. We then perform max pooling to keep only the
    important features of the image. In a similar fashion, we perform several convolutional
    and max-pooling operations and obtain a latent representation of the image, called
    a **bottleneck**.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we feed the bottleneck to the decoder that consists of deconvolutional
    layers, and the deconvolutional layer performs the deconvolution operation and
    tries to reconstruct the image from the bottleneck. It consists of several deconvolutional
    and upsampling operations to reconstruct the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, this is how CAE uses convolutional layers in the encoder and transpose
    convolutional layers in the decoders to reconstruct the image.
  prefs: []
  type: TYPE_NORMAL
- en: Building a convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as we learned how to implement an autoencoder in the previous section,
    implementing a CAE is also the same, but the only difference is here we use convolutional
    layers in the encoder and decoder instead of a feedforward network. We will use
    the same MNIST dataset to reconstruct the images using CAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Read and reshape the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the shape of our input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Defining the encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's define our encoder. Unlike vanilla autoencoders, where we use feedforward
    networks, here we use a convolutional network. Hence, our encoder comprises three
    convolutional layers, followed by a max pooling layer with `relu` activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the first convolutional layer, followed by a max pooling operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second convolutional and max pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the final convolutional and max pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Defining the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we define our decoder; in the decoder, we perform the deconvolution operation
    with three layers, that is, we upsample the encodings created by the encoder and
    reconstruct the original image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the first convolutional layer followed by upsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second convolutional layer with upsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the final convolutional layer with upsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the model that takes an input image and returns the images generated
    by the decoder, which is reconstructed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile the model with loss as binary cross-entropy and we use `adadelta`
    as our optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, train the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Reconstructing the images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reconstruct the images using our trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let us plot the input images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the input images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91fae3e-e957-48a5-bbef-ed6872786eea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we plot the reconstructed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the reconstructed images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f684ba9-6b34-4595-b0e8-fa916886d968.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DAE are another small variant of the autoencoder. They are mainly used to remove
    noise from the image, audio, and other inputs. So, when we feed the corrupted
    input to the DAE, it learns to reconstruct the original uncorrupted input. Now
    we inspect how DAEs remove the noise.
  prefs: []
  type: TYPE_NORMAL
- en: With a DAE, instead of feeding the raw input to the autoencoder, we corrupt
    the input by adding some stochastic noise and feed the corrupted input. We know
    that the encoder learns the representation of the input by keeping only important
    information and maps the compressed representation to the bottleneck. When the
    corrupted input is sent to the encoder, while learning the representation of the
    input encoder will learn that noise is unwanted information and removes its representation.
    Thus, encoders learn the compact representation of the input without noise by
    keeping only necessary information, and map the learned representation to the
    bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Now the decoder tries to reconstruct the image using the representation learned
    by the encoder, that is, the bottleneck. Since the representation, does not contain
    any noise, decoders reconstruct the input without noise. This is how a DAE removes
    noise from the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical DAE is shown in the following diagram. First, we corrupt the input
    by adding some noise and feed the corrupted input to the encoder, which learns
    the representation of the input without the noise, while the decoder reconstructs
    the uncorrupted input using the representation learned by the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d4a068f-ddc8-4eb9-9611-0c7777d485fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Mathematically, this can be expressed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have an image, ![](img/ee167338-3666-4b9c-b1ad-53533b983029.png), and
    we add noise to the image and get ![](img/b8bb320e-3dc9-4bdf-8661-0143a805cfad.png)
    which is the corrupted image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b85c1b7-2605-410e-a823-69da021bace2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now feed this corrupted image to the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5845ebff-5585-46e3-baf0-9408d64afc78.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3387b54b-01f1-446a-bac0-e700b2bb7eba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The decoder tries to reconstruct the actual image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/872eb111-1108-4874-a983-4b2b9f925f69.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/bcee23a0-4d4e-4313-96e0-969c734eedf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Denoising images using DAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to denoise the images using DAE. We use CAE
    for denoising the images. The code for DAE is just the same as CAE, except that
    here, we use noisy images in the input. Instead of looking at the whole code,
    we will see only the respective changes. The complete code is available on GitHub
    at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the noise factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Add noise to the train and test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Clip the train and test set by 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the model. Since, we want the model to learn to remove noise in
    the image, input to the model is the noisy images, that is, `x_train_noisy` and
    output is the denoised images, that is, `x_train`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Reconstruct images using our trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let us plot the input image which is the corrupted image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the input noisy image is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7086c1f1-cf80-4f56-a3c6-a3dec5336f7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us plot the reconstructed images by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, our model has learned to remove noise from the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/214f7f30-9858-445e-9a5e-875142681057.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding sparse autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that autoencoders learn to reconstruct the input. But when we set the
    number of nodes in the hidden layer greater than the number of nodes in the input
    layer, then it will learn an identity function which is not favorable, because
    it just completely copies the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having more nodes in the hidden layer helps us to learn robust latent representation.
    But when there are more nodes in the hidden layer, the autoencoder tries to completely
    mimic the input and thus it overfits the training data. To resolve the problem
    of overfitting, we introduce a new constraint to our loss function called the
    **sparsity constraint** or **sparsity penalty**. The loss function with sparsity
    penalty can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbda4978-3f00-4edc-a739-8dbd1de211ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term ![](img/d270d179-f57f-45e5-906f-28816e7158ad.png) represents
    the reconstruction error between the original input ![](img/8535044c-1614-4749-980f-9e7cd0cd3632.png)
    and reconstructed input, ![](img/5d8aa7a5-a8ce-483d-94f9-824d79491ba8.png). The
    second term implies the sparsity constraint. Now we will explore how this sparse
    constraint mitigates the problem of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Using the sparsity constraint, we activate only specific neurons on the hidden
    layer instead of activating all the neurons. Based on the input, we activate and
    deactivate specific neurons so the neurons, when they are activated, will learn
    to extract important features from the input. By having the sparse penalty, autoencoders
    will not copy the input exactly to the output and it can also learn the robust
    latent representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, sparse autoencoders have more units in the
    hidden layer than the input layer; however, only a few neurons in the hidden layer
    are activated. The unshaded neurons represent the neurons that are currently activated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae4a2689-b6dc-4aae-8167-e662895f00bd.png)'
  prefs: []
  type: TYPE_IMG
- en: A neuron returns 1 if it's active and 0 if it is inactive. In sparse autoencoders,
    we set most of the neurons in the hidden layer to inactive. We know that the sigmoid
    activation function squashes the value to between 0 and 1\. So, when we use the
    sigmoid activation function, we try to keep the values of neurons close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: We typically try to keep the average activation value of each neuron in the
    hidden layer close to zero, say 0.05, but not equal to zero, and this value is
    called ![](img/060f2b67-744d-48dd-b158-29257cebee90.png), which is our sparsity
    parameter. We typically set the value of ![](img/060f2b67-744d-48dd-b158-29257cebee90.png)
    to 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: First, we calculate the average activation of a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The average activation of the ![](img/27815132-2e2f-4c4f-b541-72464797a683.png)
    neuron in the hidden layer ![](img/3728baba-3198-4a8c-9617-946925898ac3.png) over
    the whole training set can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb87428-1fc2-40b3-9251-fec0b5d2027f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96632d24-2330-47b2-b2b5-39bb98b48348.png)denotes the average activation
    of the ![](img/bc7e31cd-af8d-4470-a98d-473a048fd780.png)neuron in the hidden layer
    ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/1b6dd202-1bed-4a64-9785-18c50e18d543.png)is the number of the training
    sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/39738fc2-1272-4ce6-b312-2812dfa82d5f.png)is the activation of ![](img/c3aaa92e-1573-4bd5-b679-e6a435ae45e2.png)neuron
    in the hidden layer ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/a940740f-e484-41e7-8a9c-31b9340ade49.png)is the ![](img/9c486b3d-9c82-4b8a-8449-70628da1cf67.png)training
    sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ea1e30a9-b545-4416-a16f-5f11103b55ba.png)implies the activation of
    the [![](img/912fa4b2-4be0-49ef-9f00-82abba816b56.png)] neuron in the hidden layer
    ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)for the [![](img/b6259e8c-84dd-44e7-917c-dcd8fd58809c.png)]^(th)
    training sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We try to keep the average activation value, ![](img/4d1aed0a-39d6-449f-8c66-627df7c0e9bb.png),
    of the neurons close to ![](img/060f2b67-744d-48dd-b158-29257cebee90.png). That
    is, we try to keep the average activation values of the neurons to 0.05:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c875dca7-014c-4e2a-b19c-e71cf3b7097d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we penalize the value of ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png),
    which is varying from ![](img/060f2b67-744d-48dd-b158-29257cebee90.png). We know
    that the **Kullback–Leibler** (**KL**) divergence is widely used for measuring
    the difference between the two probability distributions. So, here, we use the
    KL divergence to measure the difference between two **Bernoulli distributions**,
    that is, mean ![](img/060f2b67-744d-48dd-b158-29257cebee90.png) and mean ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png)
    and it can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d68f8f6-497d-4141-bef3-39ffca40111d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the earlier equation, ![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png) denotes
    the hidden layer ![](img/3f117009-6b17-42df-9aae-35da95019646.png), and ![](img/702ac143-726d-44a4-8b2e-83cbfdb643a8.png)
    denotes the ![](img/02d427ef-0ba3-48a1-9c60-eeeb796a9216.png)neurons in the hidden
    layer ![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png). The earlier equation
    is basically the sparse penalty or sparsity constraint. Thus, with the sparsity
    constraint, all the neurons will never be active at the same time, and on average,
    they are set to 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can rewrite the loss function with a sparse penalty as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acc8da4c-6096-4595-85e8-996472f633b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, sparse autoencoders allow us to have a greater number of nodes in the
    hidden layer than the input layer, yet reduce the problem of overfitting with
    the help of the sparsity constraint in the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Building the sparse autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a sparse autoencoder is just as same as building a regular autoencoder,
    except that we use a sparse regularizer in the encoder and decoder, so instead
    of looking at the whole code in the following sections, we will only look at the
    parts related to implementing the sparse regularizer; the complete code with an
    explanation is available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the sparse regularizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the code to define the sparse regularizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Set our ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png) value to `0.05`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png), which is the
    mean activation value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the KL divergence between the mean ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png)
    and the mean ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png) according to equation
    *(1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Sum the KL divergence values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiply the `sum` by `beta` and return the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The whole function for the sparse regularizer is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Learning to use contractive autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like sparse autoencoders, **contractive autoencoders** add a new regularization
    term to the loss function of the autoencoders. They try to make our encodings
    less sensitive to the small variations in the training data. So, with contractive
    autoencoders, our encodings become more robust to small perturbations such as
    noise present in our training dataset. We now introduce a new term called the
    **regularizer** or **penalty term** to our loss function. It helps to penalize
    the representations that are too sensitive to the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss function can be mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/555d6657-2908-4e34-afc0-ef9483f96144.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term represents the reconstruction error and the second term represents
    the penalty term or the regularizer and it is basically the **Frobenius** **norm**
    of the **Jacobian matrix**. Wait! What does that mean?
  prefs: []
  type: TYPE_NORMAL
- en: The Frobenius norm, also called the **Hilbert-Schmidt norm**, of a matrix is
    defined as the square root of the sum of the absolute square of its elements.
    A matrix comprising a partial derivative of the vector-valued function is called
    the **Jacobian matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, calculating the Frobenius norm of the Jacobian matrix implies our penalty
    term is the sum of squares of all partial derivatives of the hidden layer with
    respect to the input. It is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89f50a8e-25a1-4d00-8bae-3ed4c134e2ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculating the partial derivative of the hidden layer with respect to the
    input is similar to calculating gradients of loss. Assuming we are using the sigmoid
    activation function, then the partial derivative of the hidden layer with respect
    to the input is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32880779-7c18-4c95-b720-3753579a110a.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding the penalty term to our loss function helps in reducing the sensitivity
    of the model to the variations in the input and makes our model more robust to
    the outliers. Thus, contractive autoencoders reduce the sensitivity of the model
    to the small variations in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the contractive autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building the contractive autoencoder is just as same as building the autoencoder,
    except that we use the contractive loss regularizer in the model, so instead of
    looking at the whole code, we will only look at the parts related to implementing
    the contractive loss.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the contractive loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's see how to define the loss function in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the mean squared loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain the weights from our encoder layer and transpose the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the output of our encoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the penalty term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The final loss is the sum of mean squared error and the penalty term multiplied
    by `lambda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for contractive loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Dissecting variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see another very interesting type of autoencoders called **variational
    autoencoders** (**VAE**). Unlike other autoencoders, VAEs are generative models
    that imply they learn to generate new data just like GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a dataset containing facial images of many individuals. When
    we train our variational autoencoder with this dataset, it learns to generate
    new realistic faces that are not seen in the dataset. VAEs have various applications
    because of their generative nature and some of them include generating images,
    songs, and so on. But what makes VAE generative and how is it different than other
    autoencoders? Let's learn that in the coming section.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we learned when discussing GANs, for a model to be generative, it has
    to learn the distribution of the inputs. For instance, let's say we have a dataset
    that consists of handwritten digits, such as the MNIST dataset. Now, in order
    to generate new handwritten digits, our model has to learn the distribution of
    the digits in the given dataset. Learning the distribution of the digits present
    in the dataset helps VAE to learn useful properties such as digit width, stroke,
    height, and so on. Once the model encodes this property in its distribution, then
    it can generate new handwritten digits by sampling from the learned distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have a dataset of human faces, then learning the distribution of the
    faces in the dataset helps us to learn various properties such as gender, facial
    expression, hair color, and so on. Once the model learns and encode these properties
    in its distribution, then it can generate a new face just by sampling from the
    learned distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in VAE, instead of mapping the encoder''s encodings directly to the latent
    vector (bottleneck), we map the encodings to a distribution; usually, it is a
    Gaussian distribution. We sample a latent vector from this distribution and feed
    it to a decoder then the decoder learns to reconstruct the image. As shown in
    the following diagram, an encoder maps its encodings to a distribution and we
    sample a latent vector from this distribution and feed it to a decoder to reconstruct
    an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9861a0f1-589e-4d93-a158-46310c4d5b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A **gaussian distribution** can be parameterized by its mean and covariance
    matrix. Thus, we can make our encoder generate its encoding and maps it to a mean
    vector and standard deviation vector that approximately follows the Gaussian distribution.
    Now, from this distribution, we sample a latent vector and feed it to our decoder
    which then reconstructs an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60c96b04-e98b-449e-854f-db846d3c2577.png)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, the encoder learns the desirable properties of the given input
    and encodes them into distribution. We sample a latent vector from this distribution
    and feed the latent vector as input to the decoder which then generates images
    learned from the encoder's distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In VAE, the encoder is also called as **recognition** **model** and the decoder
    is also called as **g****enerative** **model**. Now that we have an intuitive
    understanding of VAE, in the next section, we will go into detail and learn how
    VAE works.
  prefs: []
  type: TYPE_NORMAL
- en: Variational inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s get familiar with the notations:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's represent the distribution of the input dataset by ![](img/7cb7aa48-8ffc-405f-b340-25d5b2790817.png),
    where ![](img/a613c746-aa3a-42f2-ac6d-5bcd8fd5bc24.png)represents the parameter
    of the network that will be learned during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We represent the latent variable by ![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png),
    which encodes all the properties of the input by sampling from the distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/91dc9cc1-f422-456f-8d57-25ca9b52708a.png)denotes the joint distribution
    of the input ![](img/e05c5e69-f4c0-443f-b911-07041c10049c.png)with their properties,
    ![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/bec5dd91-4523-4f5a-8567-6bbb50b974ad.png)represents the distribution
    of the latent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the Bayesian theorem, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7bcfc0-e7c4-472c-95be-a0ae6b641a77.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation helps us to compute the probability distribution of the
    input dataset. But the problem lies in computing ![](img/275d0bd5-e949-4dff-84a3-5e478ad671b5.png),
    because computing it is intractable. Thus, we need to find a tractable way to
    estimate the ![](img/4b09580c-ca0a-4926-bd62-e25d1e3f1528.png). Here, we introduce
    a concept called **variational inference**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of inferring the distribution of ![](img/eef7c7d5-7470-4ea3-8696-06ea4042bc47.png)
    directly, we approximate them using another distribution, say a Gaussian distribution
    ![](img/65d7617b-e92b-45a5-8e95-55e84dd2ff70.png). That is, we use ![](img/7d233498-9a18-43cb-be9e-30f5fb70e7a0.png)
    which is basically a neural network parameterized by ![](img/61a4f823-3ae8-46c4-ae6e-665e03617e23.png)
    parameter to estimate the value of ![](img/3d86662e-e642-453a-a9d6-1badf3fbb723.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab54b77c-6849-40bc-bc5d-def8243a2b8a.png)is basically our probabilistic
    encoder; that is, they to create a latent vector *z* given ![](img/3566f116-c99f-4b80-9ea3-74fc64ae306d.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/b44be5c0-1e45-4c12-8449-91435f295a2e.png)is the probabilistic decoder;
    that is, it tries to construct the input ![](img/ccacf299-a48d-48e5-92cb-6c8ebb552af7.png)given
    the latent vector ![](img/8e8f5f3a-17ab-454a-8de9-661720e62c10.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'The following diagram helps you attain good clarity on the notations and what
    we have seen so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d58c046-7c2f-43e3-b1ec-7eb361ed0ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned that we use ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)
    to approximate ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png). Thus, the estimated
    value of ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) should be close to
    ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png). Since these both are distributions,
    we use KL divergence to measure how ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)
    diverges from ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png) and we need to
    minimize the divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'A KL divergence between ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) and
    ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png) is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/373898a4-30a7-43b9-8ede-c0374190d5c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know ![](img/d96ce8de-ee39-40be-8f5c-34bc988755c9.png), substituting
    this in the preceding equation, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f04d2335-6171-43bd-8e54-934550d990f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know *log (a/b) = log(a) - log(b)*, we can rewrite the preceding equation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13b728b6-3bce-4261-ab1c-3e89e509324d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can take the ![](img/01774ce0-9e3e-4b97-9c58-74b450efc2af.png) outside the
    expectations since it has no dependency on ![](img/60780d92-f46d-413e-9da5-038b51feefe8.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/174ab8f7-b314-4740-9eac-013a1d1f5a5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know *log(ab) = log (a) + log(b)*, we can rewrite the preceding equation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65e3f3ee-ab4a-46c7-bb7f-453f73f467ce.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/71755a3b-640a-466d-8dbf-2b70dc4f95de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that KL divergence between ![](img/3c1b3c68-c1dd-4e27-bee9-393c60797ebb.png)
    and ![](img/24a816e7-23a9-43b2-8dff-fcd64ef66295.png) can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/946b6261-6e49-4f14-8bef-99671173b5f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(2)* in equation *(1)* we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/588f307e-c39f-4807-94c5-c4110aa99bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Rearranging the left and right-hand sides of the equation, we can write the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfda9f6f-60f9-45fe-ba31-cc1aa663e194.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Rearranging the terms, our final equation can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/def29c99-e58f-465e-88d3-89540f1dbd80.png)'
  prefs: []
  type: TYPE_IMG
- en: What does the above equation imply?
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side of the equation is also known as the **variational lower
    bound** or the **evidence lower bound** (**ELBO**). The first term in the left-hand
    side ![](img/c3658f8d-cd12-47e9-88cc-f29bcebdc976.png) implies the distribution
    of the input *x*, which we want to maximize and ![](img/8c095851-14f2-45ba-a43f-f3d53ad08caf.png)
    implies the KL divergence between the estimated and the real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b36c24f-e352-4352-a777-13ddb87c51ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, you will notice the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3147eae6-43f5-4dd8-bbc6-83242ab53e3c.png)implies we are maximizing
    the distribution of the input; we can convert the maximization problem into minimization
    by simply adding a negative sign; thus, we can write ![](img/352d5d33-1842-4d4a-8aad-1ac9f7a45865.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/b477c3d6-8c06-4433-9a64-a63a0ba04d3f.png)implies we are maximizing
    the KL divergence between the estimated and real distribution, but we want to
    minimize them, so we can write ![](img/673c19ad-c94e-4e0b-b2a0-169a3dc59ab1.png)to
    minimize the KL divergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, our loss function becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: ^(![](img/83fbb709-c90b-402c-b991-9c89c2a46405.png))
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b784e93-0e8c-455a-b8f3-9f1ba156ea73.png)'
  prefs: []
  type: TYPE_IMG
- en: If you look at this equation, ![](img/cb826f28-0f1c-4619-9919-f426752f7735.png)
    basically implies the reconstruction of the input, that is, the decoder which
    takes the latent vector ![](img/bafe6f36-ba5c-4d77-bd79-62ea348576e3.png) and
    reconstructs the input ![](img/0408a610-5dd5-4478-bac9-7b1da33170a8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, our final loss function is the sum of the reconstruction loss and the
    KL divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34903840-530c-4af8-bdd8-5cc37922025a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value for KL divergence is simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddfcb727-e6b5-4ca0-82eb-8875888153b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, minimizing the preceding loss function implies we are minimizing the reconstruction
    loss and also minimizing the KL divergence between the estimated and real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Reparameterization trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We face a problem while training VAE a through gradient descent. Remember,
    we are performing a sampling operation to generate a latent vector. Since a sampling
    operation is not differentiable, we cannot calculate gradients. That is, while
    backpropagating the network to minimize the error, we cannot calculate the gradients
    of the sampling operation as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e16aa56-b27b-4198-9b08-678a5039bfed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, to combat this, we introduce a new trick called the **reparameterization
    trick**. We introduce a new parameter called **epsilon**, which we randomly sample
    from a unit Gaussian, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edc09dff-7c1c-4b9f-8c06-d8a4647da372.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And now we can rewrite our latent vector ![](img/2782f2b6-cb1f-4318-a3b5-b40add7dd37a.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1a1d374-301f-4928-a2d9-e652e98c7e4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The reparameterization trick is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7932bf5-a710-4e8a-a152-696d466210ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, with the reparameterization trick, we can train the VAE with the gradient
    descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images using VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have understood how the VAE model works, in this section, we will
    learn how to use VAE to generate images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define some important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Defining the encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Encoder hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean and the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Defining the sampling operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the sampling operation with a reparameterization trick that samples
    the latent vector from the encoder''s distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the latent vector *z* from the mean and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Defining the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the decoder with two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Reconstruct the images using decoder which takes the latent vector ![](img/7b049780-80c1-4a44-8bef-98d45421b9a3.png)
    as input and returns the reconstructed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We build the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the reconstruction loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Define KL divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the total loss can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Add loss and compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Defining the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the generator samples from the learned distribution and generates an
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Plotting generated images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we plot the image generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the plot of the image generated by a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a7a7d2e-177b-4966-af88-d8baf6897a0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by learning what autoencoders are and how autoencoders
    are used to reconstruct their own input. We explored convolutional autoencoders,
    where instead of using feedforward networks, we used convolutional and deconvolutional
    layers for encoding and decoding, respectively. Following this, we learned about
    sparse which activate only certain neurons. Then, we learned about another type
    of regularizing autoencoder, called a contractive autoencoder, and at the end
    of the chapter, we learned about VAE which is a generative autoencoder model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about how to learn from a less data points
    using few-shot learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s examine our knowledge of autoencoders by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are autoencoders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the objective function of autoencoders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do convolutional autoencoders differ from vanilla autoencoders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are denoising autoencoders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the average activation of the neuron computed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function of contractive autoencoders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the Frobenius norm and Jacobian matrix?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also check the following links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sparse autoencoder* notes by Andrew Ng, [https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contractive Auto-Encoders: Explicit Invariance During Feature Extraction*
    by Salah Rifai, et al., [http://www.icml-2011.org/papers/455_icmlpaper.pdf](http://www.icml-2011.org/papers/455_icmlpaper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Variational Autoencoder for Deep Learning of Images, Labels and Captions*
    by Yunchen Pu, et al., [https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf](https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
