- en: Reconstructing Inputs Using Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器重建输入
- en: '**Autoencoders** are unsupervised learning algorithm. Unlike other algorithms,
    autoencoders learn to reconstruct the input, that is, an autoencoder takes the
    input and learns to reproduce the input as an output. We start the chapter by
    understanding what are autoencoders and how exactly they reconstruct the input.
    Then, we will learn how autoencoders reconstruct MNIST images.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**是一种无监督学习算法。与其他算法不同，自编码器学习重建输入，即自编码器接收输入并学习将其重现为输出。我们从理解什么是自编码器及其如何重建输入开始这一章节。然后，我们将学习自编码器如何重建MNIST图像。'
- en: Going ahead, we will learn about the different variants of autoencoders; first,
    we will learn about **convolutional autoencoders** (**CAEs**), which use convolutional
    layers; then, we will learn about how **denoising autoencoders** (**DAEs**) which
    learn to remove noise in the input. After this, we will understand sparse autoencoders
    and how they learn from sparse inputs. At the end of the chapter, we will learn
    about an interesting generative type of autoencoders called **variational autoencoders**.
    We will understand how variational autoencoders learn to generate new inputs and
    how they differ from other autoencoders.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解自编码器的不同变体；首先，我们将学习使用卷积层的**卷积自编码器**（**CAEs**）；然后，我们将学习**去噪自编码器**（**DAEs**），它们学习如何去除输入中的噪音。之后，我们将了解稀疏自编码器及其如何从稀疏输入中学习。在本章末尾，我们将学习一种有趣的生成型自编码器，称为**变分自编码器**。我们将理解变分自编码器如何学习生成新的输入及其与其他自编码器的不同之处。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Autoencoders and their architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器及其架构
- en: Reconstructing MNIST images using autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器重建MNIST图像
- en: Convolutional autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: Building convolutional autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建卷积自编码器
- en: Denoising autoencoders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降噪自编码器
- en: Removing noise in the image using denoising autoencoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降噪自编码器去除图像中的噪音
- en: Sparse autoencoders
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Contractive autoencoders
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧束缚自编码器
- en: Variational autoencoders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: What is an autoencoder?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自编码器？
- en: An **autoencoder** is an interesting unsupervised learning algorithm. Unlike
    other neural networks, the objective of the autoencoder is to reconstruct the
    given input; that is, the output of the autoencoders is the same as the input.
    It consists of two important components called the **encoder** and the **decoder**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**是一种有趣的无监督学习算法。与其他神经网络不同，自编码器的目标是重建给定的输入；即自编码器的输出与输入相同。它由称为**编码器**和**解码器**的两个重要组件组成。'
- en: 'The role of the encoder is to encode the input by learning the latent representation
    of the input, and the role of the decoder is to reconstruct the input from the
    latent representation produced by the encoder. The latent representation is also
    called **bottleneck** or **code**. As shown in the following diagram, an image
    is passed as an input to the autoencoder. An encoder takes the image and learns
    the latent representation of the image. The decoder takes the latent representation
    and tries to reconstruct the image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的作用是通过学习输入的潜在表示来编码输入，解码器的作用是从编码器生成的潜在表示中重建输入。潜在表示也称为**瓶颈**或**编码**。如下图所示，将图像作为输入传递给自编码器。编码器获取图像并学习图像的潜在表示。解码器获取潜在表示并尝试重建图像：
- en: '![](img/e8c5f6fa-9522-4fc2-b1a8-f10d4e451148.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8c5f6fa-9522-4fc2-b1a8-f10d4e451148.png)'
- en: 'A simple vanilla autoencoder with two layers is shown in the following diagram;
    as you may notice, it consists of an input layer, a hidden layer, and an output
    layer. First, we feed the input to the input layer, and then the encoder learns
    the representation of the input and maps it to the bottleneck. From the bottleneck,
    the decoder reconstructs the input:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个简单的双层香草自编码器；正如您可能注意到的，它由输入层、隐藏层和输出层组成。首先，我们将输入馈送到输入层，然后编码器学习输入的表示并将其映射到瓶颈。从瓶颈处，解码器重建输入：
- en: '![](img/08400dbf-eb6e-4d1d-b6b2-8189fc9b2542.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08400dbf-eb6e-4d1d-b6b2-8189fc9b2542.png)'
- en: We might wonder what the use of this is. Why do we need to encode and decode
    the inputs? Why do we just have to reconstruct the input? Well, there are various
    applications such as dimensionality reduction, data compression, image denoising,
    and more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想知道这的用途是什么。为什么我们需要编码和解码输入？为什么我们只需重建输入？嗯，有各种应用，如降维、数据压缩、图像去噪等。
- en: Since the autoencoder reconstructs the inputs, the number of nodes in the input
    and output layers are always the same. Let's assume we have a dataset that contains
    100 input features, and we have a neural network with an input layer of 100 units,
    a hidden layer of 50 units, and an output layer with 100 units. When we feed the
    dataset to the autoencoder, the encoder tries to learn the important features
    in the dataset and reduces the number of features to 50 and forms the bottleneck.
    The bottleneck holds the representations of the data, that is, the embeddings
    of the data, and encompasses only the necessary information. Then, the bottleneck
    is fed to the decoder to reconstruct the original input. If the decoder reconstructs
    the original input successfully, then it means that the encoder has successfully
    learned the encodings or representations of the given input. That is, the encoder
    has successfully encoded or compressed the dataset of 100 features into a representation
    with only 50 features by capturing the necessary information.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器重构输入，输入层和输出层中的节点数始终相同。假设我们有一个包含100个输入特征的数据集，我们有一个神经网络，其输入层有100个单元，隐藏层有50个单元，输出层有100个单元。当我们将数据集输入到自编码器中时，编码器尝试学习数据集中的重要特征，并将特征数减少到50并形成瓶颈。瓶颈保存数据的表示，即数据的嵌入，并仅包含必要信息。然后，将瓶颈输入到解码器中以重构原始输入。如果解码器成功地重构了原始输入，那么说明编码器成功地学习了给定输入的编码或表示。也就是说，编码器成功地将包含100个特征的数据集编码成仅包含50个特征的表示，通过捕获必要信息。
- en: So, essentially the encoder tries to learn to reduce the dimensionality of the
    data without losing the useful information. We can think of autoencoders as similar
    to dimensionality reduction techniques such as **Principal Component Analysis**
    (**PCA**). In PCA, we project the data into a low dimension using linear transformation
    and remove the features that are not required. The difference between PCA and
    an autoencoder is that PCA uses linear transformation for dimensionality reduction
    while the autoencoder uses a nonlinear transformation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编码器本质上尝试学习如何在不丢失有用信息的情况下减少数据的维度。我们可以将自编码器视为类似于**主成分分析**（**PCA**）的降维技术。在PCA中，我们通过线性变换将数据投影到低维，并去除不需要的特征。PCA和自编码器的区别在于PCA使用线性变换进行降维，而自编码器使用非线性变换。
- en: Apart from dimensionality reduction, autoencoders are also widely used for denoising
    noise in the images, audio, and so on. We know that the encoder in the autoencoder
    reduces the dimensionality of the dataset by learning only the necessary information
    and forms the bottleneck or code. Thus, when the noisy image is fed as an input
    to the autoencoder, the encoder learns only the necessary information of the image
    and forms the bottleneck. Since the encoder learns only the important and necessary
    information to represent the image, it learns that noise is unwanted information
    and removes the representations of noise from the bottleneck.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了降维之外，自编码器还广泛用于去噪图像、音频等中的噪声。我们知道自编码器中的编码器通过仅学习必要信息并形成瓶颈或代码来减少数据集的维度。因此，当噪声图像作为自编码器的输入时，编码器仅学习图像的必要信息并形成瓶颈。由于编码器仅学习表示图像的重要和必要信息，它学习到噪声是不需要的信息并从瓶颈中移除噪声的表示。
- en: Thus, now we will have a bottleneck, that is, a representation of the image
    without any noise information. When this learned representation of the encoder,
    that is, the bottleneck, is fed to the decoder, the decoder reconstructs the input
    image from the encodings produced by the encoder. Since the encodings have no
    noise, the reconstructed image will not contain any noise.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们将会有一个瓶颈，也就是说，一个没有任何噪声信息的图像的表示。当编码器学习到这个被称为瓶颈的编码表示时，将其输入到解码器中，解码器会从编码器生成的编码中重建输入图像。由于编码没有噪声，重建的图像将不包含任何噪声。
- en: In a nutshell, autoencoders map our data of a high dimension data to a low-level
    representation. This low-level data representation of data is called as **latent
    representations** or **bottleneck** which have only meaningful and important features
    that represent the input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，自编码器将我们的高维数据映射到一个低级表示。这种数据的低级表示被称为**潜在表示**或**瓶颈**，只包含代表输入的有意义和重要特征。
- en: Since the role of our autoencoder is to reconstruct its input, we use a reconstruction
    error as our loss function, which implies we try to understand how much of the
    input is properly reconstructed by the decoder. So, we can use mean squared error
    loss as our loss function to quantify the performance of autoencoders.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的自编码器的角色是重建其输入，我们使用重构误差作为我们的损失函数，这意味着我们试图了解解码器正确重构输入的程度。因此，我们可以使用均方误差损失作为我们的损失函数来量化自编码器的性能。
- en: Now that we have understood what autoencoders are, we will explore the architecture
    of autoencoders in the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了什么是自编码器，我们将在下一节探讨自编码器的架构。
- en: Understanding the architecture of autoencoders
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自编码器的架构
- en: 'As we have just learned, autoencoders consist of two important components:
    an encoder ![](img/c65fad3f-6838-4bf8-b679-003d28baa3a7.png) and a decoder ![](img/cf3ce104-f87f-4ab7-835c-c995932034e4.png).
    Let''s look at each one of them closely:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚学到的，自编码器由两个重要组件组成：编码器 ![](img/c65fad3f-6838-4bf8-b679-003d28baa3a7.png)
    和解码器 ![](img/cf3ce104-f87f-4ab7-835c-c995932034e4.png)。让我们仔细看看它们各自的作用：
- en: '**Encoder**: The encoder ![](img/9a47e5e0-17ff-4efe-b17e-b17526fa5cb4.png)learns
    the input and returns the latent representation of the input. Let''s assume we
    have an input, ![](img/4470ee88-2391-41ec-9171-68702209ff03.png). When we feed
    the input to the encoder, it returns a low-dimensional latent representation of
    the input called code or a bottleneck, ![](img/91e148bc-9386-4668-9944-9e6df178308c.png).
    We represent the parameter of the encoder by ![](img/321cd6ec-64b6-4d9f-b925-56f95d03b72d.png):'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器 ![](img/9a47e5e0-17ff-4efe-b17e-b17526fa5cb4.png) 学习输入并返回输入的潜在表示。假设我们有一个输入，![](img/4470ee88-2391-41ec-9171-68702209ff03.png)。当我们将输入馈送给编码器时，它返回输入的低维潜在表示，称为编码或瓶颈，![](img/91e148bc-9386-4668-9944-9e6df178308c.png)。我们用
    ![](img/321cd6ec-64b6-4d9f-b925-56f95d03b72d.png) 表示编码器的参数：'
- en: '![](img/a82daa05-5fb1-43df-84c2-c2a4b7f2c50e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a82daa05-5fb1-43df-84c2-c2a4b7f2c50e.png)'
- en: '![](img/4bfa12c0-9d86-429e-b58f-a354d5f6a92b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bfa12c0-9d86-429e-b58f-a354d5f6a92b.png)'
- en: '**Decoder**: The decoder ![](img/d1fb2440-9b44-452c-878f-3b0e32ac279b.png)tries
    to reconstruct the original input ![](img/cff89e6e-fe17-4a4c-a132-24984ddd4d06.png)using
    the output of the encoder that is code ![](img/ff91f3f9-49f1-4b43-abc4-176737c14fc1.png)as
    an input. The reconstructed image is represented by ![](img/8166f7f5-d832-4f16-a73d-ccf5e48e894f.png).
    We represent the parameters of the decoder by ![](img/3d8f2b54-3177-4537-98d4-c0a90116f100.png):'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：解码器 ![](img/d1fb2440-9b44-452c-878f-3b0e32ac279b.png) 尝试使用编码器的输出，即编码
    ![](img/ff91f3f9-49f1-4b43-abc4-176737c14fc1.png) 作为输入来重建原始输入 ![](img/cff89e6e-fe17-4a4c-a132-24984ddd4d06.png)。重建图像由
    ![](img/8166f7f5-d832-4f16-a73d-ccf5e48e894f.png) 表示。我们用 ![](img/3d8f2b54-3177-4537-98d4-c0a90116f100.png)
    表示解码器的参数：'
- en: '![](img/f30df43a-7e30-4226-8a47-1d1ca038d699.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f30df43a-7e30-4226-8a47-1d1ca038d699.png)'
- en: '![](img/8a27d14b-69f3-46fb-8045-597e058b1932.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a27d14b-69f3-46fb-8045-597e058b1932.png)'
- en: 'We need to learn the optimal parameters, ![](img/bfdff208-233d-410f-9bf8-a9d9b8ba6aaa.png)
    and ![](img/3ab8fb2d-f6e3-404b-85e5-1410304272ef.png), of our encoder and decoder
    respectively so that we can minimize the reconstruction loss. We can define our
    loss function as the mean squared error between the actual input and reconstructed
    input:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要学习编码器和解码器的优化参数，分别表示为 ![](img/bfdff208-233d-410f-9bf8-a9d9b8ba6aaa.png) 和
    ![](img/3ab8fb2d-f6e3-404b-85e5-1410304272ef.png)，以便我们可以最小化重构损失。我们可以定义我们的损失函数为实际输入与重构输入之间的均方误差：
- en: '![](img/7fda0da7-1495-4aed-a959-687fcf7eb9fb.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fda0da7-1495-4aed-a959-687fcf7eb9fb.png)'
- en: Here, ![](img/aa817e65-f1ca-498f-9e6f-147c514a640c.png) is the number of training
    samples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/aa817e65-f1ca-498f-9e6f-147c514a640c.png) 是训练样本的数量。
- en: When the latent representation has a lesser dimension than the input, then it
    is called an **undercomplete autoencoder.** Since the dimensions are less, undercomplete
    autoencoders try to learn and retain the only useful distinguishing and important
    features of the input and remove the rest. When the latent representation has
    a dimension greater than or the same as the input, the autoencoders will just
    copy the input without learning any useful features, and such a type of autoencoder
    is called **overcomplete autoencoders**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当潜在表示的维度小于输入时，这被称为**欠完备自编码器**。由于维度较小，欠完备自编码器试图学习和保留输入的仅有的有用和重要特征，并去除其余部分。当潜在表示的维度大于或等于输入时，自编码器将只是复制输入而不学习任何有用的特征，这种类型的自编码器称为**过完备自编码器**。
- en: 'Undercomplete and overcomplete autoencoders are shown in the following diagram.
    Undercomplete autoencoders have fewer neurons in the hidden layer (code) than
    the number of neurons in the input layer; while in the overcomplete autoencoders,
    the number of neurons in the hidden layer (code) is greater than the number of
    units in the input layer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了欠完备和过完备自编码器。欠完备自编码器在隐藏层（`code`）中的神经元少于输入层中的单元数；而在过完备自编码器中，隐藏层（`code`）中的神经元数大于输入层中的单元数：
- en: '![](img/1fceabe6-527b-4d7b-853b-680a094ec75d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fceabe6-527b-4d7b-853b-680a094ec75d.png)'
- en: Thus, by limiting the neurons in the hidden layer (code), we can learn the useful
    representations of the input. Autoencoders can also have any number of hidden
    layers. Autoencoders with multiple hidden layers are called **multilayer autoencoders**
    or **deep autoencoders**. What we have learned so far is just the **vanilla**
    or the **shallow autoencoder**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过限制隐藏层（`code`）中的神经元，我们可以学习输入的有用表示。自编码器也可以有任意数量的隐藏层。具有多个隐藏层的自编码器称为**多层自编码器**或**深层自编码器**。到目前为止，我们所学到的只是**普通**或**浅层自编码器**。
- en: Reconstructing the MNIST images using an autoencoder
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器重构 MNIST 图像
- en: 'Now we will learn how autoencoders reconstruct handwritten digits using the
    MNIST dataset. First, let''s import the necessary libraries:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习如何使用 MNIST 数据集重构手写数字的自编码器。首先，让我们导入必要的库：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preparing the dataset
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'Let''s load the MNIST dataset. Since we are reconstructing the given input,
    we don''t need the labels. So, we just load `x_train` for training and `x_test`
    for testing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载 MNIST 数据集。由于我们正在重建给定的输入，所以不需要标签。因此，我们只加载 `x_train` 用于训练和 `x_test` 用于测试：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Normalize the data by dividing by the max pixel value, which is `255`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过除以最大像素值 `255` 来对数据进行归一化：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Print the `shape` of our dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 打印我们数据集的 `shape`：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Reshape the images as a 2D array:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像重塑为二维数组：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, the shape of the data would become as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据的形状将变为如下所示：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining the encoder
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义编码器
- en: Now we define the encoder layer, which takes the images as an input and returns
    the encodings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义编码器层，它将图像作为输入并返回编码。
- en: 'Define the size of the encodings:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 定义编码的大小：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the placeholders for the input:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入的占位符：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the encoder which takes `input_image` and returns the encodings:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义编码器，它接受 `input_image` 并返回编码：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the decoder
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义解码器
- en: 'Let''s define the decoder which takes the encoded values from the encoder and
    returns the reconstructed image:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义解码器，它从编码器中获取编码值并返回重构的图像：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Building the model
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Now that we defined encoder and decoder, we define the model which takes images
    as input and returns the output of the decoder which is the reconstructed image:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义了编码器和解码器，我们定义一个模型，该模型接受图像作为输入，并返回解码器的输出，即重构图像：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s look at a summary of the model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型的摘要：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Compile the model with `loss` as a binary cross-entropy and minimize the loss
    using the `adadelta` optimizer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二元交叉熵作为损失编译模型，并使用 `adadelta` 优化器最小化损失：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now let's train the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练模型。
- en: 'Generally, we train the model as `model.fit(x,y)` where `x` is the input and
    `y` is the label. But since autoencoders reconstruct their inputs, the input and
    output to the model should be the same. So, here, we train the model as `model.fit(x_train,
    x_train)`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们按 `model.fit(x,y)` 训练模型，其中 `x` 是输入，`y` 是标签。但由于自编码器重构它们的输入，模型的输入和输出应该相同。因此，在这里，我们按
    `model.fit(x_train, x_train)` 训练模型：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Reconstructing images
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重构图像
- en: 'Now that we have trained the model, we see how the model is reconstructing
    the images of the test set. Feed the test images to the model and get the reconstructed
    images:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，我们看看模型如何重构测试集的图像。将测试图像输入模型并获取重构的图像：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Plotting reconstructed images
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制重构图像
- en: 'First, let us plot the actual images, that is, input images:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们绘制实际图像，即输入图像：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The plot of the actual images is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际图像的绘制如下所示：
- en: '![](img/37773424-afde-4562-bd8c-7320360ad538.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37773424-afde-4562-bd8c-7320360ad538.png)'
- en: 'Plot the reconstructed image as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制重构图像如下所示：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following shows the reconstructed images:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了重构后的图像：
- en: '![](img/1f12fa78-b120-43e2-b4a0-8fb789bf913a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f12fa78-b120-43e2-b4a0-8fb789bf913a.png)'
- en: As you can see, the autoencoder has learned better representations of the input
    images and reconstructed them.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，自编码器已经学习了输入图像的更好表示并对其进行了重构。
- en: Autoencoders with convolutions
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积的自编码器
- en: We just learned what autoencoders are in the previous section. We learned about
    a vanilla autoencoder, which is basically the feedforward shallow network with
    one hidden layer. Instead of keeping them as a feedforward network, can we use
    them as a convolutional network? Since we know that a convolutional network is
    good at classifying and recognizing images (provided that we use convolutional
    layers instead of feedforward layers in the autoencoders), it will learn to reconstruct
    the inputs better when the inputs are images.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在前一节学习了什么是自编码器。我们了解了传统自编码器，它基本上是具有一个隐藏层的前馈浅网络。我们可以不将它们保持为前馈网络，而是可以将它们作为卷积网络使用吗？由于我们知道卷积网络在分类和识别图像方面表现良好（前提是在自编码器中使用卷积层而不是前馈层），当输入是图像时，它将学习更好地重建输入。
- en: 'Thus, we introduce a new type of autoencoders called CAEs that use a convolutional
    network instead of a vanilla neural network. In the vanilla autoencoders, encoders
    and decoders are basically a feedforward network. But in CAEs, they are basically
    convolutional networks. This means the encoder consists of convolutional layers
    and the decoder consists of transposed convolutional layers, instead of a feedforward
    network. A CAE is shown in the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们介绍一种新类型的自编码器称为CAE，它使用卷积网络而不是传统的神经网络。在传统的自编码器中，编码器和解码器基本上是一个前馈网络。但在CAE中，它们基本上是卷积网络。这意味着编码器由卷积层组成，解码器由转置卷积层组成，而不是前馈网络。CAE如下图所示：
- en: '![](img/b5250f8e-2b95-4554-9fb5-7f84ac083f25.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5250f8e-2b95-4554-9fb5-7f84ac083f25.png)'
- en: As shown, we feed the input image to the encoder that consists of a convolutional
    layer, and the convolutional layer performs the convolution operation and extracts
    important features from the image. We then perform max pooling to keep only the
    important features of the image. In a similar fashion, we perform several convolutional
    and max-pooling operations and obtain a latent representation of the image, called
    a **bottleneck**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们将输入图像提供给编码器，编码器由卷积层组成，卷积层执行卷积操作并从图像中提取重要特征。然后我们执行最大池化以仅保留图像的重要特征。以类似的方式，我们执行多个卷积和最大池化操作，并获得图像的潜在表示，称为**瓶颈**。
- en: Next, we feed the bottleneck to the decoder that consists of deconvolutional
    layers, and the deconvolutional layer performs the deconvolution operation and
    tries to reconstruct the image from the bottleneck. It consists of several deconvolutional
    and upsampling operations to reconstruct the original image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将瓶颈输入解码器，解码器由反卷积层组成，反卷积层执行反卷积操作并试图从瓶颈中重建图像。它包括多个反卷积和上采样操作以重建原始图像。
- en: Thus, this is how CAE uses convolutional layers in the encoder and transpose
    convolutional layers in the decoders to reconstruct the image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是CAE如何在编码器中使用卷积层和在解码器中使用转置卷积层来重建图像的方法。
- en: Building a convolutional autoencoder
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建卷积自编码器
- en: Just as we learned how to implement an autoencoder in the previous section,
    implementing a CAE is also the same, but the only difference is here we use convolutional
    layers in the encoder and decoder instead of a feedforward network. We will use
    the same MNIST dataset to reconstruct the images using CAE.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前一节学习如何实现自编码器一样，实现CAE也是一样的，唯一的区别是这里我们在编码器和解码器中使用卷积层，而不是前馈网络。我们将使用相同的MNIST数据集来使用CAE重建图像。
- en: 'Import the libraries:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Read and reshape the dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 读取并重塑数据集：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s define the shape of our input image:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们输入图像的形状：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Defining the encoder
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义编码器
- en: Now, let's define our encoder. Unlike vanilla autoencoders, where we use feedforward
    networks, here we use a convolutional network. Hence, our encoder comprises three
    convolutional layers, followed by a max pooling layer with `relu` activations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义我们的编码器。与传统自编码器不同，在这里我们使用卷积网络而不是前馈网络。因此，我们的编码器包括三个卷积层，后跟具有`relu`激活函数的最大池化层。
- en: 'Define the first convolutional layer, followed by a max pooling operation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一个卷积层，然后进行最大池化操作：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the second convolutional and max pooling layer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二个卷积和最大池化层：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the final convolutional and max pooling layer:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定义最终的卷积和最大池化层：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Defining the decoder
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义解码器
- en: Now, we define our decoder; in the decoder, we perform the deconvolution operation
    with three layers, that is, we upsample the encodings created by the encoder and
    reconstruct the original image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义我们的解码器；在解码器中，我们执行三层反卷积操作，即对编码器创建的编码进行上采样并重建原始图像。
- en: 'Define the first convolutional layer followed by upsampling:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一个卷积层，并进行上采样：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the second convolutional layer with upsampling:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二个卷积层，并进行上采样：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the final convolutional layer with upsampling:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 定义最终的卷积层并进行上采样：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Building the model
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Define the model that takes an input image and returns the images generated
    by the decoder, which is reconstructed images:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 定义接收输入图像并返回解码器生成的图像（即重建图像）的模型：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s compile the model with loss as binary cross-entropy and we use `adadelta`
    as our optimizer:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用二进制交叉熵作为损失来编译模型，并使用`adadelta`作为优化器：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, train the model as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按以下方式训练模型：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Reconstructing the images
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建图像
- en: 'Reconstruct the images using our trained model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们训练好的模型重建图像：
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, let us plot the input images:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们绘制输入图像：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The plot of the input images is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像的绘图如下所示：
- en: '![](img/c91fae3e-e957-48a5-bbef-ed6872786eea.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c91fae3e-e957-48a5-bbef-ed6872786eea.png)'
- en: 'Now, we plot the reconstructed images:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们绘制重建的图像：
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The plot of the reconstructed images is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重建图像的绘图如下所示：
- en: '![](img/7f684ba9-6b34-4595-b0e8-fa916886d968.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f684ba9-6b34-4595-b0e8-fa916886d968.png)'
- en: Exploring denoising autoencoders
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索去噪自编码器
- en: DAE are another small variant of the autoencoder. They are mainly used to remove
    noise from the image, audio, and other inputs. So, when we feed the corrupted
    input to the DAE, it learns to reconstruct the original uncorrupted input. Now
    we inspect how DAEs remove the noise.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器（DAE）是自编码器的另一种小变体。它们主要用于去除图像、音频和其他输入中的噪音。因此，当我们将破坏的输入提供给DAE时，它学会重建原始未破坏的输入。现在我们来看看DAE如何去除噪音。
- en: With a DAE, instead of feeding the raw input to the autoencoder, we corrupt
    the input by adding some stochastic noise and feed the corrupted input. We know
    that the encoder learns the representation of the input by keeping only important
    information and maps the compressed representation to the bottleneck. When the
    corrupted input is sent to the encoder, while learning the representation of the
    input encoder will learn that noise is unwanted information and removes its representation.
    Thus, encoders learn the compact representation of the input without noise by
    keeping only necessary information, and map the learned representation to the
    bottleneck.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DAE时，我们不是直接将原始输入馈送给自编码器，而是通过添加一些随机噪音来破坏输入，然后再馈送破坏的输入。我们知道编码器通过仅保留重要信息来学习输入的表示，并将压缩表示映射到瓶颈。当破坏的输入被送到编码器时，编码器将学习到噪音是不需要的信息，并且移除其表示。因此，编码器通过仅保留必要信息来学习无噪音的输入的紧凑表示，并将学习到的表示映射到瓶颈。
- en: Now the decoder tries to reconstruct the image using the representation learned
    by the encoder, that is, the bottleneck. Since the representation, does not contain
    any noise, decoders reconstruct the input without noise. This is how a DAE removes
    noise from the input.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解码器尝试使用由编码器学习到的表示重建图像，也就是瓶颈。由于该表示不包含任何噪音，解码器在没有噪音的情况下重建输入。这就是去噪自编码器从输入中去除噪音的方式。
- en: 'A typical DAE is shown in the following diagram. First, we corrupt the input
    by adding some noise and feed the corrupted input to the encoder, which learns
    the representation of the input without the noise, while the decoder reconstructs
    the uncorrupted input using the representation learned by the encoder:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的DAE如下图所示。首先，我们通过添加一些噪音来破坏输入，然后将破坏的输入提供给编码器，编码器学习到去除噪音的输入的表示，而解码器使用编码器学习到的表示重建未破坏的输入：
- en: '![](img/1d4a068f-ddc8-4eb9-9611-0c7777d485fd.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d4a068f-ddc8-4eb9-9611-0c7777d485fd.png)'
- en: Mathematically, this can be expressed as follows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，这可以表示如下。
- en: 'Say we have an image, ![](img/ee167338-3666-4b9c-b1ad-53533b983029.png), and
    we add noise to the image and get ![](img/b8bb320e-3dc9-4bdf-8661-0143a805cfad.png)
    which is the corrupted image:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一张图片，![](img/ee167338-3666-4b9c-b1ad-53533b983029.png)，我们向图片添加噪声后得到![](img/b8bb320e-3dc9-4bdf-8661-0143a805cfad.png)，这是被破坏的图片：
- en: '![](img/3b85c1b7-2605-410e-a823-69da021bace2.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b85c1b7-2605-410e-a823-69da021bace2.png)'
- en: 'Now feed this corrupted image to the encoder:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将此破坏的图像馈送给编码器：
- en: '![](img/5845ebff-5585-46e3-baf0-9408d64afc78.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5845ebff-5585-46e3-baf0-9408d64afc78.png)'
- en: '![](img/3387b54b-01f1-446a-bac0-e700b2bb7eba.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3387b54b-01f1-446a-bac0-e700b2bb7eba.png)'
- en: 'The decoder tries to reconstruct the actual image:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器尝试重建实际图像：
- en: '![](img/872eb111-1108-4874-a983-4b2b9f925f69.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/872eb111-1108-4874-a983-4b2b9f925f69.png)'
- en: '![](img/bcee23a0-4d4e-4313-96e0-969c734eedf3.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcee23a0-4d4e-4313-96e0-969c734eedf3.png)'
- en: Denoising images using DAE
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DAE 进行图像去噪
- en: In this section, we will learn how to denoise the images using DAE. We use CAE
    for denoising the images. The code for DAE is just the same as CAE, except that
    here, we use noisy images in the input. Instead of looking at the whole code,
    we will see only the respective changes. The complete code is available on GitHub
    at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 DAE 对图像进行去噪。我们使用 CAE 来对图像进行去噪。DAE 的代码与 CAE 完全相同，只是这里我们在输入中使用了嘈杂的图像。而不是查看整个代码，我们只会看到相应的更改。完整的代码可以在
    GitHub 上查看：[https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python)。
- en: 'Set the noise factor:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 设置噪声因子：
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add noise to the train and test images:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 向训练和测试图像添加噪声：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Clip the train and test set by 0 and 1:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练集和测试集裁剪为 0 和 1：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s train the model. Since, we want the model to learn to remove noise in
    the image, input to the model is the noisy images, that is, `x_train_noisy` and
    output is the denoised images, that is, `x_train`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型。由于我们希望模型学会去除图像中的噪声，因此模型的输入是嘈杂的图像，即 `x_train_noisy`，输出是去噪后的图像，即 `x_train`：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Reconstruct images using our trained model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们训练好的模型重建图像：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'First, let us plot the input image which is the corrupted image:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们绘制输入图像，即损坏的图像：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The plot of the input noisy image is shown as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了输入嘈杂图像的绘图：
- en: '![](img/7086c1f1-cf80-4f56-a3c6-a3dec5336f7f.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7086c1f1-cf80-4f56-a3c6-a3dec5336f7f.png)'
- en: 'Now, let us plot the reconstructed images by the model:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制模型重建的图像：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you can see, our model has learned to remove noise from the image:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的模型已经学会从图像中去除噪声：
- en: '![](img/214f7f30-9858-445e-9a5e-875142681057.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/214f7f30-9858-445e-9a5e-875142681057.png)'
- en: Understanding sparse autoencoders
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解稀疏自编码器
- en: We know that autoencoders learn to reconstruct the input. But when we set the
    number of nodes in the hidden layer greater than the number of nodes in the input
    layer, then it will learn an identity function which is not favorable, because
    it just completely copies the input.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道自编码器学习重建输入。但是当我们设置隐藏层中的节点数大于输入层中的节点数时，它将学习一个恒等函数，这是不利的，因为它只是完全复制输入。
- en: 'Having more nodes in the hidden layer helps us to learn robust latent representation.
    But when there are more nodes in the hidden layer, the autoencoder tries to completely
    mimic the input and thus it overfits the training data. To resolve the problem
    of overfitting, we introduce a new constraint to our loss function called the
    **sparsity constraint** or **sparsity penalty**. The loss function with sparsity
    penalty can be represented as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐藏层中增加更多节点有助于学习稳健的潜在表示。但是当隐藏层中的节点更多时，自编码器会试图完全模仿输入，从而过度拟合训练数据。为了解决过拟合问题，我们在损失函数中引入了一个称为**稀疏约束**或**稀疏惩罚**的新约束。带有稀疏惩罚的损失函数可以表示如下：
- en: '![](img/bbda4978-3f00-4edc-a739-8dbd1de211ba.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbda4978-3f00-4edc-a739-8dbd1de211ba.png)'
- en: The first term ![](img/d270d179-f57f-45e5-906f-28816e7158ad.png) represents
    the reconstruction error between the original input ![](img/8535044c-1614-4749-980f-9e7cd0cd3632.png)
    and reconstructed input, ![](img/5d8aa7a5-a8ce-483d-94f9-824d79491ba8.png). The
    second term implies the sparsity constraint. Now we will explore how this sparse
    constraint mitigates the problem of overfitting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项 ![](img/d270d179-f57f-45e5-906f-28816e7158ad.png) 表示原始输入 ![](img/8535044c-1614-4749-980f-9e7cd0cd3632.png)
    与重建输入 ![](img/5d8aa7a5-a8ce-483d-94f9-824d79491ba8.png) 之间的重构误差。第二项表示稀疏约束。现在我们将探讨这种稀疏约束如何缓解过拟合问题。
- en: Using the sparsity constraint, we activate only specific neurons on the hidden
    layer instead of activating all the neurons. Based on the input, we activate and
    deactivate specific neurons so the neurons, when they are activated, will learn
    to extract important features from the input. By having the sparse penalty, autoencoders
    will not copy the input exactly to the output and it can also learn the robust
    latent representation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过稀疏约束，我们仅激活隐藏层中特定的神经元，而不是激活所有神经元。根据输入，我们激活和取消激活特定的神经元，因此当这些神经元被激活时，它们将学会从输入中提取重要特征。通过施加稀疏惩罚，自编码器不会精确复制输入到输出，并且它还可以学习到稳健的潜在表示。
- en: 'As shown in the following diagram, sparse autoencoders have more units in the
    hidden layer than the input layer; however, only a few neurons in the hidden layer
    are activated. The unshaded neurons represent the neurons that are currently activated:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，稀疏自编码器的隐藏层单元数比输入层多；然而，只有少数隐藏层中的神经元被激活。未阴影的神经元表示当前激活的神经元：
- en: '![](img/ae4a2689-b6dc-4aae-8167-e662895f00bd.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae4a2689-b6dc-4aae-8167-e662895f00bd.png)'
- en: A neuron returns 1 if it's active and 0 if it is inactive. In sparse autoencoders,
    we set most of the neurons in the hidden layer to inactive. We know that the sigmoid
    activation function squashes the value to between 0 and 1\. So, when we use the
    sigmoid activation function, we try to keep the values of neurons close to 0.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经元活跃则返回1，非活跃则返回0。在稀疏自编码器中，我们将大多数隐藏层中的神经元设置为非活跃状态。我们知道sigmoid激活函数将值压缩到0到1之间。因此，当我们使用sigmoid激活函数时，我们尝试将神经元的值保持接近于0。
- en: We typically try to keep the average activation value of each neuron in the
    hidden layer close to zero, say 0.05, but not equal to zero, and this value is
    called ![](img/060f2b67-744d-48dd-b158-29257cebee90.png), which is our sparsity
    parameter. We typically set the value of ![](img/060f2b67-744d-48dd-b158-29257cebee90.png)
    to 0.05.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常试图保持隐藏层中每个神经元的平均激活值接近于零，比如0.05，但不等于零，这个值被称为 ![](img/060f2b67-744d-48dd-b158-29257cebee90.png)，即我们的稀疏参数。我们通常将
    ![](img/060f2b67-744d-48dd-b158-29257cebee90.png) 的值设为0.05。
- en: First, we calculate the average activation of a neuron.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算神经元的平均激活值。
- en: 'The average activation of the ![](img/27815132-2e2f-4c4f-b541-72464797a683.png)
    neuron in the hidden layer ![](img/3728baba-3198-4a8c-9617-946925898ac3.png) over
    the whole training set can be calculated as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个训练集上，隐藏层中 ![](img/27815132-2e2f-4c4f-b541-72464797a683.png) 神经元的平均激活可以计算如下：
- en: '![](img/ccb87428-1fc2-40b3-9251-fec0b5d2027f.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb87428-1fc2-40b3-9251-fec0b5d2027f.png)'
- en: 'Here, the following holds true:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下内容成立：
- en: '![](img/96632d24-2330-47b2-b2b5-39bb98b48348.png)denotes the average activation
    of the ![](img/bc7e31cd-af8d-4470-a98d-473a048fd780.png)neuron in the hidden layer
    ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/96632d24-2330-47b2-b2b5-39bb98b48348.png)表示隐藏层中 ![](img/bc7e31cd-af8d-4470-a98d-473a048fd780.png)
    神经元的平均激活'
- en: '![](img/1b6dd202-1bed-4a64-9785-18c50e18d543.png)is the number of the training
    sample'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/1b6dd202-1bed-4a64-9785-18c50e18d543.png)是训练样本的编号'
- en: '![](img/39738fc2-1272-4ce6-b312-2812dfa82d5f.png)is the activation of ![](img/c3aaa92e-1573-4bd5-b679-e6a435ae45e2.png)neuron
    in the hidden layer ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/39738fc2-1272-4ce6-b312-2812dfa82d5f.png)是隐藏层中 ![](img/c3aaa92e-1573-4bd5-b679-e6a435ae45e2.png)
    神经元的激活'
- en: '![](img/a940740f-e484-41e7-8a9c-31b9340ade49.png)is the ![](img/9c486b3d-9c82-4b8a-8449-70628da1cf67.png)training
    sample'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/a940740f-e484-41e7-8a9c-31b9340ade49.png)是**稀疏自编码器**的训练样本'
- en: '![](img/ea1e30a9-b545-4416-a16f-5f11103b55ba.png)implies the activation of
    the [![](img/912fa4b2-4be0-49ef-9f00-82abba816b56.png)] neuron in the hidden layer
    ![](img/3728baba-3198-4a8c-9617-946925898ac3.png)for the [![](img/b6259e8c-84dd-44e7-917c-dcd8fd58809c.png)]^(th)
    training sample'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/ea1e30a9-b545-4416-a16f-5f11103b55ba.png)表示隐藏层中 [![](img/912fa4b2-4be0-49ef-9f00-82abba816b56.png)]
    神经元对于第 [![](img/b6259e8c-84dd-44e7-917c-dcd8fd58809c.png)] 个训练样本的激活'
- en: 'We try to keep the average activation value, ![](img/4d1aed0a-39d6-449f-8c66-627df7c0e9bb.png),
    of the neurons close to ![](img/060f2b67-744d-48dd-b158-29257cebee90.png). That
    is, we try to keep the average activation values of the neurons to 0.05:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力使神经元的平均激活值 ![](img/4d1aed0a-39d6-449f-8c66-627df7c0e9bb.png) 接近于 ![](img/060f2b67-744d-48dd-b158-29257cebee90.png)。也就是说，我们尝试保持神经元的平均激活值接近于
    0.05：
- en: '![](img/c875dca7-014c-4e2a-b19c-e71cf3b7097d.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c875dca7-014c-4e2a-b19c-e71cf3b7097d.png)'
- en: 'So, we penalize the value of ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png),
    which is varying from ![](img/060f2b67-744d-48dd-b158-29257cebee90.png). We know
    that the **Kullback–Leibler** (**KL**) divergence is widely used for measuring
    the difference between the two probability distributions. So, here, we use the
    KL divergence to measure the difference between two **Bernoulli distributions**,
    that is, mean ![](img/060f2b67-744d-48dd-b158-29257cebee90.png) and mean ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png)
    and it can be given as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对值 ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png) 进行惩罚，其变化范围为 ![](img/060f2b67-744d-48dd-b158-29257cebee90.png)。我们知道**Kullback-Leibler**（**KL**）散度广泛用于衡量两个概率分布之间的差异。因此，在这里，我们使用KL散度来衡量两个**伯努利分布**，即平均
    ![](img/060f2b67-744d-48dd-b158-29257cebee90.png) 和平均 ![](img/adb924e9-98c0-4ec0-a7ba-ec078ccbda66.png)，可以表示如下：
- en: '![](img/1d68f8f6-497d-4141-bef3-39ffca40111d.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d68f8f6-497d-4141-bef3-39ffca40111d.png)'
- en: In the earlier equation, ![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png) denotes
    the hidden layer ![](img/3f117009-6b17-42df-9aae-35da95019646.png), and ![](img/702ac143-726d-44a4-8b2e-83cbfdb643a8.png)
    denotes the ![](img/02d427ef-0ba3-48a1-9c60-eeeb796a9216.png)neurons in the hidden
    layer ![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png). The earlier equation
    is basically the sparse penalty or sparsity constraint. Thus, with the sparsity
    constraint, all the neurons will never be active at the same time, and on average,
    they are set to 0.05.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方程中，![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png)表示隐藏层 ![](img/3f117009-6b17-42df-9aae-35da95019646.png)，![](img/702ac143-726d-44a4-8b2e-83cbfdb643a8.png)表示隐藏层
    ![](img/e9acda55-c52f-4a33-80e8-fd74159dd204.png)中的神经元。前述方程基本上是稀疏惩罚或稀疏性约束。因此，通过稀疏约束，所有神经元永远不会同时活动，并且平均而言，它们被设置为0.05。
- en: 'Now we can rewrite the loss function with a sparse penalty as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以根据稀疏惩罚重新编写损失函数，如下所示：
- en: '![](img/acc8da4c-6096-4595-85e8-996472f633b0.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acc8da4c-6096-4595-85e8-996472f633b0.png)'
- en: Thus, sparse autoencoders allow us to have a greater number of nodes in the
    hidden layer than the input layer, yet reduce the problem of overfitting with
    the help of the sparsity constraint in the loss function.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，稀疏自编码器允许我们在隐藏层中拥有比输入层更多的节点，然而通过损失函数中的稀疏约束来减少过拟合问题。
- en: Building the sparse autoencoder
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建稀疏自编码器
- en: Building a sparse autoencoder is just as same as building a regular autoencoder,
    except that we use a sparse regularizer in the encoder and decoder, so instead
    of looking at the whole code in the following sections, we will only look at the
    parts related to implementing the sparse regularizer; the complete code with an
    explanation is available on GitHub.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 构建稀疏自编码器与构建常规自编码器相同，只是在编码器和解码器中使用稀疏正则化器，因此在下面的部分中我们只会看到与实现稀疏正则化器相关的部分；完整的代码及解释可以在GitHub上找到。
- en: Defining the sparse regularizer
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义稀疏正则化器
- en: 'The following is the code to define the sparse regularizer:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是定义稀疏正则化器的代码：
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Set our ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png) value to `0.05`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的 ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png) 值设为 `0.05`：
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Calculate the ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png), which is the
    mean activation value:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png)，即平均激活值：
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Compute the KL divergence between the mean ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png)
    and the mean ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png) according to equation
    *(1)*:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程*(1)*计算平均 ![](img/beb50c08-7bf9-487b-8165-fa933f5e34c2.png) 和平均 ![](img/8f0b0be9-23e1-4923-8f93-931292555dc5.png)
    之间的KL散度：
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Sum the KL divergence values:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 求和KL散度值：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Multiply the `sum` by `beta` and return the results:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sum`乘以`beta`并返回结果：
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The whole function for the sparse regularizer is given as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏正则化器的整个函数定义如下：
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Learning to use contractive autoencoders
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习使用收缩自编码器
- en: Like sparse autoencoders, **contractive autoencoders** add a new regularization
    term to the loss function of the autoencoders. They try to make our encodings
    less sensitive to the small variations in the training data. So, with contractive
    autoencoders, our encodings become more robust to small perturbations such as
    noise present in our training dataset. We now introduce a new term called the
    **regularizer** or **penalty term** to our loss function. It helps to penalize
    the representations that are too sensitive to the input.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于稀疏自编码器，**收缩自编码器**在自编码器的损失函数中添加了新的正则化项。它们试图使我们的编码对训练数据中的小变化不那么敏感。因此，使用收缩自编码器，我们的编码变得更加稳健，对于训练数据中存在的噪声等小扰动更加鲁棒。我们现在引入一个称为**正则化器**或**惩罚项**的新术语到我们的损失函数中。它有助于惩罚对输入过于敏感的表示。
- en: 'Our loss function can be mathematically represented as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数可以用数学方式表示如下：
- en: '![](img/555d6657-2908-4e34-afc0-ef9483f96144.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/555d6657-2908-4e34-afc0-ef9483f96144.png)'
- en: The first term represents the reconstruction error and the second term represents
    the penalty term or the regularizer and it is basically the **Frobenius** **norm**
    of the **Jacobian matrix**. Wait! What does that mean?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项表示重构误差，第二项表示惩罚项或正则化器，基本上是**雅可比矩阵**的**Frobenius** **范数**。等等！这是什么意思？
- en: The Frobenius norm, also called the **Hilbert-Schmidt norm**, of a matrix is
    defined as the square root of the sum of the absolute square of its elements.
    A matrix comprising a partial derivative of the vector-valued function is called
    the **Jacobian matrix**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的Frobenius范数，也称为**Hilbert-Schmidt范数**，定义为其元素的绝对值平方和的平方根。由向量值函数的偏导数组成的矩阵称为**雅可比矩阵**。
- en: 'Thus, calculating the Frobenius norm of the Jacobian matrix implies our penalty
    term is the sum of squares of all partial derivatives of the hidden layer with
    respect to the input. It is given as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算雅可比矩阵的Frobenius范数意味着我们的惩罚项是隐藏层对输入的所有偏导数的平方和。其表示如下：
- en: '![](img/89f50a8e-25a1-4d00-8bae-3ed4c134e2ee.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89f50a8e-25a1-4d00-8bae-3ed4c134e2ee.png)'
- en: 'Calculating the partial derivative of the hidden layer with respect to the
    input is similar to calculating gradients of loss. Assuming we are using the sigmoid
    activation function, then the partial derivative of the hidden layer with respect
    to the input is given as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 计算隐藏层对输入的偏导数类似于计算损失的梯度。假设我们使用sigmoid激活函数，则隐藏层对输入的偏导数表示如下：
- en: '![](img/32880779-7c18-4c95-b720-3753579a110a.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32880779-7c18-4c95-b720-3753579a110a.png)'
- en: Adding the penalty term to our loss function helps in reducing the sensitivity
    of the model to the variations in the input and makes our model more robust to
    the outliers. Thus, contractive autoencoders reduce the sensitivity of the model
    to the small variations in the training data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将惩罚项添加到我们的损失函数中有助于减少模型对输入变化的敏感性，并使我们的模型更加鲁棒，能够抵抗异常值。因此，收缩自编码器减少了模型对训练数据中小变化的敏感性。
- en: Implementing the contractive autoencoder
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现收缩自编码器
- en: Building the contractive autoencoder is just as same as building the autoencoder,
    except that we use the contractive loss regularizer in the model, so instead of
    looking at the whole code, we will only look at the parts related to implementing
    the contractive loss.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 建立收缩自编码器与建立普通自编码器几乎相同，只是在模型中使用了收缩损失正则化器，因此我们将只查看与实现收缩损失相关的部分，而不是整个代码。
- en: Defining the contractive loss
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义收缩损失
- en: Now let's see how to define the loss function in Python.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在Python中定义损失函数。
- en: 'Define the mean squared loss as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 定义均方损失如下：
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Obtain the weights from our encoder layer and transpose the weights:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的编码器层获取权重并转置权重：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Get the output of our encoder layer:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 获取我们的编码器层的输出：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the penalty term:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 定义惩罚项：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The final loss is the sum of mean squared error and the penalty term multiplied
    by `lambda`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失是均方误差和乘以`lambda`的惩罚项的总和：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The complete code for contractive loss is given as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩损失的完整代码如下所示：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Dissecting variational autoencoders
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解剖变分自编码器
- en: Now we will see another very interesting type of autoencoders called **variational
    autoencoders** (**VAE**). Unlike other autoencoders, VAEs are generative models
    that imply they learn to generate new data just like GANs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到另一种非常有趣的自编码器类型，称为**变分自编码器**（**VAE**）。与其他自编码器不同，VAE是生成模型，意味着它们学习生成新数据，就像GANs一样。
- en: Let's say we have a dataset containing facial images of many individuals. When
    we train our variational autoencoder with this dataset, it learns to generate
    new realistic faces that are not seen in the dataset. VAEs have various applications
    because of their generative nature and some of them include generating images,
    songs, and so on. But what makes VAE generative and how is it different than other
    autoencoders? Let's learn that in the coming section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含许多个体面部图像的数据集。当我们用这个数据集训练我们的变分自编码器时，它学会了生成新的逼真面部图像，这些图像在数据集中没有见过。由于其生成性质，变分自编码器有各种应用，包括生成图像、歌曲等。但是，什么使变分自编码器具有生成性质，它与其他自编码器有何不同？让我们在接下来的部分中学习。
- en: Just as we learned when discussing GANs, for a model to be generative, it has
    to learn the distribution of the inputs. For instance, let's say we have a dataset
    that consists of handwritten digits, such as the MNIST dataset. Now, in order
    to generate new handwritten digits, our model has to learn the distribution of
    the digits in the given dataset. Learning the distribution of the digits present
    in the dataset helps VAE to learn useful properties such as digit width, stroke,
    height, and so on. Once the model encodes this property in its distribution, then
    it can generate new handwritten digits by sampling from the learned distribution.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在讨论GAN时学到的那样，要使模型具有生成性，它必须学习输入的分布。例如，假设我们有一个包含手写数字的数据集，如MNIST数据集。现在，为了生成新的手写数字，我们的模型必须学习数据集中数字的分布。学习数据集中数字的分布有助于VAE学习有用的属性，如数字的宽度、笔画、高度等。一旦模型在其分布中编码了这些属性，那么它就可以通过从学习到的分布中抽样来生成新的手写数字。
- en: Say we have a dataset of human faces, then learning the distribution of the
    faces in the dataset helps us to learn various properties such as gender, facial
    expression, hair color, and so on. Once the model learns and encode these properties
    in its distribution, then it can generate a new face just by sampling from the
    learned distribution.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含人脸数据的数据集，那么学习数据集中人脸的分布有助于我们学习各种属性，如性别、面部表情、发色等。一旦模型学习并在其分布中编码了这些属性，那么它就可以通过从学习到的分布中抽样来生成新的人脸。
- en: 'Thus, in VAE, instead of mapping the encoder''s encodings directly to the latent
    vector (bottleneck), we map the encodings to a distribution; usually, it is a
    Gaussian distribution. We sample a latent vector from this distribution and feed
    it to a decoder then the decoder learns to reconstruct the image. As shown in
    the following diagram, an encoder maps its encodings to a distribution and we
    sample a latent vector from this distribution and feed it to a decoder to reconstruct
    an image:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在变分自编码器中，我们不直接将编码器的编码映射到潜在向量（瓶颈），而是将编码映射到一个分布中；通常是高斯分布。我们从这个分布中抽样一个潜在向量，然后将其馈送给解码器来重构图像。如下图所示，编码器将其编码映射到一个分布中，我们从该分布中抽样一个潜在向量，并将其馈送给解码器来重构图像：
- en: '![](img/9861a0f1-589e-4d93-a158-46310c4d5b77.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9861a0f1-589e-4d93-a158-46310c4d5b77.png)'
- en: 'A **gaussian distribution** can be parameterized by its mean and covariance
    matrix. Thus, we can make our encoder generate its encoding and maps it to a mean
    vector and standard deviation vector that approximately follows the Gaussian distribution.
    Now, from this distribution, we sample a latent vector and feed it to our decoder
    which then reconstructs an image:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯分布**可以通过其均值和协方差矩阵来参数化。因此，我们可以让我们的编码器生成其编码，并将其映射到一个接近高斯分布的均值向量和标准差向量。现在，从这个分布中，我们抽样一个潜在向量并将其馈送给我们的解码器，解码器然后重构图像：'
- en: '![](img/60c96b04-e98b-449e-854f-db846d3c2577.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60c96b04-e98b-449e-854f-db846d3c2577.png)'
- en: In a nutshell, the encoder learns the desirable properties of the given input
    and encodes them into distribution. We sample a latent vector from this distribution
    and feed the latent vector as input to the decoder which then generates images
    learned from the encoder's distribution.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，编码器学习给定输入的理想属性，并将其编码成分布。我们从该分布中抽样一个潜在向量，并将潜在向量作为输入馈送给解码器，解码器然后生成从编码器分布中学习的图像。
- en: In VAE, the encoder is also called as **recognition** **model** and the decoder
    is also called as **g****enerative** **model**. Now that we have an intuitive
    understanding of VAE, in the next section, we will go into detail and learn how
    VAE works.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分自编码器中，编码器也称为**识别模型**，解码器也称为**生成模型**。现在我们对变分自编码器有了直观的理解，接下来的部分中，我们将详细了解变分自编码器的工作原理。
- en: Variational inference
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分推断
- en: 'Before going ahead, let''s get familiar with the notations:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们熟悉一下符号：
- en: Let's represent the distribution of the input dataset by ![](img/7cb7aa48-8ffc-405f-b340-25d5b2790817.png),
    where ![](img/a613c746-aa3a-42f2-ac6d-5bcd8fd5bc24.png)represents the parameter
    of the network that will be learned during training
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们用 ![](img/7cb7aa48-8ffc-405f-b340-25d5b2790817.png) 来表示输入数据集的分布，其中 ![](img/a613c746-aa3a-42f2-ac6d-5bcd8fd5bc24.png)
    表示在训练过程中将学习的网络参数。
- en: We represent the latent variable by ![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png),
    which encodes all the properties of the input by sampling from the distribution
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用 ![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png) 表示潜在变量，通过从分布中采样来编码输入的所有属性。
- en: '![](img/91dc9cc1-f422-456f-8d57-25ca9b52708a.png)denotes the joint distribution
    of the input ![](img/e05c5e69-f4c0-443f-b911-07041c10049c.png)with their properties,
    ![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/91dc9cc1-f422-456f-8d57-25ca9b52708a.png)表示输入 ![](img/e05c5e69-f4c0-443f-b911-07041c10049c.png)及其属性的联合分布，![](img/c3aa8cce-dabb-403f-9fc1-d3feb2485ce6.png)。'
- en: '![](img/bec5dd91-4523-4f5a-8567-6bbb50b974ad.png)represents the distribution
    of the latent variable'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/bec5dd91-4523-4f5a-8567-6bbb50b974ad.png) 表示潜在变量的分布。'
- en: 'Using the Bayesian theorem, we can write the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们可以写出以下内容：
- en: '![](img/6d7bcfc0-e7c4-472c-95be-a0ae6b641a77.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d7bcfc0-e7c4-472c-95be-a0ae6b641a77.png)'
- en: The preceding equation helps us to compute the probability distribution of the
    input dataset. But the problem lies in computing ![](img/275d0bd5-e949-4dff-84a3-5e478ad671b5.png),
    because computing it is intractable. Thus, we need to find a tractable way to
    estimate the ![](img/4b09580c-ca0a-4926-bd62-e25d1e3f1528.png). Here, we introduce
    a concept called **variational inference**.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程帮助我们计算输入数据集的概率分布。但问题在于计算 ![](img/275d0bd5-e949-4dff-84a3-5e478ad671b5.png)，因为其计算是不可解的。因此，我们需要找到一种可行的方法来估计
    ![](img/4b09580c-ca0a-4926-bd62-e25d1e3f1528.png)。在这里，我们介绍一种称为**变分推断**的概念。
- en: 'Instead of inferring the distribution of ![](img/eef7c7d5-7470-4ea3-8696-06ea4042bc47.png)
    directly, we approximate them using another distribution, say a Gaussian distribution
    ![](img/65d7617b-e92b-45a5-8e95-55e84dd2ff70.png). That is, we use ![](img/7d233498-9a18-43cb-be9e-30f5fb70e7a0.png)
    which is basically a neural network parameterized by ![](img/61a4f823-3ae8-46c4-ae6e-665e03617e23.png)
    parameter to estimate the value of ![](img/3d86662e-e642-453a-a9d6-1badf3fbb723.png):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 不直接推断 ![](img/eef7c7d5-7470-4ea3-8696-06ea4042bc47.png) 的分布，我们用另一个分布（例如高斯分布
    ![](img/65d7617b-e92b-45a5-8e95-55e84dd2ff70.png)）来近似它们。也就是说，我们使用 ![](img/7d233498-9a18-43cb-be9e-30f5fb70e7a0.png)，这基本上是由
    ![](img/61a4f823-3ae8-46c4-ae6e-665e03617e23.png) 参数化的神经网络，来估计 ![](img/3d86662e-e642-453a-a9d6-1badf3fbb723.png)
    的值：
- en: '![](img/ab54b77c-6849-40bc-bc5d-def8243a2b8a.png)is basically our probabilistic
    encoder; that is, they to create a latent vector *z* given ![](img/3566f116-c99f-4b80-9ea3-74fc64ae306d.png)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/ab54b77c-6849-40bc-bc5d-def8243a2b8a.png)基本上是我们的概率编码器；即，它们用来创建给定 ![](img/3566f116-c99f-4b80-9ea3-74fc64ae306d.png)
    的潜在向量 *z*。'
- en: '![](img/b44be5c0-1e45-4c12-8449-91435f295a2e.png)is the probabilistic decoder;
    that is, it tries to construct the input ![](img/ccacf299-a48d-48e5-92cb-6c8ebb552af7.png)given
    the latent vector ![](img/8e8f5f3a-17ab-454a-8de9-661720e62c10.png)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/b44be5c0-1e45-4c12-8449-91435f295a2e.png) 是概率解码器；也就是说，它试图构建给定潜在向量 ![](img/8e8f5f3a-17ab-454a-8de9-661720e62c10.png)
    的输入 ![](img/ccacf299-a48d-48e5-92cb-6c8ebb552af7.png)。'
- en: 'The following diagram helps you attain good clarity on the notations and what
    we have seen so far:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下图帮助你更好地理解符号及我们迄今为止看到的内容：
- en: '![](img/1d58c046-7c2f-43e3-b1ec-7eb361ed0ccd.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d58c046-7c2f-43e3-b1ec-7eb361ed0ccd.png)'
- en: The loss function
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: We just learned that we use ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)
    to approximate ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png). Thus, the estimated
    value of ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) should be close to
    ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png). Since these both are distributions,
    we use KL divergence to measure how ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)
    diverges from ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png) and we need to
    minimize the divergence.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学到，我们使用 ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) 来近似 ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png)。因此，![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)
    的估计值应接近 ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png)。由于这两者都是分布，我们使用KL散度来衡量
    ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) 与 ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png)
    的差异，并且我们需要将这种差异最小化。
- en: 'A KL divergence between ![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png) and
    ![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png) is given as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/f1438493-9995-40ff-90e1-29490b93ccc9.png)和![](img/79338d90-75e3-4362-87be-cb9b0b9e5528.png)之间的KL散度如下所示：'
- en: '![](img/373898a4-30a7-43b9-8ede-c0374190d5c9.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/373898a4-30a7-43b9-8ede-c0374190d5c9.png)'
- en: 'Since we know ![](img/d96ce8de-ee39-40be-8f5c-34bc988755c9.png), substituting
    this in the preceding equation, we can write the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道![](img/d96ce8de-ee39-40be-8f5c-34bc988755c9.png)，将其代入前述方程中，我们可以写出以下内容：
- en: '![](img/f04d2335-6171-43bd-8e54-934550d990f6.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f04d2335-6171-43bd-8e54-934550d990f6.png)'
- en: 'Since we know *log (a/b) = log(a) - log(b)*, we can rewrite the preceding equation
    as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道*log (a/b) = log(a) - log(b)*，我们可以将前述方程重写为：
- en: '![](img/13b728b6-3bce-4261-ab1c-3e89e509324d.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13b728b6-3bce-4261-ab1c-3e89e509324d.png)'
- en: 'We can take the ![](img/01774ce0-9e3e-4b97-9c58-74b450efc2af.png) outside the
    expectations since it has no dependency on ![](img/60780d92-f46d-413e-9da5-038b51feefe8.png):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将![](img/01774ce0-9e3e-4b97-9c58-74b450efc2af.png)从期望值中取出，因为它不依赖于![](img/60780d92-f46d-413e-9da5-038b51feefe8.png)：
- en: '![](img/174ab8f7-b314-4740-9eac-013a1d1f5a5e.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/174ab8f7-b314-4740-9eac-013a1d1f5a5e.png)'
- en: 'Since we know *log(ab) = log (a) + log(b)*, we can rewrite the preceding equation
    as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道*log(ab) = log (a) + log(b)*，我们可以将前述方程重写为：
- en: '![](img/65e3f3ee-ab4a-46c7-bb7f-453f73f467ce.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65e3f3ee-ab4a-46c7-bb7f-453f73f467ce.png)'
- en: '![](img/71755a3b-640a-466d-8dbf-2b70dc4f95de.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71755a3b-640a-466d-8dbf-2b70dc4f95de.png)'
- en: 'We know that KL divergence between ![](img/3c1b3c68-c1dd-4e27-bee9-393c60797ebb.png)
    and ![](img/24a816e7-23a9-43b2-8dff-fcd64ef66295.png) can be given as:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道![](img/3c1b3c68-c1dd-4e27-bee9-393c60797ebb.png)和![](img/24a816e7-23a9-43b2-8dff-fcd64ef66295.png)之间的KL散度可以表示为：
- en: '![](img/946b6261-6e49-4f14-8bef-99671173b5f2.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/946b6261-6e49-4f14-8bef-99671173b5f2.png)'
- en: 'Substituting equation *(2)* in equation *(1)* we can write:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程*(2)*代入方程*(1)*我们可以写出：
- en: '![](img/588f307e-c39f-4807-94c5-c4110aa99bdf.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/588f307e-c39f-4807-94c5-c4110aa99bdf.png)'
- en: 'Rearranging the left and right-hand sides of the equation, we can write the
    following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列方程的左侧和右侧，我们可以写成以下内容：
- en: '![](img/bfda9f6f-60f9-45fe-ba31-cc1aa663e194.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfda9f6f-60f9-45fe-ba31-cc1aa663e194.png)'
- en: 'Rearranging the terms, our final equation can be given as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列项，我们的最终方程可以表示为：
- en: '![](img/def29c99-e58f-465e-88d3-89540f1dbd80.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/def29c99-e58f-465e-88d3-89540f1dbd80.png)'
- en: What does the above equation imply?
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程意味着什么？
- en: The left-hand side of the equation is also known as the **variational lower
    bound** or the **evidence lower bound** (**ELBO**). The first term in the left-hand
    side ![](img/c3658f8d-cd12-47e9-88cc-f29bcebdc976.png) implies the distribution
    of the input *x*, which we want to maximize and ![](img/8c095851-14f2-45ba-a43f-f3d53ad08caf.png)
    implies the KL divergence between the estimated and the real distribution.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 方程左侧也被称为**变分下界**或**证据下界**（**ELBO**）。左侧第一项![](img/c3658f8d-cd12-47e9-88cc-f29bcebdc976.png)表示我们希望最大化的输入*x*的分布，![](img/8c095851-14f2-45ba-a43f-f3d53ad08caf.png)表示估计和真实分布之间的KL散度。
- en: 'The loss function can be written as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以写成以下内容：
- en: '![](img/7b36c24f-e352-4352-a777-13ddb87c51ba.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b36c24f-e352-4352-a777-13ddb87c51ba.png)'
- en: 'In this equation, you will notice the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，您会注意到以下内容：
- en: '![](img/3147eae6-43f5-4dd8-bbc6-83242ab53e3c.png)implies we are maximizing
    the distribution of the input; we can convert the maximization problem into minimization
    by simply adding a negative sign; thus, we can write ![](img/352d5d33-1842-4d4a-8aad-1ac9f7a45865.png)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/3147eae6-43f5-4dd8-bbc6-83242ab53e3c.png)意味着我们在最大化输入的分布；通过简单地添加一个负号，我们可以将最大化问题转化为最小化问题，因此我们可以写成![](img/352d5d33-1842-4d4a-8aad-1ac9f7a45865.png)'
- en: '![](img/b477c3d6-8c06-4433-9a64-a63a0ba04d3f.png)implies we are maximizing
    the KL divergence between the estimated and real distribution, but we want to
    minimize them, so we can write ![](img/673c19ad-c94e-4e0b-b2a0-169a3dc59ab1.png)to
    minimize the KL divergence'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/b477c3d6-8c06-4433-9a64-a63a0ba04d3f.png)意味着我们在最大化估计和真实分布之间的KL散度，但我们想要将它们最小化，因此我们可以写成![](img/673c19ad-c94e-4e0b-b2a0-169a3dc59ab1.png)来最小化KL散度'
- en: 'Thus, our loss function becomes the following:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的损失函数变为以下内容：
- en: ^(![](img/83fbb709-c90b-402c-b991-9c89c2a46405.png))
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^(![](img/83fbb709-c90b-402c-b991-9c89c2a46405.png))
- en: '![](img/9b784e93-0e8c-455a-b8f3-9f1ba156ea73.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b784e93-0e8c-455a-b8f3-9f1ba156ea73.png)'
- en: If you look at this equation, ![](img/cb826f28-0f1c-4619-9919-f426752f7735.png)
    basically implies the reconstruction of the input, that is, the decoder which
    takes the latent vector ![](img/bafe6f36-ba5c-4d77-bd79-62ea348576e3.png) and
    reconstructs the input ![](img/0408a610-5dd5-4478-bac9-7b1da33170a8.png).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看这个方程式，![](img/cb826f28-0f1c-4619-9919-f426752f7735.png) 基本上意味着输入的重建，即解码器采用潜在向量
    ![](img/bafe6f36-ba5c-4d77-bd79-62ea348576e3.png) 并重建输入 ![](img/0408a610-5dd5-4478-bac9-7b1da33170a8.png)。
- en: 'Thus, our final loss function is the sum of the reconstruction loss and the
    KL divergence:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终损失函数是重建损失和KL散度的总和：
- en: '![](img/34903840-530c-4af8-bdd8-5cc37922025a.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34903840-530c-4af8-bdd8-5cc37922025a.png)'
- en: 'The value for KL divergence is simplified as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的KL散度值如下所示：
- en: '![](img/ddfcb727-e6b5-4ca0-82eb-8875888153b4.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddfcb727-e6b5-4ca0-82eb-8875888153b4.png)'
- en: Thus, minimizing the preceding loss function implies we are minimizing the reconstruction
    loss and also minimizing the KL divergence between the estimated and real distribution.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小化上述损失函数意味着我们在最小化重建损失的同时，还在最小化估计和真实分布之间的KL散度。
- en: Reparameterization trick
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重参数化技巧
- en: 'We face a problem while training VAE a through gradient descent. Remember,
    we are performing a sampling operation to generate a latent vector. Since a sampling
    operation is not differentiable, we cannot calculate gradients. That is, while
    backpropagating the network to minimize the error, we cannot calculate the gradients
    of the sampling operation as shown in the following diagram:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练VAE时遇到了一个问题，即通过梯度下降。请记住，我们正在执行一个采样操作来生成潜在向量。由于采样操作不可微分，我们无法计算梯度。也就是说，在反向传播网络以最小化错误时，我们无法计算采样操作的梯度，如下图所示：
- en: '![](img/7e16aa56-b27b-4198-9b08-678a5039bfed.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e16aa56-b27b-4198-9b08-678a5039bfed.png)'
- en: 'So, to combat this, we introduce a new trick called the **reparameterization
    trick**. We introduce a new parameter called **epsilon**, which we randomly sample
    from a unit Gaussian, which is given as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了应对这一问题，我们引入了一种称为**重参数化技巧**的新技巧。我们引入了一个称为**epsilon**的新参数，它是从单位高斯分布中随机采样的，具体如下所示：
- en: '![](img/edc09dff-7c1c-4b9f-8c06-d8a4647da372.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edc09dff-7c1c-4b9f-8c06-d8a4647da372.png)'
- en: 'And now we can rewrite our latent vector ![](img/2782f2b6-cb1f-4318-a3b5-b40add7dd37a.png)
    as:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以重新定义我们的潜在向量 ![](img/2782f2b6-cb1f-4318-a3b5-b40add7dd37a.png) 为：
- en: '![](img/c1a1d374-301f-4928-a2d9-e652e98c7e4d.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1a1d374-301f-4928-a2d9-e652e98c7e4d.png)'
- en: 'The reparameterization trick is shown in the following diagram:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 重参数化技巧如下图所示：
- en: '![](img/a7932bf5-a710-4e8a-a152-696d466210ca.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7932bf5-a710-4e8a-a152-696d466210ca.png)'
- en: Thus, with the reparameterization trick, we can train the VAE with the gradient
    descent algorithm.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过重参数化技巧，我们可以使用梯度下降算法训练VAE。
- en: Generating images using VAE
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VAE生成图像
- en: Now that we have understood how the VAE model works, in this section, we will
    learn how to use VAE to generate images.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了VAE模型的工作原理，在本节中，我们将学习如何使用VAE生成图像。
- en: 'Import the required libraries:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE52]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Preparing the dataset
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'Load the MNIST dataset:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: '[PRE53]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Normalize the dataset:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化数据集：
- en: '[PRE54]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Reshape the dataset:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑数据集：
- en: '[PRE55]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now let''s define some important parameters:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一些重要的参数：
- en: '[PRE56]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Defining the encoder
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义编码器
- en: 'Define the input:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入：
- en: '[PRE57]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Encoder hidden layer:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器隐藏层：
- en: '[PRE58]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Compute the mean and the variance:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 计算均值和方差：
- en: '[PRE59]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Defining the sampling operation
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义采样操作
- en: 'Define the sampling operation with a reparameterization trick that samples
    the latent vector from the encoder''s distribution:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重参数化技巧定义采样操作，从编码器分布中采样潜在向量：
- en: '[PRE60]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Sample the latent vector *z* from the mean and variance:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 从均值和方差中采样潜在向量 *z*：
- en: '[PRE61]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Defining the decoder
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义解码器
- en: 'Define the decoder with two layers:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 定义具有两层的解码器：
- en: '[PRE62]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Reconstruct the images using decoder which takes the latent vector ![](img/7b049780-80c1-4a44-8bef-98d45421b9a3.png)
    as input and returns the reconstructed image:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解码器重建图像，解码器将潜在向量 ![](img/7b049780-80c1-4a44-8bef-98d45421b9a3.png) 作为输入并返回重建的图像：
- en: '[PRE63]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Building the model
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'We build the model as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式构建模型：
- en: '[PRE64]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Define the reconstruction loss:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 定义重建损失：
- en: '[PRE65]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define KL divergence:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 定义KL散度：
- en: '[PRE66]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Thus, the total loss can be defined as:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总损失可以定义为：
- en: '[PRE67]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Add loss and compile the model:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 添加损失并编译模型：
- en: '[PRE68]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Train the model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE69]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Defining the generator
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器
- en: 'Define the generator samples from the learned distribution and generates an
    image:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器从学习到的分布中取样并生成图像：
- en: '[PRE70]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Plotting generated images
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制生成的图像
- en: 'Now we plot the image generated by the generator:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制由生成器生成的图像：
- en: '[PRE71]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following is the plot of the image generated by a generator:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由生成器生成的图像的绘图：
- en: '![](img/0a7a7d2e-177b-4966-af88-d8baf6897a0a.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a7a7d2e-177b-4966-af88-d8baf6897a0a.png)'
- en: Summary
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by learning what autoencoders are and how autoencoders
    are used to reconstruct their own input. We explored convolutional autoencoders,
    where instead of using feedforward networks, we used convolutional and deconvolutional
    layers for encoding and decoding, respectively. Following this, we learned about
    sparse which activate only certain neurons. Then, we learned about another type
    of regularizing autoencoder, called a contractive autoencoder, and at the end
    of the chapter, we learned about VAE which is a generative autoencoder model.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习自编码器及其如何用于重构其自身输入来开始本章。我们探讨了卷积自编码器，其中我们使用卷积层和反卷积层进行编码和解码。随后，我们学习了稀疏自编码器，它只激活特定的神经元。然后，我们学习了另一种正则化自编码器类型，称为压缩自编码器，最后，我们学习了VAE，这是一种生成自编码器模型。
- en: In the next chapter, we will learn about how to learn from a less data points
    using few-shot learning algorithms.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用少量数据点来进行学习，使用few-shot学习算法。
- en: Questions
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s examine our knowledge of autoencoders by answering the following questions:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来检验我们对自编码器的了解：
- en: What are autoencoders?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是自编码器？
- en: What is the objective function of autoencoders?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器的目标函数是什么？
- en: How do convolutional autoencoders differ from vanilla autoencoders?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积自编码器与普通自编码器有何不同？
- en: What are denoising autoencoders?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是去噪自编码器？
- en: How is the average activation of the neuron computed?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何计算神经元的平均激活？
- en: Define the loss function of contractive autoencoders.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了压缩自编码器的损失函数。
- en: What are the Frobenius norm and Jacobian matrix?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是Frobenius范数和雅可比矩阵？
- en: Further reading
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can also check the following links for more information:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以查看以下链接以获取更多信息：
- en: '*Sparse autoencoder* notes by Andrew Ng, [https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稀疏自编码器* 的笔记由Andrew Ng提供，[https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)'
- en: '*Contractive Auto-Encoders: Explicit Invariance During Feature Extraction*
    by Salah Rifai, et al., [http://www.icml-2011.org/papers/455_icmlpaper.pdf](http://www.icml-2011.org/papers/455_icmlpaper.pdf)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*压缩自编码器：特征提取期间的显式不变性* 由Salah Rifai等人撰写，[http://www.icml-2011.org/papers/455_icmlpaper.pdf](http://www.icml-2011.org/papers/455_icmlpaper.pdf)'
- en: '*Variational Autoencoder for Deep Learning of Images, Labels and Captions*
    by Yunchen Pu, et al., [https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf](https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分自编码器用于深度学习图像、标签和标题* 由Yunchen Pu等人撰写，[https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf](https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf)'
