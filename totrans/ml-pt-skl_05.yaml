- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Compressing Data via Dimensionality Reduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过降维压缩数据
- en: In *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*, you
    learned about the different approaches for reducing the dimensionality of a dataset
    using different feature selection techniques. An alternative approach to feature
    selection for dimensionality reduction is **feature extraction**. In this chapter,
    you will learn about two fundamental techniques that will help you to summarize
    the information content of a dataset by transforming it onto a new feature subspace
    of lower dimensionality than the original one. Data compression is an important
    topic in machine learning, and it helps us to store and analyze the increasing
    amounts of data that are produced and collected in the modern age of technology.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，*构建良好的训练数据集 – 数据预处理*中，您已经了解了使用不同特征选择技术来降低数据集维度的不同方法。作为降维的替代方法，**特征提取**允许您通过将数据转换到比原始维度更低的新特征子空间来总结数据集的信息内容。数据压缩是机器学习中的重要主题，它帮助我们存储和分析在技术发展的现代时代中产生和收集的增加量的数据。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Principal component analysis for unsupervised data compression
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析用于无监督数据压缩
- en: Linear discriminant analysis as a supervised dimensionality reduction technique
    for maximizing class separability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析作为一种监督降维技术，旨在最大化类别可分性。
- en: A brief overview of nonlinear dimensionality reduction techniques and t-distributed
    stochastic neighbor embedding for data visualization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于非线性降维技术的简要概述和用于数据可视化的t-分布随机邻居嵌入
- en: Unsupervised dimensionality reduction via principal component analysis
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过主成分分析实现无监督降维
- en: Similar to feature selection, we can use different feature extraction techniques
    to reduce the number of features in a dataset. The difference between feature
    selection and feature extraction is that while we maintain the original features
    when we use feature selection algorithms, such as **sequential backward selection**,
    we use feature extraction to transform or project the data onto a new feature
    space.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于特征选择，我们可以使用不同的特征提取技术来减少数据集中的特征数量。特征选择和特征提取的区别在于，特征选择算法如**顺序向后选择**保留原始特征，而特征提取则通过转换或投影数据到新的特征空间来实现降维。
- en: In the context of dimensionality reduction, feature extraction can be understood
    as an approach to data compression with the goal of maintaining most of the relevant
    information. In practice, feature extraction is not only used to improve storage
    space or the computational efficiency of the learning algorithm but can also improve
    the predictive performance by reducing the **curse of dimensionality**—especially
    if we are working with non-regularized models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在降维的背景下，特征提取可以被理解为一种数据压缩的方法，其目标是保留大部分相关信息。实际上，特征提取不仅用于改善存储空间或学习算法的计算效率，而且可以通过减少**维度诅咒**（特别是在使用非正则化模型时）来提高预测性能。
- en: The main steps in principal component analysis
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析中的主要步骤
- en: In this section, we will discuss **principal component analysis** (**PCA**),
    an unsupervised linear transformation technique that is widely used across different
    fields, most prominently for feature extraction and dimensionality reduction.
    Other popular applications of PCA include exploratory data analysis and the denoising
    of signals in stock market trading, and the analysis of genome data and gene expression
    levels in the field of bioinformatics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论**主成分分析**（**PCA**），这是一种广泛应用于不同领域的无监督线性变换技术，最突出的用途是特征提取和降维。PCA的其他流行应用包括探索性数据分析、股市交易中信号去噪以及生物信息学领域中基因组数据和基因表达水平的分析。
- en: 'PCA helps us to identify patterns in data based on the correlation between
    features. In a nutshell, PCA aims to find the directions of maximum variance in
    high-dimensional data and projects the data onto a new subspace with equal or
    fewer dimensions than the original one. The orthogonal axes (principal components)
    of the new subspace can be interpreted as the directions of maximum variance given
    the constraint that the new feature axes are orthogonal to each other, as illustrated
    in *Figure 5.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Using PCA to find the directions of maximum variance in a dataset'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.1*, *x*[1] and *x*[2] are the original feature axes, and **PC 1**
    and **PC 2** are the principal components.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use PCA for dimensionality reduction, we construct a *d*×*k*-dimensional
    transformation matrix, **W**, that allows us to map a vector of the features of
    the training example, **x**, onto a new *k*-dimensional feature subspace that
    has fewer dimensions than the original *d*-dimensional feature space. For instance,
    the process is as follows. Suppose we have a feature vector, **x**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'which is then transformed by a transformation matrix, ![](img/B17582_05_002.png):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**xW** = **z**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'resulting in the output vector:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_003.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: As a result of transforming the original *d*-dimensional data onto this new
    *k*-dimensional subspace (typically *k* << *d*), the first principal component
    will have the largest possible variance. All consequent principal components will
    have the largest variance given the constraint that these components are uncorrelated
    (orthogonal) to the other principal components—even if the input features are
    correlated, the resulting principal components will be mutually orthogonal (uncorrelated).
    Note that the PCA directions are highly sensitive to data scaling, and we need
    to standardize the features prior to PCA if the features were measured on different
    scales and we want to assign equal importance to all features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Before looking at the PCA algorithm for dimensionality reduction in more detail,
    let’s summarize the approach in a few simple steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the *d*-dimensional dataset.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the covariance matrix.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decompose the covariance matrix into its eigenvectors and eigenvalues.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B17582_05_004.png)).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a projection matrix, **W**, from the “top” *k* eigenvectors.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the *d*-dimensional input dataset, **X**, using the projection matrix,
    **W**, to obtain the new *k*-dimensional feature subspace.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we will perform a PCA step by step using Python as
    a learning exercise. Then, we will see how to perform a PCA more conveniently
    using scikit-learn.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigendecomposition: Decomposing a Matrix into Eigenvectors and Eigenvalues**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征分解：将矩阵分解为特征向量和特征值**'
- en: Eigendecomposition, the factorization of a square matrix into so-called **eigenvalues**
    and **eigenvectors**, is at the core of the PCA procedure described in this section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分解，将一个方阵分解成所谓的**特征值**和**特征向量**，是本节描述的PCA过程的核心。
- en: 'The covariance matrix is a special case of a square matrix: it’s a symmetric
    matrix, which means that the matrix is equal to its transpose, *A* = *A*^T.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是方阵的一种特殊情况：它是对称矩阵，这意味着矩阵等于其转置，*A* = *A*^T。
- en: When we decompose such a symmetric matrix, the eigenvalues are real (rather
    than complex) numbers, and the eigenvectors are orthogonal (perpendicular) to
    each other. Furthermore, eigenvalues and eigenvectors come in pairs. If we decompose
    a covariance matrix into its eigenvectors and eigenvalues, the eigenvectors associated
    with the highest eigenvalue corresponds to the direction of maximum variance in
    the dataset. Here, this “direction” is a linear transformation of the dataset’s
    feature columns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这样的对称矩阵分解时，特征值是实数（而不是复数），特征向量彼此正交（垂直）。此外，特征值和特征向量是成对出现的。如果我们将协方差矩阵分解为其特征向量和特征值，与最高特征值相关联的特征向量对应于数据集中方差的最大方向。在这里，这个“方向”是数据集特征列的线性变换。
- en: While a more detailed discussion of eigenvalues and eigenvectors is beyond the
    scope of this book, a relatively thorough treatment with pointers to additional
    resources can be found on Wikipedia at [https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书不涉及特征值和特征向量的详细讨论，但可以在维基百科上找到相对详尽的处理方法和指向其他资源的指针，网址为[https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)。
- en: Extracting the principal components step by step
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步提取主成分
- en: 'In this subsection, we will tackle the first four steps of a PCA:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将解决PCA的前四个步骤：
- en: Standardizing the data
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Constructing the covariance matrix
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建协方差矩阵
- en: Obtaining the eigenvalues and eigenvectors of the covariance matrix
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得协方差矩阵的特征值和特征向量
- en: Sorting the eigenvalues by decreasing order to rank the eigenvectors
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征值按降序排列以排名特征向量
- en: 'First, we will start by loading the Wine dataset that we worked with in *Chapter
    4*, *Building Good Training Datasets – Data Preprocessing*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载在*第4章* *构建良好的训练数据集 - 数据预处理* 中使用过的葡萄酒数据集：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Obtaining the Wine dataset**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取葡萄酒数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, you can replace the following lines:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的代码包中找到葡萄酒数据集的副本（以及本书中使用的所有其他数据集），如果您离线工作或UCI服务器在[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)暂时不可用时，您可以使用它。例如，要从本地目录加载葡萄酒数据集，可以替换以下行：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with these ones:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下方法：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will process the Wine data into separate training and test datasets—using
    70 percent and 30 percent of the data, respectively—and standardize it to unit
    variance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将逐步处理葡萄酒数据，将其分为独立的训练和测试数据集—分别使用数据的70%和30%，并将其标准化为单位方差：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After completing the mandatory preprocessing by executing the preceding code,
    let’s advance to the second step: constructing the covariance matrix. The symmetric
    *d*×*d*-dimensional covariance matrix, where *d* is the number of dimensions in
    the dataset, stores the pairwise covariances between the different features. For
    example, the covariance between two features, *x*[j] and *x*[k], on the population
    level can be calculated via the following equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了通过执行上述代码的强制预处理之后，让我们进入第二步：构建协方差矩阵。这是一个对称的*d*×*d*维协方差矩阵，其中*d*是数据集中的维数，它存储不同特征之间的成对协方差。例如，人口水平上两个特征*x*[j]和*x*[k]之间的协方差可以通过以下方程计算：
- en: '![](img/B17582_05_005.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_005.png)'
- en: 'Here, ![](img/B17582_05_006.png) and ![](img/B17582_05_007.png) are the sample
    means of features *j* and *k*, respectively. Note that the sample means are zero
    if we standardized the dataset. A positive covariance between two features indicates
    that the features increase or decrease together, whereas a negative covariance
    indicates that the features vary in opposite directions. For example, the covariance
    matrix of three features can then be written as follows (note that ![](img/B17582_05_008.png)
    is the Greek uppercase letter sigma, which is not to be confused with the summation
    symbol):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_05_006.png)和![](img/B17582_05_007.png)分别是特征*j*和*k*的样本均值。请注意，如果我们标准化了数据集，样本均值将为零。两个特征之间的正协方差表明特征一起增加或减少，而负协方差表明特征以相反的方向变化。例如，三个特征的协方差矩阵可以写成如下形式（注意![](img/B17582_05_008.png)是希腊大写字母sigma，与求和符号不要混淆）：
- en: '![](img/B17582_05_009.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_009.png)'
- en: The eigenvectors of the covariance matrix represent the principal components
    (the directions of maximum variance), whereas the corresponding eigenvalues will
    define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors
    and eigenvalues from the 13×13-dimensional covariance matrix.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵的特征向量代表主成分（方差最大的方向），而相应的特征值则定义了它们的大小。在Wine数据集的情况下，我们将从13×13维度的协方差矩阵中获得13个特征向量和特征值。
- en: 'Now, for our third step, let’s obtain the eigenpairs of the covariance matrix.
    If you have taken a linear algebra class, you may have learned that an eigenvector,
    **v**, satisfies the following condition:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入我们的第三步，让我们获取协方差矩阵的特征对。如果您上过线性代数课程，可能已经了解到特征向量**v**满足以下条件：
- en: '![](img/B17582_05_010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_010.png)'
- en: 'Here, ![](img/B17582_03_040.png) is a scalar: the eigenvalue. Since the manual
    computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate
    task, we will use the `linalg.eig` function from NumPy to obtain the eigenpairs
    of the Wine covariance matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_03_040.png)是一个标量：特征值。由于手动计算特征向量和特征值有些冗长且复杂，我们将使用NumPy的`linalg.eig`函数来获取Wine协方差矩阵的特征对：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using the `numpy.cov` function, we computed the covariance matrix of the standardized
    training dataset. Using the `linalg.eig` function, we performed the eigendecomposition,
    which yielded a vector (`eigen_vals`) consisting of 13 eigenvalues and the corresponding
    eigenvectors stored as columns in a 13×13-dimensional matrix (`eigen_vecs`).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numpy.cov`函数，我们计算了标准化训练数据集的协方差矩阵。使用`linalg.eig`函数，我们进行了特征分解，得到一个向量（`eigen_vals`），其中包含13个特征值，并且将相应的特征向量存储为13×13维矩阵的列（`eigen_vecs`）。
- en: '**Eigendecomposition in NumPy**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy中的特征分解**'
- en: The `numpy.linalg.eig` function was designed to operate on both symmetric and
    non-symmetric square matrices. However, you may find that it returns complex eigenvalues
    in certain cases.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy.linalg.eig`函数被设计用于操作对称和非对称方阵。但是，在某些情况下，您可能会发现它返回复数特征值。'
- en: A related function, `numpy.linalg.eigh`, has been implemented to decompose Hermetian
    matrices, which is a numerically more stable approach to working with symmetric
    matrices such as the covariance matrix; `numpy.linalg.eigh` always returns real
    eigenvalues.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 相关函数`numpy.linalg.eigh`已实现对分解Hermetian矩阵的操作，这是一种在处理诸如协方差矩阵等对称矩阵时更稳定的数值方法；`numpy.linalg.eigh`总是返回实数特征值。
- en: Total and explained variance
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总和和解释的方差
- en: 'Since we want to reduce the dimensionality of our dataset by compressing it
    onto a new feature subspace, we only select the subset of the eigenvectors (principal
    components) that contains most of the information (variance). The eigenvalues
    define the magnitude of the eigenvectors, so we have to sort the eigenvalues by
    decreasing magnitude; we are interested in the top *k* eigenvectors based on the
    values of their corresponding eigenvalues. But before we collect those *k* most
    informative eigenvectors, let’s plot the **variance explained ratios** of the
    eigenvalues. The variance explained ratio of an eigenvalue, ![](img/B17582_05_012.png),
    is simply the fraction of an eigenvalue, ![](img/B17582_05_012.png), and the total
    sum of the eigenvalues:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_014.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'Using the NumPy `cumsum` function, we can then calculate the cumulative sum
    of explained variances, which we will then plot via Matplotlib’s `step` function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting plot indicates that the first principal component alone accounts
    for approximately 40 percent of the variance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can see that the first two principal components combined explain almost
    60 percent of the variance in the dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B17582_05_02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The proportion of the total variance captured by the principal
    components'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Although the explained variance plot reminds us of the feature importance values
    that we computed in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*,
    via random forests, we should remind ourselves that PCA is an unsupervised method,
    which means that information about the class labels is ignored. Whereas a random
    forest uses the class membership information to compute the node impurities, variance
    measures the spread of values along a feature axis.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have successfully decomposed the covariance matrix into eigenpairs,
    let’s proceed with the last three steps to transform the Wine dataset onto the
    new principal component axes. The remaining steps we are going to tackle in this
    section are the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B17582_05_004.png)).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a projection matrix, **W**, from the “top” *k* eigenvectors.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the *d*-dimensional input dataset, **X**, using the projection matrix,
    **W**, to obtain the new *k*-dimensional feature subspace.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Or, in less technical terms, we will sort the eigenpairs by descending order
    of the eigenvalues, construct a projection matrix from the selected eigenvectors,
    and use the projection matrix to transform the data onto the lower-dimensional
    subspace.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by sorting the eigenpairs by decreasing order of the eigenvalues:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we collect the two eigenvectors that correspond to the two largest eigenvalues,
    to capture about 60 percent of the variance in this dataset. Note that two eigenvectors
    have been chosen for the purpose of illustration, since we are going to plot the
    data via a two-dimensional scatterplot later in this subsection. In practice,
    the number of principal components has to be determined by a tradeoff between
    computational efficiency and the performance of the classifier:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们收集与两个最大特征值对应的两个特征向量，以捕获数据集中约60%的方差。请注意，出于说明目的，选择了两个特征向量，因为我们将在后面的小节中通过二维散点图绘制数据。实际上，主成分的数量必须通过计算效率和分类器性能之间的权衡来确定：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By executing the preceding code, we have created a 13×2-dimensional projection
    matrix, **W**, from the top two eigenvectors.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行上述代码，我们创建了一个13×2维的投影矩阵**W**，由前两个特征向量构成。
- en: '**Mirrored projections**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**镜像投影**'
- en: 'Depending on which versions of NumPy and LAPACK you are using, you may obtain
    the matrix, **W**, with its signs flipped. Please note that this is not an issue;
    if **v** is an eigenvector of a matrix, ![](img/B17582_05_016.png), we have:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于您使用的NumPy和LAPACK版本，您可能会获得矩阵**W**及其符号翻转的情况。请注意，这不是问题；如果**v**是矩阵![](img/B17582_05_016.png)的特征向量，则有：
- en: '![](img/B17582_05_017.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_017.png)'
- en: 'Here, **v** is the eigenvector, and –**v** is also an eigenvector, which we
    can show as follows. Using basic algebra, we can multiply both sides of the equation
    by a scalar, ![](img/B17582_05_018.png):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**v**是特征向量，–**v**也是特征向量，我们可以如下展示。使用基本代数，我们可以将方程两边乘以标量![](img/B17582_05_018.png)：
- en: '![](img/B17582_05_019.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_019.png)'
- en: 'Since matrix multiplication is associative for scalar multiplication, we can
    then rearrange this to the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵乘法对标量乘法是结合的，我们可以将其重新排列为以下形式：
- en: '![](img/B17582_05_020.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_020.png)'
- en: Now, we can see that ![](img/B17582_05_021.png) is an eigenvector with the same
    eigenvalue, ![](img/B17582_05_022.png), for both ![](img/B17582_05_023.png) and
    ![](img/B17582_05_024.png). Hence, both **v** and –**v** are eigenvectors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到![](img/B17582_05_021.png)是具有相同特征值![](img/B17582_05_022.png)的特征向量，适用于![](img/B17582_05_023.png)和![](img/B17582_05_024.png)。因此，**v**和–**v**都是特征向量。
- en: 'Using the projection matrix, we can now transform an example, **x** (represented
    as a 13-dimensional row vector), onto the PCA subspace (the principal components
    one and two) obtaining **x**′, now a two-dimensional example vector consisting
    of two new features:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 利用投影矩阵，我们现在可以将一个例子**x**（表示为13维行向量）转换到PCA子空间（主成分一和二），得到**x**′，现在是一个由两个新特征组成的二维例子向量：
- en: '**x**′ = **xW**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**x**′ = **xW**'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, we can transform the entire 124×13-dimensional training dataset
    onto the two principal components by calculating the matrix dot product:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过计算矩阵点积，将整个124×13维的训练数据集转换为两个主成分：
- en: '**X**′ = **XW**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**X**′ = **XW**'
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Lastly, let’s visualize the transformed Wine training dataset, now stored as
    an 124×2-dimensional matrix, in a two-dimensional scatterplot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将转换后的Wine训练数据集可视化为一个124×2维的矩阵，在二维散点图中显示：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see in *Figure 5.3*, the data is more spread along the first principal
    component (*x* axis) than the second principal component (*y* axis), which is
    consistent with the explained variance ratio plot that we created in the previous
    subsection. However, we can tell that a linear classifier will likely be able
    to separate the classes well:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图5.3*中所看到的，数据沿第一主成分（*x*轴）的分布比第二主成分（*y*轴）更广泛，这与我们在前一小节创建的解释方差比例图一致。然而，我们可以看出线性分类器很可能能够很好地分离这些类别：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图，散点图 说明自动生成](img/B17582_05_03.png)'
- en: 'Figure 5.3: Data records from the Wine dataset projected onto a 2D feature
    space via PCA'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：通过PCA将Wine数据集投影到二维特征空间
- en: Although we encoded the class label information for the purpose of illustration
    in the preceding scatterplot, we have to keep in mind that PCA is an unsupervised
    technique that doesn’t use any class label information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在前面的散点图中对类标签信息进行了编码以说明问题，但我们必须记住，PCA是一种不使用任何类标签信息的无监督技术。
- en: Principal component analysis in scikit-learn
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在scikit-learn中的主成分分析
- en: Although the verbose approach in the previous subsection helped us to follow
    the inner workings of PCA, we will now discuss how to use the `PCA` class implemented
    in scikit-learn.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前一小节中详细的方法帮助我们理解 PCA 的内部工作，但现在我们将讨论如何使用 scikit-learn 中实现的 `PCA` 类。
- en: 'The `PCA` class is another one of scikit-learn’s transformer classes, with
    which we first fit the model using the training data before we transform both
    the training data and the test dataset using the same model parameters. Now, let’s
    use the `PCA` class from scikit-learn on the Wine training dataset, classify the
    transformed examples via logistic regression, and visualize the decision regions
    via the `plot_decision_regions` function that we defined in *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`PCA` 类是 scikit-learn 的另一个转换器类之一，我们首先使用训练数据拟合模型，然后使用相同的模型参数转换训练数据和测试数据集。现在，让我们在
    Wine 训练数据集上使用 scikit-learn 中的 `PCA` 类，通过逻辑回归对转换后的示例进行分类，并通过我们在 *第二章*，*分类简单机器学习算法的训练*
    中定义的 `plot_decision_regions` 函数可视化决策区域：'
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For your convenience, you can place the preceding `plot_decision_regions` code
    into a separate code file in your current working directory, for example, `plot_decision_regions_script.py`,
    and import it into your current Python session:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了您的方便，您可以将前述的 `plot_decision_regions` 代码放入当前工作目录中的单独代码文件中，例如 `plot_decision_regions_script.py`，并将其导入到当前的
    Python 会话中：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By executing this code, we should now see the decision regions for the training
    data reduced to two principal component axes:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行此代码，我们现在应该可以看到将训练数据的决策区域减少为两个主成分轴：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_05_04.png)'
- en: 'Figure 5.4: Training examples and logistic regression decision regions after
    using scikit-learn’s PCA for dimensionality reduction'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.4: 使用 scikit-learn 的 PCA 进行降维后的训练示例和逻辑回归决策区域'
- en: When we compare the PCA projections via scikit-learn with our own PCA implementation,
    we might see that the resulting plots are mirror images of each other. Note that
    this is not due to an error in either of those two implementations; the reason
    for this difference is that, depending on the eigensolver, eigenvectors can have
    either negative or positive signs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 scikit-learn 中的 PCA 投影与我们自己的 PCA 实现进行比较时，我们可能会看到生成的图是彼此的镜像。请注意，这不是这两个实现中的任何一个错误的原因；这种差异的原因是，依赖于特征求解器，特征向量可以具有负或正的符号。
- en: 'Not that it matters, but we could simply revert the mirror image by multiplying
    the data by –1 if we wanted to; note that eigenvectors are typically scaled to
    unit length 1\. For the sake of completeness, let’s plot the decision regions
    of the logistic regression on the transformed test dataset to see if it can separate
    the classes well:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 并不重要，但如果我们希望，可以通过将数据乘以 -1 简单地将镜像图像还原；请注意，特征向量通常缩放为单位长度 1。为了完整起见，让我们绘制转换后测试数据集上的逻辑回归决策区域，以查看它是否能很好地分离类别：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After we plot the decision regions for the test dataset by executing the preceding
    code, we can see that logistic regression performs quite well on this small two-dimensional
    feature subspace and only misclassifies a few examples in the test dataset:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上述代码并为测试数据集绘制决策区域之后，我们可以看到逻辑回归在这个小的二维特征子空间上表现相当不错，仅在测试数据集中错误分类了一些示例：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_05.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_05_05.png)'
- en: 'Figure 5.5: Test datapoints with logistic regression decision regions in the
    PCA-based feature space'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.5: 在基于 PCA 特征空间中的测试数据点与逻辑回归决策区域'
- en: 'If we are interested in the explained variance ratios of the different principal
    components, we can simply initialize the `PCA` class with the `n_components` parameter
    set to `None`, so all principal components are kept and the explained variance
    ratio can then be accessed via the `explained_variance_ratio_` attribute:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对不同主成分的解释方差比感兴趣，可以简单地使用 `n_components` 参数设置为 `None` 初始化 `PCA` 类，这样所有主成分都会被保留，并且可以通过
    `explained_variance_ratio_` 属性访问解释的方差比：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we set `n_components=None` when we initialized the `PCA` class so
    that it will return all principal components in a sorted order, instead of performing
    a dimensionality reduction.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们初始化 `PCA` 类时，设置 `n_components=None`，以便按排序顺序返回所有主成分，而不进行降维。
- en: Assessing feature contributions
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估特征贡献
- en: In this section, we will take a brief look at how we can assess the contributions
    of the original features to the principal components. As we learned, via PCA,
    we create principal components that represent linear combinations of the features.
    Sometimes, we are interested to know about how much each original feature contributes
    to a given principal component. These contributions are often called **loadings**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The factor loadings can be computed by scaling the eigenvectors by the square
    root of the eigenvalues. The resulting values can then be interpreted as the correlation
    between the original features and the principal component. To illustrate this,
    let us plot the loadings for the first principal component.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the 13×13-dimensional loadings matrix by multiplying the
    eigenvectors by the square root of the eigenvalues:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we plot the loadings for the first principal component, `loadings[:,
    0]`, which is the first column in this matrix:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In *Figure 5.6*, we can see that, for example, **Alcohol** has a negative correlation
    with the first principal component (approximately –0.3), whereas **Malic acid**
    has a positive correlation (approximately 0.54). Note that a value of 1 describes
    a perfect positive correlation whereas a value of –1 corresponds to a perfect
    negative correlation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_05_06.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Feature correlations with the first principal component'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code example, we compute the factor loadings for our own PCA
    implementation. We can obtain the loadings from a fitted scikit-learn PCA object
    in a similar manner, where `pca.components_` represents the eigenvectors and `pca.explained_variance_`
    represents the eigenvalues:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To compare the scikit-learn PCA loadings with those we created previously,
    let us create a similar bar plot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we can see, the bar plots look the same:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_05_07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Feature correlations to the first principal component using scikit-learn'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: After exploring PCA as an unsupervised feature extraction technique, the next
    section will introduce **linear discriminant analysis** (**LDA**), which is a
    linear transformation technique that takes class label information into account.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Supervised data compression via linear discriminant analysis
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA can be used as a technique for feature extraction to increase computational
    efficiency and reduce the degree of overfitting due to the curse of dimensionality
    in non-regularized models. The general concept behind LDA is very similar to PCA,
    but whereas PCA attempts to find the orthogonal component axes of maximum variance
    in a dataset, the goal in LDA is to find the feature subspace that optimizes class
    separability. In the following sections, we will discuss the similarities between
    LDA and PCA in more detail and walk through the LDA approach step by step.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis versus linear discriminant analysis
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both PCA and LDA are *linear transformation techniques* that can be used to
    reduce the number of dimensions in a dataset; the former is an unsupervised algorithm,
    whereas the latter is supervised. Thus, we might think that LDA is a superior
    feature extraction technique for classification tasks compared to PCA. However,
    A.M. Martinez reported that preprocessing via PCA tends to result in better classification
    results in an image recognition task in certain cases, for instance, if each class
    consists of only a small number of examples (*PCA Versus LDA* by *A. M. Martinez*
    and *A. C. Kak*, *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    23(2): 228-233, 2001).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Fisher LDA**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA is sometimes also called **Fisher’s LDA**. Ronald A. Fisher initially formulated
    *Fisher’s Linear Discriminant* for two-class classification problems in 1936 (*The
    Use of Multiple Measurements in Taxonomic Problems*, *R. A. Fisher*, *Annals of
    Eugenics*, 7(2): 179-188, 1936). In 1948, Fisher’s linear discriminant was generalized
    for multiclass problems by C. Radhakrishna Rao under the assumption of equal class
    covariances and normally distributed classes, which we now call LDA (*The Utilization
    of Multiple Measurements in Problems of Biological Classification* by *C. R. Rao*,
    *Journal of the Royal Statistical Society*. Series B (Methodological), 10(2):
    159-203, 1948).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.8* summarizes the concept of LDA for a two-class problem. Examples
    from class 1 are shown as circles, and examples from class 2 are shown as crosses:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: The concept of LDA for a two-class problem'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: A linear discriminant, as shown on the *x* axis (*LD 1*), would separate the
    two normal distributed classes well. Although the exemplary linear discriminant
    shown on the *y* axis (*LD 2*) captures a lot of the variance in the dataset,
    it would fail as a good linear discriminant since it does not capture any of the
    class-discriminatory information.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: One assumption in LDA is that the data is normally distributed. Also, we assume
    that the classes have identical covariance matrices and that the training examples
    are statistically independent of each other. However, even if one, or more, of
    those assumptions is (slightly) violated, LDA for dimensionality reduction can
    still work reasonably well (*Pattern Classification 2nd Edition* by *R. O. Duda*,
    *P. E. Hart*, and *D. G. Stork*, *New York*, 2001).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The inner workings of linear discriminant analysis
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into the code implementation, let’s briefly summarize the main
    steps that are required to perform LDA:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the *d*-dimensional dataset (*d* is the number of features).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each class, compute the *d*-dimensional mean vector.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the between-class scatter matrix, **S**[B], and the within-class scatter
    matrix, **S**[W].
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvectors and corresponding eigenvalues of the matrix, ![](img/B17582_05_025.png).
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the *k* eigenvectors that correspond to the *k* largest eigenvalues to
    construct a *d*×*k*-dimensional transformation matrix, **W**; the eigenvectors
    are the columns of this matrix.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the examples onto the new feature subspace using the transformation
    matrix, **W**.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, LDA is quite similar to PCA in the sense that we are decomposing
    matrices into eigenvalues and eigenvectors, which will form the new lower-dimensional
    feature space. However, as mentioned before, LDA takes class label information
    into account, which is represented in the form of the mean vectors computed in
    *step 2*. In the following sections, we will discuss these seven steps in more
    detail, accompanied by illustrative code implementations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Computing the scatter matrices
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we already standardized the features of the Wine dataset in the PCA section
    at the beginning of this chapter, we can skip the first step and proceed with
    the calculation of the mean vectors, which we will use to construct the within-class
    scatter matrix and between-class scatter matrix, respectively. Each mean vector,
    **m**[i], stores the mean feature value, ![](img/B17582_05_026.png), with respect
    to the examples of class *i*:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_027.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'This results in three mean vectors:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_028.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'These mean vectors can be computed by the following code, where we compute
    one mean vector for each of the three labels:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using the mean vectors, we can now compute the within-class scatter matrix,
    **S**[W]:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_029.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'This is calculated by summing up the individual scatter matrices, **S**[i],
    of each individual class *i*:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_030.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The assumption that we are making when we are computing the scatter matrices
    is that the class labels in the training dataset are uniformly distributed. However,
    if we print the number of class labels, we see that this assumption is violated:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Thus, we want to scale the individual scatter matrices, **S**[i], before we
    sum them up as the scatter matrix, **S**[W]. When we divide the scatter matrices
    by the number of class-examples, *n*[i], we can see that computing the scatter
    matrix is in fact the same as computing the covariance matrix, ![](img/B17582_05_031.png)—the
    covariance matrix is a normalized version of the scatter matrix:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_032.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'The code for computing the scaled within-class scatter matrix is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After we compute the scaled within-class scatter matrix (or covariance matrix),
    we can move on to the next step and compute the between-class scatter matrix **S**[B]:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_033.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Here, **m** is the overall mean that is computed, including examples from all
    **c** classes:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Selecting linear discriminants for the new feature subspace
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The remaining steps of the LDA are similar to the steps of the PCA. However,
    instead of performing the eigendecomposition on the covariance matrix, we solve
    the generalized eigenvalue problem of the matrix, ![](img/B17582_05_034.png):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After we compute the eigenpairs, we can sort the eigenvalues in descending
    order:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In LDA, the number of linear discriminants is at most *c* – 1, where *c* is
    the number of class labels, since the in-between scatter matrix, **S**[B], is
    the sum of *c* matrices with rank one or less. We can indeed see that we only
    have two nonzero eigenvalues (the eigenvalues 3-13 are not exactly zero, but this
    is due to the floating-point arithmetic in NumPy.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '**Collinearity**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the rare case of perfect collinearity (all aligned example points
    fall on a straight line), the covariance matrix would have rank one, which would
    result in only one eigenvector with a nonzero eigenvalue.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure how much of the class-discriminatory information is captured by
    the linear discriminants (eigenvectors), let’s plot the linear discriminants by
    decreasing eigenvalues, similar to the explained variance plot that we created
    in the PCA section. For simplicity, we will call the content of class-discriminatory
    information **discriminability**:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we can see in *Figure 5.9*, the first two linear discriminants alone capture
    100 percent of the useful information in the Wine training dataset:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing chart  Description automatically generated](img/B17582_05_09.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: The top two discriminants capture 100 percent of the useful information'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now stack the two most discriminative eigenvector columns to create the
    transformation matrix, **W**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Projecting examples onto the new feature space
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the transformation matrix **W** that we created in the previous subsection,
    we can now transform the training dataset by multiplying the matrices:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '**X**′ = **XW**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we can see in *Figure 5.10*, the three Wine classes are now perfectly linearly
    separable in the new feature subspace:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_10.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Wine classes perfectly separable after projecting the data onto
    the first two discriminants'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: LDA via scikit-learn
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'That step-by-step implementation was a good exercise to understand the inner
    workings of LDA and understand the differences between LDA and PCA. Now, let’s
    look at the `LDA` class implemented in scikit-learn:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s see how the logistic regression classifier handles the lower-dimensional
    training dataset after the LDA transformation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Looking at *Figure 5.11*, we can see that the logistic regression model misclassifies
    one of the examples from class 2:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_11.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: The logistic regression model misclassifies one of the classes'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'By lowering the regularization strength, we could probably shift the decision
    boundaries so that the logistic regression model classifies all examples in the
    training dataset correctly. However, and more importantly, let’s take a look at
    the results on the test dataset:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低正则化强度，我们可能可以移动决策边界，使得逻辑回归模型能够在训练数据集中正确分类所有示例。然而，更重要的是，让我们看看在测试数据集上的结果：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As we can see in *Figure 5.12*, the logistic regression classifier is able
    to get a perfect accuracy score for classifying the examples in the test dataset
    by only using a two-dimensional feature subspace, instead of the original 13 Wine
    features:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图5.12*中所见，逻辑回归分类器能够在测试数据集中只使用一个二维特征子空间，而不是原始的13个葡萄酒特征，获得完美的准确度分数：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_12.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B17582_05_12.png)'
- en: 'Figure 5.12: The logistic regression model works perfectly on the test data'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：逻辑回归模型在测试数据上的完美表现
- en: Nonlinear dimensionality reduction and visualization
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性降维和可视化
- en: In the previous section, we covered linear transformation techniques, such as
    PCA and LDA, for feature extraction. In this section, we will discuss why considering
    nonlinear dimensionality reduction techniques might be worthwhile.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了主成分分析（PCA）和线性判别分析（LDA）等线性变换技术进行特征提取。在本节中，我们将讨论为什么考虑非线性降维技术可能是值得的。
- en: One nonlinear dimensionality reduction technique that is particularly worth
    highlighting is **t-distributed stochastic neighbor embedding** (**t-SNE**) since
    it is frequently used in literature to visualize high-dimensional datasets in
    two or three dimensions. We will see how we can apply t-SNE to plot images of
    handwritten images in a 2-dimensional feature space.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得强调的一个非线性降维技术是**t分布随机邻居嵌入**（**t-SNE**），因为它经常用于文献中以二维或三维形式可视化高维数据集。我们将看到如何应用t-SNE来在二维特征空间中绘制手写图像的图像。
- en: Why consider nonlinear dimensionality reduction?
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么考虑非线性降维？
- en: 'Many machine learning algorithms make assumptions about the linear separability
    of the input data. You have learned that the perceptron even requires perfectly
    linearly separable training data to converge. Other algorithms that we have covered
    so far assume that the lack of perfect linear separability is due to noise: Adaline,
    logistic regression, and the (standard) SVM to just name a few.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法对输入数据的线性可分性有假设。您已经学到了感知器甚至需要完全线性可分的训练数据才能收敛。到目前为止，我们所涵盖的其他算法假设缺乏完全线性可分性是由于噪声引起的：Adaline、逻辑回归和（标准）支持向量机等。
- en: 'However, if we are dealing with nonlinear problems, which we may encounter
    rather frequently in real-world applications, linear transformation techniques
    for dimensionality reduction, such as PCA and LDA, may not be the best choice:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们处理非线性问题，这在实际应用中可能经常遇到，那么线性变换技术如主成分分析（PCA）和线性判别分析（LDA）可能不是最佳选择：
- en: '![](img/B17582_05_13.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_05_13.png)'
- en: 'Figure 5.13: The difference between linear and nonlinear problems'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：线性与非线性问题的区别
- en: The scikit-learn library implements a selection of advanced techniques for nonlinear
    dimensionality reduction that are beyond the scope of this book. The interested
    reader can find a nice overview of the current implementations in scikit-learn,
    complemented by illustrative examples, at [http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库实现了一些先进的非线性降维技术，超出了本书的范围。有兴趣的读者可以在[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)找到scikit-learn当前实现的良好概述，并配有说明性示例。
- en: The development and application of nonlinear dimensionality reduction techniques
    is also often referred to as manifold learning, where a manifold refers to a lower
    dimensional topological space embedded in a high-dimensional space. Algorithms
    for manifold learning have to capture the complicated structure of the data in
    order to project it onto a lower-dimensional space where the relationship between
    data points is preserved.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性降维技术的开发和应用通常也被称为流形学习，其中流形指的是嵌入在高维空间中的低维拓扑空间。流形学习算法必须捕捉数据的复杂结构，以便将其投影到一个保持数据点关系的低维空间中。
- en: 'A classic example of manifold learning is the 3-dimensional Swiss roll illustrated
    in *Figure 5.14*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B17582_05_14.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Three-dimensional Swiss roll projected into a lower, two-dimensional
    space'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: While nonlinear dimensionality reduction and manifold learning algorithms are
    very powerful, we should note that these techniques are notoriously hard to use,
    and with non-ideal hyperparameter choices, they may cause more harm than good.
    The reason behind this difficulty is that we are often working with high-dimensional
    datasets that we cannot readily visualize and where the structure is not obvious
    (unlike the Swiss roll example in *Figure 5.14*). Moreover, unless we project
    the dataset into two or three dimensions (which is often not sufficient for capturing
    more complicated relationships), it is hard or even impossible to assess the quality
    of the results. Hence, many people still rely on simpler techniques such as PCA
    and LDA for dimensionality reduction.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data via t-distributed stochastic neighbor embedding
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After introducing nonlinear dimensionality reduction and discussing some of
    its challenges, let’s take a look at a hands-on example involving t-SNE, which
    is often used for visualizing complex datasets in two or three dimensions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, t-SNE is modeling data points based on their pair-wise distances
    in the high-dimensional (original) feature space. Then, it finds a probability
    distribution of pair-wise distances in the new, lower-dimensional space that is
    close to the probability distribution of pair-wise distances in the original space.
    Or, in other words, t-SNE learns to embed data points into a lower-dimensional
    space such that the pairwise distances in the original space are preserved. You
    can find more details about this method in the original research paper *Visualizing
    data using t-SNE* by *Maaten and Hinton, Journal of Machine Learning Research*,
    2018 ([https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)).
    However, as the research paper title suggests, t-SNE is a technique intended for
    visualization purposes as it requires the whole dataset for the projection. Since
    it projects the points directly (unlike PCA, it does not involve a projection
    matrix), we cannot apply t-SNE to new data points.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows a quick demonstration of how t-SNE can be applied
    to a 64-dimensional dataset. First, we load the Digits dataset from scikit-learn,
    which consists of low-resolution handwritten digits (the numbers 0-9):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The digits are 8×8 grayscale images. The following code plots the first four
    images in the dataset, which consists of 1,797 images in total:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As we can see in *Figure 5.15*, the images are relatively low resolution, 8×8
    pixels (that is, 64 pixels per image):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart  Description automatically generated](img/B17582_05_15.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Low resolution images of handwritten digits'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `digits.data` attribute lets us access a tabular version of this
    dataset where the examples are represented by the rows, and the columns correspond
    to the pixels:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, let us assign the features (pixels) to a new variable `X_digits` and
    the labels to another new variable `y_digits`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we import the t-SNE class from scikit-learn and fit a new `tsne` object.
    Using `fit_transform`, we perform the t-SNE fitting and data transformation in
    one step:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using this code, we projected the 64-dimensional dataset onto a 2-dimensional
    space. We specified `init='pca'`, which initializes the t-SNE embedding using
    PCA as it is recommended in the research article *Initialization is critical for
    preserving global data structure in both t-SNE and UMAP* by *Kobak* and *Linderman*,
    *Nature Biotechnology Volume 39*, pages 156–157, 2021 ([https://www.nature.com/articles/s41587-020-00809-z](https://www.nature.com/articles/s41587-020-00809-z)).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note that t-SNE includes additional hyperparameters such as the perplexity and
    learning rate (often called **epsilon**), which we omitted in the example (we
    used the scikit-learn default values). In practice, we recommend you explore these
    parameters as well. More information about these parameters and their effects
    on the results can be found in the excellent article *How to Use t-SNE Effectively*
    by *Wattenberg*, *Viegas*, and *Johnson*, *Distill*, 2016 ([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let us visualize the 2D t-SNE embeddings using the following code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Like PCA, t-SNE is an unsupervised method, and in the preceding code, we use
    the class labels `y_digits` (0-9) only for visualization purposes via the functions
    color argument. Matplotlib’s `PathEffects` are used for visual purposes, such
    that the class label is displayed in the center (via `np.median`) of data points
    belonging to each respective digit. The resulting plot is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter chart  Description automatically generated](img/B17582_05_16.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: A visualization of how t-SNE embeds the handwritten digits in
    a 2D feature space'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, t-SNE is able to separate the different digits (classes) nicely,
    although not perfectly. It might be possible to achieve better separation by tuning
    the hyperparameters. However, a certain degree of class mixing might be unavoidable
    due to illegible handwriting. For instance, by inspecting individual images, we
    might find that certain instances of the number 3 indeed look like the number
    9, and so forth.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform manifold approximation and projection**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular visualization technique is **uniform manifold approximation
    and projection** (**UMAP**). While UMAP can produce similarly good results as
    t-SNE (for example, see the Kobak and Linderman paper referenced previously),
    it is typically faster, and it can also be used to project new data, which makes
    it more attractive as a dimensionality reduction technique in a machine learning
    context, similar to PCA. Interested readers can find more information about UMAP
    in the original paper: *UMAP: Uniform manifold approximation and projection for
    dimension reduction* by *McInnes, Healy*, and *Melville*,2018([https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426)).A
    scikit-learn compatible implementation of UMAP can be found at [https://umap-learn.readthedocs.io](https://umap-learn.readthedocs.io).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种流行的可视化技术是**均匀流形逼近和投影**（**UMAP**）。虽然UMAP可以产生与t-SNE类似好的结果（例如，请参阅之前引用的Kobak和Linderman的论文），但通常速度更快，并且还可以用于投影新数据，这使其在机器学习背景下作为降维技术更具吸引力，类似于PCA。对UMAP感兴趣的读者可以在原始论文中找到更多信息：*UMAP:
    Uniform manifold approximation and projection for dimension reduction*，作者是*McInnes,
    Healy*和*Melville*，2018年（[https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426)）。UMAP的scikit-learn兼容实现可以在[https://umap-learn.readthedocs.io](https://umap-learn.readthedocs.io)找到。'
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, you learned about two fundamental dimensionality reduction
    techniques for feature extraction: PCA and LDA. Using PCA, we projected data onto
    a lower-dimensional subspace to maximize the variance along the orthogonal feature
    axes, while ignoring the class labels. LDA, in contrast to PCA, is a technique
    for supervised dimensionality reduction, which means that it considers class information
    in the training dataset to attempt to maximize the class separability in a linear
    feature space. Lastly, you also learned about t-SNE, which is a nonlinear feature
    extraction technique that can be used for visualizing data in two or three dimensions.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了两种用于特征提取的基本降维技术：PCA和LDA。使用PCA，我们将数据投影到低维子空间中，以最大化沿正交特征轴的方差，同时忽略类标签。与PCA相反，LDA是一种用于监督降维的技术，这意味着它考虑训练数据集中的类信息，试图在线性特征空间中最大化类的可分离性。最后，您还了解了t-SNE，这是一种非线性特征提取技术，可用于在二维或三维中可视化数据。
- en: Equipped with PCA and LDA as fundamental data preprocessing techniques, you
    are now well prepared to learn about the best practices for efficiently incorporating
    different preprocessing techniques and evaluating the performance of different
    models in the next chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了PCA和LDA作为基本数据预处理技术，您现在已经准备好学习如何在下一章高效地结合不同的预处理技术并评估不同模型的性能。
- en: Join our book’s Discord space
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 参加每月一次的作者问答活动，可在书籍的Discord工作区参与：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
