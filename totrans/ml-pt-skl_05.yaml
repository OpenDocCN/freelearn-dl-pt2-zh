- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compressing Data via Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*, you
    learned about the different approaches for reducing the dimensionality of a dataset
    using different feature selection techniques. An alternative approach to feature
    selection for dimensionality reduction is **feature extraction**. In this chapter,
    you will learn about two fundamental techniques that will help you to summarize
    the information content of a dataset by transforming it onto a new feature subspace
    of lower dimensionality than the original one. Data compression is an important
    topic in machine learning, and it helps us to store and analyze the increasing
    amounts of data that are produced and collected in the modern age of technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis for unsupervised data compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear discriminant analysis as a supervised dimensionality reduction technique
    for maximizing class separability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief overview of nonlinear dimensionality reduction techniques and t-distributed
    stochastic neighbor embedding for data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised dimensionality reduction via principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to feature selection, we can use different feature extraction techniques
    to reduce the number of features in a dataset. The difference between feature
    selection and feature extraction is that while we maintain the original features
    when we use feature selection algorithms, such as **sequential backward selection**,
    we use feature extraction to transform or project the data onto a new feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of dimensionality reduction, feature extraction can be understood
    as an approach to data compression with the goal of maintaining most of the relevant
    information. In practice, feature extraction is not only used to improve storage
    space or the computational efficiency of the learning algorithm but can also improve
    the predictive performance by reducing the **curse of dimensionality**—especially
    if we are working with non-regularized models.
  prefs: []
  type: TYPE_NORMAL
- en: The main steps in principal component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss **principal component analysis** (**PCA**),
    an unsupervised linear transformation technique that is widely used across different
    fields, most prominently for feature extraction and dimensionality reduction.
    Other popular applications of PCA include exploratory data analysis and the denoising
    of signals in stock market trading, and the analysis of genome data and gene expression
    levels in the field of bioinformatics.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA helps us to identify patterns in data based on the correlation between
    features. In a nutshell, PCA aims to find the directions of maximum variance in
    high-dimensional data and projects the data onto a new subspace with equal or
    fewer dimensions than the original one. The orthogonal axes (principal components)
    of the new subspace can be interpreted as the directions of maximum variance given
    the constraint that the new feature axes are orthogonal to each other, as illustrated
    in *Figure 5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Using PCA to find the directions of maximum variance in a dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.1*, *x*[1] and *x*[2] are the original feature axes, and **PC 1**
    and **PC 2** are the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use PCA for dimensionality reduction, we construct a *d*×*k*-dimensional
    transformation matrix, **W**, that allows us to map a vector of the features of
    the training example, **x**, onto a new *k*-dimensional feature subspace that
    has fewer dimensions than the original *d*-dimensional feature space. For instance,
    the process is as follows. Suppose we have a feature vector, **x**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is then transformed by a transformation matrix, ![](img/B17582_05_002.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '**xW** = **z**'
  prefs: []
  type: TYPE_NORMAL
- en: 'resulting in the output vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: As a result of transforming the original *d*-dimensional data onto this new
    *k*-dimensional subspace (typically *k* << *d*), the first principal component
    will have the largest possible variance. All consequent principal components will
    have the largest variance given the constraint that these components are uncorrelated
    (orthogonal) to the other principal components—even if the input features are
    correlated, the resulting principal components will be mutually orthogonal (uncorrelated).
    Note that the PCA directions are highly sensitive to data scaling, and we need
    to standardize the features prior to PCA if the features were measured on different
    scales and we want to assign equal importance to all features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before looking at the PCA algorithm for dimensionality reduction in more detail,
    let’s summarize the approach in a few simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the *d*-dimensional dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decompose the covariance matrix into its eigenvectors and eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B17582_05_004.png)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a projection matrix, **W**, from the “top” *k* eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the *d*-dimensional input dataset, **X**, using the projection matrix,
    **W**, to obtain the new *k*-dimensional feature subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we will perform a PCA step by step using Python as
    a learning exercise. Then, we will see how to perform a PCA more conveniently
    using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigendecomposition: Decomposing a Matrix into Eigenvectors and Eigenvalues**'
  prefs: []
  type: TYPE_NORMAL
- en: Eigendecomposition, the factorization of a square matrix into so-called **eigenvalues**
    and **eigenvectors**, is at the core of the PCA procedure described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix is a special case of a square matrix: it’s a symmetric
    matrix, which means that the matrix is equal to its transpose, *A* = *A*^T.'
  prefs: []
  type: TYPE_NORMAL
- en: When we decompose such a symmetric matrix, the eigenvalues are real (rather
    than complex) numbers, and the eigenvectors are orthogonal (perpendicular) to
    each other. Furthermore, eigenvalues and eigenvectors come in pairs. If we decompose
    a covariance matrix into its eigenvectors and eigenvalues, the eigenvectors associated
    with the highest eigenvalue corresponds to the direction of maximum variance in
    the dataset. Here, this “direction” is a linear transformation of the dataset’s
    feature columns.
  prefs: []
  type: TYPE_NORMAL
- en: While a more detailed discussion of eigenvalues and eigenvectors is beyond the
    scope of this book, a relatively thorough treatment with pointers to additional
    resources can be found on Wikipedia at [https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the principal components step by step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will tackle the first four steps of a PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constructing the covariance matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtaining the eigenvalues and eigenvectors of the covariance matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sorting the eigenvalues by decreasing order to rank the eigenvectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will start by loading the Wine dataset that we worked with in *Chapter
    4*, *Building Good Training Datasets – Data Preprocessing*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Obtaining the Wine dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, you can replace the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'with these ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will process the Wine data into separate training and test datasets—using
    70 percent and 30 percent of the data, respectively—and standardize it to unit
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After completing the mandatory preprocessing by executing the preceding code,
    let’s advance to the second step: constructing the covariance matrix. The symmetric
    *d*×*d*-dimensional covariance matrix, where *d* is the number of dimensions in
    the dataset, stores the pairwise covariances between the different features. For
    example, the covariance between two features, *x*[j] and *x*[k], on the population
    level can be calculated via the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_05_006.png) and ![](img/B17582_05_007.png) are the sample
    means of features *j* and *k*, respectively. Note that the sample means are zero
    if we standardized the dataset. A positive covariance between two features indicates
    that the features increase or decrease together, whereas a negative covariance
    indicates that the features vary in opposite directions. For example, the covariance
    matrix of three features can then be written as follows (note that ![](img/B17582_05_008.png)
    is the Greek uppercase letter sigma, which is not to be confused with the summation
    symbol):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_009.png)'
  prefs: []
  type: TYPE_IMG
- en: The eigenvectors of the covariance matrix represent the principal components
    (the directions of maximum variance), whereas the corresponding eigenvalues will
    define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors
    and eigenvalues from the 13×13-dimensional covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for our third step, let’s obtain the eigenpairs of the covariance matrix.
    If you have taken a linear algebra class, you may have learned that an eigenvector,
    **v**, satisfies the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_03_040.png) is a scalar: the eigenvalue. Since the manual
    computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate
    task, we will use the `linalg.eig` function from NumPy to obtain the eigenpairs
    of the Wine covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using the `numpy.cov` function, we computed the covariance matrix of the standardized
    training dataset. Using the `linalg.eig` function, we performed the eigendecomposition,
    which yielded a vector (`eigen_vals`) consisting of 13 eigenvalues and the corresponding
    eigenvectors stored as columns in a 13×13-dimensional matrix (`eigen_vecs`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigendecomposition in NumPy**'
  prefs: []
  type: TYPE_NORMAL
- en: The `numpy.linalg.eig` function was designed to operate on both symmetric and
    non-symmetric square matrices. However, you may find that it returns complex eigenvalues
    in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: A related function, `numpy.linalg.eigh`, has been implemented to decompose Hermetian
    matrices, which is a numerically more stable approach to working with symmetric
    matrices such as the covariance matrix; `numpy.linalg.eigh` always returns real
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: Total and explained variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we want to reduce the dimensionality of our dataset by compressing it
    onto a new feature subspace, we only select the subset of the eigenvectors (principal
    components) that contains most of the information (variance). The eigenvalues
    define the magnitude of the eigenvectors, so we have to sort the eigenvalues by
    decreasing magnitude; we are interested in the top *k* eigenvectors based on the
    values of their corresponding eigenvalues. But before we collect those *k* most
    informative eigenvectors, let’s plot the **variance explained ratios** of the
    eigenvalues. The variance explained ratio of an eigenvalue, ![](img/B17582_05_012.png),
    is simply the fraction of an eigenvalue, ![](img/B17582_05_012.png), and the total
    sum of the eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the NumPy `cumsum` function, we can then calculate the cumulative sum
    of explained variances, which we will then plot via Matplotlib’s `step` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot indicates that the first principal component alone accounts
    for approximately 40 percent of the variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can see that the first two principal components combined explain almost
    60 percent of the variance in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B17582_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The proportion of the total variance captured by the principal
    components'
  prefs: []
  type: TYPE_NORMAL
- en: Although the explained variance plot reminds us of the feature importance values
    that we computed in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*,
    via random forests, we should remind ourselves that PCA is an unsupervised method,
    which means that information about the class labels is ignored. Whereas a random
    forest uses the class membership information to compute the node impurities, variance
    measures the spread of values along a feature axis.
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have successfully decomposed the covariance matrix into eigenpairs,
    let’s proceed with the last three steps to transform the Wine dataset onto the
    new principal component axes. The remaining steps we are going to tackle in this
    section are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B17582_05_004.png)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a projection matrix, **W**, from the “top” *k* eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the *d*-dimensional input dataset, **X**, using the projection matrix,
    **W**, to obtain the new *k*-dimensional feature subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Or, in less technical terms, we will sort the eigenpairs by descending order
    of the eigenvalues, construct a projection matrix from the selected eigenvectors,
    and use the projection matrix to transform the data onto the lower-dimensional
    subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by sorting the eigenpairs by decreasing order of the eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we collect the two eigenvectors that correspond to the two largest eigenvalues,
    to capture about 60 percent of the variance in this dataset. Note that two eigenvectors
    have been chosen for the purpose of illustration, since we are going to plot the
    data via a two-dimensional scatterplot later in this subsection. In practice,
    the number of principal components has to be determined by a tradeoff between
    computational efficiency and the performance of the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By executing the preceding code, we have created a 13×2-dimensional projection
    matrix, **W**, from the top two eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mirrored projections**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on which versions of NumPy and LAPACK you are using, you may obtain
    the matrix, **W**, with its signs flipped. Please note that this is not an issue;
    if **v** is an eigenvector of a matrix, ![](img/B17582_05_016.png), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, **v** is the eigenvector, and –**v** is also an eigenvector, which we
    can show as follows. Using basic algebra, we can multiply both sides of the equation
    by a scalar, ![](img/B17582_05_018.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since matrix multiplication is associative for scalar multiplication, we can
    then rearrange this to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see that ![](img/B17582_05_021.png) is an eigenvector with the same
    eigenvalue, ![](img/B17582_05_022.png), for both ![](img/B17582_05_023.png) and
    ![](img/B17582_05_024.png). Hence, both **v** and –**v** are eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the projection matrix, we can now transform an example, **x** (represented
    as a 13-dimensional row vector), onto the PCA subspace (the principal components
    one and two) obtaining **x**′, now a two-dimensional example vector consisting
    of two new features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x**′ = **xW**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can transform the entire 124×13-dimensional training dataset
    onto the two principal components by calculating the matrix dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X**′ = **XW**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let’s visualize the transformed Wine training dataset, now stored as
    an 124×2-dimensional matrix, in a two-dimensional scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in *Figure 5.3*, the data is more spread along the first principal
    component (*x* axis) than the second principal component (*y* axis), which is
    consistent with the explained variance ratio plot that we created in the previous
    subsection. However, we can tell that a linear classifier will likely be able
    to separate the classes well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Data records from the Wine dataset projected onto a 2D feature
    space via PCA'
  prefs: []
  type: TYPE_NORMAL
- en: Although we encoded the class label information for the purpose of illustration
    in the preceding scatterplot, we have to keep in mind that PCA is an unsupervised
    technique that doesn’t use any class label information.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis in scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the verbose approach in the previous subsection helped us to follow
    the inner workings of PCA, we will now discuss how to use the `PCA` class implemented
    in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PCA` class is another one of scikit-learn’s transformer classes, with
    which we first fit the model using the training data before we transform both
    the training data and the test dataset using the same model parameters. Now, let’s
    use the `PCA` class from scikit-learn on the Wine training dataset, classify the
    transformed examples via logistic regression, and visualize the decision regions
    via the `plot_decision_regions` function that we defined in *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For your convenience, you can place the preceding `plot_decision_regions` code
    into a separate code file in your current working directory, for example, `plot_decision_regions_script.py`,
    and import it into your current Python session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing this code, we should now see the decision regions for the training
    data reduced to two principal component axes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Training examples and logistic regression decision regions after
    using scikit-learn’s PCA for dimensionality reduction'
  prefs: []
  type: TYPE_NORMAL
- en: When we compare the PCA projections via scikit-learn with our own PCA implementation,
    we might see that the resulting plots are mirror images of each other. Note that
    this is not due to an error in either of those two implementations; the reason
    for this difference is that, depending on the eigensolver, eigenvectors can have
    either negative or positive signs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not that it matters, but we could simply revert the mirror image by multiplying
    the data by –1 if we wanted to; note that eigenvectors are typically scaled to
    unit length 1\. For the sake of completeness, let’s plot the decision regions
    of the logistic regression on the transformed test dataset to see if it can separate
    the classes well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After we plot the decision regions for the test dataset by executing the preceding
    code, we can see that logistic regression performs quite well on this small two-dimensional
    feature subspace and only misclassifies a few examples in the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Test datapoints with logistic regression decision regions in the
    PCA-based feature space'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are interested in the explained variance ratios of the different principal
    components, we can simply initialize the `PCA` class with the `n_components` parameter
    set to `None`, so all principal components are kept and the explained variance
    ratio can then be accessed via the `explained_variance_ratio_` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that we set `n_components=None` when we initialized the `PCA` class so
    that it will return all principal components in a sorted order, instead of performing
    a dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing feature contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will take a brief look at how we can assess the contributions
    of the original features to the principal components. As we learned, via PCA,
    we create principal components that represent linear combinations of the features.
    Sometimes, we are interested to know about how much each original feature contributes
    to a given principal component. These contributions are often called **loadings**.
  prefs: []
  type: TYPE_NORMAL
- en: The factor loadings can be computed by scaling the eigenvectors by the square
    root of the eigenvalues. The resulting values can then be interpreted as the correlation
    between the original features and the principal component. To illustrate this,
    let us plot the loadings for the first principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the 13×13-dimensional loadings matrix by multiplying the
    eigenvectors by the square root of the eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we plot the loadings for the first principal component, `loadings[:,
    0]`, which is the first column in this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 5.6*, we can see that, for example, **Alcohol** has a negative correlation
    with the first principal component (approximately –0.3), whereas **Malic acid**
    has a positive correlation (approximately 0.54). Note that a value of 1 describes
    a perfect positive correlation whereas a value of –1 corresponds to a perfect
    negative correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Feature correlations with the first principal component'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code example, we compute the factor loadings for our own PCA
    implementation. We can obtain the loadings from a fitted scikit-learn PCA object
    in a similar manner, where `pca.components_` represents the eigenvectors and `pca.explained_variance_`
    represents the eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the scikit-learn PCA loadings with those we created previously,
    let us create a similar bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the bar plots look the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Feature correlations to the first principal component using scikit-learn'
  prefs: []
  type: TYPE_NORMAL
- en: After exploring PCA as an unsupervised feature extraction technique, the next
    section will introduce **linear discriminant analysis** (**LDA**), which is a
    linear transformation technique that takes class label information into account.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised data compression via linear discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA can be used as a technique for feature extraction to increase computational
    efficiency and reduce the degree of overfitting due to the curse of dimensionality
    in non-regularized models. The general concept behind LDA is very similar to PCA,
    but whereas PCA attempts to find the orthogonal component axes of maximum variance
    in a dataset, the goal in LDA is to find the feature subspace that optimizes class
    separability. In the following sections, we will discuss the similarities between
    LDA and PCA in more detail and walk through the LDA approach step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis versus linear discriminant analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both PCA and LDA are *linear transformation techniques* that can be used to
    reduce the number of dimensions in a dataset; the former is an unsupervised algorithm,
    whereas the latter is supervised. Thus, we might think that LDA is a superior
    feature extraction technique for classification tasks compared to PCA. However,
    A.M. Martinez reported that preprocessing via PCA tends to result in better classification
    results in an image recognition task in certain cases, for instance, if each class
    consists of only a small number of examples (*PCA Versus LDA* by *A. M. Martinez*
    and *A. C. Kak*, *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    23(2): 228-233, 2001).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fisher LDA**'
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA is sometimes also called **Fisher’s LDA**. Ronald A. Fisher initially formulated
    *Fisher’s Linear Discriminant* for two-class classification problems in 1936 (*The
    Use of Multiple Measurements in Taxonomic Problems*, *R. A. Fisher*, *Annals of
    Eugenics*, 7(2): 179-188, 1936). In 1948, Fisher’s linear discriminant was generalized
    for multiclass problems by C. Radhakrishna Rao under the assumption of equal class
    covariances and normally distributed classes, which we now call LDA (*The Utilization
    of Multiple Measurements in Problems of Biological Classification* by *C. R. Rao*,
    *Journal of the Royal Statistical Society*. Series B (Methodological), 10(2):
    159-203, 1948).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.8* summarizes the concept of LDA for a two-class problem. Examples
    from class 1 are shown as circles, and examples from class 2 are shown as crosses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: The concept of LDA for a two-class problem'
  prefs: []
  type: TYPE_NORMAL
- en: A linear discriminant, as shown on the *x* axis (*LD 1*), would separate the
    two normal distributed classes well. Although the exemplary linear discriminant
    shown on the *y* axis (*LD 2*) captures a lot of the variance in the dataset,
    it would fail as a good linear discriminant since it does not capture any of the
    class-discriminatory information.
  prefs: []
  type: TYPE_NORMAL
- en: One assumption in LDA is that the data is normally distributed. Also, we assume
    that the classes have identical covariance matrices and that the training examples
    are statistically independent of each other. However, even if one, or more, of
    those assumptions is (slightly) violated, LDA for dimensionality reduction can
    still work reasonably well (*Pattern Classification 2nd Edition* by *R. O. Duda*,
    *P. E. Hart*, and *D. G. Stork*, *New York*, 2001).
  prefs: []
  type: TYPE_NORMAL
- en: The inner workings of linear discriminant analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into the code implementation, let’s briefly summarize the main
    steps that are required to perform LDA:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the *d*-dimensional dataset (*d* is the number of features).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each class, compute the *d*-dimensional mean vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the between-class scatter matrix, **S**[B], and the within-class scatter
    matrix, **S**[W].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvectors and corresponding eigenvalues of the matrix, ![](img/B17582_05_025.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the *k* eigenvectors that correspond to the *k* largest eigenvalues to
    construct a *d*×*k*-dimensional transformation matrix, **W**; the eigenvectors
    are the columns of this matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the examples onto the new feature subspace using the transformation
    matrix, **W**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, LDA is quite similar to PCA in the sense that we are decomposing
    matrices into eigenvalues and eigenvectors, which will form the new lower-dimensional
    feature space. However, as mentioned before, LDA takes class label information
    into account, which is represented in the form of the mean vectors computed in
    *step 2*. In the following sections, we will discuss these seven steps in more
    detail, accompanied by illustrative code implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the scatter matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we already standardized the features of the Wine dataset in the PCA section
    at the beginning of this chapter, we can skip the first step and proceed with
    the calculation of the mean vectors, which we will use to construct the within-class
    scatter matrix and between-class scatter matrix, respectively. Each mean vector,
    **m**[i], stores the mean feature value, ![](img/B17582_05_026.png), with respect
    to the examples of class *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This results in three mean vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These mean vectors can be computed by the following code, where we compute
    one mean vector for each of the three labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the mean vectors, we can now compute the within-class scatter matrix,
    **S**[W]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is calculated by summing up the individual scatter matrices, **S**[i],
    of each individual class *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The assumption that we are making when we are computing the scatter matrices
    is that the class labels in the training dataset are uniformly distributed. However,
    if we print the number of class labels, we see that this assumption is violated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we want to scale the individual scatter matrices, **S**[i], before we
    sum them up as the scatter matrix, **S**[W]. When we divide the scatter matrices
    by the number of class-examples, *n*[i], we can see that computing the scatter
    matrix is in fact the same as computing the covariance matrix, ![](img/B17582_05_031.png)—the
    covariance matrix is a normalized version of the scatter matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for computing the scaled within-class scatter matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After we compute the scaled within-class scatter matrix (or covariance matrix),
    we can move on to the next step and compute the between-class scatter matrix **S**[B]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, **m** is the overall mean that is computed, including examples from all
    **c** classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Selecting linear discriminants for the new feature subspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The remaining steps of the LDA are similar to the steps of the PCA. However,
    instead of performing the eigendecomposition on the covariance matrix, we solve
    the generalized eigenvalue problem of the matrix, ![](img/B17582_05_034.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After we compute the eigenpairs, we can sort the eigenvalues in descending
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In LDA, the number of linear discriminants is at most *c* – 1, where *c* is
    the number of class labels, since the in-between scatter matrix, **S**[B], is
    the sum of *c* matrices with rank one or less. We can indeed see that we only
    have two nonzero eigenvalues (the eigenvalues 3-13 are not exactly zero, but this
    is due to the floating-point arithmetic in NumPy.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Collinearity**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the rare case of perfect collinearity (all aligned example points
    fall on a straight line), the covariance matrix would have rank one, which would
    result in only one eigenvector with a nonzero eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure how much of the class-discriminatory information is captured by
    the linear discriminants (eigenvectors), let’s plot the linear discriminants by
    decreasing eigenvalues, similar to the explained variance plot that we created
    in the PCA section. For simplicity, we will call the content of class-discriminatory
    information **discriminability**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in *Figure 5.9*, the first two linear discriminants alone capture
    100 percent of the useful information in the Wine training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing chart  Description automatically generated](img/B17582_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: The top two discriminants capture 100 percent of the useful information'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now stack the two most discriminative eigenvector columns to create the
    transformation matrix, **W**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Projecting examples onto the new feature space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the transformation matrix **W** that we created in the previous subsection,
    we can now transform the training dataset by multiplying the matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X**′ = **XW**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in *Figure 5.10*, the three Wine classes are now perfectly linearly
    separable in the new feature subspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Wine classes perfectly separable after projecting the data onto
    the first two discriminants'
  prefs: []
  type: TYPE_NORMAL
- en: LDA via scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'That step-by-step implementation was a good exercise to understand the inner
    workings of LDA and understand the differences between LDA and PCA. Now, let’s
    look at the `LDA` class implemented in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s see how the logistic regression classifier handles the lower-dimensional
    training dataset after the LDA transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at *Figure 5.11*, we can see that the logistic regression model misclassifies
    one of the examples from class 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: The logistic regression model misclassifies one of the classes'
  prefs: []
  type: TYPE_NORMAL
- en: 'By lowering the regularization strength, we could probably shift the decision
    boundaries so that the logistic regression model classifies all examples in the
    training dataset correctly. However, and more importantly, let’s take a look at
    the results on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in *Figure 5.12*, the logistic regression classifier is able
    to get a perfect accuracy score for classifying the examples in the test dataset
    by only using a two-dimensional feature subspace, instead of the original 13 Wine
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: The logistic regression model works perfectly on the test data'
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear dimensionality reduction and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we covered linear transformation techniques, such as
    PCA and LDA, for feature extraction. In this section, we will discuss why considering
    nonlinear dimensionality reduction techniques might be worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: One nonlinear dimensionality reduction technique that is particularly worth
    highlighting is **t-distributed stochastic neighbor embedding** (**t-SNE**) since
    it is frequently used in literature to visualize high-dimensional datasets in
    two or three dimensions. We will see how we can apply t-SNE to plot images of
    handwritten images in a 2-dimensional feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Why consider nonlinear dimensionality reduction?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many machine learning algorithms make assumptions about the linear separability
    of the input data. You have learned that the perceptron even requires perfectly
    linearly separable training data to converge. Other algorithms that we have covered
    so far assume that the lack of perfect linear separability is due to noise: Adaline,
    logistic regression, and the (standard) SVM to just name a few.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we are dealing with nonlinear problems, which we may encounter
    rather frequently in real-world applications, linear transformation techniques
    for dimensionality reduction, such as PCA and LDA, may not be the best choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: The difference between linear and nonlinear problems'
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn library implements a selection of advanced techniques for nonlinear
    dimensionality reduction that are beyond the scope of this book. The interested
    reader can find a nice overview of the current implementations in scikit-learn,
    complemented by illustrative examples, at [http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html).
  prefs: []
  type: TYPE_NORMAL
- en: The development and application of nonlinear dimensionality reduction techniques
    is also often referred to as manifold learning, where a manifold refers to a lower
    dimensional topological space embedded in a high-dimensional space. Algorithms
    for manifold learning have to capture the complicated structure of the data in
    order to project it onto a lower-dimensional space where the relationship between
    data points is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classic example of manifold learning is the 3-dimensional Swiss roll illustrated
    in *Figure 5.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B17582_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Three-dimensional Swiss roll projected into a lower, two-dimensional
    space'
  prefs: []
  type: TYPE_NORMAL
- en: While nonlinear dimensionality reduction and manifold learning algorithms are
    very powerful, we should note that these techniques are notoriously hard to use,
    and with non-ideal hyperparameter choices, they may cause more harm than good.
    The reason behind this difficulty is that we are often working with high-dimensional
    datasets that we cannot readily visualize and where the structure is not obvious
    (unlike the Swiss roll example in *Figure 5.14*). Moreover, unless we project
    the dataset into two or three dimensions (which is often not sufficient for capturing
    more complicated relationships), it is hard or even impossible to assess the quality
    of the results. Hence, many people still rely on simpler techniques such as PCA
    and LDA for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data via t-distributed stochastic neighbor embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After introducing nonlinear dimensionality reduction and discussing some of
    its challenges, let’s take a look at a hands-on example involving t-SNE, which
    is often used for visualizing complex datasets in two or three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, t-SNE is modeling data points based on their pair-wise distances
    in the high-dimensional (original) feature space. Then, it finds a probability
    distribution of pair-wise distances in the new, lower-dimensional space that is
    close to the probability distribution of pair-wise distances in the original space.
    Or, in other words, t-SNE learns to embed data points into a lower-dimensional
    space such that the pairwise distances in the original space are preserved. You
    can find more details about this method in the original research paper *Visualizing
    data using t-SNE* by *Maaten and Hinton, Journal of Machine Learning Research*,
    2018 ([https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)).
    However, as the research paper title suggests, t-SNE is a technique intended for
    visualization purposes as it requires the whole dataset for the projection. Since
    it projects the points directly (unlike PCA, it does not involve a projection
    matrix), we cannot apply t-SNE to new data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows a quick demonstration of how t-SNE can be applied
    to a 64-dimensional dataset. First, we load the Digits dataset from scikit-learn,
    which consists of low-resolution handwritten digits (the numbers 0-9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The digits are 8×8 grayscale images. The following code plots the first four
    images in the dataset, which consists of 1,797 images in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in *Figure 5.15*, the images are relatively low resolution, 8×8
    pixels (that is, 64 pixels per image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart  Description automatically generated](img/B17582_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Low resolution images of handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `digits.data` attribute lets us access a tabular version of this
    dataset where the examples are represented by the rows, and the columns correspond
    to the pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let us assign the features (pixels) to a new variable `X_digits` and
    the labels to another new variable `y_digits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we import the t-SNE class from scikit-learn and fit a new `tsne` object.
    Using `fit_transform`, we perform the t-SNE fitting and data transformation in
    one step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using this code, we projected the 64-dimensional dataset onto a 2-dimensional
    space. We specified `init='pca'`, which initializes the t-SNE embedding using
    PCA as it is recommended in the research article *Initialization is critical for
    preserving global data structure in both t-SNE and UMAP* by *Kobak* and *Linderman*,
    *Nature Biotechnology Volume 39*, pages 156–157, 2021 ([https://www.nature.com/articles/s41587-020-00809-z](https://www.nature.com/articles/s41587-020-00809-z)).
  prefs: []
  type: TYPE_NORMAL
- en: Note that t-SNE includes additional hyperparameters such as the perplexity and
    learning rate (often called **epsilon**), which we omitted in the example (we
    used the scikit-learn default values). In practice, we recommend you explore these
    parameters as well. More information about these parameters and their effects
    on the results can be found in the excellent article *How to Use t-SNE Effectively*
    by *Wattenberg*, *Viegas*, and *Johnson*, *Distill*, 2016 ([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let us visualize the 2D t-SNE embeddings using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Like PCA, t-SNE is an unsupervised method, and in the preceding code, we use
    the class labels `y_digits` (0-9) only for visualization purposes via the functions
    color argument. Matplotlib’s `PathEffects` are used for visual purposes, such
    that the class label is displayed in the center (via `np.median`) of data points
    belonging to each respective digit. The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter chart  Description automatically generated](img/B17582_05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: A visualization of how t-SNE embeds the handwritten digits in
    a 2D feature space'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, t-SNE is able to separate the different digits (classes) nicely,
    although not perfectly. It might be possible to achieve better separation by tuning
    the hyperparameters. However, a certain degree of class mixing might be unavoidable
    due to illegible handwriting. For instance, by inspecting individual images, we
    might find that certain instances of the number 3 indeed look like the number
    9, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform manifold approximation and projection**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular visualization technique is **uniform manifold approximation
    and projection** (**UMAP**). While UMAP can produce similarly good results as
    t-SNE (for example, see the Kobak and Linderman paper referenced previously),
    it is typically faster, and it can also be used to project new data, which makes
    it more attractive as a dimensionality reduction technique in a machine learning
    context, similar to PCA. Interested readers can find more information about UMAP
    in the original paper: *UMAP: Uniform manifold approximation and projection for
    dimension reduction* by *McInnes, Healy*, and *Melville*,2018([https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426)).A
    scikit-learn compatible implementation of UMAP can be found at [https://umap-learn.readthedocs.io](https://umap-learn.readthedocs.io).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about two fundamental dimensionality reduction
    techniques for feature extraction: PCA and LDA. Using PCA, we projected data onto
    a lower-dimensional subspace to maximize the variance along the orthogonal feature
    axes, while ignoring the class labels. LDA, in contrast to PCA, is a technique
    for supervised dimensionality reduction, which means that it considers class information
    in the training dataset to attempt to maximize the class separability in a linear
    feature space. Lastly, you also learned about t-SNE, which is a nonlinear feature
    extraction technique that can be used for visualizing data in two or three dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: Equipped with PCA and LDA as fundamental data preprocessing techniques, you
    are now well prepared to learn about the best practices for efficiently incorporating
    different preprocessing techniques and evaluating the performance of different
    models in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
