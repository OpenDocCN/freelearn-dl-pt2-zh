- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifying Images with Deep Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked in depth at different aspects of the PyTorch
    neural network and automatic differentiation modules, you became familiar with
    tensors and decorating functions, and you learned how to work with `torch.nn`.
    In this chapter, you will now learn about **convolutional neural networks** (**CNNs**)
    for image classification. We will start by discussing the basic building blocks
    of CNNs, using a bottom-up approach. Then, we will take a deeper dive into the
    CNN architecture and explore how to implement CNNs in PyTorch. In this chapter,
    we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution operations in one and two dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The building blocks of CNN architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing deep CNNs in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation techniques for improving the generalization performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a facial CNN classifier for recognizing if someone is smiling or
    not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The building blocks of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are a family of models that were originally inspired by how the visual
    cortex of the human brain works when recognizing objects. The development of CNNs
    goes back to the 1990s, when Yann LeCun and his colleagues proposed a novel NN
    architecture for classifying handwritten digits from images (*Handwritten Digit
    Recognition with a Back-Propagation Network* by *Y. LeCun*, and colleagues, 1989,
    published at the *Neural Information Processing Systems (NeurIPS)* conference).
  prefs: []
  type: TYPE_NORMAL
- en: '**The human visual cortex**'
  prefs: []
  type: TYPE_NORMAL
- en: The original discovery of how the visual cortex of our brain functions was made
    by David H. Hubel and Torsten Wiesel in 1959, when they inserted a microelectrode
    into the primary visual cortex of an anesthetized cat. They observed that neurons
    respond differently after projecting different patterns of light in front of the
    cat. This eventually led to the discovery of the different layers of the visual
    cortex. While the primary layer mainly detects edges and straight lines, higher-order
    layers focus more on extracting complex shapes and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the outstanding performance of CNNs for image classification tasks, this
    particular type of feedforward NN gained a lot of attention and led to tremendous
    improvements in machine learning for computer vision. Several years later, in
    2019, Yann LeCun received the Turing award (the most prestigious award in computer
    science) for his contributions to the field of **artificial intelligence** (**AI**),
    along with two other researchers, Yoshua Bengio and Geoffrey Hinton, whose names
    you encountered in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss the broader concepts of CNNs and
    why convolutional architectures are often described as “feature extraction layers.”
    Then, we will delve into the theoretical definition of the type of convolution
    operation that is commonly used in CNNs and walk through examples of computing
    convolutions in one and two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs and feature hierarchies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Successfully extracting **salient** (**relevant**) **features** is key to the
    performance of any machine learning algorithm, and traditional machine learning
    models rely on input features that may come from a domain expert or are based
    on computational feature extraction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Certain types of NNs, such as CNNs, can automatically learn the features from
    raw data that are most useful for a particular task. For this reason, it’s common
    to consider CNN layers as feature extractors: the early layers (those right after
    the input layer) extract **low-level features** from raw data, and the later layers
    (often **fully connected layers**, as in a **multilayer perceptron** (**MLP**))
    use these features to predict a continuous target value or class label.'
  prefs: []
  type: TYPE_NORMAL
- en: Certain types of multilayer NNs, and in particular, deep CNNs, construct a so-called
    **feature hierarchy** by combining the low-level features in a layer-wise fashion
    to form high-level features. For example, if we’re dealing with images, then low-level
    features, such as edges and blobs, are extracted from the earlier layers, which
    are combined to form high-level features. These high-level features can form more
    complex shapes, such as the general contours of objects like buildings, cats,
    or dogs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 14.1*, a CNN computes **feature maps** from an input
    image, where each element comes from a local patch of pixels in the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Creating feature maps from an image (photo by Alexander Dummer
    on Unsplash)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This local patch of pixels is referred to as the **local receptive field**.
    CNNs will usually perform very well on image-related tasks, and that’s largely
    due to two important ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse connectivity**: A single element in the feature map is connected to
    only a small patch of pixels. (This is very different from connecting to the whole
    input image, as in the case of MLPs. You may find it useful to look back and compare
    how we implemented a fully connected network that connected to the whole image
    in *Chapter 11*, *Implementing a Multilayer Artificial Neural Network from Scratch*.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter sharing**: The same weights are used for different patches of the
    input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a direct consequence of these two ideas, replacing a conventional, fully
    connected MLP with a convolution layer substantially decreases the number of weights
    (parameters) in the network, and we will see an improvement in the ability to
    capture *salient* features. In the context of image data, it makes sense to assume
    that nearby pixels are typically more relevant to each other than pixels that
    are far away from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, CNNs are composed of several **convolutional** and subsampling layers
    that are followed by one or more fully connected layers at the end. The fully
    connected layers are essentially an MLP, where every input unit, *i*, is connected
    to every output unit, *j*, with weight *w*[ij] (which we covered in more detail
    in *Chapter 11*).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that subsampling layers, commonly known as **pooling layers**, do
    not have any learnable parameters; for instance, there are no weights or bias
    units in pooling layers. However, both the convolutional and fully connected layers
    have weights and biases that are optimized during training.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will study convolutional and pooling layers in
    more detail and see how they work. To understand how convolution operations work,
    let’s start with a convolution in one dimension, which is sometimes used for working
    with certain types of sequence data, such as text. After discussing one-dimensional
    convolutions, we will work through the typical two-dimensional ones that are commonly
    applied to two-dimensional images.
  prefs: []
  type: TYPE_NORMAL
- en: Performing discrete convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **discrete convolution** (or simply **convolution**) is a fundamental operation
    in a CNN. Therefore, it’s important to understand how this operation works. In
    this section, we will cover the mathematical definition and discuss some of the
    **naive** algorithms to compute convolutions of one-dimensional tensors (vectors)
    and two-dimensional tensors (matrices).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the formulas and descriptions in this section are solely for
    understanding how convolution operations in CNNs work. Indeed, much more efficient
    implementations of convolutional operations already exist in packages such as
    PyTorch, as you will see later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mathematical notation**'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use subscript to denote the size of a multidimensional
    array (tensor); for example, ![](img/B17582_14_001.png) is a two-dimensional array
    of size *n*[1]×*n*[2]. We use brackets, [ ], to denote the indexing of a multidimensional
    array. For example, *A*[*i*, *j*] refers to the element at index *i*, *j* of matrix
    *A*. Furthermore, note that we use a special symbol, ![](img/B17582_14_002.png),
    to denote the convolution operation between two vectors or matrices, which is
    not to be confused with the multiplication operator, `*`, in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete convolutions in one dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with some basic definitions and notations that we are going to
    use. A discrete convolution for two vectors, **x** and **w**, is denoted by ![](img/B17582_14_003.png),
    in which vector **x** is our input (sometimes called **signal**) and **w** is
    called the **filter** or **kernel**. A discrete convolution is mathematically
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned earlier, the brackets, [ ], are used to denote the indexing for
    vector elements. The index, *i*, runs through each element of the output vector,
    **y**. There are two odd things in the preceding formula that we need to clarify:
    –∞ to +∞ indices and negative indexing for **x**.'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the sum runs through indices from –∞ to +∞ seems odd, mainly because
    in machine learning applications, we always deal with finite feature vectors.
    For example, if **x** has 10 features with indices 0, 1, 2, ..., 8, 9, then indices
    –∞: –1 and 10: +∞ are out of bounds for **x**. Therefore, to correctly compute
    the summation shown in the preceding formula, it is assumed that **x** and **w**
    are filled with zeros. This will result in an output vector, **y**, that also
    has infinite size, with lots of zeros as well. Since this is not useful in practical
    situations, **x** is padded only with a finite number of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is called **zero-padding** or simply **padding**. Here, the number
    of zeros padded on each side is denoted by *p*. An example padding of a one-dimensional
    vector, **x**, is shown in *Figure 14.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: An example of padding'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that the original input, **x**, and filter, **w**, have *n* and
    *m* elements, respectively, where ![](img/B17582_14_005.png). Therefore, the padded
    vector, *x*^p, has size *n* + 2*p*. The practical formula for computing a discrete
    convolution will change to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have solved the infinite index issue, the second issue is indexing
    **x** with *i* + *m* – *k*. The important point to notice here is that **x** and
    **w** are indexed in different directions in this summation. Computing the sum
    with one index going in the reverse direction is equivalent to computing the sum
    with both indices in the forward direction after flipping one of those vectors,
    **x** or **w**, after they are padded. Then, we can simply compute their dot product.
    Let’s assume we flip (rotate) the filter, **w**, to get the rotated filter, **w**^r.
    Then, the dot product, *x*[*i*: *i* + *m*].**w**^r, is computed to get one element,
    *y*[*i*], where **x**[*i*: *i* + *m*] is a patch of **x** with size *m*. This
    operation is repeated like in a sliding window approach to get all the output
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure provides an example with **x** = [3 2 1 7 1 2 5 4] and
    ![](img/B17582_14_007.png) so that the first three output elements are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: The steps for computing a discrete convolution'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the preceding example that the padding size is zero (*p* =  0).
    Notice that the rotated filter, **w**^r, is shifted by two cells each time we
    **shift**. This shift is another hyperparameter of a convolution, the **stride**,
    *s*. In this example, the stride is two, *s* = 2\. Note that the stride has to
    be a positive number smaller than the size of the input vector. We will talk more
    about padding and strides in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-correlation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-correlation (or simply correlation) between an input vector and a filter
    is denoted by ![](img/B17582_14_008.png) and is very much like a sibling of a
    convolution, with a small difference: in cross-correlation, the multiplication
    is performed in the same direction. Therefore, it is not a requirement to rotate
    the filter matrix, *w*, in each dimension. Mathematically, cross-correlation is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_009.png)'
  prefs: []
  type: TYPE_IMG
- en: The same rules for padding and stride may be applied to cross-correlation as
    well. Note that most deep learning frameworks (including PyTorch) implement cross-correlation
    but refer to it as convolution, which is a common convention in the deep learning
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Padding inputs to control the size of the output feature maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve only used zero-padding in convolutions to compute finite-sized
    output vectors. Technically, padding can be applied with any ![](img/B17582_14_010.png).
    Depending on the choice of *p*, boundary cells may be treated differently than
    the cells located in the middle of **x**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider an example where *n* = 5 and *m* = 3\. Then, with *p* = 0, *x*[0]
    is only used in computing one output element (for instance, *y*[0]), while *x*[1]
    is used in the computation of two output elements (for instance, *y*[0] and *y*[1]).
    So, you can see that this different treatment of elements of **x** can artificially
    put more emphasis on the middle element, *x*[2], since it has appeared in most
    computations. We can avoid this issue if we choose *p* = 2, in which case, each
    element of **x** will be involved in computing three elements of **y**.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the size of the output, **y**, also depends on the choice of the
    padding strategy we use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three modes of padding that are commonly used in practice: *full*,
    *same*, and *valid*.'
  prefs: []
  type: TYPE_NORMAL
- en: In full mode, the padding parameter, *p*, is set to *p* = *m* – 1\. Full padding
    increases the dimensions of the output; thus, it is rarely used in CNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The same padding mode is usually used to ensure that the output vector has the
    same size as the input vector, **x**. In this case, the padding parameter, *p*,
    is computed according to the filter size, along with the requirement that the
    input size and output size are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, computing a convolution in valid mode refers to the case where *p* = 0
    (no padding).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.4* illustrates the three different padding modes for a simple 5×5 pixel
    input with a kernel size of 3×3 and a stride of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: The three modes of padding'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used padding mode in CNNs is same padding. One of its advantages
    over the other padding modes is that same padding preserves the size of the vector—or
    the height and width of the input images when we are working on image-related
    tasks in computer vision—which makes designing a network architecture more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: 'One big disadvantage of valid padding versus full and same padding is that
    the volume of the tensors will decrease substantially in NNs with many layers,
    which can be detrimental to the network’s performance. In practice, you should
    preserve the spatial size using same padding for the convolutional layers and
    decrease the spatial size via pooling layers or convolutional layers with stride
    2 instead, as described in *Striving for Simplicity: The All Convolutional Net*
    ICLR (workshop track), by *Jost Tobias Springenberg*, *Alexey Dosovitskiy*, and
    others, 2015 ([https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)).'
  prefs: []
  type: TYPE_NORMAL
- en: As for full padding, its size results in an output larger than the input size.
    Full padding is usually used in signal processing applications where it is important
    to minimize boundary effects. However, in a deep learning context, boundary effects
    are usually not an issue, so we rarely see full padding being used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the size of the convolution output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output size of a convolution is determined by the total number of times
    that we shift the filter, **w**, along the input vector. Let’s assume that the
    input vector is of size *n* and the filter is of size *m*. Then, the size of the
    output resulting from ![](img/B17582_14_011.png), with padding *p* and stride *s*,
    would be determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_012.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17582_14_013.png) denotes the *floor* operation.
  prefs: []
  type: TYPE_NORMAL
- en: '**The floor operation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The floor operation returns the largest integer that is equal to or smaller
    than the input, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the following two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output size for an input vector of size 10 with a convolution kernel
    of size 5, padding 2, and stride 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_015.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (Note that in this case, the output size turns out to be the same as the input;
    therefore, we can conclude this to be same padding mode.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How does the output size change for the same input vector when we have a kernel
    of size 3 and stride 2?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_016.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: If you are interested in learning more about the size of the convolution output,
    we recommend the manuscript *A guide to convolution arithmetic for deep learning*
    by *Vincent Dumoulin* and *Francesco Visin*, which is freely available at [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in order to learn how to compute convolutions in one dimension, a
    naive implementation is shown in the following code block, and the results are
    compared with the `numpy.convolve` function. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have mostly focused on convolutions for vectors (1D convolutions).
    We started with the 1D case to make the concepts easier to understand. In the
    next section, we will cover 2D convolutions in more detail, which are the building
    blocks of CNNs for image-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a discrete convolution in 2D
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concepts you learned in the previous sections are easily extendible to
    2D. When we deal with 2D inputs, such as a matrix, ![](img/B17582_14_017.png),
    and the filter matrix, ![](img/B17582_14_018.png), where ![](img/B17582_14_019.png)
    and ![](img/B17582_14_020.png), then the matrix ![](img/B17582_14_021.png) is
    the result of a 2D convolution between **X** and **W**. This is defined mathematically
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that if you omit one of the dimensions, the remaining formula is exactly
    the same as the one we used previously to compute the convolution in 1D. In fact,
    all the previously mentioned techniques, such as zero padding, rotating the filter
    matrix, and the use of strides, are also applicable to 2D convolutions, provided
    that they are extended to both dimensions independently. *Figure 14.5* demonstrates
    the 2D convolution of an input matrix of size 8×8, using a kernel of size 3×3\.
    The input matrix is padded with zeros with *p* = 1\. As a result, the output of
    the 2D convolution will have a size of 8×8:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17582_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: The output of a 2D convolution'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates the computation of a 2D convolution between
    an input matrix, **X**[3×3], and a kernel matrix, **W**[3×3], using padding *p* = (1, 1)
    and stride *s* = (2, 2). According to the specified padding, one layer of zeros
    is added on each side of the input matrix, which results in the padded matrix
    ![](img/B17582_14_023.png), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Computing a 2D convolution between an input and kernel matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the preceding filter, the rotated filter will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that this rotation is not the same as the transpose matrix. To get the
    rotated filter in NumPy, we can write `W_rot=W[::-1,::-1]`. Next, we can shift
    the rotated filter matrix along the padded input matrix, **X**^(padded), like
    a sliding window, and compute the sum of the element-wise product, which is denoted
    by the ![](img/B17582_14_025.png) operator in *Figure 14.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Computing the sum of the element-wise product'
  prefs: []
  type: TYPE_NORMAL
- en: The result will be the 2×2 matrix, **Y**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also implement the 2D convolution according to the *naive* algorithm
    described. The `scipy.signal` package provides a way to compute 2D convolution
    via the `scipy.signal.convolve2d` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Efficient algorithms for computing convolution**'
  prefs: []
  type: TYPE_NORMAL
- en: We provided a naive implementation to compute a 2D convolution for the purpose
    of understanding the concepts. However, this implementation is very inefficient
    in terms of memory requirements and computational complexity. Therefore, it should
    not be used in real-world NN applications.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect is that the filter matrix is actually not rotated in most tools like
    PyTorch. Moreover, in recent years, much more efficient algorithms have been developed
    that use the Fourier transform to compute convolutions. It is also important to
    note that in the context of NNs, the size of a convolution kernel is usually much
    smaller than the size of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: For example, modern CNNs usually use kernel sizes such as 1×1, 3×3, or 5×5,
    for which efficient algorithms have been designed that can carry out the convolutional
    operations much more efficiently, such as Winograd’s minimal filtering algorithm.
    These algorithms are beyond the scope of this book, but if you are interested
    in learning more, you can read the manuscript *Fast Algorithms for Convolutional
    Neural Networks* by *Andrew Lavin* and *Scott Gray*, 2015, which is freely available
    at [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss subsampling or pooling, which is another
    important operation often used in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Subsampling is typically applied in two forms of pooling operations in CNNs:
    **max-pooling** and **mean-pooling** (also known as **average-pooling**). The
    pooling layer is usually denoted by ![](img/B17582_14_026.png). Here, the subscript
    determines the size of the neighborhood (the number of adjacent pixels in each
    dimension) where the max or mean operation is performed. We refer to such a neighborhood
    as the **pooling size**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The operation is described in *Figure 14.8*. Here, max-pooling takes the maximum
    value from a neighborhood of pixels, and mean-pooling computes their average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: An example of max-pooling and mean-pooling'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of pooling is twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling (max-pooling) introduces a local invariance. This means that small
    changes in a local neighborhood do not change the result of max-pooling. Therefore,
    it helps with generating features that are more robust to noise in the input data.
    Refer to the following example, which shows that the max-pooling of two different
    input matrices, **X**[1] and **X**[2], results in the same output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_027.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Pooling decreases the size of features, which results in higher computational
    efficiency. Furthermore, reducing the number of features may reduce the degree
    of overfitting as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overlapping versus non-overlapping pooling**'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, pooling is assumed to be non-overlapping. Pooling is typically
    performed on non-overlapping neighborhoods, which can be done by setting the stride
    parameter equal to the pooling size. For example, a non-overlapping pooling layer,
    ![](img/B17582_14_028.png), requires a stride parameter *s* = (*n*[1], *n*[2]).
    On the other hand, overlapping pooling occurs if the stride is smaller than the
    pooling size. An example where overlapping pooling is used in a convolutional
    network is described in *ImageNet Classification with Deep Convolutional Neural
    Networks* by *A. Krizhevsky*, *I. Sutskever*, and *G. Hinton*, 2012, which is
    freely available as a manuscript at [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).
  prefs: []
  type: TYPE_NORMAL
- en: While pooling is still an essential part of many CNN architectures, several
    CNN architectures have also been developed without using pooling layers. Instead
    of using pooling layers to reduce the feature size, researchers use convolutional
    layers with a stride of 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a sense, you can think of a convolutional layer with stride 2 as a pooling
    layer with learnable weights. If you are interested in an empirical comparison
    of different CNN architectures developed with and without pooling layers, we recommend
    reading the research article *Striving for Simplicity: The All Convolutional Net*
    by *Jost Tobias Springenberg*, *Alexey Dosovitskiy*, *Thomas Brox*, and *Martin
    Riedmiller*. This article is freely available at [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).'
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together – implementing a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you have learned about the basic building blocks of CNNs. The concepts
    illustrated in this chapter are not really more difficult than traditional multilayer
    NNs. We can say that the most important operation in a traditional NN is matrix
    multiplication. For instance, we use matrix multiplications to compute the pre-activations
    (or net inputs), as in **z** = **Wx** + *b*. Here, **x** is a column vector (![](img/B17582_14_029.png)
    matrix) representing pixels, and **W** is the weight matrix connecting the pixel
    inputs to each hidden unit.
  prefs: []
  type: TYPE_NORMAL
- en: In a CNN, this operation is replaced by a convolution operation, as in ![](img/B17582_14_030.png),
    where **X** is a matrix representing the pixels in a *height*×*width* arrangement.
    In both cases, the pre-activations are passed to an activation function to obtain
    the activation of a hidden unit, ![](img/B17582_14_031.png), where ![](img/B17582_14_032.png)
    is the activation function. Furthermore, you will recall that subsampling is another
    building block of a CNN, which may appear in the form of pooling, as was described
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Working with multiple input or color channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An input to a convolutional layer may contain one or more 2D arrays or matrices
    with dimensions *N*[1]×*N*[2] (for example, the image height and width in pixels).
    These *N*[1]×*N*[2] matrices are called *channels*. Conventional implementations
    of convolutional layers expect a rank-3 tensor representation as an input, for
    example, a three-dimensional array, ![](img/B17582_14_033.png), where *C*[in]
    is the number of input channels. For example, let’s consider images as input to
    the first layer of a CNN. If the image is colored and uses the RGB color mode,
    then *C*[in] = 3 (for the red, green, and blue color channels in RGB). However,
    if the image is in grayscale, then we have *C*[in] = 1, because there is only
    one channel with the grayscale pixel intensity values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading an image file**'
  prefs: []
  type: TYPE_NORMAL
- en: When we work with images, we can read images into NumPy arrays using the `uint8`
    (unsigned 8-bit integer) data type to reduce memory usage compared to 16-bit,
    32-bit, or 64-bit integer types, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Unsigned 8-bit integers take values in the range [0, 255], which are sufficient
    to store the pixel information in RGB images, which also take values in the same
    range.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, you
    saw that PyTorch provides a module for loading/storing and manipulating images
    via `torchvision`. Let’s recap how to read an image (this example RGB image is
    located in the code bundle folder that is provided with this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that with `torchvision`, the input and output image tensors are in the
    format of `Tensor[channels, image_height, image_width]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you are familiar with the structure of input data, the next question
    is, how can we incorporate multiple input channels in the convolution operation
    that we discussed in the previous sections? The answer is very simple: we perform
    the convolution operation for each channel separately and then add the results
    together using the matrix summation. The convolution associated with each channel
    (*c*) has its own kernel matrix as *W*[:, :, *c*].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The total pre-activation result is computed in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final result, **A**, is a feature map. Usually, a convolutional layer of
    a CNN has more than one feature map. If we use multiple feature maps, the kernel
    tensor becomes four-dimensional: *width*×*height*×*C*[in]×*C*[out]. Here, *width*×*height*
    is the kernel size, *C*[in] is the number of input channels, and *C*[out] is the
    number of output feature maps. So, now let’s include the number of output feature
    maps in the preceding formula and update it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To conclude our discussion of computing convolutions in the context of NNs,
    let’s look at the example in *Figure 14.9*, which shows a convolutional layer,
    followed by a pooling layer. In this example, there are three input channels.
    The kernel tensor is four-dimensional. Each kernel matrix is denoted as *m*[1]×*m*[2],
    and there are three of them, one for each input channel. Furthermore, there are
    five such kernels, accounting for five output feature maps. Finally, there is
    a pooling layer for subsampling the feature maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Implementing a CNN'
  prefs: []
  type: TYPE_NORMAL
- en: '**How many trainable parameters exist in the preceding example?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the advantages of convolution, parameter sharing, and sparse
    connectivity, let’s work through an example. The convolutional layer in the network
    shown in *Figure 14.9* is a four-dimensional tensor. So, there are *m*[1]×*m*[2]×3×5
    parameters associated with the kernel. Furthermore, there is a bias vector for
    each output feature map of the convolutional layer. Thus, the size of the bias
    vector is 5\. Pooling layers do not have any (trainable) parameters; therefore,
    we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m*[1] × *m*[2] × 3 × 5 + 5'
  prefs: []
  type: TYPE_NORMAL
- en: If the input tensor is of size *n*[1]×*n*[2]×3, assuming that the convolution
    is performed with the same-padding mode, then the size of the output feature maps
    would be *n*[1] × *n*[2] × 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if we use a fully connected layer instead of a convolutional layer,
    this number will be much larger. In the case of a fully connected layer, the number
    of parameters for the weight matrix to reach the same number of output units would
    have been as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (*n*[1] × *n*[2] × 3) × (*n*[1] × *n*[2] × 5) = (*n*[1] × *n*[2])² × 3 × 5
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the size of the bias vector is *n*[1] × *n*[2] × 5 (one bias element
    for each output unit). Given that *m*[1] < *n*[1] and *m*[2] < *n*[2], we can
    see that the difference in the number of trainable parameters is significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, as was already mentioned, the convolution operations typically are
    carried out by treating an input image with multiple color channels as a stack
    of matrices; that is, we perform the convolution on each matrix separately and
    then add the results, as was illustrated in the previous figure. However, convolutions
    can also be extended to 3D volumes if you are working with 3D datasets, for example,
    as shown in the paper *VoxNet: A 3D Convolutional Neural Network for Real-Time
    Object Recognition* by *Daniel Maturana* and *Sebastian Scherer*, 2015, which
    can be accessed at [https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about how to regularize an NN.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing an NN with L2 regularization and dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the size of a network, whether we are dealing with a traditional (fully
    connected) NN or a CNN, has always been a challenging problem. For instance, the
    size of a weight matrix and the number of layers need to be tuned to achieve a
    reasonably good performance.
  prefs: []
  type: TYPE_NORMAL
- en: You will recall from *Chapter 13*, *Going Deeper – The Mechanics of PyTorch*,
    that a simple network without a hidden layer could only capture a linear decision
    boundary, which is not sufficient for dealing with an exclusive or (or XOR) or
    similar problem. The *capacity* of a network refers to the level of complexity
    of the function that it can learn to approximate. Small networks, or networks
    with a relatively small number of parameters, have a low capacity and are therefore
    likely to *underfit*, resulting in poor performance, since they cannot learn the
    underlying structure of complex datasets. However, very large networks may result
    in *overfitting*, where the network will memorize the training data and do extremely
    well on the training dataset while achieving a poor performance on the held-out
    test dataset. When we deal with real-world machine learning problems, we do not
    know how large the network should be *a priori*.
  prefs: []
  type: TYPE_NORMAL
- en: One way to address this problem is to build a network with a relatively large
    capacity (in practice, we want to choose a capacity that is slightly larger than
    necessary) to do well on the training dataset. Then, to prevent overfitting, we
    can apply one or multiple regularization schemes to achieve good generalization
    performance on new data, such as the held-out test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapters 3* and *4*, we covered L1 and L2 regularization. Both techniques
    can prevent or reduce the effect of overfitting by adding a penalty to the loss
    that results in shrinking the weight parameters during training. While both L1
    and L2 regularization can be used for NNs as well, with L2 being the more common
    choice of the two, there are other methods for regularizing NNs, such as dropout,
    which we discuss in this section. But before we move on to discussing dropout,
    to use L2 regularization within a convolutional or fully connected network (recall,
    fully connected layers are implemented via `torch.nn.Linear` in PyTorch), you
    can simply add the L2 penalty of a particular layer to the loss function in PyTorch,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Weight decay versus L2 regularization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to use L2 regularization is by setting the `weight_decay`
    parameter in a PyTorch optimizer to a positive value, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: While L2 regularization and `weight_decay` are not strictly identical, it can
    be shown that they are equivalent when using **stochastic gradient descent** (**SGD**)
    optimizers. Interested readers can find more information in the article *Decoupled
    Weight Decay Regularization* by *Ilya* *Loshchilov* and *Frank* *Hutter*, 2019,
    which is freely available at [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, **dropout** has emerged as a popular technique for regularizing
    (deep) NNs to avoid overfitting, thus improving the generalization performance
    (*Dropout: A Simple Way to Prevent Neural Networks from Overfitting* by *N. Srivastava*,
    *G. Hinton*, *A. Krizhevsky*, *I. Sutskever*, and *R. Salakhutdinov*, *Journal
    of Machine Learning Research 15.1*, pages 1929-1958, 2014, [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)).
    Dropout is usually applied to the hidden units of higher layers and works as follows:
    during the training phase of an NN, a fraction of the hidden units is randomly
    dropped at every iteration with probability *p*[drop] (or keep probability *p*[keep] = 1 – *p*[drop]).
    This dropout probability is determined by the user and the common choice is *p* = 0.5,
    as discussed in the previously mentioned article by *Nitish Srivastava* and others,
    2014\. When dropping a certain fraction of input neurons, the weights associated
    with the remaining neurons are rescaled to account for the missing (dropped) neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: The effect of this random dropout is that the network is forced to learn a redundant
    representation of the data. Therefore, the network cannot rely on the activation
    of any set of hidden units, since they may be turned off at any time during training,
    and is forced to learn more general and robust patterns from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This random dropout can effectively prevent overfitting. *Figure 14.10* shows
    an example of applying dropout with probability *p* = 0.5 during the training
    phase, whereby half of the neurons will become inactive randomly (dropped units
    are selected randomly in each forward pass of training). However, during prediction,
    all neurons will contribute to computing the pre-activations of the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Applying dropout during the training phase'
  prefs: []
  type: TYPE_NORMAL
- en: As shown here, one important point to remember is that units may drop randomly
    during training only, whereas for the evaluation (inference) phase, all the hidden
    units must be active (for instance, *p*[drop] = 0 or *p*[keep] = 1). To ensure
    that the overall activations are on the same scale during training and prediction,
    the activations of the active neurons have to be scaled appropriately (for example,
    by halving the activation if the dropout probability was set to *p* = 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: However, since it is inconvenient to always scale activations when making predictions,
    PyTorch and other tools scale the activations during training (for example, by
    doubling the activations if the dropout probability was set to *p* = 0.5). This
    approach is commonly referred to as *inverse dropout*.
  prefs: []
  type: TYPE_NORMAL
- en: While the relationship is not immediately obvious, dropout can be interpreted
    as the consensus (averaging) of an ensemble of models. As discussed in *Chapter
    7*, *Combining Different Models for Ensemble Learning*, in ensemble learning,
    we train several models independently. During prediction, we then use the consensus
    of all the trained models. We already know that model ensembles are known to perform
    better than single models. In deep learning, however, both training several models
    and collecting and averaging the output of multiple models is computationally
    expensive. Here, dropout offers a workaround, with an efficient way to train many
    models at once and compute their average predictions at test or prediction time.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the relationship between model ensembles and dropout
    is not immediately obvious. However, consider that in dropout, we have a different
    model for each mini-batch (due to setting the weights to zero randomly during
    each forward pass).
  prefs: []
  type: TYPE_NORMAL
- en: Then, via iterating over the mini-batches, we essentially sample over *M* = 2^h
    models, where *h* is the number of hidden units.
  prefs: []
  type: TYPE_NORMAL
- en: The restriction and aspect that distinguishes dropout from regular ensembling,
    however, is that we share the weights over these “different models,” which can
    be seen as a form of regularization. Then, during “inference” (for instance, predicting
    the labels in the test dataset), we can average over all these different models
    that we sampled over during training. This is very expensive, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, averaging the models, that is, computing the geometric mean of the class-membership
    probability that is returned by a model, *i*, can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_036.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the trick behind dropout is that this geometric mean of the model ensembles
    (here, *M* models) can be approximated by scaling the predictions of the last
    (or final) model sampled during training by a factor of 1/(1 – *p*), which is
    much cheaper than computing the geometric mean explicitly using the previous equation.
    (In fact, the approximation is exactly equivalent to the true geometric mean if
    we consider linear models.)
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, we saw
    different activation functions, such as ReLU, sigmoid, and tanh. Some of these
    activation functions, like ReLU, are mainly used in the intermediate (hidden)
    layers of an NN to add non-linearities to our model. But others, like sigmoid
    (for binary) and softmax (for multiclass), are added at the last (output) layer,
    which results in class-membership probabilities as the output of the model. If
    the sigmoid or softmax activations are not included at the output layer, then
    the model will compute the logits instead of the class-membership probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on classification problems here, depending on the type of problem (binary
    versus multiclass) and the type of output (logits versus probabilities), we should
    choose the appropriate loss function to train our model. **Binary cross-entropy**
    is the loss function for a binary classification (with a single output unit),
    and **categorical cross-entropy** is the loss function for multiclass classification.
    In the `torch.nn` module, the categorical cross-entropy loss takes in ground truth
    labels as integers (for example, *y*=2, out of three classes, 0, 1, and 2).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.11* describes two loss functions available in `torch.nn` for dealing
    with both cases: binary classification and multiclass with integer labels. Each
    one of these two loss functions also has the option to receive the predictions
    in the form of logits or class-membership probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17582_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Two examples of loss functions in PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that computing the cross-entropy loss by providing the logits, and
    not the class-membership probabilities, is usually preferred due to numerical
    stability reasons. For binary classification, we can either provide logits as
    inputs to the loss function `nn.BCEWithLogitsLoss()`, or compute the probabilities
    based on the logits and feed them to the loss function `nn.BCELoss()`. For multiclass
    classification, we can either provide logits as inputs to the loss function `nn.CrossEntropyLoss()`,
    or compute the log probabilities based on the logits and feed them to the negative
    log-likelihood loss function `nn.NLLLoss()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will show you how to use these loss functions with two different
    formats, where either the logits or class-membership probabilities are given as
    inputs to the loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that sometimes, you may come across an implementation where a categorical
    cross-entropy loss is used for binary classification. Typically, when we have
    a binary classification task, the model returns a single output value for each
    example. We interpret this single model output as the probability of the positive
    class (for example, class 1), *P*(class = 1|**x**). In a binary classification
    problem, it is implied that *P*(class = 0|**x**)= 1 – *P*(class = 1|**x**); hence,
    we do not need a second output unit in order to obtain the probability of the
    negative class. However, sometimes practitioners choose to return two outputs
    for each training example and interpret them as probabilities of each class: *P*(class = 0|**x**)
    versus *P*(class = 1|**x**). Then, in such a case, using a softmax function (instead
    of the logistic sigmoid) to normalize the outputs (so that they sum to 1) is recommended,
    and categorical cross-entropy is the appropriate loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep CNN using PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 13*, as you may recall, we solved the handwritten digit recognition
    problem using the `torch.nn` module. You may also recall that we achieved about
    95.6 percent accuracy using an NN with two linear hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s implement a CNN and see whether it can achieve a better predictive
    performance compared to the previous model for classifying handwritten digits.
    Note that the fully connected layers that we saw in *Chapter 13* were able to
    perform well on this problem. However, in some applications, such as reading bank
    account numbers from handwritten digits, even tiny mistakes can be very costly.
    Therefore, it is crucial to reduce this error as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The multilayer CNN architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of the network that we are going to implement is shown in *Figure
    14.12*. The inputs are 28×28 grayscale images. Considering the number of channels
    (which is 1 for grayscale images) and a batch of input images, the input tensor’s
    dimensions will be *batchsize*×28×28×1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data goes through two convolutional layers that have a kernel size
    of 5×5\. The first convolution has 32 output feature maps, and the second one
    has 64 output feature maps. Each convolution layer is followed by a subsampling
    layer in the form of a max-pooling operation, *P*[2×2]. Then a fully connected
    layer passes the output to a second fully connected layer, which acts as the final
    *softmax* output layer. The architecture of the network that we are going to implement
    is shown in *Figure 14.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: A deep CNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dimensions of the tensors in each layer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: [*batchsize*×28×28×1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv_1: [*batchsize*×28×28×32]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pooling_1: [*batchsize*×14×14×32]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conv_2: [*batchsize*×14×14×64]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pooling_2: [*batchsize*×7×7×64]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FC_1: [*batchsize*×1024]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FC_2 and softmax layer: [*batchsize*×10]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the convolutional kernels, we are using `stride=1` such that the input dimensions
    are preserved in the resulting feature maps. For the pooling layers, we are using
    `kernel_size=2` to subsample the image and shrink the size of the output feature
    maps. We will implement this network using the PyTorch NN module.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and preprocessing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will load the MNIST dataset using the `torchvision` module and construct
    the training and test sets, as we did in *Chapter 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The MNIST dataset comes with a pre-specified training and test dataset partitioning
    scheme, but we also want to create a validation split from the train partition.
    Hence, we used the first 10,000 training examples for validation. Note that the
    images are not sorted by class label, so we do not have to worry about whether
    those validation set images are from the same classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will construct the data loader with batches of 64 images for the training
    set and validation set, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The features we read are of values in the range [0, 1]. Also, we already converted
    the images to tensors. The labels are integers from 0 to 9, representing ten digits.
    Hence, we don’t need to do any scaling or further conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Now, after preparing the dataset, we are ready to implement the CNN we just
    described.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CNN using the torch.nn module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For implementing a CNN in PyTorch, we use the `torch.nn` `Sequential` class
    to stack different layers, such as convolution, pooling, and dropout, as well
    as the fully connected layers. The `torch.nn` module provides classes for each
    one: `nn.Conv2d` for a two-dimensional convolution layer; `nn.MaxPool2d` and `nn.AvgPool2d`
    for subsampling (max-pooling and average-pooling); and `nn.Dropout` for regularization
    using dropout. We will go over each of these classes in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring CNN layers in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constructing a layer with the `Conv2d` class requires us to specify the number
    of output channels (which is equivalent to the number of output feature maps,
    or the number of output filters) and kernel sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, there are optional parameters that we can use to configure a convolutional
    layer. The most commonly used ones are the strides (with a default value of 1
    in both *x*, *y* dimensions) and padding, which controls the amount of implicit
    padding on both dimensions. Additional configuration parameters are listed in
    the official documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that usually, when we read an image, the default dimension
    for the channels is the first dimension of the tensor array (or the second dimension
    considering the batch dimension). This is called the NCHW format, where *N* stands
    for the number of images within the batch, *C* stands for channels, and *H* and
    *W* stand for height and width, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `Conv2D` class assumes that inputs are in NCHW format by default.
    (Other tools, such as TensorFlow, use NHWC format.) However, if you come across
    some data whose channels are placed at the last dimension, you would need to swap
    the axes in your data to move the channels to the first dimension (or the second
    dimension considering the batch dimension). After the layer is constructed, it
    can be called by providing a four-dimensional tensor, with the first dimension
    reserved for a batch of examples; the second dimension corresponds to the channel;
    and the other two dimensions are the spatial dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the architecture of the CNN model that we want to build, each convolution
    layer is followed by a pooling layer for subsampling (reducing the size of feature
    maps). The `MaxPool2d` and `AvgPool2d` classes construct the max-pooling and average-pooling
    layers, respectively. The `kernel_size` argument determines the size of the window
    (or neighborhood) that will be used to compute the max or mean operations. Furthermore,
    the `stride` parameter can be used to configure the pooling layer, as we discussed
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `Dropout` class will construct the dropout layer for regularization,
    with the argument `p` that denotes the drop probability *p*[drop], which is used
    to determine the probability of dropping the input units during training, as we
    discussed earlier. When calling this layer, its behavior can be controlled via
    `model.train()` and `model.eval()`, to specify whether this call will be made
    during training or during the inference. When using dropout, alternating between
    these two modes is crucial to ensure that it behaves correctly; for instance,
    nodes are only randomly dropped during training, not evaluation or inference.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a CNN in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have learned about these classes, we can construct the CNN model
    that was shown in the previous figure. In the following code, we will use the
    `Sequential` class and add the convolution and pooling layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have added two convolution layers to the model. For each convolutional
    layer, we used a kernel of size 5×5 and `padding=2`. As discussed earlier, using
    same padding mode preserves the spatial dimensions (vertical and horizontal dimensions)
    of the feature maps such that the inputs and outputs have the same height and
    width (and the number of channels may only differ in terms of the number of filters
    used). As mentioned before, the spatial dimension of the output feature map is
    calculated by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_037.png)'
  prefs: []
  type: TYPE_IMG
- en: where *n* is the spatial dimension of the input feature map, and *p*, *m*, and
    *s* denote the padding, kernel size, and stride, respectively. We obtain *p* = 2
    in order to achieve *o* = *i*.
  prefs: []
  type: TYPE_NORMAL
- en: The max-pooling layers with pooling size 2×2 and stride of 2 will reduce the
    spatial dimensions by half. (Note that if the `stride` parameter is not specified
    in `MaxPool2D`, by default, it is set equal to the pooling kernel size.)
  prefs: []
  type: TYPE_NORMAL
- en: 'While we can calculate the size of the feature maps at this stage manually,
    PyTorch provides a convenient method to compute this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By providing the input shape as a tuple `(4, 1, 28, 28)` (4 images within the
    batch, 1 channel, and image size 28×28), specified in this example, we calculated
    the output to have a shape `(4, 64, 7, 7)`, indicating feature maps with 64 channels
    and a spatial size of 7×7\. The first dimension corresponds to the batch dimension,
    for which we used 4 arbitrarily.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next layer that we want to add is a fully connected layer for implementing
    a classifier on top of our convolutional and pooling layers. The input to this
    layer must have rank 2, that is, shape [*batchsize* × *input_units*]. Thus, we
    need to flatten the output of the previous layers to meet this requirement for
    the fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As the output shape indicates, the input dimensions for the fully connected
    layer are correctly set up. Next, we will add two fully connected layers with
    a dropout layer in between:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The last fully connected layer, named `''fc2''`, has 10 output units for the
    10 class labels in the MNIST dataset. In practice, we usually use the sofmax activation
    to obtain the class-membership probabilities of each input example, assuming that
    the classes are mutually exclusive, so the probabilities for each example sum
    to 1\. However, the softmax function is already used internally inside PyTorch''s
    `CrossEntropyLoss` implementation, which is why don''t have to explicitly add
    it as a layer after the output layer above. The following code will create the
    loss function and optimizer for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**The Adam optimizer**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in this implementation, we used the `torch.optim.Adam` class for
    training the CNN model. The Adam optimizer is a robust, gradient-based optimization
    method suited to nonconvex optimization and machine learning problems. Two popular
    optimization methods inspired Adam: `RMSProp` and `AdaGrad`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key advantage of Adam is in the choice of update step size derived from
    the running average of gradient moments. Please feel free to read more about the
    Adam optimizer in the manuscript, *Adam: A Method for Stochastic Optimization*
    by *Diederik P. Kingma* and *Jimmy Lei Ba*, 2014\. The article is freely available
    at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can train the model by defining the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that using the designated settings for training `model.train()` and evaluation
    `model.eval()` will automatically set the mode for the dropout layer and rescale
    the hidden units appropriately so that we do not have to worry about that at all.
    Next, we will train this CNN model and use the validation dataset that we created
    for monitoring the learning progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the 20 epochs of training are finished, we can visualize the learning
    curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17582_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Loss and accuracy graphs for the training and validation data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we evaluate the trained model on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The CNN model achieves an accuracy of 99.07 percent. Remember that in *Chapter
    13*, we got approximately 95 percent accuracy using only fully connected (instead
    of convolutional) layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can get the prediction results in the form of class-membership
    probabilities and convert them to predicted labels by using the `torch.argmax`
    function to find the element with the maximum probability. We will do this for
    a batch of 12 examples and visualize the input and predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 14.14* shows the handwritten inputs and their predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Predicted labels for handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: In this set of plotted examples, all the predicted labels are correct.
  prefs: []
  type: TYPE_NORMAL
- en: We leave the task of showing some of the misclassified digits, as we did in
    *Chapter 11*, *Implementing a Multilayer Artificial Neural Network from Scratch*,
    as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Smile classification from face images using a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to implement a CNN for smile classification from
    face images using the CelebA dataset. As you saw in *Chapter 12*, the CelebA dataset
    contains 202,599 images of celebrities’ faces. In addition, 40 binary facial attributes
    are available for each image, including whether a celebrity is smiling (or not)
    and their age (young or old).
  prefs: []
  type: TYPE_NORMAL
- en: Based on what you have learned so far, the goal of this section is to build
    and train a CNN model for predicting the smile attribute from these face images.
    Here, for simplicity, we will only be using a small portion of the training data
    (16,000 training examples) to speed up the training process. However, to improve
    the generalization performance and reduce overfitting on such a small dataset,
    we will use a technique called **data augmentation**.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the CelebA dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s load the data similarly to how we did in the previous section
    for the MNIST dataset. CelebA data comes in three partitions: a training dataset,
    a validation dataset, and a test dataset. Next, we will count the number of examples
    in each partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Alternative ways to download the CelebA dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CelebA dataset is relatively large (approximately 1.5 GB) and the `torchvision`
    download link is notoriously unstable. If you encounter problems executing the
    previous code, you can download the files from the official CelebA website manually
    ([https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    or use our download link: [https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing](https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing).
    If you use our download link, it will download a `celeba.zip` file, which you
    need to unpack in the current directory where you are running the code. Also,
    after downloading and unzipping the `celeba` folder, you need to rerun the code
    above with the setting `download=False` instead of `download=True`. In case you
    are encountering problems with this approach, please do not hesitate to open a
    new issue or start a discussion at [https://github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book)
    so that we can provide you with additional information.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss data augmentation as a technique for boosting the performance
    of deep NNs.
  prefs: []
  type: TYPE_NORMAL
- en: Image transformation and data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data augmentation summarizes a broad set of techniques for dealing with cases
    where the training data is limited. For instance, certain data augmentation techniques
    allow us to modify or even artificially synthesize more data and thereby boost
    the performance of a machine or deep learning model by reducing overfitting. While
    data augmentation is not only for image data, there is a set of transformations
    uniquely applicable to image data, such as cropping parts of an image, flipping,
    and changing the contrast, brightness, and saturation. Let’s see some of these
    transformations that are available via the `torchvision.transforms` module. In
    the following code block, we will first get five examples from the `celeba_train_dataset`
    dataset and apply five different types of transformation: 1) cropping an image
    to a bounding box, 2) flipping an image horizontally, 3) adjusting the contrast,
    4) adjusting the brightness, and 5) center-cropping an image and resizing the
    resulting image back to its original size, (218, 178). In the following code,
    we will visualize the results of these transformations, showing each one in a
    separate column for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 14.15* shows the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Different image transformations'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 14.15*, the original images are shown in the first row and their
    transformed versions in the second row. Note that for the first transformation
    (leftmost column), the bounding box is specified by four numbers: the coordinate
    of the upper-left corner of the bounding box (here *x*=20, *y*=50), and the width
    and height of the box (width=128, height=128). Also note that the origin (the
    coordinates at the location denoted as (0, 0)) for images loaded by PyTorch (as
    well as other packages such as `imageio`) is the upper-left corner of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: The transformations in the previous code block are deterministic. However, all
    such transformations can also be randomized, which is recommended for data augmentation
    during model training. For example, a random bounding box (where the coordinates
    of the upper-left corner are selected randomly) can be cropped from an image,
    an image can be randomly flipped along either the horizontal or vertical axes
    with a probability of 0.5, or the contrast of an image can be changed randomly,
    where the `contrast_factor` is selected at random, but with uniform distribution,
    from a range of values. In addition, we can create a pipeline of these transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can first randomly crop an image, then flip it randomly, and
    finally, resize it to the desired size. The code is as follows (since we have
    random elements, we set the random seed for reproducibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 14.16* shows random transformations on three example images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Random image transformations'
  prefs: []
  type: TYPE_NORMAL
- en: Note that each time we iterate through these three examples, we get slightly
    different images due to random transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we can define transform functions to use this pipeline for
    data augmentation during dataset loading. In the following code, we will define
    the function `get_smile`, which will extract the smile label from the `''attributes''`
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the `transform_train` function that will produce the transformed
    image (where we will first randomly crop the image, then flip it randomly, and
    finally, resize it to the desired size 64×64):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will only apply data augmentation to the training examples, however, and
    not to the validation or test images. The code for the validation or test set
    is as follows (where we will first simply crop the image and then resize it to
    the desired size 64×64):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to see data augmentation in action, let’s apply the `transform_train`
    function to our training dataset and iterate over the dataset five times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 14.17* shows the five resulting transformations for data augmentation
    on two example images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: The result of five image transformations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will apply the `transform` function to our validation and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, instead of using all the available training and validation data,
    we will take a subset of 16,000 training examples and 1,000 examples for validation,
    as our goal here is to intentionally train our model with a small dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create data loaders for three datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now that the data loaders are ready, we will develop a CNN model, and train
    and evaluate it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN smile classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By now, building a model with `torch.nn` module and training it should be straightforward.
    The design of our CNN is as follows: the CNN model receives input images of size
    3×64×64 (the images have three color channels).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data goes through four convolutional layers to make 32, 64, 128,
    and 256 feature maps using filters with a kernel size of 3×3 and padding of 1
    for same padding. The first three convolution layers are followed by max-pooling,
    *P*[2×2]. Two dropout layers are also included for regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the shape of the output feature maps after applying these layers
    using a toy batch input (four images arbitrarily):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There are 256 feature maps (or channels) of size 8×8\. Now, we can add a fully
    connected layer to get to the output layer with a single unit. If we reshape (flatten)
    the feature maps, the number of input units to this fully connected layer will
    be 8 × 8 × 256 = 16,384\. Alternatively, let’s consider a new layer, called *global*
    *average-pooling*, which computes the average of each feature map separately,
    thereby reducing the hidden units to 256\. We can then add a fully connected layer.
    Although we have not discussed global average-pooling explicitly, it is conceptually
    very similar to other pooling layers. Global average-pooling can be viewed, in
    fact, as a special case of average-pooling when the pooling size is equal to the
    size of the input feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, consider *Figure 14.18*, showing an example of input feature
    maps of shape *batchsize*×8×64×64\. The channels are numbered *k* =0, 1,  ..., 7\.
    The global average-pooling operation calculates the average of each channel so
    that the output will have the shape [*batchsize*×8]. After this, we will squeeze
    the output of the global average-pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without squeezing the output, the shape would be [*batchsize*×8×1×1], as the
    global average-pooling would reduce the spatial dimension of 64×64 to 1×1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant objet, antenne  Description générée automatiquement](img/B17582_14_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: Input feature maps'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, given that, in our case, the shape of the feature maps prior to
    this layer is [*batchsize*×256×8×8], we expect to get 256 units as output, that
    is, the shape of the output will be [*batchsize*×256]. Let’s add this layer and
    recompute the output shape to verify that this is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can add a fully connected layer to get a single output unit. In
    this case, we can specify the activation function to be `''sigmoid''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create a loss function and optimizer (Adam optimizer again).
    For a binary classification with a single probabilistic output, we use `BCELoss`
    for the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can train the model by defining the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will train this CNN model for 30 epochs and use the validation dataset
    that we created for monitoring the learning progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now visualize the learning curve and compare the training and validation
    loss and accuracies after each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17582_14_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: A comparison of the training and validation results'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we are happy with the learning curves, we can evaluate the model on the
    hold-out test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we already know how to get the prediction results on some test examples.
    In the following code, we will take a small subset of 10 examples from the last
    batch of our pre-processed test dataset (`test_dl`). Then, we will compute the
    probabilities of each example being from class 1 (which corresponds to *smile*
    based on the labels provided in CelebA) and visualize the examples along with
    their ground truth label and the predicted probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 14.20*, you can see 10 example images along with their ground truth
    labels and the probabilities that they belong to class 1, smile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: Image labels and their probabilities that they belong to class
    1'
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities of class 1 (that is, *smile* according to CelebA) are provided
    below each image. As you can see, our trained model is completely accurate on
    this set of 10 test examples.
  prefs: []
  type: TYPE_NORMAL
- en: As an optional exercise, you are encouraged to try using the entire training
    dataset instead of the small subset we created. Furthermore, you can change or
    modify the CNN architecture. For example, you can change the dropout probabilities
    and the number of filters in the different convolutional layers. Also, you could
    replace the global average-pooling with a fully connected layer. If you are using
    the entire training dataset with the CNN architecture we trained in this chapter,
    you should be able to achieve above 90 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about CNNs and their main components. We started
    with the convolution operation and looked at 1D and 2D implementations. Then,
    we covered another type of layer that is found in several common CNN architectures:
    the subsampling or so-called pooling layers. We primarily focused on the two most
    common forms of pooling: max-pooling and average-pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, putting all these individual concepts together, we implemented deep CNNs
    using the `torch.nn` module. The first network we implemented was applied to the
    already familiar MNIST handwritten digit recognition problem.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we implemented a second CNN on a more complex dataset consisting of face
    images and trained the CNN for smile classification. Along the way, you also learned
    about data augmentation and different transformations that we can apply to face
    images using the `torchvision.transforms` module.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to **recurrent neural networks** (**RNNs**).
    RNNs are used for learning the structure of sequence data, and they have some
    fascinating applications, including language translation and image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
