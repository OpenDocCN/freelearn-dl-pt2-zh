- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Classifying Images with Deep Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度卷积神经网络进行图像分类
- en: 'In the previous chapter, we looked in depth at different aspects of the PyTorch
    neural network and automatic differentiation modules, you became familiar with
    tensors and decorating functions, and you learned how to work with `torch.nn`.
    In this chapter, you will now learn about **convolutional neural networks** (**CNNs**)
    for image classification. We will start by discussing the basic building blocks
    of CNNs, using a bottom-up approach. Then, we will take a deeper dive into the
    CNN architecture and explore how to implement CNNs in PyTorch. In this chapter,
    we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们深入研究了PyTorch神经网络和自动微分模块的不同方面，您熟悉了张量和装饰函数，并学习了如何使用`torch.nn`。在本章中，您将学习有关用于图像分类的**卷积神经网络**（**CNNs**）。我们将从底层开始讨论CNN的基本构建模块。接下来，我们将深入探讨CNN架构，并探讨如何在PyTorch中实现CNN。本章将涵盖以下主题：
- en: Convolution operations in one and two dimensions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维和二维卷积操作
- en: The building blocks of CNN architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN架构的构建模块
- en: Implementing deep CNNs in PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中实现深度CNN
- en: Data augmentation techniques for improving the generalization performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强技术以提高泛化性能
- en: Implementing a facial CNN classifier for recognizing if someone is smiling or
    not
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现用于识别笑容的面部CNN分类器
- en: The building blocks of CNNs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的构建模块
- en: CNNs are a family of models that were originally inspired by how the visual
    cortex of the human brain works when recognizing objects. The development of CNNs
    goes back to the 1990s, when Yann LeCun and his colleagues proposed a novel NN
    architecture for classifying handwritten digits from images (*Handwritten Digit
    Recognition with a Back-Propagation Network* by *Y. LeCun*, and colleagues, 1989,
    published at the *Neural Information Processing Systems (NeurIPS)* conference).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一个模型家族，最初受人类大脑视觉皮层在识别物体时的工作启发。CNN的发展可以追溯到1990年代，当时Yann LeCun及其同事提出了一种新颖的神经网络架构，用于从图像中识别手写数字（*Y.
    LeCun*和同事在1989年发表在*神经信息处理系统（NeurIPS）*会议上的文章 *Handwritten Digit Recognition with
    a Back-Propagation Network*）。
- en: '**The human visual cortex**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类视觉皮层**'
- en: The original discovery of how the visual cortex of our brain functions was made
    by David H. Hubel and Torsten Wiesel in 1959, when they inserted a microelectrode
    into the primary visual cortex of an anesthetized cat. They observed that neurons
    respond differently after projecting different patterns of light in front of the
    cat. This eventually led to the discovery of the different layers of the visual
    cortex. While the primary layer mainly detects edges and straight lines, higher-order
    layers focus more on extracting complex shapes and patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对大脑视觉皮层如何运作的最初发现是由David H. Hubel和Torsten Wiesel在1959年完成的，当时他们在麻醉猫的主视觉皮层插入了微电极。他们观察到，神经元在向猫前方不同模式的光投影后会产生不同的反应。这最终导致了对视觉皮层不同层次的发现。尽管主要层主要检测边缘和直线，但高阶层更专注于提取复杂的形状和图案。
- en: Due to the outstanding performance of CNNs for image classification tasks, this
    particular type of feedforward NN gained a lot of attention and led to tremendous
    improvements in machine learning for computer vision. Several years later, in
    2019, Yann LeCun received the Turing award (the most prestigious award in computer
    science) for his contributions to the field of **artificial intelligence** (**AI**),
    along with two other researchers, Yoshua Bengio and Geoffrey Hinton, whose names
    you encountered in previous chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CNN在图像分类任务中表现出色，这种特定类型的前馈神经网络引起了广泛关注，并在计算机视觉的机器学习领域取得了巨大进展。几年后的2019年，Yann
    LeCun与其他两位研究者Yoshua Bengio和Geoffrey Hinton因其在**人工智能**（**AI**）领域的贡献而共同获得了图灵奖（计算机科学中最负盛名的奖项）。
- en: In the following sections, we will discuss the broader concepts of CNNs and
    why convolutional architectures are often described as “feature extraction layers.”
    Then, we will delve into the theoretical definition of the type of convolution
    operation that is commonly used in CNNs and walk through examples of computing
    convolutions in one and two dimensions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论CNN的更广泛概念以及为什么卷积架构通常被描述为“特征提取层”。然后，我们将深入探讨在CNN中常用的卷积操作类型的理论定义，并通过计算一维和二维卷积的示例来详细讨论。
- en: Understanding CNNs and feature hierarchies
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解CNN和特征层级
- en: Successfully extracting **salient** (**relevant**) **features** is key to the
    performance of any machine learning algorithm, and traditional machine learning
    models rely on input features that may come from a domain expert or are based
    on computational feature extraction techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 成功提取**显著**（**相关**）**特征**对于任何机器学习算法的性能至关重要，传统的机器学习模型依赖于可能来自领域专家或基于计算特征提取技术的输入特征。
- en: 'Certain types of NNs, such as CNNs, can automatically learn the features from
    raw data that are most useful for a particular task. For this reason, it’s common
    to consider CNN layers as feature extractors: the early layers (those right after
    the input layer) extract **low-level features** from raw data, and the later layers
    (often **fully connected layers**, as in a **multilayer perceptron** (**MLP**))
    use these features to predict a continuous target value or class label.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 某些类型的NN，如CNNs，可以自动从原始数据中学习对特定任务最有用的特征。因此，将CNN层视为特征提取器是很常见的：早期层（紧接着输入层的那些层）从原始数据中提取**低级特征**，后续层（通常是**全连接层**，如**多层感知器（MLP）**）利用这些特征来预测连续目标值或类别标签。
- en: Certain types of multilayer NNs, and in particular, deep CNNs, construct a so-called
    **feature hierarchy** by combining the low-level features in a layer-wise fashion
    to form high-level features. For example, if we’re dealing with images, then low-level
    features, such as edges and blobs, are extracted from the earlier layers, which
    are combined to form high-level features. These high-level features can form more
    complex shapes, such as the general contours of objects like buildings, cats,
    or dogs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 某些类型的多层NNs，特别是深度CNNs，通过逐层组合低级特征以形成高级特征的方式构建所谓的**特征层次结构**。例如，如果我们处理图像，则从较早的层中提取低级特征（如边缘和斑点），然后将它们组合形成高级特征。这些高级特征可以形成更复杂的形状，例如建筑物、猫或狗等对象的一般轮廓。
- en: 'As you can see in *Figure 14.1*, a CNN computes **feature maps** from an input
    image, where each element comes from a local patch of pixels in the input image:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图14.1*中看到的那样，CNN从输入图像计算**特征图**，其中每个元素来自输入图像中的一个局部像素块：
- en: '![](img/B17582_14_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_01.png)'
- en: 'Figure 14.1: Creating feature maps from an image (photo by Alexander Dummer
    on Unsplash)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：从图像创建特征图（由Alexander Dummer在Unsplash上拍摄的照片）
- en: 'This local patch of pixels is referred to as the **local receptive field**.
    CNNs will usually perform very well on image-related tasks, and that’s largely
    due to two important ideas:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个局部像素块被称为**局部感受野**。CNN在与图像相关的任务上通常表现非常出色，这在很大程度上归功于两个重要的思想：
- en: '**Sparse connectivity**: A single element in the feature map is connected to
    only a small patch of pixels. (This is very different from connecting to the whole
    input image, as in the case of MLPs. You may find it useful to look back and compare
    how we implemented a fully connected network that connected to the whole image
    in *Chapter 11*, *Implementing a Multilayer Artificial Neural Network from Scratch*.)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏连接性**：特征图中的单个元素仅连接到一个小的像素块。（这与MLP连接到整个输入图像非常不同。你可能会发现回顾并比较我们如何在*第11章*中从头实现全连接网络的方式很有用。）'
- en: '**Parameter sharing**: The same weights are used for different patches of the
    input image.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数共享**：相同的权重用于输入图像的不同补丁。'
- en: As a direct consequence of these two ideas, replacing a conventional, fully
    connected MLP with a convolution layer substantially decreases the number of weights
    (parameters) in the network, and we will see an improvement in the ability to
    capture *salient* features. In the context of image data, it makes sense to assume
    that nearby pixels are typically more relevant to each other than pixels that
    are far away from each other.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个思想的直接结果，用卷积层替换传统的全连接MLP会大大减少网络中的权重（参数），我们将看到在捕获*显著*特征方面的改进。在图像数据的背景下，假设附近的像素通常比远离的像素更相关是有意义的。
- en: Typically, CNNs are composed of several **convolutional** and subsampling layers
    that are followed by one or more fully connected layers at the end. The fully
    connected layers are essentially an MLP, where every input unit, *i*, is connected
    to every output unit, *j*, with weight *w*[ij] (which we covered in more detail
    in *Chapter 11*).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 典型地，CNN由若干**卷积**和子采样层组成，最后跟随一个或多个完全连接层。全连接层本质上就是一个MLP，其中每个输入单元*i*与每个输出单元*j*通过权重*w*[ij]连接（我们在*第11章*中更详细地介绍过）。
- en: Please note that subsampling layers, commonly known as **pooling layers**, do
    not have any learnable parameters; for instance, there are no weights or bias
    units in pooling layers. However, both the convolutional and fully connected layers
    have weights and biases that are optimized during training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，常称为 **池化层** 的子采样层没有任何可学习的参数；例如，池化层中没有权重或偏置单元。然而，卷积层和全连接层都有在训练期间优化的权重和偏置。
- en: In the following sections, we will study convolutional and pooling layers in
    more detail and see how they work. To understand how convolution operations work,
    let’s start with a convolution in one dimension, which is sometimes used for working
    with certain types of sequence data, such as text. After discussing one-dimensional
    convolutions, we will work through the typical two-dimensional ones that are commonly
    applied to two-dimensional images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地研究卷积层和池化层，并了解它们的工作原理。为了理解卷积操作的工作原理，让我们从一维卷积开始，这在处理某些类型的序列数据（如文本）时有时会用到。在讨论一维卷积之后，我们将探讨通常应用于二维图像的二维卷积。
- en: Performing discrete convolutions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行离散卷积
- en: A **discrete convolution** (or simply **convolution**) is a fundamental operation
    in a CNN. Therefore, it’s important to understand how this operation works. In
    this section, we will cover the mathematical definition and discuss some of the
    **naive** algorithms to compute convolutions of one-dimensional tensors (vectors)
    and two-dimensional tensors (matrices).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散卷积**（或简称为 **卷积**）是 CNN 中的一个基本操作。因此，理解这个操作如何工作非常重要。在本节中，我们将涵盖数学定义并讨论一些计算一维张量（向量）和二维张量（矩阵）卷积的**朴素**算法。'
- en: Please note that the formulas and descriptions in this section are solely for
    understanding how convolution operations in CNNs work. Indeed, much more efficient
    implementations of convolutional operations already exist in packages such as
    PyTorch, as you will see later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本节中的公式和描述仅用于理解 CNN 中的卷积操作。实际上，像 PyTorch 这样的软件包中已经存在更高效的卷积操作实现，你将在本章后面看到。
- en: '**Mathematical notation**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学符号**'
- en: In this chapter, we will use subscript to denote the size of a multidimensional
    array (tensor); for example, ![](img/B17582_14_001.png) is a two-dimensional array
    of size *n*[1]×*n*[2]. We use brackets, [ ], to denote the indexing of a multidimensional
    array. For example, *A*[*i*, *j*] refers to the element at index *i*, *j* of matrix
    *A*. Furthermore, note that we use a special symbol, ![](img/B17582_14_002.png),
    to denote the convolution operation between two vectors or matrices, which is
    not to be confused with the multiplication operator, `*`, in Python.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用下标表示多维数组（张量）的大小；例如，![](img/B17582_14_001.png) 是一个大小为 *n*[1]×*n*[2]
    的二维数组。我们使用方括号 [ ] 表示多维数组的索引。例如，*A*[*i*, *j*] 指的是矩阵 *A* 中索引为 *i*, *j* 的元素。此外，请注意我们使用特殊符号
    ![](img/B17582_14_002.png) 表示两个向量或矩阵之间的卷积运算，这与 Python 中的乘法运算符 `*` 不要混淆。
- en: Discrete convolutions in one dimension
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一维离散卷积
- en: 'Let’s start with some basic definitions and notations that we are going to
    use. A discrete convolution for two vectors, **x** and **w**, is denoted by ![](img/B17582_14_003.png),
    in which vector **x** is our input (sometimes called **signal**) and **w** is
    called the **filter** or **kernel**. A discrete convolution is mathematically
    defined as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些基本定义和符号开始。两个向量 **x** 和 **w** 的离散卷积表示为 ![](img/B17582_14_003.png)，其中向量
    **x** 是我们的输入（有时称为 **信号**），**w** 被称为 **滤波器** 或 **核**。离散卷积的数学定义如下：
- en: '![](img/B17582_14_004.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_004.png)'
- en: 'As mentioned earlier, the brackets, [ ], are used to denote the indexing for
    vector elements. The index, *i*, runs through each element of the output vector,
    **y**. There are two odd things in the preceding formula that we need to clarify:
    –∞ to +∞ indices and negative indexing for **x**.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，方括号 [ ] 用于表示向量元素的索引。索引 *i* 遍历输出向量 **y** 的每个元素。在前述公式中有两件需要澄清的奇怪之处：负无穷到正无穷的指数和
    **x** 的负索引。
- en: The fact that the sum runs through indices from –∞ to +∞ seems odd, mainly because
    in machine learning applications, we always deal with finite feature vectors.
    For example, if **x** has 10 features with indices 0, 1, 2, ..., 8, 9, then indices
    –∞: –1 and 10: +∞ are out of bounds for **x**. Therefore, to correctly compute
    the summation shown in the preceding formula, it is assumed that **x** and **w**
    are filled with zeros. This will result in an output vector, **y**, that also
    has infinite size, with lots of zeros as well. Since this is not useful in practical
    situations, **x** is padded only with a finite number of zeros.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来从–∞到+∞的索引和，主要因为在机器学习应用中，我们总是处理有限的特征向量。例如，如果**x**具有10个特征，索引为0、1、2、...、8、9，那么索引–∞: –1和10: +∞对于**x**来说是超出范围的。因此，为了正确计算上述公式中的求和，假设**x**和**w**填充了零。这将导致输出向量**y**也具有无限大小，并且也有很多零。由于在实际情况下这并不有用，因此**x**仅填充有限数量的零。
- en: 'This process is called **zero-padding** or simply **padding**. Here, the number
    of zeros padded on each side is denoted by *p*. An example padding of a one-dimensional
    vector, **x**, is shown in *Figure 14.2*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为**零填充**或简称**填充**。这里，每一侧填充的零的数量由*p*表示。图14.2展示了一维向量**x**的一个填充示例：
- en: '![](img/B17582_14_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_02.png)'
- en: 'Figure 14.2: An example of padding'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：填充的示例
- en: 'Let’s assume that the original input, **x**, and filter, **w**, have *n* and
    *m* elements, respectively, where ![](img/B17582_14_005.png). Therefore, the padded
    vector, *x*^p, has size *n* + 2*p*. The practical formula for computing a discrete
    convolution will change to the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始输入**x**和滤波器**w**分别具有*n*和*m*个元素，其中![](img/B17582_14_005.png)。因此，填充向量*x*^p的大小为*n* + 2*p*。用于计算离散卷积的实际公式将变为：
- en: '![](img/B17582_14_006.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_006.png)'
- en: Now that we have solved the infinite index issue, the second issue is indexing
    **x** with *i* + *m* – *k*. The important point to notice here is that **x** and
    **w** are indexed in different directions in this summation. Computing the sum
    with one index going in the reverse direction is equivalent to computing the sum
    with both indices in the forward direction after flipping one of those vectors,
    **x** or **w**, after they are padded. Then, we can simply compute their dot product.
    Let’s assume we flip (rotate) the filter, **w**, to get the rotated filter, **w**^r.
    Then, the dot product, *x*[*i*: *i* + *m*].**w**^r, is computed to get one element,
    *y*[*i*], where **x**[*i*: *i* + *m*] is a patch of **x** with size *m*. This
    operation is repeated like in a sliding window approach to get all the output
    elements.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了无限索引问题，第二个问题是用*i* + *m* – *k*索引**x**。这里需要注意的重要一点是，在这个求和中**x**和**w**的索引方向不同。用一个索引反向的方法计算总和等同于在填充后翻转其中一个向量**x**或**w**后，同时用正向的方法计算总和。然后，我们可以简单地计算它们的点积。假设我们翻转（旋转）滤波器**w**，得到旋转后的滤波器**w**^r。那么，点积*x*[i: *i* + *m*].**w**^r得到一个元素*y*[i]，其中**x**[i: *i* + *m*]是大小为*m*的**x**的一个片段。这个操作像滑动窗口方法一样重复，以获取所有的输出元素。
- en: 'The following figure provides an example with **x** = [3 2 1 7 1 2 5 4] and
    ![](img/B17582_14_007.png) so that the first three output elements are computed:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示给出了一个例子，其中**x** = [3 2 1 7 1 2 5 4]和![](img/B17582_14_007.png)，以便计算前三个输出元素：
- en: '![](img/B17582_14_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_03.png)'
- en: 'Figure 14.3: The steps for computing a discrete convolution'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：计算离散卷积的步骤
- en: You can see in the preceding example that the padding size is zero (*p* =  0).
    Notice that the rotated filter, **w**^r, is shifted by two cells each time we
    **shift**. This shift is another hyperparameter of a convolution, the **stride**,
    *s*. In this example, the stride is two, *s* = 2\. Note that the stride has to
    be a positive number smaller than the size of the input vector. We will talk more
    about padding and strides in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，您可以看到填充大小为零（*p* =  0）。注意，旋转后的滤波器**w**^r每次**移动**两个单元。这种移动是卷积的另一个超参数，**步长**，*s*。在这个例子中，步长是二，*s* = 2。注意，步长必须是一个小于输入向量大小的正数。我们将在下一节详细讨论填充和步长。
- en: '**Cross-correlation**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉相关**'
- en: 'Cross-correlation (or simply correlation) between an input vector and a filter
    is denoted by ![](img/B17582_14_008.png) and is very much like a sibling of a
    convolution, with a small difference: in cross-correlation, the multiplication
    is performed in the same direction. Therefore, it is not a requirement to rotate
    the filter matrix, *w*, in each dimension. Mathematically, cross-correlation is
    defined as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量和滤波器之间的交叉相关（或简称相关）用 ![](img/B17582_14_008.png) 表示，与卷积非常相似，唯一的区别在于：在交叉相关中，乘法是在相同方向上进行的。因此，在每个维度上不需要旋转滤波器矩阵
    *w*。数学上，交叉相关定义如下：
- en: '![](img/B17582_14_009.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_009.png)'
- en: The same rules for padding and stride may be applied to cross-correlation as
    well. Note that most deep learning frameworks (including PyTorch) implement cross-correlation
    but refer to it as convolution, which is a common convention in the deep learning
    field.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于交叉相关，填充和步长的规则也可以应用。请注意，大多数深度学习框架（包括 PyTorch）实现的是交叉相关，但在深度学习领域中通常称之为卷积，这是一种常见的约定。
- en: Padding inputs to control the size of the output feature maps
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充输入以控制输出特征图的大小
- en: So far, we’ve only used zero-padding in convolutions to compute finite-sized
    output vectors. Technically, padding can be applied with any ![](img/B17582_14_010.png).
    Depending on the choice of *p*, boundary cells may be treated differently than
    the cells located in the middle of **x**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只在卷积中使用了零填充来计算有限大小的输出向量。从技术上讲，填充可以应用于任何 ![](img/B17582_14_010.png)。根据
    *p* 的选择，边界单元格的处理方式可能与位于 **x** 中间的单元格有所不同。
- en: Now, consider an example where *n* = 5 and *m* = 3\. Then, with *p* = 0, *x*[0]
    is only used in computing one output element (for instance, *y*[0]), while *x*[1]
    is used in the computation of two output elements (for instance, *y*[0] and *y*[1]).
    So, you can see that this different treatment of elements of **x** can artificially
    put more emphasis on the middle element, *x*[2], since it has appeared in most
    computations. We can avoid this issue if we choose *p* = 2, in which case, each
    element of **x** will be involved in computing three elements of **y**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个例子，其中 *n* = 5，*m* = 3\. 那么，当 *p* = 0 时，*x*[0] 仅用于计算一个输出元素（例如 *y*[0]），而
    *x*[1] 用于计算两个输出元素（例如 *y*[0] 和 *y*[1]）。因此，你可以看到，对 **x** 元素的这种不同处理可以在较多的计算中人为地突出中间元素
    *x*[2]。如果我们选择 *p* = 2，我们可以避免这个问题，这样 **x** 的每个元素将参与计算 **y** 的三个元素。
- en: Furthermore, the size of the output, **y**, also depends on the choice of the
    padding strategy we use.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输出 **y** 的大小也取决于我们使用的填充策略。
- en: 'There are three modes of padding that are commonly used in practice: *full*,
    *same*, and *valid*.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，有三种常用的填充模式：*full*、*same* 和 *valid*。
- en: In full mode, the padding parameter, *p*, is set to *p* = *m* – 1\. Full padding
    increases the dimensions of the output; thus, it is rarely used in CNN architectures.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 full 模式下，填充参数 *p* 被设置为 *p* = *m* – 1\. 全填充会增加输出的维度，因此在 CNN 架构中很少使用。
- en: The same padding mode is usually used to ensure that the output vector has the
    same size as the input vector, **x**. In this case, the padding parameter, *p*,
    is computed according to the filter size, along with the requirement that the
    input size and output size are the same.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用 same padding 模式来确保输出向量与输入向量 **x** 的大小相同。在这种情况下，填充参数 *p* 根据滤波器大小计算，同时要求输入大小和输出大小相同。
- en: Finally, computing a convolution in valid mode refers to the case where *p* = 0
    (no padding).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 valid 模式下进行卷积计算时，表示 *p* = 0（无填充）。
- en: '*Figure 14.4* illustrates the three different padding modes for a simple 5×5 pixel
    input with a kernel size of 3×3 and a stride of 1:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.4* 展示了一个简单的 5×5 像素输入与 3×3 的核大小和步长为 1 的三种不同填充模式：'
- en: '![Diagram  Description automatically generated](img/B17582_14_04.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图示  自动生成的描述](img/B17582_14_04.png)'
- en: 'Figure 14.4: The three modes of padding'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：三种填充模式
- en: The most commonly used padding mode in CNNs is same padding. One of its advantages
    over the other padding modes is that same padding preserves the size of the vector—or
    the height and width of the input images when we are working on image-related
    tasks in computer vision—which makes designing a network architecture more convenient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 中最常用的填充模式是 same padding。它与其他填充模式相比的优势之一是保持了向量的大小，或者在计算机视觉中处理图像相关任务时保持了输入图像的高度和宽度，这使得设计网络架构更加方便。
- en: 'One big disadvantage of valid padding versus full and same padding is that
    the volume of the tensors will decrease substantially in NNs with many layers,
    which can be detrimental to the network’s performance. In practice, you should
    preserve the spatial size using same padding for the convolutional layers and
    decrease the spatial size via pooling layers or convolutional layers with stride
    2 instead, as described in *Striving for Simplicity: The All Convolutional Net*
    ICLR (workshop track), by *Jost Tobias Springenberg*, *Alexey Dosovitskiy*, and
    others, 2015 ([https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: As for full padding, its size results in an output larger than the input size.
    Full padding is usually used in signal processing applications where it is important
    to minimize boundary effects. However, in a deep learning context, boundary effects
    are usually not an issue, so we rarely see full padding being used in practice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Determining the size of the convolution output
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output size of a convolution is determined by the total number of times
    that we shift the filter, **w**, along the input vector. Let’s assume that the
    input vector is of size *n* and the filter is of size *m*. Then, the size of the
    output resulting from ![](img/B17582_14_011.png), with padding *p* and stride *s*,
    would be determined as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_012.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17582_14_013.png) denotes the *floor* operation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**The floor operation**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'The floor operation returns the largest integer that is equal to or smaller
    than the input, for example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_014.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Consider the following two cases:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output size for an input vector of size 10 with a convolution kernel
    of size 5, padding 2, and stride 1:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_015.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (Note that in this case, the output size turns out to be the same as the input;
    therefore, we can conclude this to be same padding mode.)
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How does the output size change for the same input vector when we have a kernel
    of size 3 and stride 2?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_016.png)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: If you are interested in learning more about the size of the convolution output,
    we recommend the manuscript *A guide to convolution arithmetic for deep learning*
    by *Vincent Dumoulin* and *Francesco Visin*, which is freely available at [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in order to learn how to compute convolutions in one dimension, a
    naive implementation is shown in the following code block, and the results are
    compared with the `numpy.convolve` function. The code is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So far, we have mostly focused on convolutions for vectors (1D convolutions).
    We started with the 1D case to make the concepts easier to understand. In the
    next section, we will cover 2D convolutions in more detail, which are the building
    blocks of CNNs for image-related tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Performing a discrete convolution in 2D
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concepts you learned in the previous sections are easily extendible to
    2D. When we deal with 2D inputs, such as a matrix, ![](img/B17582_14_017.png),
    and the filter matrix, ![](img/B17582_14_018.png), where ![](img/B17582_14_019.png)
    and ![](img/B17582_14_020.png), then the matrix ![](img/B17582_14_021.png) is
    the result of a 2D convolution between **X** and **W**. This is defined mathematically
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你在前几节学到的概念很容易扩展到2D。当我们处理2D输入，比如一个矩阵，![](img/B17582_14_017.png)，和滤波器矩阵，![](img/B17582_14_018.png)，其中![](img/B17582_14_019.png)和![](img/B17582_14_020.png)，那么矩阵![](img/B17582_14_021.png)就是**X**和**W**之间的2D卷积的结果。数学上定义如下：
- en: '![](img/B17582_14_022.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_022.png)'
- en: 'Notice that if you omit one of the dimensions, the remaining formula is exactly
    the same as the one we used previously to compute the convolution in 1D. In fact,
    all the previously mentioned techniques, such as zero padding, rotating the filter
    matrix, and the use of strides, are also applicable to 2D convolutions, provided
    that they are extended to both dimensions independently. *Figure 14.5* demonstrates
    the 2D convolution of an input matrix of size 8×8, using a kernel of size 3×3\.
    The input matrix is padded with zeros with *p* = 1\. As a result, the output of
    the 2D convolution will have a size of 8×8:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果省略其中一个维度，剩余的公式与我们之前用于计算1D卷积的公式完全相同。事实上，所有先前提到的技术，如零填充、旋转滤波器矩阵和使用步长，也适用于2D卷积，只要它们分别扩展到两个维度。*图14.5*展示了使用3×3大小的核对大小为8×8的输入矩阵进行的2D卷积。输入矩阵通过*p* = 1进行了零填充。因此，2D卷积的输出大小为8×8：
- en: '![Chart  Description automatically generated](img/B17582_14_05.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图表说明自动生成](img/B17582_14_05.png)'
- en: 'Figure 14.5: The output of a 2D convolution'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：2D卷积的输出
- en: 'The following example illustrates the computation of a 2D convolution between
    an input matrix, **X**[3×3], and a kernel matrix, **W**[3×3], using padding *p* = (1, 1)
    and stride *s* = (2, 2). According to the specified padding, one layer of zeros
    is added on each side of the input matrix, which results in the padded matrix
    ![](img/B17582_14_023.png), as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子演示了如何计算输入矩阵**X**[3×3]和核矩阵**W**[3×3]之间的2D卷积，使用填充*p* = (1, 1)和步长*s* = (2, 2)。根据指定的填充，每侧都添加了一层零，得到填充后的矩阵![](img/B17582_14_023.png)，如下所示：
- en: '![](img/B17582_14_06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_06.png)'
- en: 'Figure 14.6: Computing a 2D convolution between an input and kernel matrix'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：计算输入和核矩阵之间的2D卷积
- en: 'With the preceding filter, the rotated filter will be:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述滤波器，旋转后的滤波器将是：
- en: '![](img/B17582_14_024.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_024.png)'
- en: 'Note that this rotation is not the same as the transpose matrix. To get the
    rotated filter in NumPy, we can write `W_rot=W[::-1,::-1]`. Next, we can shift
    the rotated filter matrix along the padded input matrix, **X**^(padded), like
    a sliding window, and compute the sum of the element-wise product, which is denoted
    by the ![](img/B17582_14_025.png) operator in *Figure 14.7*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此旋转与转置矩阵不同。在NumPy中获取旋转滤波器，我们可以写成`W_rot=W[::-1,::-1]`。接下来，我们可以将旋转的滤波器矩阵沿着填充的输入矩阵**X**^(padded)移动，如滑动窗口一样，并计算元素乘积的和，这在*图14.7*中由![](img/B17582_14_025.png)运算符表示：
- en: '![](img/B17582_14_07.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_07.png)'
- en: 'Figure 14.7: Computing the sum of the element-wise product'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：计算元素乘积的和
- en: The result will be the 2×2 matrix, **Y**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是2×2矩阵**Y**。
- en: 'Let’s also implement the 2D convolution according to the *naive* algorithm
    described. The `scipy.signal` package provides a way to compute 2D convolution
    via the `scipy.signal.convolve2d` function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据描述的*朴素*算法也实现2D卷积。`scipy.signal`包提供了通过`scipy.signal.convolve2d`函数计算2D卷积的方法：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Efficient algorithms for computing convolution**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**高效计算卷积的算法**'
- en: We provided a naive implementation to compute a 2D convolution for the purpose
    of understanding the concepts. However, this implementation is very inefficient
    in terms of memory requirements and computational complexity. Therefore, it should
    not be used in real-world NN applications.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个朴素的实现来计算2D卷积，以便理解这些概念。然而，这种实现在内存需求和计算复杂度方面非常低效。因此，在现实世界的神经网络应用中不应该使用它。
- en: One aspect is that the filter matrix is actually not rotated in most tools like
    PyTorch. Moreover, in recent years, much more efficient algorithms have been developed
    that use the Fourier transform to compute convolutions. It is also important to
    note that in the context of NNs, the size of a convolution kernel is usually much
    smaller than the size of the input image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: For example, modern CNNs usually use kernel sizes such as 1×1, 3×3, or 5×5,
    for which efficient algorithms have been designed that can carry out the convolutional
    operations much more efficiently, such as Winograd’s minimal filtering algorithm.
    These algorithms are beyond the scope of this book, but if you are interested
    in learning more, you can read the manuscript *Fast Algorithms for Convolutional
    Neural Networks* by *Andrew Lavin* and *Scott Gray*, 2015, which is freely available
    at [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss subsampling or pooling, which is another
    important operation often used in CNNs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling layers
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Subsampling is typically applied in two forms of pooling operations in CNNs:
    **max-pooling** and **mean-pooling** (also known as **average-pooling**). The
    pooling layer is usually denoted by ![](img/B17582_14_026.png). Here, the subscript
    determines the size of the neighborhood (the number of adjacent pixels in each
    dimension) where the max or mean operation is performed. We refer to such a neighborhood
    as the **pooling size**.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The operation is described in *Figure 14.8*. Here, max-pooling takes the maximum
    value from a neighborhood of pixels, and mean-pooling computes their average:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_08.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: An example of max-pooling and mean-pooling'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of pooling is twofold:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling (max-pooling) introduces a local invariance. This means that small
    changes in a local neighborhood do not change the result of max-pooling. Therefore,
    it helps with generating features that are more robust to noise in the input data.
    Refer to the following example, which shows that the max-pooling of two different
    input matrices, **X**[1] and **X**[2], results in the same output:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_14_027.png)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Pooling decreases the size of features, which results in higher computational
    efficiency. Furthermore, reducing the number of features may reduce the degree
    of overfitting as well.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overlapping versus non-overlapping pooling**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, pooling is assumed to be non-overlapping. Pooling is typically
    performed on non-overlapping neighborhoods, which can be done by setting the stride
    parameter equal to the pooling size. For example, a non-overlapping pooling layer,
    ![](img/B17582_14_028.png), requires a stride parameter *s* = (*n*[1], *n*[2]).
    On the other hand, overlapping pooling occurs if the stride is smaller than the
    pooling size. An example where overlapping pooling is used in a convolutional
    network is described in *ImageNet Classification with Deep Convolutional Neural
    Networks* by *A. Krizhevsky*, *I. Sutskever*, and *G. Hinton*, 2012, which is
    freely available as a manuscript at [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: While pooling is still an essential part of many CNN architectures, several
    CNN architectures have also been developed without using pooling layers. Instead
    of using pooling layers to reduce the feature size, researchers use convolutional
    layers with a stride of 2.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'In a sense, you can think of a convolutional layer with stride 2 as a pooling
    layer with learnable weights. If you are interested in an empirical comparison
    of different CNN architectures developed with and without pooling layers, we recommend
    reading the research article *Striving for Simplicity: The All Convolutional Net*
    by *Jost Tobias Springenberg*, *Alexey Dosovitskiy*, *Thomas Brox*, and *Martin
    Riedmiller*. This article is freely available at [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together – implementing a CNN
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you have learned about the basic building blocks of CNNs. The concepts
    illustrated in this chapter are not really more difficult than traditional multilayer
    NNs. We can say that the most important operation in a traditional NN is matrix
    multiplication. For instance, we use matrix multiplications to compute the pre-activations
    (or net inputs), as in **z** = **Wx** + *b*. Here, **x** is a column vector (![](img/B17582_14_029.png)
    matrix) representing pixels, and **W** is the weight matrix connecting the pixel
    inputs to each hidden unit.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In a CNN, this operation is replaced by a convolution operation, as in ![](img/B17582_14_030.png),
    where **X** is a matrix representing the pixels in a *height*×*width* arrangement.
    In both cases, the pre-activations are passed to an activation function to obtain
    the activation of a hidden unit, ![](img/B17582_14_031.png), where ![](img/B17582_14_032.png)
    is the activation function. Furthermore, you will recall that subsampling is another
    building block of a CNN, which may appear in the form of pooling, as was described
    in the previous section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Working with multiple input or color channels
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An input to a convolutional layer may contain one or more 2D arrays or matrices
    with dimensions *N*[1]×*N*[2] (for example, the image height and width in pixels).
    These *N*[1]×*N*[2] matrices are called *channels*. Conventional implementations
    of convolutional layers expect a rank-3 tensor representation as an input, for
    example, a three-dimensional array, ![](img/B17582_14_033.png), where *C*[in]
    is the number of input channels. For example, let’s consider images as input to
    the first layer of a CNN. If the image is colored and uses the RGB color mode,
    then *C*[in] = 3 (for the red, green, and blue color channels in RGB). However,
    if the image is in grayscale, then we have *C*[in] = 1, because there is only
    one channel with the grayscale pixel intensity values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输入可以包含一个或多个2D数组或矩阵，维度为*N*[1]×*N*[2]（例如，像素的图像高度和宽度）。这些*N*[1]×*N*[2]矩阵称为*通道*。传统的卷积层实现期望输入为秩-3张量表示，例如，三维数组，![](img/B17582_14_033.png)，其中*C*[in]是输入通道数。例如，让我们考虑图像作为CNN的第一层的输入。如果图像是彩色并使用RGB色彩模式，则*C*[in]=3（对应于RGB中的红色、绿色和蓝色通道）。然而，如果图像是灰度的，则*C*[in]=1，因为只有一个灰度像素强度通道。
- en: '**Reading an image file**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**读取图像文件**'
- en: When we work with images, we can read images into NumPy arrays using the `uint8`
    (unsigned 8-bit integer) data type to reduce memory usage compared to 16-bit,
    32-bit, or 64-bit integer types, for example.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理图像时，可以使用`uint8`（无符号8位整数）数据类型将图像读入NumPy数组中，以减少内存使用，与16位、32位或64位整数类型相比。
- en: Unsigned 8-bit integers take values in the range [0, 255], which are sufficient
    to store the pixel information in RGB images, which also take values in the same
    range.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号8位整数的取值范围为[0, 255]，足以存储RGB图像中的像素信息，其值也在同一范围内。
- en: 'In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, you
    saw that PyTorch provides a module for loading/storing and manipulating images
    via `torchvision`. Let’s recap how to read an image (this example RGB image is
    located in the code bundle folder that is provided with this chapter):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第12章*中，*使用PyTorch并行化神经网络训练*，你看到PyTorch提供了一个模块，用于通过`torchvision`加载/存储和操作图像。让我们回顾一下如何读取图像（本示例中的RGB图像位于本章提供的代码包文件夹中）：
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that with `torchvision`, the input and output image tensors are in the
    format of `Tensor[channels, image_height, image_width]`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`torchvision`时，输入和输出的图像张量格式为`Tensor[通道数, 图像高度, 图像宽度]`。
- en: 'Now that you are familiar with the structure of input data, the next question
    is, how can we incorporate multiple input channels in the convolution operation
    that we discussed in the previous sections? The answer is very simple: we perform
    the convolution operation for each channel separately and then add the results
    together using the matrix summation. The convolution associated with each channel
    (*c*) has its own kernel matrix as *W*[:, :, *c*].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您熟悉输入数据的结构，接下来的问题是，我们如何在我们讨论的卷积操作中结合多个输入通道？答案非常简单：我们分别为每个通道执行卷积操作，然后使用矩阵求和将结果相加。与每个通道相关联的卷积(*c*)有其自己的核矩阵，例如*W*[:,
    :, *c*]。
- en: 'The total pre-activation result is computed in the following formula:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总的预激活结果由以下公式计算：
- en: '![](img/B17582_14_034.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_034.png)'
- en: 'The final result, **A**, is a feature map. Usually, a convolutional layer of
    a CNN has more than one feature map. If we use multiple feature maps, the kernel
    tensor becomes four-dimensional: *width*×*height*×*C*[in]×*C*[out]. Here, *width*×*height*
    is the kernel size, *C*[in] is the number of input channels, and *C*[out] is the
    number of output feature maps. So, now let’s include the number of output feature
    maps in the preceding formula and update it, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果**A**是一个特征映射。通常，CNN的卷积层具有多个特征映射。如果使用多个特征映射，则核张量变为四维：*宽度*×*高度*×*C*[in]×*C*[out]。因此，现在让我们在前述公式中包含输出特征映射的数量，并进行更新，如下所示：
- en: '![](img/B17582_14_035.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_035.png)'
- en: 'To conclude our discussion of computing convolutions in the context of NNs,
    let’s look at the example in *Figure 14.9*, which shows a convolutional layer,
    followed by a pooling layer. In this example, there are three input channels.
    The kernel tensor is four-dimensional. Each kernel matrix is denoted as *m*[1]×*m*[2],
    and there are three of them, one for each input channel. Furthermore, there are
    five such kernels, accounting for five output feature maps. Finally, there is
    a pooling layer for subsampling the feature maps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_09.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Implementing a CNN'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**How many trainable parameters exist in the preceding example?**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the advantages of convolution, parameter sharing, and sparse
    connectivity, let’s work through an example. The convolutional layer in the network
    shown in *Figure 14.9* is a four-dimensional tensor. So, there are *m*[1]×*m*[2]×3×5
    parameters associated with the kernel. Furthermore, there is a bias vector for
    each output feature map of the convolutional layer. Thus, the size of the bias
    vector is 5\. Pooling layers do not have any (trainable) parameters; therefore,
    we can write the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*m*[1] × *m*[2] × 3 × 5 + 5'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: If the input tensor is of size *n*[1]×*n*[2]×3, assuming that the convolution
    is performed with the same-padding mode, then the size of the output feature maps
    would be *n*[1] × *n*[2] × 5.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if we use a fully connected layer instead of a convolutional layer,
    this number will be much larger. In the case of a fully connected layer, the number
    of parameters for the weight matrix to reach the same number of output units would
    have been as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: (*n*[1] × *n*[2] × 3) × (*n*[1] × *n*[2] × 5) = (*n*[1] × *n*[2])² × 3 × 5
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the size of the bias vector is *n*[1] × *n*[2] × 5 (one bias element
    for each output unit). Given that *m*[1] < *n*[1] and *m*[2] < *n*[2], we can
    see that the difference in the number of trainable parameters is significant.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, as was already mentioned, the convolution operations typically are
    carried out by treating an input image with multiple color channels as a stack
    of matrices; that is, we perform the convolution on each matrix separately and
    then add the results, as was illustrated in the previous figure. However, convolutions
    can also be extended to 3D volumes if you are working with 3D datasets, for example,
    as shown in the paper *VoxNet: A 3D Convolutional Neural Network for Real-Time
    Object Recognition* by *Daniel Maturana* and *Sebastian Scherer*, 2015, which
    can be accessed at [https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about how to regularize an NN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing an NN with L2 regularization and dropout
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the size of a network, whether we are dealing with a traditional (fully
    connected) NN or a CNN, has always been a challenging problem. For instance, the
    size of a weight matrix and the number of layers need to be tuned to achieve a
    reasonably good performance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 选择网络的大小，无论是传统（全连接）NN 还是 CNN，一直都是一个具有挑战性的问题。例如，需要调整权重矩阵的大小和层数，以达到合理的性能。
- en: You will recall from *Chapter 13*, *Going Deeper – The Mechanics of PyTorch*,
    that a simple network without a hidden layer could only capture a linear decision
    boundary, which is not sufficient for dealing with an exclusive or (or XOR) or
    similar problem. The *capacity* of a network refers to the level of complexity
    of the function that it can learn to approximate. Small networks, or networks
    with a relatively small number of parameters, have a low capacity and are therefore
    likely to *underfit*, resulting in poor performance, since they cannot learn the
    underlying structure of complex datasets. However, very large networks may result
    in *overfitting*, where the network will memorize the training data and do extremely
    well on the training dataset while achieving a poor performance on the held-out
    test dataset. When we deal with real-world machine learning problems, we do not
    know how large the network should be *a priori*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您会从 *第13章*，*深入了解 - PyTorch 的机制* 中记得，一个没有隐藏层的简单网络只能捕捉线性决策边界，这对处理异或（或 XOR）或类似问题是不够的。网络的
    *容量* 是指它可以学习逼近的函数复杂性级别。小网络或具有相对少参数的网络具有低容量，因此可能会 *欠拟合*，导致性能不佳，因为它们无法学习复杂数据集的底层结构。但是，非常大的网络可能会导致
    *过拟合*，即网络会记住训练数据，并在训练数据集上表现极好，但在留置测试数据集上表现不佳。当我们处理现实中的机器学习问题时，我们不知道网络应该有多大 *先验*。
- en: One way to address this problem is to build a network with a relatively large
    capacity (in practice, we want to choose a capacity that is slightly larger than
    necessary) to do well on the training dataset. Then, to prevent overfitting, we
    can apply one or multiple regularization schemes to achieve good generalization
    performance on new data, such as the held-out test dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是构建一个具有相对较大容量的网络（在实践中，我们希望选择一个略大于必要的容量），以在训练数据集上表现良好。然后，为了防止过拟合，我们可以应用一个或多个正则化方案，以在新数据（如留置测试数据集）上实现良好的泛化性能。
- en: 'In *Chapters 3* and *4*, we covered L1 and L2 regularization. Both techniques
    can prevent or reduce the effect of overfitting by adding a penalty to the loss
    that results in shrinking the weight parameters during training. While both L1
    and L2 regularization can be used for NNs as well, with L2 being the more common
    choice of the two, there are other methods for regularizing NNs, such as dropout,
    which we discuss in this section. But before we move on to discussing dropout,
    to use L2 regularization within a convolutional or fully connected network (recall,
    fully connected layers are implemented via `torch.nn.Linear` in PyTorch), you
    can simply add the L2 penalty of a particular layer to the loss function in PyTorch,
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第3章* 和 *第4章* 中，我们介绍了 L1 和 L2 正则化。这两种技术都可以通过在训练过程中对损失函数增加惩罚来缩小权重参数，从而防止或减少过拟合的影响。虽然
    L1 和 L2 正则化都可以用于神经网络，L2 是其中更常见的选择，但对于神经网络的正则化还有其他方法，如我们在本节讨论的 dropout。但在讨论 dropout
    之前，在卷积或全连接网络中使用 L2 正则化（回想一下，全连接层是通过 `torch.nn.Linear` 在 PyTorch 中实现的），你可以简单地将特定层的
    L2 惩罚添加到损失函数中，如下所示：
- en: '[PRE3]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Weight decay versus L2 regularization**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重衰减与 L2 正则化**'
- en: 'An alternative way to use L2 regularization is by setting the `weight_decay`
    parameter in a PyTorch optimizer to a positive value, for example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 PyTorch 优化器中将 `weight_decay` 参数设置为正值，可以替代使用 L2 正则化的另一种方法，例如：
- en: '[PRE4]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: While L2 regularization and `weight_decay` are not strictly identical, it can
    be shown that they are equivalent when using **stochastic gradient descent** (**SGD**)
    optimizers. Interested readers can find more information in the article *Decoupled
    Weight Decay Regularization* by *Ilya* *Loshchilov* and *Frank* *Hutter*, 2019,
    which is freely available at [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 L2 正则化和 `weight_decay` 不严格相同，但可以证明它们在使用 **随机梯度下降**（**SGD**）优化器时是等效的。感兴趣的读者可以在
    *Ilya Loshchilov* 和 *Frank Hutter*，2019 年的文章 *Decoupled Weight Decay Regularization*
    中找到更多信息，该文章可以免费在 [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)
    上获取。
- en: 'In recent years, **dropout** has emerged as a popular technique for regularizing
    (deep) NNs to avoid overfitting, thus improving the generalization performance
    (*Dropout: A Simple Way to Prevent Neural Networks from Overfitting* by *N. Srivastava*,
    *G. Hinton*, *A. Krizhevsky*, *I. Sutskever*, and *R. Salakhutdinov*, *Journal
    of Machine Learning Research 15.1*, pages 1929-1958, 2014, [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)).
    Dropout is usually applied to the hidden units of higher layers and works as follows:
    during the training phase of an NN, a fraction of the hidden units is randomly
    dropped at every iteration with probability *p*[drop] (or keep probability *p*[keep] = 1 – *p*[drop]).
    This dropout probability is determined by the user and the common choice is *p* = 0.5,
    as discussed in the previously mentioned article by *Nitish Srivastava* and others,
    2014\. When dropping a certain fraction of input neurons, the weights associated
    with the remaining neurons are rescaled to account for the missing (dropped) neurons.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，**dropout** 已经成为一种流行的技术，用于正则化（深度）神经网络以避免过拟合，从而提高泛化性能（*Dropout: A Simple
    Way to Prevent Neural Networks from Overfitting*，作者为 *N. Srivastava*、*G. Hinton*、*A.
    Krizhevsky*、*I. Sutskever* 和 *R. Salakhutdinov*，*Journal of Machine Learning Research
    15.1*，页面 1929-1958，2014 年，[http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)）。dropout
    通常应用于更高层的隐藏单元，并按以下方式工作：在 NN 的训练阶段，每次迭代时随机丢弃一部分隐藏单元，丢弃概率为 *p*[drop]（或保留概率 *p*[keep]
    = 1 - *p*[drop]）。此丢弃概率由用户确定，常见选择为 *p* = 0.5，如前述的 *Nitish Srivastava* 等人在 2014
    年的文章中讨论的那样。当丢弃一定比例的输入神经元时，与剩余神经元相关联的权重会重新缩放，以考虑缺失的（丢弃的）神经元。'
- en: The effect of this random dropout is that the network is forced to learn a redundant
    representation of the data. Therefore, the network cannot rely on the activation
    of any set of hidden units, since they may be turned off at any time during training,
    and is forced to learn more general and robust patterns from the data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随机 dropout 的效果是，网络被迫学习数据的冗余表示。因此，网络不能依赖于任何一组隐藏单元的激活，因为它们在训练过程中可能随时关闭，并被迫从数据中学习更一般和更稳健的模式。
- en: 'This random dropout can effectively prevent overfitting. *Figure 14.10* shows
    an example of applying dropout with probability *p* = 0.5 during the training
    phase, whereby half of the neurons will become inactive randomly (dropped units
    are selected randomly in each forward pass of training). However, during prediction,
    all neurons will contribute to computing the pre-activations of the next layer:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随机 dropout 可以有效地防止过拟合。*图 14.10* 显示了在训练阶段应用 dropout 的示例，其中丢失概率为 *p* = 0.5，因此在每次训练的前向传递中，一半的神经元将随机失活。然而，在预测阶段，所有神经元将有助于计算下一层的预激活：
- en: '![](img/B17582_14_10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_10.png)'
- en: 'Figure 14.10: Applying dropout during the training phase'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：在训练阶段应用 dropout
- en: As shown here, one important point to remember is that units may drop randomly
    during training only, whereas for the evaluation (inference) phase, all the hidden
    units must be active (for instance, *p*[drop] = 0 or *p*[keep] = 1). To ensure
    that the overall activations are on the same scale during training and prediction,
    the activations of the active neurons have to be scaled appropriately (for example,
    by halving the activation if the dropout probability was set to *p* = 0.5).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所示，一个重要的记住的要点是，单位在训练过程中可能会随机丢失，而在评估（推断）阶段，所有隐藏单元必须处于活跃状态（例如，*p*[drop] = 0
    或 *p*[keep] = 1）。为了确保训练和预测期间的总激活在同一尺度上，必须适当缩放活跃神经元的激活（例如，如果丢失概率设置为 *p* = 0.5，则通过减半激活来进行缩放）。
- en: However, since it is inconvenient to always scale activations when making predictions,
    PyTorch and other tools scale the activations during training (for example, by
    doubling the activations if the dropout probability was set to *p* = 0.5). This
    approach is commonly referred to as *inverse dropout*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于在进行预测时始终缩放激活比较麻烦，因此 PyTorch 和其他工具在训练期间缩放激活（例如，如果丢失概率设置为 *p* = 0.5，则通过加倍激活）。这种方法通常被称为
    *逆 dropout*。
- en: While the relationship is not immediately obvious, dropout can be interpreted
    as the consensus (averaging) of an ensemble of models. As discussed in *Chapter
    7*, *Combining Different Models for Ensemble Learning*, in ensemble learning,
    we train several models independently. During prediction, we then use the consensus
    of all the trained models. We already know that model ensembles are known to perform
    better than single models. In deep learning, however, both training several models
    and collecting and averaging the output of multiple models is computationally
    expensive. Here, dropout offers a workaround, with an efficient way to train many
    models at once and compute their average predictions at test or prediction time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关系并不立即明显，dropout可以被解释为一组模型的共识（平均）。正如在*第7章*，*集成学习中的不同模型组合*中讨论的那样，在集成学习中，我们独立训练几个模型。在预测期间，我们然后使用所有训练模型的共识。我们已经知道，模型集成比单个模型表现更好。然而，在深度学习中，训练多个模型、收集和平均多个模型的输出都是计算昂贵的。在这里，dropout提供了一种解决方案，以一种有效的方式同时训练多个模型，并在测试或预测时计算它们的平均预测。
- en: As mentioned previously, the relationship between model ensembles and dropout
    is not immediately obvious. However, consider that in dropout, we have a different
    model for each mini-batch (due to setting the weights to zero randomly during
    each forward pass).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，模型集成与dropout之间的关系并不立即明显。但是请考虑，在dropout中，由于在每次前向传递期间随机将权重设置为零，我们对每个小批量都有一个不同的模型。
- en: Then, via iterating over the mini-batches, we essentially sample over *M* = 2^h
    models, where *h* is the number of hidden units.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过迭代小批量，我们本质上对*M* = 2^h个模型进行采样，其中*h*是隐藏单元的数量。
- en: The restriction and aspect that distinguishes dropout from regular ensembling,
    however, is that we share the weights over these “different models,” which can
    be seen as a form of regularization. Then, during “inference” (for instance, predicting
    the labels in the test dataset), we can average over all these different models
    that we sampled over during training. This is very expensive, though.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规集成不同，dropout 的限制和方面在于我们在这些“不同模型”上共享权重，这可以看作是一种正则化形式。然后，在“推断”期间（例如，在测试数据集中预测标签），我们可以对训练过程中采样的所有这些不同模型进行平均。尽管如此，这是非常昂贵的。
- en: 'Then, averaging the models, that is, computing the geometric mean of the class-membership
    probability that is returned by a model, *i*, can be computed as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对模型进行平均化，即计算模型*i*返回的类成员概率的几何平均，可以如下计算：
- en: '![](img/B17582_14_036.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_036.png)'
- en: Now, the trick behind dropout is that this geometric mean of the model ensembles
    (here, *M* models) can be approximated by scaling the predictions of the last
    (or final) model sampled during training by a factor of 1/(1 – *p*), which is
    much cheaper than computing the geometric mean explicitly using the previous equation.
    (In fact, the approximation is exactly equivalent to the true geometric mean if
    we consider linear models.)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，dropout 背后的技巧在于，这个模型集合（这里是*M*个模型）的几何平均可以通过在训练过程中采样的最后一个（或最终）模型的预测，乘以一个因子1/(1 – *p*)来近似计算，这比显式计算使用上述方程求解几何平均要便宜得多。（事实上，如果我们考虑线性模型，这个近似等同于真正的几何平均。）
- en: Loss functions for classification
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于分类的损失函数
- en: In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, we saw
    different activation functions, such as ReLU, sigmoid, and tanh. Some of these
    activation functions, like ReLU, are mainly used in the intermediate (hidden)
    layers of an NN to add non-linearities to our model. But others, like sigmoid
    (for binary) and softmax (for multiclass), are added at the last (output) layer,
    which results in class-membership probabilities as the output of the model. If
    the sigmoid or softmax activations are not included at the output layer, then
    the model will compute the logits instead of the class-membership probabilities.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第12章*，*使用PyTorch并行化神经网络训练*中，我们看到了不同的激活函数，如ReLU、sigmoid和tanh。这些激活函数中的一些，如ReLU，主要用于神经网络的中间（隐藏）层，以为我们的模型添加非线性。但其他的，如sigmoid（用于二元）和softmax（用于多类别），则添加在最后（输出）层，将结果作为模型的类成员概率输出。如果在输出层没有包含sigmoid或softmax激活函数，则模型将计算logits而不是类成员概率。
- en: Focusing on classification problems here, depending on the type of problem (binary
    versus multiclass) and the type of output (logits versus probabilities), we should
    choose the appropriate loss function to train our model. **Binary cross-entropy**
    is the loss function for a binary classification (with a single output unit),
    and **categorical cross-entropy** is the loss function for multiclass classification.
    In the `torch.nn` module, the categorical cross-entropy loss takes in ground truth
    labels as integers (for example, *y*=2, out of three classes, 0, 1, and 2).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里关注分类问题，根据问题的类型（二元分类还是多类分类）和输出的类型（logits还是概率），我们应选择适当的损失函数来训练我们的模型。**二元交叉熵**是二元分类（具有单个输出单元）的损失函数，**分类交叉熵**是多类分类的损失函数。在`torch.nn`模块中，分类交叉熵损失接受整数形式的真实标签（例如，*y*=2，对于三个类别0、1和2）。
- en: '*Figure 14.11* describes two loss functions available in `torch.nn` for dealing
    with both cases: binary classification and multiclass with integer labels. Each
    one of these two loss functions also has the option to receive the predictions
    in the form of logits or class-membership probabilities:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.11*描述了在`torch.nn`中可用的两个损失函数，用于处理二元分类和整数标签的多类分类。这两个损失函数中的每一个还可以选择以logits或类成员概率的形式接收预测值：'
- en: '![Table  Description automatically generated](img/B17582_14_11.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B17582_14_11.png)'
- en: 'Figure 14.11: Two examples of loss functions in PyTorch'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图14.11: PyTorch中两个损失函数的示例'
- en: Please note that computing the cross-entropy loss by providing the logits, and
    not the class-membership probabilities, is usually preferred due to numerical
    stability reasons. For binary classification, we can either provide logits as
    inputs to the loss function `nn.BCEWithLogitsLoss()`, or compute the probabilities
    based on the logits and feed them to the loss function `nn.BCELoss()`. For multiclass
    classification, we can either provide logits as inputs to the loss function `nn.CrossEntropyLoss()`,
    or compute the log probabilities based on the logits and feed them to the negative
    log-likelihood loss function `nn.NLLLoss()`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于数值稳定性原因，通常通过提供logits而不是类成员概率来计算交叉熵损失更为可取。对于二元分类，我们可以将logits作为输入提供给损失函数`nn.BCEWithLogitsLoss()`，或者基于logits计算概率并将其馈送给损失函数`nn.BCELoss()`。对于多类分类，我们可以将logits作为输入提供给损失函数`nn.CrossEntropyLoss()`，或者基于logits计算对数概率并将其馈送给负对数似然损失函数`nn.NLLLoss()`。
- en: 'The following code will show you how to use these loss functions with two different
    formats, where either the logits or class-membership probabilities are given as
    inputs to the loss functions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将向您展示如何使用这些损失函数来处理两种不同的格式，其中损失函数的输入可以是logits或类成员概率：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that sometimes, you may come across an implementation where a categorical
    cross-entropy loss is used for binary classification. Typically, when we have
    a binary classification task, the model returns a single output value for each
    example. We interpret this single model output as the probability of the positive
    class (for example, class 1), *P*(class = 1|**x**). In a binary classification
    problem, it is implied that *P*(class = 0|**x**)= 1 – *P*(class = 1|**x**); hence,
    we do not need a second output unit in order to obtain the probability of the
    negative class. However, sometimes practitioners choose to return two outputs
    for each training example and interpret them as probabilities of each class: *P*(class = 0|**x**)
    versus *P*(class = 1|**x**). Then, in such a case, using a softmax function (instead
    of the logistic sigmoid) to normalize the outputs (so that they sum to 1) is recommended,
    and categorical cross-entropy is the appropriate loss function.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有时您可能会遇到使用分类交叉熵损失进行二元分类的实现。通常情况下，当我们有一个二元分类任务时，模型为每个示例返回单个输出值。我们将这个单一模型输出解释为正类（例如类1）的概率
    *P*(class = 1|**x**)。在二元分类问题中，我们隐含地有 *P*(class = 0|**x**)= 1 – *P*(class = 1|**x**)，因此我们不需要第二个输出单元来获取负类的概率。然而，有时实践者选择为每个训练示例返回两个输出，并将其解释为每个类的概率：*P*(class = 0|**x**)与*P*(class = 1|**x**)。在这种情况下，建议使用softmax函数（而不是逻辑sigmoid）来归一化输出（使它们总和为1），并且分类交叉熵是适当的损失函数。
- en: Implementing a deep CNN using PyTorch
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch实现深度CNN
- en: In *Chapter 13*, as you may recall, we solved the handwritten digit recognition
    problem using the `torch.nn` module. You may also recall that we achieved about
    95.6 percent accuracy using an NN with two linear hidden layers.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能还记得的*第13章*中，我们使用`torch.nn`模块解决了手写数字识别问题。您可能还记得，我们使用具有两个线性隐藏层的NN达到了约95.6%的准确率。
- en: Now, let’s implement a CNN and see whether it can achieve a better predictive
    performance compared to the previous model for classifying handwritten digits.
    Note that the fully connected layers that we saw in *Chapter 13* were able to
    perform well on this problem. However, in some applications, such as reading bank
    account numbers from handwritten digits, even tiny mistakes can be very costly.
    Therefore, it is crucial to reduce this error as much as possible.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个CNN，看看它是否能够比之前的模型在分类手写数字方面实现更好的预测性能。请注意，在*第13章*中看到的全连接层在此问题上表现良好。然而，在某些应用中，比如从手写数字中读取银行账号号码，即使是微小的错误也可能非常昂贵。因此，尽可能减少这种错误至关重要。
- en: The multilayer CNN architecture
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层CNN架构
- en: The architecture of the network that we are going to implement is shown in *Figure
    14.12*. The inputs are 28×28 grayscale images. Considering the number of channels
    (which is 1 for grayscale images) and a batch of input images, the input tensor’s
    dimensions will be *batchsize*×28×28×1.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实现的网络架构如*图14.12*所示。输入为28×28的灰度图像。考虑到通道数（对于灰度图像为1）和输入图像的批量，输入张量的维度将是*batchsize*×28×28×1。
- en: 'The input data goes through two convolutional layers that have a kernel size
    of 5×5\. The first convolution has 32 output feature maps, and the second one
    has 64 output feature maps. Each convolution layer is followed by a subsampling
    layer in the form of a max-pooling operation, *P*[2×2]. Then a fully connected
    layer passes the output to a second fully connected layer, which acts as the final
    *softmax* output layer. The architecture of the network that we are going to implement
    is shown in *Figure 14.12*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据通过两个卷积层，卷积核大小为5×5。第一个卷积层有32个输出特征图，第二个卷积层有64个输出特征图。每个卷积层后面跟着一个池化层，采用最大池化操作，*P*[2×2]。然后一个全连接层将输出传递给第二个全连接层，它作为最终的*softmax*输出层。我们将要实现的网络架构如*图14.12*所示：
- en: '![](img/B17582_14_12.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_12.png)'
- en: 'Figure 14.12: A deep CNN'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：一个深度CNN
- en: 'The dimensions of the tensors in each layer are as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每层张量的尺寸如下：
- en: 'Input: [*batchsize*×28×28×1]'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：[*batchsize*×28×28×1]
- en: 'Conv_1: [*batchsize*×28×28×32]'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积_1：[*batchsize*×28×28×32]
- en: 'Pooling_1: [*batchsize*×14×14×32]'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化_1：[*batchsize*×14×14×32]
- en: 'Conv_2: [*batchsize*×14×14×64]'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积_2：[*batchsize*×14×14×64]
- en: 'Pooling_2: [*batchsize*×7×7×64]'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化_2：[*batchsize*×7×7×64]
- en: 'FC_1: [*batchsize*×1024]'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC_1：[*batchsize*×1024]
- en: 'FC_2 and softmax layer: [*batchsize*×10]'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC_2和softmax层：[*batchsize*×10]
- en: For the convolutional kernels, we are using `stride=1` such that the input dimensions
    are preserved in the resulting feature maps. For the pooling layers, we are using
    `kernel_size=2` to subsample the image and shrink the size of the output feature
    maps. We will implement this network using the PyTorch NN module.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积核，我们使用`stride=1`以保持输入维度在生成的特征图中的尺寸不变。对于池化层，我们使用`kernel_size=2`来对图像进行子采样并缩小输出特征图的尺寸。我们将使用PyTorch
    NN模块来实现这个网络。
- en: Loading and preprocessing the data
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和预处理数据
- en: 'First, we will load the MNIST dataset using the `torchvision` module and construct
    the training and test sets, as we did in *Chapter 13*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用`torchvision`模块加载MNIST数据集，并构建训练集和测试集，就像在*第13章*中一样：
- en: '[PRE6]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The MNIST dataset comes with a pre-specified training and test dataset partitioning
    scheme, but we also want to create a validation split from the train partition.
    Hence, we used the first 10,000 training examples for validation. Note that the
    images are not sorted by class label, so we do not have to worry about whether
    those validation set images are from the same classes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集附带了一个预先指定的训练和测试数据集分割方案，但我们还想从训练分区创建一个验证集分割。因此，我们使用了前10,000个训练示例用于验证。注意，图像并未按类标签排序，因此我们不必担心这些验证集图像是否来自相同的类别。
- en: 'Next, we will construct the data loader with batches of 64 images for the training
    set and validation set, respectively:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用64个图像的批量构建数据加载器，分别用于训练集和验证集：
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The features we read are of values in the range [0, 1]. Also, we already converted
    the images to tensors. The labels are integers from 0 to 9, representing ten digits.
    Hence, we don’t need to do any scaling or further conversion.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Now, after preparing the dataset, we are ready to implement the CNN we just
    described.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CNN using the torch.nn module
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For implementing a CNN in PyTorch, we use the `torch.nn` `Sequential` class
    to stack different layers, such as convolution, pooling, and dropout, as well
    as the fully connected layers. The `torch.nn` module provides classes for each
    one: `nn.Conv2d` for a two-dimensional convolution layer; `nn.MaxPool2d` and `nn.AvgPool2d`
    for subsampling (max-pooling and average-pooling); and `nn.Dropout` for regularization
    using dropout. We will go over each of these classes in more detail.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Configuring CNN layers in PyTorch
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constructing a layer with the `Conv2d` class requires us to specify the number
    of output channels (which is equivalent to the number of output feature maps,
    or the number of output filters) and kernel sizes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, there are optional parameters that we can use to configure a convolutional
    layer. The most commonly used ones are the strides (with a default value of 1
    in both *x*, *y* dimensions) and padding, which controls the amount of implicit
    padding on both dimensions. Additional configuration parameters are listed in
    the official documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that usually, when we read an image, the default dimension
    for the channels is the first dimension of the tensor array (or the second dimension
    considering the batch dimension). This is called the NCHW format, where *N* stands
    for the number of images within the batch, *C* stands for channels, and *H* and
    *W* stand for height and width, respectively.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `Conv2D` class assumes that inputs are in NCHW format by default.
    (Other tools, such as TensorFlow, use NHWC format.) However, if you come across
    some data whose channels are placed at the last dimension, you would need to swap
    the axes in your data to move the channels to the first dimension (or the second
    dimension considering the batch dimension). After the layer is constructed, it
    can be called by providing a four-dimensional tensor, with the first dimension
    reserved for a batch of examples; the second dimension corresponds to the channel;
    and the other two dimensions are the spatial dimensions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the architecture of the CNN model that we want to build, each convolution
    layer is followed by a pooling layer for subsampling (reducing the size of feature
    maps). The `MaxPool2d` and `AvgPool2d` classes construct the max-pooling and average-pooling
    layers, respectively. The `kernel_size` argument determines the size of the window
    (or neighborhood) that will be used to compute the max or mean operations. Furthermore,
    the `stride` parameter can be used to configure the pooling layer, as we discussed
    earlier.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `Dropout` class will construct the dropout layer for regularization,
    with the argument `p` that denotes the drop probability *p*[drop], which is used
    to determine the probability of dropping the input units during training, as we
    discussed earlier. When calling this layer, its behavior can be controlled via
    `model.train()` and `model.eval()`, to specify whether this call will be made
    during training or during the inference. When using dropout, alternating between
    these two modes is crucial to ensure that it behaves correctly; for instance,
    nodes are only randomly dropped during training, not evaluation or inference.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a CNN in PyTorch
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have learned about these classes, we can construct the CNN model
    that was shown in the previous figure. In the following code, we will use the
    `Sequential` class and add the convolution and pooling layers:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So far, we have added two convolution layers to the model. For each convolutional
    layer, we used a kernel of size 5×5 and `padding=2`. As discussed earlier, using
    same padding mode preserves the spatial dimensions (vertical and horizontal dimensions)
    of the feature maps such that the inputs and outputs have the same height and
    width (and the number of channels may only differ in terms of the number of filters
    used). As mentioned before, the spatial dimension of the output feature map is
    calculated by:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_037.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: where *n* is the spatial dimension of the input feature map, and *p*, *m*, and
    *s* denote the padding, kernel size, and stride, respectively. We obtain *p* = 2
    in order to achieve *o* = *i*.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The max-pooling layers with pooling size 2×2 and stride of 2 will reduce the
    spatial dimensions by half. (Note that if the `stride` parameter is not specified
    in `MaxPool2D`, by default, it is set equal to the pooling kernel size.)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'While we can calculate the size of the feature maps at this stage manually,
    PyTorch provides a convenient method to compute this for us:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By providing the input shape as a tuple `(4, 1, 28, 28)` (4 images within the
    batch, 1 channel, and image size 28×28), specified in this example, we calculated
    the output to have a shape `(4, 64, 7, 7)`, indicating feature maps with 64 channels
    and a spatial size of 7×7\. The first dimension corresponds to the batch dimension,
    for which we used 4 arbitrarily.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The next layer that we want to add is a fully connected layer for implementing
    a classifier on top of our convolutional and pooling layers. The input to this
    layer must have rank 2, that is, shape [*batchsize* × *input_units*]. Thus, we
    need to flatten the output of the previous layers to meet this requirement for
    the fully connected layer:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As the output shape indicates, the input dimensions for the fully connected
    layer are correctly set up. Next, we will add two fully connected layers with
    a dropout layer in between:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The last fully connected layer, named `''fc2''`, has 10 output units for the
    10 class labels in the MNIST dataset. In practice, we usually use the sofmax activation
    to obtain the class-membership probabilities of each input example, assuming that
    the classes are mutually exclusive, so the probabilities for each example sum
    to 1\. However, the softmax function is already used internally inside PyTorch''s
    `CrossEntropyLoss` implementation, which is why don''t have to explicitly add
    it as a layer after the output layer above. The following code will create the
    loss function and optimizer for the model:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**The Adam optimizer**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in this implementation, we used the `torch.optim.Adam` class for
    training the CNN model. The Adam optimizer is a robust, gradient-based optimization
    method suited to nonconvex optimization and machine learning problems. Two popular
    optimization methods inspired Adam: `RMSProp` and `AdaGrad`.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'The key advantage of Adam is in the choice of update step size derived from
    the running average of gradient moments. Please feel free to read more about the
    Adam optimizer in the manuscript, *Adam: A Method for Stochastic Optimization*
    by *Diederik P. Kingma* and *Jimmy Lei Ba*, 2014\. The article is freely available
    at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can train the model by defining the following function:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that using the designated settings for training `model.train()` and evaluation
    `model.eval()` will automatically set the mode for the dropout layer and rescale
    the hidden units appropriately so that we do not have to worry about that at all.
    Next, we will train this CNN model and use the validation dataset that we created
    for monitoring the learning progress:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the 20 epochs of training are finished, we can visualize the learning
    curves:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/B17582_14_13.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Loss and accuracy graphs for the training and validation data'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we evaluate the trained model on the test dataset:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The CNN model achieves an accuracy of 99.07 percent. Remember that in *Chapter
    13*, we got approximately 95 percent accuracy using only fully connected (instead
    of convolutional) layers.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can get the prediction results in the form of class-membership
    probabilities and convert them to predicted labels by using the `torch.argmax`
    function to find the element with the maximum probability. We will do this for
    a batch of 12 examples and visualize the input and predicted labels:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过使用`torch.argmax`函数将类成员概率形式的预测结果转换为预测标签。我们将对一批12个示例执行此操作，并可视化输入和预测标签：
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 14.14* shows the handwritten inputs and their predicted labels:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.14*显示了手写输入及其预测标签：'
- en: '![](img/B17582_14_14.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_14_14.png)'
- en: 'Figure 14.14: Predicted labels for handwritten digits'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：手写数字的预测标签
- en: In this set of plotted examples, all the predicted labels are correct.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这组绘图示例中，所有预测标签都是正确的。
- en: We leave the task of showing some of the misclassified digits, as we did in
    *Chapter 11*, *Implementing a Multilayer Artificial Neural Network from Scratch*,
    as an exercise for the reader.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留给读者作为练习的任务是展示一些被错误分类的数字，就像我们在*第11章*，*从头开始实现多层人工神经网络*中所做的那样。
- en: Smile classification from face images using a CNN
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN从面部图像进行微笑分类
- en: In this section, we are going to implement a CNN for smile classification from
    face images using the CelebA dataset. As you saw in *Chapter 12*, the CelebA dataset
    contains 202,599 images of celebrities’ faces. In addition, 40 binary facial attributes
    are available for each image, including whether a celebrity is smiling (or not)
    and their age (young or old).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用CelebA数据集实现一个CNN，用于从面部图像进行微笑分类。正如您在*第12章*中看到的那样，CelebA数据集包含202,599张名人面部的图像。此外，每个图像还有40个二进制面部属性，包括名人是否微笑以及他们的年龄（年轻或老年）。
- en: Based on what you have learned so far, the goal of this section is to build
    and train a CNN model for predicting the smile attribute from these face images.
    Here, for simplicity, we will only be using a small portion of the training data
    (16,000 training examples) to speed up the training process. However, to improve
    the generalization performance and reduce overfitting on such a small dataset,
    we will use a technique called **data augmentation**.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于你目前所学的内容，本节的目标是构建并训练一个CNN模型，用于从这些面部图像中预测微笑属性。在这里，为了简化起见，我们将仅使用训练数据的一小部分（16,000个训练示例）来加快训练过程。然而，为了提高泛化性能并减少在这样一个小数据集上的过拟合，我们将使用一种称为**数据增强**的技术。
- en: Loading the CelebA dataset
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载CelebA数据集
- en: 'First, let’s load the data similarly to how we did in the previous section
    for the MNIST dataset. CelebA data comes in three partitions: a training dataset,
    a validation dataset, and a test dataset. Next, we will count the number of examples
    in each partition:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据，类似于我们在前一节中为MNIST数据集所做的方式。CelebA数据集分为三个部分：训练数据集、验证数据集和测试数据集。接下来，我们将计算每个分区中的示例数量：
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Alternative ways to download the CelebA dataset**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载CelebA数据集的替代方法**'
- en: 'The CelebA dataset is relatively large (approximately 1.5 GB) and the `torchvision`
    download link is notoriously unstable. If you encounter problems executing the
    previous code, you can download the files from the official CelebA website manually
    ([https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    or use our download link: [https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing](https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing).
    If you use our download link, it will download a `celeba.zip` file, which you
    need to unpack in the current directory where you are running the code. Also,
    after downloading and unzipping the `celeba` folder, you need to rerun the code
    above with the setting `download=False` instead of `download=True`. In case you
    are encountering problems with this approach, please do not hesitate to open a
    new issue or start a discussion at [https://github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book)
    so that we can provide you with additional information.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss data augmentation as a technique for boosting the performance
    of deep NNs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Image transformation and data augmentation
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data augmentation summarizes a broad set of techniques for dealing with cases
    where the training data is limited. For instance, certain data augmentation techniques
    allow us to modify or even artificially synthesize more data and thereby boost
    the performance of a machine or deep learning model by reducing overfitting. While
    data augmentation is not only for image data, there is a set of transformations
    uniquely applicable to image data, such as cropping parts of an image, flipping,
    and changing the contrast, brightness, and saturation. Let’s see some of these
    transformations that are available via the `torchvision.transforms` module. In
    the following code block, we will first get five examples from the `celeba_train_dataset`
    dataset and apply five different types of transformation: 1) cropping an image
    to a bounding box, 2) flipping an image horizontally, 3) adjusting the contrast,
    4) adjusting the brightness, and 5) center-cropping an image and resizing the
    resulting image back to its original size, (218, 178). In the following code,
    we will visualize the results of these transformations, showing each one in a
    separate column for comparison:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 14.15* shows the results:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_15.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Different image transformations'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 14.15*, the original images are shown in the first row and their
    transformed versions in the second row. Note that for the first transformation
    (leftmost column), the bounding box is specified by four numbers: the coordinate
    of the upper-left corner of the bounding box (here *x*=20, *y*=50), and the width
    and height of the box (width=128, height=128). Also note that the origin (the
    coordinates at the location denoted as (0, 0)) for images loaded by PyTorch (as
    well as other packages such as `imageio`) is the upper-left corner of the image.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The transformations in the previous code block are deterministic. However, all
    such transformations can also be randomized, which is recommended for data augmentation
    during model training. For example, a random bounding box (where the coordinates
    of the upper-left corner are selected randomly) can be cropped from an image,
    an image can be randomly flipped along either the horizontal or vertical axes
    with a probability of 0.5, or the contrast of an image can be changed randomly,
    where the `contrast_factor` is selected at random, but with uniform distribution,
    from a range of values. In addition, we can create a pipeline of these transformations.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can first randomly crop an image, then flip it randomly, and
    finally, resize it to the desired size. The code is as follows (since we have
    random elements, we set the random seed for reproducibility):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Figure 14.16* shows random transformations on three example images:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_16.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Random image transformations'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Note that each time we iterate through these three examples, we get slightly
    different images due to random transformations.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we can define transform functions to use this pipeline for
    data augmentation during dataset loading. In the following code, we will define
    the function `get_smile`, which will extract the smile label from the `''attributes''`
    list:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will define the `transform_train` function that will produce the transformed
    image (where we will first randomly crop the image, then flip it randomly, and
    finally, resize it to the desired size 64×64):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will only apply data augmentation to the training examples, however, and
    not to the validation or test images. The code for the validation or test set
    is as follows (where we will first simply crop the image and then resize it to
    the desired size 64×64):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, to see data augmentation in action, let’s apply the `transform_train`
    function to our training dataset and iterate over the dataset five times:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*Figure 14.17* shows the five resulting transformations for data augmentation
    on two example images:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_17.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: The result of five image transformations'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will apply the `transform` function to our validation and test datasets:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Furthermore, instead of using all the available training and validation data,
    we will take a subset of 16,000 training examples and 1,000 examples for validation,
    as our goal here is to intentionally train our model with a small dataset:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can create data loaders for three datasets:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that the data loaders are ready, we will develop a CNN model, and train
    and evaluate it in the next section.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN smile classifier
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By now, building a model with `torch.nn` module and training it should be straightforward.
    The design of our CNN is as follows: the CNN model receives input images of size
    3×64×64 (the images have three color channels).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data goes through four convolutional layers to make 32, 64, 128,
    and 256 feature maps using filters with a kernel size of 3×3 and padding of 1
    for same padding. The first three convolution layers are followed by max-pooling,
    *P*[2×2]. Two dropout layers are also included for regularization:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s see the shape of the output feature maps after applying these layers
    using a toy batch input (four images arbitrarily):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: There are 256 feature maps (or channels) of size 8×8\. Now, we can add a fully
    connected layer to get to the output layer with a single unit. If we reshape (flatten)
    the feature maps, the number of input units to this fully connected layer will
    be 8 × 8 × 256 = 16,384\. Alternatively, let’s consider a new layer, called *global*
    *average-pooling*, which computes the average of each feature map separately,
    thereby reducing the hidden units to 256\. We can then add a fully connected layer.
    Although we have not discussed global average-pooling explicitly, it is conceptually
    very similar to other pooling layers. Global average-pooling can be viewed, in
    fact, as a special case of average-pooling when the pooling size is equal to the
    size of the input feature maps.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, consider *Figure 14.18*, showing an example of input feature
    maps of shape *batchsize*×8×64×64\. The channels are numbered *k* =0, 1,  ..., 7\.
    The global average-pooling operation calculates the average of each channel so
    that the output will have the shape [*batchsize*×8]. After this, we will squeeze
    the output of the global average-pooling layer.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Without squeezing the output, the shape would be [*batchsize*×8×1×1], as the
    global average-pooling would reduce the spatial dimension of 64×64 to 1×1:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant objet, antenne  Description générée automatiquement](img/B17582_14_18.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: Input feature maps'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, given that, in our case, the shape of the feature maps prior to
    this layer is [*batchsize*×256×8×8], we expect to get 256 units as output, that
    is, the shape of the output will be [*batchsize*×256]. Let’s add this layer and
    recompute the output shape to verify that this is true:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we can add a fully connected layer to get a single output unit. In
    this case, we can specify the activation function to be `''sigmoid''`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The next step is to create a loss function and optimizer (Adam optimizer again).
    For a binary classification with a single probabilistic output, we use `BCELoss`
    for the loss function:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can train the model by defining the following function:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will train this CNN model for 30 epochs and use the validation dataset
    that we created for monitoring the learning progress:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s now visualize the learning curve and compare the training and validation
    loss and accuracies after each epoch:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/B17582_14_19.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: A comparison of the training and validation results'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we are happy with the learning curves, we can evaluate the model on the
    hold-out test dataset:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we already know how to get the prediction results on some test examples.
    In the following code, we will take a small subset of 10 examples from the last
    batch of our pre-processed test dataset (`test_dl`). Then, we will compute the
    probabilities of each example being from class 1 (which corresponds to *smile*
    based on the labels provided in CelebA) and visualize the examples along with
    their ground truth label and the predicted probabilities:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In *Figure 14.20*, you can see 10 example images along with their ground truth
    labels and the probabilities that they belong to class 1, smile:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_14_20.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: Image labels and their probabilities that they belong to class
    1'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities of class 1 (that is, *smile* according to CelebA) are provided
    below each image. As you can see, our trained model is completely accurate on
    this set of 10 test examples.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: As an optional exercise, you are encouraged to try using the entire training
    dataset instead of the small subset we created. Furthermore, you can change or
    modify the CNN architecture. For example, you can change the dropout probabilities
    and the number of filters in the different convolutional layers. Also, you could
    replace the global average-pooling with a fully connected layer. If you are using
    the entire training dataset with the CNN architecture we trained in this chapter,
    you should be able to achieve above 90 percent accuracy.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about CNNs and their main components. We started
    with the convolution operation and looked at 1D and 2D implementations. Then,
    we covered another type of layer that is found in several common CNN architectures:
    the subsampling or so-called pooling layers. We primarily focused on the two most
    common forms of pooling: max-pooling and average-pooling.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Next, putting all these individual concepts together, we implemented deep CNNs
    using the `torch.nn` module. The first network we implemented was applied to the
    already familiar MNIST handwritten digit recognition problem.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Then, we implemented a second CNN on a more complex dataset consisting of face
    images and trained the CNN for smile classification. Along the way, you also learned
    about data augmentation and different transformations that we can apply to face
    images using the `torchvision.transforms` module.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to **recurrent neural networks** (**RNNs**).
    RNNs are used for learning the structure of sequence data, and they have some
    fascinating applications, including language translation and image captioning.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
