- en: '*Chapter 5*: Fine-Tuning Language Models for Text Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to configure a pre-trained model for text
    classification and how to fine-tune it to any text classification downstream task,
    such as sentiment analysis or multi-class classification. We will also discuss
    how to handle sentence-pair and regression problems by covering an implementation.
    We will work with well-known datasets such as GLUE, as well as our own custom
    datasets. We will then take advantage of the Trainer class, which deals with the
    complexity of processes for training and fine-tuning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: First, we will learn how to fine-tune single-sentence binary sentiment classification
    with the Trainer class. Then, we will train for sentiment classification with
    native PyTorch without the Trainer class. In multi-class classification, more
    than two classes will be taken into consideration. We will have seven class classification
    fine-tuning tasks to perform. Finally, we will train a text regression model to
    predict numerical values with sentence pairs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to text classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the BERT model for single-sentence binary classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a classification model with native PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning BERT for multi-class classification with custom datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning BERT for sentence-pair regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing `run_glue.py` to fine-tune the models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using Jupyter Notebook to run our coding exercises. You will need
    Python 3.6+ for this. Ensure that the following packages are installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers 4.0+
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasets`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the notebooks for the coding exercises in this chapter will be available
    at the following GitHub link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH05](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH05).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3y5Fe6R](https://bit.ly/3y5Fe6R)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to text classification
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification (also known as text categorization) is a way of mapping
    a document (sentence, Twitter post, book chapter, email content, and so on) to
    a category out of a predefined list (classes). In the case of two classes that
    have positive and negative labels, we call this **binary classification** – more
    specifically, **sentiment analysis**. For more than two classes, we call this
    **multi-class classification**, where the classes are mutually exclusive, or **multi-label
    classification**, where the classes are not mutually exclusive, which means a
    document can receive more than one label. For instance, the content of a news
    article may be related to sport and politics at the same time. Beyond this classification,
    we may want to score the documents in a range of [-1,1] or rank them in a range
    of [1-5]. We can solve this kind of problem with a regression model, where the
    type of the output is numeric, not categorical.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, the transformer architecture allows us to efficiently solve these
    problems. For sentence-pair tasks such as document similarity or textual entailment,
    the input is not a single sentence, but rather two sentences, as illustrated in
    the following diagram. We can score to what degree two sentences are semantically
    similar or predict whether they are semantically similar. Another sentence-pair
    task is **textual entailment**, where the problem is defined as multi-class classification.
    Here, two sequences are consumed in the GLUE benchmark: entail/contradict/neutral:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，变换器架构使我们能够高效地解决这些问题。对于句对任务，如文档相似性或文本蕴涵，输入不是单一句子，而是两个句子，如下图所示。我们可以评分两个句子在语义上相似的程度，或者预测它们是否在语义上相似。另一个句对任务是**文本蕴涵**，其中问题定义为多类分类。在
    GLUE 基准测试中，两个序列被消耗：蕴含/矛盾/中性：
- en: '![](img/B17123_05_001.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17123_05_001.jpg)'
- en: Figure 5.1 – Text classification scheme
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 文本分类方案
- en: 'Let''s start our training process by fine-tuning a pre-trained BERT model for
    a common problem: sentiment analysis.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过微调预训练的 BERT 模型开始我们的训练过程，针对一个常见问题：情感分析。
- en: Fine-tuning a BERT model for single-sentence binary classification
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为单句二元分类微调 BERT 模型
- en: 'In this section, we will discuss how to fine-tune a pre-trained BERT model
    for sentiment analysis by using the popular `IMDb sentiment` dataset. Working
    with a GPU will speed up our learning process, but if you do not have such resources,
    you can work with a CPU as well for fine-tuning. Let''s get started:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用流行的`IMDb情感`数据集，通过微调预训练的BERT模型进行情感分析。使用 GPU 可以加快我们的学习过程，但如果您没有这样的资源，您也可以通过
    CPU 进行微调。让我们开始吧：
- en: 'To learn about and save our current device, we can execute the following lines
    of code:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要了解并保存当前设备的信息，我们可以执行以下代码行：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will use the `DistilBertForSequenceClassification` class here, which is
    inherited from the `DistilBert` class, with a special sequence classification
    head at the top. We can utilize this *classification head* to train the classification
    model, where the number of classes is `2` by default:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在这里使用 `DistilBertForSequenceClassification` 类，它是从 `DistilBert` 类继承而来，顶部有一个特殊的序列分类头。我们可以利用这个*分类头*来训练分类模型，其中默认类别数为`2`：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice that two parameters called `id2label` and `label2id` are passed to the
    model to use during inference. Alternatively, we can instantiate a particular
    `config` object and pass it to the model, as follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意传递给模型的两个参数称为`id2label`和`label2id`，用于推理。或者，我们可以实例化一个特定的`config`对象并将其传递给模型，如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s select a popular sentiment classification dataset called `IMDB
    Dataset`. The original dataset consists of two sets of data: 25,000 examples for
    training and 25 examples for testing. We will split the dataset into test and
    validation sets. Note that the examples for the first half of the dataset are
    positive, while the second half''s examples are all negative. We can distribute
    the examples as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们选择一个名为`IMDB Dataset`的流行情感分类数据集。原始数据集包含两组数据：25,000个训练示例和25个测试示例。我们将数据集分成测试集和验证集。请注意，数据集的前一半示例为正面，而后一半的示例都为负面。我们可以按以下方式分布示例：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s check the shape of the dataset:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查数据集的形状：
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can take a small portion of the dataset based on your computational resources.
    For a smaller portion, you should run the following code to select 4,000 examples
    for training, 1,000 for testing, and 1,000 for validation, like so:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以根据计算资源的情况从数据集中取出一小部分。对于较小的部分，您应该运行以下代码，选择4,000个示例进行训练，1,000个进行测试，以及1,000个进行验证，如下所示：
- en: '[PRE5]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can pass these datasets through the `tokenizer` model to make them
    ready for training:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些数据集通过`tokenizer`模型，使它们准备好进行训练：
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s see what the training set looks like. The attention mask and input IDs
    were added to the dataset by the tokenizer so that the BERT model can process:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看训练集的样子。注意力掩码和输入ID是由分词器添加到数据集中的，以便 BERT 模型进行处理：
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B17123_05_002.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17123_05_002.jpg)'
- en: Figure 5.2 – Encoded training dataset
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.2 – 编码后的训练数据集
- en: At this point, the datasets are ready for training and testing. The `Trainer`
    class (`TFTrainer` for TensorFlow) and the `TrainingArguments` class (`TFTrainingArguments`
    for TensorFlow) will help us with much of the training complexity. We will define
    our argument set within the `TrainingArguments` class, which will then be passed
    to the `Trainer` object.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，数据集已准备好用于训练和测试。`Trainer`类（`TFTrainer`用于TensorFlow）和`TrainingArguments`类（`TFTrainingArguments`用于TensorFlow）将帮助我们处理训练的许多复杂性。我们将在`TrainingArguments`类中定义我们的参数集，然后将其传递给`Trainer`对象。
- en: 'Let''s define what each training argument does:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们定义每个训练参数的作用：
- en: '![](img/B17123_05_Table_01.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17123_05_Table_01.jpg)'
- en: Table 1 – Table of different training argument definitions
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表1 - 不同训练参数定义表
- en: 'For more information, please check the API documentation of `TrainingArguments`
    or execute the following code in a Python notebook:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 若要获取更多信息，请查看`TrainingArguments`的API文档，或在Python notebook中执行以下代码：
- en: '[PRE8]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Although deep learning architectures such as LSTM need many epochs, sometimes
    more than 50, for transformer-based fine-tuning, we will typically be satisfied
    with an epoch number of 3 due to transfer learning. Most of the time, this number
    is enough for fine-tuning, as a pre-trained model learns a lot about the language
    during the pre-training phase, which takes about 50 epochs on average. To determine
    the correct number of epochs, we need to monitor training and evaluation loss.
    We will learn how to track training in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152),
    *Attention Visualization and Experiment Tracking*.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然像LSTM这样的深度学习架构需要许多epoch，有时超过50个，但对于基于transformer的微调，由于迁移学习，我们通常会满足于3个epoch的数量。大部分时间，这个数量已经足够进行微调，因为预训练模型在预训练阶段已经学到了很多关于语言的知识，通常需要大约50个epoch。要确定正确的epoch数量，我们需要监控训练和评估损失。我们将学习如何在[*第11章*](B17123_11_Epub_AM.xhtml#_idTextAnchor152)中跟踪训练，*注意力可视化和实验追踪*。
- en: 'This will be enough for many downstream task problems, as we will see here.
    During the training process, our model checkpoints will be saved under the `./MyIMDBModel`
    folder for every 200 steps:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于许多下游任务问题，这将足够用。在训练过程中，我们的模型检查点将被保存在`./MyIMDBModel`文件夹中，每200步保存一次：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Before instantiating a `Trainer` object, we will define the `compute_metrics()`
    method, which helps us monitor the progress of the training in terms of particular
    metrics for whatever we need, such as Precision, RMSE, Pearson correlation, BLEU,
    and so on. Text classification problems (such as sentiment classification or multi-class
    classification) are mostly evaluated with **micro-averaging** or **macro-averaging
    F1**. While the macro-averaging method gives equal weight to each class, micro-averaging
    gives equal weight to each per-text or per-token classification decision. Micro-averaging
    is equal to the ratio of the number of times the model decides correctly to the
    total number of decisions that have been made. On the other hand, the macro-averaging
    method computes the average score of Precision, Recall, and F1 for each class.
    For our classification problem, macro-averaging is more convenient for evaluation
    since we want to give equal weight to each label, as follows:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例化`Trainer`对象之前，我们将定义`compute_metrics()`方法，它可以帮助我们监控训练过程中特定指标的进展，如Precision、RMSE、Pearson相关性、BLEU等。文本分类问题（如情感分类或多类分类）大多使用**微平均**或**宏平均
    F1** 进行评估。而宏平均方法平等对待每个类别，微平均对每个文本或每个标记的分类决策平等对待。微平均等于模型正确决策的次数与总决策次数的比率。而宏平均方法计算每个类别的Precision、Recall和F1的平均分数。对于我们的分类问题，宏平均更方便进行评估，因为我们希望给每个标签平等的权重，如下所示：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are almost ready to start the training process. Now, let''s instantiate
    the `Trainer` object and start it. The `Trainer` class is a very powerful and
    optimized tool for organizing complex training and evaluation processes for PyTorch
    and TensorFlow (`TFTrainer` for TensorFlow) thanks to the `transformers` library:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们几乎已经准备好开始训练过程。现在，让我们实例化`Trainer`对象并启动它。`Trainer`类是一个非常强大和优化的工具，用于组织PyTorch和TensorFlow（`TFTrainer`用于TensorFlow）的复杂训练和评估过程，这得益于`transformers`库：
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can start the training process:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始训练过程：
- en: '[PRE12]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding call starts logging metrics, which we will discuss in more detail
    in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention Visualization
    and Experiment Tracking*. The entire IMDb dataset includes 25,000 training examples.
    With a batch size of 32, we have 25K/32 ~=782 steps, and 2,346 (782 x 3) steps
    to go for 3 epochs, as shown in the following progress bar:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的调用开始记录指标，我们将在[*第11章*](B17123_11_Epub_AM.xhtml#_idTextAnchor152)，*注意力可视化和实验跟踪*中更详细地讨论这些内容。整个IMDb数据集包括25,000个训练示例。使用批量大小为32，我们有25K/32约等于782个步骤，并且对于3个时期还有2,346个步骤（782
    x 3），如下所示的进度条显示：
- en: '![](img/B17123_05_003.jpg)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17123_05_003.jpg)'
- en: Figure 5.3 – The output produced by the Trainer object
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.3 – Trainer对象生成的输出
- en: 'The `Trainer` object keeps the checkpoint whose validation loss is the smallest
    at the end. It selects the checkpoint at step 1,400 since the validation loss
    at this step is the minimum. Let''s evaluate the best checkpoint on three (train/test/validation)
    datasets:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Trainer`对象保留了验证损失最小的检查点。它选择了步骤1,400处的检查点，因为该步骤的验证损失最小。让我们在三个（训练/测试/验证）数据集上评估最佳检查点：'
- en: '[PRE13]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B17123_05_004.jpg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17123_05_004.jpg)'
- en: Figure 5.4 – Classification model's performance on the train/validation/test
    dataset
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.4 – 分类模型在训练/验证/测试数据集上的性能
- en: 'Well done! We have successfully completed the training/testing phase and received
    92.6 accuracy and 92.6 F1 for our macro-average. To monitor your training process
    in more detail, you can call advanced tools such as TensorBoard. These tools parse
    the logs and enable us to track various metrics for comprehensive analysis. We''ve
    already logged the performance and other metrics under the `./logs` folder. Just
    running the `tensorboard` function within our Python notebook will be enough,
    as shown in the following code block (we will discuss TensorBoard and other monitoring
    tools in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention
    Visualization and Experiment Tracking*, in detail):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 干得好！我们成功完成了训练/测试阶段，并获得了92.6的准确度和92.6的宏平均F1值。为了更详细地监视您的训练过程，您可以调用高级工具，如TensorBoard。这些工具会解析日志，并使我们能够跟踪各种指标以进行全面分析。我们已经在`./logs`文件夹下记录了性能和其他指标。只需在我们的Python笔记本中运行`tensorboard`函数就足够了，如下面的代码块所示（我们将在[*第11章*](B17123_11_Epub_AM.xhtml#_idTextAnchor152)中详细讨论TensorBoard和其他监控工具的可视化和实验跟踪）：
- en: '[PRE14]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we will use the model for inference to check if it works properly. Let''s
    define a prediction function to simplify the prediction steps, as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用模型进行推理以检查其是否正常工作。让我们定义一个预测函数来简化预测步骤，如下所示：
- en: '[PRE15]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, run the model for inference:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，运行模型进行推理：
- en: '[PRE16]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'What we got here is `0`, which is a negative. We have already defined which
    ID refers to which label. We can use this mapping scheme to get the label. Alternatively,
    we can simply pass all these boring steps to a dedicated API, namely Pipeline,
    which we are already familiar with. Before instantiating it, let''s save the best
    model for further inference:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里得到的是`0`，表示的是负面。我们已经定义了哪个ID表示哪个标签。我们可以使用这种映射方案来获取标签。或者，我们可以将所有这些乏味的步骤简单地传递给一个专用的API，即Pipeline，这是我们已经熟悉的。在实例化之前，让我们保存最佳模型以进行进一步的推理：
- en: '[PRE17]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The Pipeline API is an easy way to use pre-trained models for inference. We
    load the model from where we saved it and pass it to the Pipeline API, which does
    the rest. We can skip this saving step and instead directly pass our `model` and
    `tokenizer` objects in memory to the Pipeline API. If you do so, you will get
    the same result.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pipeline API是使用预训练模型进行推理的简便方法。我们从保存模型的位置加载模型并将其传递给Pipeline API，其余工作由其完成。我们可以跳过保存步骤，而是直接将`model`和`tokenizer`对象在内存中传递给Pipeline
    API。如果这样做，将获得相同的结果。
- en: 'As shown in the following code, we need to specify the task name argument of
    Pipeline as `sentiment-analysis` when we perform binary classification:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下面的代码所示，当我们执行二元分类时，需要将Pipeline的任务名称参数指定为`sentiment-analysis`：
- en: '[PRE18]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Pipeline knows how to treat the input and somehow learned which ID refers to
    which (`POS` or `NEG`) label. It also yields the class probabilities.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pipeline知道如何处理输入，并某种方式学会了哪个ID表示哪个（`POS`或`NEG`）标签。它还产生类别概率。
- en: Well done! We have fine-tuned a sentiment prediction model for the IMDb dataset
    using the `Trainer` class. In the next section, we will do the same binary classification
    training but with native PyTorch. We will also use a different dataset.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 干得好！我们已经使用`Trainer`类为IMDb数据集微调了情感预测模型。在接下来的部分中，我们将使用原生PyTorch进行相同的二元分类培训。我们还将使用其他数据集。
- en: Training a classification model with native PyTorch
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Trainer` class is very powerful, and we have the HuggingFace team to thank
    for providing such a useful tool. However, in this section, we will fine-tune
    the pre-trained model from scratch to see what happens under the hood. Let''s
    get started:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the model for fine-tuning. We will select `DistilBERT` here
    since it is a small, fast, and cheap version of BERT:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To fine-tune any model, we need to put it into training mode, as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we must load the tokenizer:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Since the `Trainer` class organized the entire process for us, we did not deal
    with optimization and other training settings in the previous IMDb sentiment classification
    exercise. Now, we need to instantiate the optimizer ourselves. Here, we must select
    `AdamW`, which is an implementation of the Adam algorithm but with a weight decay
    fix. Recently, it has been shown that `AdamW` produces better training loss and
    validation loss than models trained with Adam. Hence, it is a widely used optimizer
    within many transformer training processes:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To design the fine-tuning process from scratch, we must understand how to implement
    a single step forward and backpropagation. We can pass a single batch through
    the transformer layer and get the output, which is called `input_ids` and `attention_mask`,
    which were produced by the tokenizer, and computes the loss using ground truth
    labels. As we can see, the output consists of both `loss` and `logits`. Now, `loss.backward()`
    computes the gradient of the tensor by evaluating the model with the inputs and
    labels. `optimizer.step()` performs a single optimization step and updates the
    weight using the gradients that were computed, which is called backpropagation.
    When we put all these lines into a loop shortly, we will also add `optimizer.zero_grad()`,
    which clears the gradient of all the parameters. It is important to call this
    at the beginning of the loop; otherwise, we may accumulate the gradients from
    multiple steps. The second tensor of the output is **logits**. In the context
    of deep learning, the term logits (short for **logistic units**) is the last layer
    of the neural architecture and consists of prediction values as real numbers.
    Logits need to be turned into probabilities by the softmax function in the case
    of classification. Otherwise, they are simply normalized for regression.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we want to manually calculate the loss, we must not pass the labels to the
    model. Due to this, the model only yields the logits and does not calculate the
    loss. In the following example, we are computing the cross-entropy loss manually:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With that, we''ve learned how batch input is fed in the forward direction through
    the network in a single step. Now, it is time to design a loop that iterates over
    the entire dataset in batches to train the model with several epochs. To do so,
    we will start by designing the `Dataset` class. It is a subclass of `torch.Dataset`,
    inherits member variables and functions, and implements `__init__()` and `__getitem()__`
    abstract functions:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这个，我们学会了如何将批量输入通过网络的前向方向在单个步骤中进行传递。现在，是时候设计一个循环，以批量迭代整个数据集来训练模型进行多个epochs。为此，我们将首先设计`Dataset`类。它是`torch.Dataset`的子类，继承成员变量和函数，并实现`__init__()`和`__getitem()__`抽象函数：
- en: '[PRE24]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s fine-tune the model for sentiment analysis by taking another sentiment
    analysis dataset called the SST-2 dataset; that is, **Stanford Sentiment Treebank
    v2** (**SST2**). We will also load the corresponding metric for SST-2 for evaluation,
    as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拿取另一个情感分析数据集SST-2数据集，即**斯坦福情感树库v2**（**SST2**）来对情感分析的模型进行微调。我们还将加载SST-2的相应度量进行评估，如下所示：
- en: '[PRE25]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will extract the sentences and the labels accordingly:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将相应地提取句子和标签：
- en: '[PRE26]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can pass the datasets through the tokenizer and instantiate the `MyDataset`
    object to make the BERT models work with them:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过标记器传递数据集并实例化`MyDataset`对象，使BERT模型可以与它们一起工作：
- en: '[PRE27]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s instantiate a `Dataloader` class that provides an interface to iterate
    through the data samples by loading order. This also helps with batching and memory
    pinning:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实例化一个`Dataloader`类，它提供了通过加载顺序迭代数据样本的接口。这也有助于批处理和内存固定：
- en: '[PRE28]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following lines detect the device and define the `AdamW` optimizer properly:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下行检测设备并适当地定义`AdamW`优化器：
- en: '[PRE29]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'So far, we know how to implement forward propagation, which is where we process
    a batch of examples. Here, batch data is fed in the forward direction through
    the neural network. In a single step, each layer from the first to the final one
    is processed by the batch data, as per the activation function, and is passed
    to the successive layer. To go through the entire dataset in several epochs, we
    designed two nested loops: the outer loop is for the epoch, while the inner loop
    is for the steps for each batch. The inner part is made up of two blocks; one
    is for training, while the other one is for evaluating each epoch. As you may
    have noticed, we called `model.train()` at the first training loop, and when we
    moved the second evaluation block, we called `model.eval()`. This is important
    as we put the model into training and inference mode.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道如何实现前向传播，这是我们处理一批示例的地方。在这里，批量数据通过神经网络的前向方向进行传递。在单个步骤中，每层从第一层到最后一层都由批量数据处理，根据激活函数，传递到下一层。为了在多个epochs中遍历整个数据集，我们设计了两个嵌套循环：外部循环是为了epochs，而内部循环是为了每批次的步骤。内部部分由两个块组成；一个用于训练，另一个用于评估每个epochs。您可能已经注意到，我们在第一个训练循环中调用了`model.train()`，当移动第二个评估块时，我们调用了`model.eval()`。这很重要，因为我们使模型处于训练和推理模式。
- en: 'We have already discussed the inner block. Note that we track the model''s
    performance by means of the corresponding the `metric` object:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经讨论了内部块。注意，我们通过相应的`metric`对象跟踪模型的性能：
- en: '[PRE30]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Well done! We've fine-tuned our model and got around 90.94 accuracy. The remaining
    processes, such as saving, loading, and inference, will be similar to what we
    did with the `Trainer` class.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 做得好！我们已经对模型进行了微调，并获得了大约90.94的准确度。剩下的流程，如保存、加载和推理，将类似于我们在`Trainer`类中所做的。
- en: With that, we are done with binary classification. In the next section, we will
    learn how to implement a model for multi-class classification for a language other
    than English.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们已经完成了二元分类。在下一节中，我们将学习如何为非英语语言实现多类分类模型。
- en: Fine-tuning BERT for multi-class classification with custom datasets
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用自定义数据集对多类分类进行BERT微调
- en: 'In this section, we will fine-tune the Turkish BERT, namely **BERTurk**, to
    perform seven-class classification downstream tasks with a custom dataset. This
    dataset has been compiled from Turkish newspapers and consists of seven categories.
    We will start by getting the dataset. Alternatively, you can find it in this book''s
    GitHub respository or get it from [https://www.kaggle.com/savasy/ttc4900](https://www.kaggle.com/savasy/ttc4900):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对土耳其BERT，即**BERTurk**，进行多类分类下游任务的微调，其自定义数据集包含从土耳其报纸编制的七个类别。我们将从获取数据集开始。或者，您可以在本书的GitHub存储库中找到它，或者从[https://www.kaggle.com/savasy/ttc4900](https://www.kaggle.com/savasy/ttc4900)
    获取它：
- en: 'First, run the following code to get data within a Python notebook:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Start by loading the data:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s organize the IDs and labels with `id2label` and `label2id` to make the
    model figure out which ID refers to which label. We will also pass the number
    of labels, `NUM_LABELS`, to the model to specify the size of a thin classification
    head layer on top of the BERT model:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_005.jpg)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.5 – Text classification dataset – TTC 4900
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s count and plot the number of classes using a pandas object:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As shown in the following diagram, the dataset classes have been fairly distributed:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_006.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.6 – The class distribution
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following execution instantiates a sequence classification model with the
    number of labels (`7`), label ID mappings, and a Turkish BERT model (`dbmdz/bert-base-turkish-uncased`),
    namely BERTurk. To check this, execute the following:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be a summary of the model and is too long to show here. Instead,
    let''s turn our attention to the last layer by using the following code:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You may have noticed that we did not choose `DistilBert` as there is no pre-trained
    *uncased* `DistilBert` for the Turkish language:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, let''s prepare the training (%50), validation (%25), and test (%25) datasets,
    as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following code tokenizes the sentences of three datasets and their tokens
    and converts them into integers (`input_ids`), which are then fed into the BERT
    model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We have already implemented the `MyDataset` class (please see page 14). The
    class inherits from the abstract `Dataset` class by overwriting the `__getitem__`
    and `__len__()` methods, which are expected to return the items and the size of
    the dataset using any data loader, respectively:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We will keep batch size as `16` since we have a relatively small dataset. Notice
    that the other parameters of `TrainingArguments` are almost the same as they were
    for the previous sentiment analysis experiment:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Sentiment analysis and text classification are objects of the same evaluation
    metrics; that is, macro-averaging macro-averaged F1, Precision, and Recall. Therefore,
    we will not define the `compute_metric()` function again. Here is the code for
    instantiating a `Trainer` object:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, let''s start the training process:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_007.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.7 – The output of the Trainer class for text classification
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To check the trained model, we must evaluate the fine-tuned model on three
    dataset splits, as follows. Our best model is fine-tuned at step 300 with a loss
    of 0.28012:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – The text classification model''s performance on the train/validation/test
    dataset ](img/B17123_05_008.jpg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.8 – The text classification model's performance on the train/validation/test
    dataset
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The classification accuracy is around 92.6, while the F1 macro-average is around
    92.5\. In the literature, many approaches have been tested on this Turkish benchmark
    dataset. They mostly followed TF-IDF and linear classifier, word2vec embeddings,
    or an LSTM-based classifier and got around 90.0 F1 at best. Compared to those
    approaches, other than transformer, the fine-tuned BERT model outperforms them.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类准确率约为92.6，而F1宏平均约为92.5。在文献中，许多方法都在这个土耳其基准数据集上进行了测试。它们大多采用TF-IDF和线性分类器、word2vec嵌入，或基于LSTM的分类器，最好的F1也达到了90.0。与这些方法相比，除了transformer，微调的BERT模型表现更佳。
- en: 'As with any other experiment, we can track the experiment via TensorBoard:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与任何其他实验一样，我们可以通过TensorBoard跟踪实验：
- en: '[PRE45]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s design a function that will run the model for inference. If you want
    to see a real label instead of an ID, you can use the `config` object of our model,
    as shown in the following `predict` function:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设计一个运行推理模型的函数。如果你想看到真实标签而不是ID，你可以使用我们模型的`config`对象，如下面的`predict`函数所示：
- en: '[PRE46]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we are ready to call the `predict` function for text classification inference.
    The following code classifies a sentence about a football team:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备调用`predict`函数进行文本分类推理。以下代码对一个关于足球队的句子进行分类：
- en: '[PRE47]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As we can see, the model correctly predicted the sentence as sports (`spor`).
    Now, it is time to save the model and reload it using the `from_pre-trained()`
    function. Here is the code:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所看到的，该模型正确地预测了句子为体育（`spor`）。现在，是时候保存模型并使用`from_pre-trained()`函数重新加载它了。以下是代码：
- en: '[PRE48]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can reload the saved model and run inference with the help of the `pipeline`
    class:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以重新加载已保存的模型，并借助`pipeline`类进行推理：
- en: '[PRE49]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You may have noticed that the task''s name is `sentiment-analysis`. This term
    may be confusing but this argument will actually return `TextClassificationPipeline`
    at the end. Let''s run the pipeline:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可能已经注意到任务的名称是`sentiment-analysis`。这个术语可能令人困惑，但这个参数实际上会返回最终的`TextClassificationPipeline`。让我们运行pipeline：
- en: '[PRE50]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: That's our model! It has predicted successfully.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是我们的模型！它已经成功预测了。
- en: So far, we have implemented two single-sentence tasks; that is, sentiment analysis
    and multi-class classification. In the next section, we will learn how to handle
    sentence-pair input and how to design a regression model with BERT.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了两个单句任务；即情感分析和多类分类。在下一节，我们将学习如何处理句对输入，以及如何使用BERT设计回归模型。
- en: Fine-tuning the BERT model for sentence-pair regression
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为句对回归微调BERT模型
- en: 'The regression model is considered to be for classification, but the last layer
    only contains a single unit. This is not processed by softmax logistic regression
    but normalized. To specify the model and put a single-unit head layer at the top,
    we can either directly pass the `num_labels=1` parameter to the `BERT.from_pre-trained()`
    method or pass this information through a `Config` object. Initially, this needs
    to be copied from the `config` object of the pre-trained model, as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型被认为是用于分类的，但最后一层只包含一个单元。这不是通过softmax logistic回归进行处理，而是进行了归一化。为了指定模型并在顶部放置单单元头层，我们可以直接通过`BERT.from_pre-trained()`方法传递`num_labels=1`参数，或者通过`Config`对象传递此信息。最初，这需要从预训练模型的`config`对象中复制，如下所示：
- en: '[PRE51]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Well, our pre-trained model has a single-unit head layer thanks to the `num_labels=1`
    parameter. Now, we are ready to fine-tune the model with our dataset. Here, we
    will use the **Semantic Textual Similarity-Benchmark** (**STS-B**), which is a
    collection of sentence pairs that have been drawn from a variety of content, such
    as news headlines. Each pair has been annotated with a similarity score from 1
    to 5\. Our task is to fine-tune the BERT model to predict these scores. We will
    evaluate the model using the Pearson/Spearman correlation coefficients while following
    the literature. Let''s get started:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们的预训练模型由于`num_labels=1`参数具有单单元头层。现在，我们准备用我们的数据集对模型进行微调。在这里，我们将使用**语义文本相似性基准**（**STS-B**），它是从各种内容中引用的句对集合，如新闻标题等。每对句子都被注释了从1到5的相似度分数。我们的任务是微调BERT模型以预测这些分数。我们将遵循文献，使用皮尔逊/斯皮尔曼相关系数评估模型。让我们开始吧：
- en: 'The following code loads the data. The original data was splits into three.
    However, the test split has no label so that we can divide the validation data
    into two parts, as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码加载了数据。原始数据被分成了三部分。然而，测试分组没有标签，因此我们可以将验证数据分为两部分，如下所示：
- en: '[PRE52]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s make the `stsb_train` training data neat by wrapping it with pandas:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过pandas将`stsb_train`训练数据整理整齐：
- en: '[PRE53]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here is what the training data looks like:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.9 – STS-B training dataset ](img/B17123_05_009.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.9 – STS-B training dataset
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following code to check the shape of the three sets:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Run the following code to tokenize the datasets:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The tokenizer merges two sentences with a `[SEP]` delimiter and produces single
    `input_ids` and an `attention_mask` for a sentence pair, as shown here:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Encoded training dataset ](img/B17123_05_010.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE57]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Another important difference between the current regression task and the previous
    classification tasks is the design of `compute_metrics`. Here, our evaluation
    metric will be based on the **Pearson Correlation Coefficient** and the **Spearman''s
    Rank Correlation** following the common practice provided in the literature. We
    also provide the **Mean Squared Error** (**MSE**), **Root Mean Square Error**
    (**RMSE**), and **Mean Absolute Error** (**MAE**) metrics, which are commonly
    used, especially for regression models:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, let''s instantiate the `Trainer` object:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Run the training, like so:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Training result for text regression ](img/B17123_05_011.jpg)'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.11 – Training result for text regression
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The best validation loss that''s computed is `0.544973` at step `450`. Let''s
    evaluate the best checkpoint model at that step, as follows:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Regression performance on the training/validation/test dataset
    ](img/B17123_05_012.jpg)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.12 – Regression performance on the training/validation/test dataset
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Pearson and Spearman correlation scores are around 87.54 and 87.28 on the
    test dataset, respectively. We did not get a SoTA result, but we did get a comparable
    result for the STS-B task based on the GLUE Benchmark leaderboard. Please check
    the leaderboard!
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are now ready to run the model for inference. Let''s take the following
    two sentences, which share the same meaning, and pass them to the model:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The following code consumes the negative sentence pair, which means the sentences
    are semantically different:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, we will save the model, as follows:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Well done! We can congratulate ourselves since we have successfully completed
    three tasks: sentiment analysis, multi-class classification, and sentence pair
    regression.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing run_glue.py to fine-tune the models
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have designed a fine-tuning architecture from scratch using both
    native PyTorch and the `Trainer` class. The HuggingFace community also provides
    another powerful script called `run_glue.py` for GLUE benchmark and GLUE-like
    classification downstream tasks. This script can handle and organize the entire
    training/validation process for us. If you want to do quick prototyping, you should
    use this script. It can fine-tune any pre-trained models on the HuggingFace hub.
    We can also feed it with our own data in any format.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Please go to the following link to access the script and to learn more: [https://github.com/huggingface/transformers/tree/master/examples](https://github.com/huggingface/transformers/tree/master/examples).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请转到以下链接获取脚本并了解更多信息：[https://github.com/huggingface/transformers/tree/master/examples](https://github.com/huggingface/transformers/tree/master/examples)。
- en: 'The script can perform nine different GLUE tasks. With the script, we can do
    everything that we have done with the `Trainer` class so far. The task name could
    be one of the following GLUE tasks: `cola`, `sst2`, `mrpc`, `stsb`, `qqp`, `mnli`,
    `qnli`, `rte`, or `wnli`.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本可以执行九种不同的GLUE任务。通过该脚本，我们可以做到目前为止我们使用`Trainer`类所做的一切。任务名称可以是以下GLUE任务之一：`cola`、`sst2`、`mrpc`、`stsb`、`qqp`、`mnli`、`qnli`、`rte`或`wnli`。
- en: 'Here is the script scheme for fine-tuning a model:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是微调模型的脚本方案：
- en: '[PRE65]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The community provides another script called `run_glue_no_trainer.py`. The main
    difference between the original script and this one is that this no-trainer script
    gives us more chances to change the options for the optimizer, or add any customization
    that we want to do.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 社区提供了另一个名为`run_glue_no_trainer.py`的脚本。与原始脚本的主要区别在于，这个无Trainer的脚本给了我们更多改变优化器选项或添加任何自定义的机会。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed how to fine-tune a pre-trained model for any text
    classification downstream task. We fine-tuned the models using sentiment analysis,
    multi-class classification, and sentence-pair classification – more specifically,
    sentence-pair regression. We worked with a well-known IMDb dataset and our own
    custom dataset to train the models. While we took advantage of the `Trainer` class
    to cope with much of the complexity of the processes for training and fine-tuning,
    we learned how to train from scratch with native libraries to understand forward
    propagation and backpropagation with the `transformers` library. To summarize,
    we discussed and conducted fine-tuning single-sentence classification with Trainer,
    sentiment classification with native PyTorch without Trainer, single-sentence
    multi-class classification, and fine-tuning sentence-pair regression.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何针对任何文本分类的下游任务对预训练模型进行微调。我们使用情感分析、多类别分类和句子对分类（具体而言，句子对回归）对模型进行了微调。我们使用了一个著名的IMDb数据集和我们自己的自定义数据集来训练模型。虽然我们利用了`Trainer`类来处理训练和微调过程的复杂性，但我们学会了如何使用原生库从头开始训练，以了解`transformers`库中的前向传播和反向传播。总而言之，我们讨论并进行了使用Trainer进行微调单句分类、使用原生PyTorch进行情感分类、单句多类别分类以及微调句子对回归。
- en: In the next chapter, we will learn how to fine-tune a pre-trained model to any
    token classification downstream task, such as parts-of-speech tagging or named-entity
    recognition.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何针对任何标记分类的下游任务（如词性标注或命名实体识别）对预训练模型进行微调。
