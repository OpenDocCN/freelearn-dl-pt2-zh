- en: '*Chapter 5*: Fine-Tuning Language Models for Text Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to configure a pre-trained model for text
    classification and how to fine-tune it to any text classification downstream task,
    such as sentiment analysis or multi-class classification. We will also discuss
    how to handle sentence-pair and regression problems by covering an implementation.
    We will work with well-known datasets such as GLUE, as well as our own custom
    datasets. We will then take advantage of the Trainer class, which deals with the
    complexity of processes for training and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will learn how to fine-tune single-sentence binary sentiment classification
    with the Trainer class. Then, we will train for sentiment classification with
    native PyTorch without the Trainer class. In multi-class classification, more
    than two classes will be taken into consideration. We will have seven class classification
    fine-tuning tasks to perform. Finally, we will train a text regression model to
    predict numerical values with sentence pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the BERT model for single-sentence binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a classification model with native PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning BERT for multi-class classification with custom datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning BERT for sentence-pair regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing `run_glue.py` to fine-tune the models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using Jupyter Notebook to run our coding exercises. You will need
    Python 3.6+ for this. Ensure that the following packages are installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers 4.0+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasets`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the notebooks for the coding exercises in this chapter will be available
    at the following GitHub link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH05](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH05).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3y5Fe6R](https://bit.ly/3y5Fe6R)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification (also known as text categorization) is a way of mapping
    a document (sentence, Twitter post, book chapter, email content, and so on) to
    a category out of a predefined list (classes). In the case of two classes that
    have positive and negative labels, we call this **binary classification** – more
    specifically, **sentiment analysis**. For more than two classes, we call this
    **multi-class classification**, where the classes are mutually exclusive, or **multi-label
    classification**, where the classes are not mutually exclusive, which means a
    document can receive more than one label. For instance, the content of a news
    article may be related to sport and politics at the same time. Beyond this classification,
    we may want to score the documents in a range of [-1,1] or rank them in a range
    of [1-5]. We can solve this kind of problem with a regression model, where the
    type of the output is numeric, not categorical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, the transformer architecture allows us to efficiently solve these
    problems. For sentence-pair tasks such as document similarity or textual entailment,
    the input is not a single sentence, but rather two sentences, as illustrated in
    the following diagram. We can score to what degree two sentences are semantically
    similar or predict whether they are semantically similar. Another sentence-pair
    task is **textual entailment**, where the problem is defined as multi-class classification.
    Here, two sequences are consumed in the GLUE benchmark: entail/contradict/neutral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17123_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Text classification scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our training process by fine-tuning a pre-trained BERT model for
    a common problem: sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model for single-sentence binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to fine-tune a pre-trained BERT model
    for sentiment analysis by using the popular `IMDb sentiment` dataset. Working
    with a GPU will speed up our learning process, but if you do not have such resources,
    you can work with a CPU as well for fine-tuning. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn about and save our current device, we can execute the following lines
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use the `DistilBertForSequenceClassification` class here, which is
    inherited from the `DistilBert` class, with a special sequence classification
    head at the top. We can utilize this *classification head* to train the classification
    model, where the number of classes is `2` by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that two parameters called `id2label` and `label2id` are passed to the
    model to use during inference. Alternatively, we can instantiate a particular
    `config` object and pass it to the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s select a popular sentiment classification dataset called `IMDB
    Dataset`. The original dataset consists of two sets of data: 25,000 examples for
    training and 25 examples for testing. We will split the dataset into test and
    validation sets. Note that the examples for the first half of the dataset are
    positive, while the second half''s examples are all negative. We can distribute
    the examples as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check the shape of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can take a small portion of the dataset based on your computational resources.
    For a smaller portion, you should run the following code to select 4,000 examples
    for training, 1,000 for testing, and 1,000 for validation, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can pass these datasets through the `tokenizer` model to make them
    ready for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s see what the training set looks like. The attention mask and input IDs
    were added to the dataset by the tokenizer so that the BERT model can process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.2 – Encoded training dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, the datasets are ready for training and testing. The `Trainer`
    class (`TFTrainer` for TensorFlow) and the `TrainingArguments` class (`TFTrainingArguments`
    for TensorFlow) will help us with much of the training complexity. We will define
    our argument set within the `TrainingArguments` class, which will then be passed
    to the `Trainer` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s define what each training argument does:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_Table_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Table 1 – Table of different training argument definitions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For more information, please check the API documentation of `TrainingArguments`
    or execute the following code in a Python notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although deep learning architectures such as LSTM need many epochs, sometimes
    more than 50, for transformer-based fine-tuning, we will typically be satisfied
    with an epoch number of 3 due to transfer learning. Most of the time, this number
    is enough for fine-tuning, as a pre-trained model learns a lot about the language
    during the pre-training phase, which takes about 50 epochs on average. To determine
    the correct number of epochs, we need to monitor training and evaluation loss.
    We will learn how to track training in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152),
    *Attention Visualization and Experiment Tracking*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will be enough for many downstream task problems, as we will see here.
    During the training process, our model checkpoints will be saved under the `./MyIMDBModel`
    folder for every 200 steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before instantiating a `Trainer` object, we will define the `compute_metrics()`
    method, which helps us monitor the progress of the training in terms of particular
    metrics for whatever we need, such as Precision, RMSE, Pearson correlation, BLEU,
    and so on. Text classification problems (such as sentiment classification or multi-class
    classification) are mostly evaluated with **micro-averaging** or **macro-averaging
    F1**. While the macro-averaging method gives equal weight to each class, micro-averaging
    gives equal weight to each per-text or per-token classification decision. Micro-averaging
    is equal to the ratio of the number of times the model decides correctly to the
    total number of decisions that have been made. On the other hand, the macro-averaging
    method computes the average score of Precision, Recall, and F1 for each class.
    For our classification problem, macro-averaging is more convenient for evaluation
    since we want to give equal weight to each label, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are almost ready to start the training process. Now, let''s instantiate
    the `Trainer` object and start it. The `Trainer` class is a very powerful and
    optimized tool for organizing complex training and evaluation processes for PyTorch
    and TensorFlow (`TFTrainer` for TensorFlow) thanks to the `transformers` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can start the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding call starts logging metrics, which we will discuss in more detail
    in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention Visualization
    and Experiment Tracking*. The entire IMDb dataset includes 25,000 training examples.
    With a batch size of 32, we have 25K/32 ~=782 steps, and 2,346 (782 x 3) steps
    to go for 3 epochs, as shown in the following progress bar:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.3 – The output produced by the Trainer object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `Trainer` object keeps the checkpoint whose validation loss is the smallest
    at the end. It selects the checkpoint at step 1,400 since the validation loss
    at this step is the minimum. Let''s evaluate the best checkpoint on three (train/test/validation)
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.4 – Classification model's performance on the train/validation/test
    dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Well done! We have successfully completed the training/testing phase and received
    92.6 accuracy and 92.6 F1 for our macro-average. To monitor your training process
    in more detail, you can call advanced tools such as TensorBoard. These tools parse
    the logs and enable us to track various metrics for comprehensive analysis. We''ve
    already logged the performance and other metrics under the `./logs` folder. Just
    running the `tensorboard` function within our Python notebook will be enough,
    as shown in the following code block (we will discuss TensorBoard and other monitoring
    tools in [*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention
    Visualization and Experiment Tracking*, in detail):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will use the model for inference to check if it works properly. Let''s
    define a prediction function to simplify the prediction steps, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, run the model for inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'What we got here is `0`, which is a negative. We have already defined which
    ID refers to which label. We can use this mapping scheme to get the label. Alternatively,
    we can simply pass all these boring steps to a dedicated API, namely Pipeline,
    which we are already familiar with. Before instantiating it, let''s save the best
    model for further inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Pipeline API is an easy way to use pre-trained models for inference. We
    load the model from where we saved it and pass it to the Pipeline API, which does
    the rest. We can skip this saving step and instead directly pass our `model` and
    `tokenizer` objects in memory to the Pipeline API. If you do so, you will get
    the same result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in the following code, we need to specify the task name argument of
    Pipeline as `sentiment-analysis` when we perform binary classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pipeline knows how to treat the input and somehow learned which ID refers to
    which (`POS` or `NEG`) label. It also yields the class probabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Well done! We have fine-tuned a sentiment prediction model for the IMDb dataset
    using the `Trainer` class. In the next section, we will do the same binary classification
    training but with native PyTorch. We will also use a different dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training a classification model with native PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Trainer` class is very powerful, and we have the HuggingFace team to thank
    for providing such a useful tool. However, in this section, we will fine-tune
    the pre-trained model from scratch to see what happens under the hood. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the model for fine-tuning. We will select `DistilBERT` here
    since it is a small, fast, and cheap version of BERT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To fine-tune any model, we need to put it into training mode, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must load the tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the `Trainer` class organized the entire process for us, we did not deal
    with optimization and other training settings in the previous IMDb sentiment classification
    exercise. Now, we need to instantiate the optimizer ourselves. Here, we must select
    `AdamW`, which is an implementation of the Adam algorithm but with a weight decay
    fix. Recently, it has been shown that `AdamW` produces better training loss and
    validation loss than models trained with Adam. Hence, it is a widely used optimizer
    within many transformer training processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To design the fine-tuning process from scratch, we must understand how to implement
    a single step forward and backpropagation. We can pass a single batch through
    the transformer layer and get the output, which is called `input_ids` and `attention_mask`,
    which were produced by the tokenizer, and computes the loss using ground truth
    labels. As we can see, the output consists of both `loss` and `logits`. Now, `loss.backward()`
    computes the gradient of the tensor by evaluating the model with the inputs and
    labels. `optimizer.step()` performs a single optimization step and updates the
    weight using the gradients that were computed, which is called backpropagation.
    When we put all these lines into a loop shortly, we will also add `optimizer.zero_grad()`,
    which clears the gradient of all the parameters. It is important to call this
    at the beginning of the loop; otherwise, we may accumulate the gradients from
    multiple steps. The second tensor of the output is **logits**. In the context
    of deep learning, the term logits (short for **logistic units**) is the last layer
    of the neural architecture and consists of prediction values as real numbers.
    Logits need to be turned into probabilities by the softmax function in the case
    of classification. Otherwise, they are simply normalized for regression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we want to manually calculate the loss, we must not pass the labels to the
    model. Due to this, the model only yields the logits and does not calculate the
    loss. In the following example, we are computing the cross-entropy loss manually:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we''ve learned how batch input is fed in the forward direction through
    the network in a single step. Now, it is time to design a loop that iterates over
    the entire dataset in batches to train the model with several epochs. To do so,
    we will start by designing the `Dataset` class. It is a subclass of `torch.Dataset`,
    inherits member variables and functions, and implements `__init__()` and `__getitem()__`
    abstract functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s fine-tune the model for sentiment analysis by taking another sentiment
    analysis dataset called the SST-2 dataset; that is, **Stanford Sentiment Treebank
    v2** (**SST2**). We will also load the corresponding metric for SST-2 for evaluation,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will extract the sentences and the labels accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can pass the datasets through the tokenizer and instantiate the `MyDataset`
    object to make the BERT models work with them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s instantiate a `Dataloader` class that provides an interface to iterate
    through the data samples by loading order. This also helps with batching and memory
    pinning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following lines detect the device and define the `AdamW` optimizer properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So far, we know how to implement forward propagation, which is where we process
    a batch of examples. Here, batch data is fed in the forward direction through
    the neural network. In a single step, each layer from the first to the final one
    is processed by the batch data, as per the activation function, and is passed
    to the successive layer. To go through the entire dataset in several epochs, we
    designed two nested loops: the outer loop is for the epoch, while the inner loop
    is for the steps for each batch. The inner part is made up of two blocks; one
    is for training, while the other one is for evaluating each epoch. As you may
    have noticed, we called `model.train()` at the first training loop, and when we
    moved the second evaluation block, we called `model.eval()`. This is important
    as we put the model into training and inference mode.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have already discussed the inner block. Note that we track the model''s
    performance by means of the corresponding the `metric` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Well done! We've fine-tuned our model and got around 90.94 accuracy. The remaining
    processes, such as saving, loading, and inference, will be similar to what we
    did with the `Trainer` class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With that, we are done with binary classification. In the next section, we will
    learn how to implement a model for multi-class classification for a language other
    than English.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning BERT for multi-class classification with custom datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will fine-tune the Turkish BERT, namely **BERTurk**, to
    perform seven-class classification downstream tasks with a custom dataset. This
    dataset has been compiled from Turkish newspapers and consists of seven categories.
    We will start by getting the dataset. Alternatively, you can find it in this book''s
    GitHub respository or get it from [https://www.kaggle.com/savasy/ttc4900](https://www.kaggle.com/savasy/ttc4900):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, run the following code to get data within a Python notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start by loading the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s organize the IDs and labels with `id2label` and `label2id` to make the
    model figure out which ID refers to which label. We will also pass the number
    of labels, `NUM_LABELS`, to the model to specify the size of a thin classification
    head layer on top of the BERT model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.5 – Text classification dataset – TTC 4900
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s count and plot the number of classes using a pandas object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the following diagram, the dataset classes have been fairly distributed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.6 – The class distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following execution instantiates a sequence classification model with the
    number of labels (`7`), label ID mappings, and a Turkish BERT model (`dbmdz/bert-base-turkish-uncased`),
    namely BERTurk. To check this, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be a summary of the model and is too long to show here. Instead,
    let''s turn our attention to the last layer by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You may have noticed that we did not choose `DistilBert` as there is no pre-trained
    *uncased* `DistilBert` for the Turkish language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s prepare the training (%50), validation (%25), and test (%25) datasets,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code tokenizes the sentences of three datasets and their tokens
    and converts them into integers (`input_ids`), which are then fed into the BERT
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have already implemented the `MyDataset` class (please see page 14). The
    class inherits from the abstract `Dataset` class by overwriting the `__getitem__`
    and `__len__()` methods, which are expected to return the items and the size of
    the dataset using any data loader, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will keep batch size as `16` since we have a relatively small dataset. Notice
    that the other parameters of `TrainingArguments` are almost the same as they were
    for the previous sentiment analysis experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sentiment analysis and text classification are objects of the same evaluation
    metrics; that is, macro-averaging macro-averaged F1, Precision, and Recall. Therefore,
    we will not define the `compute_metric()` function again. Here is the code for
    instantiating a `Trainer` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s start the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17123_05_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.7 – The output of the Trainer class for text classification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To check the trained model, we must evaluate the fine-tuned model on three
    dataset splits, as follows. Our best model is fine-tuned at step 300 with a loss
    of 0.28012:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – The text classification model''s performance on the train/validation/test
    dataset ](img/B17123_05_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.8 – The text classification model's performance on the train/validation/test
    dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The classification accuracy is around 92.6, while the F1 macro-average is around
    92.5\. In the literature, many approaches have been tested on this Turkish benchmark
    dataset. They mostly followed TF-IDF and linear classifier, word2vec embeddings,
    or an LSTM-based classifier and got around 90.0 F1 at best. Compared to those
    approaches, other than transformer, the fine-tuned BERT model outperforms them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As with any other experiment, we can track the experiment via TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s design a function that will run the model for inference. If you want
    to see a real label instead of an ID, you can use the `config` object of our model,
    as shown in the following `predict` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to call the `predict` function for text classification inference.
    The following code classifies a sentence about a football team:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, the model correctly predicted the sentence as sports (`spor`).
    Now, it is time to save the model and reload it using the `from_pre-trained()`
    function. Here is the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can reload the saved model and run inference with the help of the `pipeline`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You may have noticed that the task''s name is `sentiment-analysis`. This term
    may be confusing but this argument will actually return `TextClassificationPipeline`
    at the end. Let''s run the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's our model! It has predicted successfully.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, we have implemented two single-sentence tasks; that is, sentiment analysis
    and multi-class classification. In the next section, we will learn how to handle
    sentence-pair input and how to design a regression model with BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the BERT model for sentence-pair regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The regression model is considered to be for classification, but the last layer
    only contains a single unit. This is not processed by softmax logistic regression
    but normalized. To specify the model and put a single-unit head layer at the top,
    we can either directly pass the `num_labels=1` parameter to the `BERT.from_pre-trained()`
    method or pass this information through a `Config` object. Initially, this needs
    to be copied from the `config` object of the pre-trained model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, our pre-trained model has a single-unit head layer thanks to the `num_labels=1`
    parameter. Now, we are ready to fine-tune the model with our dataset. Here, we
    will use the **Semantic Textual Similarity-Benchmark** (**STS-B**), which is a
    collection of sentence pairs that have been drawn from a variety of content, such
    as news headlines. Each pair has been annotated with a similarity score from 1
    to 5\. Our task is to fine-tune the BERT model to predict these scores. We will
    evaluate the model using the Pearson/Spearman correlation coefficients while following
    the literature. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code loads the data. The original data was splits into three.
    However, the test split has no label so that we can divide the validation data
    into two parts, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s make the `stsb_train` training data neat by wrapping it with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is what the training data looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.9 – STS-B training dataset ](img/B17123_05_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.9 – STS-B training dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following code to check the shape of the three sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code to tokenize the datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The tokenizer merges two sentences with a `[SEP]` delimiter and produces single
    `input_ids` and an `attention_mask` for a sentence pair, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Encoded training dataset ](img/B17123_05_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another important difference between the current regression task and the previous
    classification tasks is the design of `compute_metrics`. Here, our evaluation
    metric will be based on the **Pearson Correlation Coefficient** and the **Spearman''s
    Rank Correlation** following the common practice provided in the literature. We
    also provide the **Mean Squared Error** (**MSE**), **Root Mean Square Error**
    (**RMSE**), and **Mean Absolute Error** (**MAE**) metrics, which are commonly
    used, especially for regression models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s instantiate the `Trainer` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the training, like so:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Training result for text regression ](img/B17123_05_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.11 – Training result for text regression
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The best validation loss that''s computed is `0.544973` at step `450`. Let''s
    evaluate the best checkpoint model at that step, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Regression performance on the training/validation/test dataset
    ](img/B17123_05_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5.12 – Regression performance on the training/validation/test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Pearson and Spearman correlation scores are around 87.54 and 87.28 on the
    test dataset, respectively. We did not get a SoTA result, but we did get a comparable
    result for the STS-B task based on the GLUE Benchmark leaderboard. Please check
    the leaderboard!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are now ready to run the model for inference. Let''s take the following
    two sentences, which share the same meaning, and pass them to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code consumes the negative sentence pair, which means the sentences
    are semantically different:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will save the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Well done! We can congratulate ourselves since we have successfully completed
    three tasks: sentiment analysis, multi-class classification, and sentence pair
    regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing run_glue.py to fine-tune the models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have designed a fine-tuning architecture from scratch using both
    native PyTorch and the `Trainer` class. The HuggingFace community also provides
    another powerful script called `run_glue.py` for GLUE benchmark and GLUE-like
    classification downstream tasks. This script can handle and organize the entire
    training/validation process for us. If you want to do quick prototyping, you should
    use this script. It can fine-tune any pre-trained models on the HuggingFace hub.
    We can also feed it with our own data in any format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please go to the following link to access the script and to learn more: [https://github.com/huggingface/transformers/tree/master/examples](https://github.com/huggingface/transformers/tree/master/examples).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script can perform nine different GLUE tasks. With the script, we can do
    everything that we have done with the `Trainer` class so far. The task name could
    be one of the following GLUE tasks: `cola`, `sst2`, `mrpc`, `stsb`, `qqp`, `mnli`,
    `qnli`, `rte`, or `wnli`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the script scheme for fine-tuning a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The community provides another script called `run_glue_no_trainer.py`. The main
    difference between the original script and this one is that this no-trainer script
    gives us more chances to change the options for the optimizer, or add any customization
    that we want to do.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to fine-tune a pre-trained model for any text
    classification downstream task. We fine-tuned the models using sentiment analysis,
    multi-class classification, and sentence-pair classification – more specifically,
    sentence-pair regression. We worked with a well-known IMDb dataset and our own
    custom dataset to train the models. While we took advantage of the `Trainer` class
    to cope with much of the complexity of the processes for training and fine-tuning,
    we learned how to train from scratch with native libraries to understand forward
    propagation and backpropagation with the `transformers` library. To summarize,
    we discussed and conducted fine-tuning single-sentence classification with Trainer,
    sentiment classification with native PyTorch without Trainer, single-sentence
    multi-class classification, and fine-tuning sentence-pair regression.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to fine-tune a pre-trained model to any
    token classification downstream task, such as parts-of-speech tagging or named-entity
    recognition.
  prefs: []
  type: TYPE_NORMAL
