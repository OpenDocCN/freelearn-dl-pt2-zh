["```py\npip install tensorflow-datasets \n```", "```py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds \n```", "```py\nmnist_builder = tfds.builder(\"mnist\")\nmnist_builder.download_and_prepare() \n```", "```py\ninfo = mnist_builder.info\nprint(info) \n```", "```py\ntfds.core.DatasetInfo(\n    name='mnist',\n    version=3.0.1\n    description='The MNIST database of handwritten digits.',\n    homepage='http://yann.lecun.com/exdb/mnist/',\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n    }),\n    total_num_examples=70000,\n    splits={\n        'test': 10000,\n        'train': 60000,\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"@article{lecun2010mnist,\n      title={MNIST handwritten digit database},\n      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n      volume={2},\n      year={2010}\n    }\"\"\",\n    redistribution_info=,\n) \n```", "```py\nmnist_train = mnist_builder.as_dataset(split=\"train\") \n```", "```py\nfig = tfds.show_examples(info, mnist_train) \n```", "```py\nflatten_image = partial(flatten_image, label=True)\n\nfor image, label in mnist_train.map(flatten_image).take(1):\n    plt.imshow(image.numpy().reshape(28,28).astype(np.float32), \n               cmap=plt.get_cmap(\"gray\"))\n    print(\"Label: %d\" % label.numpy()) \n```", "```py\ndef flatten_image(x, label=True):\n    if label:\n        return (tf.divide(tf.dtypes.cast(tf.reshape(x[\"image\"], (1,28*28)), tf.float32), 256.0) , x[\"label\"])\n    else:\n        return (tf.divide(tf.dtypes.cast(tf.reshape(x[\"image\"], (1,28*28)), tf.float32), 256.0))\nfor image, label in mnist_train.map(flatten_image).take(1):\n    plt.imshow(image.numpy().astype(np.float32), cmap=plt.get_cmap(\"gray\"))\n    print(\"Label: %d\" % label.numpy()) \n```", "```py\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nclass RBM(layers.Layer):\n    def __init__(self, number_hidden_units=10, number_visible_units=None, learning_rate=0.1, cd_steps=1):\n        super().__init__()\n        self.number_hidden_units = number_hidden_units\n        self.number_visible_units = number_visible_units\n        self.learning_rate = learning_rate\n        self.cd_steps = cd_steps \n```", "```py\ndef build(self, input_shape):\n    if not self.number_visible_units:\n        self.number_visible_units = input_shape[-1]\n        self.w_rec = self.add_weight(shape=(self.number_visible_units, self.number_hidden_units),\n                          initializer='random_normal',\n                          trainable=True)\n        self.w_gen = self.add_weight(shape=(self.number_hidden_units, self.number_visible_units),\n                           initializer='random_normal',\n                           trainable=True)\n        self.hb = self.add_weight(shape=(self.number_hidden_units, ),\n                           initializer='random_normal',\n                           trainable=True)\n        self.vb = self.add_weight(shape=(self.number_visible_units, ),\n                           initializer='random_normal',\n                           trainable=True) \n```", "```py\ndef forward(self, x):\n    return tf.sigmoid(tf.add(tf.matmul(x, self.w), self.hb))\ndef sample_h(self, x):\n    u_sample = tfp.distributions.Uniform().sample((x.shape[1], \n                                                   self.hb.shape[-1]))\n    return tf.cast((x) > u_sample, tf.float32) \n```", "```py\ndef reverse(self, x):\n    return tf.sigmoid(tf.add(tf.matmul(x, self.w_gen), self.vb))\ndef sample_v(self, x):\n    u_sample = tfp.distributions.Uniform().sample((x.shape[1],\n                                           self.vb.shape[-1]))\n    return tf.cast(self.reverse(x) > u_sample, tf.float32) \n```", "```py\ndef call(self, inputs):\n    return tf.sigmoid(tf.add(tf.matmul(inputs, self.w), self.hb)) \n```", "```py\ndef free_energy(self, x):\n    return -tf.tensordot(x, self.vb, 1)\\\n    -tf.reduce_sum(tf.math.log(1+tf.math.exp(tf.add(tf.matmul(x, self.w), self.hb))), 1) \n```", "```py\ndef reverse_gibbs(self, x):\n    return self.sample_h(self.sample_v(x)) \n```", "```py\ndef cd_update(self, x):\n    with tf.GradientTape(watch_accessed_variables=False) as g:\n        h_sample = self.sample_h(x)\n        for step in range(self.cd_steps):\n            v_sample = tf.constant(self.sample_v(h_sample))\n            h_sample = self.sample_h(v_sample) \n        g.watch(self.w_rec)\n        g.watch(self.hb)\n        g.watch(self.vb)\n        cost = tf.reduce_mean(self.free_energy(x)) - tf.reduce_mean(self.free_energy(v_sample))\n        w_grad, hb_grad, vb_grad = g.gradient(cost, [self.w_rec, self.hb, self.vb])\n        self.w_rec.assign_sub(self.learning_rate * w_grad)\n        self.w_gen = tf.Variable(tf.transpose(self.w_rec)) # force\n                                                           # tieing\n        self.hb.assign_sub(self.learning_rate * hb_grad)\n        self.vb.assign_sub(self.learning_rate * vb_grad)\n        return self.reconstruction_cost(x).numpy() \n```", "```py\ndef reconstruction_cost(self, x):\n        return tf.reduce_mean(\n            tf.reduce_sum(tf.math.add(\n            tf.math.multiply(x,tf.math.log(self.reverse(self.forward(x)))),\ntf.math.multiply(tf.math.subtract(1,x),tf.math.log(tf.math.subtract(1,self.reverse(self.forward(x)))))\n        ), 1),) \n```", "```py\nrbm = RBM(500)\nrbm.build([784])\nnum_epochs=100\ndef train_rbm(rbm=None, data=mnist_train, map_fn=flatten_image, \n              num_epochs=100, tolerance=1e-3, batch_size=32, shuffle_buffer=1024):\n    last_cost = None\n\n    for epoch in range(num_epochs):\n        cost = 0.0\n        count = 0.0\n        for datapoints in data.map(map_fn).shuffle(shuffle_buffer).batch(batch_size):\n            cost += rbm.cd_update(datapoints)\n            count += 1.0\n        cost /= count\n        print(\"epoch: {}, cost: {}\".format(epoch, cost))\n        if last_cost and abs(last_cost-cost) <= tolerance:\n            break\n        last_cost = cost\n\n    return rbm\n\nrbm = train_rbm(rbm, mnist_train, partial(flatten_image, label=False), 100, 0.5, 2000) \n```", "```py\nfig, axarr = plt.subplots(10,10)\nplt.axis('off')\nfor i in range(10):\n    for j in range(10):\n        fig.axes[i*10+j].get_xaxis().set_visible(False)\n        fig.axes[i*10+j].get_yaxis().set_visible(False)\n        axarr[i,j].imshow(rbm.w_rec.numpy()[:,i*10+j].reshape(28,28), cmap=plt.get_cmap(\"gray\")) \n```", "```py\ni=0\nfor image, label in mnist_train.map(flatten_image).batch(1).take(10):\n    plt.figure(i)\n    plt.imshow(rbm.forward_gibbs(image).numpy().reshape(28,28).astype(np.float32), cmap=plt.get_cmap(\"gray\"))\n    i+=1\n    plt.figure(i)\n    plt.imshow(image.numpy().reshape(28,28).astype(np.float32), \n               cmap=plt.get_cmap(\"gray\"))\n    i+=1 \n```", "```py\nclass DBN(tf.keras.Model):\n    def __init__(self, rbm_params=None, name='deep_belief_network', \n                 num_epochs=100, tolerance=1e-3, batch_size=32, shuffle_buffer=1024, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._rbm_params = rbm_params\n        self._rbm_layers = list()\n        self._dense_layers = list()\n        for num, rbm_param in enumerate(rbm_params):\n            self._rbm_layers.append(RBM(**rbm_param))\n            self._rbm_layers[-1].build([rbm_param[\"number_visible_units\"]])\n            if num < len(rbm_params)-1:\n                self._dense_layers.append(\n                    tf.keras.layers.Dense(rbm_param[\"number_hidden_units\"], activation=tf.nn.sigmoid))\n            else:\n                self._dense_layers.append(\n                    tf.keras.layers.Dense(rbm_param[\"number_hidden_units\"], activation=tf.nn.softmax))\n            self._dense_layers[-1].build([rbm_param[\"number_visible_units\"]])\n        self._num_epochs = num_epochs\n        self._tolerance = tolerance\n        self._batch_size = batch_size\n        self._shuffle_buffer = shuffle_buffer \n```", "```py\n# pretraining:\n\n        inputs_layers = []\n        for num in range(len(self._rbm_layers)):\n            if num == 0:\n                inputs_layers.append(inputs)\n                self._rbm_layers[num] = \\\n                    self.train_rbm(self._rbm_layers[num],\n                                   inputs)\n            else:  # pass all data through previous layer\n                inputs_layers.append(inputs_layers[num-1].map(\n                    self._rbm_layers[num-1].forward))\n                self._rbm_layers[num] = \\\n                    self.train_rbm(self._rbm_layers[num],\n                                   inputs_layers[num]) \n```", "```py\ndef train_rbm(self, rbm, inputs,\n              num_epochs, tolerance, batch_size, shuffle_buffer):\n    last_cost = None\n    for epoch in range(num_epochs):\n        cost = 0.0\n        count = 0.0\n        for datapoints in inputs.shuffle(shuffle_buffer).batch(batch_size).take(1):\n            cost += rbm.cd_update(datapoints)\n            count += 1.0\n        cost /= count\n        print(\"epoch: {}, cost: {}\".format(epoch, cost))\n        if last_cost and abs(last_cost-cost) <= tolerance:\n            break\n        last_cost = cost\n    return rbm \n```", "```py\n# wake-sleep:\n\n    for epoch in range(self._num_epochs):\n        # wake pass\n        inputs_layers = []\n        for num, rbm in enumerate(self._rbm_layers):\n            if num == 0:\n                inputs_layers.append(inputs)\n            else:\n                inputs_layers.append(inputs_layers[num-1].map(self._rbm_layers[num-1].forward))\n        for num, rbm in enumerate(self._rbm_layers[:-1]):\n            cost = 0.0\n            count = 0.0\n            for datapoints in inputs_layers[num].shuffle(\n                self._shuffle_buffer).batch(self._batch_size):\n                cost += self._rbm_layers[num].wake_update(datapoints)\n                count += 1.0\n            cost /= count\n            print(\"epoch: {}, wake_cost: {}\".format(epoch, cost)) \n```", "```py\ndef wake_update(self, x):\n    with tf.GradientTape(watch_accessed_variables=False) as g:\n        h_sample = self.sample_h(x)\n        for step in range(self.cd_steps):\n            v_sample = self.sample_v(h_sample)\n            h_sample = self.sample_h(v_sample)\n        g.watch(self.w_gen)\n        g.watch(self.vb)\n        cost = tf.reduce_mean(self.free_energy(x)) - tf.reduce_mean(self.free_energy_reverse(h_sample))\n    w_grad, vb_grad = g.gradient(cost, [self.w_gen, self.vb])\n\n    self.w_gen.assign_sub(self.learning_rate * w_grad)\n    self.vb.assign_sub(self.learning_rate * vb_grad)\n    return self.reconstruction_cost(x).numpy() \n```", "```py\n# top-level associative:\n        self._rbm_layers[-1] = self.train_rbm(self._rbm_layers[-1],\n            inputs_layers[-2].map(self._rbm_layers[-2].forward), \n            num_epochs=self._num_epochs, \n            tolerance=self._tolerance, batch_size=self._batch_size, \n            shuffle_buffer=self._shuffle_buffer) \n```", "```py\nreverse_inputs = inputs_layers[-1].map(self._rbm_layers[-1].forward) \n```", "```py\nreverse_inputs_layers = []\n        for num, rbm in enumerate(self._rbm_layers[::-1]):\n            if num == 0:\n                reverse_inputs_layers.append(reverse_inputs)\n            else:\n                reverse_inputs_layers.append(\n                    reverse_inputs_layers[num-1].map(\n                    self._rbm_layers[len(self._rbm_layers)-num].reverse)) \n```", "```py\nfor num, rbm in enumerate(self._rbm_layers[::-1]):\n            if num > 0:\n                cost = 0.0\n                count = 0.0\n                for datapoints in reverse_inputs_layers[num].shuffle(\n                    self._shuffle_buffer).batch(self._batch_size):\n                    cost += self._rbm_layers[len(self._rbm_layers)-1-num].sleep_update(datapoints)\n                    count += 1.0\n                cost /= count\n                print(\"epoch: {}, sleep_cost: {}\".format(epoch, cost)) \n```", "```py\nfor dense_layer, rbm_layer in zip(dbn._dense_layers, dbn._rbm_layers):\n    dense_layer.set_weights([rbm_layer.w_rec.numpy(), rbm_layer.hb.numpy()] \n```", "```py\ndef call(self, x, training):\n    for dense_layer in self._dense_layers:\n        x = dense_layer(x)\n    return x \n```", "```py\ndbn.compile(loss=tf.keras.losses.CategoricalCrossentropy())\ndbn.fit(x=mnist_train.map(lambda x: flatten_image(x, label=True)).batch(32), ) \n```"]