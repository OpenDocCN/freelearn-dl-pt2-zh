["```py\n!pip install allennlp==1.0.0 allennlp-models==1.0.0 \n```", "```py\n!echo '{\"sentence\": \"Whether or not you're enlightened by any of Derrida's lectures on the other and the self, Derrida is an undeniably fascinating and playful fellow.\"}' | \\\nallennlp predict https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz - \n```", "```py\n\"architectures\": [\n  \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n} \n```", "```py\nprediction:  {\"logits\": [3.646597385406494, -2.9539334774017334], \"probs\": [0.9986421465873718, 0.001357800210826099] \n```", "```py\n\"token_ids\": [0, 5994, 50, 45, 47, 769, 38853, 30, 143, 9, 6113, 10505, 281, 25798, 15, 5, 97, 8, 5, 1403, 2156, 211, 14385, 4347, 16, 41, 35559, 12509, 8, 23317, 2598, 479, 2], \"label\": \"1\", \n```", "```py\n\"tokens\": [\"<s>\", \"\\u0120Whether\", \"\\u0120or\", \"\\u0120not\", \"\\u0120you\", \"\\u0120re\", \"\\u0120enlightened\", \"\\u0120by\", \"\\u0120any\", \"\\u0120of\", \"\\u0120Der\", \"rid\", \"as\", \"\\u0120lectures\", \"\\u0120on\", \"\\u0120the\", \"\\u0120other\", \"\\u0120and\", \"\\u0120the\", \"\\u0120self\", \"\\u0120,\", \"\\u0120D\", \"err\", \"ida\", \"\\u0120is\", \"\\u0120an\", \"\\u0120undeniably\", \"\\u0120fascinating\", \"\\u0120and\", \"\\u0120playful\", \"\\u0120fellow\", \"\\u0120.\", \"</s>\"]} \n```", "```py\n!pip install -q transformers\nfrom transformers import pipeline \n```", "```py\ndef classify(sequence,M):\n   #DistilBertForSequenceClassification(default model)\n    nlp_cls = pipeline('sentiment-analysis')\n    if M==1:\n      print(nlp_cls.model.config)\n    return nlp_cls(sequence) \n```", "```py\nDistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForSequenceClassification\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"finetuning_task\": \"sst-2\",\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"NEGATIVE\",\n    \"1\": \"POSITIVE\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"NEGATIVE\": 0,\n    \"POSITIVE\": 1\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"vocab_size\": 30522\n} \n```", "```py\nseq=3\nif seq==1:\n  sequence=\"The battery on my Model9X phone doesn't last more than 6 hours and I'm unhappy about that.\"\nif seq==2:\n  sequence=\"The battery on my Model9X phone doesn't last more than 6 hours and I'm unhappy about that. I was really mad! I bought a Moel10x and things seem to be better. I'm super satisfied now.\"\nif seq==3:\n  sequence=\"The customer was very unhappy\"\nif seq==4:\n  sequence=\"The customer was very satisfied\"\nprint(sequence)\nM=0 #display model cofiguration=1, default=0\nCS=classify(sequence,M)\nprint(CS) \n```", "```py\n[{'label': 'NEGATIVE', 'score': 0.9997098445892334}] \n```", "```py\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\") \n```", "```py\nresponse = openai.Completion.create(\n  engine=\"davinci\",\n  prompt=\"This is a Sentence sentiment classifier\\nSentence: \\\"The customer was satisfied\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"The customer was not satisfied\\\"\\nSentiment: Negative\\n###\\nSentence: \\\"The service was `![](img/Icon_01.png)`\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"This is the link to the review\\\"\\nSentiment: Neutral\\n###\\nSentence text\\n\\n\\n1\\. \\\"I loved the new Batman movie!\\\"\\n2\\. \\\"I hate it when my phone battery dies\\\"\\n3\\. \\\"My day has been `![](img/Icon_01.png)`\\\"\\n4\\. \\\"This is the link to the article\\\"\\n5\\. \\\"This new music video blew my mind\\\"\\n\\n\\nSentence sentiment ratings:\\n1: Positive\\n2: Negative\\n3: Positive\\n4: Neutral\\n5: Positive\\n\\n\\n###\\nSentence text\\n\\n\\n1\\. \\\"I can't stand this product\\\"\\n2\\. \\\"The service was bad! ![](img/Icon_02.png)\\\"\\n3\\. \\\"Though the customer seemed unhappy she was in fact satisfied but thinking of something else at the time, which gave a false impression\\\"\\n4\\. \\\"The support team was `![](img/Icon_03.png)`![](img/Icon_03.png)\\\"\\n5\\. \\\"Here is the link to the product.\\\"\\n\\n\\nSentence sentiment ratings:\\n\",\n  temperature=0.3,\n  max_tokens=60,\n  top_p=1,\n  frequency_penalty=0,\n  presence_penalty=0,\n  stop=[\"###\"]\n)\nr = (response[\"choices\"][0])\nprint(r[\"text\"]) \n```"]