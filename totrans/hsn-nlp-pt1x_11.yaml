- en: '*Chapter 8*: Building a Chatbot Using Attention-Based Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have ever watched any futuristic sci-fi movies, chances are you will
    have seen a human talk to a robot. Machine-based intelligence has been a long-standing
    feature in works of fiction; however, thanks to recent advances in NLP and deep
    learning, conversations with a computer are no longer a fantasy. While we may
    be many years away from true intelligence, where computers are able to understand
    the meaning of language in the same way that humans do, machines are at least
    capable of holding a basic conversation and giving a rudimentary impression of
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at how to construct sequence-to-sequence
    models to translate sentences from one language into another. A conversational
    chatbot that is capable of basic interactions works in much the same way. When
    we talk to a chatbot, our sentence becomes the input to the model. The output
    is whatever the chatbot chooses to reply with. Therefore, rather than training
    our chatbot to learn how to interpret our input sentence, we are teaching it how
    to respond.
  prefs: []
  type: TYPE_NORMAL
- en: We will expand on our sequence-to-sequence models from the previous chapter,
    adding something called **attention** to our models. This improvement to the sequence-to-sequence
    models means that our model learns where in the input sentence to look to obtain
    the information it needs, rather than using the whole input sentence decision.
    This improvement allows us to create much more efficient sequence-to-sequence
    models with state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The theory of attention within neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing attention within a neural network to construct a chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x).
  prefs: []
  type: TYPE_NORMAL
- en: The theory of attention within neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, in our sequence-to-sequence model for sentence translation
    (with no attention implemented), we used both encoders and decoders. The encoder
    obtained a hidden state from the input sentence, which was a representation of
    our sentence. The decoder then used this hidden state to perform the translation
    steps. A basic graphical illustration of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Graphical representation of sequence-to-sequence models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Graphical representation of sequence-to-sequence models
  prefs: []
  type: TYPE_NORMAL
- en: However, decoding over the entirety of the hidden state is not necessarily the
    most efficient way of using this task. This is because the hidden state represents
    the entirety of the input sentence; however, in some tasks (such as predicting
    the next word in a sentence), we do not need to consider the entirety of the input
    sentence, just the parts that are relevant to the prediction we are trying to
    make. We can show that by using attention within our sequence-to-sequence neural
    network. We can teach our model to only look at the relevant parts of the input
    in order to make its prediction, resulting in a much more efficient and accurate
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: I will be traveling to Paris, the capital city of France, on the 2nd of March.
    My flight leaves from London Heathrow airport and will take approximately one
    hour.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we are training a model to predict the next word in a sentence.
    We can first input the start of the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: The capital city of France is _____.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would expect our model to be able to retrieve the word **Paris**, in this
    case. If we were to use our basic sequence-to-sequence model, we would transform
    our entire input into a hidden state, which our model would then try to extract
    the relevant information out of. This includes all the extraneous information
    about flights. You may notice here that we only need to look at a small part of
    our input sentence in order to identify the relevant information required to complete
    our sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: I will be traveling to **Paris, the capital city of France**, on the 2nd of
    March. My flight leaves from London Heathrow airport and will take approximately
    one hour.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if we can train our model to only use the relevant information within
    the input sentence, we can make more accurate and relevant predictions. We can
    implement **attention** within our networks in order to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of attention mechanisms that we can implement: local
    and global attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing local and global attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two forms of attention that we can implement within our networks are very
    similar, but with subtle key differences. We will start by looking at local attention.
  prefs: []
  type: TYPE_NORMAL
- en: In **local attention**, our model only looks at a few hidden states from the
    encoder. For example, if we are performing a sentence translation task and we
    are calculating the second word in our translation, the model may wish to only
    look at the hidden states from the encoder related to the second word in the input
    sentence. This would mean that our model needs to look at the second hidden state
    from our encoder (*h*2) but maybe also the hidden state before it (*h*1).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see this in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Local attention model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Local attention model
  prefs: []
  type: TYPE_NORMAL
- en: We first start by calculating the aligned position, *p*t, from our final hidden
    state, *h*n. This tells us which hidden states we need to be looking at to make
    our prediction. We then calculate our local weights and apply them to our hidden
    states in order to determine our context vector. These weights may tell us to
    pay more attention to the most relevant hidden state (*h*2) but less attention
    to the preceding hidden state (*h*1).
  prefs: []
  type: TYPE_NORMAL
- en: We then take our context vector and pass it forward to our decoder in order
    to make its prediction. In our non-attention based sequence-to-sequence model,
    we would have only passed our final hidden state, *h*n, forward, but we see here
    that instead, we only consider the relevant hidden states that our model deems
    necessary to make its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **global attention** model works in a very similar way. However, instead
    of only looking at a few of the hidden states, we want to look at all of our model''s
    hidden states—hence the name global. We can see a graphical illustration of a
    global attention layer here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Global attention model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Global attention model
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see in the preceding diagram that although this appears very similar
    to our local attention framework, our model is now looking at all the hidden states
    and calculating the global weights across all of them. This allows our model to
    look at any given part of the input sentence that it considers relevant, instead
    of being limited to a local area determined by the local attention methodology.
    Our model may wish to only look at a small, local area, but this is within the
    capabilities of the model. An easy way to think of the global attention framework
    is that it is essentially learning a mask that only allows through hidden states
    that are relevant to our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Combined model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Combined model
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the preceding diagram that by learning which hidden states to
    pay attention to, our model controls which states are used in the decoding step
    to determine our predicted output. Once we decide which hidden states to pay attention
    to, we can combine them using a number of different methods—either by concatenating
    or taking the weighted dot product.
  prefs: []
  type: TYPE_NORMAL
- en: Building a chatbot using sequence-to-sequence neural networks with attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to illustrate exactly how to implement attention within our
    neural network is to work through an example. We will now go through the steps
    required to build a chatbot from scratch using a sequence-to-sequence model with
    an attention framework applied.
  prefs: []
  type: TYPE_NORMAL
- en: As with all of our other NLP models, our first step is to obtain and process
    a dataset to use to train our model.
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring our dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train our chatbot, we need a dataset of conversations by which our model
    can learn how to respond. Our chatbot will take a line of human-entered input
    and respond to it with a generated sentence. Therefore, an ideal dataset would
    consist of a number of lines of dialogue with appropriate responses. The perfect
    dataset for a task such as this would be actual chat logs from conversations between
    two human users. Unfortunately, this data consists of private information and
    is very hard to come by within the public domain, so for this task, we will be
    using a dataset of movie scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Movie scripts consist of conversations between two or more characters. While
    this data is not naturally in the format we would like it to be in, we can easily
    transform it into the format that we need. Take, for example, a simple conversation
    between two characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 1**: Hello Bethan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 2**: Hello Tom, how are you?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 3**: I''m great thanks, what are you doing this evening?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 4**: I haven''t got anything planned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 5**: Would you like to come to dinner with me?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we need to transform this into input and output pairs of call and response,
    where the input is a line in the script (the call) and the expected output is
    the next line of the script (the response). We can transform a script of *n* lines
    into *n-1* pairs of input/output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Table of input and output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Table of input and output
  prefs: []
  type: TYPE_NORMAL
- en: We can use these input/output pairs to train our network, where the input is
    a proxy for human input and the output is the response that we would expect from
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in building our model is to read this data in and perform all
    the necessary preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Processing our dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fortunately, the dataset provided for this example has already been formatted
    so that each line represents a single input/output pair. We can first read the
    data in and examine some lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Examining the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Examining the dataset
  prefs: []
  type: TYPE_NORMAL
- en: You will first notice that our lines are as expected, as the second half of
    the first line becomes the first half of the next line. We can also note that
    the call and response halves of each line are separated by a tab delimiter (`/t`)
    and that each of our lines is separated by a new line delimiter (`/n`). We will
    have to account for this when we process our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a vocabulary or corpus that contains all the unique
    words within our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the past, our corpus has comprised of several dictionaries consisting of
    the unique words in our corpus and lookups between word and indices. However,
    we can do this in a far more elegant way by creating a vocabulary class that consists
    of all of the elements required:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating our `Vocabulary` class. We initialize this class with
    empty dictionaries—`word2index` and `word2count`. We also initialize the `index2word`
    dictionary with placeholders for our padding tokens, as well as our **Start-of-Sentence**
    (**SOS**) and **End-of-Sentence** (**EOS**) tokens. We keep a running count of
    the number of words in our vocabulary, too (which is 3 to start with as our corpus
    already contains the three tokens mentioned). These are the default values for
    an empty vocabulary; however, they will be populated as we read our data in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create the functions that we will use to populate our vocabulary.
    `addWord` takes a word as input. If this is a new word that is not already in
    our vocabulary, we add this word to our indices, set the count of this word to
    1, and increment the total number of words in our vocabulary by 1\. If the word
    in question is already in our vocabulary, we simply increment the count of this
    word by 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also use the `addSentence` function to apply the `addWord` function to all
    the words within a given sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: One thing we can do to speed up the training of our model is reduce the size
    of our vocabulary. This means that any embedding layers will be much smaller and
    the total number of learned parameters within our model can be fewer. An easy
    way to do this is to remove any low-frequency words from our vocabulary. Any words
    occurring just once or twice in our dataset are unlikely to have huge predictive
    power, and so removing them from our corpus and replacing them with blank tokens
    in our final model could reduce the time taken for our model to train and reduce
    overfitting without having much of a negative impact on our model's predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To remove low-frequency words from our vocabulary, we can implement a `trim`
    function. The function first loops through the word count dictionary and if the
    occurrence of the word is greater than the minimum required count, it is appended
    to a new list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, our indices are rebuilt from the new `words_to_keep` list. We set
    all the indices to their initial empty values and then repopulate them by looping
    through our kept words with the `addWord` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now defined a vocabulary class that can be easily populated with our
    input sentences. Next, we actually need to load in our dataset to create our training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start loading in the data using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step for reading in our data is to perform any necessary steps to
    clean the data and make it more human-readable. We start by converting it from
    Unicode into ASCII format. We can easily use a function to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we want to process our input strings so that they are all in lowercase
    and do not contain any trailing whitespace or punctuation, except the most basic
    characters. We can do this by using a series of regular expressions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we apply this function within a wider function—`readVocs`. This function
    reads our data file into lines and then applies the `cleanString` function to
    every line. It also creates an instance of the `Vocabulary` class that we created
    earlier, meaning this function outputs both our data and vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we filter our input pairs by their maximum length. This is again done
    to reduce the potential dimensionality of our model. Predicting sentences that
    are hundreds of words long would require a very deep architecture. In the interest
    of training time, we want to limit our training data here to instances where the
    input and output are less than 10 words long.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To do this, we create a couple of filter functions. The first one, `filterPair`,
    returns a Boolean value based on whether the current line has an input and output
    length that is less than the maximum length. Our second function, `filterPairs`,
    simply applies this condition to all the pairs within our dataset, only keeping
    the ones that meet this condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we just need to create one final function that applies all the previous
    functions we have put together and run it to create our vocabulary and data pairs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that our input dataset consists of over 200,000 pairs. When we filter
    this to sentences where both the input and output are less than 10 words long,
    this reduces to just 64,000 pairs consisting of 18,000 distinct words:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Value of sentences in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B12365_08_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Value of sentences in the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can print a selection of our processed input/output pairs in order to verify
    that our functions have all worked correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is generated:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Processed input/output pairs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Processed input/output pairs
  prefs: []
  type: TYPE_NORMAL
- en: It appears that we have successfully split our dataset into input and output
    pairs upon which we can train our network.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, before we begin building the model, we must remove the rare words from
    our corpus and data pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Removing rare words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously mentioned, including words that only occur a few times within
    our dataset will increase the dimensionality of our model, increasing our model's
    complexity and the time it will take to train the model. Therefore, it is preferred
    to remove them from our training data to keep our model as streamlined and efficient
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall earlier that we built a `trim` function into our vocabulary,
    which will allow us to remove infrequently occurring words from our vocabulary.
    We can now create a function to remove these rare words and call the `trim` method
    from our vocabulary as our first step. You will see that this removes a large
    percentage of words from our vocabulary, indicating that the majority of the words
    in our vocabulary occur infrequently. This is expected as the distribution of
    words within any language model will follow a long-tail distribution. We will
    use the following steps to remove the words:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first calculate the percentage of words that we will keep within our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Percentage of words to be kept'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B12365_08_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Percentage of words to be kept
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Within this same function, we loop through all the words in the input and output
    sentences. If for a given pair either the input or output sentence has a word
    that isn''t in our new trimmed corpus, we drop this pair from our dataset. We
    print the output and see that even though we have dropped over half of our vocabulary,
    we only drop around 17% of our training pairs. This again reflects how our corpus
    of words is distributed over our individual training pairs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Final value after building our dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_10.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Final value after building our dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our finalized dataset, we need to build some functions that
    transform our dataset into batches of tensors that we can pass to our model.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming sentence pairs to tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We know that our model will not take raw text as input, but rather, tensor
    representations of sentences. We will also not process our sentences one by one,
    but instead in smaller batches. For this, we require both our input and output
    sentences to be transformed into tensors, where the width of the tensor represents
    the size of the batch that we wish to train on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating several helper functions, which we can use to transform
    our pairs into tensors. We first create a `indexFromSentence` function, which
    grabs the index of each word in the sentence from the vocabulary and appends an
    EOS token to the end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Secondly, we create a `zeroPad` function, which pads any tensors with zeroes
    so that all of the sentences within the tensor are effectively the same length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, to generate our input tensor, we apply both of these functions. First,
    we get the indices of our input sentence, then apply padding, and then transform
    the output into `LongTensor`. We will also obtain the lengths of each of our input
    sentences out output this as a tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within our network, our padded tokens should generally be ignored. We don''t
    want to train our model on these padded tokens, so we create a Boolean mask to
    ignore these tokens. To do so, we use a `getMask` function, which we apply to
    our output tensor. This simply returns `1` if the output consists of a word and
    `0` if it consists of a padding token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then apply this to our `outputVar` function. This is identical to the `inputVar`
    function, except that along with the indexed output tensor and the tensor of lengths,
    we also return the Boolean mask of our output tensor. This Boolean mask just returns
    `True` when there is a word within the output tensor and `False` when there is
    a padding token. We also return the maximum length of sentences within our output
    tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, in order to create our input and output batches concurrently, we loop
    through the pairs in our batch and create input and output tensors for both pairs
    using the functions we created previously. We then return all the necessary variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "This function should be all we need to transform our training pairs into tensors\
    \ for training our model. We can validate that this is working correctly by performing\
    \ \La single iteration of our `batch2Train` function on a random selection of\
    \ our data. We set our batch size to `5` and run this once:"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can validate that our input tensor has been created correctly. Note
    how the sentences end with padding (0 tokens) where the sentence length is less
    than the maximum length for the tensor (in this instance, 9). The width of the
    tensor also corresponds to the batch size (in this case, 5):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Input tensor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Input tensor
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also validate the corresponding output data and mask. Notice how the
    `False` values in the mask overlap with the padding tokens (the zeroes) in our
    output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – The target and mask tensors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – The target and mask tensors
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have obtained, cleaned, and transformed our data, we are ready to
    begin training the attention-based model that will form the basis of our chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start, as with our other sequence-to-sequence models, by creating our encoder.
    This will transform the initial tensor representation of our input sentence into
    hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now create the encoder by taking the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all of our PyTorch models, we start by creating an `Encoder` class
    that inherits from `nn.Module`. All the elements here should look familiar to
    the ones used in previous chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create our **Recurrent** **Neural** **Network** (**RNN**) module.
    In this chatbot, we will be using a **Gated Recurrent Unit** (**GRU**) instead
    of the **Long** **Short-Term** **Memory** (**LSTM**) models we saw before. GRUs
    are slightly less complex than LSTMs as although they still control the flow of
    information through the RNN, they don''t have separate forget and update gates
    like the LSTM. We use GRUs in this instance for a few main reasons:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) GRUs have proven to be more computationally efficient as there are fewer
    parameters to learn. This means that our model will train much more quickly with
    GRUs than with LSTMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) GRUs have proven to have similar performance levels over short sequences
    of data as LSTMs. LSTMs are more useful when learning longer sequences of data.
    In this instance we are only using input sentences with 10 words or less, so GRUs
    should produce similar results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) GRUs have proven to be more effective at learning from small datasets than
    LSTMs. As the size of our training data is small relative to the complexity of
    the task we're trying to learn, we should opt to use GRUs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now define our GRU, taking into account the size of our input, the number
    of layers, and whether we should implement dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice here how we implement bidirectionality into our model. You will recall
    from previous chapters that a bidirectional RNN allows us to learn from a sentence
    moving sequentially forward through a sentence, as well as moving sequentially
    backward. This allows us to better capture the context of each word in the sentence
    relative to those that appear before and after it. Bidirectionality in our GRU
    means our encoder looks something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Encoder layout'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B12365_08_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13 – Encoder layout
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We maintain two hidden states, as well as outputs at each step, within our input
    sentence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to create a forward pass for our encoder. We do this by first
    embedding our input sentences and then using the `pack_padded_sequence` function
    on our embeddings. This function "packs" our padded sequence so that all of our
    inputs are of the same length. We then pass out the packed sequences through our
    GRU to perform a forward pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After this, we unpack our padding and sum the GRU outputs. We can then return
    this summed output, along with our final hidden state, to complete our forward
    pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we will move on to creating an attention module in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the attention module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we need to build our attention module, which we will apply to our encoder
    so that we can learn from the relevant parts of the encoder''s output. We will
    do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a class for the attention model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create the `dot_score` function within this class. This function simply
    calculates the dot product of our encoder output with the output of our hidden
    state by our encoder. While there are other ways of transforming these two tensors
    into a single representation, using a dot product is one of the simplest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then use this function within our forward pass. First, calculate the attention
    weights/energies based on the `dot_score` method, then transpose the results,
    and return the softmax transformed probability scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we can use this attention module within our decoder to create an attention-focused
    decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now construct the decoder, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by creating our `DecoderRNN` class, inheriting from `nn.Module` and
    defining the initialization parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create our layers within this module. We will create an embedding layer
    and a corresponding dropout layer. We use GRUs again for our decoder; however,
    this time, we do not need to make our GRU layer bidirectional as we will be decoding
    the output from our encoder sequentially. We will also create two linear layers—one
    regular layer for calculating our output and one layer that can be used for concatenation.
    This layer is twice the width of the regular hidden layer as it will be used on
    two concatenated vectors, each with a length of `hidden_size`. We also initialize
    an instance of our attention module from the last section in order to be able
    to use it within our `Decoder` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining all of our layers, we need to create a forward pass for the
    decoder. Notice how the forward pass will be used one step (word) at a time. We
    start by getting the embedding of the current input word and making a forward
    pass through the GRU layer to get our output and hidden states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use the attention module to get the attention weights from the GRU
    output. These weights are then multiplied by the encoder outputs to effectively
    give us a weighted sum of our attention weights and our encoder output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then concatenate our weighted context vector with the output of our GRU
    and apply a `tanh` function to get out final concatenated output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the final step within our decoder, we simply use this final concatenated
    output to predict the next word and apply a `softmax` function. The forward pass
    finally returns this output, along with the final hidden state. This forward pass
    will be iterated upon, with the next forward pass using the next word in the sentence
    and this new hidden state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have defined our models, we are ready to define the training process
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step of the training process is to define the measure of loss for
    our models. As our input tensors may consist of padded sequences, owing to our
    input sentences all being of different lengths, we cannot simply calculate the
    difference between the true output and the predicted output tensors. To account
    for this, we will define a loss function that applies a Boolean mask over our
    outputs and only calculates the loss of the non-padded tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following function, we can see that we calculate cross-entropy loss
    across the whole output tensors. However, to get the total loss, we only average
    over the elements of the tensor that are selected by the Boolean mask:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the majority of our training, we need two main functions—one function,
    `train()`, which performs training on a single batch of our training data and
    another function, `trainIters()`, which iterates through our whole dataset and
    calls `train()` on each of the individual batches. We start by defining `train()`
    in order to train on a single batch of data. Create the `train()` function, then
    get the gradients to 0, define the device options, and initialize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, perform a forward pass of the inputs and sequence lengths though the
    encoder to get the output and hidden states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create our initial decoder input, starting with SOS tokens for each
    sentence. We then set the initial hidden state of our decoder to be equal to that
    of the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we implement teacher forcing. If you recall from the last chapter, teacher
    forcing, when generating output sequences with some given probability, we use
    the true previous output token rather than the predicted previous output token
    to generate the next word in our output sequence. Using teacher forcing helps
    our model converge much more quickly; however, we must be careful not to make
    the teacher forcing ratio too high or else our model will be too reliant on the
    teacher forcing and will not learn to generate the correct output independently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Determine whether we should use teacher forcing for the current step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, if we do need to implement teacher forcing, run the following code. We
    pass each of our sequence batches through the decoder to obtain our output. We
    then set the next input as the true output (`target`). Finally, we calculate and
    accumulate the loss using our loss function and print this to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we do not implement teacher forcing on a given batch, the procedure is almost
    identical. However, instead of using the true output as the next input into the
    sequence, we use the one generated by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, as with all of our models, the final steps are to perform backpropagation,
    implement gradient clipping, and step through both of our encoder and decoder
    optimizers to update the weights using gradient descent. Remember that we clip
    out gradients in order to prevent the vanishing/exploding gradient problem, which
    was discussed in earlier chapters. Finally, our training step returns our average
    loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as previously stated, we need to create the `trainIters()` function,
    which repeatedly calls our training function on different batches of input data.
    We start by splitting our data into batches using the `batch2Train` function we
    created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a few variables that will allow us to count iterations and keep
    track of the total loss over each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define our training loop. For each iteration, we get a training batch
    from our list of batches. We then extract the relevant fields from our batch and
    run a single training iteration using these parameters. Finally, we add the loss
    from this batch to our overall loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On every iteration, we also make sure we print our progress so far, keeping
    track of how many iterations we have completed and what our loss was for each
    epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the sake of completion, we also need to save our model state after every
    few epochs. This allows us to revisit any historical models we have trained; for
    example, if our model were to begin overfitting, we could revert back to an earlier
    iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have completed all the necessary steps to begin training our model,
    we need to create functions to allow us to evaluate the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the evaluating process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating a chatbot is slightly different from evaluating other sequence-to-sequence
    models. In our text translation task, an English sentence will have one direct
    translation into German. While there may be multiple correct translations, for
    the most part, there is a single correct translation from one language into another.
  prefs: []
  type: TYPE_NORMAL
- en: 'For chatbots, there are multiple different valid outputs. Take the following
    three lines from some conversations with a chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *"Hello"*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output**: *"Hello"*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *"Hello"*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output**: *"Hello. How are you?"*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: "*Hello"*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output**: *"What do you want?"*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have three different responses, each one equally valid as a response.
    Therefore, at each stage of our conversation with our chatbot, there will be no
    single "correct" response. So, evaluation is much more difficult. The most intuitive
    way of testing whether a chatbot produces a valid output is by having a conversation
    with it! This means we need to set up our chatbot in a way that enables us to
    have a conversation with it to determine whether it is working well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining a class that will allow us to decode the encoded
    input and produce text. We do this by using what is known as a `GreedyEncoder()`
    class with our pretrained encoder and decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a forward pass for our decoder. We pass the input through our
    encoder to get our encoder''s output and hidden state. We take the encoder''s
    final hidden layer to be the first hidden input to the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create the decoder input with SOS tokens and initialize the tensors to
    append decoded words to (initialized as a single zero value):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, iterate through the sequence, decoding one word at a time. We perform
    a forward pass through the encoder and add a `max` function to obtain the highest-scoring
    predicted word and its score, which we then append to the `all_tokens` and `all_scores`
    variables. Finally, we take this predicted token and use it as the next input
    to our decoder. After the whole sequence has been iterated over, we return the
    complete predicted sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: All the pieces are beginning to come together. We have the defined training
    and evaluation functions, so the final step is to write a function that will actually
    take our input as text, pass it to our model, and obtain a response from the model.
    This will be the "interface" of our chatbot, where we actually have our conversation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We first define an `evaluate()` function, which takes our input function and
    returns the predicted output words. We start by transforming our input sentence
    into indices using our vocabulary. We then obtain a tensor of the lengths of each
    of these sentences and transpose it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we assign our lengths and input tensors to the relevant devices. Next,
    run the inputs through the searcher (`GreedySearchDecoder`) to obtain the word
    indices of the predicted output. Finally, we transform these word indices back
    into word tokens before returning them as the function output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we create a `runchatbot` function, which acts as the interface with
    our chatbot. This function takes human-typed input and prints the chatbot''s response.
    We create this function as a `while` loop that continues until we terminate the
    function or type `quit` as our input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then take the typed input and normalize it, before passing the normalized
    input to our `evaluate()` function, which returns the predicted words from the
    chatbot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we take these output words and format them, ignoring the EOS and padding
    tokens, before printing the chatbot''s response. Because this is a `while` loop,
    this allows us to continue the conversation with the chatbot indefinitely:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have constructed all the functions necessary to train, evaluate,
    and use our chatbot, it's time to begin the final step—training our model and
    conversing with our trained chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have defined all the necessary functions, training the model just becomes
    a case or initializing our hyperparameters and calling our training functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first initialize our hyperparameters. While these are only suggested hyperparameters,
    our models have been set up in a way that will allow them to adapt to whatever
    hyperparameters they are passed. It is good practice to experiment with different
    hyperparameters to see which ones result in an optimal model configuration. Here,
    you could experiment with increasing the number of layers in your encoder and
    decoder, increasing or decreasing the size of the hidden layers, or increasing
    the batch size. All of these hyperparameters will have an effect on how well your
    model learns, as well as a number of other factors, such as the time it takes
    to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we can load our checkpoints. If we have previously trained a model,
    we can load the checkpoints and model states from previous iterations. This saves
    us from having to retrain our model each time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we can begin to build our models. We first load our embeddings
    from the vocabulary. If we have already trained a model, we can load the trained
    embeddings layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then do the same for our encoder and decoder, creating model instances using
    the defined hyperparameters. Again, if we have already trained a model, we simply
    load the trained model states into our models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Last but not least, we specify a device for each of our models to be trained
    on. Remember, this is a crucial step if you wish to use GPU training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If this has all worked correctly and your models have been created with no
    errors, you should see the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Successful output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B12365_08_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.14 – Successful output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have created instances of both our encoder and decoders, we are
    ready to begin training them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We start by initializing some training hyperparameters. In the same way as our
    model hyperparameters, these can be adjusted to influence training time and how
    our model learns. Clip controls the gradient clipping and teacher forcing controls
    how often we use teacher forcing within our model. Notice how we use a teacher
    forcing ratio of 1 so that we always use teacher forcing. Lowering the teaching
    forcing ratio would mean our model takes much longer to converge; however, it
    might help our model generate correct sentences by itself better in the long run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also need to define the learning rates of our models and our decoder learning
    ratio. You will find that your model performs better when the decoder carries
    out larger parameter updates during gradient descent. Therefore, we introduce
    a decoder learning ratio to apply a multiplier to the learning rate so that the
    learning rate is greater for the decoder than it is for the encoder. We also define
    how often our model prints and saves the results, as well as how many epochs we
    want our model to run for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as always when training models in PyTorch, we switch our models to training
    mode to allow the parameters to be updated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create optimizers for both the encoder and decoder. We initialize
    these as Adam optimizers, but other optimizers will work equally well. Experimenting
    with different optimizers may yield different levels of model performance. If
    you have trained a model previously, you can also load the optimizer states if
    required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final step before running the training is to make sure CUDA is configured
    to be called if you wish to use GPU training. To do this, we simply loop through
    the optimizer states for both the encoder and decoder and enable CUDA across all
    of the states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we are ready to train our model. This can be done by simply calling
    the `trainIters` function with all the required parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If this is working correctly, you should see the following output start to
    print:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Training the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.15 – Training the model
  prefs: []
  type: TYPE_NORMAL
- en: 'Your model is now training! Depending on a number of factors, such as how many
    epochs you have set your model to train for and whether you are using a GPU, your
    model may take some time to train. When it is complete, you will see the following
    output. If everything has worked correctly, your model''s average loss will be
    significantly lower than when you started training, showing that your model has
    learned something useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Average loss after 4,000 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.16 – Average loss after 4,000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: Now that our model has been trained, we can begin the evaluation process and
    start using our chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have successfully created and trained our model, it is time to
    evaluate its performance. We will do so by taking the following steps:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To begin the evaluation, we first switch our model into evaluation mode. As
    with all other PyTorch models, this is done to prevent any further parameter updates
    occurring within the evaluation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also initialize an instance of `GreedySearchDecoder` in order to be able
    to perform the evaluation and return the predicted output as text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to run the chatbot, we simply call the `runchatbot` function, passing
    it `encoder`, `decoder`, `searcher`, and `voc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Doing so will open up an input prompt for you to enter your text:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17 – UI element for entering text'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.17 – UI element for entering text
  prefs: []
  type: TYPE_NORMAL
- en: 'Entering your text here and pressing *Enter* will send your input to the chatbot.
    Using our trained model, our chatbot will create a response and print it to the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Output for the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.18 – Output for the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'You can repeat this process as many times as you like to have a "conversation"
    with the chatbot. At a simple conversational level, the chatbot can produce surprisingly
    good results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Output for the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.19 – Output for the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'However, once the conversation gets more complex, it will become obvious that
    the chatbot isn''t capable of the same level of conversation as a human:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Limitations of the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.20 – Limitations of the chatbot
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases, your chatbot''s responses may be nonsensical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Wrong output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B12365_08_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.21 – Wrong output
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that we have created a chatbot capable of simple back and forth
    conversations. However, we still have a long way to go before our chatbot is able
    to pass the Turing test and be able to convince us that we are actually talking
    to a human being. However, considering the relatively small corpus of data we
    have trained our model on, the use of attention in our sequence-to-sequence model
    has shown reasonably good results, demonstrating just how versatile these architectures
    can be.
  prefs: []
  type: TYPE_NORMAL
- en: While the best chatbots are trained on vast corpuses of billions of data points,
    our model has proven reasonably effective with a relatively small one. However,
    basic attention networks are no longer state-of-the-art and in our next chapter,
    we will discuss some of the more recent developments for NLP learning that have
    resulted in even more realistic chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we applied all the knowledge we learned from our recurrent
    models and our sequence-to-sequence models and combined them with an attention
    mechanism to construct a fully working chatbot. While conversing with our chatbot
    is unlikely to be indistinguishable from talking to a real human, with a considerably
    larger dataset we might hope to achieve an even more realistic chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Although sequence-to-sequence models with attention were state-of-the-art in
    2017, machine learning is a rapidly progressing field and since then, there have
    been multiple improvements made to these models. In the final chapter, we will
    discuss some of these state-of-the-art models in more detail, as well as cover
    several other contemporary techniques used in machine learning for NLP, many of
    which are still in development.
  prefs: []
  type: TYPE_NORMAL
