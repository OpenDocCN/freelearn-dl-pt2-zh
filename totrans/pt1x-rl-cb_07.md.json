["```py\n>>> import gym\n>>> import torch\n>>> from torch.autograd import Variable\n>>> import random\n```", "```py\n>>> class DQN():\n ...     def __init__(self, n_state, n_action, n_hidden=50, \n                     lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = torch.nn.Sequential(\n ...                         torch.nn.Linear(n_state, n_hidden),\n ...                         torch.nn.ReLU(),\n ...                         torch.nn.Linear(n_hidden, n_action)\n ...                 )\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>>     def update(self, s, y):\n ...         \"\"\"\n ...         Update the weights of the DQN given a training sample\n ...         @param s: state\n ...         @param y: target value\n ...         \"\"\"\n ...         y_pred = self.model(torch.Tensor(s))\n ...         loss = self.criterion(y_pred, \n                         Variable(torch.Tensor(y)))\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n```", "```py\n>>>     def predict(self, s):\n ...     \"\"\"\n ...     Compute the Q values of the state for all \n              actions using the learning model\n ...     @param s: input state\n ...     @return: Q values of the state for all actions\n ...     \"\"\"\n ...     with torch.no_grad():\n ...          return self.model(torch.Tensor(s))\n```", "```py\n>>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n ...     def policy_function(state):\n ...         if random.random() < epsilon:\n ...             return random.randint(0, n_action - 1)\n ...         else:\n ...             q_values = estimator.predict(state)\n ...             return torch.argmax(q_values).item()\n ...     return policy_function\n```", "```py\n>>> def q_learning(env, estimator, n_episode, gamma=1.0,\n                   epsilon=0.1, epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using DQN\n ...     @param env: Gym environment\n ...     @param estimator: Estimator object\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             modified_reward = next_state[0] + 0.5\n ...             if next_state[0] >= 0.5:\n ...                 modified_reward += 100\n ...             elif next_state[0] >= 0.25:\n ...                 modified_reward += 20\n ...             elif next_state[0] >= 0.1:\n ...                 modified_reward += 10\n ...             elif next_state[0] >= 0:\n ...                 modified_reward += 5\n ...\n ...             q_values = estimator.predict(state).tolist()\n ...\n ...             if is_done:\n ...                 q_values[action] = modified_reward\n ...                 estimator.update(state, q_values)\n ...                 break\n ...             q_values_next = estimator.predict(next_state)\n ...             q_values[action] = modified_reward + gamma * \n                             torch.max(q_values_next).item()\n ...             estimator.update(state, q_values)\n ...             state = next_state\n ...         print('Episode: {}, total reward: {}, epsilon: \n                     {}'.format(episode,\n                     total_reward_episode[episode], epsilon))\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n >>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_hidden = 50\n >>> lr = 0.001\n >>> dqn = DQN(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> n_episode = 1000\n>>> total_reward_episode = [0] * n_episode\n>>> q_learning(env, dqn, n_episode, gamma=.99, epsilon=.3)\n Episode: 0, total reward: -200.0, epsilon: 0.3\n Episode: 1, total reward: -200.0, epsilon: 0.297\n Episode: 2, total reward: -200.0, epsilon: 0.29402999999999996\n ……\n ……\n Episode: 993, total reward: -177.0, epsilon: 0.01\n Episode: 994, total reward: -200.0, epsilon: 0.01\n Episode: 995, total reward: -172.0, epsilon: 0.01\n Episode: 996, total reward: -200.0, epsilon: 0.01\n Episode: 997, total reward: -200.0, epsilon: 0.01\n Episode: 998, total reward: -173.0, epsilon: 0.01\n Episode: 999, total reward: -200.0, epsilon: 0.01\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n >>> import gym\n >>> import torch\n >>> from collections import deque\n >>> import random\n >>> from torch.autograd import Variable >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> def replay(self, memory, replay_size, gamma):\n ...     \"\"\"\n ...     Experience replay\n ...     @param memory: a list of experience\n ...     @param replay_size: the number of samples we use to \n             update the model each time\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     if len(memory) >= replay_size:\n ...         replay_data = random.sample(memory, replay_size)\n ...         states = []\n ...         td_targets = []\n ...         for state, action, next_state, reward, \n                                     is_done in replay_data:\n ...             states.append(state)\n ...             q_values = self.predict(state).tolist()\n ...             if is_done:\n ...                 q_values[action] = reward\n ...             else:\n ...                 q_values_next = self.predict(next_state)\n ...                 q_values[action] = reward + gamma * \n                         torch.max(q_values_next).item()\n ...             td_targets.append(q_values)\n ...\n ...         self.update(states, td_targets)\n```", "```py\n >>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_hidden = 50\n >>> lr = 0.001\n >>> dqn = DQN(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> memory = deque(maxlen=10000)\n```", "```py\n>>> def q_learning(env, estimator, n_episode, replay_size, \n             gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using DQN, with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: Estimator object\n ...     @param replay_size: the number of samples we use to \n                 update the model each time\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             modified_reward = next_state[0] + 0.5\n ...             if next_state[0] >= 0.5:\n ...                 modified_reward += 100\n ...             elif next_state[0] >= 0.25:\n ...                 modified_reward += 20\n ...             elif next_state[0] >= 0.1:\n ...                 modified_reward += 10\n ...             elif next_state[0] >= 0:\n ...                 modified_reward += 5\n ...             memory.append((state, action, next_state, \n                               modified_reward, is_done))\n ...             if is_done:\n ...                 break\n ...             estimator.replay(memory, replay_size, gamma)\n ...             state = next_state\n ...         print('Episode: {}, total reward: {}, epsilon: \n             {}'.format(episode, total_reward_episode[episode],\n              epsilon))\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n>>> n_episode = 600\n```", "```py\n>>> replay_size = 20\n```", "```py\n >>> total_reward_episode = [0] * n_episode\n >>> q_learning(env, dqn, n_episode, replay_size, gamma=.9, epsilon=.3)\n```", "```py\n >>> import matplotlib.pyplot as plt\n >>> plt.plot(total_reward_episode)\n >>> plt.title('Episode reward over time')\n >>> plt.xlabel('Episode')\n >>> plt.ylabel('Total reward')\n >>> plt.show()\n```", "```py\n >>> import gym\n >>> import torch\n >>> from collections import deque\n >>> import random\n >>> import copy\n >>> from torch.autograd import Variable >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> class DQN():\n ...     def __init__(self, n_state, n_action, \n                     n_hidden=50, lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = torch.nn.Sequential(\n ...                         torch.nn.Linear(n_state, n_hidden),\n ...                         torch.nn.ReLU(),\n ...                         torch.nn.Linear(n_hidden, n_action)\n ...                 )\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n ...         self.model_target = copy.deepcopy(self.model)\n```", "```py\n>>>     def target_predict(self, s):\n ...         \"\"\"\n ...         Compute the Q values of the state for all actions \n             using the target network\n ...         @param s: input state\n ...         @return: targeted Q values of the state for all actions\n ...         \"\"\"\n ...         with torch.no_grad():\n ...             return self.model_target(torch.Tensor(s))\n```", "```py\n>>>     def copy_target(self):\n ...         self.model_target.load_state_dict(self.model.state_dict())\n```", "```py\n>>>     def replay(self, memory, replay_size, gamma):\n ...         \"\"\"\n ...         Experience replay with target network\n ...         @param memory: a list of experience\n ...         @param replay_size: the number of samples \n             we use to update the model each time\n ...         @param gamma: the discount factor\n ...         \"\"\"\n ...         if len(memory) >= replay_size:\n ...             replay_data = random.sample(memory, replay_size)\n ...             states = []\n ...             td_targets = []\n ...             for state, action, next_state, reward, is_done \n                                                 in replay_data:\n ...                 states.append(state)\n ...                 q_values = self.predict(state).tolist()\n ...                 if is_done:\n ...                     q_values[action] = reward\n ...                 else:\n ...                     q_values_next = self.target_predict( next_state).detach()\n ...                     q_values[action] = reward + gamma * \n                             torch.max(q_values_next).item()\n ...\n ...                 td_targets.append(q_values)\n ...\n ...             self.update(states, td_targets)\n```", "```py\n >>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_hidden = 50\n >>> lr = 0.01\n >>> dqn = DQN(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> memory = deque(maxlen=10000)\n```", "```py\n>>> def q_learning(env, estimator, n_episode, replay_size, \n         target_update=10, gamma=1.0, epsilon=0.1,\n         epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using double DQN, with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: DQN object\n ...     @param replay_size: number of samples we use \n             to update the model each time\n ...     @param target_update: number of episodes before \n             updating the target network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         if episode % target_update == 0:\n ...             estimator.copy_target()\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             modified_reward = next_state[0] + 0.5\n ...             if next_state[0] >= 0.5:\n ...                 modified_reward += 100\n ...             elif next_state[0] >= 0.25:\n ...                 modified_reward += 20\n ...             elif next_state[0] >= 0.1:\n ...                 modified_reward += 10\n ...             elif next_state[0] >= 0:\n ...                 modified_reward += 5\n ...             memory.append((state, action, next_state, \n                             modified_reward, is_done))\n ...             if is_done:\n ...                 break\n ...             estimator.replay(memory, replay_size, gamma)\n ...             state = next_state\n ...         print('Episode: {}, total reward: {}, epsilon: {}'.format(episode, total_reward_episode[episode], epsilon))\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n>>> n_episode = 1000\n```", "```py\n>>> replay_size = 20\n```", "```py\n>>> target_update = 10\n```", "```py\n >>> total_reward_episode = [0] * n_episode\n >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)\n Episode: 0, total reward: -200.0, epsilon: 1\n Episode: 1, total reward: -200.0, epsilon: 0.99\n Episode: 2, total reward: -200.0, epsilon: 0.9801\n ……\n ……\n Episode: 991, total reward: -151.0, epsilon: 0.01\n Episode: 992, total reward: -200.0, epsilon: 0.01\n Episode: 993, total reward: -158.0, epsilon: 0.01\n Episode: 994, total reward: -160.0, epsilon: 0.01\n Episode: 995, total reward: -200.0, epsilon: 0.01\n Episode: 996, total reward: -200.0, epsilon: 0.01\n Episode: 997, total reward: -200.0, epsilon: 0.01\n Episode: 998, total reward: -151.0, epsilon: 0.01\n Episode: 999, total reward: -200.0, epsilon: 0.01 \n```", "```py\n >>> import matplotlib.pyplot as plt\n >>> plt.plot(total_reward_episode)\n >>> plt.title('Episode reward over time')\n >>> plt.xlabel('Episode')\n >>> plt.ylabel('Total reward')\n >>> plt.show()\n```", "```py\n >>> import gym\n >>> import torch\n >>> from collections import deque\n >>> import random\n >>> import copy\n >>> from torch.autograd import Variable >>> env = gym.envs.make(\"CartPole-v0\")\n```", "```py\n>>> def q_learning(env, estimator, n_episode, replay_size, \n                 target_update=10, gamma=1.0, epsilon=0.1,\n                 epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using double DQN, with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: DQN object\n ...     @param replay_size: number of samples we use to \n                 update the model each time\n ...     @param target_update: number of episodes before \n                 updating the target network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         if episode % target_update == 0:\n ...             estimator.copy_target()\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         state = env.reset()\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             memory.append((state, action, \n                         next_state, reward, is_done))\n ...             if is_done:\n ...                 break\n ...             estimator.replay(memory, replay_size, gamma)\n ...             state = next_state\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n >>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_episode = 600\n >>> last_episode = 200\n```", "```py\n >>> n_hidden_options = [30, 40]\n >>> lr_options = [0.001, 0.003]\n >>> replay_size_options = [20, 25]\n >>> target_update_options = [30, 35]\n```", "```py\n>>> for n_hidden in n_hidden_options:\n ...     for lr in lr_options:\n ...         for replay_size in replay_size_options:\n ...             for target_update in target_update_options:\n ...                 env.seed(1)\n ...                 random.seed(1)\n ...                 torch.manual_seed(1)\n ...                 dqn = DQN(n_state, n_action, n_hidden, lr)\n ...                 memory = deque(maxlen=10000)\n ...                 total_reward_episode = [0] * n_episode\n ...                 q_learning(env, dqn, n_episode, replay_size, \n                         target_update, gamma=.9, epsilon=1)\n ...                 print(n_hidden, lr, replay_size, target_update, \n             sum(total_reward_episode[-last_episode:])/last_episode)\n```", "```py\n30 0.001 20 30 143.15\n 30 0.001 20 35 156.165\n 30 0.001 25 30 180.575\n 30 0.001 25 35 192.765\n 30 0.003 20 30 187.435\n 30 0.003 20 35 122.42\n 30 0.003 25 30 169.32\n 30 0.003 25 35 172.65\n 40 0.001 20 30 136.64\n 40 0.001 20 35 160.08\n 40 0.001 25 30 141.955\n 40 0.001 25 35 122.915\n 40 0.003 20 30 143.855\n 40 0.003 20 35 178.52\n 40 0.003 25 30 125.52\n 40 0.003 25 35 178.85\n```", "```py\n >>> import gym\n >>> import torch\n >>> from collections import deque\n >>> import random\n >>> from torch.autograd import Variable\n >>> import torch.nn as nn >>> env = gym.envs.make(\"MountainCar-v0\")\n```", "```py\n>>> class DuelingModel(nn.Module):\n ...     def __init__(self, n_input, n_output, n_hidden):\n ...         super(DuelingModel, self).__init__()\n ...         self.adv1 = nn.Linear(n_input, n_hidden)\n ...         self.adv2 = nn.Linear(n_hidden, n_output)\n ...         self.val1 = nn.Linear(n_input, n_hidden)\n ...         self.val2 = nn.Linear(n_hidden, 1)\n ...\n ...     def forward(self, x):\n ...         adv = nn.functional.relu(self.adv1(x))\n ...         adv = self.adv2(adv)\n ...         val = nn.functional.relu(self.val1(x))\n ...         val = self.val2(val)\n ...         return val + adv - adv.mean()\n```", "```py\n>>> class DQN():\n ...     def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = DuelingModel(n_state, n_action, n_hidden)\n ...         self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n```", "```py\n >>> n_state = env.observation_space.shape[0]\n >>> n_action = env.action_space.n\n >>> n_hidden = 50\n >>> lr = 0.001\n >>> dqn = DQN(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> memory = deque(maxlen=10000)\n```", "```py\n>>> n_episode = 600\n```", "```py\n>>> replay_size = 20\n```", "```py\n >>> total_reward_episode = [0] * n_episode\n >>> q_learning(env, dqn, n_episode, replay_size, gamma=.9,\n epsilon=.3)\n```", "```py\n >>> import matplotlib.pyplot as plt\n >>> plt.plot(total_reward_episode)\n >>> plt.title('Episode reward over time')\n >>> plt.xlabel('Episode')\n >>> plt.ylabel('Total reward')\n >>> plt.show()\n```", "```py\n >>> import gym\n >>> import torch\n >>> import random >>> env = gym.envs.make(\"PongDeterministic-v4\")\n```", "```py\n >>> state_shape = env.observation_space.shape\n >>> n_action = env.action_space.n\n >>> print(state_shape)\n (210, 160, 3)\n >>> print(n_action)\n 6\n >>> print(env.unwrapped.get_action_meanings())\n ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n```", "```py\n >>> ACTIONS = [0, 2, 3]\n >>> n_action = 3\n```", "```py\n >>> env.reset()\n >>> is_done = False\n >>> while not is_done:\n ...     action = ACTIONS[random.randint(0, n_action - 1)]\n ...     obs, reward, is_done, _ = env.step(action)\n ...     print(reward, is_done)\n ...     env.render()\n 0.0 False\n 0.0 False\n 0.0 False\n ……\n ……\n 0.0 False\n 0.0 False\n 0.0 False\n -1.0 True\n```", "```py\n >>> import torchvision.transforms as T\n >>> from PIL import Image\n >>> image_size = 84\n >>> transform = T.Compose([T.ToPILImage(),\n ...                        T.Grayscale(num_output_channels=1),\n ...                        T.Resize((image_size, image_size),\n                                 interpolation=Image.CUBIC),\n ...                        T.ToTensor(),\n ...                        ])\n```", "```py\n >>> def get_state(obs):\n ...     state = obs.transpose((2, 0, 1))\n ...     state = torch.from_numpy(state)\n ...     state = transform(state)\n ...     return state\n```", "```py\n >>> state = get_state(obs)\n >>> print(state.shape)\n torch.Size([1, 84, 84])\n```", "```py\n >>> from collections import deque \n >>> import copy\n >>> from torch.autograd import Variable\n >>> class DQN():\n ...     def __init__(self, n_state, n_action, n_hidden, lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = torch.nn.Sequential(\n ...                  torch.nn.Linear(n_state, n_hidden[0]),\n ...                  torch.nn.ReLU(),\n ...                  torch.nn.Linear(n_hidden[0], n_hidden[1]),\n ...                  torch.nn.ReLU(),\n ...                  torch.nn.Linear(n_hidden[1], n_action)\n ...                  )\n ...         self.model_target = copy.deepcopy(self.model)\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>> def replay(self, memory, replay_size, gamma):\n ...     \"\"\"\n ...     Experience replay with target network\n ...     @param memory: a list of experience\n ...     @param replay_size: the number of samples we use \n                         to update the model each time\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     if len(memory) >= replay_size:\n ...         replay_data = random.sample(memory, replay_size)\n ...         states = []\n ...         td_targets = []\n ...         for state, action, next_state, reward, \n                                 is_done in replay_data:\n ...             states.append(state.tolist())\n ...             q_values = self.predict(state).tolist()\n ...             if is_done:\n ...                 q_values[action] = reward\n ...             else:\n ...                 q_values_next = self.target_predict( next_state).detach()\n ...                 q_values[action] = reward + gamma * \n                         torch.max(q_values_next).item()\n ...             td_targets.append(q_values)\n ...         self.update(states, td_targets)\n```", "```py\n>>> def q_learning(env, estimator, n_episode, replay_size, \n             target_update=10, gamma=1.0, epsilon=0.1,\n             epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using double DQN, with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: DQN object\n ...     @param replay_size: number of samples we use to \n                             update the model each time\n ...     @param target_update: number of episodes before \n                             updating the target network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         if episode % target_update == 0:\n ...             estimator.copy_target()\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         obs = env.reset()\n ...         state = get_state(obs).view(image_size * image_size)[0]\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_obs, reward, is_done, _ = \n                                 env.step(ACTIONS[action])\n ...             total_reward_episode[episode] += reward\n ...             next_state = get_state(obs).view( image_size * image_size)\n ...             memory.append((state, action, next_state, \n                                 reward, is_done))\n ...             if is_done:\n ...                 break\n ...             estimator.replay(memory, replay_size, gamma)\n ...             state = next_state\n ...         print('Episode: {}, total reward: {}, epsilon: \n             {}'.format(episode, total_reward_episode[episode],\n             epsilon))\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n >>> n_state = image_size * image_size\n >>> n_hidden = [200, 50]\n```", "```py\n >>> n_episode = 1000\n >>> lr = 0.003\n >>> replay_size = 32\n >>> target_update = 10\n```", "```py\n>>> dqn = DQN(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> memory = deque(maxlen=10000)\n```", "```py\n >>> total_reward_episode = [0] * n_episode\n >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)\n```", "```py\n >>> import gym\n >>> import torch\n >>> import random >>> from collections import deque\n >>> import copy\n >>> from torch.autograd import Variable\n >>> import torch.nn as nn\n >>> import torch.nn.functional as F\n >>> env = gym.envs.make(\"PongDeterministic-v4\")\n```", "```py\n >>> ACTIONS = [0, 2, 3]\n >>> n_action = 3\n```", "```py\n >>> import torchvision.transforms as T\n >>> from PIL import Image\n >>> image_size = 84\n >>> transform = T.Compose([T.ToPILImage(),\n ...                        T.Resize((image_size, image_size), \n                              interpolation=Image.CUBIC),\n ...                        T.ToTensor()])\n```", "```py\n>>> def get_state(obs):\n ...     state = obs.transpose((2, 0, 1))\n ...     state = torch.from_numpy(state)\n ...     state = transform(state).unsqueeze(0)\n ...     return state\n```", "```py\n>>> class CNNModel(nn.Module):\n ...     def __init__(self, n_channel, n_action):\n ...         super(CNNModel, self).__init__()\n ...         self.conv1 = nn.Conv2d(in_channels=n_channel, \n                     out_channels=32, kernel_size=8, stride=4)\n ...         self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n ...         self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n ...         self.fc = torch.nn.Linear(7 * 7 * 64, 512)\n ...         self.out = torch.nn.Linear(512, n_action)\n ...\n ...     def forward(self, x):\n ...         x = F.relu(self.conv1(x))\n ...         x = F.relu(self.conv2(x))\n ...         x = F.relu(self.conv3(x))\n ...         x = x.view(x.size(0), -1)\n ...         x = F.relu(self.fc(x))\n ...         output = self.out(x)\n ...         return output\n```", "```py\n>>> class DQN():\n ...     def __init__(self, n_channel, n_action, lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = CNNModel(n_channel, n_action)\n ...         self.model_target = copy.deepcopy(self.model)\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>> def replay(self, memory, replay_size, gamma):\n ...     \"\"\"\n ...     Experience replay with target network\n ...     @param memory: a list of experience\n ...     @param replay_size: the number of samples we use \n                         to update the model each time\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     if len(memory) >= replay_size:\n ...         replay_data = random.sample(memory, replay_size)\n ...         states = []\n ...         td_targets = []\n ...         for state, action, next_state, reward,     \n                                 is_done in replay_data:\n ...             states.append(state.tolist()[0])\n ...             q_values = self.predict(state).tolist()[0]\n ...             if is_done:\n ...                 q_values[action] = reward\n ...             else:\n ...                 q_values_next = self.target_predict( next_state).detach()\n ...                 q_values[action] = reward + gamma *         \n                             torch.max(q_values_next).item()\n ...             td_targets.append(q_values)\n ...         self.update(states, td_targets)\n```", "```py\n >>> def q_learning(env, estimator, n_episode, replay_size, \n             target_update=10, gamma=1.0, epsilon=0.1,   \n             epsilon_decay=.99):\n ...     \"\"\"\n ...     Deep Q-Learning using double DQN, with experience replay\n ...     @param env: Gym environment\n ...     @param estimator: DQN object\n ...     @param replay_size: number of samples we use to \n                             update the model each time\n ...     @param target_update: number of episodes before \n                             updating the target network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     @param epsilon: parameter for epsilon_greedy\n ...     @param epsilon_decay: epsilon decreasing factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         if episode % target_update == 0:\n ...             estimator.copy_target()\n ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)\n ...         obs = env.reset()\n ...         state = get_state(obs)\n ...         is_done = False\n ...         while not is_done:\n ...             action = policy(state)\n ...             next_obs, reward, is_done, _ = \n                             env.step(ACTIONS[action])\n ...             total_reward_episode[episode] += reward\n ...             next_state = get_state(obs)\n ...             memory.append((state, action, next_state, \n                                 reward, is_done))\n ...             if is_done:\n ...                 break\n ...             estimator.replay(memory, replay_size, gamma)\n ...             state = next_state\n ...         print('Episode: {}, total reward: {}, epsilon: {}' .format(episode, total_reward_episode[episode], epsilon))\n ...         epsilon = max(epsilon * epsilon_decay, 0.01)\n```", "```py\n >>> n_episode = 1000\n >>> lr = 0.00025\n >>> replay_size = 32\n >>> target_update = 10\n```", "```py\n >>> dqn = DQN(3, n_action, lr)\n```", "```py\n>>> memory = deque(maxlen=100000)\n```", "```py\n >>> total_reward_episode = [0] * n_episode >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)\n```"]