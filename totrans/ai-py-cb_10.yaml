- en: Natural Language Processing
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) is about analyzing texts and designing
    algorithms to process texts, making predictions from texts, or generating more
    text. NLP covers anything related to language, often including speech similar
    to what we saw in the *R**ecognizing voice commands* recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml), *Deep
    Learning in Audio and Speech*. You might also want to refer to the *Battling algorithmic
    bias* recipe in [Chapter 2](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml), *Advanced
    Topics in Supervised Machine Learning*, or the *Representing for similarity search* recipe
    in [Chapter 3](424f3988-2d11-4098-9c52-beb685a6ed27.xhtml), *Patterns, Outliers,
    and Recommendations*, for more traditional approaches. Most of this chapter will
    deal with the deep learning models behind the breakthroughs in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: Language is often seen to be intrinsically linked to human intelligence, and
    machines mastering communication capabilities have long been seen as closely intertwined
    with the goal of achieving **Artificial General Intelligence** (**AGI**). Alan
    Turing, in his 1950 article *Computing Machinery and Intelligence*, suggested
    a test, since then called the **Turing test**, in which interrogators have to
    find out whether their interlocutor (in a different room) is a computer or a human.
    It has been argued, however, that successfully tricking interrogators into thinking
    they are dealing with humans is not a proof of true understanding (or intelligence),
    but rather of manipulating symbols (the **Chinese room argument**; John Searle, *Minds,
    Brains, and Programs*, 1980). Whichever is the case, in recent years, with the
    availability of parallel computing devices such as GPUs, NLP has been making impressive
    progress in many benchmark tests, for example, in text classification: [http://nlpprogress.com/english/text_classification.html](http://nlpprogress.com/english/text_classification.html).
  prefs: []
  type: TYPE_NORMAL
- en: We'll first do a simple supervised task, where we determine the sentiment of
    paragraphs, then we'll set up an Alexa-style chatbot that responds to commands.
    Next, we'll translate a text using sequence-to-sequence models. Finally, we'll
    attempt to write a popular novel using state-of-the-art text generation models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll be doing these recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying newsgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatting to users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating a text from English to German
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a popular novel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in most chapters so far, we'll try both PyTorch and TensorFlow-based models.
    We'll apply different, more specialized libraries in each recipe.
  prefs: []
  type: TYPE_NORMAL
- en: As always, you can find the recipe notebooks on GitHub: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Classifying newsgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll do a relatively simple supervised task: based on texts,
    we''ll train a model to determine what an article is about, from a selection of
    topics. This is a relatively common task with NLP; we''ll try to give an overview
    of different ways to approach this.'
  prefs: []
  type: TYPE_NORMAL
- en: You might also want to compare the *Battling algorithmic bias* recipe in [Chapter
    2](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml), *Advanced Topics in Supervised
    Machine Learning*, on how to approach this problem using a bag-of-words approach
    (`CountVectorizer` in scikit-learn). In this recipe, we'll be using approaches
    with word embeddings and deep learning models using word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be using scikit-learn and TensorFlow (Keras), as in
    so many other recipes of this book. Additionally, we''ll use word embeddings that
    we''ll have to download, and we''ll use utility functions from the Gensim library
    to apply them in our machine learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll be using a dataset from scikit-learn, but we still need to download
    the word embeddings. We''ll use Facebook''s fastText word embeddings trained on
    Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the download can take a while and should take around 6 GB of
    disk space. If you are running on Colab, you might want to put the embedding file
    into a directory of your Google Drive, so you don't have to download it again
    when you restart your notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The newsgroups dataset is a collection of around 20,000 newsgroup documents
    divided into 20 different groups. The 20 newsgroups collection is a popular dataset
    for testing machine learning techniques in NLP, such as text classification and
    text clustering.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be classifying a selection of newsgroups into three different topics,
    and we'll be approaching this task with three different techniques that we can
    compare. We'll first get the dataset, and then apply a bag-of-words technique,
    using word embeddings, training custom word embeddings in a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll download the dataset using scikit-learn functionality. We''ll
    download the newgroup dataset in two batches, for training and testing, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This conveniently gives us training and test datasets, which we can use in the
    three approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin with covering the first one, using a bag-of-words approach.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll build a pipeline of counting words and reweighing them according to
    their frequency. The final classifier is a random forest. We train this on our
    training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`CountVectorizer` counts tokens in texts and `tfidfTransformer` reweighs the
    counts. We''ll discuss the **term frequency-inverse document frequency** (**TFIDF**)
    reweighting in the *How it works...* section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training, we can test the accuracy on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We get an accuracy of about 0.805\. Let's see how our other two methods will
    do. Using word embeddings is next.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll load up our previously downloaded word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The most straightforward strategy to vectorize a text of several words is to
    average word embeddings across words. This works usually at least reasonably well
    for short texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll apply this vectorization to our dataset and then train a random forest
    classifier on top of these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then test the performance of our approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We get an accuracy of about 0.862.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether our last method does any better than this. We'll build customized
    word embeddings using Keras' embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: Custom word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An embedding layer is a way to create customized word embeddings on the fly
    in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to tell the embedding layer how many words you want to store, how many
    dimensions your word embeddings should have, and how many words are in each text.
    We feed in arrays of integers that each refer to words in a dictionary. We can
    delegate the job of creating the input for the embedding layer to TensorFlow utility
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the dictionary. Now we need to tokenize the text and pad sequences
    to the right length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to build our neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Our model contains half a million parameters. Approximately half of them sit
    in the embedding, and the other half in the feedforward fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit our networks for a few epochs, and then we can test our accuracy on
    the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We get about 0.902 accuracy. We haven't tweaked the model architecture yet.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our newsgroup classification using bag-of-words, pre-trained
    word embeddings, and custom word embeddings. We'll now come to some background.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve classified texts based on three different approaches of featurization:
    bag-of-words, pre-trained word embeddings, and custom word embeddings. Let''s
    briefly delve into word embeddings and TFIDF.'
  prefs: []
  type: TYPE_NORMAL
- en: We've already talked about the Skipgram and the **Continuous Bag of Words**
    (**CBOW**) algorithms in the *Making decisions based on knowledge *recipe in [Chapter
    5](146f9a36-b2f6-4853-9fed-229537c08052.xhtml), *Heuristic Search Techniques and
    Logical Inference* (within the *Graph embedding with Walklets *subsection).
  prefs: []
  type: TYPE_NORMAL
- en: Very briefly, word vectors are a simple machine learning model that can predict
    the next word based on the context (the CBOW algorithm) or can predict the context
    based on a single word (the Skipgram algorithm). Let's quickly look at the CBOW
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The CBOW algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CBOW algorithm is a two-layer feedforward neural network that predicts
    words (rather, the sparse index vector) from their context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1ff8dab-e14b-4ecd-8223-865fec0b449a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This illustration shows how, in the CBOW model, words are predicted based on
    the surrounding context. Here, words are represented as bag-of-words vectors.
    The hidden layer is composed of a weighted average of the context (linear projection).
    The output word is a prediction based on the hidden layer. This is adapted from
    an image on the French-language Wikipedia page on word embeddings: [https://fr.wikipedia.org/wiki/Word_embedding](https://fr.wikipedia.org/wiki/Word_embedding).'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we haven''t talked about is the implication of these word embeddings,
    which created such a stir when they came out. The embeddings are the network activations
    for a single word, and they have a compositional property that gave a title to
    many talks and a few papers. We can combine vectors to do semantic algebra or
    make analogies. The best-known example of this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c86828c-52e3-4874-90d7-981bb90d8792.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, a king and a queen are similar societal positions, only one is
    taken up by a man, the other by a woman. This is reflected in the embedding space
    learned on billions of words. Starting with the vector of king, subtracting the
    vector of man, and finally adding the vector of woman, the closest word that we
    end up at is queen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding space can tell us a lot about how we use language, some of it
    a bit concerning, such as when the word vectors exhibit gender stereotypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1efd0ca5-7f8b-44d1-a001-c2c8a6b5389a.png)'
  prefs: []
  type: TYPE_IMG
- en: This can actually be corrected to some degree using affine transformations as
    shown by Tolga Bolukbasi and others (*Man* *is to Computer Programmer as Woman
    is to Homemaker? Debiasing Word Embeddings*, 2016; [https://arxiv.org/abs/1607.06520](https://arxiv.org/abs/1607.06520)).
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a quick look at the reweighting employed in the bag-of-words approach
    of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: TFIDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the *Bag-of-words* section, we counted words using `CountVectorizer`. This
    gives us a vector of ![](img/44e2f2df-1941-479d-ab2d-16eeb139b557.png), where
    ![](img/fa33ae63-50e8-44fb-9a40-4d3bc9a20d99.png) is the number of words in the
    vocabulary. The vocabulary has to be created during the `fit()` stage of `CountVectorizer`,
    before `transform()` will be able to create the (sparse) vector based on the positions
    of tokens (words) in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: By applying `CountVectorizer` for a number of documents, we get a sparse matrix
    of shape ![](img/43576faf-0270-40f4-bccc-87fb8526b49d.png), where ![](img/dcc5aab4-b1da-471e-bad9-e651b6474bb6.png) is
    the corpus (the collection of documents), and ![](img/647ead55-3682-4f3d-928d-44af1d8bfcd0.png) the
    number of documents. Each position in this matrix accounts for the number of times
    a certain token occurs in a document. In this recipe, a token corresponds to a
    word, however, it can equally be a character or any collection of characters.
  prefs: []
  type: TYPE_NORMAL
- en: Some words might occur in every document; others might occur only in a small
    subset of documents, suggesting they are more specific and precise. That's the
    intuition of TFIDF, where the importance of counts (columns in the matrix) is
    raised if a word frequency across the corpus (the collection of documents) is
    low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse-term given a term for a set of documents ![](img/8562cff9-aa50-416a-9def-f79755765a60.png)
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04c10278-22df-4bc1-b90b-539e0648ba00.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/2686c059-3f84-4c5f-b5f5-efe4a9bae38c.png) is the count of a term
    ![](img/8fc9090d-e14e-4dda-954e-c91c0ba9792b.png) in a document ![](img/14778b5a-a60b-4c39-8904-05e8fb17293d.png),
    and ![](img/9ae48337-e38e-4601-839a-c867bec58164.png) is the number of documents
    where ![](img/3b2b845a-66fa-4808-819c-5b008263395b.png) appears. You should see
    that the TFIDF value decreases with ![](img/e2a4a99e-9466-4b5a-a8ac-78a50e58af85.png). As
    a term occurs in more documents, the logarithm and the TFIDF value approach 0.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipes of this chapter, we'll go beyond the encodings of single
    words and study more complex language models.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll briefly look at learning your own word embeddings using Gensim, building
    more complex deep learning models, and using pre-trained word embeddings in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: We can easily train our own word embeddings on texts in Gensim.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s read in a text file in order to feed it as the training dataset for
    fastText:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This can be useful for transfer learning, search applications, or for cases
    when learning the embeddings would take too long. Using Gensim, this is only a
    few lines of code (adapted from the Gensim documentation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The training itself is straightforward and, since our text file is small, relatively
    quick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can find the *Crime and Punishment* novel at Project Gutenberg, where there
    are many more classic novels: [http://www.gutenberg.org/ebooks/2554](http://www.gutenberg.org/ebooks/2554).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can retrieve vectors from the trained model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Gensim comes with a lot of functionality, and we recommend you have a read through
    some of its documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building more complex deep learning models: for more difficult problems, we
    can use stacked `conv1d` layers on top of the embedding, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Convolutional layers come with very few parameters, which is another advantage
    of using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using pretrained word embeddings in a Keras model: If we want to use downloaded
    (or previously customized word embeddings), we can do this as well. We first need
    to create a dictionary, which we can easily do ourselves after loading them in,
    for example, with Gensim:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then feed these vectors into the embedding layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For training and testing, you have to feed in the word indices by looking them
    up in our new dictionary and pad them to the same length as we've done before.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our recipe on classifying newsgroups. We''ve applied three different
    types of featurization: bag-of-words, a pre-trained word embedding, and a custom
    word embedding in a simple neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We used word embeddings in this recipe. A lot of different embedding methods
    have been introduced, and quite a few word embedding matrices have been published
    that were trained on hundreds of billions of words from many millions of documents. Such
    large-scale training could cost as much as hundreds of thousands of dollars if
    done on rented hardware. The most popular word embeddings are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: GloVe: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText: [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Popular libraries for dealing with word embeddings are these:'
  prefs: []
  type: TYPE_NORMAL
- en: Gensim: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText: [https://fasttext.cc/](https://fasttext.cc/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spaCy: [https://spacy.io/](https://spacy.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Kashgari` is a library built on top of Keras for text labeling and text classification
    and includes Word2vec and more advanced models such as BERT and GPT2 language
    embeddings: [https://github.com/BrikerMan/Kashgari](https://github.com/BrikerMan/Kashgari).'
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face transformer library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers))
    includes many advanced architectures and pre-trained weights for many transformer
    models that can be used for text embedding. These models can achieve state-of-the-art
    performance in many NLP tasks. For instance, companies such as Google have moved
    many of their language applications to the BERT architecture. We'll learn more
    about transformer architectures in the *Translating a text from English to German*
    recipe in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: fast.ai provides a compendium of tutorials and courses on deep learning with
    PyTorch; it includes many resources about NLP as well: [https://nlp.fast.ai/](https://nlp.fast.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in NLP, often there can be thousands or even millions of different
    labels in classification tasks. This has been termed an **eXtreme MultiLabel** (**XML**)
    scenario. You can find a notebook tutorial on XML here: [https://github.com/ppontisso/Extreme-Multi-Label-Classification](https://github.com/ppontisso/Extreme-Multi-Label-Classification).
  prefs: []
  type: TYPE_NORMAL
- en: Chatting to users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 1966, Joseph Weizenbaum published an article about his chatbot ELIZA, called *ELIZA—a
    computer program for the study of natural language communication between man and
    machine*. Created with a sense of humor to show the limitations of technology,
    the chatbot employed simplistic rules and vague, open-ended questions as a way
    of giving an impression of empathic understanding in the conversation, and was
    in an ironic twist often seen as a milestone of artificial intelligence. The field
    has moved on, and today, AI assistants are around us: you might have an Alexa,
    a Google Echo, or any of the other commercial home assistants in the market.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll be building an AI assistant. The difficulty with this
    is that there are an infinite amount of ways for people to express themselves
    and that it is simply impossible to anticipate everything your users might say.
    In this recipe, we'll be training a model to infer what they want and we'll respond
    accordingly in consequence.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we''ll be using a framework developed by Fariz Rahman called **Eywa**.
    We''ll install it with `pip` from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Eywa has the main capabilities of what's expected from a conversational agent,
    and we can look at its code for some of the modeling that's behind its power.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also going to be using the OpenWeatherMap Web API through the `pyOWM`
    library, so we''ll install this library as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With this library, we can request weather data in response to user requests
    as part of our chatbot functionality. If you want to use this in your own chatbot,
    you should register a free user account and get your API key on [OpenWeatherMap.org](https://openweathermap.org/)
    for up to 1,000 requests a day.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we implement this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our agent will process sentences by the user, interpret them, and respond accordingly.
    It will first predict the intent of user queries, and then extract entities in
    order to know more precisely what the query is about, before returning an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the intent classes – based on a few samples of phrases each,
    we''ll define intents such as `greetings`, `taxi`, `weather`, `datetime`, and
    `music`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve created a classifier based on conversation samples. We can quickly test
    how this works using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can successfully predict whether the required action is regarding the weather,
    a hotel booking, music, or about the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the next step, we need to understand whether there''s something more specific
    to the intent, such as the weather in London versus the weather in New York, or
    playing the Beatles versus Kanye West. We can use `eywa` entity extraction for
    this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This is to check for a specific place for the weather prediction. We can test
    the entity extraction for the weather as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We ask for the weather in London, and, in fact, our entity extraction successfully
    comes back with the place name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to code the functionality of our conversational agent, such as
    looking up the weather forecast. Let''s first do a weather request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can request weather forecasts given a location with the Python OpenWeatherMap
    library (`pyOWM`). Calling the new function, `get_weather_forecast()`, with `London`
    as its argument results in this as of in the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Please note that you need to use your own (free) OpenWeatherMap API key if you
    want to execute this.
  prefs: []
  type: TYPE_NORMAL
- en: 'No chatbot is complete without a greeting and the date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create some interaction based on the classifier and entity extraction.
    We''ll write a response function that can greet, tell the date, and give a weather
    forecast:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We are leaving out functionality for calling taxis or playing music:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `question_and_answer()` function answers a user query.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now have a limited conversation with our agent if we ask it questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This wraps up our recipe. We've implemented a simple chatbot that first predicts
    intent and then extracts entities. Based on intent and entities, a user query
    is answered based on rules.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to ask for the date and the weather in different places,
    however, it will tell you to upgrade your software if you ask for taxis or music. You
    should be able to implement and extend this functionality by yourself if you are
    interested.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've implemented a very simple, though effective, chatbot for basic tasks.
    It should be clear how this can be extended and customized for more or other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go through some of this, it might be of interest to look at the ELIZA
    chatbot mentioned in the introduction of this recipe. This will hopefully shed
    some light on what improvements we need to understand a broader set of languages.
  prefs: []
  type: TYPE_NORMAL
- en: How did ELIZA work?
  prefs: []
  type: TYPE_NORMAL
- en: ELIZA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original ELIZA mentioned in the introduction has many statement-response
    pairs, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a match of the regular expression, one of the possible responses is chosen
    randomly, while verbs are transformed if necessary, including contractions, using
    logic like this, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: These are excerpts from Jez Higgins' ELIZA knock-off on GitHub: [https://github.com/jezhiggins/eliza.py](https://github.com/jezhiggins/eliza.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sadly perhaps experiences with call centers might seem similar. They often
    employ scripts as well, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <Greeting>
  prefs: []
  type: TYPE_NORMAL
- en: '"Thank you for calling, my name is _. How can I help you today?"'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '"Do you have any other questions or concerns that I can help you with today?"'
  prefs: []
  type: TYPE_NORMAL
- en: While for machines, in the beginning, it is easier to hardcode some rules, if
    you want to handle more complexity, you'll be building models that interpret intentions
    and references such as locations.
  prefs: []
  type: TYPE_NORMAL
- en: Eywa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Eywa, a framework for conversational agents, comes with three main functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A classifier** – to decide what class the user input belongs to from a choice
    of a few'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An entity extractor** – to extract named entities from sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pattern matching** – for variable matching based on parts of speech and semantic
    meaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All three are very simple to use, though quite powerful. We''ve seen the first
    two functionalities in action in the *How to do it...* section. Let''s see the
    pattern matching for food types based on semantic context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a variable food with sample values: `pizza`, `banana`, `yogurt`,
    and `kebab`. Using food terms in similar contexts will match our variables. The
    expression should return this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The usage looks very similar to regular expressions, however, while regular
    expressions are based on words and their morphology, `eywa.nlu.Pattern` works
    semantically, anchored in word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **regular expression** (short: regex) is a sequence of characters that define
    a search pattern. It was first formalized by Steven Kleene and implemented by
    Ken Thompson and others in Unix tools such as QED, ed, grep, and sed in the 1960s.
    This syntax has entered the POSIX standard and is therefore sometimes referred
    to as **POSIX regular expressions**. A different standard emerged in the late
    1990s with the Perl programming language, termed **Perl Compatible Regular Expressions** (**PCRE**),
    which has been adopted in different programming languages, including Python.'
  prefs: []
  type: TYPE_NORMAL
- en: How do these models work?
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the `eywa` library relies on sense2vec word embeddings from explosion.ai.
    Sense2vec word embeddings were introduced by Andrew Trask and others (*sense2vec
    – A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings*,
    2015). This idea was taken up by explosion.ai, who trained part-of-speech disambiguated
    word embeddings on Reddit discussions. You can read up on these on the explosion.ai
    website: [https://explosion.ai/blog/sense2vec-reloaded](https://explosion.ai/blog/sense2vec-reloaded).'
  prefs: []
  type: TYPE_NORMAL
- en: The classifier goes through the stored conversational items and picks out the
    match with the highest similarity score based on these embeddings. Please note
    that `eywa` has another model implementation based on recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libraries and frameworks abound for creating chatbots with different ideas
    and integrations:'
  prefs: []
  type: TYPE_NORMAL
- en: ParlAI is a library for training and testing dialog models. It comes with more
    than 80 dialog datasets out of the box as well as, integration with Facebook Messenger
    and Mechanical Turk: [https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA has its own toolkit for conversational AI applications and comes with
    many modules providing additional functionality such as automatic speech recognition
    and speech synthesis: [https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Research open sourced their code for an open-domain dialog system: [https://github.com/google-research/google-research/tree/master/meena](https://github.com/google-research/google-research/tree/master/meena).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasa incorporates feedback on every interaction to improve the chatbot: [https://rasa.com/](https://rasa.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chatterbot, a spaCy-based library: [https://spacy.io/universe/project/Chatterbot](https://spacy.io/universe/project/Chatterbot).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating a text from English to German
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll be implementing a transformer network from scratch, and
    we'll be training it for translation tasks from English to German. In the *How
    it works...* section, we'll go through a lot of the details.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We recommend using a machine with a **GPU**. The Colab environment is highly
    recommended, however, please make sure you are using a runtime with GPU enabled.
    If you want to check that you have access to a GPU, you can call the NVIDIA System
    Management Interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This tells you you are using an NVIDIA Tesla T4 with 0 MB of about 1.5 GB used
    (1 MiB corresponds to approximately 1.049 MB).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need a relatively new version of `torchtext`, a library with text datasets
    and utilities for `pytorch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For the part in the *There''s more...* section, you might need to install an
    additional dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using spaCy for tokenization. This comes preinstalled in Colab. In other
    environments, you might have to `pip-install` it. We do need to install the German
    core functionality, such as tokenization for `spacy`, which we''ll rely on in
    this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We'll load up this functionality in the main part of the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be implementing a transformer model from scratch, and
    we'll be training it for a translation task. We've adapted this notebook from Ben
    Trevett's excellent tutorials on implementing a transformer sequence-to-sequence
    model with PyTorch and TorchText: [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll first prepare the dataset, then implement the transformer architecture,
    then we''ll train, and finally test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing the dataset – let''s import all the required modules upfront:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The dataset we'll be training on is the Multi30k dataset. This is a dataset
    of about 30,000 parallel English, German, and French short sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll load the `spacy` functionality and we''ll implement functions to tokenize
    German and English text, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: These functions tokenize German and English text from a string into a list of
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: '`Field` defines operations for converting text to tensors. It provides interfaces
    to common text processing tools and holds a `Vocab` that maps tokens or words
    to a numerical representation. We are passing our preceding tokenization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll create a train-test-validation split from the dataset. The `exts` parameter
    specifies which languages to use as the source and target, and `fields` specifies
    which fields to feed. After that, we build the vocabulary from the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can define our data iterator over the train, validation, and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We can build our transformer architecture now before we train it with this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing the transformer architecture, important parts are the multi-head
    attention and the feedforward connections. Let''s define them, first starting
    with the attention:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The feedforward layer is just a single forward pass with a non-linear activation,
    and a dropout, and a linear read-out. The first projection is much larger than
    the original hidden dimension. In our case, we use a hidden dimension of 512 and
    a `pf` dimension of 2048:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We'll need `Encoder` and `Decoder` parts, each with their own layers. Then we'll
    connect these two with the `Seq2Seq` model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the encoder looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It consists of a number of encoder layers. These look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder is not too different from the encoder, however, it comes with two
    multi-head attention layers. The decoder looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In sequence, the decoder layer does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention with masking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mask in the self-attention layer is to avoid the model including the next
    token in its prediction (which would be cheating).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the decoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it all comes together in the `Seq2Seq` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now instantiate our model with actual parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This whole model comes with 9,543,087 trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training the translation model, we can initialize the parameters using Xavier
    uniform normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to set the learning rate much lower than the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In our loss function, `CrossEntropyLoss`, we have to make sure to ignore padded
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Our training function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The training is then performed in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We are slightly simplifying things here. You can find the full notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: This trains for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model, we'll first have to write functions to encode a sentence
    for the model and decode the model output back to get a sentence. Then we can
    run some sentences and have a look at the translations. Finally, we can calculate
    a metric of the translation performance across the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to translate a sentence, we have to encode it numerically using the
    source vocabulary created before and append stop tokens before feeding this into
    our model. The model output then has to be decoded from the target vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look at an example pair and check the translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compare this with the translation we get from our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This is our translated sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Our translation looks actually better than the original translation. A purse
    is not really a wallet (`geldbörse`), but a small bag (`handtasche`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then calculate a metric, the BLEU score, of our model versus the gold
    standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We get a BLEU score of 33.57, which is not bad while training fewer parameters
    and the training finishes in a matter of a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: In translation, a useful metric is the **Bilingual Evaluation Understudy** (**BLEU**) score,
    where 1 is the best possible value. It is the ratio of parts in the candidate
    translation over parts in a reference translation (gold standard), where parts
    can be single words or a sequence of words (**n-grams**).
  prefs: []
  type: TYPE_NORMAL
- en: This wraps up our translation model. We can see it's actually not that hard
    to create a translation model. However, there's quite a lot of theory, part of
    which we'll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we trained a transformer model from scratch for an English to
    German translation task. Let's look a bit into what a transformer is and how it
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Until not long ago, **Long Short-Term Memory networks** (**LSTMs**) had been
    the prevalent choice for deep learning models, however, since words are processed
    sequentially, training can take a long time to converge. We have seen in previous
    recipes how recurrent neural networks can be used for sequence processing (please
    compare it with the *Generating melodies *recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml),
    *Deep Learning in Audio and Speech*). In yet other recipes, for example, the *Recognizing
    voice commands* recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml),
    *Deep Learning in Audio and Speech*, we discussed how convolutional models have
    been replacing these recurrent networks with an advantage in speed and prediction
    performance. In NLP, convolutional networks have been tried as well (for example, Jonas
    Gehring and others, *Convolutional Sequence to Sequence Learning*, 2017) with
    improvements in speed and prediction performance with regard to recurrent models,
    however, the transformer architecture proved more powerful and still faster.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture was originally created for machine translation
    (Ashish Vaswani and others, *Attention is All you Need*, 2017). Dispensing with
    recurrence and convolutions, transformer networks are much faster to train and
    predict since words are processed in parallel. Transformer architectures provide
    universal language models that have pushed the envelope in a broad set of tasks
    such as **Neural Machine Translation** (**NMT**), **Question Answering** (**QA**),
    **Named-Entity Recognition** (**NER**), **Textual Entailment** (**TE**), abstractive
    text summarization, and other tasks. Transformer models are often taken off the
    shelf and fine-tuned for specific tasks in order to profit from general language
    understanding acquired through a long and expensive training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers come in two parts, similar to an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An encoder** – it encodes the input into a series of context vectors (aka
    hidden states).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A decoder** – it takes the context vector and decodes it into a target representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The differences between the implementation in our recipe and the original transformer
    implementation (Ashish Vaswani and others, *Attention is All you Need*, 2017) is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We use a learned positional encoding instead of a static one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a fixed learning rate (no warm-up and cool-down steps).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't use label smoothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These changes are in sync with modern transformers such as BERT.
  prefs: []
  type: TYPE_NORMAL
- en: First, the input is passed through an embedding layer and a positional embedding
    layer in order to encode the positions of tokens in the sequence. Token embeddings
    are scaled by ![](img/35f98b55-821b-45c8-a75a-22519eddd6e9.png), the square root
    of the size of the hidden layers, and added to positional embeddings. Finally,
    dropout is applied for regularization.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder then passes through stacked modules, each consisting of attention,
    feedforward fully connected layers, and normalization. Attention layers are linear
    combinations of scaled multiplicative (dot product) attention layers (**Multi-Head
    Attention**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some transformer architectures only contain one of the two parts. For example,
    the OpenAI GPT transformer architecture (Alec Radfor and others, *Improving Language
    Understanding by Generative Pre-Training*, 2018), which generates amazingly coherent
    texts and consists of stacked decoders, while Google''s BERT architecture (Jacob
    Devlin and others, *BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding*, 2019) also consists of stacked encoders.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both Torch and TensorFlow have a repository for pretrained models. We can download
    a translation model from the Torch hub and use it straight away. This is what
    we''ll quickly show. For the `pytorch` model, we need to have a few dependencies
    installed first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we can download the model. It is quite big, which means it''ll
    take up a lot of disk space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This model (Nathan Ng and others, *Facebook FAIR's WMT19 News Translation Task
    Submission*, 2019) is state-of-the-art for translation. It even outperforms human
    translations in precision (BLEU score). `fairseq` comes with tutorials for training
    translation models on your own datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Torch hub provides a lot of different translation models, but also generic
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find a guide about the transformer architecture complete with PyTorch
    code (and an explanation on positional embeddings) on the Harvard NLP group website,
    which can also run on Google Colab: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lilian Weng of OpenAI has written about language modeling and transformer models,
    and provides a concise and clear overview:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalized Language Models – about the history of language and **Neural Machine
    Translation Models** (**NMTs**): [https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Transformer Family – about the history of transformer models: [https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for libraries supporting translation tasks, both `pytorch` and `tensorflow`
    provide pre-trained models, and support architectures useful in translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fairseq` is a library for sequence-to-sequence models in PyTorch: [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find tutorials on TensorFlow by Google Research on GitHub: [https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, OpenNMT is a framework based on PyTorch and TensorFlow for translation
    tasks with many tutorials and pre-trained models: [https://opennmt.net/](https://opennmt.net/).
  prefs: []
  type: TYPE_NORMAL
- en: Writing a popular novel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've mentioned the Turing test before as a way of finding out whether a computer
    is intelligent enough to trick an interrogator into thinking it is human. Some
    text generation tools generate essays that could possibly make sense, however,
    contain no intellectual merit behind the appearance of scientific language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same could be said of some human essays and utterances, however. Nassim
    Taleb, in his book *Fooled by Randomness*, argued a person should be called unintelligent
    if their writing could not be distinguished from an artificially generated one
    (a **reverse Turing test**). In a similar vein, Alan Sokal''s 1996 hoax article
    *Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum
    Gravity*, accepted by and published in a well-known social science journal, was
    a deliberate attempt by the university professor of physics to expose a lack of
    intellectual rigor and the misuse of scientific terminology without understanding.
    A possible conclusion could be that imitating humans might not be the way forward
    toward intellectual progress.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI GPT-3, with 175 billion parameters, has pushed the field of language
    models considerably, having learned facts in physics, being able to generate programming
    code based on descriptions, and being able to compose entertaining and funny prose.
  prefs: []
  type: TYPE_NORMAL
- en: Millions of fans across the world have been waiting for more than 200 years
    to know how the story of *Pride and Prejudice* continues with Elizabeth and Mr
    Darcy. In this recipe, we'll be generating *Pride and Prejudice 2* using a transformer-based
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Project Gutenberg is a digital library of (mostly) public domain e-books hosting
    more than 60,000 books in different languages and in formats such as plain text,
    HTML, PDF, EPUB, MOBI, and Plucker. Project Gutenberg also lists the most popular
    downloads: [http://www.gutenberg.org/browse/scores/top](http://www.gutenberg.org/browse/scores/top).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, Jane Austen''s romantic early-19th century novel *Pride
    and Prejudice* had by far the most downloads over the last 30 days (more than
    47,000). We''ll download the book in plain text format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We save the text file as `pride_and_prejudice.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be working in Colab, where you'll have access to Nvidia T4 or Nvidia K80
    GPUs. However, you can use your own computer as well, using either GPUs or even
    CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working in Colab, you'll need to upload your text file to your Google
    Drive ([https://drive.google.com](https://drive.google.com)), where you can access
    it from Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using a wrapper library for OpenAI''s GPT-2 that''s called `gpt-2-simple`,
    which is created and maintained by Max Woolf, a data scientist at BuzzFeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: This library will make it easy to fine-tune the model to new texts and show
    us text samples along the way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then have a choice of the size of the GPT-2 model. Four sizes of GPT-2 have
    been released by OpenAI as pretrained models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Small** (124 million parameters; occupies 500 MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medium** (355 million parameters; 1.5 GB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large** (774 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extra large** (1,558 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The large model cannot currently be fine-tuned in Colab, but can generate text
    from the pretrained model. The extra large model is too large to load into memory
    in Colab, and can therefore neither be fine-tuned nor generate text. While bigger
    models will achieve better performance and have more knowledge, they will take
    longer to train and to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll choose the small model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve downloaded the text of a popular novel, *Pride and Prejudice*, and we''ll
    first fine-tune the model, then we''ll generate similar text to *Pride and Prejudice*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning the model: We''ll load a pre-trained model and fine-tune it for
    our texts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll mount Google Drive. The `gpt-2-simple` library provides a utility function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you''d need to authorize the Colab notebook to have access to
    your Google Drive. We''ll use the *Pride and Prejudice* text file that we uploaded
    to our Google Drive before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then start fine-tuning based on our downloaded text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the training loss going down over the span of at least a couple
    of hours. We see samples of generated text during training such as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gpt-2-simple` library is really making it easy to train and continue training.
    All model checkpoints can be stored on Google Drive, so they aren''t lost when
    the runtime times out. We might have to restart several times, so it''s good to
    always store backups on Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to continue training after Colab has restarted, we can do this as
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We can now generate our new novel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing our new bestseller: We might need to get the model from Google Drive
    and load it up into the GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Please note that you might have to restart your notebook (Colab) again so that
    the TensorFlow variables don't clash.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can call a utility function in `gpt-2-simple` to generate the text into
    a file. Finally, we can download the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The `gpt_2_simple.generate()` function takes an optional `prefix` parameter,
    which is the text that is to be continued.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pride and Prejudice* – the saga continues; reading the text, there are sometimes
    some obvious flaws in the continuity, however, some passages are captivating to
    read. We can always generate a few samples so that we have a choice of how our
    novel continues.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we've used the GPT-2 model to generate text. This is called **neural
    story generation** and is a subset of **neural text generation**. Simply put,
    neural text generation is the process of building a statistical model of a text
    or of a language and applying this model to generate more text.
  prefs: []
  type: TYPE_NORMAL
- en: 'XLNet, OpenAI''s GPT and GPT-2, Google''s Reformer, OpenAI''s Sparse Transformers,
    and other transformer-based models have one thing in common: they are generative
    because of a modeling choice – they are autoregressive rather than auto-encoding.
    This autoregressive language generation is based on the assumption that the probability
    of a token can be predicted given a context sequence of length ![](img/3660a617-b11e-4ed3-8a5c-b76d66950499.png)
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bfcdb76-4980-4b66-8a88-c9c0788b5dea.png)'
  prefs: []
  type: TYPE_IMG
- en: This can be approximated via minimizing the cross-entropy of the predicted token
    versus the actual token. LSTMs, **G****enerative Adversarial Networks** (**GANs**),
    or autoregressive transformer architectures have been used for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'One major choice we have to make in our text generation is how to sample, and
    we have a few choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Greedy search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beam search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-k sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-p (nucleus) sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In greedy search, we take the highest rated choice each time, ignoring other
    choices. In contrast, rather than taking a high-scoring token, beam search tracks
    the scores of several choices in parallel in order to take the highest-scored
    sequence. Top-k sampling was introduced by Angela Fan and others (*Hierarchical
    Neural Story Generation*, 2018). In top-k sampling, all but the *k* most likely
    words are discarded. Conversely, in top-p (also called: nucleus sampling), the
    highest-scoring tokens surpassing probability threshold *p* are chosen, while
    the others are discarded. Top-k and top-p can be combined in order to avoid low-ranking
    words.'
  prefs: []
  type: TYPE_NORMAL
- en: While the `huggingface transformers` library gives us all of these choices, with `gpt-2-simple`,
    we have the choice of top-k sampling and top-p sampling.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many fantastic libraries that make training a model or applying an
    off-the-shelf model much easier. First of all, perhaps `Hugging Face transformers`,
    which is a library for language understanding and language generation supporting
    architectures and pretrained models for BERT, GPT-2, RoBERTa, XLM, DistilBert,
    XLNet, T5, and CTRL, among others: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers).
  prefs: []
  type: TYPE_NORMAL
- en: The `Hugging Face transformers` library comes with a few pre-trained transformer
    models, including a distilled GPT-2 model, which provides performance at the level
    of GPT-2, but with about 30% fewer parameters, bringing advantages of higher speed
    and lower resource demands in terms of memory and processing power. You can find
    a few notebooks linked from the Hugging Face GitHub repository that describe text
    generation and the fine-tuning of transformer models: [https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks](https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Hugging Face provides a website called *Write with Transformers* that
    – according to their slogan – can *autocomplete your thoughts*: [https://transformer.huggingface.co/](https://transformer.huggingface.co/).
  prefs: []
  type: TYPE_NORMAL
- en: You can find a tutorial on text generation with **recurrent neural networks **in
    the TensorFlow documentation: [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation).
  prefs: []
  type: TYPE_NORMAL
- en: Such models are also prepackaged in libraries such as textgenrnn: [https://github.com/minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn).
  prefs: []
  type: TYPE_NORMAL
- en: More complex, transformer-based models are also available from TensorFlow Hub,
    as demonstrated in another tutorial: [https://www.tensorflow.org/hub/tutorials/wiki40b_lm](https://www.tensorflow.org/hub/tutorials/wiki40b_lm).
  prefs: []
  type: TYPE_NORMAL
