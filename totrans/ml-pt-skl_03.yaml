- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: A Tour of Machine Learning Classifiers Using Scikit-Learn
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn进行机器学习分类器的介绍
- en: In this chapter, we will take a tour of a selection of popular and powerful
    machine learning algorithms that are commonly used in academia as well as in industry.
    While learning about the differences between several supervised learning algorithms
    for classification, we will also develop an appreciation of their individual strengths
    and weaknesses. In addition, we will take our first steps with the scikit-learn
    library, which offers a user-friendly and consistent interface for using those
    algorithms efficiently and productively.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些在学术界和工业界常用的流行和强大的机器学习算法。在学习了解几种用于分类的监督学习算法之间的差异时，我们还将理解它们各自的优缺点。此外，我们将首次使用Scikit-Learn库，该库提供了一个用户友好且一致的接口，有效和高效地使用这些算法。
- en: 'The topics that will be covered throughout this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节将涵盖以下主题：
- en: An introduction to robust and popular algorithms for classification, such as
    logistic regression, support vector machines, decision trees, and *k*-nearest
    neighbors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍了用于分类的稳健和流行算法，如逻辑回归、支持向量机、决策树和*k*最近邻算法。
- en: Examples and explanations using the scikit-learn machine learning library, which
    provides a wide variety of machine learning algorithms via a user-friendly Python
    API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn机器学习库的示例和解释，该库通过用户友好的Python API提供了各种机器学习算法。
- en: Discussions about the strengths and weaknesses of classifiers with linear and
    nonlinear decision boundaries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨了具有线性和非线性决策边界分类器的优缺点。
- en: Choosing a classification algorithm
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择分类算法
- en: 'Choosing an appropriate classification algorithm for a particular problem task
    requires practice and experience; each algorithm has its own quirks and is based
    on certain assumptions. To paraphrase the **no free lunch theorem** by David H.
    Wolpert, no single classifier works best across all possible scenarios (*The Lack
    of A Priori Distinctions Between Learning Algorithms*, *Wolpert, David H*, *Neural
    Computation 8.7* (1996): 1341-1390). In practice, it is always recommended that
    you compare the performance of at least a handful of different learning algorithms
    to select the best model for the particular problem; these may differ in the number
    of features or examples, the amount of noise in a dataset, and whether the classes
    are linearly separable.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '为特定问题任务选择适当的分类算法需要实践和经验；每种算法都有其独特之处，并基于某些假设。用David H. Wolpert的**没有免费午餐定理**的话来说，没有单一的分类器适用于所有可能的场景（*The
    Lack of A Priori Distinctions Between Learning Algorithms*, *Wolpert, David H*,
    *Neural Computation 8.7* (1996): 1341-1390）。实际上，建议您始终比较至少几种不同学习算法的性能，以选择最适合特定问题的模型；这些算法可能在特征或示例数量、数据集中的噪声量以及类别是否线性可分方面有所不同。'
- en: 'Eventually, the performance of a classifier—computational performance as well
    as predictive power—depends heavily on the underlying data that is available for
    learning. The five main steps that are involved in training a supervised machine
    learning algorithm can be summarized as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，分类器的性能——包括计算性能和预测能力——在很大程度上取决于可用于学习的基础数据。训练监督机器学习算法所涉及的五个主要步骤可以总结如下：
- en: Selecting features and collecting labeled training examples
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特征和收集带标签的训练样本
- en: Choosing a performance metric
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择性能度量标准
- en: Choosing a learning algorithm and training a model
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择学习算法并训练模型
- en: Evaluating the performance of the model
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的性能
- en: Changing the settings of the algorithm and tuning the model.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改算法的设置并调整模型。
- en: Since the approach of this book is to build machine learning knowledge step
    by step, we will mainly focus on the main concepts of the different algorithms
    in this chapter and revisit topics such as feature selection and preprocessing,
    performance metrics, and hyperparameter tuning for more detailed discussions later
    in the book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的方法是逐步建立机器学习知识，因此我们将主要关注本章节中不同算法的主要概念，并在本书的后期重新讨论诸如特征选择和预处理、性能指标以及超参数调整等主题，进行更详细的讨论。
- en: First steps with scikit-learn – training a perceptron
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn的第一步——训练感知器
- en: In *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*,
    you learned about two related learning algorithms for classification, the **perceptron**
    rule and **Adaline**, which we implemented in Python and NumPy by ourselves. Now
    we will take a look at the scikit-learn API, which, as mentioned, combines a user-friendly
    and consistent interface with a highly optimized implementation of several classification
    algorithms. The scikit-learn library offers not only a large variety of learning
    algorithms, but also many convenient functions to preprocess data and to fine-tune
    and evaluate our models. We will discuss this in more detail, together with the
    underlying concepts, in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*,
    and *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 2 章*，*训练简单的机器学习算法进行分类* 中，您学习了两种相关的分类学习算法，即**感知器**规则和**Adaline**，我们自己用 Python
    和 NumPy 实现了这些算法。现在我们将看看 scikit-learn API，正如前文所述，它结合了用户友好和一致的界面与几种分类算法的高度优化实现。scikit-learn
    库不仅提供多种学习算法，还提供许多方便的函数来预处理数据、微调和评估我们的模型。我们将在 *第 4 章*，*构建良好的训练数据集 - 数据预处理* 和 *第
    5 章*，*通过降维压缩数据* 中更详细地讨论这些内容及其概念。
- en: To get started with the scikit-learn library, we will train a perceptron model
    similar to the one that we implemented in *Chapter 2*. For simplicity, we will
    use the already familiar **Iris dataset** throughout the following sections. Conveniently,
    the Iris dataset is already available via scikit-learn, since it is a simple yet
    popular dataset that is frequently used for testing and experimenting with algorithms.
    Similar to the previous chapter, we will only use two features from the Iris dataset
    for visualization purposes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始使用 scikit-learn 库，我们将训练一个与 *第 2 章* 中实现的感知器模型类似的模型。为简单起见，我们将在以下各节中一直使用已经熟悉的**鸢尾花数据集**。方便的是，鸢尾花数据集已经通过
    scikit-learn 提供，因为它是一个简单而受欢迎的数据集，经常用于测试和实验算法。与前一章节类似，我们将仅使用鸢尾花数据集的两个特征进行可视化目的。
- en: 'We will assign the petal length and petal width of the 150 flower examples
    to the feature matrix, `X`, and the corresponding class labels of the flower species
    to the vector array, `y`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这 150 个花例子的花瓣长度和花瓣宽度分配给特征矩阵 `X`，并将相应的花种类的类标签分配给向量数组 `y`：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `np.unique(y)` function returned the three unique class labels stored in
    `iris.target`, and as we can see, the Iris flower class names, `Iris-setosa`,
    `Iris-versicolor`, and `Iris-virginica`, are already stored as integers (here:
    `0`, `1`, `2`). Although many scikit-learn functions and class methods also work
    with class labels in string format, using integer labels is a recommended approach
    to avoid technical glitches and improve computational performance due to a smaller
    memory footprint; furthermore, encoding class labels as integers is a common convention
    among most machine learning libraries.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `np.unique(y)` 返回了存储在 `iris.target` 中的三个唯一类标签，正如我们所看到的，鸢尾花的类名 `Iris-setosa`、`Iris-versicolor`
    和 `Iris-virginica` 已经存储为整数（这里是 `0`、`1`、`2`）。虽然许多 scikit-learn 函数和类方法也可以处理字符串格式的类标签，但使用整数标签是一种推荐的方法，可以避免技术故障并提高计算性能，因为整数标签具有较小的内存占用；此外，将类标签编码为整数是大多数机器学习库的常规约定。
- en: 'To evaluate how well a trained model performs on unseen data, we will further
    split the dataset into separate training and test datasets. In *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will discuss
    the best practices around model evaluation in more detail. Using the `train_test_split`
    function from scikit-learn’s `model_selection` module, we randomly split the `X`
    and `y` arrays into 30 percent test data (45 examples) and 70 percent training
    data (105 examples):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估训练模型在未见数据上的表现如何，我们将进一步将数据集分割为单独的训练和测试数据集。在 *第 6 章*，*学习模型评估和超参数调优的最佳实践* 中，我们将更详细地讨论模型评估的最佳实践。使用
    scikit-learn 的 `model_selection` 模块中的 `train_test_split` 函数，我们随机将 `X` 和 `y` 数组分割为
    30% 的测试数据（45 个例子）和 70% 的训练数据（105 个例子）：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the `train_test_split` function already shuffles the training datasets
    internally before splitting; otherwise, all examples from class `0` and class
    `1` would have ended up in the training datasets, and the test dataset would consist
    of 45 examples from class `2`. Via the `random_state` parameter, we provided a
    fixed random seed (`random_state=1`) for the internal pseudo-random number generator
    that is used for shuffling the datasets prior to splitting. Using such a fixed
    `random_state` ensures that our results are reproducible.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`train_test_split`函数在分割之前已经内部对训练数据集进行了洗牌；否则，所有来自类`0`和类`1`的示例都将最终进入训练数据集，而测试数据集将由类`2`的45个示例组成。通过`random_state`参数，我们为内部伪随机数生成器提供了一个固定的随机种子（`random_state=1`），用于洗牌数据集之前的操作。使用这样一个固定的`random_state`可以确保我们的结果是可重现的。
- en: 'Lastly, we took advantage of the built-in support for stratification via `stratify=y`.
    In this context, stratification means that the `train_test_split` method returns
    training and test subsets that have the same proportions of class labels as the
    input dataset. We can use NumPy’s `bincount` function, which counts the number
    of occurrences of each value in an array, to verify that this is indeed the case:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们利用内置的支持通过`stratify=y`进行分层。在这个上下文中，分层意味着`train_test_split`方法返回的训练和测试子集具有与输入数据集相同的类标签比例。我们可以使用NumPy的`bincount`函数来验证这一点，该函数统计数组中每个值的出现次数：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Many machine learning and optimization algorithms also require feature scaling
    for optimal performance, as we saw in the **gradient descent** example in *Chapter
    2*. Here, we will standardize the features using the `StandardScaler` class from
    scikit-learn’s `preprocessing` module:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习和优化算法也需要特征缩放以获得最佳性能，正如我们在*第二章*中的**梯度下降**示例中看到的那样。在这里，我们将使用scikit-learn的`preprocessing`模块中的`StandardScaler`类对特征进行标准化：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the preceding code, we loaded the `StandardScaler` class from the `preprocessing`
    module and initialized a new `StandardScaler` object that we assigned to the `sc`
    variable. Using the `fit` method, `StandardScaler` estimated the parameters, ![](img/B17582_03_001.png)
    (sample mean) and ![](img/B17582_03_002.png) (standard deviation), for each feature
    dimension from the training data. By calling the `transform` method, we then standardized
    the training data using those estimated parameters, ![](img/B17582_03_001.png)
    and ![](img/B17582_03_002.png). Note that we used the same scaling parameters
    to standardize the test dataset so that both the values in the training and test
    dataset are comparable with one another.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码，我们从`preprocessing`模块加载了`StandardScaler`类，并初始化了一个新的`StandardScaler`对象，将其赋给了变量`sc`。使用`fit`方法，`StandardScaler`估计了每个特征维度的训练数据的参数，即样本均值
    ![](img/B17582_03_001.png) 和标准差 ![](img/B17582_03_002.png) 。通过调用`transform`方法，我们使用这些估计参数标准化了训练数据，![](img/B17582_03_001.png)
    和 ![](img/B17582_03_002.png) 。请注意，我们使用相同的缩放参数来标准化测试数据集，以便训练数据集和测试数据集的值可以相互比较。
- en: 'Having standardized the training data, we can now train a perceptron model.
    Most algorithms in scikit-learn already support multiclass classification by default
    via the **one-versus-rest** (**OvR**) method, which allows us to feed the three
    flower classes to the perceptron all at once. The code is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准化训练数据之后，我们现在可以训练感知器模型。scikit-learn中的大多数算法默认支持多类分类，通过**一对多**（**OvR**）方法，我们可以一次将三个花类别的数据输入到感知器中。代码如下所示：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The scikit-learn interface will remind you of our perceptron implementation
    in *Chapter 2*. After loading the `Perceptron` class from the `linear_model` module,
    we initialized a new `Perceptron` object and trained the model via the `fit` method.
    Here, the model parameter, `eta0`, is equivalent to the learning rate, `eta`,
    that we used in our own perceptron implementation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`接口会让你想起我们在*第二章*中实现的感知器。在从`linear_model`模块加载`Perceptron`类之后，我们初始化了一个新的`Perceptron`对象，并通过`fit`方法训练了模型。在这里，模型参数`eta0`相当于我们自己感知器实现中使用的学习率`eta`。'
- en: As you will remember from *Chapter 2*, finding an appropriate learning rate
    requires some experimentation. If the learning rate is too large, the algorithm
    will overshoot the global loss minimum. If the learning rate is too small, the
    algorithm will require more epochs until convergence, which can make the learning
    slow—especially for large datasets. Also, we used the `random_state` parameter
    to ensure the reproducibility of the initial shuffling of the training dataset
    after each epoch.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*第2章*中记得的那样，找到合适的学习率需要一些实验。如果学习率太大，算法将会超出全局损失最小值。如果学习率太小，算法将需要更多的周期直到收敛，这可能会使学习变慢，尤其是对于大型数据集。此外，我们使用了`random_state`参数来确保每个周期后对训练数据集的初始洗牌具有可重复性。
- en: 'Having trained a model in scikit-learn, we can make predictions via the `predict`
    method, just like in our own perceptron implementation in *Chapter 2*. The code
    is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中训练了一个模型后，我们可以通过`predict`方法进行预测，就像在*第2章*中我们自己的感知器实现中一样。代码如下：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Executing the code, we can see that the perceptron misclassifies 1 out of the
    45 flower examples. Thus, the misclassification error on the test dataset is approximately
    0.022, or 2.2 percent (![](img/B17582_03_005.png)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码，我们可以看到感知器在45个花示例中误分类了1个。因此，测试数据集上的误分类率约为0.022，或2.2% (![](img/B17582_03_005.png))。
- en: '**Classification error versus accuracy**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类错误与准确率**'
- en: 'Instead of the misclassification error, many machine learning practitioners
    report the classification accuracy of a model, which is simply calculated as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习实践者报告模型的分类准确率，而不是错误率，计算方法如下：
- en: 1–*error* = 0.978, or 97.8 percent
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1–*error* = 0.978，即97.8%
- en: Whether we use the classification error or accuracy is merely a matter of preference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类错误或准确率仅仅是一种偏好。
- en: 'Note that scikit-learn also implements a large variety of different performance
    metrics that are available via the `metrics` module. For example, we can calculate
    the classification accuracy of the perceptron on the test dataset as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，scikit-learn还实现了许多不同的性能指标，这些指标可以通过`metrics`模块获得。例如，我们可以如下计算感知器在测试数据集上的分类准确率：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `y_test` is the true class labels and `y_pred` is the class labels that
    we predicted previously. Alternatively, each classifier in scikit-learn has a
    `score` method, which computes a classifier’s prediction accuracy by combining
    the `predict` call with `accuracy_score`, as shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`y_test`是真实的类标签，`y_pred`是我们先前预测的类标签。另外，scikit-learn中的每个分类器都有一个`score`方法，通过将`predict`调用与`accuracy_score`结合来计算分类器的预测准确率，如下所示：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Overfitting**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**'
- en: Note that we will evaluate the performance of our models based on the test dataset
    in this chapter. In *Chapter 6*, you will learn about useful techniques, including
    graphical analysis, such as learning curves, to detect and prevent overfitting.
    Overfitting, which we will return to later in this chapter, means that the model
    captures the patterns in the training data well but fails to generalize well to
    unseen data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本章中，我们将根据测试数据集评估模型的性能。在*第6章*中，您将学习到一些有用的技术，包括图形分析，如学习曲线，以检测和预防过拟合。过拟合是指模型在训练数据中捕捉到模式，但在未见数据中无法很好泛化。
- en: 'Finally, we can use our `plot_decision_regions` function from *Chapter 2* to
    plot the **decision regions** of our newly trained perceptron model and visualize
    how well it separates the different flower examples. However, let’s add a small
    modification to highlight the data instances from the test dataset via small circles:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用*第2章*中的`plot_decision_regions`函数来绘制新训练的感知器模型的**决策区域**，并可视化它如何有效地分离不同的花示例。不过，让我们做一个小修改，通过小圆圈突出显示来自测试数据集的数据实例：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the slight modification that we made to the `plot_decision_regions` function,
    we can now specify the indices of the examples that we want to mark on the resulting
    plots. The code is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对`plot_decision_regions`函数进行轻微修改，我们现在可以指定要在结果图中标记的示例的索引。代码如下：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we can see in the resulting plot, the three flower classes can’t be perfectly
    separated by a linear decision boundary:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在结果图中所看到的，三种花的类别不能完全通过线性决策边界分开：
- en: '![Chart  Description automatically generated with low confidence](img/B17582_03_01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成，置信度较低](img/B17582_03_01.png)'
- en: 'Figure 3.1: Decision boundaries of a multi-class perceptron model fitted to
    the Iris dataset'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：适用于鸢尾花数据集的多类感知器模型的决策边界
- en: However, remember from our discussion in *Chapter 2* that the perceptron algorithm
    never converges on datasets that aren’t perfectly linearly separable, which is
    why the use of the perceptron algorithm is typically not recommended in practice.
    In the following sections, we will look at more powerful linear classifiers that
    converge to a loss minimum even if the classes are not perfectly linearly separable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住我们在*第2章*中的讨论，感知器算法在数据集上永远不会收敛，除非数据集完全线性可分，这就是为什么实际上不建议在实践中使用感知器算法的原因。在接下来的章节中，我们将看看更强大的线性分类器，即使类别不是完全线性可分的，它们也会收敛到损失最小值。
- en: '**Additional perceptron settings**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**额外的感知器设置**'
- en: The `Perceptron`, as well as other scikit-learn functions and classes, often
    has additional parameters that we omit for clarity. You can read more about those
    parameters using the `help` function in Python (for instance, `help(Perceptron)`)
    or by going through the excellent scikit-learn online documentation at [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`Perceptron`以及其他scikit-learn函数和类通常具有我们为了清晰起见而省略的其他参数。您可以使用Python中的`help`函数（例如`help(Perceptron)`）或通过查阅优秀的scikit-learn在线文档[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)了解更多有关这些参数的信息。'
- en: Modeling class probabilities via logistic regression
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过逻辑回归对类别概率建模
- en: Although the perceptron rule offers a nice and easy-going introduction to machine
    learning algorithms for classification, its biggest disadvantage is that it never
    converges if the classes are not perfectly linearly separable. The classification
    task in the previous section would be an example of such a scenario. The reason
    for this is that the weights are continuously being updated since there is always
    at least one misclassified training example present in each epoch. Of course,
    you can change the learning rate and increase the number of epochs, but be warned
    that the perceptron will never converge on this dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器规则为分类的机器学习算法提供了一个不错和简单的介绍，但它的最大缺点是，如果类别不是完全线性可分的，它永远不会收敛。前一节中的分类任务就是这种情况的一个例子。原因在于权重不断更新，因为每个时期至少存在一个误分类的训练示例。当然，您可以改变学习率并增加时期的数量，但请注意，感知器将永远不会在这个数据集上收敛。
- en: 'To make better use of our time, we will now take a look at another simple,
    yet more powerful, algorithm for linear and binary classification problems: **logistic
    regression**. Note that, despite its name, logistic regression is a model for
    classification, not regression.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地利用我们的时间，现在我们将看一看另一种简单但更强大的线性和二元分类问题的算法：**逻辑回归**。请注意，尽管名为逻辑回归，但逻辑回归是一种分类模型，而不是回归模型。
- en: Logistic regression and conditional probabilities
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归和条件概率
- en: Logistic regression is a classification model that is very easy to implement
    and performs very well on linearly separable classes. It is one of the most widely
    used algorithms for classification in industry. Similar to the perceptron and
    Adaline, the logistic regression model in this chapter is also a linear model
    for binary classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种非常容易实现并且在线性可分类中表现非常好的分类模型。它是工业界中最广泛使用的分类算法之一。与感知器和Adaline类似，本章中的逻辑回归模型也是二元分类的线性模型。
- en: '**Logistic regression for multiple classes**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归用于多类**'
- en: Note that logistic regression can be readily generalized to multiclass settings,
    which is known as **multinomial logistic regression**, or **softmax regression**.
    More detailed coverage of multinomial logistic regression is outside the scope
    of this book, but the interested reader can find more information in my lecture
    notes at [https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L08_logistic__slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L08_logistic__slides.pdf)
    or [https://youtu.be/L0FU8NFpx4E](https://youtu.be/L0FU8NFpx4E).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可以方便地推广到多类设置中，这被称为**多项逻辑回归**或**Softmax回归**。关于多项逻辑回归的更详细覆盖超出了本书的范围，但有兴趣的读者可以在我的讲义笔记中找到更多信息，网址为[https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L08_logistic__slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L08_logistic__slides.pdf)或[https://youtu.be/L0FU8NFpx4E](https://youtu.be/L0FU8NFpx4E)。
- en: Another way to use logistic regression in multiclass settings is via the OvR
    technique, which we discussed previously.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类设置中使用逻辑回归的另一种方法是通过OvR技术，这是我们之前讨论过的。
- en: 'To explain the main mechanics behind logistic regression as a probabilistic
    model for binary classification, let’s first introduce the **odds**: the odds
    in favor of a particular event. The odds can be written as ![](img/B17582_03_006.png),
    where *p* stands for the probability of the positive event. The term “positive
    event” does not necessarily mean “good,” but refers to the event that we want
    to predict, for example, the probability that a patient has a certain disease
    given certain symptoms; we can think of the positive event as class label *y* = 1
    and the symptoms as features **x**. Hence, for brevity, we can define the probability
    *p* as *p* := *p*(*y* = 1|**x**), the conditional probability that a particular
    example belongs to a certain class 1 given its features, **x**.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释作为二元分类的概率模型的逻辑回归的主要机制，让我们首先介绍**几率**：支持特定事件的几率。几率可以写作![](img/B17582_03_006.png)，其中*p*代表积极事件的概率。术语“积极事件”并不一定意味着“好”，而是指我们要预测的事件，例如，患者在某些症状下患某种疾病的概率；我们可以将积极事件看作类标签*y* = 1，症状看作特征**x**。因此，简要地说，我们可以定义概率*p*为*p*(*y* = 1|**x**)，即给定其特征**x**的特定示例属于某个类1的条件概率。
- en: 'We can then further define the **logit** function, which is simply the logarithm
    of the odds (log-odds):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们可以进一步定义**logit**函数，它就是对数几率的对数（log-odds）：
- en: '![](img/B17582_03_007.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_007.png)'
- en: Note that *log* refers to the natural logarithm, as it is the common convention
    in computer science. The *logit* function takes input values in the range 0 to
    1 and transforms them into values over the entire real-number range.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*log*表示自然对数，这是计算机科学中的常规约定。*logit*函数接受范围为0到1的输入值，并将它们转换为整个实数范围的值。
- en: 'Under the logistic model, we assume that there is a linear relationship between
    the weighted inputs (referred to as net inputs in *Chapter 2*) and the log-odds:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑模型下，我们假设加权输入（在第2章中称为净输入）与对数几率之间存在线性关系：
- en: '![](img/B17582_03_008.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_008.png)'
- en: While the preceding describes an assumption we make about the linear relationship
    between the log-odds and the net inputs, what we are actually interested in is
    the probability *p*, the class-membership probability of an example given its
    features. While the logit function maps the probability to a real-number range,
    we can consider the inverse of this function to map the real-number range back
    to a [0, 1] range for the probability *p*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面描述了我们对于对数几率与净输入之间的线性关系的假设，我们实际上感兴趣的是概率*p*，即给定其特征的示例的类成员概率。虽然logit函数将概率映射到实数范围，我们可以考虑该函数的反函数将实数范围映射回[0, 1]范围的概率*p*。
- en: 'This inverse of the logit function is typically called the **logistic sigmoid
    function**, which is sometimes simply abbreviated to **sigmoid function** due
    to its characteristic S-shape:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个logit函数的反函数通常称为**逻辑Sigmoid函数**，由于其典型的S形状，有时简称为**Sigmoid函数**：
- en: '![](img/B17582_03_009.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_009.png)'
- en: 'Here, *z* is the net input, the linear combination of weights, and the inputs
    (that is, the features associated with the training examples):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*z*是净输入，即权重和输入（即与训练示例相关联的特征）的线性组合：
- en: '*z* = **w**^T**x** + *b*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = **w**^T**x** + *b*'
- en: 'Now, let’s simply plot the sigmoid function for some values in the range –7
    to 7 to see how it looks:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简单地绘制Sigmoid函数在范围–7到7之间的一些值，以查看其外观：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result of executing the previous code example, we should now see the S-shaped
    (sigmoidal) curve:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码示例后，我们现在应该看到S形（sigmoid）曲线：
- en: '![Chart, histogram  Description automatically generated](img/B17582_03_02.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图 自动生成的描述](img/B17582_03_02.png)'
- en: 'Figure 3.2: A plot of the logistic sigmoid function'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：逻辑Sigmoid函数的绘图
- en: We can see that ![](img/B17582_02_039.png) approaches 1 if *z* goes toward infinity
    (*z*→∞) since *e*^–^z becomes very small for large values of *z*. Similarly, ![](img/B17582_02_039.png)
    goes toward 0 for *z*→–∞ as a result of an increasingly large denominator. Thus,
    we can conclude that this sigmoid function takes real-number values as input and
    transforms them into values in the range [0, 1] with an intercept at ![](img/B17582_03_012.png).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果*z*趋向于无穷大（*z*→∞），![](img/B17582_02_039.png)接近1，因为*e*^–^z在*z*较大时变得非常小。类似地，如果*z*趋向于负无穷大（*z*→–∞），![](img/B17582_02_039.png)趋向于0，这是因为分母变得越来越大。因此，我们可以得出结论，这个Sigmoid函数接受实数值作为输入，并将它们转换为[0, 1]范围内的值，并在![](img/B17582_03_012.png)处截距。
- en: To build some understanding of the logistic regression model, we can relate
    it to *Chapter 2*. In Adaline, we used the identity function, ![](img/B17582_02_040.png),
    as the activation function. In logistic regression, this activation function simply
    becomes the sigmoid function that we defined earlier.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要对逻辑回归模型建立一些理解，我们可以将其与*第2章*相关联。在Adaline中，我们使用了恒等函数，![](img/B17582_02_040.png)，作为激活函数。在逻辑回归中，这个激活函数简单地变成了我们之前定义的sigmoid函数。
- en: 'The difference between Adaline and logistic regression is illustrated in the
    following figure, where the only difference is the activation function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline与逻辑回归之间的区别在下图中有所说明，唯一的区别在于激活函数：
- en: '![Diagram, schematic  Description automatically generated](img/B17582_03_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram, schematic  Description automatically generated](img/B17582_03_03.png)'
- en: 'Figure 3.3: Logistic regression compared to Adaline'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：逻辑回归与Adaline比较
- en: The output of the sigmoid function is then interpreted as the probability of
    a particular example belonging to class 1, ![](img/B17582_03_014.png), given its
    features, *x*, and parameterized by the weights and bias, *w* and *b*. For example,
    if we compute ![](img/B17582_03_015.png) for a particular flower example, it means
    that the chance that this example is an `Iris-versicolor` flower is 80 percent.
    Therefore, the probability that this flower is an `Iris-setosa` flower can be
    calculated as *p*(*y* = 0|**x**; **w**, *b*) = 1 – *p*(*y* = 1|**x**; **w**, *b*) = 0.2,
    or 20 percent.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过sigmoid函数的输出解释为特定示例属于类1的概率，![](img/B17582_03_014.png)，给定其特征，*x*，并由权重和偏置参数化，*w*
    和 *b*。例如，如果我们计算一个特定花示例的 ![](img/B17582_03_015.png)，这意味着这个示例是`Iris-versicolor`花的概率为80%。因此，这种花是`Iris-setosa`花的概率可以计算为
    *p*(*y* = 0|**x**; **w**, *b*) = 1 – *p*(*y* = 1|**x**; **w**, *b*) = 0.2，或者20%。
- en: 'The predicted probability can then simply be converted into a binary outcome
    via a threshold function:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的概率可以简单地通过阈值函数转换为二进制结果：
- en: '![](img/B17582_03_016.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_016.png)'
- en: 'If we look at the preceding plot of the sigmoid function, this is equivalent
    to the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看sigmoid函数的前面的图，这等同于以下内容：
- en: '![](img/B17582_03_017.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_017.png)'
- en: In fact, there are many applications where we are not only interested in the
    predicted class labels, but where the estimation of the class-membership probability
    is particularly useful (the output of the sigmoid function prior to applying the
    threshold function). Logistic regression is used in weather forecasting, for example,
    not only to predict whether it will rain on a particular day, but also to report
    the chance of rain. Similarly, logistic regression can be used to predict the
    chance that a patient has a particular disease given certain symptoms, which is
    why logistic regression enjoys great popularity in the field of medicine.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有许多应用场景不仅关注预测的类标签，而是特别关注预测类成员概率的估计（应用阈值函数之前的sigmoid函数输出）。例如，逻辑回归在天气预报中使用，不仅可以预测某一天是否会下雨，还可以报告降雨的可能性。同样地，逻辑回归可以用于根据某些症状预测患者患某种疾病的概率，这就是为什么逻辑回归在医学领域中非常流行的原因。
- en: Learning the model weights via the logistic loss function
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过逻辑损失函数学习模型权重
- en: 'You have learned how we can use the logistic regression model to predict probabilities
    and class labels; now, let’s briefly talk about how we fit the parameters of the
    model, for instance, the weights and bias unit, *w* and *b*. In the previous chapter,
    we defined the mean squared error loss function as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了如何使用逻辑回归模型预测概率和类标签；现在，让我们简要讨论如何拟合模型的参数，例如权重和偏置单元，*w* 和 *b*。在前一章中，我们定义了均方误差损失函数如下：
- en: '![](img/B17582_03_018.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_018.png)'
- en: 'We minimized this function in order to learn the parameters for our Adaline
    classification model. To explain how we can derive the loss function for logistic
    regression, let’s first define the likelihood, ![](img/B17582_03_019.png), that
    we want to maximize when we build a logistic regression model, assuming that the
    individual examples in our dataset are independent of one another. The formula
    is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最小化这个函数，以便学习我们的Adaline分类模型的参数。为了解释我们如何推导逻辑回归的损失函数，让我们首先定义可能性，![](img/B17582_03_019.png)，即当我们构建逻辑回归模型时要最大化的可能性，假设数据集中的个体示例是彼此独立的。该公式如下：
- en: '![](img/B17582_03_020.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_020.png)'
- en: 'In practice, it is easier to maximize the (natural) log of this equation, which
    is called the **log-likelihood** function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，最大化（自然）对数化简化了这个方程，这被称为**对数似然**函数：
- en: '![](img/B17582_03_021.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_021.png)'
- en: Firstly, applying the log function reduces the potential for numerical underflow,
    which can occur if the likelihoods are very small. Secondly, we can convert the
    product of factors into a summation of factors, which makes it easier to obtain
    the derivative of this function via the addition trick, as you may remember from
    calculus.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应用对数函数减少了数值下溢的可能性，这种情况可能发生在似然值非常小的情况下。其次，我们可以将因子的乘积转换为因子的总和，这样可以更容易地通过加法技巧获取此函数的导数，正如你可能从微积分中记得的那样。
- en: '**Deriving the likelihood function**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**推导似然函数**'
- en: 'We can obtain the expression for the likelihood of the model given the data,
    ![](img/B17582_03_022.png), as follows. Given that we have a binary classification
    problem with class labels 0 and 1, we can think of the label 1 as a Bernoulli
    variable—it can take on two values, 0 and 1, with the probability *p* of being
    1: ![](img/B17582_03_023.png). For a single data point, we can write this probability
    as ![](img/B17582_03_024.png) and ![](img/B17582_03_025.png).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得到给定数据的模型似然性表达式，![](img/B17582_03_022.png)，如下所示。考虑到我们有一个二元分类问题，类标签为 0 和
    1，我们可以将标签 1 视为伯努利变量——它可以取两个值，0 和 1，概率为 *p*：![](img/B17582_03_023.png)。对于单个数据点，我们可以将这个概率写为
    ![](img/B17582_03_024.png) 和 ![](img/B17582_03_025.png)。
- en: 'Putting these two expressions together, and using the shorthand ![](img/B17582_03_026.png),
    we get the probability mass function of the Bernoulli variable:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个表达式放在一起，并使用简写 ![](img/B17582_03_026.png)，我们得到了伯努利变量的概率质量函数：
- en: '![](img/B17582_03_027.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_027.png)'
- en: 'We can write the likelihood of the training labels given the assumption that
    all training examples are independent, using the multiplication rule to compute
    the probability that all events occur, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写出训练标签的似然性，假设所有训练示例是独立的，使用乘法规则计算所有事件发生的概率，如下所示：
- en: '![](img/B17582_03_028.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_028.png)'
- en: 'Now, substituting the probability mass function of the Bernoulli variable,
    we arrive at the expression of the likelihood, which we attempt to maximize by
    changing the model parameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代入伯努利变量的概率质量函数，我们得到似然性的表达式，通过改变模型参数来最大化：
- en: '![](img/B17582_03_029.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_029.png)'
- en: 'Now, we could use an optimization algorithm such as gradient ascent to maximize
    this log-likelihood function. (Gradient ascent works exactly the same way as gradient
    descent explained in *Chapter 2*, except that gradient ascent maximizes a function
    instead of minimizing it.) Alternatively, let’s rewrite the log-likelihood as
    a loss function, *L*, that can be minimized using gradient descent as in *Chapter
    2*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用梯度上升等优化算法来最大化这个对数似然函数。（梯度上升的工作方式与第 2 章中解释的梯度下降完全相同，只是梯度上升是最大化一个函数，而不是最小化它。）或者，让我们将对数似然重写为可以使用梯度下降最小化的损失函数
    *L*，如第 2 章中所述：
- en: '![](img/B17582_03_030.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_030.png)'
- en: 'To get a better grasp of this loss function, let’s take a look at the loss
    that we calculate for one single training example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个损失函数，让我们看看我们为单个训练样本计算的损失：
- en: '![](img/B17582_03_031.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_031.png)'
- en: 'Looking at the equation, we can see that the first term becomes zero if *y* = 0,
    and the second term becomes zero if *y* = 1:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 查看方程式，我们可以看到如果 *y* = 0，第一项变为零，如果 *y* = 1，第二项变为零：
- en: '![](img/B17582_03_032.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_032.png)'
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The resulting plot shows the sigmoid activation on the *x* axis in the range
    0 to 1 (the inputs to the sigmoid function were *z* values in the range –10 to
    10) and the associated logistic loss on the *y* axis:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果绘制显示了在 *x* 轴上 0 到 1 范围内的 sigmoid 激活（sigmoid 函数的输入为范围在 –10 到 10 的 *z* 值），以及与之相关联的逻辑损失在
    *y* 轴上：
- en: '![Chart  Description automatically generated](img/B17582_03_04.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图表 说明自动生成](img/B17582_03_04.png)'
- en: 'Figure 3.4: A plot of the loss function used in logistic regression'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：逻辑回归中使用的损失函数图
- en: We can see that the loss approaches 0 (continuous line) if we correctly predict
    that an example belongs to class 1\. Similarly, we can see on the *y* axis that
    the loss also approaches 0 if we correctly predict *y* = 0 (dashed line). However,
    if the prediction is wrong, the loss goes toward infinity. The main point is that
    we penalize wrong predictions with an increasingly larger loss.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Converting an Adaline implementation into an algorithm for logistic regression
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we were to implement logistic regression ourselves, we could simply substitute
    the loss function, *L*, in our Adaline implementation from *Chapter 2*, with the
    new loss function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_03_034.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'We use this to compute the loss of classifying all training examples per epoch.
    Also, we need to swap the linear activation function with the sigmoid. If we make
    those changes to the Adaline code, we will end up with a working logistic regression
    implementation. The following is an implementation for full-batch gradient descent
    (but note that the same changes could be made to the stochastic gradient descent
    version as well):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When we fit a logistic regression model, we have to keep in mind that it only
    works for binary classification tasks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s consider only setosa and versicolor flowers (classes `0` and `1`)
    and check that our implementation of logistic regression works:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting decision region plot looks as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_03_05.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: The decision region plot for the logistic regression model'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**The gradient descent learning algorithm for logistic regression**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: If you compared the `LogisticRegressionGD` in the previous code with the `AdalineGD`
    code from *Chapter 2*, you may have noticed that the weight and bias update rules
    remained unchanged (except for the scaling factor 2). Using calculus, we can show
    that the parameter updates via gradient descent are indeed similar for logistic
    regression and Adaline. However, please note that the following derivation of
    the gradient descent learning rule is intended for readers who are interested
    in the mathematical concepts behind the gradient descent learning rule for logistic
    regression. It is not essential for following the rest of this chapter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.6* summarizes how we can calculate the partial derivative of the
    log-likelihood function with respect to the *j*th weight:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_03_06.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Calculating the partial derivative of the log-likelihood function'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Note that we omitted averaging over the training examples for brevity.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember from *Chapter 2* that we take steps in the opposite direction of the
    gradient. Hence, we flip ![](img/B17582_03_035.png) and update the *j*th weight
    as follows, including the learning rate ![](img/B17582_02_065.png):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_03_037.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'While the partial derivative of the loss function with respect to the bias
    unit is not shown, bias derivation follows the same overall concept using the
    chain rule, resulting in the following update rule:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然未显示损失函数对偏置单位的偏导数，但是偏置导数遵循相同的链式法则概念，导致以下更新规则：
- en: '![](img/B17582_03_038.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_038.png)'
- en: Both the weight and bias unit updates are equal to the ones for Adaline in *Chapter
    2*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏置单位的更新与 *第 2 章* 中 Adaline 的更新相同。
- en: Training a logistic regression model with scikit-learn
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 训练 logistic 回归模型
- en: We just went through useful coding and math exercises in the previous subsection,
    which helped to illustrate the conceptual differences between Adaline and logistic
    regression. Now, let’s learn how to use scikit-learn’s more optimized implementation
    of logistic regression, which also supports multiclass settings off the shelf.
    Note that in recent versions of scikit-learn, the technique used for multiclass
    classification, multinomial, or OvR, is chosen automatically. In the following
    code example, we will use the `sklearn.linear_model.LogisticRegression` class
    as well as the familiar `fit` method to train the model on all three classes in
    the standardized flower training dataset. Also, we set `multi_class='ovr'` for
    illustration purposes. As an exercise for the reader, you may want to compare
    the results with `multi_class='multinomial'`. Note that the `multinomial` setting
    is now the default choice in scikit-learn’s `LogisticRegression` class and recommended
    in practice for mutually exclusive classes, such as those found in the Iris dataset.
    Here, “mutually exclusive” means that each training example can only belong to
    a single class (in contrast to multilabel classification, where a training example
    can be a member of multiple classes).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们刚刚完成了有用的编码和数学练习，这有助于说明 Adaline 和 logistic 回归之间的概念差异。 现在，让我们学习如何使用 scikit-learn
    更优化的 logistic 回归实现，它还支持即插即用的多类设置。 请注意，在 scikit-learn 的最新版本中，用于多类分类的技术，即 multinomial
    或 OvR，是自动选择的。 在以下代码示例中，我们将使用 `sklearn.linear_model.LogisticRegression` 类以及熟悉的
    `fit` 方法在标准化的花卉训练数据集中训练模型的所有三个类。 另外，我们设置 `multi_class='ovr'` 以进行说明目的。 作为读者的练习，您可能希望将结果与
    `multi_class='multinomial'` 进行比较。 请注意，`multinomial` 设置现在是 scikit-learn 的 `LogisticRegression`
    类的默认选择，并且在实践中推荐用于互斥类，例如在 Iris 数据集中找到的类。 在这里，“互斥” 意味着每个训练示例只能属于一个单一类（与多标签分类相对，其中训练示例可以是多个类的成员）。
- en: 'Now, let’s have a look at the code example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看代码示例：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After fitting the model on the training data, we plotted the decision regions,
    training examples, and test examples, as shown in *Figure 3.7*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在对训练数据拟合模型之后，我们绘制了决策区域、训练示例和测试示例，如 *图 3.7* 所示：
- en: '![A picture containing chart  Description automatically generated](img/B17582_03_07.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图表描述的图片](img/B17582_03_07.png)'
- en: 'Figure 3.7: Decision regions for scikit-learn’s multi-class logistic regression
    model'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：scikit-learn 的多类 logistic 回归模型的决策区域
- en: '**Algorithms for convex optimization**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**凸优化算法**'
- en: Note that there exist many different algorithms for solving optimization problems.
    For minimizing convex loss functions, such as the logistic regression loss, it
    is recommended to use more advanced approaches than regular **stochastic gradient
    descent** (**SGD**). In fact, scikit-learn implements a whole range of such optimization
    algorithms, which can be specified via the `solver` parameter, namely, `'newton-cg'`,
    `'lbfgs'`, `'``liblinear'`, `'sag'`, and `'saga'`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，存在许多不同的算法用于解决优化问题。 对于最小化凸损失函数（例如 logistic 回归损失），建议使用比常规的 **随机梯度下降** (**SGD**)
    更高级的方法。 实际上，scikit-learn 实现了一整套这样的优化算法，可以通过 `solver` 参数指定，即 `'newton-cg'`、`'lbfgs'`、`'liblinear'`、`'sag'`
    和 `'saga'`。
- en: While the logistic regression loss is convex, most optimization algorithms should
    converge to the global loss minimum with ease. However, there are certain advantages
    of using one algorithm over the other. For example, in previous versions (for
    instance, v 0.21), scikit-learn used `'liblinear'` as a default, which cannot
    handle the multinomial loss and is limited to the OvR scheme for multiclass classification.
    However, in scikit-learn v 0.22, the default solver was changed to `'lbfgs'`,
    which stands for the limited-memory **Broyden–Fletcher–Goldfarb–Shanno** (**BFGS**)
    algorithm ([https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS))
    and is more flexible in this regard.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管逻辑回归损失是凸的，大多数优化算法应该可以轻松收敛到全局损失最小值。然而，使用一种算法而不是另一种算法有一定的优势。例如，在之前的版本（例如v 0.21）中，scikit-learn默认使用`'liblinear'`，它不能处理多项式损失，并且仅限于多类分类的OvR方案。然而，在scikit-learn
    v 0.22中，默认解算器更改为`'lbfgs'`，它代表了有限内存**Broyden–Fletcher–Goldfarb–Shanno**（**BFGS**）算法（[https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)），在这方面更加灵活。
- en: Looking at the preceding code that we used to train the `LogisticRegression`
    model, you might now be wondering, “What is this mysterious parameter C?” We will
    discuss this parameter in the next subsection, where we will introduce the concepts
    of overfitting and regularization. However, before we move on to those topics,
    let’s finish our discussion of class membership probabilities.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们用于训练`LogisticRegression`模型的前述代码时，您可能会想，“这个神秘的参数C是什么？”在下一小节中，我们将讨论这个参数，介绍过拟合和正则化的概念。然而，在我们继续讨论这些主题之前，让我们完成我们关于类成员概率的讨论。
- en: 'The probability that training examples belong to a certain class can be computed
    using the `predict_proba` method. For example, we can predict the probabilities
    of the first three examples in the test dataset as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`predict_proba`方法计算训练样本属于某一类的概率。例如，我们可以预测测试数据集中前三个示例的概率如下：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This code snippet returns the following array:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段返回以下数组：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first row corresponds to the class membership probabilities of the first
    flower, the second row corresponds to the class membership probabilities of the
    second flower, and so forth. Notice that the column-wise sum in each row is 1,
    as expected. (You can confirm this by executing `lr.predict_proba(X_test_std[:3,
    :]).sum(axis=1)`.)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行对应于第一朵花的类成员概率，第二行对应于第二朵花的类成员概率，依此类推。请注意，每行中的列总和为1，如预期那样。（您可以通过执行`lr.predict_proba(X_test_std[:3,
    :]).sum(axis=1)`来确认这一点。）
- en: 'The highest value in the first row is approximately 0.85, which means that
    the first example belongs to class 3 (`Iris-virginica`) with a predicted probability
    of 85 percent. So, as you may have already noticed, we can get the predicted class
    labels by identifying the largest column in each row, for example, using NumPy’s
    `argmax` function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行中的最高值约为0.85，这意味着第一个示例属于类别3（`Iris-virginica`），预测概率为85%。因此，正如您可能已经注意到的，我们可以通过识别每行中最大的列来获取预测的类标签，例如使用NumPy的`argmax`函数：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The returned class indices are shown here (they correspond to `Iris-virginica`,
    `Iris-setosa`, and `Iris-setosa`):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的类索引如下所示（它们对应于`Iris-virginica`，`Iris-setosa`和`Iris-setosa`）：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code example, we computed the conditional probabilities and
    converted these into class labels manually by using NumPy’s `argmax` function.
    In practice, the more convenient way of obtaining class labels when using scikit-learn
    is to call the `predict` method directly:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码示例中，我们计算了条件概率，并通过使用NumPy的`argmax`函数手动将其转换为类标签。在实践中，当使用scikit-learn时，更方便的获取类标签的方法是直接调用`predict`方法：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, a word of caution if you want to predict the class label of a single
    flower example: scikit-learn expects a two-dimensional array as data input; thus,
    we have to convert a single row slice into such a format first. One way to convert
    a single row entry into a two-dimensional data array is to use NumPy’s `reshape`
    method to add a new dimension, as demonstrated here:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想预测单个花例的类标签，请注意：scikit-learn期望数据输入为二维数组；因此，我们首先必须将单行切片转换为这样的格式。将单行条目转换为二维数据数组的一种方法是使用NumPy的`reshape`方法添加一个新维度，如下所示：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Tackling overfitting via regularization
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过正则化解决过拟合
- en: Overfitting is a common problem in machine learning, where a model performs
    well on training data but does not generalize well to unseen data (test data).
    If a model suffers from overfitting, we also say that the model has a high variance,
    which can be caused by having too many parameters, leading to a model that is
    too complex given the underlying data. Similarly, our model can also suffer from
    **underfitting** (high bias), which means that our model is not complex enough
    to capture the pattern in the training data well and therefore also suffers from
    low performance on unseen data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是机器学习中常见的问题，指的是模型在训练数据上表现良好，但在未见过的数据（测试数据）上表现不佳。如果一个模型存在过拟合问题，我们也会说这个模型具有高方差，这可能是由于参数过多导致模型过于复杂，无法很好地适应基础数据。同样，我们的模型也可能遭受**欠拟合**（高偏差）的问题，这意味着我们的模型不够复杂，无法很好地捕捉训练数据中的模式，因此在未见数据上也表现较差。
- en: 'Although we have only encountered linear models for classification so far,
    the problems of overfitting and underfitting can be best illustrated by comparing
    a linear decision boundary to more complex, nonlinear decision boundaries, as
    shown in *Figure 3.8*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管到目前为止我们只遇到过用于分类的线性模型，但通过将线性决策边界与更复杂、非线性决策边界进行比较，可以最好地说明过拟合和欠拟合的问题，如在*图3.8*中所示：
- en: '![Chart, radar chart  Description automatically generated](img/B17582_03_08.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图表，雷达图 说明自动生成](img/B17582_03_08.png)'
- en: 'Figure 3.8: Examples of underfitted, well-fitted, and overfitted models'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：欠拟合、拟合良好和过拟合模型的示例
- en: '**The bias-variance tradeoff**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差-方差权衡**'
- en: Often, researchers use the terms “bias” and “variance” or “bias-variance tradeoff”
    to describe the performance of a model—that is, you may stumble upon talks, books,
    or articles where people say that a model has a “high variance” or “high bias.”
    So, what does that mean? In general, we might say that “high variance” is proportional
    to overfitting and “high bias” is proportional to underfitting.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员经常使用“偏差”和“方差”或“偏差-方差权衡”这些术语来描述模型的性能，也就是说，你可能会听到有人说某个模型具有“高方差”或“高偏差”。那么，这是什么意思呢？一般来说，我们可能会说“高方差”与过拟合成正比，“高偏差”与欠拟合成正比。
- en: In the context of machine learning models, variance measures the consistency
    (or variability) of the model prediction for classifying a particular example
    if we retrain the model multiple times, for example, on different subsets of the
    training dataset. We can say that the model is sensitive to the randomness in
    the training data. In contrast, bias measures how far off the predictions are
    from the correct values in general if we rebuild the model multiple times on different
    training datasets; bias is the measure of the systematic error that is not due
    to randomness.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的背景下，方差衡量了在多次重新训练模型时，例如在训练数据集的不同子集上，为对特定示例进行分类，模型预测的一致性（或变异性）。我们可以说模型对训练数据中随机性敏感。相反，偏差衡量了多次在不同训练数据集上重新构建模型时，预测值与正确值的偏差程度；偏差是衡量由于非随机性而产生的系统误差的指标。
- en: 'If you are interested in the technical specification and derivation of the
    “bias” and “variance” terms, I’ve written about it in my lecture notes here: [https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/08-model-eval-1-intro__notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/08-model-eval-1-intro__notes.pdf).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对“偏差”和“方差”这两个术语的技术规范和推导感兴趣，可以在我的讲义中找到相关内容：[https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/08-model-eval-1-intro__notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/08-model-eval-1-intro__notes.pdf)。
- en: One way of finding a good bias-variance tradeoff is to tune the complexity of
    the model via regularization. Regularization is a very useful method for handling
    collinearity (high correlation among features), filtering out noise from data,
    and eventually preventing overfitting.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找一个良好的偏差-方差平衡的一种方式是通过正则化调整模型的复杂性。正则化是处理共线性（特征之间的高相关性）、从数据中过滤噪声以及最终预防过拟合的非常有用的方法。
- en: 'The concept behind regularization is to introduce additional information to
    penalize extreme parameter (weight) values. The most common form of regularization
    is so-called **L2 regularization** (sometimes also called L2 shrinkage or weight
    decay), which can be written as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化背后的概念是引入额外信息来惩罚极端参数（权重）值。最常见的正则化形式是所谓的**L2正则化**（有时也称为L2缩减或权重衰减），可以写作如下形式：
- en: '![](img/B17582_03_039.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_039.png)'
- en: Here, ![](img/B17582_03_040.png) is the so-called **regularization parameter**.
    Note that the 2 in the denominator is merely a scaling factor, such that it cancels
    when computing the loss gradient. The sample size *n* is added to scale the regularization
    term similar to the loss.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_03_040.png)被称为**正则化参数**。请注意，分母中的2只是一个缩放因子，因此在计算损失梯度时会被抵消。样本大小*n*被添加以将正则化项缩放类似于损失。
- en: '**Regularization and feature normalization**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化和特征标准化**'
- en: Regularization is another reason why feature scaling such as standardization
    is important. For regularization to work properly, we need to ensure that all
    our features are on comparable scales.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是特征缩放如标准化之类的另一个重要原因。为了使正则化正常工作，我们需要确保所有特征都在可比较的尺度上。
- en: 'The loss function for logistic regression can be regularized by adding a simple
    regularization term, which will shrink the weights during model training:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的损失函数可以通过添加简单的正则化项进行正则化，这将在模型训练过程中缩小权重：
- en: '![](img/B17582_03_041.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_041.png)'
- en: 'The partial derivative of the unregularized loss is defined as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 未正则化损失的偏导数定义如下：
- en: '![](img/B17582_03_042.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_042.png)'
- en: 'Adding the regularization term to the loss changes the partial derivative to
    the following form:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将正则化项添加到损失函数中会改变偏导数的形式如下：
- en: '![](img/B17582_03_043.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_043.png)'
- en: Via the regularization parameter, ![](img/B17582_03_040.png), we can then control
    how closely we fit the training data, while keeping the weights small. By increasing
    the value of ![](img/B17582_03_040.png), we increase the regularization strength.
    Please note that the bias unit, which is essentially an intercept term or negative
    threshold, as we learned in *Chapter 2*, is usually not regularized.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正则化参数![](img/B17582_03_040.png)，我们可以控制拟合训练数据的紧密程度，同时保持权重较小。通过增加![](img/B17582_03_040.png)的值，我们增加了正则化强度。请注意，偏置单元，本质上是拦截项或负阈值，如我们在*第2章*中学到的那样，通常不被正则化。
- en: 'The parameter, `C`, that is implemented for the `LogisticRegression` class
    in scikit-learn comes from a convention in support vector machines, which will
    be the topic of the next section. The term `C` is inversely proportional to the
    regularization parameter, ![](img/B17582_03_040.png). Consequently, decreasing
    the value of the inverse regularization parameter, `C`, means that we are increasing
    the regularization strength, which we can visualize by plotting the L2 regularization
    path for the two weight coefficients:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现的`LogisticRegression`类中的参数`C`，来自支持向量机的一个约定，这将是下一节的主题。术语`C`与正则化参数的倒数成反比，![](img/B17582_03_040.png)。因此，减少逆正则化参数`C`的值意味着增加正则化强度，我们可以通过绘制两个权重系数的L2正则化路径来进行可视化：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By executing the preceding code, we fitted 10 logistic regression models with
    different values for the inverse-regularization parameter, `C`. For illustration
    purposes, we only collected the weight coefficients of class `1` (here, the second
    class in the dataset: `Iris-versicolor`) versus all classifiers—remember that
    we are using the OvR technique for multiclass classification.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行前述代码，我们拟合了10个逻辑回归模型，使用不同的逆正则化参数`C`的值。为了说明目的，我们仅收集了类别`1`（在这里是数据集中的第二类：`Iris-versicolor`）的权重系数，对所有分类器进行了采集。请记住，我们正在使用一对多技术进行多类分类。
- en: 'As we can see in the resulting plot, the weight coefficients shrink if we decrease
    parameter `C`, that is, if we increase the regularization strength:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在生成的图表中所看到的，如果减少参数`C`，即增加正则化强度，权重系数会收缩：
- en: '![Chart, line chart  Description automatically generated](img/B17582_03_09.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图  自动生成描述](img/B17582_03_09.png)'
- en: 'Figure 3.9: The impact of the inverse regularization strength parameter C on
    L2 regularized model results'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：逆正则化强度参数C对L2正则化模型结果的影响
- en: Increasing the regularization strength can reduce overfitting, so we might ask
    why we don’t strongly regularize all models by default. The reason is that we
    have to be careful when adjusting the regularization strength. For instance, if
    the regularization strength is too high and the weights coefficients approach
    zero, the model can perform very poorly due to underfitting, as illustrated in
    *Figure 3.8*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 增加正则化强度可以减少过拟合，因此我们可能会问为什么不默认强力正则化所有模型。原因是我们在调整正则化强度时必须小心。例如，如果正则化强度过高，权重系数接近零，模型可能因为欠拟合而表现非常糟糕，正如*图
    3.8*所示。
- en: '**An additional resource on logistic regression**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于逻辑回归的额外资源**'
- en: 'Since in-depth coverage of the individual classification algorithms exceeds
    the scope of this book, *Logistic Regression: From Introductory to Advanced Concepts
    and Applications*, *Dr. Scott Menard*, *Sage Publications*, *2009*, is recommended
    to readers who want to learn more about logistic regression.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于个别分类算法的深入涵盖超出了本书的范围，推荐给希望了解更多关于逻辑回归的读者，《逻辑回归：从入门到高级概念与应用》，斯科特·梅纳德博士，Sage Publications，2009年。
- en: Maximum margin classification with support vector machines
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机进行最大间隔分类
- en: Another powerful and widely used learning algorithm is the **support vector
    machine** (**SVM**), which can be considered an extension of the perceptron. Using
    the perceptron algorithm, we minimized misclassification errors. However, in SVMs,
    our optimization objective is to maximize the margin. The margin is defined as
    the distance between the separating hyperplane (decision boundary) and the training
    examples that are closest to this hyperplane, which are the so-called **support
    vectors**.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大且广泛使用的学习算法是**支持向量机**（**SVM**），可以看作是感知器的扩展。使用感知器算法，我们最小化了分类错误。然而，在SVM中，我们的优化目标是最大化边界。边界被定义为分离超平面（决策边界）与离该超平面最近的训练样本之间的距离，这些样本被称为**支持向量**。
- en: 'This is illustrated in *Figure 3.10*:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这在*图3.10*中有所说明：
- en: '![Chart, diagram, scatter chart  Description automatically generated](img/B17582_03_10.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图，图表，散点图 描述自动生成](img/B17582_03_10.png)'
- en: 'Figure 3.10: SVM maximizes the margin between the decision boundary and training
    data points'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：SVM在决策边界和训练数据点之间最大化间隔
- en: Maximum margin intuition
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大间隔直觉
- en: The rationale behind having decision boundaries with large margins is that they
    tend to have a lower generalization error, whereas models with small margins are
    more prone to overfitting.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界具有大间隔的背后理念在于，它们往往具有较低的泛化误差，而具有小间隔的模型更容易过拟合。
- en: Unfortunately, while the main intuition behind SVMs is relatively simple, the
    mathematics behind them is quite advanced and would require sound knowledge of
    constrained optimization.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，虽然SVM背后的主要直觉相对简单，但其中的数学内容相当深奥，需要对约束优化有扎实的理解。
- en: 'Hence, the details behind maximum margin optimization in SVMs are beyond the
    scope of this book. However, we recommend the following resources if you are interested
    in learning more:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM中最大间隔优化背后的细节超出了本书的范围。但是，如果您有兴趣了解更多内容，我们建议以下资源：
- en: 'Chris J.C. Burges’ excellent explanation in *A Tutorial on Support Vector Machines
    for Pattern Recognition* (Data Mining and Knowledge Discovery, 2(2): 121-167,
    1998)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '克里斯·J·C·伯吉斯在《支持向量机模式识别的教程》（数据挖掘与知识发现，2(2): 121-167, 1998）中的出色解释'
- en: Vladimir Vapnik’s book *The Nature of Statistical Learning Theory*, Springer
    Science+Business Media, Vladimir Vapnik, 2000
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弗拉基米尔·瓦普尼克的著作《统计学习理论的本质》，Springer Science+Business Media, 2000
- en: Andrew Ng’s very detailed lecture notes available at [https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf](https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安德鲁·吴的非常详细的讲义笔记，可在[https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf](https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf)获取
- en: Dealing with a nonlinearly separable case using slack variables
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用松弛变量处理非线性可分情况
- en: Although we don’t want to dive much deeper into the more involved mathematical
    concepts behind the maximum-margin classification, let’s briefly mention the so-called
    *slack variable*, which was introduced by Vladimir Vapnik in 1995 and led to the
    so-called **soft-margin classification**. The motivation for introducing the slack
    variable was that the linear constraints in the SVM optimization objective need
    to be relaxed for nonlinearly separable data to allow the convergence of the optimization
    in the presence of misclassifications, under appropriate loss penalization.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不打算深入探讨最大间隔分类背后更复杂的数学概念，但让我们简要提到所谓的*松弛变量*，它由弗拉基米尔·瓦普尼克在1995年引入，导致了所谓的**软间隔分类**。引入松弛变量的动机是，在非线性可分数据中，SVM优化目标中的线性约束需要放宽，以允许在适当的损失惩罚下优化的收敛，即使存在分类错误。
- en: 'The use of the slack variable, in turn, introduces the variable, which is commonly
    referred to as *C* in SVM contexts. We can consider *C* as a hyperparameter for
    controlling the penalty for misclassification. Large values of *C* correspond
    to large error penalties, whereas we are less strict about misclassification errors
    if we choose smaller values for *C*. We can then use the *C* parameter to control
    the width of the margin and therefore tune the bias-variance tradeoff, as illustrated
    in *Figure 3.11*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 松弛变量的使用引入了常被称为*S*的变量，在支持向量机的上下文中，我们可以把*S*看作是控制误分类惩罚的超参数。较大的*S*值对应着较大的误差惩罚，而如果选择较小的*S*值，则对误分类错误的严格性较低。我们可以使用*S*参数来控制边界的宽度，从而调整偏差-方差的权衡，如*图
    3.11*所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_03_11.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17582_03_11.png)'
- en: 'Figure 3.11: The impact of large and small values of the inverse regularization
    strength *C* on classification'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.11: 逆正则化强度大值和小值对分类的影响'
- en: This concept is related to regularization, which we discussed in the previous
    section in the context of regularized regression, where decreasing the value of
    `C` increases the bias (underfitting) and lowers the variance (overfitting) of
    the model.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念与正则化有关，我们在前一节中讨论了正则化回归的情况，在这种情况下，减小`C`的值会增加模型的偏差（欠拟合），降低方差（过拟合）。
- en: 'Now that we have learned the basic concepts behind a linear SVM, let’s train
    an SVM model to classify the different flowers in our Iris dataset:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了线性支持向量机的基本概念，让我们训练一个SVM模型来对我们鸢尾花数据集中的不同花进行分类：
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The three decision regions of the SVM, visualized after training the classifier
    on the Iris dataset by executing the preceding code example, are shown in *Figure
    3.12*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用前述代码示例训练鸢尾花数据集的分类器后，展示了支持向量机的三个决策区域，如*图 3.12*所示：
- en: '![A picture containing scatter chart  Description automatically generated](img/B17582_03_12.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含散点图  自动生成的图片描述](img/B17582_03_12.png)'
- en: 'Figure 3.12: SVM’s decision regions'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.12: 支持向量机的决策区域'
- en: '**Logistic regression versus SVMs**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归与支持向量机**'
- en: In practical classification tasks, linear logistic regression and linear SVMs
    often yield very similar results. Logistic regression tries to maximize the conditional
    likelihoods of the training data, which makes it more prone to outliers than SVMs,
    which mostly care about the points that are closest to the decision boundary (support
    vectors). On the other hand, logistic regression has the advantage of being a
    simpler model and can be implemented more easily, and is mathematically easier
    to explain. Furthermore, logistic regression models can be easily updated, which
    is attractive when working with streaming data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际分类任务中，线性逻辑回归和线性支持向量机通常产生非常相似的结果。逻辑回归试图最大化训练数据的条件概率，这使得它比支持向量机更容易受到异常值的影响，后者主要关注最靠近决策边界的点（支持向量）。另一方面，逻辑回归有简单模型的优势，并且更容易实现，数学上更容易解释。此外，逻辑回归模型可以轻松更新，在处理流数据时非常有吸引力。
- en: Alternative implementations in scikit-learn
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在scikit-learn中的替代实现方式
- en: The scikit-learn library’s `LogisticRegression` class, which we used in the
    previous sections, can make use of the LIBLINEAR library by setting `solver='liblinear'`.
    LIBLINEAR is a highly optimized C/C++ library developed at the National Taiwan
    University ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库中的`LogisticRegression`类，我们在前面的章节中使用过，可以通过设置`solver='liblinear'`来使用LIBLINEAR库。LIBLINEAR是台湾大学开发的高度优化的C/C++库（[http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)）。
- en: Similarly, the `SVC` class that we used to train an SVM makes use of LIBSVM,
    which is an equivalent C/C++ library specialized for SVMs ([http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们用来训练支持向量机的`SVC`类使用了LIBSVM，这是一种专门用于支持向量机的等效C/C++库（[http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)）。
- en: The advantage of using LIBLINEAR and LIBSVM over, for example, native Python
    implementations is that they allow the extremely quick training of large amounts
    of linear classifiers. However, sometimes our datasets are too large to fit into
    computer memory. Thus, scikit-learn also offers alternative implementations via
    the `SGDClassifier` class, which also supports online learning via the `partial_fit`
    method. The concept behind the `SGDClassifier` class is similar to the stochastic
    gradient algorithm that we implemented in *Chapter 2* for Adaline.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LIBLINEAR和LIBSVM相比于例如原生Python实现的优势在于，它们允许快速训练大量线性分类器。然而，有时我们的数据集太大而无法放入计算机内存。因此，scikit-learn还通过`SGDClassifier`类提供了替代实现，该类还通过`partial_fit`方法支持在线学习。`SGDClassifier`类的概念类似于我们在*第2章*中为Adaline实现的随机梯度算法。
- en: 'We could initialize the SGD version of the perceptron (`loss=''perceptron''`),
    logistic regression (`loss=''log''`), and an SVM with default parameters (`loss=''hinge''`),
    as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以初始化感知器的SGD版本（`loss='perceptron'`）、逻辑回归（`loss='log'`）和具有默认参数的SVM（`loss='hinge'`），如下所示：
- en: '[PRE24]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Solving nonlinear problems using a kernel SVM
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核SVM解决非线性问题
- en: Another reason why SVMs enjoy high popularity among machine learning practitioners
    is that they can be easily **kernelized** to solve nonlinear classification problems.
    Before we discuss the main concept behind the so-called **kernel SVM**, the most
    common variant of SVMs, let’s first create a synthetic dataset to see what such
    a nonlinear classification problem may look like.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）在机器学习从业者中享有极高的流行度的另一个原因是，它们可以轻松地应用**核技巧**来解决非线性分类问题。在我们讨论所谓的**核SVM**的主要概念之前，让我们首先创建一个合成数据集，看看这样一个非线性分类问题可能会是什么样子。
- en: Kernel methods for linearly inseparable data
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性不可分数据的核方法
- en: 'Using the following code, we will create a simple dataset that has the form
    of an XOR gate using the `logical_or` function from NumPy, where 100 examples
    will be assigned the class label `1`, and 100 examples will be assigned the class
    label `-1`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们将使用NumPy的`logical_or`函数创建一个形如XOR门的简单数据集，其中100个示例将被分配类标签`1`，另外100个示例将被分配类标签`-1`：
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After executing the code, we will have an XOR dataset with random noise, as
    shown in *Figure 3.13*:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们将得到一个带有随机噪声的XOR数据集，如*图3.13*所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_03_13.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的散点图描述](img/B17582_03_13.png)'
- en: 'Figure 3.13: A plot of the XOR dataset'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：XOR数据集的绘图
- en: Obviously, we would not be able to separate the examples from the positive and
    negative class very well using a linear hyperplane as a decision boundary via
    the linear logistic regression or linear SVM model that we discussed in earlier
    sections.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果我们使用线性超平面作为决策边界，通过前面章节讨论过的线性逻辑回归或线性支持向量机模型，我们将无法很好地将正类和负类的示例分开。
- en: 'The basic idea behind **kernel methods** for dealing with such linearly inseparable
    data is to create nonlinear combinations of the original features to project them
    onto a higher-dimensional space via a mapping function, ![](img/B17582_03_047.png),
    where the data becomes linearly separable. As shown in *Figure 3.14*, we can transform
    a two-dimensional dataset into a new three-dimensional feature space, where the
    classes become separable via the following projection:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种线性不可分数据的**核方法**的基本思想是通过映射函数创建原始特征的非线性组合，将其投影到高维空间中，使数据变得线性可分，如![](img/B17582_03_047.png)所示。正如*图3.14*所示，我们可以将一个二维数据集转换为一个新的三维特征空间，其中通过以下投影使得类别变得可分：
- en: '![](img/B17582_03_048.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_048.png)'
- en: 'This allows us to separate the two classes shown in the plot via a linear hyperplane
    that becomes a nonlinear decision boundary if we project it back onto the original
    feature space, as illustrated with the following concentric circle dataset:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够通过一个线性超平面将绘图中显示的两类分开，如果我们将其投影回原始特征空间，则这将成为一个非线性决策边界，如下所示的同心圆数据集：
- en: '![Diagram  Description automatically generated](img/B17582_03_14.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_03_14.png)'
- en: 'Figure 3.14: The process of classifying nonlinear data using kernel methods'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：使用核方法对非线性数据进行分类的过程
- en: Using the kernel trick to find separating hyperplanes in a high-dimensional
    space
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在高维空间中使用核技巧找到分离超平面
- en: To solve a nonlinear problem using an SVM, we would transform the training data
    into a higher-dimensional feature space via a mapping function, ![](img/B17582_03_047.png),
    and train a linear SVM model to classify the data in this new feature space. Then,
    we could use the same mapping function, ![](img/B17582_03_050.png), to transform
    new, unseen data to classify it using the linear SVM model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用支持向量机解决非线性问题，我们会通过映射函数 ![](img/B17582_03_047.png) 将训练数据转换为更高维度的特征空间，并训练一个线性支持向量机模型来在这个新的特征空间中对数据进行分类。然后，我们可以使用同样的映射函数
    ![](img/B17582_03_050.png) 将新的未见数据进行转换，并使用线性支持向量机模型进行分类。
- en: However, one problem with this mapping approach is that the construction of
    the new features is computationally very expensive, especially if we are dealing
    with high-dimensional data. This is where the so-called **kernel trick** comes
    into play.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种映射方法的一个问题是构建新特征在计算上非常昂贵，特别是在处理高维数据时。这就是所谓的**核技巧**发挥作用的地方。
- en: 'Although we did not go into much detail about how to solve the quadratic programming
    task to train an SVM, in practice, we just need to replace the dot product **x**^(^i^)^T**x**^(^j^)
    by ![](img/B17582_03_051.png). To save the expensive step of calculating this
    dot product between two points explicitly, we define a so-called **kernel function**:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有详细介绍如何解决二次规划任务来训练支持向量机，但在实践中，我们只需将点积 **x**^(^i^)^T**x**^(^j^) 替换为 ![](img/B17582_03_051.png)。为了避免显式计算两个点之间的点积步骤，我们定义了所谓的**核函数**：
- en: '![](img/B17582_03_052.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_052.png)'
- en: 'One of the most widely used kernels is the **radial basis function** (**RBF**)
    kernel, which can simply be called the **Gaussian kernel**:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的核函数之一是**径向基函数**（**RBF**）核，也可以简称为**高斯核**：
- en: '![](img/B17582_03_053.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_053.png)'
- en: 'This is often simplified to:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被简化为：
- en: '![](img/B17582_03_054.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_054.png)'
- en: Here, ![](img/B17582_03_055.png) is a free parameter to be optimized.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B17582_03_055.png) 是要优化的自由参数。
- en: Roughly speaking, the term “kernel” can be interpreted as a **similarity function**
    between a pair of examples. The minus sign inverts the distance measure into a
    similarity score, and, due to the exponential term, the resulting similarity score
    will fall into a range between 1 (for exactly similar examples) and 0 (for very
    dissimilar examples).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 大致而言，术语“核”可以被解释为一对示例之间的**相似度函数**。负号将距离度量反转为相似度评分，并且由于指数项的存在，结果的相似度评分将落入1（完全相似示例）到0（非常不相似示例）的范围内。
- en: 'Now that we have covered the big picture behind the kernel trick, let’s see
    if we can train a kernel SVM that is able to draw a nonlinear decision boundary
    that separates the XOR data well. Here, we simply use the `SVC` class from scikit-learn
    that we imported earlier and replace the `kernel=''linear''` parameter with `kernel=''rbf''`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了核技巧背后的大局，让我们看看能否训练一个能够很好地分离异或数据的核支持向量机。在这里，我们简单地使用我们之前导入的 `SVC` 类，并将
    `kernel='linear'` 参数替换为 `kernel='rbf'`：
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we can see in the resulting plot, the kernel SVM separates the XOR data
    relatively well:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在结果图中所见，核支持向量机相对较好地分离了异或数据：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_03_15.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  生成的描述](img/B17582_03_15.png)'
- en: 'Figure 3.15: The decision boundary on the XOR data using a kernel method'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：使用核方法在异或数据上的决策边界
- en: 'The ![](img/B17582_03_056.png) parameter, which we set to `gamma=0.1`, can
    be understood as a cut-off parameter for the Gaussian sphere. If we increase the
    value for ![](img/B17582_03_056.png), we increase the influence or reach of the
    training examples, which leads to a tighter and bumpier decision boundary. To
    get a better understanding of ![](img/B17582_03_056.png), let’s apply an RBF kernel
    SVM to our Iris flower dataset:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将参数 ![](img/B17582_03_056.png)，设置为 `gamma=0.1`，可以理解为高斯球体的截断参数。如果增加 ![](img/B17582_03_056.png)
    的值，我们将增加训练示例的影响或范围，这将导致更紧密和更崎岖的决策边界。为了更好地理解 ![](img/B17582_03_056.png)，让我们将RBF核支持向量机应用于我们的鸢尾花数据集：
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since we chose a relatively small value for ![](img/B17582_03_056.png), the
    resulting decision boundary of the RBF kernel SVM model will be relatively soft,
    as shown in *Figure 3.16*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了相对较小的值来设置 ![](img/B17582_03_056.png)，所以RBF核支持向量机模型的决策边界将比较柔和，如*图 3.16*所示：
- en: '![A picture containing chart  Description automatically generated](img/B17582_03_16.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图表的图片  生成的描述](img/B17582_03_16.png)'
- en: 'Figure 3.16: The decision boundaries on the Iris dataset using an RBF kernel
    SVM model with a small ![](img/B17582_03_056.png) value'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16：使用具有小*图 3.17*值的RBF核SVM模型在鸢尾花数据集上的决策边界
- en: 'Now, let’s increase the value of ![](img/B17582_03_056.png) and observe the
    effect on the decision boundary:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们增加*图 3.17*中的值并观察决策边界的效果：
- en: '[PRE28]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In *Figure 3.17*, we can now see that the decision boundary around the classes
    `0` and `1` is much tighter using a relatively large value of ![](img/B17582_03_056.png):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 3.17*中，我们现在可以看到围绕类别`0`和`1`的决策边界使用了一个相对较大的值的情况：
- en: '![Scatter chart  Description automatically generated](img/B17582_03_17.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![散点图 描述自动生成](img/B17582_03_17.png)'
- en: 'Figure 3.17: The decision boundaries on the Iris dataset using an RBF kernel
    SVM model with a large ![](img/B17582_03_056.png) value'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17：使用具有大*图 3.17*值的RBF核SVM模型在鸢尾花数据集上的决策边界
- en: Although the model fits the training dataset very well, such a classifier will
    likely have a high generalization error on unseen data. This illustrates that
    the ![](img/B17582_03_064.png) parameter also plays an important role in controlling
    overfitting or variance when the algorithm is too sensitive to fluctuations in
    the training dataset.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型非常适合训练数据集，这样的分类器可能在未见数据上具有很高的泛化误差。这说明了当算法对训练数据中的波动过于敏感时，参数![](img/B17582_03_064.png)也在控制过拟合或方差中扮演着重要角色。
- en: Decision tree learning
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树学习
- en: '**Decision tree** classifiers are attractive models if we care about interpretability.
    As the name “decision tree” suggests, we can think of this model as breaking down
    our data by making a decision based on asking a series of questions.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**分类器是有吸引力的模型，如果我们关心解释性的话。正如“决策树”这个名称所暗示的，我们可以将这个模型看作通过提出一系列问题来分解我们的数据。'
- en: 'Let’s consider the following example in which we use a decision tree to decide
    upon an activity on a particular day:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子，我们使用决策树来决定特定一天的活动：
- en: '![](img/B17582_03_18.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_18.png)'
- en: 'Figure 3.18: An example of a decision tree'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18：决策树的一个示例
- en: 'Based on the features in our training dataset, the decision tree model learns
    a series of questions to infer the class labels of the examples. Although *Figure
    3.18* illustrates the concept of a decision tree based on categorical variables,
    the same concept applies if our features are real numbers, like in the Iris dataset.
    For example, we could simply define a cut-off value along the **sepal width**
    feature axis and ask a binary question: “Is the sepal width ≥ 2.8 cm?”'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们训练数据集中的特征，决策树模型学习一系列问题，以推断示例的类别标签。虽然*图 3.18*说明了基于分类变量的决策树的概念，但如果我们的特征是实数，比如在鸢尾花数据集中，相同的概念也适用。例如，我们可以简单地沿着**萼片宽度**特征轴定义一个截断值，并提出一个二元问题：“萼片宽度
    ≥ 2.8 cm吗？”
- en: Using the decision algorithm, we start at the tree root and split the data on
    the feature that results in the largest **information gain** (**IG**), which will
    be explained in more detail in the following section. In an iterative process,
    we can then repeat this splitting procedure at each child node until the leaves
    are pure. This means that the training examples at each node all belong to the
    same class. In practice, this can result in a very deep tree with many nodes,
    which can easily lead to overfitting. Thus, we typically want to **prune** the
    tree by setting a limit for the maximum depth of the tree.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策算法，我们从树根开始，并根据导致最大**信息增益**（**IG**）的特征来分割数据，这将在接下来的部分详细解释。在迭代过程中，我们可以在每个子节点重复这个分割过程，直到叶子节点是纯净的。这意味着每个节点上的训练示例都属于同一类别。实际上，这可能导致一个非常深的树，有许多节点，这很容易导致过拟合。因此，我们通常希望通过设置树的最大深度来**修剪**树。
- en: Maximizing IG – getting the most bang for your buck
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大化信息增益 - 在投入最少的前提下获得最大的收益
- en: 'To split the nodes at the most informative features, we need to define an objective
    function to optimize via the tree learning algorithm. Here, our objective function
    is to maximize the IG at each split, which we define as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 要在最具信息性的特征上分割节点，我们需要定义一个通过树学习算法优化的目标函数。在这里，我们的目标函数是在每次分割时最大化信息增益，定义如下：
- en: '![](img/B17582_03_065.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_065.png)'
- en: 'Here, *f* is the feature to perform the split; *D*[p] and *D*[j] are the dataset
    of the parent and *j*th child node; *I* is our **impurity** measure; *N*[p] is
    the total number of training examples at the parent node; and *N*[j] is the number
    of examples in the *j*th child node. As we can see, the information gain is simply
    the difference between the impurity of the parent node and the sum of the child
    node impurities—the lower the impurities of the child nodes, the larger the information
    gain. However, for simplicity and to reduce the combinatorial search space, most
    libraries (including scikit-learn) implement binary decision trees. This means
    that each parent node is split into two child nodes, *D*[left] and *D*[right]:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是要执行分割的特征；*D*[p] 和 *D*[j] 分别是父节点和第 *j* 个子节点的数据集；*I* 是我们的**不纯度**度量；*N*[p]
    是父节点的总训练示例数；*N*[j] 是第 *j* 个子节点中示例的数目。正如我们所见，信息增益简单地是父节点的不纯度与子节点不纯度之和的差异——子节点的不纯度越低，信息增益越大。然而，为了简化和减少组合搜索空间，大多数库（包括
    scikit-learn）实现二叉决策树。这意味着每个父节点分裂为两个子节点，*D*[left] 和 *D*[right]：
- en: '![](img/B17582_03_066.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_066.png)'
- en: 'The three impurity measures or splitting criteria that are commonly used in
    binary decision trees are **Gini impurity** (*I*[G]), **entropy** (*I*[H]), and
    the **classification error** (*I*[E]). Let’s start with the definition of entropy
    for all **non-empty** classes (![](img/B17582_03_067.png)):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在二叉决策树中常用的三种不纯度度量或分裂准则是**基尼不纯度**（*I*[G]）、**熵**（*I*[H]）和**分类错误**（*I*[E]）。让我们从所有**非空**类的熵定义开始（![](img/B17582_03_067.png)）：
- en: '![](img/B17582_03_068.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_068.png)'
- en: Here, *p*(*i*|*t*) is the proportion of the examples that belong to class *i*
    for a particular node, *t*. The entropy is therefore 0 if all examples at a node
    belong to the same class, and the entropy is maximal if we have a uniform class
    distribution. For example, in a binary class setting, the entropy is 0 if *p*(*i*=1|*t*) = 1
    or *p*(*i*=0|*t*) = 0\. If the classes are distributed uniformly with *p*(*i*=1|*t*) = 0.5
    and *p*(*i*=0|*t*) = 0.5, the entropy is 1\. Therefore, we can say that the entropy
    criterion attempts to maximize the mutual information in the tree.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*(*i*|*t*) 是属于特定节点 *t* 的类 *i* 示例的比例。因此，如果一个节点上的所有示例属于同一类，则熵为 0，如果我们具有均匀的类分布，则熵是最大的。例如，在二元类设置中，如果
    *p*(*i*=1|*t*) = 1 或 *p*(*i*=0|*t*) = 0，则熵为 0。如果类均匀分布，其中 *p*(*i*=1|*t*) = 0.5
    和 *p*(*i*=0|*t*) = 0.5，则熵为 1。因此，我们可以说熵准则试图在树中最大化互信息。
- en: 'To provide a visual intuition, let us visualize the entropy values for different
    class distributions via the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供直观感受，让我们通过以下代码可视化不同类分布的熵值：
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*Figure 3.19* below shows the plot produced by the preceding code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.19* 如下所示是前述代码生成的图形：'
- en: '![Venn diagram  Description automatically generated](img/B17582_03_19.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![Venn diagram  Description automatically generated](img/B17582_03_19.png)'
- en: 'Figure 3.19: Entropy values for different class-membership probabilities'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19：不同类成员概率的熵值
- en: 'The Gini impurity can be understood as a criterion to minimize the probability
    of misclassification:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度可以理解为最小化误分类的概率准则：
- en: '![](img/B17582_03_069.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_069.png)'
- en: 'Similar to entropy, the Gini impurity is maximal if the classes are perfectly
    mixed, for example, in a binary class setting (*c* = 2):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 与熵类似，**基尼不纯度**在类别完全混合时达到最大值，例如，在二元类别设置中（*c* = 2）：
- en: '![](img/B17582_03_070.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_070.png)'
- en: However, in practice, both the Gini impurity and entropy typically yield very
    similar results, and it is often not worth spending much time on evaluating trees
    using different impurity criteria rather than experimenting with different pruning
    cut-offs. In fact, as you will see later in *Figure 3.21*, both the Gini impurity
    and entropy have a similar shape.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，基尼不纯度和熵通常产生非常相似的结果，因此通常不值得花费大量时间评估不同的不纯度准则，而是试验不同的修剪阈值。实际上，正如您将在图 3.21
    中看到的那样，基尼不纯度和熵的形状是相似的。
- en: 'Another impurity measure is the classification error:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不纯度度量是分类错误：
- en: '![](img/B17582_03_071.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_071.png)'
- en: 'This is a useful criterion for pruning, but not recommended for growing a decision
    tree, since it is less sensitive to changes in the class probabilities of the
    nodes. We can illustrate this by looking at the two possible splitting scenarios
    shown in *Figure 3.20*:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有用的修剪准则，但不推荐用于生成决策树，因为它对节点类别概率的变化不敏感。我们可以通过查看图 3.20 中显示的两种可能的分割场景来说明这一点：
- en: '![](img/B17582_03_20.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_20.png)'
- en: 'Figure 3.20: Decision tree data splits'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20：决策树数据分割
- en: 'We start with a dataset, *D*[p], at the parent node, which consists of 40 examples
    from class 1 and 40 examples from class 2 that we split into two datasets, *D*[left]
    and *D*[right]. The information gain using the classification error as a splitting
    criterion would be the same (*IG*[E] = 0.25) in both scenarios, *A* and *B*:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从父节点开始，有一个数据集 *D*[p]，其中包含来自类别 1 的 40 个示例和来自类别 2 的 40 个示例，我们将其分成两个数据集 *D*[left]
    和 *D*[right]。使用分类错误作为分裂准则的信息增益在情景 A 和 B 中是相同的 (*IG*[E] = 0.25)：
- en: '![](img/B17582_03_072.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_072.png)'
- en: 'However, the Gini impurity would favor the split in scenario *B* (![](img/B17582_03_073.png))
    over scenario *A* (*IG*[G] = 0.125), which is indeed purer:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基尼不纯度会偏向于情景 B 的分裂（![](img/B17582_03_073.png))，而非情景 A（*IG*[G] = 0.125），这的确更加纯净：
- en: '![](img/B17582_03_074.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_074.png)'
- en: 'Similarly, the entropy criterion would also favor scenario B (*IG*[H] = 0.31)
    over scenario A (*IG*[H] = 0.19):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，熵准则也会偏向于情景 B (*IG*[H] = 0.31)，而非情景 A (*IG*[H] = 0.19)：
- en: '![](img/B17582_03_075.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_075.png)'
- en: 'For a more visual comparison of the three different impurity criteria that
    we discussed previously, let’s plot the impurity indices for the probability range
    [0, 1] for class 1\. Note that we will also add a scaled version of the entropy
    (entropy / 2) to observe that the Gini impurity is an intermediate measure between
    entropy and the classification error. The code is as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地比较我们之前讨论过的三种不纯度准则，让我们绘制类别 1 的概率范围 [0, 1] 内的不纯度指数。请注意，我们还将添加熵的缩放版本（熵 /
    2）以观察到基尼不纯度是熵和分类错误之间的中间度量。代码如下：
- en: '[PRE30]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The plot produced by the preceding code example is as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码示例生成的图形如下：
- en: '![Chart  Description automatically generated](img/B17582_03_21.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成描述的图表](img/B17582_03_21.png)'
- en: 'Figure 3.21: The different impurity indices for different class-membership
    probabilities between 0 and 1'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21：不同类别成员概率在 0 到 1 之间的不纯度指数
- en: Building a decision tree
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: Decision trees can build complex decision boundaries by dividing the feature
    space into rectangles. However, we have to be careful since the deeper the decision
    tree, the more complex the decision boundary becomes, which can easily result
    in overfitting. Using scikit-learn, we will now train a decision tree with a maximum
    depth of 4, using the Gini impurity as a criterion for impurity.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以通过将特征空间划分为矩形来构建复杂的决策边界。然而，我们必须小心，因为决策树越深，决策边界就会变得越复杂，这很容易导致过拟合。使用 scikit-learn，我们现在将训练一个最大深度为
    4 的决策树，使用基尼不纯度作为不纯度的标准。
- en: 'Although feature scaling may be desired for visualization purposes, note that
    feature scaling is not a requirement for decision tree algorithms. The code is
    as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管出于可视化目的可能希望进行特征缩放，请注意，对于决策树算法来说，特征缩放并不是必需的。代码如下：
- en: '[PRE31]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After executing the code example, we get the typical axis-parallel decision
    boundaries of the decision tree:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码示例后，我们得到了决策树的典型轴对齐决策边界：
- en: '![Scatter chart  Description automatically generated](img/B17582_03_22.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成描述的散点图](img/B17582_03_22.png)'
- en: 'Figure 3.22: The decision boundaries of the Iris data using a decision tree'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22：使用决策树的鸢尾花数据的决策边界
- en: 'A nice feature in scikit-learn is that it allows us to readily visualize the
    decision tree model after training via the following code:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中的一个很好的特性是，它允许我们在训练后直接可视化决策树模型，代码如下：
- en: '[PRE32]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![A picture containing text, sign  Description automatically generated](img/B17582_03_23.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![包含自动生成描述的文字的图片](img/B17582_03_23.png)'
- en: 'Figure 3.23: A decision tree model fit to the Iris dataset'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23：拟合到鸢尾花数据集的决策树模型
- en: Setting `filled=True` in the `plot_tree` function we called colors the nodes
    by the majority class label at that node. There are many additional options available,
    which you can find in the documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们调用的 `plot_tree` 函数中设置 `filled=True` 将会根据该节点处的主要类标签为节点着色。还有许多其他选项可供选择，您可以在文档中找到：[https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)。
- en: Looking at the decision tree figure, we can now nicely trace back the splits
    that the decision tree determined from our training dataset. Regarding the feature
    splitting criterion at each node, note that the branches to the left correspond
    to “True” and branches to the right correspond to “False.”
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 查看决策树图，现在我们可以清楚地追溯决策树从训练数据集中确定的分割。关于每个节点的特征分割标准，请注意，向左的分支对应“True”，向右的分支对应“False”。
- en: Looking at the root node, it starts with 105 examples at the top. The first
    split uses a sepal width cut-off ≤ 0.75 cm for splitting the root node into two
    child nodes with 35 examples (left child node) and 70 examples (right child node).
    After the first split, we can see that the left child node is already pure and
    only contains examples from the `Iris-setosa` class (Gini impurity = 0). The further
    splits on the right are then used to separate the examples from the `Iris-versicolor`
    and `Iris-virginica` class.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 查看根节点，顶部有105个示例。第一个分割使用萼片宽度≤ 0.75 cm的截止值将根节点分割为两个子节点，左子节点有35个示例，右子节点有70个示例。第一个分割后，我们可以看到左子节点已经是纯净的，只包含来自`Iris-setosa`类的示例（基尼不纯度 = 0）。然后，右侧的进一步分割用于将示例从`Iris-versicolor`和`Iris-virginica`类中分离出来。
- en: Looking at this tree, and the decision region plot of the tree, we can see that
    the decision tree does a very good job of separating the flower classes. Unfortunately,
    scikit-learn currently does not implement functionality to manually post-prune
    a decision tree. However, we could go back to our previous code example, change
    the `max_depth` of our decision tree to `3`, and compare it to our current model,
    but we leave this as an exercise for the interested reader.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这棵树，以及树的决策区域图，我们可以看到决策树在分离花卉类别方面做得非常好。不幸的是，scikit-learn目前没有实现手动后修剪决策树的功能。但是，我们可以回到之前的代码示例，将我们的决策树的`max_depth`更改为`3`，然后与当前模型进行比较，但我们将此留给有兴趣的读者作为练习。
- en: 'Alternatively, scikit-learn provides an automatic cost complexity post-pruning
    procedure for decision trees. Interested readers can find more information about
    this more advanced topic in the following tutorial: [https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，scikit-learn提供了决策树的自动成本复杂度后修剪过程。有兴趣的读者可以在以下教程中找到关于这个更高级主题的更多信息：[https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)。
- en: Combining multiple decision trees via random forests
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过随机森林组合多个决策树
- en: 'Ensemble methods have gained huge popularity in applications of machine learning
    during the last decade due to their good classification performance and robustness
    toward overfitting. While we are going to cover different ensemble methods, including
    **bagging** and **boosting**, later in *Chapter 7*, *Combining Different Models
    for Ensemble Learning*, let’s discuss the decision tree-based **random forest**
    algorithm, which is known for its good scalability and ease of use. A random forest
    can be considered as an **ensemble** of decision trees. The idea behind a random
    forest is to average multiple (deep) decision trees that individually suffer from
    high variance to build a more robust model that has a better generalization performance
    and is less susceptible to overfitting. The random forest algorithm can be summarized
    in four simple steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在机器学习应用中已经广受欢迎，因为它们具有良好的分类性能并且对过拟合具有鲁棒性。虽然我们将在*第7章*“组合不同模型进行集成学习”中涵盖不同的集成方法，包括**装袋**和**提升**，让我们先讨论基于决策树的**随机森林**算法，这个算法以其良好的可扩展性和易用性而闻名。随机森林可以被看作是决策树的**集成**。随机森林的理念是将多个（深度）决策树进行平均，这些树个体上具有高方差，以建立一个更健壮的模型，具有更好的泛化性能，并且不易过拟合。随机森林算法可以总结为四个简单步骤：
- en: Draw a random **bootstrap** sample of size *n* (randomly choose *n* examples
    from the training dataset with replacement).
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制大小为*n*的随机**bootstrap**样本（从训练数据集中用替换随机选择*n*个示例）。
- en: 'Grow a decision tree from the bootstrap sample. At each node:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从bootstrap样本中生长一棵决策树。在每个节点：
- en: Randomly select *d* features without replacement.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择*d*个特征，不替换。
- en: Split the node using the feature that provides the best split according to the
    objective function, for instance, maximizing the information gain.
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供最佳分割的特征来分割节点，例如，最大化信息增益的目标函数。
- en: Repeat *steps 1*-*2* *k* times.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤1*-*2* *k*次。
- en: Aggregate the prediction by each tree to assign the class label by **majority
    vote**. Majority voting will be discussed in more detail in *Chapter 7*.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过每棵树的预测结果进行聚合，通过**多数投票**分配类标签。关于多数投票的详细讨论将在*第7章*中进行。
- en: 'We should note one slight modification in *step 2* when we are training the
    individual decision trees: instead of evaluating all features to determine the
    best split at each node, we only consider a random subset of those.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练单独的决策树时，我们需要注意*步骤2*中的一个小修改：在每个节点评估最佳分割时，我们只考虑一部分随机选择的特征。
- en: '**Sampling with and without replacement**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**带替换和不带替换抽样**'
- en: In case you are not familiar with the terms sampling “with” and “without” replacement,
    let’s walk through a simple thought experiment. Let’s assume that we are playing
    a lottery game where we randomly draw numbers from an urn. We start with an urn
    that holds five unique numbers, 0, 1, 2, 3, and 4, and we draw exactly one number
    on each turn. In the first round, the chance of drawing a particular number from
    the urn would be 1/5\. Now, in sampling without replacement, we do not put the
    number back into the urn after each turn. Consequently, the probability of drawing
    a particular number from the set of remaining numbers in the next round depends
    on the previous round. For example, if we have a remaining set of numbers 0, 1,
    2, and 4, the chance of drawing number 0 would become 1/4 in the next turn.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对“带”和“不带”替换抽样这些术语不太熟悉，让我们通过一个简单的思维实验来理解一下。假设我们在玩一个抽奖游戏，从一个罐子里随机抽取数字。开始时，罐子里有五个唯一的数字：0、1、2、3和4，每次抽取一个数字。在第一轮中，从罐子中抽取特定数字的概率为1/5。现在，在不替换抽样中，我们在每次抽取后不将数字放回罐子里。因此，在下一轮中，从剩余数字集合中抽取特定数字的概率取决于上一轮的情况。例如，如果我们剩下的数字集合是0、1、2和4，那么下一轮抽取数字0的概率将变为1/4。
- en: 'However, in random sampling with replacement, we always return the drawn number
    to the urn so that the probability of drawing a particular number at each turn
    does not change; we can draw the same number more than once. In other words, in
    sampling *with* replacement, the samples (numbers) are independent and have a
    covariance of zero. For example, the results from five rounds of drawing random
    numbers could look like this:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在有放回随机抽样中，我们总是将抽取的数字放回罐子中，因此每次抽取特定数字的概率不会改变；我们可以多次抽取相同的数字。换句话说，在带替换抽样中，样本（数字）是独立的，且具有零的协方差。例如，五轮随机抽取数字的结果可能如下：
- en: 'Random sampling without replacement: 2, 1, 3, 4, 0'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无放回随机抽样：2, 1, 3, 4, 0
- en: 'Random sampling with replacement: 1, 3, 3, 4, 1'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有放回随机抽样：1, 3, 3, 4, 1
- en: Although random forests don’t offer the same level of interpretability as decision
    trees, a big advantage of random forests is that we don’t have to worry so much
    about choosing good hyperparameter values. We typically don’t need to prune the
    random forest since the ensemble model is quite robust to noise from averaging
    the predictions among the individual decision trees. The only parameter that we
    need to care about in practice is the number of trees, *k*, (*step 3*) that we
    choose for the random forest. Typically, the larger the number of trees, the better
    the performance of the random forest classifier at the expense of an increased
    computational cost.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随机森林的解释能力不如决策树，但随机森林的一个很大优势是我们不必过多担心选择良好的超参数值。通常情况下，我们不需要修剪随机森林，因为集成模型对于来自个体决策树预测的噪声具有相当的鲁棒性。在实践中，我们只需要关心一个参数，即随机森林中选择的树的数量，*k*，（*步骤3*）。通常来说，树的数量越多，随机森林分类器的性能越好，但计算成本也会增加。
- en: Although it is less common in practice, other hyperparameters of the random
    forest classifier that can be optimized—using techniques that we will discuss
    in *Chapter 6*, *Learning Best Practices for Model Evaluation and Hyperparameter**Tuning*—are
    the size, *n*, of the bootstrap sample (*step 1*) and the number of features,
    *d*, that are randomly chosen for each split (*step 2a*), respectively. Via the
    sample size, *n*, of the bootstrap sample, we control the bias-variance tradeoff
    of the random forest.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在实践中较少见，随机森林分类器的其他超参数也可以进行优化——我们将在*第6章*“学习模型评估和超参数调优最佳实践”中讨论使用的技术——分别是bootstrap样本的大小*n*（*步骤1*）和每次分割时随机选择的特征数*d*（*步骤2a*）。通过bootstrap样本的大小*n*，我们控制随机森林的偏差-方差权衡。
- en: Decreasing the size of the bootstrap sample increases the diversity among the
    individual trees since the probability that a particular training example is included
    in the bootstrap sample is lower. Thus, shrinking the size of the bootstrap samples
    may increase the *randomness* of the random forest, and it can help to reduce
    the effect of overfitting. However, smaller bootstrap samples typically result
    in a lower overall performance of the random forest and a small gap between training
    and test performance, but a low test performance overall. Conversely, increasing
    the size of the bootstrap sample may increase the degree of overfitting. Because
    the bootstrap samples, and consequently the individual decision trees, become
    more similar to one another, they learn to fit the original training dataset more
    closely.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 减小引导样本的大小会增加单个树之间的多样性，因为特定训练示例被包含在引导样本中的概率较低。因此，缩小引导样本的大小可能会增加随机森林的*随机性*，有助于减少过拟合的影响。然而，较小的引导样本通常导致随机森林的整体性能较低，并且训练和测试性能之间的差距较小，但总体测试性能较低。相反，增加引导样本的大小可能会增加过拟合的程度。因为引导样本，以及因此单个决策树，变得更加相似，它们学会更加紧密地拟合原始训练数据集。
- en: In most implementations, including the `RandomForestClassifier` implementation
    in scikit-learn, the size of the bootstrap sample is chosen to be equal to the
    number of training examples in the original training dataset, which usually provides
    a good bias-variance tradeoff. For the number of features, *d*, at each split,
    we want to choose a value that is smaller than the total number of features in
    the training dataset. A reasonable default that is used in scikit-learn and other
    implementations is ![](img/B17582_03_076.png), where *m* is the number of features
    in the training dataset.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实现中，包括scikit-learn中的`RandomForestClassifier`实现，引导样本的大小通常选择与原始训练数据集中的训练示例数量相等，这通常可以提供良好的偏差-方差权衡。对于每次分裂时特征数*d*，我们希望选择一个比训练数据集中总特征数更小的值。scikit-learn和其他实现中使用的合理默认值为
    ![](img/B17582_03_076.png)，其中*m*是训练数据集中的特征数。
- en: 'Conveniently, we don’t have to construct the random forest classifier from
    individual decision trees by ourselves because there is already an implementation
    in scikit-learn that we can use:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不必自己构建随机森林分类器，因为在scikit-learn中已经有了一个可以使用的实现：
- en: '[PRE33]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After executing the preceding code, we should see the decision regions formed
    by the ensemble of trees in the random forest, as shown in *Figure 3.24*:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码后，我们应该能够看到由随机森林中树的集合形成的决策区域，如*图 3.24*所示：
- en: '![Scatter chart  Description automatically generated](img/B17582_03_24.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![散点图 描述自动生成](img/B17582_03_24.png)'
- en: 'Figure 3.24: Decision boundaries on the Iris dataset using a random forest'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.24: 使用随机森林在鸢尾花数据集上的决策边界'
- en: Using the preceding code, we trained a random forest from 25 decision trees
    via the `n_estimators` parameter. By default, it uses the Gini impurity measure
    as a criterion to split the nodes. Although we are growing a very small random
    forest from a very small training dataset, we used the `n_jobs` parameter for
    demonstration purposes, which allows us to parallelize the model training using
    multiple cores of our computer (here, two cores). If you encounter errors with
    this code, your computer may not support multiprocessing. You can omit the `n_jobs`
    parameter or set it to `n_jobs=None`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们通过`n_estimators`参数训练了一个包含25棵决策树的随机森林。默认情况下，它使用基尼不纯度作为节点分裂的标准。尽管我们从一个非常小的训练数据集中生成了一个非常小的随机森林，但出于演示目的，我们使用了`n_jobs`参数，它允许我们在计算机的多个核心（这里是两个核心）上并行化模型训练。如果您在此代码中遇到错误，则可能是您的计算机不支持多进程。您可以省略`n_jobs`参数或将其设置为`n_jobs=None`。
- en: K-nearest neighbors – a lazy learning algorithm
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K最近邻算法 — 一种惰性学习算法
- en: The last supervised learning algorithm that we want to discuss in this chapter
    is the **k-nearest neighbor** (**KNN**) classifier, which is particularly interesting
    because it is fundamentally different from the learning algorithms that we have
    discussed so far.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在本章中讨论的最后一个监督学习算法是**k最近邻**（**KNN**）分类器，这个算法特别有趣，因为它与我们迄今为止讨论过的学习算法在根本上是不同的。
- en: KNN is a typical example of a **lazy learner**. It is called “lazy” not because
    of its apparent simplicity, but because it doesn’t learn a discriminative function
    from the training data but memorizes the training dataset instead.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是“懒惰学习器”的典型例子。它之所以被称为“懒惰”，不是因为它的表面简单性，而是因为它不从训练数据中学习判别函数，而是记忆训练数据集。
- en: '**Parametric versus non-parametric models**'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数化与非参数化模型**'
- en: Machine learning algorithms can be grouped into parametric and non-parametric
    models. Using parametric models, we estimate parameters from the training dataset
    to learn a function that can classify new data points without requiring the original
    training dataset anymore. Typical examples of parametric models are the perceptron,
    logistic regression, and the linear SVM. In contrast, non-parametric models can’t
    be characterized by a fixed set of parameters, and the number of parameters changes
    with the amount of training data. Two examples of non-parametric models that we
    have seen so far are the decision tree classifier/random forest and the kernel
    (but not linear) SVM.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以分为参数化和非参数化模型。使用参数化模型，我们从训练数据集中估计参数，以学习可以分类新数据点的函数，而无需再需要原始训练数据集。典型的参数化模型包括感知器、逻辑回归和线性支持向量机。相比之下，非参数化模型无法用固定的一组参数来描述，并且参数数量随着训练数据量的增加而变化。到目前为止，我们看到的两个非参数化模型的例子是决策树分类器/随机森林和核（非线性）支持向量机。
- en: KNN belongs to a subcategory of non-parametric models described as instance-based
    learning. Models based on instance-based learning are characterized by memorizing
    the training dataset, and lazy learning is a special case of instance-based learning
    that is associated with no (zero) cost during the learning process.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: KNN属于描述为基于实例学习的非参数模型的子类。基于实例学习的模型以记忆训练数据集为特征，惰性学习是与学习过程中没有（零）成本相关的基于实例学习的特殊情况。
- en: 'The KNN algorithm itself is fairly straightforward and can be summarized by
    the following steps:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法本身非常简单，可以通过以下步骤进行总结：
- en: Choose the number of *k* and a distance metric
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*的数量和距离度量
- en: Find the *k*-nearest neighbors of the data record that we want to classify
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出我们要分类的数据记录的*k*个最近邻居
- en: Assign the class label by majority vote
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票分配类标签
- en: '*Figure 3.25* illustrates how a new data point (**?**) is assigned the triangle
    class label based on majority voting among its five nearest neighbors:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.25*说明了如何根据其五个最近邻中的多数投票将新数据点（**?**）分配到三角形类标签：'
- en: '![](img/B17582_03_25.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_25.png)'
- en: 'Figure 3.25: How k-nearest neighbors works'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25：k近邻如何工作
- en: Based on the chosen distance metric, the KNN algorithm finds the *k* examples
    in the training dataset that are closest (most similar) to the point that we want
    to classify. The class label of the data point is then determined by a majority
    vote among its *k* nearest neighbors.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 根据选择的距离度量，KNN算法找出训练数据集中与我们要分类的点最接近（最相似）的*k*个示例。然后，数据点的类标签由其*k*个最近邻之间的多数投票确定。
- en: '**Advantages and disadvantages of memory-based approaches**'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于内存的方法的优缺点**'
- en: The main advantage of such a memory-based approach is that the classifier immediately
    adapts as we collect new training data. However, the downside is that the computational
    complexity for classifying new examples grows linearly with the number of examples
    in the training dataset in the worst-case scenario—unless the dataset has very
    few dimensions (features) and the algorithm has been implemented using efficient
    data structures for querying the training data more effectively. Such data structures
    include k-d tree ([https://en.wikipedia.org/wiki/K-d_tree](https://en.wikipedia.org/wiki/K-d_tree))
    and ball tree ([https://en.wikipedia.org/wiki/Ball_tree](https://en.wikipedia.org/wiki/Ball_tree)),
    which are both supported in scikit-learn. Furthermore, next to computational costs
    for querying data, large datasets can also be problematic in terms of limited
    storage capacities.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于内存的方法的主要优势在于，随着我们收集新的训练数据，分类器立即适应。然而，缺点是在最坏情况下，对新示例进行分类的计算复杂度随着训练数据集中示例数量的线性增长而增加，除非数据集的维度（特征）非常少，并且算法使用了有效的数据结构来更有效地查询训练数据。这些数据结构包括k-d树（[https://en.wikipedia.org/wiki/K-d_tree](https://en.wikipedia.org/wiki/K-d_tree)）和球树（[https://en.wikipedia.org/wiki/Ball_tree](https://en.wikipedia.org/wiki/Ball_tree)），这两者都受到scikit-learn支持。此外，除了查询数据的计算成本之外，大型数据集在存储能力有限的情况下也可能存在问题。
- en: However, in many cases when we are working with relatively small to medium-sized
    datasets, memory-based methods can provide good predictive and computational performance
    and are thus a good choice for approaching many real-world problems. Recent examples
    of using nearest neighbor methods include predicting properties of pharmaceutical
    drug targets (*Machine Learning to Identify Flexibility Signatures of Class A
    GPCR Inhibition*, Biomolecules, 2020, Joe Bemister-Buffington, Alex J. Wolf, Sebastian
    Raschka, and Leslie A. Kuhn, [https://www.mdpi.com/2218-273X/10/3/454](https://www.mdpi.com/2218-273X/10/3/454))
    and state-of-the-art language models (*Efficient Nearest Neighbor Language Models*,
    2021, Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick, [https://arxiv.org/abs/2109.04212](https://arxiv.org/abs/2109.04212)).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，当我们处理相对较小到中等大小的数据集时，基于内存的方法可以提供良好的预测和计算性能，因此是处理许多现实世界问题的良好选择。最近使用最近邻方法的示例包括预测制药药物靶标属性（*Machine
    Learning to Identify Flexibility Signatures of Class A GPCR Inhibition*，Biomolecules，2020，Joe
    Bemister-Buffington，Alex J. Wolf，Sebastian Raschka和Leslie A. Kuhn，[https://www.mdpi.com/2218-273X/10/3/454](https://www.mdpi.com/2218-273X/10/3/454)）和最先进的语言模型（*Efficient
    Nearest Neighbor Language Models*，2021，Junxian He，Graham Neubig和Taylor Berg-Kirkpatrick，[https://arxiv.org/abs/2109.04212](https://arxiv.org/abs/2109.04212)）。
- en: 'By executing the following code, we will now implement a KNN model in scikit-learn
    using a Euclidean distance metric:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下代码，我们将使用scikit-learn中的欧氏距离度量实现一个KNN模型：
- en: '[PRE34]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'By specifying five neighbors in the KNN model for this dataset, we obtain a
    relatively smooth decision boundary, as shown in *Figure 3.26*:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在此数据集的KNN模型中指定五个邻居，我们得到一个相对平滑的决策边界，如*图3.26*所示：
- en: '![A picture containing map  Description automatically generated](img/B17582_03_26.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![包含地图描述的图片 自动产生](img/B17582_03_26.png)'
- en: 'Figure 3.26: k-nearest neighbors’ decision boundaries on the Iris dataset'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26：鸢尾花数据集上的k近邻决策边界
- en: '**Resolving ties**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决平局**'
- en: In the case of a tie, the scikit-learn implementation of the KNN algorithm will
    prefer the neighbors with a closer distance to the data record to be classified.
    If the neighbors have similar distances, the algorithm will choose the class label
    that comes first in the training dataset.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在平局的情况下，scikit-learn实现的KNN算法会优先选择与要分类的数据记录距离较近的邻居。如果邻居的距离相似，则算法会选择在训练数据集中出现较早的类标签。
- en: 'The *right* choice of *k* is crucial to finding a good balance between overfitting
    and underfitting. We also have to make sure that we choose a distance metric that
    is appropriate for the features in the dataset. Often, a simple Euclidean distance
    measure is used for real-value examples, for example, the flowers in our Iris
    dataset, which have features measured in centimeters. However, if we are using
    a Euclidean distance measure, it is also important to standardize the data so
    that each feature contributes equally to the distance. The `minkowski` distance
    that we used in the previous code is just a generalization of the Euclidean and
    Manhattan distance, which can be written as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '*正确*选择*k*值对于找到过拟合和欠拟合之间的良好平衡至关重要。我们还必须确保选择一个适合数据集特征的距离度量。通常，对于实值示例，如我们的鸢尾花数据集中的花，简单的欧氏距离度量常被使用，其特征以厘米为单位测量。然而，如果我们使用欧氏距离度量，同样重要的是标准化数据，以确保每个特征对距离的贡献相等。我们在之前代码中使用的`minkowski`距离只是欧氏距离和曼哈顿距离的一般化，可以写成如下形式：'
- en: '![](img/B17582_03_077.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_03_077.png)'
- en: It becomes the Euclidean distance if we set the parameter `p=2` or the Manhattan
    distance at `p=1`. Many other distance metrics are available in scikit-learn and
    can be provided to the `metric` parameter. They are listed at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置参数`p=2`，则变为欧氏距离，或在`p=1`时变为曼哈顿距离。scikit-learn中提供了许多其他距离度量，并可提供给`metric`参数。它们列在[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html)上。
- en: Lastly, it is important to mention that KNN is very susceptible to overfitting
    due to the **curse of dimensionality**. The curse of dimensionality describes
    the phenomenon where the feature space becomes increasingly sparse for an increasing
    number of dimensions of a fixed-size training dataset. We can think of even the
    closest neighbors as being too far away in a high-dimensional space to give a
    good estimate.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要提到的是KNN非常容易因为**维度灾难**而过拟合。维度灾难描述了在固定大小的训练数据集的维度增加时，特征空间变得越来越稀疏的现象。即使是最近的邻居在高维空间中也可能相距甚远，导致估计不准确。
- en: We discussed the concept of regularization in the section about logistic regression
    as one way to avoid overfitting. However, in models where regularization is not
    applicable, such as decision trees and KNN, we can use feature selection and dimensionality
    reduction techniques to help us to avoid the curse of dimensionality. This will
    be discussed in more detail in the next two chapters.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在逻辑回归章节讨论了正则化的概念，作为避免过拟合的一种方法。然而，在无法应用正则化的模型中，例如决策树和KNN，我们可以使用特征选择和降维技术来帮助我们避免维度灾难。这将在接下来的两章中详细讨论。
- en: '**Alternative machine learning implementations with GPU support**'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**带有GPU支持的替代机器学习实现**'
- en: 'When working with large datasets, running k-nearest neighbors or fitting random
    forests with many estimators can require substantial computing resources and processing
    time. If you have a computer equipped with an NVIDIA GPU that is compatible with
    recent versions of NVIDIA’s CUDA library, we recommend considering the RAPIDS
    ecosystem ([https://docs.rapids.ai/api](https://docs.rapids.ai/api)). For instance,
    RAPIDS’ cuML ([https://docs.rapids.ai/api/cuml/stable/](https://docs.rapids.ai/api/cuml/stable/))
    library implements many of scikit-learn’s machine learning algorithms with GPU
    support to accelerate the processing speeds. You can find an introduction to cuML
    at [https://docs.rapids.ai/api/cuml/stable/estimator_intro.html](https://docs.rapids.ai/api/cuml/stable/estimator_intro.html).
    If you are interested in learning more about the RAPIDS ecosystem, please also
    see the freely accessible journal article that we wrote in collaboration with
    the RAPIDS team: *Machine Learning in Python: Main Developments and Technology
    Trends in Data Science, Machine Learning, and Artificial Intelligence* ([https://www.mdpi.com/2078-2489/11/4/193](https://www.mdpi.com/2078-2489/11/4/193)).'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据集时，运行k最近邻算法或拟合具有许多估计器的随机森林可能需要大量的计算资源和处理时间。如果您的计算机配备了与最新版本的NVIDIA CUDA库兼容的NVIDIA
    GPU，我们建议考虑使用RAPIDS生态系统（[https://docs.rapids.ai/api](https://docs.rapids.ai/api)）。例如，RAPIDS的cuML（[https://docs.rapids.ai/api/cuml/stable/](https://docs.rapids.ai/api/cuml/stable/)）库实现了许多带有GPU支持的scikit-learn机器学习算法，以加速处理速度。您可以在[https://docs.rapids.ai/api/cuml/stable/estimator_intro.html](https://docs.rapids.ai/api/cuml/stable/estimator_intro.html)找到cuML的介绍。如果您有兴趣了解更多关于RAPIDS生态系统的内容，请参阅我们与RAPIDS团队合作撰写的免费获取的期刊文章：《*Python中的机器学习：数据科学、机器学习和人工智能的主要发展和技术趋势*》（[https://www.mdpi.com/2078-2489/11/4/193](https://www.mdpi.com/2078-2489/11/4/193)）。
- en: Summary
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about many different machine learning algorithms
    that are used to tackle linear and nonlinear problems. You have seen that decision
    trees are particularly attractive if we care about interpretability. Logistic
    regression is not only a useful model for online learning via SGD, but also allows
    us to predict the probability of a particular event.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了许多不同的机器学习算法，用于解决线性和非线性问题。您已经看到，如果我们关心解释性，决策树尤其具有吸引力。逻辑回归不仅是通过SGD进行在线学习的有用模型，还允许我们预测特定事件的概率。
- en: Although SVMs are powerful linear models that can be extended to nonlinear problems
    via the kernel trick, they have many parameters that have to be tuned in order
    to make good predictions. In contrast, ensemble methods, such as random forests,
    don’t require much parameter tuning and don’t overfit as easily as decision trees,
    which makes them attractive models for many practical problem domains. The KNN
    classifier offers an alternative approach to classification via lazy learning
    that allows us to make predictions without any model training, but with a more
    computationally expensive prediction step.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然支持向量机（SVM）是强大的线性模型，可以通过核技巧扩展到非线性问题，但它们有许多需要调整的参数才能做出良好的预测。相比之下，集成方法如随机森林不需要太多参数调整，并且不像决策树那样容易过拟合，这使它们成为许多实际问题领域的理想模型。K最近邻分类器通过惰性学习提供了一种替代分类的方法，允许我们在没有模型训练的情况下进行预测，但预测步骤更加消耗计算资源。
- en: However, even more important than the choice of an appropriate learning algorithm
    is the available data in our training dataset. No algorithm will be able to make
    good predictions without informative and discriminatory features.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，比选择合适的学习算法更重要的是我们训练数据集中的可用数据。没有任何算法能够在没有信息丰富和有歧视性的特征的情况下做出良好的预测。
- en: In the next chapter, we will discuss important topics regarding the preprocessing
    of data, feature selection, and dimensionality reduction, which means that we
    will need to build powerful machine learning models. Later, in *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will see how
    we can evaluate and compare the performance of our models and learn useful tricks
    to fine-tune the different algorithms.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论有关数据预处理、特征选择和降维的重要主题，这意味着我们需要构建强大的机器学习模型。随后，在*第6章*，*学习模型评估和超参数调优的最佳实践*中，我们将看到如何评估和比较模型的性能，并学习优化不同算法的有用技巧。
- en: Join our book’s Discord space
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作区，与作者进行每月的*问我任何*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
