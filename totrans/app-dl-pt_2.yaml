- en: '*Chaper 2*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Blocks of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the advantages and disadvantages of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguish between different components in the anatomy of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize the most popular neural network architectures and understand what
    they are mainly used for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use techniques to prepare data to be fed into a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a regression problem using a simple architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the performance of a model by addressing high bias or high variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll look at the basic building blocks of neural networks.
    We'll explore the different architectures to solve a wide variety of tasks. Finally,
    we'll learn how to build a neural network using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although neural network theory was developed several decades ago, and the concept
    evolved from the notion of the perceptron, different architectures have been created
    to solve different data problems in recent times. This is mainly due to the different
    data formats that can be found in real-life data problems, such as text, audio,
    and images. The purpose of this chapter is to introduce the topic of neural networks
    and their main advantages and disadvantages in order to better understand when
    and how to use them. Then, the chapter will move on to explain the building blocks
    of the most popular neural network architectures: **artificial neural networks**
    (**ANNs**), **convolutional neural networks** (**CNNs**), and **recurrent neural
    networks** (**RNNs**).'
  prefs: []
  type: TYPE_NORMAL
- en: Following this, the process of building an effective model will be explained
    by solving a real-life regression problem. This includes the preparation of the
    data to be fed to the neural network (also known as data preprocessing), the definition
    of the neural network architecture to be used, and finally, the evaluation of
    the performance of the model, with the objective of determining how it can be
    improved to achieve an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned process will be done using one of the neural network architectures
    learned in the previous chapter, taking into consideration that the solution for
    each data problem should be carried out using the architecture that performs best
    for the data type in question. The other architectures will be used in subsequent
    chapters to solve more complicated data problems that involve using images and
    sequences of text as input data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, the GitHub repository containing all the code used in this chapter
    can be found at the following link:[https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch
    )
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Developed several decades ago, neural networks need to learn from training
    data, rather than being programmed to solve a particular task following a set
    of rules. The learning process can follow one of the following methodologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: This is the simplest form of learning as it consists
    of a labeled dataset, where the neural network needs to find patterns that explain
    the relationship between the features and the target. The iterations during the
    learning process aim to minimize the difference between the predicted value and
    the ground truth. One example of this would be classifying a plant based on the
    attributes of its leaves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: In contrast with the preceding methodology, unsupervised
    learning consists of training a model with unlabeled data (meaning that there
    is no target value). The purpose of this is to arrive at a better understanding
    of the input data, where, generally, networks take input data, encode it, and
    then reconstruct the content from the encoded version, ideally keeping the relevant
    information. For instance, given a paragraph, the neural network can map the words
    in order to output those words that are actually key, which can be used as tags
    to describe the paragraph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: This methodology consists of learning from the
    data at hand, with the main objective of maximizing a reward function in the long
    run. Hence, decisions are not made based on the immediate reward, but on the accumulation
    of it in the entire learning process, such as allocating resources to different
    tasks, with the objective of minimizing bottlenecks that would slow down general
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: From the learning methodologies mentioned, the most commonly used is supervised
    learning, which is the one that will be mainly used in subsequent sections. This
    means that most exercises, activities, and examples will use a labeled dataset
    as input data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What Are Neural Networks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Put simply, neural networks are a type of machine learning algorithm modeled
    on the anatomy of the human brain, which use mathematical equations to learn a
    pattern from the observation of training data.
  prefs: []
  type: TYPE_NORMAL
- en: However, to actually understand the logic behind the training process that neural
    networks typically follow, it is important to first understand the concept of
    perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: Developed during the 1950s by Frank Rosenblatt, a perceptron is an artificial
    neuron that, similar to neurons in the human brain, takes several inputs and produces
    a binary output, which becomes the input of a subsequent neuron. They are the
    essential building blocks of a neural network (just like neurons are the building
    blocks of the human brain).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Diagram of a perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Diagram of a perceptron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, X1, X2, X3, and X4 represent the different inputs that the perceptron
    receives, and there could be any number of these. The gray circle is the perceptron,
    where the processing of the inputs occurs to arrive at an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rosenblatt also introduced the concept of weights (w1, w2, â€¦, wn), which are
    numbers that express the importance of each input. The output can be either 0
    or 1, and it depends on whether the weighted sum of the inputs is above or below
    a given threshold that can be set as a parameter of the perceptron, as can be
    seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Equation of output for perceptrons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Equation of output for perceptrons'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 2: Performing the Calculations of a Perceptron'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following exercise does not require programming of any kind; instead, it
    consists of simple calculations to help you understand the notion of the perceptron.
    To perform the calculations, consider the following scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a music festival in your town next Friday, but you are ill and trying
    to decide whether to go (where 0 means you are going and 1 means your aren''t
    going). To make the decision, you decide to consider three factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Will there be good weather? (X1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have someone to go with? (X2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the music to your liking? (X3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the preceding factors, we will use 1 if the answer to the question is yes,
    and 0 if the answer is no. Additionally, as you are very sick, the factor related
    to the weather is highly relevant, and you decide to give this factor a weight
    twice as big as the other 2 factors. Hence, the weights for the factors are 4
    (w1), 2 (w2), and 2 (w3). Now, consider a threshold of 5:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the information given, calculate the output of the perceptron, considering
    that the weather is not good next Friday, but you both have someone to go with
    and like the music at the festival:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.3: Output of the perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Output of the perceptron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Considering that the output is less than the threshold, the final result will
    be equal to 1, meaning that you should ynot go to the festival to avoid the risk
    of getting even more sicker.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully performed the calculations of a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Layer Perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering the aforementioned, the notion of a multi-layered network consists
    of a network of multiple perceptrons stacked together (also known as nodes or
    neurons), such as the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Diagram of a multi-layer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Diagram of a multi-layer perceptron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The notation to refer to the layers in a neural network is as follows:The first
    layer is also known as the input layer, the last layer is also known as the output
    layer, and all layers in between are known as hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Here, again, a set of inputs is used to train the model, but instead of feeding
    a single perceptron, they are fed to all perceptrons (neurons) in the first layer.
    Next, the outputs obtained from this layer are used as inputs for the perceptrons
    in the subsequent layer, and so on until a final layer is reached, which is in
    charge of outputting a result.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to mention that the first layer of a perceptron handles a simple
    decision process by weighting the inputs, while the subsequent layer can handle
    more complex and abstract decisions based on the output of the previous layer,
    hence the state-of-the-art performance of deep neural networks (networks that
    use many layers) for complex data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Different to traditional perceptrons, neural networks have evolved to be able
    to have one or multiple nodes in the output layer, in order to be able to present
    the result either as binary or multiclass.
  prefs: []
  type: TYPE_NORMAL
- en: The Learning Process of a Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general terms, neural networks are a connection of multiple neurons, where
    each neuron computes a linear function along with an activation function to arrive
    at an output based on some inputs. This output is tied to a weight, which represents
    its level of importance, to be used for calculations in the following layer.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, these calculations are carried out throughout the entire architecture
    of the network, where a final output is reached. This output is used to determine
    the performance of the network in comparison to the ground truth, which is then
    used to adjust the different parameters of the network to start the calculation
    process over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this, the training process of a neural network can be seen as an
    iterative process that goes forward and backward through the layers of the network
    to arrive at an optimal result, which can be seen in the following figure and
    will be explained in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Diagram of the learning process of a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Diagram of the learning process of a neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Forward Propagation**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the process of going from left to right through the architecture of
    the network, while performing calculations using the input data to arrive at a
    prediction that can be compared to the ground truth. This means that every neuron
    in the network will transform the input data (the initial data or data received
    from the previous layer) according to the weights and biases that it has associated
    to it and send the output to the subsequent layer, until a final layer is reached
    and a prediction is made.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculations performed in each neuron include a linear function that multiplies
    the input data to some weights plus a bias, which is then passed through an activation
    function. The main purpose of the activation function is to break the linearity
    of the model, which is crucial considering that most real-life data problems solved
    using neural networks are not defined by a line, but rather by a complex function.
    The formulas can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Calculations performed by each neuron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Calculations performed by each neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, as mentioned before, X refers to the input data, W is the weights that
    determine the level of importance of the input data, b is the bias value, and
    sigma (![](img/02_32.png)) represents the activation function applied over the
    linear function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation serves the purpose of introducing non-linearity to the model.
    There are different activation functions to choose from, and a list of the most
    commonly used nowadays is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid**: This is S-shaped, and it basically converts values into simple
    probabilities between 0 and 1, where most of the outputs obtained by the sigmoid
    function will be close to the extremes of 0 and 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.7: Sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.7: Sigmoid activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.8: Graphical representation of the sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Graphical representation of the sigmoid activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Softmax**: Similar to the sigmoid function, it calculates the probability
    distribution of an event over n events, meaning that its output is not binary.
    In simple terms, this function calculates the probability of the output being
    one of the target classes in comparison to the other classes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.9: Softmax activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: Softmax activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Considering that its output is a probability, this activation function is often
    found in the output layer of classification networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tanh**: This function represents the relationship between the hyperbolic
    sine and the hyperbolic cosine, and the result is between -1 and 1\. The main
    advantage of this activation function is that negative values can be dealt with
    more easily:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.10: Tanh activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: Tanh activation function.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.11: Graphical representation of the tanh activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11: Graphical representation of the tanh activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Rectified Linear Function (ReLU)**: This basically activates a node given
    that the output of the linear function is above 0, otherwise its output will be
    0\. If the output of the linear function is above 0, the result from this activation
    function will be the raw number it received as input:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.12: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: ReLU activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Conventionally, this activation function is used for all hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: Graphical representation of the ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: Graphical representation of the ReLU activation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**The Calculation of Loss Functions**'
  prefs: []
  type: TYPE_NORMAL
- en: Once the forward propagation is complete, the next step in the training process
    is to calculate a loss function to estimate the error of the model by comparing
    how good or bad the prediction is in relation to the ground truth value. Considering
    this, the ideal value to be reached is 0, which would mean that there is no divergence
    between the two values.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the goal in each iteration of the training process would be
    to minimize the loss function by changing the parameters (weights and biases)
    used to perform the calculations during the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, there are multiple loss functions to choose from. However, the most
    commonly used loss functions for regression and classification tasks are explained
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error (MSE)**: Widely used to measure the performance of regression
    models, the MSE function calculates the sum of the distance between the ground
    truth and the prediction values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.14: Mean squared error loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.14: Mean squared error loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, *n* refers to the number of samples, ![](img/02_30.png) is the ground
    truth values, and ![](img/02_31.png) is the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross entropy/multi-class cross entropy**: This function is conventionally
    used for binary or multi-class classification models. It measures the divergence
    between two probability distributions; a large loss function will represent a
    large divergence. Hence, the objective here is to minimize the loss function as
    well:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.15: Cross entropy loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.15: Cross entropy loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, n refers to the number of samples. ![](img/02_301.png) and ![](img/02_311.png)
    are the ground truth and the predicted value, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backward Propagation (Backpropagation)**'
  prefs: []
  type: TYPE_NORMAL
- en: The final step in the training process consists of going from right to left
    in the architecture of the network to calculate the partial derivatives of the
    loss function in respect to the weights and biases in each layer, in order to
    update these parameters (weights and biases) so that, in the next iteration step,
    the loss function is lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the final objective of the optimization algorithm is to find the
    global minima where the loss function has reached the least possible value, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, a local minima refers to the smallest value within a section
    of the function domain. On the other hand, a global minima refers to the smallest
    value of the entire domain of the function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Loss function optimization through the iteration steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Two-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: Loss function optimization through the iteration steps. Two-dimensional
    space.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the dot furthest to the left (A) is the initial value of the loss function,
    before any optimization. The dot furthest to the right (B), at the bottom of the
    curve, is the loss function after several iteration steps, where its value has
    been minimized. The process of going from one dot to another is called **step**.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to mention that the loss function is not always as
    smooth as the preceding one, which can introduce the risk of reaching a local
    minima during the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: This process is also called optimization, and there are different algorithms
    that vary in methodology to achieve the same objective. The most commonly used
    optimization algorithm will be explained next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Descent**'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is the optimization algorithm that's most widely used among
    data scientists, and it is the basis of many other optimization algorithms. After
    the gradients for each neuron are calculated, the weights and biases are updated
    in the opposite direction of the gradient, which should be multiplied by a learning
    rate (used to control the size of the steps taken in each optimization), as seen
    in the following equation.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate is crucial during the training process as it prevents the
    updates of the weights and biases from over/undershooting, which may prevent the
    model from reaching convergence or delay the training process, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization of weights and biases in the gradient descent algorithm is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17: Optimization of parameters in the gradient descent algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.17: Optimization of parameters in the gradient descent algorithm'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, Î± refers to the learning rate and dw/db represent the gradients of the
    weights or biases in a given neuron. The product of the two values is subtracted
    from the original value of the weight or bias in order to penalize the higher
    values, which are contributing to computing a large loss function.
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to the gradient descent algorithm is called Stochastic gradient
    descent, and it basically follows the same process, with the distinction that
    it takes the input data in random batches instead of in one chunk, which improves
    the training times while reaching outstanding performance. Moreover, this approach
    allows the use of larger datasets, because by using small batches of the dataset
    as inputs we are no longer limited by computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and Disadvantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following is an explanation of the advantages and disadvantages of neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks have become increasingly popular in the last few years for
    four main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: Neural networks are widely known for their ability to capitalize
    on large amounts of data, and thanks to the advances in hardware and software,
    the recollection and storage of massive databases is now possible. This has allowed
    neural networks to show their real potential as more data is fed into them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex data problems**: As has been explained before, neural networks are
    excellent for solving complex data problems that cannot be tackled by other machine
    learning algorithms. This is mainly due to their capability to process large datasets
    and uncover complex patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational power**: Advances in technology have also increased the computational
    power available these days, which is crucial for training neural network models
    that use millions of pieces of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Academic research**: Thanks to the preceding three points, a proliferation
    of academic research on the topic is available on the internet, which not only
    facilitates the immersion of new research each day, but also helps to keep the
    algorithms and hardware/software requirements up to date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages**'
  prefs: []
  type: TYPE_NORMAL
- en: Just because there are a lot of advantages to using a neural network, it does
    not mean that every data problem should be solved this way. This is a mistake
    that is commonly made. There is no one algorithm that will perform well for all
    data problems, and the selection of the algorithm should depend on the resources
    available, as well as the data problem.
  prefs: []
  type: TYPE_NORMAL
- en: And, although neural networks are thought to outperform almost any machine learning
    algorithm, it is crucial to consider their disadvantages as well, to weigh up
    what matters most for the data problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Black box**: This is one of the most commonly-known disadvantages of neural
    networks. It basically means that how and why a neural network reached a certain
    output is unknown. For instance, when a neural network wrongly predicts a cat
    picture as a dog, it is not possible to know what the cause of the error was.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data requirements**: The vast amounts of data that they require to achieve
    optimal results can be equally an advantage and a disadvantage. Neural networks
    require more data than traditional machine learning algorithms, which can be the
    main reason to choose between them and other algorithms for some data problems.
    This becomes a greater issue when the task at hand is supervised, which means
    that the data needs to be labeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training times**: Tied to the preceding disadvantage, the need for vast amounts
    of data also makes the training process last longer than traditional machine learning
    algorithms, which in some cases is not an option. Training times can be reduced
    through the use of GPUs, which speed up computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computationally expensive**: Again, the training process of neural networks
    is computationally expensive. While one neural network could take weeks to converge,
    other machine learning algorithms can take hours or minutes to be trained. The
    amount of computational resources needed depends on the quantity of data at hand,
    as well as the complexity of the network; deeper neural networks take a longer
    time to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: There are a wide variety of neural network architectures. Three of the most
    commonly used ones will be explained in this chapter, along with their practical
    implementation in subsequent chapters. However, if you wish to learn about other
    architectures, visit [http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Introduction to Artificial Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**), also known as multi-layer perceptrons,
    are a collection of multiple perceptrons, as explained before. Here, it is important
    to mention that the connection between perceptrons occurs through layers, where
    one layer can have as many perceptrons as desired, and they are all connected
    to all the other perceptrons in the preceding and subsequent layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Networks can have one or more layers. Networks with over four layers are considered
    to be deep neural networks and are commonly used to solve complex and abstract
    data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'ANNs are typically composed of three main elements, which were explained in
    detail earlier, and can also be seen in *Figure 2.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This is the first layer of the network, conventionally located
    furthest left in the graphical representation of a network. It receives the input
    data before any calculation is performed, and completes the first set of calculations,
    where the most generic patterns are uncovered.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For supervised learning problems, the input data consists of a pair of features
    and targets. The job of the network is to uncover the correlation or dependency
    between the input and output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Hidden layers**: Next, the hidden layers can be found. A neural network can
    have as many hidden layers as possible. The more layers it has, the more complex
    data problems it can tackle, but it will also take longer to train. There are
    also neural network architectures that do not contain hidden layers at all, which
    is the case with single-layer networks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each layer, a computation is performed based on the information received
    as input from the previous layer, to output a prediction that will become the
    input of the subsequent layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Output layer**: This is the last layer of the network, located at the far
    right of the graphical representation of the network. It receives data after being
    processed by all the neurons in the network to make and display a final prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layer can have one or more neurons. The former refers to models where
    the solution is binary, in the form of 0s or 1s. On the other hand, the latter
    case consists of models that output the probability of an instance to belong to
    each of the possible class labels (targets), meaning that the layer will have
    as many neurons as there are class labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Architecture of a neural network with two hidden layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.18: Architecture of a neural network with two hidden layers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Introduction to Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNNs) are mostly used in the field of computer
    vision, where, in recent decades, machines have achieved levels of accuracy that
    surpass human ability, which has made them increasingly popular.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by human brains, CNNs search to create models that use different groups
    of neurons to recognize different aspects of an image. These groups should be
    able to communicate with each other so that, together, they can form the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, layers in the architecture of a CNN divide their recognition
    tasks. The first layers focus on trivial patterns, and the layers at the end of
    the network use that information to uncover more complex patterns.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when recognizing human faces in pictures, the first couple of
    layers focus on finding edges that separate one feature from another. Next, the
    subsequent layers emphasize certain features of the face, such as the nose. Finally,
    the last couple of layers use this information to put together the entire face
    of the person.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea of using a group of neurons to activate when certain features are
    encountered is achieved through the use of filters or kernels, which are one of
    the main building blocks of the architecture of convolutional neural networks.
    However, they are not the only element present in the architecture, which is why
    a brief explanation of all the components of CNNs will be provided:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The concepts of padding and stride, which you might have heard of when using
    CNNs, will be explained in subsequent sections of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layers**: In these layers, a convolutional computation occurs
    between an image (represented as a matrix of pixels) and a filter. This computation
    produces a feature map as an output that ultimately serves as input for the next
    layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The computation takes a subsection of the image matrix of the same shape of
    the filter and performs a multiplication of the values. Then, the sum of the product
    is set as the output for that section of the image, as shown in the following
    figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19: Convolution operation between image and filter'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/02_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: Convolution operation between image and filter'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, the matrix to the left is the input data, the matrix in the middle is
    the filter, and the matrix to the right is the output from the computation. The
    computation that occurred with the values highlighted by the red box can be seen
    here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.20: Convolution of the first section of the image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Convolution of the first section of the image'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This convolutional multiplication is done for all subsections of the image.
    Figure 2.21 shows another convolution step for the same example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.21: Further step in the convolution operation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: Further step in the convolution operation'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: An important notion of convolutional layers is that they are invariant in such
    a way that each filter will have a specific function, which does not vary during
    the training process. For instance, a filter in charge of detecting ears will
    only specialize in that function throughout the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moreover, a convolutional neural network will typically have several convolutional
    layers, considering that each of them will focus on identifying a particular feature
    of the image, depending on the filter used. Additionally, it is important to mention
    that, commonly, there is one pooling layer in between two convolutional layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Pooling layers**: Although convolutional layers are capable of extracting
    relevant features from images, their results can become enormous when analyzing
    complex geometrical shapes, which would make the training process impossible in
    terms of computational power. Hence the invention of pooling layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These layers not only accomplish the goal of reducing the output of the convolutional
    layers, but also achieve the removal of noise present in the features extracted,
    which ultimately helps the accuracy of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are two main types of pooling layers that can be applied, and the idea
    behind them is to detect the areas that express a stronger influence in the image
    so that the other areas can be overlooked:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Max pooling**: This operation consists of taking a subsection of the matrix
    of a given size and taking the maximum number in that subsection as the output
    of the max pooling operation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.22: Max pooling operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.22: Max pooling operation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, by using a 3x3 max pooling filter, the result on the
    right is achieved. Here, the yellow section (top-left corner) has a maximum number
    of 4, while the orange section (top-right corner) has a maximum number of 5.
  prefs: []
  type: TYPE_NORMAL
- en: '**Average pooling**: Similarly, the average pooling operation takes subsections
    of the matrix and takes the number that meets the rule as output, which in this
    case is the average of all the numbers in the subsection in question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.23: Average pooling operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.23: Average pooling operation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, using a 3x3 filter, we get that 8.6 is the average of all the numbers
    in the yellow section (top-left corner), while 9.6 is the average for the ones
    in the orange section (top-right corner).
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully connected layers**: Finally, and considering that the network would
    be of no use if it was only capable of detecting a set of features without having
    the capability of classifying them into a class label, fully connected layers
    are used at the end of CNNs to take the features detected by the previous layer
    (known as the features map) and output the probability of that group of features
    of belonging to a class label, which is used to make the final prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like artificial neural networks, fully connected layers use perceptrons to calculate
    an output based on a given input. Moreover, it is crucial to mention that convolutional
    neural networks typically have more than one fully connected layer at the end
    of the architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By combining all of these concepts, the conventional architecture of convolutional
    neural networks is obtained, where there can be as many layers of each type as
    desired and each convolutional layer can have as many filters as desired (each
    for a particular task), and the pooling layer should have the same number of filters,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24: Diagram of convolutional neural network architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.24: Diagram of convolutional neural network architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Introduction to Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main limitation of the aforementioned neural networks is that they learn
    only by considering the current event (the input that is being processed) without
    taking into account previous or following events, which is inconvenient considering
    that we humans do not think that way. For instance, when reading a book, you can
    understand each sentence better by considering the context from the previous paragraph
    or more.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, and taking into account that neural networks aim to optimize several
    processes traditionally done by humans, it is crucial to think of a network able
    to consider a sequence of inputs and outputs, hence the creation of recurrent
    neural networks (RNNs). They are a robust type of neural network that allow the
    solution of complex data problems through the use of an internal memory.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, these networks contain loops in them that allow for the information
    to remain in their memory for longer periods, even when a subsequent set of information
    is being processed. This means that a perceptron in an RNN not only passes over
    the output to the following perceptron, but it also passes a bit of information
    to itself, which can be useful for analyzing the next bit of information. This
    memory-keeping capability allows them to be very accurate in predicting what is
    coming next.
  prefs: []
  type: TYPE_NORMAL
- en: The learning process of a recurrent neural network, similar to other networks,
    tries to map the relationship between an input (x) and an output (y), with the
    difference being that these models also take into consideration the entire or
    partial history of previous inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs allow the processing of sequences of data in the form of a sequence of
    inputs, a sequence of outputs, or even both at the same time, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25: Sequence of data handled by RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.25: Sequence of data handled by RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, each box is a matrix and the arrows represent a function that occurs.
    The bottom boxes are the inputs, the top boxes are the outputs, and the middle
    boxes represent the state of the RNNs at that point, which holds the memory of
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'From left to right, the preceding diagrams are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: A typical model that does not require an RNN to be solved. It has a fixed input
    and a fixed output. This can refer to image classification, for instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This model takes in an input and yields a sequence of outputs. Take, for instance,
    a model that receives an image as input and the output should be an image caption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contrary to the above, this model takes a sequence of inputs and yields a single
    outcome. This type of architecture can be seen on sentiment analysis problems,
    where the input is the sentence to be analyzed and the output is the predicted
    sentiment behind the sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final two models take a sequence of inputs and return a sequence of outputs
    with the difference being that the first one first analyzes the entire set of
    inputs, to then generate the set of outputs. For instance, for language translations,
    where the entire sentence in one language needs to be understood before proceeding
    with the actual translation. On the other hand, the second many-to-many model
    analyzes the inputs and generates the outputs at the same time. For example, when
    each frame of a video is being labeled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in the development of any deep learning model, after gathering
    the data, of book, should be the preparation of the data. This is crucial to understand
    the data at hand, and hence, be able to outline the scope of the project correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Many data scientists fail to do so, which results in models that perform poorly,
    and even models that are useless as they do not answer the data problem to begin
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of preparing the data can be divided into three main tasks: 1)
    Understanding the data and dealing with any potential issues, 2) Rescaling the
    features to make sure no bias is introduced by mistake, and 3) Splitting the data
    to be able to measure performance accurately. All three tasks will be further
    explained in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the tasks explained previously are pretty much the same for applying
    any machine learning algorithm, considering that they refer to the techniques
    required to prepare data beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Messy Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This task mainly consists of performing **exploratory data analysis** (**EDA**)
    to understand the data available, as well as to detect potential issues that may
    affect the development of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The EDA process is useful as it helps the developer to uncover information
    crucial to the definition of the book of action. This information is explained
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantity of data**: This refers both to the number of instances and the number
    of features. The former is crucial for determining whether it is necessary or
    even possible to solve the data problem using a neural network, or even a deep
    neural network, considering that such models require vast amounts of data to achieve
    high levels of accuracy. The latter, on the other hand, is useful to determine
    whether it would be a good practice to develop some feature selection methodologies
    beforehand in order to reduce the number of features, to simplify the model, and
    eliminate any redundant information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The target feature**: For supervised models, data needs to be labeled. Considering
    this, it is highly important to select the target feature (the objective that
    we want to achieve by building the model) in order to assess whether the feature
    has many missing or outlier values. Additionally, this helps determine the objective
    of the development, which should be in line with the data available.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Noisy data/outliers**: Noisy data refers to values that are visibly incorrect,
    for instance, a person who is 200 years old. On the other hand, outliers refer
    to values that, although they may be correct, are very far from the mean, for
    instance, a 10-year-old college student.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is not an exact science to detecting outliers, but there are some methodologies
    that are commonly accepted. Assuming a normally distributed dataset, one of the
    most popular ones is determining as an outlier any value that is about 3-6 standard
    deviations away from the mean of all values, in both directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An equally valid approach to identifying outliers is to select those values
    at the 99th and 1st percentile.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is very important to handle such values when they represent over 5% of the
    data for a feature because failing to do so may introduce bias to the model. The
    way to handle these values, as with any other machine learning algorithm, is to
    either delete the outlier values or assign new values using mean or regression
    imputation techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Missing values**: Similar to the aforementioned, a dataset with many missing
    values can introduce bias to the model, considering that different models will
    make different assumptions about those values. Again, when missing values represent
    over 5% of the values of a feature, they should be handled by eliminating or replacing
    them, again using the mean or regression imputation techniques.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Qualitative features**: Finally, checking whether the dataset contains qualitative
    data is also a key step considering that removing or encoding data may result
    in more accurate models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, in many research developments, several algorithms are tested on
    the same data in order to determine which one performs better, and some of these
    algorithms do not tolerate the use of qualitative data, hence the importance of
    converting or encoding them to be able to feed all algorithms the same data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 3: Dealing with Messy Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the exercises in this chapter will be done using the `Appliances energy
    prediction Dataset`, from the UC Irvine Machine Learning Repository, which can
    be downloaded using the following URL, at the `Data Folder` hyperlink:[https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction
    )
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will use one of Python''s favorite packages to explore
    the data at hand and learn how to detect missing values, outliers, and qualitative
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the exercises and activities within this chapter, you will need to have
    Python 3.6, Jupyter, NumPy, and Pandas (at least version 0.21).
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter notebook to implement this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open your cmd or terminal, navigate to the desired path, and use the following
    command to open a Jupyter notebook: `jupyter notebook`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the pandas library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use pandas to read the CSV file containing the dataset previously downloaded
    from the UC Irvine Machine Learning Repository site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, drop the column named `date` as we do not want to consider it for the
    following exercises.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, print the head of the DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for categorical features in your dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting list is empty, which indicates that there are no categorical features
    to deal with.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use Python''s `isnull()` and `sum()` functions to find out whether there are
    any missing values in each column of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command counts the number of null values in each column. For the dataset
    in use, there should not be any missing values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use three standard deviations as the measure to detect outliers for all features
    in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting dictionary displays a list of all the features in the dataset,
    along with the percentage of outliers. From these results, it is possible to conclude
    that there is no need to deal with the outlier values, considering that they account
    for less than 5%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have successfully explored the dataset and dealt with potential
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Data Rescaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although data does not need to be rescaled to be fed to an algorithm for training,
    it is an important step to improve a model's accuracy. This is basically because
    having different scales for each feature may result in the model assuming a feature
    that is more important than others due to having higher numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for instance, two features: one measuring the number of kids a person
    has and another stating the age of the person. Even though the age feature may
    have higher numerical values, in a study for recommending schools, the number
    of kids feature may be more important.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, if all features are scaled equally, the model can actually
    give higher weights to those features that matter the most in respect to the target
    feature, and not the numerical values that they have. Moreover, it can also help
    accelerate the training process by removing the need for the model to learn from
    the invariance of the data.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main rescaling methodologies popular among data scientists, and
    although there is no rule for selecting one or the other, it is important to highlight
    that they are to be used individually (one or the other).
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief explanation of both of these methodologies can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization**: This consists of rescaling the values so that all values
    of all features are between zero and one, using the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.26: Data normalization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.26: Data normalization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Standardization**: In contrast, this rescaling methodology converts all values
    so that their mean is 0 and their standard deviation is equal to 1 using the following
    equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.27: Data standardization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.27: Data standardization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 4: Rescaling Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will rescale the data from the previous exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the same Jupyter notebook that you used in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Separate the features from the target. This is done to only rescale the features
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale the features data by using the normalization methodology. Display the
    head of the resulting DataFrame to verify the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully rescaled a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The purpose of splitting the dataset into three subsets is so that the model
    can be trained, fine-tuned, and measured appropriately, without the introduction
    of bias. Here is an explanation of each set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: As its name suggests, this set is fed to the neural network
    to be trained. For supervised learning, it consists of the features and the target
    values. This is typically the largest set out of the three, considering that neural
    networks require large amounts of data to be trained, as mentioned previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set (dev set)**: This set is used mainly to measure the performance
    of the model in order to make adjustments to the hyperparameters to improve performance.
    This fine-tuning process is done to arrive at the configuration of hyperparameters
    that achieve the best results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the model is not trained on this data, it indirectly has an effect
    on it, which is why the final measure of performance should not be done on it,
    as it may be a biased measure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Testing set**: This set does not have an effect on the model, which is why
    it is used to perform a final evaluation of the model on unseen data, which becomes
    a guideline of how well the model will perform on future datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no actual science on the perfect ratio for splitting data into the
    three sets mentioned, considering that every data problem is different, and developing
    deep learning solutions usually requires a trial-and-error methodology. Nevertheless,
    it is widely known that larger datasets (hundreds of thousands and millions of
    instances) should have a split ratio of 98%/1%/1% for each set, considering that
    it is crucial to use as much data as possible for the training set. For a smaller
    dataset, the conventional split ratio is 60%/20%/20%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5: Splitting a Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will split the dataset from the previous exercise into
    three subsets. For the purpose of learning, we will explore two different approaches.
    First, the dataset will be split using indexing. Next, scikit-learn''s `train_test_split()`
    function will be used for the same purpose, achieving the same result with both
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the same Jupyter notebook that you used in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Print the shape of the dataset in order to determine the split ration to be
    used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output from this operation should be `(19735, 28)`. This means that it is
    possible to use a split ratio of 60%/20%/20% for the training, validation, and
    testing sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the value to use as the bottom bound of the training and validation sets.
    This will be used to split the dataset using indexing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Shuffle the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use indexing to split the shuffled dataset into the three sets, for both the
    features and the target data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the shapes of all three sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result from the preceding operation should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `train_test_split()` function from scikit-learn''s `model_selection`
    module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the shuffled dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first line of code performs an initial split. The function takes as arguments
    both datasets to be split (X and Y), `test_size`, which is the percentage of instances
    to be contained in the test set, and `random_state` to ensure the reproducibility
    of the results. The result from this line of code is the division of each of the
    datasets (X and Y) into two subsets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to create an additional set (the validation set), we will perform a
    second split. The second line of the preceding code is in charge of determining
    the `test_size` to be used for the second split so that both the testing and validation
    sets have the same shape.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, the last line of code performs the second split, using the value calculated
    previously as the `test_size`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of all three sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result from the preceding operation should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As can be seen, the resulting sets from both approaches have the same shapes.
    Using one approach or the other is a matter of preference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have successfully split the dataset into three subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2: Performing Data Preparation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the following activity, we will prepare a dataset containing a list of songs,
    each with several attributes that help determine the year in which it was released.
    This data preparation step is crucial for subsequent activities within this chapter.
    Let's look at the following scenario.
  prefs: []
  type: TYPE_NORMAL
- en: You work at a music record company and they want to uncover the details that
    characterize records from different time periods, which is why they have put together
    a dataset that contains data on 515,345 records, with release years ranging from
    1922 to 2011\. They have tasked you with preparing the dataset so that it is ready
    to be fed to a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To download the dataset, visit the following UC Irvine Machine Learning Repository
    URL: [https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using pandas, load the text file. Since the previously downloaded text file
    has the same formatting as a CSV file, you can read it using the `read_csv()`
    function. Make sure to turn the header argument to `None`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify whether any qualitative data is present in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check for missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you add an additional `sum()` function to the line of code previously used
    for this purpose, you will get the sum of missing values in the entire dataset,
    without discriminating by column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Check for outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the features from the target data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rescale the data using the standardization methodology.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Split the data into three sets: training, validation, and testing. Use the
    approach of your preference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 188.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Building a Deep Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a neural network, in general terms, can be achieved either on a very
    simple level, using libraries such as scikit-learn (not suitable for deep learning)
    that perform all the math for you, without much flexibility; or on a very complex
    level, by coding every single step of the training process from scratch, or by
    using a more robust framework, which allows great flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch, on the other hand, was built considering the input of many developers
    in the field and has the advantage of allowing both approximations in the same
    place. As mentioned previously, it has an nn module, built to allow easy predefined
    implementations of simple architectures using the sequential container, while
    at the same time allowing the creation of custom modules that introduce flexibility
    to the process of building very complex architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the present section, we will further discuss the use of the sequential container
    for developing deep neural networks, in order to demystify their complexity. Nevertheless,
    in later sections of this book, we will move on to explore more complex and abstract
    applications, which can also be achieved with very little effort.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the sequential container is a module built to contain
    sequences of modules that follow an order. Each of the modules that it contains
    will apply some computation to a given input to arrive at an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most popular modules (layers) that can be used inside the sequential
    container to develop regular classification models are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The modules used for other types of architectures, such as convolutional neural
    networks and recurrent neural networks, will be explained in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '`True` by default) as arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tanh**: This applies the element-wise tanh function to the tensor containing
    the input data. It does not take any arguments.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sigmoid**: This applies the previously explained sigmoid function to the
    tensor containing the input data. It does not take any arguments.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Softmax**: This applies the softmax function to an n-dimensional tensor containing
    the input data. The output is rescaled so that the elements of the tensor lie
    in a range between zero and one, and sum to one. It takes as argument the dimension
    along which the softmax function should be computed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`False` by default. This technique is commonly used for dealing with overfitted
    models, which will be further explained later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalization layer**: There are different methodologies that can be used
    to add a normalization layer in the sequential container. Some of them are BatchNorm1d,
    BatchNorm2d, and BatchNorm3d. The idea behind this is to normalize the output
    from the previous layer, which ultimately achieves similar accuracy levels at
    lower training times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 6: Building a Deep Neural Network Using PyTorch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the PyTorch library to define the architecture
    of a deep neural network of four layers, which then will be trained with the dataset
    prepared previously:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the same Jupyter notebook that you used in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the PyTorch library, called `torch`, as well as the `nn` module from
    PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the different packages and libraries are being imported as they are
    needed for practical learning purposes, it is always a good practice to import
    them at the beginning of your code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the feature columns from the target, for each of the sets created
    in the previous exercise. Additionally, convert the final DataFrames into tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the network architecture using the `sequential()` container. Make sure
    to create a four-layer network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use ReLU activation functions for the first three layers, and leave the last
    layer without an activation function, considering that we are dealing with a regression
    problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The number of units for each layer should be: 100, 50, 25, and 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the loss function as the mean squared error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the optimizer algorithm as the Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use a `for` loop to train the network over the training data for 100 iteration
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test out the model, perform a prediction on the first instance of the testing
    set, and compare it to the ground truth (target value):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From this result, it is possible to see that the model is not performing well
    considering that the target value greatly differs from the predicted value. In
    subsequent sections of this book, you will learn how to improve the performance
    of your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have successfully created and trained a deep neural network
    to solve a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3: Developing a Deep Learning Solution for a Regression Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the following activity, we will create and train a four hidden layer neural
    network to solve the regression problem mentioned in the previous activity. Let''s
    look at the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You continue to work at the music record company and after seeing the great
    job you did in preparing the dataset, they have trusted you with the task of defining
    the network''s architecture and code, as well as training it with the prepared
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the same Jupyter notebook that you used in the previous activity.
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the features from the targets for all three sets of data created in the
    previous activity. Convert the DataFrames into tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the architecture of the network. Feel free to try different combinations
    of the number of layers and the number of units per layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function and the optimizer algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a `for` loop to train the network for 100 iteration steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your model by performing a prediction on the first instance of the testing
    set and comparing it to the ground truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your output should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28: Output of the activity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.28: Output of the activity'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 190.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The theory that gave birth to neural networks was developed decades ago by Frank
    Rosenblatt. It started with the definition of the perceptron, a unit inspired
    by the human neuron, that takes data as an input to perform a transformation on
    it. It consisted of assigning weights to input data to perform a calculation,
    so that the end result would be either one thing or the other, depending on the
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The most widely known form of neural networks is the one created from a succession
    of perceptrons, stacked together in layers, where the output from one column of
    perceptrons (layer) is the input for the following one.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this, the typical learning process for a neural network was explained.
    On this subject, there are three main process to consider: forward propagation,
    the calculation of the loss function, and backward propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: The end goal of this procedure is to minimize the loss function by updating
    the weights and biases that accompany each of the input values along the neural
    network. This is achieved through an iterative process that can take minutes,
    hours, or even weeks, depending on the nature of the data problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main architecture of three types of neural networks was also discussed:
    the artificial neural network, the convolutional neural network, and the recurrent
    neural network. The first is used to solve traditional classification problems,
    the second one is widely popular for its capacity to solve computer vision problems
    (image classification), and the last one, capable of processing data in sequence,
    is useful for tasks such as language translation.'
  prefs: []
  type: TYPE_NORMAL
