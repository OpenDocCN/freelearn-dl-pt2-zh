- en: Building a Deep Learning Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, for the various deep learning architectures we've discussed, we have
    assumed that our input data is static. We have had fixed sets of movie reviews,
    images, or text to process.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, whether your organization or project includes data from self-driving
    cars, IoT sensors, security cameras, or customer-product usage, your data generally
    changes over time. Therefore, you need a way of integrating this new data so that
    you can update your models. The structure of the data may change too, and in the
    case of customer or audience data, there may be new transformations you need to
    apply to the data. Also, dimensions may be added or removed in order to test whether
    they impact the quality of your predictions, are no longer relevant, or fall foul
    of privacy legislation. What do we do in these scenarios?
  prefs: []
  type: TYPE_NORMAL
- en: This is where a tool such as Pachyderm is useful. We would like to know what
    data we have, where we have it, and how we can ensure that the data is feeding
    to our model.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look into using the Pachyderm tool to handle dynamic input values
    in our networks. This will help us to prepare for the real-world use and deployment
    of our systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have learned about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Pachyderm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating our CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Pachyderm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our focus for this book is on developing deep learning systems in Go. So, naturally,
    now that we are talking about how to manage the data that we feed to our networks,
    let's take a look at a tool to do so that is also written in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Pachyderm is a mature and scalable tool that offers containerized data pipelines.
    In these, everything you could possibly need, from data to tools, is held together
    in a single place where deployments can be maintained and managed and versioning
    for the data itself. The Pachyderm team sell their tool as **Git for data**, which
    is a useful analogy. Ideally, we want to version the entire pipeline so that we
    know which data was used to train, and which, in turn, gave us the specific prediction
    of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Pachyderm removes much of the complexity of managing these pipelines. Both Docker
    and Kubernetes run under the hood. We will explore each of these tools in greater
    detail in the next chapter, but for now, all we need to know is that they are
    critical for enabling reproducible builds, as well as scalable distributed training
    of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Pachyderm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of excellent documentation for Pachyderm is available, and we won't attempt
    to rehash all of that here. Instead, we will take you through the basics and build
    a tutorial for managing a simple data pipeline to provide versioned image data
    to the CNN we built in [Chapter 6](a68dc23c-8119-4444-ac6f-beec845c9173.xhtml),
    *Object Recognition with Convolutional Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to install Docker Desktop and enable Kubernetes for your respective
    OS. For this example, we are using macOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full instructions can be found at [https://docs.docker.com/docker-for-mac/install/](https://docs.docker.com/docker-for-mac/install/),
    but let''s go over them in brief now:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Docker `.dmg` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install or launch the file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Kubernetes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To install and run Pachyderm, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable Kubernetes, select the appropriate checkbox after launching the Docker
    settings, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/746cf29d-53a7-4ce6-b1fc-eee1b0b44189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ensure that you have a couple of green blobs indicating that your Docker and
    Kubernetes installations are running. If so, we can confirm that things look okay
    under the hood by dropping into a Terminal and running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Before installing Pachyderm itself, ensure that the cluster is running. We
    are using Homebrew to install Pachyderm by using the following command (please
    note that you will need to have the latest version of Xcode installed):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to launch the Pachyderm command-line tool. First, check
    that the tool has been installed successfully by running the following command
    and observing the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We are almost done setting up our cluster so that we can focus on getting and
    storing data. The last thing to do is deploy Pachyderm on Kubernetes with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to check the status of your cluster. If you run
    the command just after deploying, you should see the containers being created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'They then transition to `Running`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The following section looks at how the data will be prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data into Pachyderm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s prepare our data. In this case, we are using the CIFAR-10 dataset from
    [Chapter 6](a68dc23c-8119-4444-ac6f-beec845c9173.xhtml), *Object Recognition with
    Convolutional Neural Networks*. If you need a refresher, pull the data from the
    source at the University of Toronto, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the data to a temporary directory, and create `repo` in Pachyderm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've got a repository, let's fill it with our CIFAR-10 image data.
    First, let's create individual directories and break up the various CIFAR-10 files
    so that we can just dump an entire directory of files (from our data or training
    sets).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can execute the following command and then confirm that the data has
    made it to `repo` successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can drill down to details of the files that `repo` contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Integrating our CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now take our CNN example from an earlier chapter and make some updates
    that are necessary to package and deploy the network using data supplied by Pachyderm.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker image of our CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pachyderm data pipelines are dependent on prebaked Docker images. The internet
    is full of Docker tutorials, so we'll keep things simple here and discuss what
    we need to do to take advantage of the simple deployment steps for any Go application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at our Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: And that's it! We're simply fetching the Go 1.12 image from Docker Hub and dropping
    our CIFAR CNN into our build. The final piece of our Dockerfile is a command to
    set `GOPATH` and meet our dependencies (for example, installing Gorgonia).
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command to build the Docker image and observe the output: `docker
    build -t cifarcnn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Our container is now ready to be referenced in the Pachyderm data pipeline specification.
  prefs: []
  type: TYPE_NORMAL
- en: Updating our CNN to save the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to add a simple function to our CNN example to ensure the model that
    gets produced is saved, so it can be managed as an object by Pachyderm. Let''s
    add the following to `main.go`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Creating a data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we need to specify a data pipeline in standard JSON. Here, we are mapping
    a repository to a directory and executing our network in either training or inference
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our `cifar_cnn.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The options we've chosen here are straightforward, and you can see the references
    to the Docker image, commands, and switches, as well as `repo` and the mount point
    we're specifying. One thing to note is the `parallelism_spec` option. Setting
    this above the default of `1` allows us to scale a specific pipeline stage as
    required; for example, during the inference phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the pipeline from the preceding template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns you to Command Prompt if there is no error. You can then check
    the status of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can change the level of *parallelism* dynamically and push the configuration
    out to our cluster by updating our template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can update our cluster and check the status of our job and the `k8s`
    cluster pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After giving it some time to run (and using `pachctl logs` to inspect progress),
    we can see our successful job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Interchangeable models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The flexibility of Pachyderm pipelines allows you to easily swap out one model
    for another with a simple update or push of the JSON pipeline we used previously.
  prefs: []
  type: TYPE_NORMAL
- en: What's the point in specifying a pipeline in JSON? It's to make it repeatable!
    Pipelines reprocess data (in our case, to make new predictions about the classes
    of labels) each time their data is updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we update the `image` flag in `cifa_cnn.json` to refer to a version of
    our containerized CNN that, for whatever reason, does not contain dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then update the pipeline on the cluster, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Mapping predictions to models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great feature of Pachyderm—particularly for enterprise use cases—is the ability
    to version both your models and your predictions. Say you are predicting the chance
    a customer will repay a loan, and you see a batch of strange predictions. As part
    of troubleshooting why the model has made these decisions, if you are training
    multiple models across a large team, trawling through email chains and commit
    histories would be a bad idea!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, work backward from the inference to the model, and simply run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then take the relevant commit hash and feed it to the following command,
    observing the details of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the exact commit of the model that was used to produce this prediction,
    the prediction''s provenance, and in turn, the data that was used to train the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Using the Pachyderm dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically, this is a feature of Pachyderm **Enterprise**, but since we want
    to be as inclusive as possible when it comes to the options you have, regardless
    of your use case, we're going to briefly cover the *dashboard* tool. Even if you
    have no need for an easy visual overview of your pipelines and data, 14-day trials
    are available for you to do some exploring of the feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch `http://localhost:30800`. You will be presented with a basic screen
    that includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Repositories (holding our CIFAR-10 data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs or logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29c1d92a-a712-4bf2-91e5-3e7bb453cdbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you may recall, Pachyderm wants you to think of your data repositories as
    Git repositories. This is clearly visible when you drill down into the next screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bae38f6-ee99-4ef7-97ff-c3675859aad2.png)'
  prefs: []
  type: TYPE_IMG
- en: The dashboard offers a familiar GUI interface for the `pachctl` tool we've been
    using up until now.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have gotten practical and looked at what is involved in
    starting the augmentation of input or output components of your model, and what
    tools we can use to do that in a maintainable and traceable way. At a high level,
    we learned about what a data pipeline is and why it is important, how to build/deploy/maintain
    pipelines in Pachyderm, and what tools to use to visualize our repositories and
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at a number of the technologies that sit underneath
    Pachyderm, including Docker and Kubernetes, and how we can use these tools to
    deploy stacks to the cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
