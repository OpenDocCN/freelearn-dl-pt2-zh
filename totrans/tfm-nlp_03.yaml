- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-Tuning BERT Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    we defined the building blocks of the architecture of the original Transformer.
    Think of the original Transformer as a model built with LEGO^® bricks. The construction
    set contains bricks such as encoders, decoders, embedding layers, positional encoding
    methods, multi-head attention layers, masked multi-head attention layers, post-layer
    normalization, feed-forward sub-layers, and linear output layers.
  prefs: []
  type: TYPE_NORMAL
- en: The bricks come in various sizes and forms. You can spend hours building all
    sorts of models using the same building kit! Some constructions will only require
    some of the bricks. Other constructions will add a new piece, just like when we
    obtain additional bricks for a model built using LEGO^® components.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT added a new piece to the Transformer building kit: a bidirectional multi-head
    attention sub-layer. When we humans have problems understanding a sentence, we
    do not just look at the past words. BERT, like us, looks at all the words in the
    same sentence at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will first explore the architecture of **Bidirectional Encoder
    Representations from Transformers** (**BERT**). BERT only uses the blocks of the
    encoders of the Transformer in a novel way and does not use the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will fine-tune a pretrained BERT model. The BERT model we will fine-tune
    was trained by a third party and uploaded to Hugging Face. Transformers can be
    pretrained. Then, a pretrained BERT, for example, can be fine-tuned on several
    NLP tasks. We will go through this fascinating experience of downstream Transformer
    usage using Hugging Face modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Encoder Representations from Transformers (BERT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two-step BERT framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the pretraining environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining pretraining encoder layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downstream multitasking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a fine-tuned BERT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading an acceptability judgment dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating attention masks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT model configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the performance of the fine-tuned model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to explore the background of the BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT introduces bidirectional attention to transformer models. Bidirectional
    attention requires many other changes to the original Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: We will not go through the building blocks of transformers described in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*. You can
    consult *Chapter 2* at any time to review an aspect of the building blocks of
    transformers. In this section, we will focus on the specific aspects of BERT models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on the evolutions designed by *Devlin* et al. (2018), which describe
    the encoder stack. We will first go through the encoder stack, then the preparation
    of the pretraining input environment. Then we will describe the two-step framework
    of BERT: pretraining and fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first explore the encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first building block we will take from the original Transformer model is
    an encoder layer. The encoder layer, as described in *Chapter 2*, *Getting Started
    with the Architecture of the Transformer Model*, is shown in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: The encoder layer'
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model does not use decoder layers. A BERT model has an encoder stack
    but no decoder stacks. The masked tokens (hiding the tokens to predict) are in
    the attention layers of the encoder, as we will see when we zoom into a BERT encoder
    layer in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Transformer contains a stack of *N*=6 layers. The number of dimensions
    of the original Transformer is *d*[model] = 512\. The number of attention heads
    of the original Transformer is *A*=8\. The dimensions of the head of the original
    Transformer are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT encoder layers are larger than the original Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two BERT models can be built with the encoder layers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT[BASE], which contains a stack of *N*=12 encoder layers. *d*[model] = 768
    and can also be expressed as *H*=768, as in the BERT paper. A multi-head attention
    sub-layer contains *A*=12 heads. The dimension of each head *z*[A] remains 64
    as in the original Transformer model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of each multi-head attention sub-layer before concatenation will
    be the output of the 12 heads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[11]*}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT[LARGE], which contains a stack of *N*=24 encoder layers. *d*[model] =
    1024\. A multi-head attention sub-layer contains *A*=16 heads. The dimension of
    each head *z*[A] also remains 64 as in the original Transformer model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of each multi-head attention sub-layer before concatenation will
    be the output of the 16 heads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[15]*}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sizes of the models can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Transformer models'
  prefs: []
  type: TYPE_NORMAL
- en: BERT models are not limited to these three configurations. These three configurations
    illustrate the main aspects of BERT models. Numerous variations are possible.
  prefs: []
  type: TYPE_NORMAL
- en: Size and dimensions play an essential role in BERT-style pretraining. BERT models
    are like humans. BERT models produce better results with more working memory (dimensions)
    and more knowledge (data). Large transformer models that learn large amounts of
    data will pretrain better for downstream NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go to the first sub-layer and see the fundamental aspects of input embedding
    and positional encoding in a BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the pretraining input environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The BERT model has no decoder stack of layers. As such, it does not have a masked
    multi-head attention sub-layer. BERT designers state that a masked multi-head
    attention layer that masks the rest of the sequence impedes the attention process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A masked multi-head attention layer masks all of the tokens that are beyond
    the present position. For example, take the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we have just reached the word `it`, the input of the encoder could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The motivation of this approach is to prevent the model from seeing the output
    it is supposed to predict. This left-to-right approach produces relatively good
    results.
  prefs: []
  type: TYPE_NORMAL
- en: However, the model cannot learn much this way. To know what `it` refers to,
    we need to see the whole sentence to reach the word `rug` and figure out that
    `it` was the rug.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of BERT came up with an idea. Why not pretrain the model to make
    predictions using a different approach?
  prefs: []
  type: TYPE_NORMAL
- en: The authors of BERT came up with bidirectional attention, letting an attention
    head attend to *all* of the words both from left to right and right to left. In
    other words, the self-attention mask of an encoder could do the job without being
    hindered by the masked multi-head attention sub-layer of the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The model was trained with two tasks. The first method is **Masked Language**
    **Modeling** (**MLM**). The second method is **Next Sentence Prediction** (**NSP**).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with masked language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Masked language modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Masked language modeling does not require training a model with a sequence of
    visible words followed by a masked sequence to predict.
  prefs: []
  type: TYPE_NORMAL
- en: BERT introduces the *bidirectional* analysis of a sentence with a random mask
    on a word of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that BERT applies `WordPiece`, a subword segmentation
    tokenization method, to the inputs. It also uses learned positional encoding,
    not the sine-cosine approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'A potential input sequence could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder would mask the attention sequence after the model reached the word
    `it`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'But the BERT encoder masks a random token to make a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The multi-attention sub-layer can now see the whole sequence, run the self-attention
    process, and predict the masked token.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input tokens were masked in a tricky way *to force the model to train longer
    but produce better results* with three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprise the model by not masking a single token on 10% of the dataset; for
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Surprise the model by replacing the token with a random token on 10% of the
    dataset; for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace a token by a `[MASK]` token on 80% of the dataset; for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The authors’ bold approach avoids overfitting and forces the model to train
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: BERT was also trained to perform next sentence prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Next sentence prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second method found to train BERT is **Next Sentence Prediction** (**NSP**).
    The input contains two sentences. In 50% of the cases, the second sentence is
    the actual second sentence of a document. In 50% of the cases, the second sentence
    was selected randomly and has no relation to the first one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two new tokens were added:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[CLS]` is a binary classification token added to the beginning of the first
    sequence to predict if the second sequence follows the first sequence. A positive
    sample is usually a pair of consecutive sentences taken from a dataset. A negative
    sample is created using sequences from different documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[SEP]` is a separation token that signals the end of a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the input sentences taken out of a book could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'These two sentences will become one complete input sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This approach requires additional encoding information to distinguish sequence
    A from sequence B.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we put the whole embedding process together, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Input embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: The input embeddings are obtained by summing the token embeddings, the segment
    (sentence, phrase, word) embeddings, and the positional encoding embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input embedding and positional encoding sub-layer of a BERT model can be
    summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A sequence of words is broken down into `WordPiece` tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `[MASK]` token will randomly replace the initial word tokens for masked language
    modeling training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `[CLS]` classification token is inserted at the beginning of a sequence for
    classification purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `[SEP]` token separates two sentences (segments, phrases) for NSP training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence embedding is added to token embedding, so that sentence A has a different
    sentence embedding value than sentence B.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional encoding is learned. The sine-cosine positional encoding method of
    the original Transformer is not applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some additional key features are:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT uses bidirectional attention in its multi-head attention sub-layers, opening
    vast horizons of learning and understanding relationships between tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT introduces scenarios of unsupervised embedding, pretraining models with
    unlabeled text. Unsupervised scenarios force the model to think harder during
    the multi-head attention learning process. This makes BERT learn how languages
    are built and apply this knowledge to downstream tasks without having to pretrain
    each time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT also uses supervised learning, covering all bases in the pretraining process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT has improved the training environment of transformers. Let’s now see the
    motivation of pretraining and how it helps the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining and fine-tuning a BERT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BERT is a two-step framework. The first step is pretraining, and the second
    is fine-tuning, as shown in *Figure 3.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The BERT framework'
  prefs: []
  type: TYPE_NORMAL
- en: Training a transformer model can take hours, if not days. It takes quite some
    time to engineer the architecture and parameters and select the proper datasets
    to train a transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretraining is the first step of the BERT framework, which can be broken down
    into two sub-steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining the model’s architecture: number of layers, number of heads, dimensions,
    and the other building blocks of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model on **MLM** and **NSP** tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second step of the BERT framework is fine-tuning, which can also be broken
    down into two sub-steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the downstream model chosen with the trained parameters of the
    pretrained BERT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the parameters for specific downstream tasks such as **Recognizing
    Textual Entailment** (**RTE**), question answering (`SQuAD v1.1`, `SQuAD v2.0`),
    and **Situations With Adversarial Generations** (**SWAG**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we covered the information we need to fine-tune a BERT model.
    In the following chapters, we will explore the topics we brought up in this section
    in more depth:'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*, we will pretrain
    a BERT-like model from scratch in 15 steps. We will even compile our data, train
    a tokenizer, and then train the model. This chapter aims to first go through the
    specific building blocks of BERT and then fine-tune an existing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Chapter 5*, *Downstream NLP Tasks with Transformers*, we will go through
    many downstream tasks, exploring `GLUE`, `SQuAD v1.1`, `SQuAD`, `SWAG`, and several
    other NLP evaluation datasets. We will run several downstream transformer models
    to illustrate key tasks. The goal of this chapter is to fine-tune a downstream
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, will
    explore the architecture and unleashed usage of OpenAI `GPT-2` and `GPT-3` transformers.
    BERT[BASE] was configured to be close to OpenAI `GPT` to show that it produced
    better performance. However, OpenAI transformers keep evolving too! We will see
    how they have reached suprahuman NLP levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, the BERT model we will fine-tune will be trained on **The Corpus
    of Linguistic Acceptability** (**CoLA**). The downstream task is based on *Neural
    Network Acceptability Judgments* by *Alex Warstadt*, *Amanpreet Singh*, and *Samuel
    R. Bowman*.
  prefs: []
  type: TYPE_NORMAL
- en: We will fine-tune a BERT model that will determine the grammatical acceptability
    of a sentence. The fine-tuned model will have acquired a certain level of linguistic
    competence.
  prefs: []
  type: TYPE_NORMAL
- en: We have gone through BERT architecture and its pretraining and fine-tuning framework.
    Let’s now fine-tune a BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will fine-tune a BERT model to predict the downstream task of *Acceptability
    Judgments* and measure the predictions with the **Matthews Correlation Coefficient**
    (**MCC**), which will be explained in the *Evaluating using Matthews Correlation
    Coefficient* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Open `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb` in Google Colab (make
    sure you have an email account). The notebook is in `Chapter03` in the GitHub
    repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The title of each cell in the notebook is also the same as or very close to
    the title of each subsection of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will first examine why transformer models must take hardware constraints
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models require multiprocessing hardware. Go to the **Runtime** menu
    in Google Colab, select **Change runtime type**, and select **GPU** in the **Hardware
    Accelerator** drop-down list.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models are hardware-driven. I recommend reading *Appendix II*, *Hardware
    Constraints for Transformer Models*, before continuing this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The program will be using `Hugging Face` modules, which we’ll install next.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Hugging Face PyTorch interface for BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face provides a pretrained BERT model. Hugging Face developed a base
    class named `PreTrainedModel`. By installing this class, we can load a model from
    a pretrained model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face provides modules in TensorFlow and PyTorch. I recommend that a
    developer be comfortable with both environments. Excellent AI research teams use
    either or both environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will install the modules required as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The installation will run, or requirement satisfied messages will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: We can now import the modules needed for the program.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import the pretrained modules required, such as the pretrained `BERT
    tokenizer` and the configuration of the BERT model. The `BERTAdam` optimizer is
    imported along with the sequence classification module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A nice progress bar module is imported from `tqdm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now import the widely used standard Python modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, no message will be displayed, bearing in mind that Google
    Colab has pre-installed the modules on the VM we are using.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying CUDA as the device for torch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now specify that torch uses the **Compute Unified Device Architecture**
    (**CUDA**) to put the parallel computing power of the NVIDIA card to work for
    our multi-head attention model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output may vary with Google Colab configurations. See *Appendix II*: *Hardware
    Constraints for Transformer Models* for explanations and screenshots.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now load the *CoLA* based on the *Warstadt* et al. (2018) paper.
  prefs: []
  type: TYPE_NORMAL
- en: '**General Language Understanding Evaluation** (**GLUE**) considers *Linguistic
    Acceptability* as a top-priority NLP task. In *Chapter 5*, *Downstream NLP Tasks
    with Transformers*, we will explore the key tasks transformers must perform to
    prove their efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following cells in the notebook automatically download the necessary files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see them appear in the file manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Uploading the datasets'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the program will load the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the shape of the dataset we have imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A 10-line sample is displayed to visualize the *Acceptability Judgment* task
    and see if a sequence makes sense or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows `10` lines of the labeled dataset, which may change after
    each run:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **sentence_source** | **label** | **label_notes** | **sentence** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each sample in the `.tsv` files contains four tab-separated columns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Column 1: the source of the sentence (code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 2: the label (`0`=unacceptable, `1`=acceptable)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 3: the label annotated by the author'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 4: the sentence to be classified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can open the `.tsv` files locally to read a few samples of the dataset.
    The program will now process the data for the BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating sentences, label lists, and adding BERT tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program will now create the sentences as described in the *Preparing the
    pretraining input environment* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `[CLS]` and `[SEP]` have now been added.
  prefs: []
  type: TYPE_NORMAL
- en: The program now activates the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Activating the BERT tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will initialize a pretrained BERT tokenizer. This will save
    the time it would take to train it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program selects an uncased tokenizer, activates it, and displays the first
    tokenized sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains the classification token and the sequence segmentation
    token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The program will now process the data.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to determine a fixed maximum length and process the data for the model.
    The sentences in the datasets are short. But, to make sure of this, the program
    sets the maximum length of a sequence to `128`, and the sequences are padded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The sequences have been processed and now the program creates the attention
    masks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating attention masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now comes a tricky part of the process. We padded the sequences in the previous
    cell. But we want to prevent the model from performing attention to those padded
    tokens!
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to apply a mask with a value of `1` for each token, which 0s will
    follow for padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The program will now split the data.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training and validation sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program now performs the standard process of splitting the data into training
    and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The data is ready to be trained, but it still needs to be adapted to torch.
  prefs: []
  type: TYPE_NORMAL
- en: Converting all the data into torch tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fine-tuning model uses torch tensors. The program must convert the data
    into torch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The conversion is over. Now we need to create an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a batch size and creating an iterator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this cell, the program selects a batch size and creates an iterator. The
    iterator is a clever way of avoiding a loop that would load all the data in memory.
    The iterator, coupled with the torch `DataLoader`, can batch train massive datasets
    without crashing the machine’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, the batch size is `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The data has been processed and is all set. The program can now load and configure
    the BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: BERT model configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program now initializes a BERT uncased configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the main Hugging Face parameters similar to the following
    (the library is often updated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through these main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob`: `0.1` applies a `0.1` dropout ratio to the
    attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act`: `"gelu"` is a non-linear activation function in the encoder.
    It is a *Gaussian Error Linear Units* activation function. The input is weighted
    by its magnitude, which makes it non-linear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob`: `0.1` is the dropout probability applied to the fully
    connected layers. Full connections can be found in the embeddings, encoder, and
    pooler layers. The output is not always a good reflection of the content of a
    sequence. Pooling the sequence of hidden states improves the output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size`: `768` is the dimension of the encoded layers and also the pooler
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range`: `0.02` is the standard deviation value when initializing
    the weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size`: `3072` is the dimension of the feed-forward layer of the
    encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps`: `1e-12` is the epsilon value for layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings`: `512` is the maximum length the model uses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_type`: `"bert"` is the name of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads`: `12` is the number of heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers`: `12` is the number of layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id`: `0` is the ID of the padding token to avoid training padding
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_vocab_size`: `2` is the size of the `token_type_ids`, which identify
    the sequences. For example, “the `dog[SEP]` The `cat.[SEP]`" can be represented
    with token IDs `[0,0,0, 1,1,1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size`: `30522` is the number of different tokens used by the model to
    represent the `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these parameters in mind, we can load the pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Hugging Face BERT uncased base model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program now loads the pretrained BERT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We have defined the model, defined parallel processing, and sent the model to
    the device. For more explanations, see *Appendix II*, *Hardware Constraints for
    Transformer Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pretrained model can be trained further if necessary. It is interesting
    to explore the architecture in detail to visualize the parameters of each sublayer,
    as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now go through the main parameters of the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer grouped parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The program will now initialize the optimizer for the model’s parameters. Fine-tuning
    a model begins with initializing the pretrained model parameter values (not their
    names).
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of the optimizer include a weight decay rate to avoid overfitting,
    and some parameters are filtered.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to prepare the model’s parameters for the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The parameters have been prepared and cleaned up. They are ready for the training
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameters for the training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hyperparameters for the training loop are critical, though they seem innocuous.
    Adam will activate weight decay and also go through a warm-up phase, for example.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate (`lr`) and warm-up rate (`warmup`) should be set to a very
    small value early in the optimization phase and gradually increase after a certain
    number of iterations. This avoids large gradients and overshooting the optimization
    goals.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers argue that the gradients at the output level of the sub-layers
    before layer normalization do not require a warm-up rate. Solving this problem
    requires many experimental runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimizer is a BERT version of Adam called `BertAdam`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The program adds an accuracy measurement function to compare the predictions
    to the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: The data is ready. The parameters are ready. It’s time to activate the training
    loop!
  prefs: []
  type: TYPE_NORMAL
- en: The training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training loop follows standard learning processes. The number of epochs
    is set to `4`, and measurement for loss and accuracy will be plotted. The training
    loop uses the `dataloader` to load and train batches. The training process is
    measured and evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code starts by initializing the `train_loss_set`, which will store the
    loss and accuracy, which will be plotted. It starts training its epochs and runs
    a standard training loop, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the information for each `epoch` with the `trange` wrapper,
    `for _ in trange(epochs, desc="Epoch")`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Transformer models are evolving very quickly, and deprecation messages and even
    errors might occur. Hugging Face is no exception to this, and we must update our
    code accordingly when this happens.
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained. We can now display the training evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Training evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss and accuracy values were stored in `train_loss_set` as defined at the
    beginning of the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now plots the measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a graph that shows that the training process went well and was
    efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Training loss per batch'
  prefs: []
  type: TYPE_NORMAL
- en: The model has been fine-tuned. We can now run predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating using the holdout dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BERT downstream model was trained with the `in_domain_train.tsv` dataset.
    The program will now make predictions using the holdout (testing) dataset in the
    `out_of_domain_dev.tsv` file. The goal is to predict whether the sentence is grammatically
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following excerpt of the code shows that the data preparation process applied
    to the training data is repeated in the part of the code for the holdout dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The program then runs batch predictions using the `dataloader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The logits and labels of the predictions are moved to the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The predictions and their true labels are stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The program can now evaluate the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating using the Matthews Correlation Coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Matthews Correlation Coefficient** (**MCC**) was initially designed to
    measure the quality of binary classifications and can be modified to be a multi-class
    correlation coefficient. A two-class classification can be made with four probabilities
    at each prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: TP = True Positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TN = True Negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP = False Positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FN = False Negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brian W. Matthews*, a biochemist, designed it in 1975, inspired by his predecessors’
    *phi* function. Since then, it has evolved into various formats such as the following
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: The value produced by MCC is between `-1` and `+1`. `+1` is the maximum positive
    value of a prediction. `-1` is an inverse prediction. `0` is an average random
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: GLUE evaluates *Linguistic Acceptability* with MCC.
  prefs: []
  type: TYPE_NORMAL
- en: 'MCC is imported from `sklearn.metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'A set of predictions is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The MCC value is calculated and stored in `matthews_set`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: You may see messages due to library and module version changes. The final score
    will be based on the entire test set, but let’s look at the scores on the individual
    batches to get a sense of the variability in the metric between batches.
  prefs: []
  type: TYPE_NORMAL
- en: The scores of individual batches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s view the scores of the individual batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The output produces MCC values between `-1` and `+1` as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Almost all the MCC values are positive, which is good news. Let’s see what the
    evaluation is for the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Matthews evaluation for the whole dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MCC is a practical way to evaluate a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program will now aggregate the true values for the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'MCC produces a correlation value between `–1` and `+1`. `0` is an average prediction,
    `-1` is an inverse one, and `1` is perfect. In this case, the output confirms
    that the MCC is positive, which shows that there is a correlation between this
    model and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: On this final positive evaluation of the fine-tuning of the BERT model, we have
    an overall view of the BERT training framework.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT brings bidirectional attention to transformers. Predicting sequences from
    left to right and masking the future tokens to train a model has serious limitations.
    If the masked sequence contains the meaning we are looking for, the model will
    produce errors. BERT attends to all of the tokens of a sequence at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We explored the architecture of BERT, which only uses the encoder stack of transformers.
    BERT was designed as a two-step framework. The first step of the framework is
    to pretrain a model. The second step is to fine-tune the model. We built a fine-tuning
    BERT model for an *Acceptability Judgment* downstream task. The fine-tuning process
    went through all phases of the process. First, we loaded the dataset and loaded
    the necessary pretrained modules of the model. Then the model was trained, and
    its performance was measured.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a pretrained model takes fewer machine resources than training downstream
    tasks from scratch. Fine-tuned models can perform a variety of tasks. BERT proves
    that we can pretrain a model on two tasks only, which is remarkable in itself.
    But producing a multitask fine-tuned model based on the trained parameters of
    the BERT pretrained model is extraordinary.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, shows
    that OpenAI has reached a zero-shot level with little to no fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we fine-tuned a BERT model. In the next chapter, *Chapter 4*,
    *Pretraining a RoBERTa Model from Scratch*, we will dig deeper into the BERT framework
    and build a pretrained BERT-like model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT stands for Bidirectional Encoder Representations from Transformers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT is a two-step framework. *Step 1* is pretraining. *Step 2* is fine-tuning.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model implies training parameters from scratch. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT only pretrains using all downstream tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains with **Masked** **Language Modeling** (**MLM**). (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains with **Next Sentence Predictions** (**NSP**). (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains mathematical functions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A question-answer task is a downstream task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A BERT pretraining model does not require tokenization. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model takes less time than pretraining. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    Is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, and *Kristina Toutanova*, *2018*,
    *BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Warstadt*, *Amanpreet Singh*, and *Samuel R. Bowman*, 2018, *Neural Network
    Acceptability Judgments*: [https://arxiv.org/abs/1805.12471](https://arxiv.org/abs/1805.12471)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Corpus of Linguistic Acceptability** (**CoLA**): [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation on Hugging Face models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/roberta.html](https://huggingface.co/transformers/model_doc/roberta.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
