- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Fine-Tuning BERT Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调BERT模型
- en: In *Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    we defined the building blocks of the architecture of the original Transformer.
    Think of the original Transformer as a model built with LEGO^® bricks. The construction
    set contains bricks such as encoders, decoders, embedding layers, positional encoding
    methods, multi-head attention layers, masked multi-head attention layers, post-layer
    normalization, feed-forward sub-layers, and linear output layers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*，*开始使用变压器模型的架构*中，我们定义了原始Transformer架构的构建块。 将原始Transformer想象成用乐高^® 砖块构建的模型。
    构建套件包含编码器、解码器、嵌入层、位置编码方法、多头注意力层、掩码多头注意力层、后层规范化、前馈子层和线性输出层等砖块。
- en: The bricks come in various sizes and forms. You can spend hours building all
    sorts of models using the same building kit! Some constructions will only require
    some of the bricks. Other constructions will add a new piece, just like when we
    obtain additional bricks for a model built using LEGO^® components.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 砖块有各种大小和形状。你可以花几个小时使用相同的建筑套件构建各种模型！一些构建只需要一些砖块。其他构建将添加一个新的部件，就像我们使用乐高^® 组件构建模型时获得额外的砖块一样。
- en: 'BERT added a new piece to the Transformer building kit: a bidirectional multi-head
    attention sub-layer. When we humans have problems understanding a sentence, we
    do not just look at the past words. BERT, like us, looks at all the words in the
    same sentence at the same time.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: BERT向Transformer构建套件添加了一个新部件：双向多头注意力子层。 当我们人类在理解一个句子时遇到问题时，我们不仅仅查看过去的单词。 BERT像我们一样，同时查看同一句子中的所有单词。
- en: This chapter will first explore the architecture of **Bidirectional Encoder
    Representations from Transformers** (**BERT**). BERT only uses the blocks of the
    encoders of the Transformer in a novel way and does not use the decoder stack.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先探讨**来自变压器的双向编码表示**（**BERT**）的架构。 BERT以一种新颖的方式仅使用变压器的编码器块，并且不使用解码器堆栈。
- en: Then we will fine-tune a pretrained BERT model. The BERT model we will fine-tune
    was trained by a third party and uploaded to Hugging Face. Transformers can be
    pretrained. Then, a pretrained BERT, for example, can be fine-tuned on several
    NLP tasks. We will go through this fascinating experience of downstream Transformer
    usage using Hugging Face modules.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对预训练的BERT模型进行微调。 我们将对将进行微调的BERT模型进行预训练，该模型由第三方训练并上传到Hugging Face。 变压器可以进行预训练。
    然后，例如，可以在几个NLP任务上对预训练的BERT进行微调。 我们将通过使用Hugging Face模块进行下游Transformer使用的这种迷人经历。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Bidirectional Encoder Representations from Transformers (BERT)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自变压器的双向编码表示（BERT）
- en: The architecture of BERT
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT的架构
- en: The two-step BERT framework
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两步BERT框架
- en: Preparing the pretraining environment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备预训练环境
- en: Defining pretraining encoder layers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义预训练编码器层
- en: Defining fine-tuning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义微调
- en: Downstream multitasking
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下游多任务处理
- en: Building a fine-tuned BERT model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建微调的BERT模型
- en: Loading an acceptability judgment dataset
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载可接受性判断数据集
- en: Creating attention masks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建注意力掩码
- en: BERT model configuration
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT模型配置
- en: Measuring the performance of the fine-tuned model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量微调模型的性能
- en: Our first step will be to explore the background of the BERT model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是探索BERT模型的背景。
- en: The architecture of BERT
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 的架构
- en: BERT introduces bidirectional attention to transformer models. Bidirectional
    attention requires many other changes to the original Transformer model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BERT将双向注意引入了变压器模型。 双向注意需要对原始Transformer模型进行许多其他更改。
- en: We will not go through the building blocks of transformers described in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*. You can
    consult *Chapter 2* at any time to review an aspect of the building blocks of
    transformers. In this section, we will focus on the specific aspects of BERT models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不会讨论*第2章*，*开始使用变压器模型的架构*中描述的变压器构建块。 您可以随时查阅*第2章*，以查看有关变压器构建块的某个方面的评论。 在本节中，我们将重点关注BERT模型的特定方面。
- en: 'We will focus on the evolutions designed by *Devlin* et al. (2018), which describe
    the encoder stack. We will first go through the encoder stack, then the preparation
    of the pretraining input environment. Then we will describe the two-step framework
    of BERT: pretraining and fine-tuning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注*Devlin*等人（2018）设计的演进，描述了编码器堆栈。 我们将首先通过编码器堆栈，然后通过预训练输入环境的准备来描述BERT的两步框架：预训练和微调。
- en: Let’s first explore the encoder stack.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先探索编码器堆栈。
- en: The encoder stack
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器堆栈
- en: 'The first building block we will take from the original Transformer model is
    an encoder layer. The encoder layer, as described in *Chapter 2*, *Getting Started
    with the Architecture of the Transformer Model*, is shown in *Figure 3.1*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从原始变换器模型中采用的第一个构建块是编码器层。正如在*第2章* *变换器模型架构入门* 中描述的那样，编码器层显示在*图3.1*中：
- en: '![](img/B17948_03_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_01.png)'
- en: 'Figure 3.1: The encoder layer'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：编码器层
- en: The BERT model does not use decoder layers. A BERT model has an encoder stack
    but no decoder stacks. The masked tokens (hiding the tokens to predict) are in
    the attention layers of the encoder, as we will see when we zoom into a BERT encoder
    layer in the following sections.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型不使用解码器层。BERT模型具有编码器堆栈，但没有解码器堆栈。掩码的标记（隐藏要预测的标记）位于编码器的注意力层中，我们将在后续部分中放大BERT编码器层时看到。
- en: 'The original Transformer contains a stack of *N*=6 layers. The number of dimensions
    of the original Transformer is *d*[model] = 512\. The number of attention heads
    of the original Transformer is *A*=8\. The dimensions of the head of the original
    Transformer are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变换器包含一个*N*=6层的堆栈。原始变换器的维度数为*d*[model] = 512。原始变换器的注意力头数为*A*=8。原始变换器的头部维度是：
- en: '![](img/B17948_03_001.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_001.png)'
- en: BERT encoder layers are larger than the original Transformer model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: BERT编码器层比原始变换器模型更大。
- en: 'Two BERT models can be built with the encoder layers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 两个BERT模型可以使用编码器层构建：
- en: 'BERT[BASE], which contains a stack of *N*=12 encoder layers. *d*[model] = 768
    and can also be expressed as *H*=768, as in the BERT paper. A multi-head attention
    sub-layer contains *A*=12 heads. The dimension of each head *z*[A] remains 64
    as in the original Transformer model:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT[BASE]，其中包含*N*=12编码器层。*d*[model] = 768，也可以表示为*H*=768，就像BERT论文中所述。多头注意力子层包含*A*=12个头。每个头的维度*z*[A]与原始变换器模型中保持为64：
- en: '![](img/B17948_03_002.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_002.png)'
- en: 'The output of each multi-head attention sub-layer before concatenation will
    be the output of the 12 heads:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连接之前，每个多头注意力子层的输出将是12个头的输出：
- en: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[11]*}*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[11]*}*'
- en: 'BERT[LARGE], which contains a stack of *N*=24 encoder layers. *d*[model] =
    1024\. A multi-head attention sub-layer contains *A*=16 heads. The dimension of
    each head *z*[A] also remains 64 as in the original Transformer model:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT[LARGE]，其中包含*N*=24编码器层。*d*[model] = 1024。多头注意力子层包含*A*=16个头。每个头的维度*z*[A]与原始变换器模型中保持为64：
- en: '![](img/B17948_03_003.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_003.png)'
- en: 'The output of each multi-head attention sub-layer before concatenation will
    be the output of the 16 heads:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连接之前，每个多头注意力子层的输出将是16个头的输出：
- en: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[15]*}*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*output_multi-head_attention={z*[0], *z*[1], *z*[2],…, *z*[15]*}*'
- en: 'The sizes of the models can be summed up as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的大小可以总结如下：
- en: '![](img/B17948_03_02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_02.png)'
- en: 'Figure 3.2: Transformer models'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：变换器模型
- en: BERT models are not limited to these three configurations. These three configurations
    illustrate the main aspects of BERT models. Numerous variations are possible.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型不仅限于这三种配置。这三种配置说明了BERT模型的主要方面。有许多变种是可能的。
- en: Size and dimensions play an essential role in BERT-style pretraining. BERT models
    are like humans. BERT models produce better results with more working memory (dimensions)
    and more knowledge (data). Large transformer models that learn large amounts of
    data will pretrain better for downstream NLP tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尺寸和维度在BERT风格的预训练中起着至关重要的作用。BERT模型就像人类一样。BERT模型随着更多的工作内存（维度）和更多的知识（数据）产生更好的结果。学习大量数据的大型变换器模型将为下游自然语言处理任务更好地进行预训练。
- en: Let’s go to the first sub-layer and see the fundamental aspects of input embedding
    and positional encoding in a BERT model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入第一个子层，看看BERT模型中输入嵌入和位置编码的基本方面。
- en: Preparing the pretraining input environment
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备预训练输入环境
- en: The BERT model has no decoder stack of layers. As such, it does not have a masked
    multi-head attention sub-layer. BERT designers state that a masked multi-head
    attention layer that masks the rest of the sequence impedes the attention process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型没有解码器层的堆栈。因此，它没有一个掩码的多头注意力子层。BERT设计者指出，掩码了其余序列的掩码的多头注意力层妨碍了注意力过程。
- en: 'A masked multi-head attention layer masks all of the tokens that are beyond
    the present position. For example, take the following sentence:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码的多头注意力层掩盖了超出当前位置的所有标记。 例如，采用以下句子：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we have just reached the word `it`, the input of the encoder could be:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们刚到达单词`it`，编码器的输入可以是:'
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The motivation of this approach is to prevent the model from seeing the output
    it is supposed to predict. This left-to-right approach produces relatively good
    results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的动机是防止模型看到它应该预测的输出。 这种从左到右的方法可以产生相当好的结果。
- en: However, the model cannot learn much this way. To know what `it` refers to,
    we need to see the whole sentence to reach the word `rug` and figure out that
    `it` was the rug.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型不能通过这种方式学到很多。 要知道`it`指的是什么，我们需要看整个句子，达到`rug`这个词，并弄清楚`it`指的是地毯。
- en: The authors of BERT came up with an idea. Why not pretrain the model to make
    predictions using a different approach?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者想到了一个主意。为什么不预训练模型使用不同的方法来进行预测？
- en: The authors of BERT came up with bidirectional attention, letting an attention
    head attend to *all* of the words both from left to right and right to left. In
    other words, the self-attention mask of an encoder could do the job without being
    hindered by the masked multi-head attention sub-layer of the decoder.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的作者提出了双向注意力，让一个注意力头同时从左到右和从右到左关注*所有*单词。 换句话说，编码器的自注意力屏蔽可以在不受解码器的掩码多头注意力子层的阻碍的情况下完成任务。
- en: The model was trained with two tasks. The first method is **Masked Language**
    **Modeling** (**MLM**). The second method is **Next Sentence Prediction** (**NSP**).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型经过了两项任务的训练。 第一种方法是**掩码语言建模**（**MLM**）。 第二种方法是**下一句预测**（**NSP**）。
- en: Let’s start with masked language modeling.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从掩码语言建模开始。
- en: Masked language modeling
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: Masked language modeling does not require training a model with a sequence of
    visible words followed by a masked sequence to predict.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言建模不需要训练模型，即可预测由可见单词序列后跟掩码序列。
- en: BERT introduces the *bidirectional* analysis of a sentence with a random mask
    on a word of the sentence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BERT引入了对句子的*双向*分析，其中对句子的一个单词进行随机掩盖。
- en: It is important to note that BERT applies `WordPiece`, a subword segmentation
    tokenization method, to the inputs. It also uses learned positional encoding,
    not the sine-cosine approach.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，BERT应用了`WordPiece`，一种子词分词的方法，以及学习的位置编码，而不是正弦-余弦方法。
- en: 'A potential input sequence could be:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '一个潜在的输入序列可能是:'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The decoder would mask the attention sequence after the model reached the word
    `it`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '当模型到达单词`it`后，解码器将掩码注意力序列:'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'But the BERT encoder masks a random token to make a prediction:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '但是BERT编码器会将一个随机标记进行掩码以进行预测:'
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The multi-attention sub-layer can now see the whole sequence, run the self-attention
    process, and predict the masked token.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 多重注意力子层现在可以看到整个序列，运行自注意力过程，并预测被掩码的标记。
- en: 'The input tokens were masked in a tricky way *to force the model to train longer
    but produce better results* with three methods:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输入标记以一种棘手的方式进行了掩码，*以便让模型训练时间更长，但产生更好的结果*，采用三种方法：
- en: 'Surprise the model by not masking a single token on 10% of the dataset; for
    example:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在数据集的10%中，出其不意地不掩盖一个标记; 例如:'
- en: '[PRE5]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Surprise the model by replacing the token with a random token on 10% of the
    dataset; for example:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在数据集的10%中，用随机标记替换标记来出其不意; 例如:'
- en: '[PRE6]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Replace a token by a `[MASK]` token on 80% of the dataset; for example:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在80%的数据集中，将一个标记替换为`[MASK]`标记; 例如:'
- en: '[PRE7]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The authors’ bold approach avoids overfitting and forces the model to train
    efficiently.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们大胆的方法避免了过拟合，并迫使模型高效训练。
- en: BERT was also trained to perform next sentence prediction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BERT还经过训练用于执行下一句预测。
- en: Next sentence prediction
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一句预测
- en: The second method found to train BERT is **Next Sentence Prediction** (**NSP**).
    The input contains two sentences. In 50% of the cases, the second sentence is
    the actual second sentence of a document. In 50% of the cases, the second sentence
    was selected randomly and has no relation to the first one.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种训练BERT的方法是**下一句预测**（**NSP**）。输入包含两个句子。 在50%的情况下，第二个句子是文档的实际第二句。 在50%的情况下，第二个句子是随机选择的，与第一个句子无关。
- en: 'Two new tokens were added:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了两个新标记：
- en: '`[CLS]` is a binary classification token added to the beginning of the first
    sequence to predict if the second sequence follows the first sequence. A positive
    sample is usually a pair of consecutive sentences taken from a dataset. A negative
    sample is created using sequences from different documents.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]`是一个二元分类标记，添加到第一个序列的开头，以预测第二个序列是否跟随第一个序列。正样本通常是从数据集中取出的连续句子对。负样本是使用来自不同文档的序列创建的。'
- en: '`[SEP]` is a separation token that signals the end of a sequence.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`是一个分隔标记，表示序列的结束。'
- en: 'For example, the input sentences taken out of a book could be:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从一本书中摘录的输入句子可能是：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These two sentences will become one complete input sequence:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个句子将成为一个完整的输入序列：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This approach requires additional encoding information to distinguish sequence
    A from sequence B.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法需要额外的编码信息来区分序列 A 和序列 B。
- en: 'If we put the whole embedding process together, we obtain:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将整个嵌入过程放在一起，我们得到：
- en: '![](img/B17948_03_03.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_03.png)'
- en: 'Figure 3.3: Input embeddings'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：输入嵌入
- en: The input embeddings are obtained by summing the token embeddings, the segment
    (sentence, phrase, word) embeddings, and the positional encoding embeddings.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入由标记嵌入、段（句子、短语、单词）嵌入和位置编码嵌入求和得到。
- en: 'The input embedding and positional encoding sub-layer of a BERT model can be
    summed up as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型的输入嵌入和位置编码子层可以总结如下：
- en: A sequence of words is broken down into `WordPiece` tokens.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列单词被分解成`WordPiece`标记。
- en: A `[MASK]` token will randomly replace the initial word tokens for masked language
    modeling training.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`[MASK]`标记将随机替换初始的单词标记，用于掩码语言建模训练。
- en: A `[CLS]` classification token is inserted at the beginning of a sequence for
    classification purposes.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了分类目的，在序列的开头插入一个`[CLS]`分类标记。
- en: A `[SEP]` token separates two sentences (segments, phrases) for NSP training.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`[SEP]`标记分隔两个句子（段落、短语）用于 NSP 训练。
- en: Sentence embedding is added to token embedding, so that sentence A has a different
    sentence embedding value than sentence B.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子嵌入添加到标记嵌入中，使句子 A 的句子嵌入值与句子 B 不同。
- en: Positional encoding is learned. The sine-cosine positional encoding method of
    the original Transformer is not applied.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习位置编码。原始 Transformer 的正弦-余弦位置编码方法没有被应用。
- en: 'Some additional key features are:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些额外的关键特性：
- en: BERT uses bidirectional attention in its multi-head attention sub-layers, opening
    vast horizons of learning and understanding relationships between tokens.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 在其多头注意力子层中使用双向注意力，打开了广阔的学习视野，并理解标记之间的关系。
- en: BERT introduces scenarios of unsupervised embedding, pretraining models with
    unlabeled text. Unsupervised scenarios force the model to think harder during
    the multi-head attention learning process. This makes BERT learn how languages
    are built and apply this knowledge to downstream tasks without having to pretrain
    each time.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 引入了无监督嵌入的场景，使用未标记的文本预训练模型。无监督场景迫使模型在多头注意力学习过程中更加努力思考。这使得 BERT 学习语言的构建方式，并将这些知识应用到下游任务中，而不必每次都进行预训练。
- en: BERT also uses supervised learning, covering all bases in the pretraining process.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 也使用监督学习，在预训练过程中涵盖了所有基础知识。
- en: BERT has improved the training environment of transformers. Let’s now see the
    motivation of pretraining and how it helps the fine-tuning process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 改进了 Transformer 的训练环境。现在让我们看看预训练的动机以及它如何帮助微调过程。
- en: Pretraining and fine-tuning a BERT model
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练和微调 BERT 模型
- en: 'BERT is a two-step framework. The first step is pretraining, and the second
    is fine-tuning, as shown in *Figure 3.4*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个两步框架。第一步是预训练，第二步是微调，如*图3.4*所示：
- en: '![](img/B17948_03_04.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_03_04.png)'
- en: 'Figure 3.4: The BERT framework'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：BERT 框架
- en: Training a transformer model can take hours, if not days. It takes quite some
    time to engineer the architecture and parameters and select the proper datasets
    to train a transformer model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个 Transformer 模型可能需要几个小时，甚至几天。设计架构和参数以及选择适当的数据集来训练 Transformer 模型需要相当长的时间。
- en: 'Pretraining is the first step of the BERT framework, which can be broken down
    into two sub-steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练是 BERT 框架的第一步，可以分解为两个子步骤：
- en: 'Defining the model’s architecture: number of layers, number of heads, dimensions,
    and the other building blocks of the model'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型的架构：层数、头数、维度和模型的其他构建模块
- en: Training the model on **MLM** and **NSP** tasks
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**MLM**和**NSP**任务上训练模型
- en: 'The second step of the BERT framework is fine-tuning, which can also be broken
    down into two sub-steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BERT框架的第二步是微调，也可以分解为两个子步骤：
- en: Initializing the downstream model chosen with the trained parameters of the
    pretrained BERT model
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用预训练的BERT模型的训练参数初始化所选的下游模型
- en: Fine-tuning the parameters for specific downstream tasks such as **Recognizing
    Textual Entailment** (**RTE**), question answering (`SQuAD v1.1`, `SQuAD v2.0`),
    and **Situations With Adversarial Generations** (**SWAG**)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对特定下游任务（如**文本蕴涵识别**（**RTE**），问答（`SQuAD v1.1`，`SQuAD v2.0`）和**带有对抗生成的情境**（**SWAG**））微调参数
- en: 'In this section, we covered the information we need to fine-tune a BERT model.
    In the following chapters, we will explore the topics we brought up in this section
    in more depth:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了微调BERT模型所需的信息。在接下来的章节中，我们将更深入地探讨本节提出的主题：
- en: In *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*, we will pretrain
    a BERT-like model from scratch in 15 steps. We will even compile our data, train
    a tokenizer, and then train the model. This chapter aims to first go through the
    specific building blocks of BERT and then fine-tune an existing model.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第4章*，*从头开始预训练RoBERTa模型*中，我们将从头开始预训练一个类似BERT的模型的15个步骤。我们甚至会编译我们的数据，训练一个分词器，然后训练模型。这一章旨在首先讨论BERT的特定构建模块，然后微调现有模型。
- en: In *Chapter 5*, *Downstream NLP Tasks with Transformers*, we will go through
    many downstream tasks, exploring `GLUE`, `SQuAD v1.1`, `SQuAD`, `SWAG`, and several
    other NLP evaluation datasets. We will run several downstream transformer models
    to illustrate key tasks. The goal of this chapter is to fine-tune a downstream
    model.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第5章*，*变形金刚进行下游自然语言处理任务*中，我们将遍历许多下游任务，探索`GLUE`，`SQuAD v1.1`，`SQuAD`，`SWAG`和其他几个自然语言处理评估数据集。我们将运行几个下游变形金刚模型来说明关键任务。本章的目标是微调一个下游模型。
- en: In *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, will
    explore the architecture and unleashed usage of OpenAI `GPT-2` and `GPT-3` transformers.
    BERT[BASE] was configured to be close to OpenAI `GPT` to show that it produced
    better performance. However, OpenAI transformers keep evolving too! We will see
    how they have reached suprahuman NLP levels.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第7章*，*GPT-3引擎的超人变形金刚的崛起*中，将探讨OpenAI `GPT-2`和`GPT-3`变形金刚的架构和使用方法。BERT[BASE]被配置得接近于OpenAI
    `GPT`，以展示它产生了更好的性能。然而，OpenAI变形金刚也在不断进化！我们将看到它们如何达到了超人类的自然语言处理水平。
- en: In this chapter, the BERT model we will fine-tune will be trained on **The Corpus
    of Linguistic Acceptability** (**CoLA**). The downstream task is based on *Neural
    Network Acceptability Judgments* by *Alex Warstadt*, *Amanpreet Singh*, and *Samuel
    R. Bowman*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将对**语言可接受性语料库**（**CoLA**）上进行微调的BERT模型进行训练。下游任务基于*Alex Warstadt*，*Amanpreet
    Singh*和*Samuel R. Bowman*的*神经网络可接受性判断*。
- en: We will fine-tune a BERT model that will determine the grammatical acceptability
    of a sentence. The fine-tuned model will have acquired a certain level of linguistic
    competence.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调一个BERT模型，该模型将确定一个句子的语法可接受性。微调后的模型将获得一定水平的语言能力。
- en: We have gone through BERT architecture and its pretraining and fine-tuning framework.
    Let’s now fine-tune a BERT model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了BERT的架构及其预训练和微调框架。现在让我们来微调一个BERT模型。
- en: Fine-tuning BERT
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调BERT
- en: This section will fine-tune a BERT model to predict the downstream task of *Acceptability
    Judgments* and measure the predictions with the **Matthews Correlation Coefficient**
    (**MCC**), which will be explained in the *Evaluating using Matthews Correlation
    Coefficient* section of this chapter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将微调一个BERT模型，以预测*可接受性判断*的下游任务，并使用**马修斯相关系数**（**MCC**）来衡量预测，这将在本章的*使用马修斯相关系数进行评估*部分进行解释。
- en: Open `BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb` in Google Colab (make
    sure you have an email account). The notebook is in `Chapter03` in the GitHub
    repository of this book.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab中打开`BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb`（确保你有一个电子邮箱账户）。笔记本在本书的GitHub存储库的`Chapter03`中。
- en: The title of each cell in the notebook is also the same as or very close to
    the title of each subsection of this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中每个单元格的标题也与本章的每个小节的标题相同或非常接近。
- en: We will first examine why transformer models must take hardware constraints
    into account.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨为什么变形金刚模型必须考虑硬件约束。
- en: Hardware constraints
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件约束
- en: Transformer models require multiprocessing hardware. Go to the **Runtime** menu
    in Google Colab, select **Change runtime type**, and select **GPU** in the **Hardware
    Accelerator** drop-down list.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models are hardware-driven. I recommend reading *Appendix II*, *Hardware
    Constraints for Transformer Models*, before continuing this chapter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The program will be using `Hugging Face` modules, which we’ll install next.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Hugging Face PyTorch interface for BERT
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face provides a pretrained BERT model. Hugging Face developed a base
    class named `PreTrainedModel`. By installing this class, we can load a model from
    a pretrained model configuration.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face provides modules in TensorFlow and PyTorch. I recommend that a
    developer be comfortable with both environments. Excellent AI research teams use
    either or both environments.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will install the modules required as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The installation will run, or requirement satisfied messages will be displayed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: We can now import the modules needed for the program.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Importing the modules
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import the pretrained modules required, such as the pretrained `BERT
    tokenizer` and the configuration of the BERT model. The `BERTAdam` optimizer is
    imported along with the sequence classification module:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A nice progress bar module is imported from `tqdm`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now import the widely used standard Python modules:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If all goes well, no message will be displayed, bearing in mind that Google
    Colab has pre-installed the modules on the VM we are using.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Specifying CUDA as the device for torch
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now specify that torch uses the **Compute Unified Device Architecture**
    (**CUDA**) to put the parallel computing power of the NVIDIA card to work for
    our multi-head attention model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output may vary with Google Colab configurations. See *Appendix II*: *Hardware
    Constraints for Transformer Models* for explanations and screenshots.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: We will now load the dataset.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now load the *CoLA* based on the *Warstadt* et al. (2018) paper.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '**General Language Understanding Evaluation** (**GLUE**) considers *Linguistic
    Acceptability* as a top-priority NLP task. In *Chapter 5*, *Downstream NLP Tasks
    with Transformers*, we will explore the key tasks transformers must perform to
    prove their efficiency.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The following cells in the notebook automatically download the necessary files:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see them appear in the file manager:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_05.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Uploading the datasets'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the program will load the datasets:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output displays the shape of the dataset we have imported:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A 10-line sample is displayed to visualize the *Acceptability Judgment* task
    and see if a sequence makes sense or not:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output shows `10` lines of the labeled dataset, which may change after
    each run:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **sentence_source** | **label** | **label_notes** | **sentence** |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '|'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '|'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '|'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '|'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '|'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Each sample in the `.tsv` files contains four tab-separated columns:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`.tsv`文件中每个样本包含四个以制表符分隔的列：'
- en: 'Column 1: the source of the sentence (code)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一列：句子的来源（代码）
- en: 'Column 2: the label (`0`=unacceptable, `1`=acceptable)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二列：标签（`0`=不可接受，`1`=可接受）
- en: 'Column 3: the label annotated by the author'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三列：作者注释的标签
- en: 'Column 4: the sentence to be classified'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四列：待分类的句子
- en: You can open the `.tsv` files locally to read a few samples of the dataset.
    The program will now process the data for the BERT model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本地打开`.tsv`文件，阅读数据集的一些样本。程序现在将处理数据用于BERT模型。
- en: Creating sentences, label lists, and adding BERT tokens
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建句子，标签列表，并添加BERT标记
- en: 'The program will now create the sentences as described in the *Preparing the
    pretraining input environment* section of this chapter:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在会按照本章*准备预训练输入环境*部分的描述创建句子：
- en: '[PRE69]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `[CLS]` and `[SEP]` have now been added.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`[CLS]`和`[SEP]`现在已经添加。'
- en: The program now activates the tokenizer.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在激活了分词器。
- en: Activating the BERT tokenizer
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活BERT分词器
- en: In this section, we will initialize a pretrained BERT tokenizer. This will save
    the time it would take to train it from scratch.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将初始化一个预训练的BERT分词器。这将节省从头开始训练它所需的时间。
- en: 'The program selects an uncased tokenizer, activates it, and displays the first
    tokenized sentence:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 程序选择了一个小写分词器，激活它，并显示了第一个标记化的句子：
- en: '[PRE70]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output contains the classification token and the sequence segmentation
    token:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 输出包含分类令牌和序列分割令牌：
- en: '[PRE71]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The program will now process the data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将处理数据。
- en: Processing the data
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'We need to determine a fixed maximum length and process the data for the model.
    The sentences in the datasets are short. But, to make sure of this, the program
    sets the maximum length of a sequence to `128`, and the sequences are padded:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确定一个固定的最大长度并为模型处理数据。数据集中的句子很短。但为了确保这一点，程序将序列的最大长度设置为`128`，然后进行填充：
- en: '[PRE72]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The sequences have been processed and now the program creates the attention
    masks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 序列已经被处理，现在程序创建了注意力掩码。
- en: Creating attention masks
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建注意力掩码
- en: Now comes a tricky part of the process. We padded the sequences in the previous
    cell. But we want to prevent the model from performing attention to those padded
    tokens!
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就是过程中的一个棘手部分了。在前一个单元格中，我们添加了填充的序列。但我们想阻止模型对这些填充的标记执行注意力！
- en: 'The idea is to apply a mask with a value of `1` for each token, which 0s will
    follow for padding:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是为每个标记应用一个值为`1`的掩码，0将用于填充：
- en: '[PRE73]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The program will now split the data.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将分割数据。
- en: Splitting the data into training and validation sets
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据分割成训练集和验证集
- en: 'The program now performs the standard process of splitting the data into training
    and validation sets:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在执行标准的数据分割过程，将数据分成训练集和验证集：
- en: '[PRE74]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The data is ready to be trained, but it still needs to be adapted to torch.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经准备好训练，但仍需调整为torch。
- en: Converting all the data into torch tensors
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有数据转换为torch张量
- en: 'The fine-tuning model uses torch tensors. The program must convert the data
    into torch tensors:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型使用torch张量。程序必须将数据转换为torch张量：
- en: '[PRE75]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The conversion is over. Now we need to create an iterator.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 转换结束了。现在我们需要创建一个迭代器。
- en: Selecting a batch size and creating an iterator
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择批处理大小并创建迭代器
- en: In this cell, the program selects a batch size and creates an iterator. The
    iterator is a clever way of avoiding a loop that would load all the data in memory.
    The iterator, coupled with the torch `DataLoader`, can batch train massive datasets
    without crashing the machine’s memory.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个单元格中，程序会选择批处理大小并创建一个迭代器。这个迭代器是避免加载所有数据到内存中并且配合torch的`DataLoader`巧妙的方式，可以批量训练大型数据集而不会使机器内存崩溃。
- en: 'In this model, the batch size is `32`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，批处理大小是`32`：
- en: '[PRE76]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The data has been processed and is all set. The program can now load and configure
    the BERT model.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经被处理并且准备就绪。程序现在可以加载并配置BERT模型。
- en: BERT model configuration
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT模型配置
- en: 'The program now initializes a BERT uncased configuration:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '程序现在初始化了一个BERT小写配置:'
- en: '[PRE77]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The output displays the main Hugging Face parameters similar to the following
    (the library is often updated):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出包含主要的Hugging Face参数，类似于以下内容（该库经常更新）：
- en: '[PRE78]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Let’s go through these main parameters:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 来看看这些主要参数：
- en: '`attention_probs_dropout_prob`: `0.1` applies a `0.1` dropout ratio to the
    attention probabilities.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob`：`0.1`将`0.1`的丢失率应用于注意力概率。'
- en: '`hidden_act`: `"gelu"` is a non-linear activation function in the encoder.
    It is a *Gaussian Error Linear Units* activation function. The input is weighted
    by its magnitude, which makes it non-linear.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act`: `"gelu"` 是编码器中的非线性激活函数。这是一个*高斯误差线性单元*的激活函数。输入按其大小加权，这使其非线性。'
- en: '`hidden_dropout_prob`: `0.1` is the dropout probability applied to the fully
    connected layers. Full connections can be found in the embeddings, encoder, and
    pooler layers. The output is not always a good reflection of the content of a
    sequence. Pooling the sequence of hidden states improves the output sequence.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob`: `0.1` 是应用于全连接层的dropout概率。在嵌入、编码器和池层中都可以找到全连接。输出并不总是对序列内容的良好反映。池化隐藏状态序列有助于改善输出序列。'
- en: '`hidden_size`: `768` is the dimension of the encoded layers and also the pooler
    layer.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`: `768` 是编码层和池层的维度。'
- en: '`initializer_range`: `0.02` is the standard deviation value when initializing
    the weight matrices.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range`: `0.02` 是初始化权重矩阵的标准差值。'
- en: '`intermediate_size`: `3072` is the dimension of the feed-forward layer of the
    encoder.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size`: `3072` 是编码器的前馈层的维度。'
- en: '`layer_norm_eps`: `1e-12` is the epsilon value for layer normalization layers.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps`: `1e-12` 是层归一化层的 epsilon 值。'
- en: '`max_position_embeddings`: `512` is the maximum length the model uses.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings`: `512` 是模型使用的最大长度。'
- en: '`model_type`: `"bert"` is the name of the model.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_type`: `"bert"` 是模型的名称。'
- en: '`num_attention_heads`: `12` is the number of heads.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`: `12` 是头的数量。'
- en: '`num_hidden_layers`: `12` is the number of layers.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`: `12` 是层数的数量。'
- en: '`pad_token_id`: `0` is the ID of the padding token to avoid training padding
    tokens.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id`: `0` 是填充标记的ID，以避免训练填充标记。'
- en: '`type_vocab_size`: `2` is the size of the `token_type_ids`, which identify
    the sequences. For example, “the `dog[SEP]` The `cat.[SEP]`" can be represented
    with token IDs `[0,0,0, 1,1,1]`.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size`: `2` 是`token_type_ids`的大小，它标识序列。例如，“the `dog[SEP]` The `cat.[SEP]`"可以用
    token IDs `[0,0,0, 1,1,1]`表示。'
- en: '`vocab_size`: `30522` is the number of different tokens used by the model to
    represent the `input_ids`.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`: `30522` 是模型用于表示`input_ids`的不同标记的数量。'
- en: With these parameters in mind, we can load the pretrained model.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些参数，我们现在可以加载预训练模型。
- en: Loading the Hugging Face BERT uncased base model
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 Hugging Face BERT uncased 基础模型
- en: 'The program now loads the pretrained BERT model:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在加载了预训练的BERT模型：
- en: '[PRE79]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We have defined the model, defined parallel processing, and sent the model to
    the device. For more explanations, see *Appendix II*, *Hardware Constraints for
    Transformer Models*.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了模型，定义了并行处理，并且将模型发送到设备。更多解释，请参见*附录II*，*Transformer模型的硬件限制*。
- en: 'This pretrained model can be trained further if necessary. It is interesting
    to explore the architecture in detail to visualize the parameters of each sublayer,
    as shown in the following excerpt:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，该预训练模型可以进一步训练。通过详细探索架构，可以可视化每个子层的参数，就像下面的摘录所示：
- en: '[PRE80]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Let’s now go through the main parameters of the optimizer.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下优化器的主要参数。
- en: Optimizer grouped parameters
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器分组参数
- en: The program will now initialize the optimizer for the model’s parameters. Fine-tuning
    a model begins with initializing the pretrained model parameter values (not their
    names).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将初始化模型参数的优化器。微调模型的起始点是初始化预训练模型的参数值（而不是它们的名称）。
- en: The parameters of the optimizer include a weight decay rate to avoid overfitting,
    and some parameters are filtered.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的参数包括权重衰减率以避免过拟合，还有一些参数被筛选了出来。
- en: 'The goal is to prepare the model’s parameters for the training loop:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是为了为训练循环准备模型的参数：
- en: '[PRE81]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The parameters have been prepared and cleaned up. They are ready for the training
    loop.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 参数已经被准备并清理干净。它们已经为训练循环做好了准备。
- en: The hyperparameters for the training loop
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环的超参数
- en: The hyperparameters for the training loop are critical, though they seem innocuous.
    Adam will activate weight decay and also go through a warm-up phase, for example.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管似乎无害，但训练循环的超参数至关重要。例如，Adam会激活权重衰减，并且还会经历一个温和的阶段。
- en: The learning rate (`lr`) and warm-up rate (`warmup`) should be set to a very
    small value early in the optimization phase and gradually increase after a certain
    number of iterations. This avoids large gradients and overshooting the optimization
    goals.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（`lr`）和温和率（`warmup`）在优化阶段的早期应该设为一个很小的值，并且在一定数量的迭代后逐渐增加。这可以避免大梯度和超出优化目标。
- en: Some researchers argue that the gradients at the output level of the sub-layers
    before layer normalization do not require a warm-up rate. Solving this problem
    requires many experimental runs.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimizer is a BERT version of Adam called `BertAdam`:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The program adds an accuracy measurement function to compare the predictions
    to the labels:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The data is ready. The parameters are ready. It’s time to activate the training
    loop!
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: The training loop
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training loop follows standard learning processes. The number of epochs
    is set to `4`, and measurement for loss and accuracy will be plotted. The training
    loop uses the `dataloader` to load and train batches. The training process is
    measured and evaluated.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'The code starts by initializing the `train_loss_set`, which will store the
    loss and accuracy, which will be plotted. It starts training its epochs and runs
    a standard training loop, as shown in the following excerpt:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output displays the information for each `epoch` with the `trange` wrapper,
    `for _ in trange(epochs, desc="Epoch")`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Transformer models are evolving very quickly, and deprecation messages and even
    errors might occur. Hugging Face is no exception to this, and we must update our
    code accordingly when this happens.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained. We can now display the training evaluation.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Training evaluation
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss and accuracy values were stored in `train_loss_set` as defined at the
    beginning of the training loop.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now plots the measurements:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is a graph that shows that the training process went well and was
    efficient:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_06.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Training loss per batch'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: The model has been fine-tuned. We can now run predictions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating using the holdout dataset
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BERT downstream model was trained with the `in_domain_train.tsv` dataset.
    The program will now make predictions using the holdout (testing) dataset in the
    `out_of_domain_dev.tsv` file. The goal is to predict whether the sentence is grammatically
    correct.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'The following excerpt of the code shows that the data preparation process applied
    to the training data is repeated in the part of the code for the holdout dataset:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The program then runs batch predictions using the `dataloader`:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The logits and labels of the predictions are moved to the CPU:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The predictions and their true labels are stored:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: The program can now evaluate the predictions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating using the Matthews Correlation Coefficient
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Matthews Correlation Coefficient** (**MCC**) was initially designed to
    measure the quality of binary classifications and can be modified to be a multi-class
    correlation coefficient. A two-class classification can be made with four probabilities
    at each prediction:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: TP = True Positive
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TN = True Negative
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP = False Positive
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FN = False Negative
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brian W. Matthews*, a biochemist, designed it in 1975, inspired by his predecessors’
    *phi* function. Since then, it has evolved into various formats such as the following
    one:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_03_005.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: The value produced by MCC is between `-1` and `+1`. `+1` is the maximum positive
    value of a prediction. `-1` is an inverse prediction. `0` is an average random
    prediction.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: MCC产生的值在`-1`和`+1`之间。 `+1`是预测的最大正值。 `-1`是反向预测。 `0`是平均随机预测。
- en: GLUE evaluates *Linguistic Acceptability* with MCC.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE用MCC评估*语言可接受性*。
- en: 'MCC is imported from `sklearn.metrics`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: MCC是导入自`sklearn.metrics`的：
- en: '[PRE91]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'A set of predictions is created:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一组预测：
- en: '[PRE92]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The MCC value is calculated and stored in `matthews_set`:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: MCC值被计算并存储在`matthews_set`中：
- en: '[PRE93]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: You may see messages due to library and module version changes. The final score
    will be based on the entire test set, but let’s look at the scores on the individual
    batches to get a sense of the variability in the metric between batches.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 由于库和模块版本更改，您可能会看到一些消息。 最终得分将基于整个测试集，但让我们看看各个批次的得分，以了解批次之间指标的变化。
- en: The scores of individual batches
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 各个批次的得分
- en: 'Let’s view the scores of the individual batches:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看各个批次的得分：
- en: '[PRE94]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output produces MCC values between `-1` and `+1` as expected:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 输出产生了预期的`-1`和`+1`之间的MCC值：
- en: '[PRE95]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Almost all the MCC values are positive, which is good news. Let’s see what the
    evaluation is for the whole dataset.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的MCC值都是正值，这是个好消息。 让我们看看整个数据集的评估如何。
- en: Matthews evaluation for the whole dataset
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对整个数据集进行Matthews评估
- en: The MCC is a practical way to evaluate a classification model.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: MCC是评估分类模型的一种实用方法。
- en: 'The program will now aggregate the true values for the whole dataset:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序现在将为整个数据集聚合真实值：
- en: '[PRE96]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'MCC produces a correlation value between `–1` and `+1`. `0` is an average prediction,
    `-1` is an inverse one, and `1` is perfect. In this case, the output confirms
    that the MCC is positive, which shows that there is a correlation between this
    model and dataset:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: MCC产生`-1`和`+1`之间的相关值。 `0`是平均预测，`-1`是反向预测，`1`是完美预测。 在这种情况下，输出证实MCC是正值，表明模型和数据集之间存在关联：
- en: '[PRE97]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: On this final positive evaluation of the fine-tuning of the BERT model, we have
    an overall view of the BERT training framework.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT模型的微调最终积极评估中，我们对BERT训练框架有了整体的认识。
- en: Summary
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: BERT brings bidirectional attention to transformers. Predicting sequences from
    left to right and masking the future tokens to train a model has serious limitations.
    If the masked sequence contains the meaning we are looking for, the model will
    produce errors. BERT attends to all of the tokens of a sequence at the same time.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: BERT为transformers引入了双向关注。 从左到右预测序列并屏蔽未来的标记以训练模型具有严格限制。 如果屏蔽的序列包含我们正在寻找的意义，模型将产生错误。
    BERT同时关注序列的所有标记。
- en: We explored the architecture of BERT, which only uses the encoder stack of transformers.
    BERT was designed as a two-step framework. The first step of the framework is
    to pretrain a model. The second step is to fine-tune the model. We built a fine-tuning
    BERT model for an *Acceptability Judgment* downstream task. The fine-tuning process
    went through all phases of the process. First, we loaded the dataset and loaded
    the necessary pretrained modules of the model. Then the model was trained, and
    its performance was measured.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了BERT的架构，它只使用transformers的编码器堆栈。 BERT被设计为一个两步框架。 框架的第一步是预训练一个模型。 第二步是微调模型。
    我们为*可接受性判断*下游任务构建了一个微调的BERT模型。 微调过程经历了所有阶段的过程。 首先，我们加载了数据集和加载了模型的必要预训练模块。 然后训练模型，并测量其性能。
- en: Fine-tuning a pretrained model takes fewer machine resources than training downstream
    tasks from scratch. Fine-tuned models can perform a variety of tasks. BERT proves
    that we can pretrain a model on two tasks only, which is remarkable in itself.
    But producing a multitask fine-tuned model based on the trained parameters of
    the BERT pretrained model is extraordinary.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个预训练模型进行微调所需的机器资源比从头开始训练下游任务要少。 细调模型可以执行各种任务。 BERT证明我们只需对模型进行两项训练预处理就能实现这一点，这本身就是了不起的。但是基于BERT预训练模型的训练参数产生多任务微调模型是异常的。
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, shows
    that OpenAI has reached a zero-shot level with little to no fine-tuning.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '*第七章*，*具有GPT-3引擎的超人变形金刚的兴起*，表明OpenAI已经达到了零调校水平。'
- en: In this chapter, we fine-tuned a BERT model. In the next chapter, *Chapter 4*,
    *Pretraining a RoBERTa Model from Scratch*, we will dig deeper into the BERT framework
    and build a pretrained BERT-like model from scratch.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT stands for Bidirectional Encoder Representations from Transformers. (True/False)
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT is a two-step framework. *Step 1* is pretraining. *Step 2* is fine-tuning.
    (True/False)
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model implies training parameters from scratch. (True/False)
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT only pretrains using all downstream tasks. (True/False)
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains with **Masked** **Language Modeling** (**MLM**). (True/False)
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains with **Next Sentence Predictions** (**NSP**). (True/False)
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT pretrains mathematical functions. (True/False)
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A question-answer task is a downstream task. (True/False)
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A BERT pretraining model does not require tokenization. (True/False)
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model takes less time than pretraining. (True/False)
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    Is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, and *Kristina Toutanova*, *2018*,
    *BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Warstadt*, *Amanpreet Singh*, and *Samuel R. Bowman*, 2018, *Neural Network
    Acceptability Judgments*: [https://arxiv.org/abs/1805.12471](https://arxiv.org/abs/1805.12471)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Corpus of Linguistic Acceptability** (**CoLA**): [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation on Hugging Face models:'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html)'
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/roberta.html](https://huggingface.co/transformers/model_doc/roberta.html)'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html)'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
