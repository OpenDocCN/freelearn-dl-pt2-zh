["```py\n    from torch import cuda\n    device = 'cuda' if cuda.is_available() else 'cpu'\n    ```", "```py\n    from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n    model_path= 'distilbert-base-uncased'\n    tokenizer = DistilBertTokenizerFast.from_pre-trained(model_path)\n    model = \\ DistilBertForSequenceClassification.from_pre-trained(model_path, id2label={0:\"NEG\", 1:\"POS\"}, label2id={\"NEG\":0, \"POS\":1})\n    ```", "```py\n    config = AutoConfig.from_pre-trained(....)\n    SequenceClassification.from_pre-trained(.... config=config)\n    ```", "```py\n    from datasets import load_dataset\n    imdb_train= load_dataset('imdb', split=\"train\")\n    imdb_test= load_dataset('imdb', split=\"test[:6250]+test[-6250:]\")\n    imdb_val= \\\n    load_dataset('imdb', split=\"test[6250:12500]+test[-12500:-6250]\")\n    ```", "```py\n    >>> imdb_train.shape, imdb_test.shape, imdb_val.shape\n    ((25000, 2), (12500, 2), (12500, 2))\n    ```", "```py\n    imdb_train= load_dataset('imdb', split=\"train[:2000]+train[-2000:]\")\n    imdb_test= load_dataset('imdb', split=\"test[:500]+test[-500:]\")\n    imdb_val= load_dataset('imdb', split=\"test[500:1000]+test[-1000:-500]\")\n    ```", "```py\n    enc_train = imdb_train.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000) \n    enc_test =  imdb_test.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000) \n    enc_val =   imdb_val.map(lambda e: tokenizer( e['text'], padding=True, truncation=True), batched=True, batch_size=1000)\n    ```", "```py\n    import pandas as pd\n    pd.DataFrame(enc_train)\n    ```", "```py\n    TrainingArguments?\n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(\n        output_dir='./MyIMDBModel', \n        do_train=True,\n        do_eval=True,\n        num_train_epochs=3,              \n        per_device_train_batch_size=32,  \n        per_device_eval_batch_size=64,\n        warmup_steps=100,                \n        weight_decay=0.01,\n        logging_strategy='steps',               \n        logging_dir='./logs',            \n        logging_steps=200,\n        evaluation_strategy= 'steps',\n             fp16= cuda.is_available(),\n        load_best_model_at_end=True\n    )\n    ```", "```py\n    from sklearn.metrics import accuracy_score, Precision_Recall_fscore_support\n    def compute_metrics(pred):\n        labels = pred.label_ids\n        preds = pred.predictions.argmax(-1)\n        Precision, Recall, f1, _ = \\                        \n        Precision_Recall_fscore_support(labels, preds, average='macro')\n        acc = accuracy_score(labels, preds)\n        return {\n            'Accuracy': acc,\n            'F1': f1,\n            'Precision': Precision,\n            'Recall': Recall\n        }\n    ```", "```py\n    trainer = Trainer(\n        model=model,                     \n        args=training_args,                 \n        train_dataset=enc_train,         \n        eval_dataset=enc_val,            \n        compute_metrics= compute_metrics\n    )\n    ```", "```py\n    results=trainer.train()\n    ```", "```py\n    >>> q=[trainer.evaluate(eval_dataset=data) for data in [enc_train, enc_val, enc_test]]\n    >>> pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]\n    ```", "```py\n    %reload_ext tensorboard\n    %tensorboard --logdir logs\n    ```", "```py\n    def get_prediction(text):\n        inputs = tokenizer(text, padding=True,truncation=True,\n        max_length=250, return_tensors=\"pt\").to(device)\n        outputs = \\ model(inputs[\"input_ids\"].to(device),inputs[\"attention_mask\"].to(device))\n        probs = outputs[0].softmax(1)\n        return probs, probs.argmax() \n    ```", "```py\n    >>> text = \"I didn't like the movie it bored me \"\n    >>> get_prediction(text)[1].item()\n    0 \n    ```", "```py\n    model_save_path = \"MyBestIMDBModel\"\n    trainer.save_model(model_save_path)\n    tokenizer.save_pre-trained(model_save_path)\n    ```", "```py\n    >>> from transformers import pipeline, \\ DistilBertForSequenceClassification, DistilBertTokenizerFast\n    >>> model = \\ DistilBertForSequenceClassification.from_pre-trained(\"MyBestIMDBModel\")\n    >>> tokenizer= \\ DistilBertTokenizerFast.from_pre-trained(\"MyBestIMDBModel\")\n    >>> nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n    >>> nlp(\"the movie was very impressive\")\n    Out:  [{'label': 'POS', 'score': 0.9621992707252502}]\n    >>> nlp(\"the text of the picture was very poor\")\n    Out:  [{'label': 'NEG', 'score': 0.9938313961029053}]\n    ```", "```py\n    from transformers import DistilBertForSequenceClassification\n    model = DistilBertForSequenceClassification.from_pre-trained('distilbert-base-uncased')\n    ```", "```py\n    model.train()\n    ```", "```py\n    from transformers import DistilBertTokenizerFast\n    tokenizer = DistilBertTokenizerFast.from_pre-trained('bert-base-uncased')\n    ```", "```py\n    from transformers import AdamW\n    optimizer = AdamW(model.parameters(), lr=1e-3)\n    ```", "```py\n    from torch.nn import functional\n    labels = torch.tensor([1,0,1])\n    outputs = model(input_ids, attention_mask=attention_mask)\n    loss = functional.cross_entropy(outputs.logits, labels)\n    loss.backward()\n    optimizer.step()\n    loss\n    Output: tensor(0.6101, grad_fn=<NllLossBackward>)\n    ```", "```py\n    from torch.utils.data import Dataset\n    class MyDataset(Dataset):\n        def __init__(self, encodings, labels):\n            self.encodings = encodings\n            self.labels = labels\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n            item['labels'] = torch.tensor(self.labels[idx])\n            return item\n        def __len__(self):\n            return len(self.labels) \n    ```", "```py\n    import datasets\n    from datasets import load_dataset\n    sst2= load_dataset(\"glue\",\"sst2\")\n    from datasets import load_metric\n    metric = load_metric(\"glue\", \"sst2\")\n    ```", "```py\n    texts=sst2['train']['sentence']\n    labels=sst2['train']['label']\n    val_texts=sst2['validation']['sentence']\n    val_labels=sst2['validation']['label']\n    ```", "```py\n    train_dataset= MyDataset(tokenizer(texts, truncation=True, padding=True), labels)\n    val_dataset=  MyDataset(tokenizer(val_texts, truncation=True, padding=True), val_labels)\n    ```", "```py\n    from torch.utils.data import DataLoader\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader =  DataLoader(val_dataset, batch_size=16, shuffle=True)\n    ```", "```py\n    from transformers import  AdamW \n    device = \\\n    torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr=1e-3)\n    ```", "```py\n    for epoch in range(3):\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = \\\n    model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs[0]\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = \\\n    model(input_ids, attention_mask=attention_mask, labels=labels)\n            predictions=outputs.logits.argmax(dim=-1)  \n            metric.add_batch(\n                    predictions=predictions,\n                    references=batch[\"labels\"],\n                )\n        eval_metric = metric.compute()\n        print(f\"epoch {epoch}: {eval_metric}\")\n    OUTPUT:\n    epoch 0: {'accuracy': 0.9048165137614679} \n    epoch 1: {'accuracy': 0.8944954128440367} \n    epoch 2: {'accuracy': 0.9094036697247706}\n    ```", "```py\n    !wget https://raw.githubusercontent.com/savasy/TurkishTextClassification/master/TTC4900.csv\n    ```", "```py\n    import pandas as pd\n    data= pd.read_csv(\"TTC4900.csv\")\n    data=data.sample(frac=1.0, random_state=42)\n    ```", "```py\n    labels=[\"teknoloji\",\"ekonomi\",\"saglik\",\"siyaset\",\"kultur\",\"spor\",\"dunya\"]\n    NUM_LABELS= len(labels)\n    id2label={i:l for i,l in enumerate(labels)}\n    label2id={l:i for i,l in enumerate(labels)}\n    data[\"labels\"]=data.category.map(lambda x: label2id[x.strip()])\n    data.head()\n    ```", "```py\n    data.category.value_counts().plot(kind='pie')\n    ```", "```py\n    >>> model\n    ```", "```py\n    (classifier): Linear(in_features=768, out_features=7, bias=True)\n    ```", "```py\n    from transformers import BertTokenizerFast\n    tokenizer = BertTokenizerFast.from_pre-trained(\"dbmdz/bert-base-turkish-uncased\", max_length=512)\n     from transformers import BertForSequenceClassification\n    model = BertForSequenceClassification.from_pre-trained(\"dbmdz/bert-base-turkish-uncased\", num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n    model.to(device)\n    ```", "```py\n    SIZE= data.shape[0]\n    ## sentences\n    train_texts= list(data.text[:SIZE//2])\n    val_texts=   list(data.text[SIZE//2:(3*SIZE)//4 ])\n    test_texts=  list(data.text[(3*SIZE)//4:])\n    ## labels\n    train_labels= list(data.labels[:SIZE//2])\n    val_labels=   list(data.labels[SIZE//2:(3*SIZE)//4])\n    test_labels=  list(data.labels[(3*SIZE)//4:])\n    ## check the size\n    len(train_texts), len(val_texts), len(test_texts)\n    (2450, 1225, 1225)\n    ```", "```py\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n    val_encodings  = tokenizer(val_texts, truncation=True,\n    padding=True)\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n    ```", "```py\n    train_dataset = MyDataset(train_encodings, train_labels)\n    val_dataset = MyDataset(val_encodings, val_labels)\n    test_dataset = MyDataset(test_encodings, test_labels)\n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(\n        output_dir='./TTC4900Model', \n        do_train=True,\n        do_eval=True,\n        num_train_epochs=3,              \n        per_device_train_batch_size=16,  \n        per_device_eval_batch_size=32,\n        warmup_steps=100,                \n        weight_decay=0.01,\n        logging_strategy='steps',                 \n        logging_dir='./multi-class-logs',            \n        logging_steps=50,\n        evaluation_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"epoch\", \n        fp16=True,\n        load_best_model_at_end=True\n    ) \n    ```", "```py\n    trainer = Trainer(\n        model=model,                     \n        args=training_args,                 \n        train_dataset=train_dataset,         \n        eval_dataset=val_dataset,            \n        compute_metrics= compute_metrics\n    )\n    ```", "```py\n    trainer.train()\n    ```", "```py\n    q=[trainer.evaluate(eval_dataset=data) for data in [train_dataset, val_dataset, test_dataset]]\n    pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]\n    ```", "```py\n    %load_ext tensorboard\n    %tensorboard --logdir multi-class-logs/\n    ```", "```py\n    def predict(text):\n        inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model(**inputs)\n        probs = outputs[0].softmax(1)\n        return probs, probs.argmax(),model.config.id2label[probs.argmax().item()]\n    ```", "```py\n    text = \"Fenerbahçeli futbolcular kısa paslarla hazırlık çalışması yaptılar\"\n    predict(text)\n    (tensor([[5.6183e-04, 4.9046e-04, 5.1385e-04, 9.9414e-04, 3.4417e-04, 9.9669e-01, 4.0617e-04]], device='cuda:0', grad_fn=<SoftmaxBackward>), tensor(5, device='cuda:0'), 'spor') \n    ```", "```py\n    model_path = \"turkish-text-classification-model\"\n    trainer.save_model(model_path)\n    tokenizer.save_pre-trained(model_path)\n    ```", "```py\n    model_path = \"turkish-text-classification-model\"\n    from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n    model = BertForSequenceClassification.from_pre-trained(model_path)\n    tokenizer= BertTokenizerFast.from_pre-trained(model_path)\n    nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n    ```", "```py\n    >>> nlp(\"Sinemada hangi filmler oynuyor bugün\")\n    [{'label': 'kultur', 'score': 0.9930670261383057}]\n    >>> nlp(\"Dolar ve Euro bugün yurtiçi piyasalarda yükseldi\")\n    [{'label': 'ekonomi', 'score': 0.9927696585655212}]\n    >>> nlp(\"Bayern Münih ile Barcelona bugün karşı karşıya geliyor. Maçı İngiliz hakem James Watts yönetecek!\")\n    [{'label': 'spor', 'score': 0.9975664019584656}]\n    ```", "```py\nfrom transformers import DistilBertConfig, DistilBertTokenizerFast, DistilBertForSequenceClassification\nmodel_path='distilbert-base-uncased'\nconfig = DistilBertConfig.from_pre-trained(model_path, num_labels=1)\ntokenizer = DistilBertTokenizerFast.from_pre-trained(model_path)\nmodel = \\\nDistilBertForSequenceClassification.from_pre-trained(model_path, config=config)\n```", "```py\n    import datasets\n    from datasets import load_dataset\n    stsb_train= load_dataset('glue','stsb', split=\"train\")\n    stsb_validation = load_dataset('glue','stsb', split=\"validation\")\n    stsb_validation=stsb_validation.shuffle(seed=42)\n    stsb_val= datasets.Dataset.from_dict(stsb_validation[:750])\n    stsb_test= datasets.Dataset.from_dict(stsb_validation[750:])\n    ```", "```py\n    pd.DataFrame(stsb_train)\n    ```", "```py\n    stsb_train.shape, stsb_val.shape, stsb_test.shape\n    ((5749, 4), (750, 4), (750, 4))\n    ```", "```py\n    enc_train = stsb_train.map(lambda e: tokenizer( e['sentence1'],e['sentence2'], padding=True, truncation=True), batched=True, batch_size=1000) \n    enc_val =   stsb_val.map(lambda e: tokenizer( e['sentence1'],e['sentence2'], padding=True, truncation=True), batched=True, batch_size=1000) \n    enc_test =  stsb_test.map(lambda e: tokenizer( e['sentence1'],e['sentence2'], padding=True, truncation=True), batched=True, batch_size=1000)\n    ```", "```py\n    pd.DataFrame(enc_train)\n    ```", "```py\n    from transformers import TrainingArguments, Trainer\n    training_args = TrainingArguments(\n        output_dir='./stsb-model', \n        do_train=True,\n        do_eval=True,\n        num_train_epochs=3,              \n        per_device_train_batch_size=32,  \n        per_device_eval_batch_size=64,\n        warmup_steps=100,                \n        weight_decay=0.01,\n        logging_strategy='steps',                \n        logging_dir='./logs',            \n        logging_steps=50,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"epoch\",\n        fp16=True,\n        load_best_model_at_end=True\n    )\n    ```", "```py\n    import numpy as np\n    from scipy.stats import pearsonr\n    from scipy.stats import spearmanr\n    def compute_metrics(pred):\n        preds = np.squeeze(pred.predictions) \n        return {\"MSE\": ((preds - pred.label_ids) ** 2).mean().item(),\n                \"RMSE\": (np.sqrt ((  (preds - pred.label_ids) ** 2).mean())).item(),\n                \"MAE\": (np.abs(preds - pred.label_ids)).mean().item(),\n         \"Pearson\" : pearsonr(preds,pred.label_ids)[0],\n         \"Spearman's Rank\":spearmanr(preds,pred.label_ids)[0]\n                }\n    ```", "```py\n    trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=enc_train,\n            eval_dataset=enc_val,\n            compute_metrics=compute_metrics,\n            tokenizer=tokenizer\n        )\n    ```", "```py\n    train_result = trainer.train()\n    ```", "```py\n    q=[trainer.evaluate(eval_dataset=data) for data in [enc_train, enc_val, enc_test]]\n    pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]\n    ```", "```py\n    s1,s2=\"A plane is taking off.\",\"An air plane is taking off.\"\n    encoding = tokenizer(s1,s2, return_tensors='pt', padding=True, truncation=True, max_length=512)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask)\n    outputs.logits.item()\n    OUTPUT: 4.033723831176758\n    ```", "```py\n    s1,s2=\"The men are playing soccer.\",\"A man is riding a motorcycle.\"\n    encoding = tokenizer(\"hey how are you there\",\"hey how are you\", return_tensors='pt', padding=True, truncation=True, max_length=512)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask)\n    outputs.logits.item()\n    OUTPUT: 2.3579328060150146\n    ```", "```py\n    model_path = \"sentence-pair-regression-model\"\n    trainer.save_model(model_path)\n    tokenizer.save_pre-trained(model_path)\n    ```", "```py\nexport TASK_NAME= \"My-Task-Name\" \npython run_glue.py \\  \n --model_name_or_path bert-base-cased \\\n --task_name $TASK_NAME \\\n --do_train \\  --do_eval \\\n --max_seq_length 128 \\ \n --per_device_train_batch_size 32 \\\n --learning_rate 2e-5 \\  \n --num_train_epochs 3 \\\n --output_dir /tmp/$TASK_NAME/\n```"]