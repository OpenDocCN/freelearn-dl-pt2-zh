- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Learning Best Practices for Model Evaluation and Hyperparameter Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习模型评估和超参数调整的最佳实践
- en: 'In the previous chapters, we learned about the essential machine learning algorithms
    for classification and how to get our data into shape before we feed it into those
    algorithms. Now, it’s time to learn about the best practices of building good
    machine learning models by fine-tuning the algorithms and evaluating the performance
    of the models. In this chapter, we will learn how to do the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了分类的基本机器学习算法，以及在将数据输入这些算法之前如何整理数据。现在，是时候学习通过微调算法和评估模型性能来构建优秀机器学习模型的最佳实践了。在本章中，我们将学习如何执行以下操作：
- en: Assess the performance of machine learning models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估机器学习模型的性能
- en: Diagnose the common problems of machine learning algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断机器学习算法的常见问题
- en: Fine-tune machine learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调机器学习模型
- en: Evaluate predictive models using different performance metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同性能指标评估预测模型的性能
- en: Streamlining workflows with pipelines
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道流程优化工作流程
- en: When we applied different preprocessing techniques in the previous chapters,
    such as standardization for feature scaling in *Chapter 4*, *Building Good Training
    Datasets – Data Preprocessing*, or principal component analysis for data compression
    in *Chapter 5*, *Compressing Data via Dimensionality Reduction*, you learned that
    we have to reuse the parameters that were obtained during the fitting of the training
    data to scale and compress any new data, such as the examples in the separate
    test dataset. In this section, you will learn about an extremely handy tool, the
    `Pipeline` class in scikit-learn. It allows us to fit a model including an arbitrary
    number of transformation steps and apply it to make predictions about new data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前几章中应用不同的预处理技术，例如特征缩放的标准化在 *第4章*，“构建良好的训练数据集 - 数据预处理” 中，或数据压缩的主成分分析在 *第5章*，“通过降维压缩数据”
    中，您学到我们必须重用在训练数据拟合期间获得的参数来缩放和压缩任何新数据，例如单独测试数据集中的示例。在本节中，您将学习到一个非常方便的工具，即`Pipeline`类在scikit-learn中。它允许我们拟合一个包含任意数量转换步骤的模型，并将其应用于对新数据进行预测。
- en: Loading the Breast Cancer Wisconsin dataset
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载乳腺癌威斯康星数据集
- en: In this chapter, we will be working with the Breast Cancer Wisconsin dataset,
    which contains 569 examples of malignant and benign tumor cells. The first two
    columns in the dataset store the unique ID numbers of the examples and the corresponding
    diagnoses (`M` = malignant, `B` = benign), respectively. Columns 3-32 contain
    30 real-valued features that have been computed from digitized images of the cell
    nuclei, which can be used to build a model to predict whether a tumor is benign
    or malignant. The Breast Cancer Wisconsin dataset has been deposited in the UCI
    Machine Learning Repository, and more detailed information about this dataset
    can be found at [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用乳腺癌威斯康星数据集，该数据集包含569个恶性和良性肿瘤细胞的示例。数据集中的前两列存储示例的唯一ID编号和相应的诊断结果（`M`
    = 恶性，`B` = 良性）。列3-32包含从细胞核数字化图像计算出的30个实值特征，可用于构建模型以预测肿瘤是良性还是恶性。乳腺癌威斯康星数据集已存放在UCI机器学习库中，有关该数据集的更详细信息可在
    [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
    找到。
- en: '**Obtaining the Breast Cancer Wisconsin dataset**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取乳腺癌威斯康星数据集**'
- en: 'You can find a copy of the dataset (and all other datasets used in this book)
    in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)
    is temporarily unavailable. For instance, to load the dataset from a local directory,
    you can replace the following lines:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的代码包中找到数据集的副本（以及本书中使用的所有其他数据集），如果您离线工作或者UCI服务器在 [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)
    暂时不可用时，您可以使用它。例如，要从本地目录加载数据集，您可以替换以下行：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'with these:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下内容：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this section, we will read in the dataset and split it into training and
    test datasets in three simple steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在三个简单步骤中读取数据集并将其分为训练集和测试集：
- en: 'We will start by reading in the dataset directly from the UCI website using
    pandas:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从 UCI 网站直接使用 pandas 读取数据集：
- en: '[PRE2]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will assign the 30 features to a NumPy array, `X`. Using a `LabelEncoder`
    object, we will transform the class labels from their original string representation
    (`''M''` and `''B''`) into integers:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将把这 30 个特征分配给一个 NumPy 数组 `X`。使用 `LabelEncoder` 对象，我们将类标签从其原始字符串表示（`'M'`
    和 `'B'`）转换为整数：
- en: '[PRE3]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After encoding the class labels (diagnosis) in an array, `y`, the malignant
    tumors are now represented as class `1`, and the benign tumors are represented
    as class `0`, respectively. We can double-check this mapping by calling the `transform`
    method of the fitted `LabelEncoder` on two dummy class labels:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将类标签（诊断）编码为数组 `y` 后，恶性肿瘤现在表示为类 `1`，良性肿瘤表示为类 `0`。我们可以通过在两个虚拟类标签上调用已拟合的 `LabelEncoder`
    的 `transform` 方法来双重检查此映射：
- en: '[PRE4]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before we construct our first model pipeline in the following subsection, let’s
    divide the dataset into a separate training dataset (80 percent of the data) and
    a separate test dataset (20 percent of the data):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一小节构建我们的第一个模型管道之前，让我们将数据集分成一个单独的训练数据集（数据的 80%）和一个单独的测试数据集（数据的 20%）：
- en: '[PRE5]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Combining transformers and estimators in a pipeline
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将转换器和估计器组合成管道
- en: In the previous chapter, you learned that many learning algorithms require input
    features on the same scale for optimal performance. Since the features in the
    Breast Cancer Wisconsin dataset are measured on various different scales, we will
    standardize the columns in the Breast Cancer Wisconsin dataset before we feed
    them to a linear classifier, such as logistic regression. Furthermore, let’s assume
    that we want to compress our data from the initial 30 dimensions into a lower
    two-dimensional subspace via **principal component analysis** (**PCA**), a feature
    extraction technique for dimensionality reduction that was introduced in *Chapter
    5*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您学习到许多学习算法需要输入特征在相同的尺度上才能获得最佳性能。由于 Breast Cancer Wisconsin 数据集中的特征在不同的尺度上测量，因此在将它们提供给线性分类器（如
    logistic 回归）之前，我们将标准化 Breast Cancer Wisconsin 数据集中的列。此外，假设我们希望通过 **主成分分析**（**PCA**），这是一种介绍在
    *第 5 章* 中的用于降维的特征提取技术，将我们的数据从初始的 30 维压缩到一个较低的二维子空间。
- en: 'Instead of going through the model fitting and data transformation steps for
    the training and test datasets separately, we can chain the `StandardScaler`,
    `PCA`, and `LogisticRegression` objects in a pipeline:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要分别对训练集和测试集进行模型拟合和数据转换步骤，我们可以将 `StandardScaler`、`PCA` 和 `LogisticRegression`
    对象串联在一个管道中：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `make_pipeline` function takes an arbitrary number of scikit-learn transformers
    (objects that support the `fit` and `transform` methods as input), followed by
    a scikit-learn estimator that implements the `fit` and `predict` methods. In our
    preceding code example, we provided two scikit-learn transformers, `StandardScaler`
    and `PCA`, and a `LogisticRegression` estimator as inputs to the `make_pipeline`
    function, which constructs a scikit-learn `Pipeline` object from these objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_pipeline` 函数接受任意数量的 scikit-learn 转换器（支持 `fit` 和 `transform` 方法的对象）作为输入，后跟一个实现
    `fit` 和 `predict` 方法的 scikit-learn 估计器。在我们前面的代码示例中，我们提供了两个 scikit-learn 转换器 `StandardScaler`
    和 `PCA`，以及一个 `LogisticRegression` 估计器作为 `make_pipeline` 函数的输入，该函数从这些对象构造了一个 scikit-learn
    `Pipeline` 对象。'
- en: We can think of a scikit-learn `Pipeline` as a meta-estimator or wrapper around
    those individual transformers and estimators. If we call the `fit` method of `Pipeline`,
    the data will be passed down a series of transformers via `fit` and `transform`
    calls on these intermediate steps until it arrives at the estimator object (the
    final element in a pipeline). The estimator will then be fitted to the transformed
    training data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 scikit-learn 的 `Pipeline` 看作是那些独立转换器和估计器的元估计器或包装器。如果我们调用 `Pipeline` 的
    `fit` 方法，数据将通过一系列转换器，通过中间步骤上的 `fit` 和 `transform` 调用传递，直到到达估计器对象（管道中的最后一个元素）。然后，估计器将被拟合到转换后的训练数据上。
- en: When we executed the `fit` method on the `pipe_lr` pipeline in the preceding
    code example, `StandardScaler` first performed `fit` and `transform` calls on
    the training data. Second, the transformed training data was passed on to the
    next object in the pipeline, `PCA`. Similar to the previous step, `PCA` also executed
    `fit` and `transform` on the scaled input data and passed it to the final element
    of the pipeline, the estimator.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前面的代码示例中对`pipe_lr`管道执行`fit`方法时，`StandardScaler`首先对训练数据执行了`fit`和`transform`调用。其次，转换后的训练数据被传递给管道中的下一个对象`PCA`。类似于前面的步骤，`PCA`也对缩放后的输入数据执行了`fit`和`transform`操作，并将其传递给管道的最后一个元素，即评估器。
- en: Finally, the `LogisticRegression` estimator was fit to the training data after
    it underwent transformations via `StandardScaler` and `PCA`. Again, we should
    note that there is no limit to the number of intermediate steps in a pipeline;
    however, if we want to use the pipeline for prediction tasks, the last pipeline
    element has to be an estimator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过`StandardScaler`和`PCA`转换后，`LogisticRegression`评估器被拟合到训练数据中。同样需要注意的是，在管道中的中间步骤数量没有限制；然而，如果我们想要将管道用于预测任务，最后一个管道元素必须是评估器。
- en: Similar to calling `fit` on a pipeline, pipelines also implement a `predict`
    method if the last step in the pipeline is an estimator. If we feed a dataset
    to the `predict` call of a `Pipeline` object instance, the data will pass through
    the intermediate steps via `transform` calls. In the final step, the estimator
    object will then return a prediction on the transformed data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与在管道上调用`fit`类似，如果管道的最后一步是评估器，管道也会实现`predict`方法。如果我们将数据集提供给`Pipeline`对象实例的`predict`调用，数据将通过中间步骤通过`transform`调用传递。在最后一步，评估器对象将返回对转换数据的预测。
- en: 'The pipelines of the scikit-learn library are immensely useful wrapper tools
    that we will use frequently throughout the rest of this book. To make sure that
    you’ve got a good grasp of how the `Pipeline` object works, please take a close
    look at *Figure 6.1*, which summarizes our discussion from the previous paragraphs:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库的管道非常实用，是我们在本书的其余部分经常使用的包装工具。为了确保您对`Pipeline`对象的工作原理有深刻理解，请仔细观察*图6.1*，该图总结了我们在前面段落中的讨论：
- en: '![Diagram  Description automatically generated](img/B17582_06_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表说明](img/B17582_06_01.png)'
- en: 'Figure 6.1: The inner workings of the Pipeline object'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：管道对象的内部工作原理
- en: Using k-fold cross-validation to assess model performance
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k折交叉验证评估模型性能
- en: In this section, you will learn about the common cross-validation techniques
    **holdout cross-validation** and **k-fold cross-validation**, which can help us
    to obtain reliable estimates of the model’s generalization performance, that is,
    how well the model performs on unseen data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解常见的交叉验证技术**留出交叉验证**和**k折交叉验证**，这些技术可以帮助我们获得模型泛化性能的可靠估计，即模型在未见数据上的表现如何。
- en: The holdout method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 留出法
- en: A classic and popular approach for estimating the generalization performance
    of machine learning models is the holdout method. Using the holdout method, we
    split our initial dataset into separate training and test datasets—the former
    is used for model training, and the latter is used to estimate its generalization
    performance. However, in typical machine learning applications, we are also interested
    in tuning and comparing different parameter settings to further improve the performance
    for making predictions on unseen data. This process is called **model selection**,
    with the name referring to a given classification problem for which we want to
    select the *optimal* values of *tuning parameters* (also called **hyperparameters**).
    However, if we reuse the same test dataset over and over again during model selection,
    it will become part of our training data and thus the model will be more likely
    to overfit. Despite this issue, many people still use the test dataset for model
    selection, which is not a good machine learning practice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 估计机器学习模型泛化性能的经典和流行方法是留出法。使用留出法，我们将初始数据集分成单独的训练和测试数据集——前者用于模型训练，后者用于估计其泛化性能。然而，在典型的机器学习应用中，我们还对调整和比较不同的参数设置感兴趣，以进一步提高在未见数据上的预测性能。这个过程称为**模型选择**，该名称指的是我们想要选择*最佳*值的*调整参数*（也称为**超参数**）给定分类问题的情况。然而，如果在模型选择过程中反复使用同一测试数据集，它将成为我们的训练数据的一部分，因此模型更可能过拟合。尽管存在这个问题，许多人仍然使用测试数据集进行模型选择，这不是一个好的机器学习实践。
- en: 'A better way of using the holdout method for model selection is to separate
    the data into three parts: a training dataset, a validation dataset, and a test
    dataset. The training dataset is used to fit the different models, and the performance
    on the validation dataset is then used for model selection. The advantage of having
    a test dataset that the model hasn’t seen before during the training and model
    selection steps is that we can obtain a less biased estimate of its ability to
    generalize to new data. *Figure 6.2* illustrates the concept of holdout cross-validation,
    where we use a validation dataset to repeatedly evaluate the performance of the
    model after training using different hyperparameter values. Once we are satisfied
    with the tuning of hyperparameter values, we estimate the model’s generalization
    performance on the test dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用留出法进行模型选择的更好方法是将数据分成三部分：训练数据集、验证数据集和测试数据集。训练数据集用于拟合不同的模型，然后利用验证数据集上的性能进行模型选择。测试数据集的优点在于，在训练和模型选择步骤中，模型之前未见过该数据，因此我们可以获得对其推广到新数据能力的较少偏见的估计。*图
    6.2*说明了留出交叉验证的概念，在这里我们使用验证数据集重复评估使用不同超参数值进行训练后模型的性能。一旦我们对超参数值的调整感到满意，我们就可以估计模型在测试数据集上的泛化性能：
- en: '![](img/B17582_06_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_02.png)'
- en: 'Figure 6.2: How to use training, validation, and test datasets'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：如何使用训练、验证和测试数据集
- en: A disadvantage of the holdout method is that the performance estimate may be
    very sensitive to how we partition the training dataset into the training and
    validation subsets; the estimate will vary for different examples of the data.
    In the next subsection, we will take a look at a more robust technique for performance
    estimation, k-fold cross-validation, where we repeat the holdout method *k* times
    on *k* subsets of the training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 留出法的一个缺点是性能估计可能对如何将训练数据集分割成训练和验证子集非常敏感；估计值会因数据的不同示例而变化。在下一小节中，我们将看一下更健壮的性能估计技术，即k折交叉验证，在这种方法中，我们对训练数据的k个子集重复使用留出法*k*次。
- en: K-fold cross-validation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k折交叉验证
- en: In k-fold cross-validation, we randomly split the training dataset into *k*
    folds without replacement. Here, *k* – 1 folds, the so-called *training folds*,
    are used for the model training, and one fold, the so-called *test fold*, is used
    for performance evaluation. This procedure is repeated *k* times so that we obtain
    *k* models and performance estimates.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，我们将训练数据集随机分成*k*个不重复的折叠。在这里，*k* – 1折叠，即所谓的*训练折叠*，用于模型训练，而一个折叠，即所谓的*测试折叠*，用于性能评估。此过程重复*k*次，以便我们获得*k*个模型和性能估计。
- en: '**Sampling with and without replacement**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**有替换和无替换抽样**'
- en: We looked at an example to illustrate sampling with and without replacement
    in *Chapter 3*. If you haven’t read that chapter, or want a refresher, refer to
    the information box titled *Sampling with and without replacement* in the *Combining
    multiple decision trees via random forests* section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*中查看了一个示例，以说明有放回和无放回的抽样。如果您还没有阅读该章节，或者需要复习，请参阅“组合多个决策树通过随机森林”章节中名为“有放回和无放回抽样”的信息框。
- en: We then calculate the average performance of the models based on the different,
    independent test folds to obtain a performance estimate that is less sensitive
    to the sub-partitioning of the training data compared to the holdout method. Typically,
    we use k-fold cross-validation for model tuning, that is, finding the optimal
    hyperparameter values that yield a satisfying generalization performance, which
    is estimated from evaluating the model performance on the test folds.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们基于不同的、独立的测试折叠计算模型的平均表现，以获得对训练数据的子分区不太敏感的性能估计，与留出方法相比。通常情况下，我们使用k折交叉验证进行模型调整，即找到能产生满意泛化性能的最优超参数值，这些值是通过评估模型在测试折叠上的性能来估计的。
- en: Once we have found satisfactory hyperparameter values, we can retrain the model
    on the complete training dataset and obtain a final performance estimate using
    the independent test dataset. The rationale behind fitting a model to the whole
    training dataset after k-fold cross-validation is that first, we are typically
    interested in a single, final model (versus *k* individual models), and second,
    providing more training examples to a learning algorithm usually results in a
    more accurate and robust model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到令人满意的超参数值，我们可以在完整的训练数据集上重新训练模型，并使用独立的测试数据集获得最终的性能估计。在k折交叉验证之后将模型拟合到整个训练数据集的理由是，首先，我们通常对单个最终模型感兴趣（而不是*k*个单独的模型），其次，将更多的训练示例提供给学习算法通常会产生更精确和更健壮的模型。
- en: Since k-fold cross-validation is a resampling technique without replacement,
    the advantage of this approach is that in each iteration, each example will be
    used exactly once, and the training and test folds are disjoint. Furthermore,
    all test folds are disjoint; that is, there is no overlap between the test folds.
    *Figure 6.3* summarizes the concept behind k-fold cross-validation with *k* = 10\.
    The training dataset is divided into 10 folds, and during the 10 iterations, 9
    folds are used for training, and 1 fold will be used as the test dataset for model
    evaluation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k折交叉验证是一种无替换的重采样技术，这种方法的优势在于在每次迭代中，每个示例都将仅使用一次，并且训练和测试折叠是不重叠的。此外，所有测试折叠也是不重叠的；也就是说，测试折叠之间没有重叠。*图6.3*总结了k折交叉验证背后的概念，其中*k* = 10\.
    训练数据集被分为10个折叠，在10次迭代期间，有9个折叠用于训练，1个折叠将用作模型评估的测试数据集。
- en: 'Also, the estimated performances, *E*[i] (for example, classification accuracy
    or error), for each fold are then used to calculate the estimated average performance,
    *E*, of the model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，每个折叠的估计表现，*E*[i]（例如，分类准确度或误差），然后用于计算模型的估计平均表现，*E*：
- en: '![](img/B17582_06_03.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_03.png)'
- en: 'Figure 6.3: How k-fold cross-validation works'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：k折交叉验证的工作原理
- en: In summary, k-fold cross-validation makes better use of the dataset than the
    holdout method with a validation set, since in k-fold cross-validation all data
    points are being used for evaluation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，k折交叉验证比使用验证集的留出方法更有效地利用数据集，因为在k折交叉验证中，所有数据点都用于评估。
- en: 'A good standard value for *k* in k-fold cross-validation is 10, as empirical
    evidence shows. For instance, experiments by Ron Kohavi on various real-world
    datasets suggest that 10-fold cross-validation offers the best tradeoff between
    bias and variance (*A Study of Cross-Validation and Bootstrap for Accuracy Estimation
    and Model Selection* by *Kohavi, Ron*, *International Joint Conference on Artificial
    Intelligence (IJCAI)*, 14 (12): 1137-43, 1995,[https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf](https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，一个良好的标准值*k*是10，正如经验证据所示。例如，Ron Kohavi在各种真实世界数据集上的实验表明，10折交叉验证在偏差和方差之间提供了最佳的权衡（*关于准确度估计和模型选择的交叉验证和自举研究*由*Ron
    Kohavi*，*国际人工智能联合会议（IJCAI）*，14（12）：1137-43，1995年，[https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf](https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf)）。
- en: However, if we are working with relatively small training sets, it can be useful
    to increase the number of folds. If we increase the value of *k*, more training
    data will be used in each iteration, which results in a lower pessimistic bias
    toward estimating the generalization performance by averaging the individual model
    estimates. However, large values of *k* will also increase the runtime of the
    cross-validation algorithm and yield estimates with higher variance, since the
    training folds will be more similar to each other. On the other hand, if we are
    working with large datasets, we can choose a smaller value for *k*, for example,
    *k* = 5, and still obtain an accurate estimate of the average performance of the
    model while reducing the computational cost of refitting and evaluating the model
    on the different folds.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们使用的训练集比较小，增加折数可能是有用的。如果我们增加 *k* 的值，每次迭代中会使用更多的训练数据，这样平均每个模型估计的泛化性能会有较低的悲观偏差。但是，较大的
    *k* 值也会增加交叉验证算法的运行时间，并导致估计的方差较高，因为训练折会更加相似。另一方面，如果我们处理大数据集，可以选择较小的 *k* 值，例如 *k*
    = 5，仍然可以准确估计模型的平均性能，同时减少在不同折上重新拟合和评估模型的计算成本。
- en: '**Leave-one-out cross-validation**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**留一法交叉验证**'
- en: A special case of k-fold cross-validation is the **leave-one-out cross-validation**
    (**LOOCV**) method. In LOOCV, we set the number of folds equal to the number of
    training examples (*k* = *n*) so that only one training example is used for testing
    during each iteration, which is a recommended approach for working with very small
    datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: k 折交叉验证的一个特例是留一法交叉验证（LOOCV）方法。在 LOOCV 中，我们将折数设置为训练示例的数量（*k* = *n*），因此在每次迭代中只使用一个训练示例进行测试，这是处理非常小数据集的推荐方法。
- en: 'A slight improvement over the standard k-fold cross-validation approach is
    stratified k-fold cross-validation, which can yield better bias and variance estimates,
    especially in cases of unequal class proportions, which has also been shown in
    the same study by Ron Kohavi referenced previously in this section. In stratified
    cross-validation, the class label proportions are preserved in each fold to ensure
    that each fold is representative of the class proportions in the training dataset,
    which we will illustrate by using the `StratifiedKFold` iterator in scikit-learn:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对标准 k 折交叉验证方法的轻微改进是分层 k 折交叉验证，它可以在类别不平衡的情况下更好地估计偏差和方差，正如在本节中前面引用的 Ron Kohavi
    的研究中所示。在分层交叉验证中，保留了每个折中类别标签的比例，以确保每个折都代表训练数据集中的类别比例，我们将通过使用 scikit-learn 中的 `StratifiedKFold`
    迭代器来说明这一点：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we initialized the `StratifiedKFold` iterator from the `sklearn.model_selection`
    module with the `y_train` class labels in the training dataset, and we specified
    the number of folds via the `n_splits` parameter. When we used the `kfold` iterator
    to loop through the `k` folds, we used the returned indices in `train` to fit
    the logistic regression pipeline that we set up at the beginning of this chapter.
    Using the `pipe_lr` pipeline, we ensured that the examples were scaled properly
    (for instance, standardized) in each iteration. We then used the `test` indices
    to calculate the accuracy score of the model, which we collected in the `scores`
    list to calculate the average accuracy and the standard deviation of the estimate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `sklearn.model_selection` 模块中的 `StratifiedKFold` 迭代器初始化了 `y_train` 训练数据集中的类标签，并通过
    `n_splits` 参数指定了折数。当我们使用 `kfold` 迭代器循环遍历 `k` 个折时，我们使用返回的 `train` 索引来拟合本章开头设置的
    logistic 回归流水线。通过 `pipe_lr` 流水线，我们确保每次迭代中的示例都被适当地（例如，标准化）缩放。然后，我们使用 `test` 索引来计算模型的准确率分数，将其收集在
    `scores` 列表中以计算估计的平均准确率和标准偏差。
- en: 'Although the previous code example was useful to illustrate how k-fold cross-validation
    works, scikit-learn also implements a k-fold cross-validation scorer, which allows
    us to evaluate our model using stratified k-fold cross-validation less verbosely:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的代码示例有助于说明 k 折交叉验证的工作原理，scikit-learn 还实现了一种 k 折交叉验证评分器，可以更简洁地使用分层 k 折交叉验证来评估我们的模型：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: An extremely useful feature of the `cross_val_score` approach is that we can
    distribute the evaluation of the different folds across multiple **central processing
    units** (**CPUs**) on our machine. If we set the `n_jobs` parameter to `1`, only
    one CPU will be used to evaluate the performances, just like in our `StratifiedKFold`
    example previously. However, by setting `n_jobs=2`, we could distribute the 10
    rounds of cross-validation to two CPUs (if available on our machine), and by setting
    `n_jobs=-1`, we can use all available CPUs on our machine to do the computation
    in parallel.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_val_score`方法的一个极其有用的特性是，我们可以将不同折叠的评估任务分布到我们机器上的多个**中央处理单元**（**CPU**）上。如果将`n_jobs`参数设为`1`，那么只会使用一个CPU来评估性能，就像我们之前的`StratifiedKFold`示例一样。然而，通过设置`n_jobs=2`，我们可以将10轮交叉验证任务分布到两个CPU上（如果机器上有的话），而通过设置`n_jobs=-1`，我们可以利用机器上所有可用的CPU并行计算。'
- en: '**Estimating generalization performance**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**估算泛化性能**'
- en: Please note that a detailed discussion of how the variance of the generalization
    performance is estimated in cross-validation is beyond the scope of this book,
    but you can refer to a comprehensive article about model evaluation and cross-validation
    (*Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning*
    by *S.* *Raschka*), which we share at [https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808).
    This article also discusses alternative cross-validation techniques, such as the
    .632 and .632+ bootstrap cross-validation methods.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本书不涵盖如何估计交叉验证中泛化性能的方差的详细讨论，但您可以参考关于模型评估和交叉验证的全面文章（*《机器学习中的模型评估、模型选择和算法选择》*，*S.*
    *Raschka*），我们在[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)分享了这篇文章。此文章还讨论了替代的交叉验证技术，例如.632和.632+自助法交叉验证方法。
- en: 'In addition, you can find a detailed discussion in an excellent article by
    M. Markatou and others (*Analysis of Variance of Cross-validation Estimators of
    the Generalization Error* by *M. Markatou*, *H. Tian*, *S. Biswas*, and *G. M.
    Hripcsak*, *Journal of Machine Learning Research*, 6: 1127-1168, 2005), which
    is available at [https://www.jmlr.org/papers/v6/markatou05a.html](https://www.jmlr.org/papers/v6/markatou05a.html).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，您可以在M. Markatou等人的优秀文章中找到详细讨论（*《分析交叉验证估计泛化误差的方差分析》*，*M. Markatou*，*H. Tian*，*S.
    Biswas*和*G. M. Hripcsak*，*机器学习研究杂志*，6: 1127-1168，2005年），该文章可在[https://www.jmlr.org/papers/v6/markatou05a.html](https://www.jmlr.org/papers/v6/markatou05a.html)找到。'
- en: Debugging algorithms with learning and validation curves
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用学习曲线和验证曲线调试算法
- en: 'In this section, we will take a look at two very simple yet powerful diagnostic
    tools that can help us to improve the performance of a learning algorithm: **learning
    curves** and **validation curves**. In the next subsections, we will discuss how
    we can use learning curves to diagnose whether a learning algorithm has a problem
    with overfitting (high variance) or underfitting (high bias). Furthermore, we
    will take a look at validation curves, which can help us to address the common
    issues of learning algorithms.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两个非常简单但功能强大的诊断工具，可以帮助我们改善学习算法的性能：**学习曲线**和**验证曲线**。在接下来的小节中，我们将讨论如何使用学习曲线来诊断学习算法是否存在过拟合（高方差）或拟合不足（高偏差）的问题。此外，我们还将看看验证曲线，它可以帮助我们解决学习算法的常见问题。
- en: Diagnosing bias and variance problems with learning curves
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用学习曲线诊断偏差和方差问题
- en: If a model is too complex for a given training dataset—for example, think of
    a very deep decision tree—the model tends to overfit the training data and does
    not generalize well to unseen data. Often, it can help to collect more training
    examples to reduce the degree of overfitting.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型对于给定的训练数据集过于复杂，例如，想象一下非常深的决策树，那么该模型倾向于过拟合训练数据，并且不能很好地泛化到未见过的数据。通常情况下，增加训练样本的数量可以帮助减少过拟合的程度。
- en: However, in practice, it can often be very expensive or simply not feasible
    to collect more data. By plotting the model training and validation accuracies
    as functions of the training dataset size, we can easily detect whether the model
    suffers from high variance or high bias, and whether the collection of more data
    could help to address this problem.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，收集更多数据往往非常昂贵，或者根本不可行。通过绘制模型训练和验证精度随训练数据集大小变化的曲线，我们可以轻松检测模型是否存在高方差或高偏差问题，以及收集更多数据是否有助于解决这一问题。
- en: 'But before we discuss how to plot learning curves in scikit-learn, let’s discuss
    those two common model issues by walking through the following illustration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但在讨论如何在scikit-learn中绘制学习曲线之前，让我们通过以下示例来讨论这两个常见的模型问题：
- en: '![](img/B17582_06_04.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_04.png)'
- en: 'Figure 6.4: Common model issues'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：常见的模型问题
- en: The graph in the upper left shows a model with a high bias. This model has both
    low training and cross-validation accuracy, which indicates that it underfits
    the training data. Common ways to address this issue are to increase the number
    of model parameters, for example, by collecting or constructing additional features,
    or by decreasing the degree of regularization, for example, in **support vector
    machine** (**SVM**) or logistic regression classifiers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 左上图显示的图表显示了高偏差的模型。这个模型具有低训练和交叉验证准确性，表明它在训练数据上拟合不足。解决这个问题的常见方法包括增加模型参数的数量，例如通过收集或构建额外的特征，或通过减少正则化的程度，例如在**支持向量机**（**SVM**）或逻辑回归分类器中。
- en: The graph in the upper-right shows a model that suffers from high variance,
    which is indicated by the large gap between the training and cross-validation
    accuracy. To address this problem of overfitting, we can collect more training
    data, reduce the complexity of the model, or increase the regularization parameter,
    for example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 右上图显示的图表显示模型存在高方差问题，这通过训练和交叉验证准确性之间的巨大差距来指示。为了解决这个过拟合问题，我们可以收集更多的训练数据，减少模型的复杂性，或增加正则化参数，例如。
- en: For unregularized models, it can also help to decrease the number of features
    via feature selection (*Chapter 4*) or feature extraction (*Chapter 5*) to decrease
    the degree of overfitting. While collecting more training data usually tends to
    decrease the chance of overfitting, it may not always help, for example, if the
    training data is extremely noisy or the model is already very close to optimal.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非正则化模型，还可以通过特征选择（*第4章*）或特征提取（*第5章*）减少特征数量，从而减少过拟合程度。虽然收集更多的训练数据通常会减少过拟合的机会，但在训练数据非常嘈杂或模型已经非常接近最优情况时，这并不总是有帮助的。
- en: 'In the next subsection, we will see how to address those model issues using
    validation curves, but let’s first see how we can use the learning curve function
    from scikit-learn to evaluate the model:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将看到如何使用验证曲线来解决这些模型问题，但让我们先看看如何使用scikit-learn中的学习曲线函数来评估模型：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that we passed `max_iter=10000` as an additional argument when instantiating
    the `LogisticRegression` object (which uses 1,000 iterations as a default) to
    avoid convergence issues for the smaller dataset sizes or extreme regularization
    parameter values (covered in the next section). After we have successfully executed
    the preceding code, we will obtain the following learning curve plot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在实例化`LogisticRegression`对象时，我们传递了`max_iter=10000`作为额外参数（默认使用1,000次迭代），以避免在较小的数据集大小或极端正则化参数值（在下一节中讨论）时出现收敛问题。执行上述代码成功后，我们将获得以下学习曲线图：
- en: '![Chart, line chart  Description automatically generated](img/B17582_06_05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 自动生成的描述](img/B17582_06_05.png)'
- en: 'Figure 6.5: A learning curve showing training and validation dataset accuracy
    by the number of training examples'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：显示训练和验证数据集准确性的学习曲线
- en: Via the `train_sizes` parameter in the `learning_curve` function, we can control
    the absolute or relative number of training examples that are used to generate
    the learning curves. Here, we set `train_sizes=np.linspace(0.1, 1.0, 10)` to use
    10 evenly spaced, relative intervals for the training dataset sizes. By default,
    the `learning_curve` function uses stratified k-fold cross-validation to calculate
    the cross-validation accuracy of a classifier, and we set *k* = 10 via the `cv`
    parameter for 10-fold stratified cross-validation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`learning_curve`函数中的`train_sizes`参数，我们可以控制用于生成学习曲线的训练示例的绝对或相对数量。在这里，我们设置`train_sizes=np.linspace(0.1,
    1.0, 10)`，以使用10个均匀间隔的相对训练数据集大小。默认情况下，`learning_curve`函数使用分层k折交叉验证来计算分类器的交叉验证准确性，并通过`cv`参数设置*k*
    = 10以进行10折分层交叉验证。
- en: Then, we simply calculated the average accuracies from the returned cross-validated
    training and test scores for the different sizes of the training dataset, which
    we plotted using Matplotlib’s `plot` function. Furthermore, we added the standard
    deviation of the average accuracy to the plot using the `fill_between` function
    to indicate the variance of the estimate.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地计算了返回的交叉验证训练和测试分数的平均精度，这些分数是针对不同大小的训练数据集绘制的，我们使用Matplotlib的`plot`函数进行了绘制。此外，我们使用`fill_between`函数将平均精度的标准偏差添加到图表中，以指示估计值的变化范围。
- en: As we can see in the preceding learning curve plot, our model performs quite
    well on both the training and validation datasets if it has seen more than 250
    examples during training. We can also see that the training accuracy increases
    for training datasets with fewer than 250 examples, and the gap between validation
    and training accuracy widens—an indicator of an increasing degree of overfitting.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的学习曲线图中所看到的，如果在训练过程中看到了超过250个示例，我们的模型在训练和验证数据集上表现相当不错。我们还可以看到，对于训练数据集少于250个示例的情况，训练精度会提高，并且验证精度与训练精度之间的差距会扩大——这是过拟合程度增加的指标。
- en: Addressing over- and underfitting with validation curves
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决过拟合和欠拟合问题的验证曲线
- en: Validation curves are a useful tool for improving the performance of a model
    by addressing issues such as overfitting or underfitting. Validation curves are
    related to learning curves, but instead of plotting the training and test accuracies
    as functions of the sample size, we vary the values of the model parameters, for
    example, the inverse regularization parameter, `C`, in logistic regression.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线是通过解决过拟合或欠拟合等问题来改善模型性能的有用工具。验证曲线与学习曲线相关，但不同于将训练和测试精度作为样本大小的函数进行绘制，我们改变模型参数的值，例如逻辑回归中的反正则化参数`C`。
- en: 'Let’s go ahead and see how we create validation curves via scikit-learn:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看看如何通过scikit-learn创建验证曲线：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using the preceding code, we obtained the validation curve plot for the parameter
    `C`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们获得了参数`C`的验证曲线图：
- en: '![Chart, line chart  Description automatically generated](img/B17582_06_06.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B17582_06_06.png)'
- en: 'Figure 6.6: A validation curve plot for the SVM hyperparameter C'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：SVM超参数C的验证曲线图
- en: Similar to the `learning_curve` function, the `validation_curve` function uses
    stratified k-fold cross-validation by default to estimate the performance of the
    classifier. Inside the `validation_curve` function, we specified the parameter
    that we wanted to evaluate. In this case, it is `C`, the inverse regularization
    parameter of the `LogisticRegression` classifier, which we wrote as `'logisticregression__C'`
    to access the `LogisticRegression` object inside the scikit-learn pipeline for
    a specified value range that we set via the `param_range` parameter. Similar to
    the learning curve example in the previous section, we plotted the average training
    and cross-validation accuracies and the corresponding standard deviations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与`learning_curve`函数类似，`validation_curve`函数默认使用分层k折交叉验证来估计分类器的性能。在`validation_curve`函数内部，我们指定了要评估的参数。在本例中，它是`C`，即`LogisticRegression`分类器的反正则化参数，我们写成`'logisticregression__C'`以访问scikit-learn流水线中的`LogisticRegression`对象，为我们通过`param_range`参数设置的指定值范围进行评估。与上一节中的学习曲线示例类似，我们绘制了平均训练和交叉验证精度以及相应的标准偏差。
- en: Although the differences in the accuracy for varying values of `C` are subtle,
    we can see that the model slightly underfits the data when we increase the regularization
    strength (small values of `C`). However, for large values of `C`, it means lowering
    the strength of regularization, so the model tends to slightly overfit the data.
    In this case, the sweet spot appears to be between 0.01 and 0.1 of the `C` value.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`C`值变化时的精度差异微妙，但我们可以看到，当增加正则化强度（`C`的小值）时，模型略微欠拟合数据。然而，对于较大的`C`值，即降低正则化强度，则模型倾向于轻微过拟合数据。在这种情况下，`C`值的甜点似乎在0.01到0.1之间。
- en: Fine-tuning machine learning models via grid search
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过网格搜索优化机器学习模型
- en: 'In machine learning, we have two types of parameters: those that are learned
    from the training data, for example, the weights in logistic regression, and the
    parameters of a learning algorithm that are optimized separately. The latter are
    the tuning parameters (or hyperparameters) of a model, for example, the regularization
    parameter in logistic regression or the maximum depth parameter of a decision
    tree.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们有两种类型的参数：一种是从训练数据中学习的参数，例如逻辑回归中的权重，另一种是学习算法单独优化的参数。后者是模型的调整参数（或超参数），例如逻辑回归中的正则化参数或决策树的最大深度参数。
- en: In the previous section, we used validation curves to improve the performance
    of a model by tuning one of its hyperparameters. In this section, we will take
    a look at a popular hyperparameter optimization technique called **grid search**,
    which can further help to improve the performance of a model by finding the *optimal*
    combination of hyperparameter values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们使用验证曲线来通过调整其中一个超参数来改善模型的性能。在本节中，我们将介绍一种名为**网格搜索**的流行超参数优化技术，它可以通过找到超参数值的*最佳*组合进一步帮助改善模型的性能。
- en: Tuning hyperparameters via grid search
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过网格搜索调整超参数
- en: 'The grid search approach is quite simple: it’s a brute-force exhaustive search
    paradigm where we specify a list of values for different hyperparameters, and
    the computer evaluates the model performance for each combination to obtain the
    optimal combination of values from this set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索方法相当简单：它是一种蛮力穷举搜索范式，我们在不同超参数的值列表中指定一组值，计算机对每个组合评估模型性能，以获取从此集合中获得的最优组合值：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the preceding code, we initialized a `GridSearchCV` object from the `sklearn.model_selection`
    module to train and tune an SVM pipeline. We set the `param_grid` parameter of
    `GridSearchCV` to a list of dictionaries to specify the parameters that we’d want
    to tune. For the linear SVM, we only evaluated the inverse regularization parameter,
    `C`; for the **radial basis function** (**RBF**) kernel SVM, we tuned both the
    `svc__C` and `svc__gamma` parameters. Note that the `svc__gamma` parameter is
    specific to kernel SVMs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们从 `sklearn.model_selection` 模块初始化了一个 `GridSearchCV` 对象来训练和调整 SVM 流水线。我们将
    `GridSearchCV` 的 `param_grid` 参数设置为一组字典，以指定我们希望调整的参数。对于线性 SVM，我们只评估了逆正则化参数 `C`；对于**径向基函数**（**RBF**）核
    SVM，我们调整了 `svc__C` 和 `svc__gamma` 参数。请注意，`svc__gamma` 参数特定于核 SVM。
- en: '`GridSearchCV` uses k-fold cross-validation for comparing models trained with
    different hyperparameter settings. Via the `cv=10` setting, it will carry out
    10-fold cross-validation and compute the average accuracy (via `scoring=''accuracy''`)
    across these 10-folds to assess the model performance. We set `n_jobs=-1` so that
    `GridSearchCV` can use all our processing cores to speed up the grid search by
    fitting models to the different folds in parallel, but if your machine has problems
    with this setting, you may change this setting to `n_jobs=None` for single processing.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 使用 k 折交叉验证来比较使用不同超参数设置训练的模型。通过 `cv=10` 设置，它将进行十折交叉验证，并计算这十个折叠中的平均准确率（通过
    `scoring=''accuracy''`）来评估模型性能。我们设置 `n_jobs=-1`，以便 `GridSearchCV` 可以利用所有处理核心并行地加速网格搜索，但如果您的计算机对此设置有问题，您可以将此设置更改为
    `n_jobs=None` 以进行单处理。'
- en: 'After we used the training data to perform the grid search, we obtained the
    score of the best-performing model via the `best_score_` attribute and looked
    at its parameters, which can be accessed via the `best_params_` attribute. In
    this particular case, the RBF kernel SVM model with `svc__C = 100.0` yielded the
    best k-fold cross-validation accuracy: 98.5 percent.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用训练数据执行网格搜索后，我们通过 `best_score_` 属性获取了表现最佳模型的分数，并查看了其参数，这些参数可以通过 `best_params_`
    属性访问。在这种特定情况下，具有 `svc__C = 100.0` 的 RBF 核 SVM 模型产生了最佳的 k 折交叉验证准确率：98.5%。
- en: 'Finally, we use the independent test dataset to estimate the performance of
    the best-selected model, which is available via the `best_estimator_` attribute
    of the `GridSearchCV` object:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用独立的测试数据集来估计选择的最佳模型的性能，该模型可通过 `GridSearchCV` 对象的 `best_estimator_` 属性获得：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Please note that fitting a model with the best settings (`gs.best_estimator_`)
    on the training set manually via `clf.fit(X_train, y_train)` after completing
    the grid search is not necessary. The `GridSearchCV` class has a `refit` parameter,
    which will refit the `gs.best_estimator_` to the whole training set automatically
    if we set `refit=True` (default).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在完成网格搜索后，手动在训练集上用最佳设置(`gs.best_estimator_`)拟合模型是不必要的。`GridSearchCV`类有一个`refit`参数，如果我们设置`refit=True`（默认），它将自动将`gs.best_estimator_`重新拟合到整个训练集上。
- en: Exploring hyperparameter configurations more widely with randomized search
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过随机搜索更广泛地探索超参数配置
- en: 'Since grid search is an exhaustive search, it is guaranteed to find the optimal
    hyperparameter configuration if it is contained in the user-specified parameter
    grid. However, specifying large hyperparameter grids makes grid search very expensive
    in practice. An alternative approach for sampling different parameter combinations
    is randomized search. In randomized search, we draw hyperparameter configurations
    randomly from distributions (or discrete sets). In contrast to grid search, randomized
    search does not do an exhaustive search over the hyperparameter space. Still,
    it allows us to explore a wider range of hyperparameter value settings in a more
    cost- and time-effective manner. This concept is illustrated in *Figure 6.7*,
    which shows a fixed grid of nine hyperparameter settings being searched via grid
    search and randomized search:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网格搜索是一种穷举搜索，如果最佳超参数配置包含在用户指定的参数网格中，它肯定能找到最优配置。然而，指定大型超参数网格在实践中使网格搜索非常昂贵。采样不同参数组合的替代方法是随机搜索。在随机搜索中，我们从分布（或离散集合）中随机抽取超参数配置。与网格搜索不同，随机搜索不会对超参数空间进行穷举搜索。尽管如此，它仍然能够以更加经济和时间有效的方式探索更广泛的超参数值设置范围。这个概念在图6.7中有所体现，图示了通过网格搜索和随机搜索对九个超参数设置进行搜索的固定网格：
- en: '![Shape  Description automatically generated](img/B17582_06_07.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Shape  Description automatically generated](img/B17582_06_07.png)'
- en: 'Figure 6.7: A comparison of grid search and randomized search for sampling
    nine different hyperparameter configurations each'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：比较网格搜索和随机搜索各自采样九种不同的超参数配置
- en: 'The main takeaway is that while grid search only explores discrete, user-specified
    choices, it may miss good hyperparameter configurations if the search space is
    too scarce. Interested readers can find additional details about randomized search,
    along with empirical studies, in the following article: *Random Search for Hyper-Parameter
    Optimization* by *J. Bergstra*, *Y. Bengio*, *Journal of Machine Learning Research*,
    pp. 281-305, 2012, [https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 主要观点是，虽然网格搜索只探索离散的、用户指定的选择，但如果搜索空间太少，可能会错过好的超参数配置。有兴趣的读者可以在以下文章中找到关于随机搜索的详细信息，以及经验研究：《*超参数优化的随机搜索*》由*J.
    Bergstra*、*Y. Bengio*、*机器学习研究杂志*，第281-305页，2012年，[https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)。
- en: 'Let’s look at how we can use randomized search for tuning an SVM. Scikit-learn
    implements a `RandomizedSearchCV` class, which is analogous to the `GridSearchCV`
    we used in the previous subsection. The main difference is that we can specify
    distributions as part of our parameter grid and specify the total number of hyperparameter
    configurations to be evaluated. For example, let’s consider the hyperparameter
    range we used for several hyperparameters when tuning the SVM in the grid search
    example in the previous section:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何利用随机搜索来调整SVM。Scikit-learn实现了一个`RandomizedSearchCV`类，类似于我们在前一小节中使用的`GridSearchCV`。主要区别在于我们可以在参数网格中指定分布，并指定要评估的超参数配置的总数。例如，让我们考虑在前一小节中网格搜索SVM时使用的几个超参数的范围：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that while `RandomizedSearchCV` can accept similar discrete lists of values
    as inputs for the parameter grid, which is useful when considering categorical
    hyperparameters, its main power lies in the fact that we can replace these lists
    with distributions to sample from. Thus, for example, we may substitute the preceding
    list with the following distribution from SciPy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然`RandomizedSearchCV`可以接受类似的离散值列表作为参数网格的输入，这在考虑分类超参数时非常有用，但它的主要优势在于我们可以用分布来采样这些列表。因此，例如，我们可以用SciPy中的以下分布替换前面的列表：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For instance, using a loguniform distribution instead of a regular uniform
    distribution will ensure that in a sufficiently large number of trials, the same
    number of samples will be drawn from the [0.0001, 0.001] range as, for example,
    the [10.0, 100.0] range. To check its behavior, we can draw 10 random samples
    from this distribution via the `rvs(10)` method, as shown here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用对数均匀分布而不是常规均匀分布将确保在足够大数量的试验中，与[0.0001, 0.001]范围相比，将从[10.0, 100.0]范围中绘制相同数量的样本。要检查其行为，我们可以通过`rvs(10)`方法从该分布中绘制10个随机样本，如下所示：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Specifying distributions**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**指定分布**'
- en: '`RandomizedSearchCV` supports arbitrary distributions as long as we can sample
    from them by calling the `rvs()` method. A list of all distributions currently
    available via `scipy.stats` can be found here: [https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomizedSearchCV`支持任意分布，只要我们可以通过调用`rvs()`方法从中抽样。可以在这里找到`scipy.stats`当前可用的所有分布列表：[https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions)。'
- en: 'Let’s now see the `RandomizedSearchCV` in action and tune an SVM as we did
    with `GridSearchCV` in the previous section:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看`RandomizedSearchCV`如何运作，并像在前一节中使用`GridSearchCV`调整SVM一样：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Based on this code example, we can see that the usage is very similar to `GridSearchCV`,
    except that we could use distributions for specifying parameter ranges and specified
    the number of iterations—20 iterations—by setting `n_iter=20`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此代码示例，我们可以看到其使用方式与`GridSearchCV`非常相似，不同之处在于我们可以使用分布来指定参数范围，并通过设置`n_iter=20`来指定迭代次数——20次迭代。
- en: More resource-efficient hyperparameter search with successive halving
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更具资源效率的超参数搜索与连续减半
- en: 'Taking the idea of randomized search one step further, scikit-learn implements
    a successive halving variant, `HalvingRandomSearchCV`, that makes finding suitable
    hyperparameter configurations more efficient. Successive halving, given a large
    set of candidate configurations, successively throws out unpromising hyperparameter
    configurations until only one configuration remains. We can summarize the procedure
    via the following steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将随机搜索的思想进一步发展，scikit-learn 实现了一种称为`HalvingRandomSearchCV`的连续减半变体，使得寻找适合的超参数配置更加高效。连续减半是指，在一个大的候选配置集合中，逐步淘汰不太有希望的超参数配置，直到只剩下一个配置。我们可以通过以下步骤总结该过程：
- en: Draw a large set of candidate configurations via random sampling
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机抽样绘制大量候选配置
- en: Train the models with limited resources, for example, a small subset of the
    training data (as opposed to using the entire training set)
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用有限的资源训练模型，例如，训练数据的一个小子集（与使用整个训练集相对）
- en: Discard the bottom 50 percent based on predictive performance
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于预测性能底部50%的丢弃
- en: Go back to *step 2* with an increased amount of available resources
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到*步骤2*并增加可用资源量
- en: The steps are repeated until only one hyperparameter configuration remains.
    Note that there is also a successive halving implementation for the grid search
    variant called `HalvingGridSearchCV`, where all specified hyperparameter configurations
    are used in *step 1* instead of random samples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 直到只剩下一个超参数配置为止重复执行上述步骤。注意，还有一个用于网格搜索变体的连续减半实现称为`HalvingGridSearchCV`，在*步骤1*中使用所有指定的超参数配置而不是随机样本。
- en: 'In scikit-learn 1.0, `HalvingRandomSearchCV` is still experimental, which is
    why we have to enable it first:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 1.0 中，`HalvingRandomSearchCV`仍处于实验阶段，因此我们必须首先启用它：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: (The above code may not work or be supported in future releases.)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: （上述代码可能在未来版本中不起作用或不被支持。）
- en: 'After enabling the experimental support, we can use randomized search with
    successive halving as shown in the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 启用实验支持后，我们可以像下面展示的那样使用带有连续减半的随机搜索：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `resource=''n_samples''` (default) setting specifies that we consider the
    training set size as the resource we vary between the rounds. Via the `factor`
    parameter, we can determine how many candidates are eliminated in each round.
    For example, setting `factor=2` eliminates half of the candidates, and setting
    `factor=1.5` means that only 100%/1.5 ≈ 66% of the candidates make it into the
    next round. Instead of choosing a fixed number of iterations as in `RandomizedSearchCV`,
    we set `n_candidates=''exhaust''` (default), which will sample the number of hyperparameter
    configurations such that the maximum number of resources (here: training examples)
    are used in the last round.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`resource=''n_samples''`（默认）设置指定我们将训练集大小作为我们在各轮之间变化的资源。通过`factor`参数，我们可以确定每轮淘汰多少候选者。例如，设置`factor=2`会淘汰一半的候选者，而设置`factor=1.5`意味着只有100%/1.5
    ≈ 66% 的候选者进入下一轮。与在`RandomizedSearchCV`中选择固定迭代次数不同，我们设置`n_candidates=''exhaust''`（默认），这将对超参数配置数量进行采样，以便在最后一轮使用最大数量的资源（在这里：训练样本）。'
- en: 'We can then carry out the search similar to `RandomizedSearchCV`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像`RandomizedSearchCV`那样进行搜索：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we compare the results from `GridSearchCV` and `RandomizedSearchCV` from
    the previous two subsections with the model from `HalvingRandomSearchCV`, we can
    see that the latter yields a model that performs slightly better on the test set
    (98.2 percent accuracy as opposed to 97.4 percent).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前两个子段中`GridSearchCV`和`RandomizedSearchCV`的结果与`HalvingRandomSearchCV`中的模型进行比较，可以看到后者在测试集上表现略优（98.2%
    的准确率，而不是97.4%）。
- en: '**Hyperparameter tuning with hyperopt**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用hyperopt进行超参数调优**'
- en: Another popular library for hyperparameter optimization is hyperopt ([https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt)),
    which implements several different methods for hyperparameter optimization, including
    randomized search and the **Tree-structured Parzen Estimators** (**TPE**) method.
    TPE is a Bayesian optimization method based on a probabilistic model that is continuously
    updated based on past hyperparameter evaluations and the associated performance
    scores instead of regarding these evaluations as independent events. You can find
    out more about TPE in *Algorithms for Hyper-Parameter Optimization*. *Bergstra
    J*, *Bardenet R, Bengio Y, Kegl B*. *NeurIPS 2011*. pp. 2546–2554, [https://dl.acm.org/doi/10.5555/2986459.2986743](https://dl.acm.org/doi/10.5555/2986459.2986743).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的超参数优化库是hyperopt ([https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt))，它实现了几种不同的超参数优化方法，包括随机搜索和**树结构贝叶斯优化器**（**TPE**）方法。TPE是一种基于概率模型的贝叶斯优化方法，根据过去的超参数评估和相关的性能分数不断更新模型，而不是将这些评估视为独立事件。您可以在*超参数优化算法*中了解更多关于TPE的信息。*Bergstra
    J*, *Bardenet R, Bengio Y, Kegl B*. *NeurIPS 2011*. pp. 2546–2554，[https://dl.acm.org/doi/10.5555/2986459.2986743](https://dl.acm.org/doi/10.5555/2986459.2986743)。
- en: 'While hyperopt provides a general-purpose interface for hyperparameter optimization,
    there is also a scikit-learn-specific package called hyperopt-sklearn for additional
    convenience: [https://github.com/hyperopt/hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然hyperopt提供了一个通用的超参数优化接口，但也有一个专门为scikit-learn设计的包叫做hyperopt-sklearn，提供了额外的便利：[https://github.com/hyperopt/hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)。
- en: Algorithm selection with nested cross-validation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用嵌套交叉验证进行算法选择
- en: 'Using k-fold cross-validation in combination with grid search or randomized
    search is a useful approach for fine-tuning the performance of a machine learning
    model by varying its hyperparameter values, as we saw in the previous subsections.
    If we want to select among different machine learning algorithms, though, another
    recommended approach is **nested cross-validation**. In a nice study on the bias
    in error estimation, Sudhir Varma and Richard Simon concluded that the true error
    of the estimate is almost unbiased relative to the test dataset when nested cross-validation
    is used (*Bias in Error Estimation When Using Cross-Validation for Model Selection*
    by *S. Varma* and *R. Simon*, *BMC Bioinformatics*, 7(1): 91, 2006,[https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91)).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 k 折交叉验证结合网格搜索或随机搜索是通过变化超参数值来微调机器学习模型性能的有用方法，就像我们在前面的子节中看到的那样。如果我们想在不同的机器学习算法之间选择，那么另一个推荐的方法是**嵌套交叉验证**。在一项关于误差估计偏差的研究中，Sudhir
    Varma 和 Richard Simon 得出结论，当使用嵌套交叉验证时，相对于测试数据集，估计的真实误差几乎是无偏的（*Bias in Error Estimation
    When Using Cross-Validation for Model Selection* by *S. Varma* and *R. Simon*,
    *BMC Bioinformatics*, 7(1): 91, 2006, [https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91))。'
- en: 'In nested cross-validation, we have an outer k-fold cross-validation loop to
    split the data into training and test folds, and an inner loop is used to select
    the model using k-fold cross-validation on the training fold. After model selection,
    the test fold is then used to evaluate the model performance. *Figure 6.8* explains
    the concept of nested cross-validation with only five outer and two inner folds,
    which can be useful for large datasets where computational performance is important;
    this particular type of nested cross-validation is also known as **5×2 cross-validation**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌套交叉验证中，我们有一个外部 k 折交叉验证循环将数据分成训练集和测试集，并且内部循环用于在训练集上使用 k 折交叉验证选择模型。在模型选择后，测试集用于评估模型性能。*图
    6.8* 解释了只有五个外部和两个内部折叠的嵌套交叉验证概念，这对于大数据集在计算性能重要时可能会很有用；这种特定类型的嵌套交叉验证也被称为**5×2 交叉验证**：
- en: '![](img/B17582_06_08.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_08.png)'
- en: 'Figure 6.8: The concept of nested cross-validation'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：嵌套交叉验证的概念
- en: 'In scikit-learn, we can perform nested cross-validation with grid search as
    follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以通过以下方式执行嵌套交叉验证和网格搜索：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The returned average cross-validation accuracy gives us a good estimate of what
    to expect if we tune the hyperparameters of a model and use it on unseen data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的平均交叉验证准确度为我们提供了一个很好的估计，如果我们调整模型的超参数并在未见数据上使用它，我们可以预期什么。
- en: 'For example, we can use the nested cross-validation approach to compare an
    SVM model to a simple decision tree classifier; for simplicity, we will only tune
    its depth parameter:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用嵌套交叉验证方法比较 SVM 模型和简单的决策树分类器；为简单起见，我们只会调整其深度参数：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As we can see, the nested cross-validation performance of the SVM model (97.4
    percent) is notably better than the performance of the decision tree (93.4 percent),
    and thus, we’d expect that it might be the better choice to classify new data
    that comes from the same population as this particular dataset.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，SVM 模型的嵌套交叉验证性能（97.4%）明显优于决策树的性能（93.4%），因此，我们预计它可能是分类来自与此特定数据集相同总体的新数据的更好选择。
- en: Looking at different performance evaluation metrics
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看不同的性能评估指标
- en: In the previous sections and chapters, we evaluated different machine learning
    models using prediction accuracy, which is a useful metric with which to quantify
    the performance of a model in general. However, there are several other performance
    metrics that can be used to measure a model’s relevance, such as precision, recall,
    the **F1 score**, and **Matthews correlation coefficient** (**MCC**).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的部分和章节中，我们使用预测准确度评估不同的机器学习模型，这是一种用于总体上量化模型性能的有用指标。然而，还有几个其他性能指标可以用来衡量模型的相关性，例如精确度、召回率、**F1
    分数**和**马修斯相关系数**（**MCC**）。
- en: Reading a confusion matrix
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读混淆矩阵
- en: Before we get into the details of different scoring metrics, let’s take a look
    at a **confusion matrix**, a matrix that lays out the performance of a learning
    algorithm.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论不同评分指标的细节之前，让我们先看看**混淆矩阵**，这是一个展示学习算法性能的矩阵。
- en: 'A confusion matrix is simply a square matrix that reports the counts of the
    **true positive** (**TP**), **true negative** (**TN**), **false positive** (**FP**),
    and **false negative** (**FN**) predictions of a classifier, as shown in *Figure
    6.9*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵只是一个简单的方阵，报告了分类器对真正预测（TP）、真负预测（TN）、假正预测（FP）和假负预测（FN）的计数，如 *图 6.9* 所示：
- en: '![](img/B17582_06_09.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_09.png)'
- en: 'Figure 6.9: The confusion matrix'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：混淆矩阵
- en: 'Although these metrics can be easily computed manually by comparing the actual
    and predicted class labels, scikit-learn provides a convenient `confusion_matrix`
    function that we can use, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些指标可以通过比较实际和预测的类别标签手动计算，但 scikit-learn 提供了一个方便的 `confusion_matrix` 函数供我们使用，如下所示：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The array that was returned after executing the code provides us with information
    about the different types of error the classifier made on the test dataset. We
    can map this information onto the confusion matrix illustration in *Figure 6.9*
    using Matplotlib’s `matshow` function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后返回的数组为我们提供了关于分类器在测试数据集上所做的不同类型错误的信息。我们可以使用 Matplotlib 的 `matshow` 函数将这些信息映射到混淆矩阵图示中
    *图 6.9*：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, the following confusion matrix plot, with the added labels, should make
    the results a little bit easier to interpret:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，以下带有附加标签的混淆矩阵图应该会使结果稍微容易解释一些：
- en: '![A picture containing application  Description automatically generated](img/B17582_06_10.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing application  Description automatically generated](img/B17582_06_10.png)'
- en: 'Figure 6.10: A confusion matrix for our data'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：我们数据的混淆矩阵
- en: Assuming that class `1` (malignant) is the positive class in this example, our
    model correctly classified 71 of the examples that belong to class `0` (TN) and
    40 examples that belong to class `1` (TP), respectively. However, our model also
    incorrectly misclassified two examples from class `1` as class `0` (FN), and it
    predicted that one example is malignant although it is a benign tumor (FP). In
    the next subsection, we will learn how we can use this information to calculate
    various error metrics.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在这个例子中类别 `1`（恶性）是正类，我们的模型正确地分类了属于类别 `0` 的 71 个示例（TN），以及属于类别 `1` 的 40 个示例（TP）。然而，我们的模型还将两个类别
    `1` 的示例错误地分类为类别 `0`（FN），并且它预测了一个示例是恶性尽管它是良性肿瘤（FP）。在下一小节中，我们将学习如何利用这些信息来计算各种误差指标。
- en: Optimizing the precision and recall of a classification model
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化分类模型的精确度和召回率
- en: 'Both the prediction **error** (**ERR**) and **accuracy** (**ACC**) provide
    general information about how many examples are misclassified. The error can be
    understood as the sum of all false predictions divided by the number of total
    predictions, and the accuracy is calculated as the sum of correct predictions
    divided by the total number of predictions, respectively:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 **错误率** (**ERR**) 和 **准确率** (**ACC**) 都提供关于有多少示例被错误分类的一般信息。错误可以理解为所有错误预测的总和除以总预测数，而准确率分别计算为正确预测的总和除以总预测数：
- en: '![](img/B17582_06_001.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_001.png)'
- en: 'The prediction accuracy can then be calculated directly from the error:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 预测准确率可以直接从误差中计算出来：
- en: '![](img/B17582_06_002.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_002.png)'
- en: 'The **true positive rate** (**TPR**) and **false positive rate** (**FPR**)
    are performance metrics that are especially useful for imbalanced class problems:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**真正率** (**TPR**) 和 **假正率** (**FPR**) 是性能指标，特别适用于不平衡类别问题：'
- en: '![](img/B17582_06_003.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_003.png)'
- en: In tumor diagnosis, for example, we are more concerned about the detection of
    malignant tumors in order to help a patient with the appropriate treatment. However,
    it is also important to decrease the number of benign tumors incorrectly classified
    as malignant (FP) to not unnecessarily concern patients. In contrast to the FPR,
    the TPR provides useful information about the fraction of positive (or relevant)
    examples that were correctly identified out of the total pool of positives (P).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在肿瘤诊断中，我们更关心恶性肿瘤的检测，以帮助患者进行适当的治疗。然而，减少将良性肿瘤错误分类为恶性肿瘤（FP）也同样重要，以免不必要地让患者担心。与
    FPR 相反，TPR 提供了关于已正确识别的正（或相关）示例在总正例（P）池中的分数的有用信息。
- en: 'The performance metrics **precision** (**PRE**) and **recall** (**REC**) are
    related to those TP and TN rates, and in fact, REC is synonymous with TPR:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标 **精确度** (**PRE**) 和 **召回率** (**REC**) 与 TP 和 TN 率有关，实际上，REC 与 TPR 是同义词。
- en: '![](img/B17582_06_004.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_004.png)'
- en: 'In other words, recall quantifies how many of the relevant records (the positives)
    are captured as such (the true positives). Precision quantifies how many of the
    records predicted as relevant (the sum of true and false positives) are actually
    relevant (true positives):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，召回率衡量了多少相关记录（阳性记录）被正确捕捉（真阳性）。精确率则衡量了预测为相关的记录中有多少确实是相关的（真阳性数与假阳性数之和）：
- en: '![](img/B17582_06_005.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_005.png)'
- en: Revisiting the malignant tumor detection example, optimizing for recall helps
    with minimizing the chance of not detecting a malignant tumor. However, this comes
    at the cost of predicting malignant tumors in patients although the patients are
    healthy (a high number of FPs). If we optimize for precision, on the other hand,
    we emphasize correctness if we predict that a patient has a malignant tumor. However,
    this comes at the cost of missing malignant tumors more frequently (a high number
    of FNs).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 再次以恶性肿瘤检测为例，优化召回率有助于最小化未检测到恶性肿瘤的风险。然而，这会导致在健康患者身上预测出恶性肿瘤（假阳性数较高）。另一方面，如果我们优化精确率，则强调当我们预测患者患有恶性肿瘤时的正确性。然而，这将导致更频繁地错过恶性肿瘤（假阴性数较高）。
- en: 'To balance the up- and downsides of optimizing PRE and REC, the harmonic mean
    of PRE and REC is used, the so-called F1 score:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平衡优化精确率和召回率的利弊，使用它们的调和平均数，即所谓的F1分数：
- en: '![](img/B17582_06_006.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_006.png)'
- en: '**Further reading on precision and recall**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多关于精确率和召回率的阅读**'
- en: 'If you are interested in a more thorough discussion of the different performance
    metrics, such as precision and recall, read David M. W. Powers’ technical report
    *Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness
    & Correlation*, which is freely available at [https://arxiv.org/abs/2010.16061](https://arxiv.org/abs/2010.16061).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对如精确率和召回率等不同性能指标的更详细讨论感兴趣，请阅读David M. W. Powers的技术报告《评估：从精确率、召回率和F-Factor到ROC、Informedness、Markedness和相关性》，该报告可以在[https://arxiv.org/abs/2010.16061](https://arxiv.org/abs/2010.16061)免费获取。
- en: 'Lastly, a measure that summarizes a confusion matrix is the MCC, which is especially
    popular in biological research contexts. The MCC is calculated as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，总结混淆矩阵的一种度量是MCC，特别受到生物研究背景中的欢迎。MCC的计算方法如下：
- en: '![](img/B17582_06_007.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_007.png)'
- en: 'In contrast to PRE, REC, and the F1 score, the MCC ranges between –1 and 1,
    and it takes all elements of a confusion matrix into account—for instance, the
    F1 score does not involve the TN. While the MCC values are harder to interpret
    than the F1 score, it is regarded as a superior metric, as described in the following
    article: *The advantages of the Matthews correlation coefficient (MCC) over F1
    score and accuracy in binary classification evaluation* by *D. Chicco* and *G.
    Jurman*, *BMC Genomics*. pp. 281-305, 2012, [https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与PRE、REC和F1分数相比，MCC的范围在-1到1之间，并且考虑了混淆矩阵的所有元素，例如F1分数不涉及TN。虽然MCC值比F1分数更难解释，但它被认为是一个更优越的度量标准，正如*D.
    Chicco*和*G. Jurman*在文章《二分类评估中Matthews相关系数（MCC）优于F1分数和准确度的优势》中描述的那样，《BMC Genomics》。pp.
    281-305, 2012, [https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7)。
- en: 'Those scoring metrics are all implemented in scikit-learn and can be imported
    from the `sklearn.metrics` module as shown in the following snippet:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评分指标都已在scikit-learn中实现，并可以从`sklearn.metrics`模块中导入，如下片段所示：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Furthermore, we can use a different scoring metric than accuracy in the `GridSearchCV`
    via the scoring parameter. A complete list of the different values that are accepted
    by the scoring parameter can be found at [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过scoring参数在`GridSearchCV`中使用不同的评分指标，而不是精度。有关scoring参数接受的不同值的完整列表，请访问[http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)。
- en: 'Remember that the positive class in scikit-learn is the class that is labeled
    as class `1`. If we want to specify a different *positive label*, we can construct
    our own scorer via the `make_scorer` function, which we can then directly provide
    as an argument to the `scoring` parameter in `GridSearchCV` (in this example,
    using the `f1_score` as a metric):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，scikit-learn中的正类是标记为类别`1`的类。如果我们想指定一个不同的*正类标签*，我们可以通过`make_scorer`函数构建自己的评分器，然后直接将其作为参数提供给`GridSearchCV`中的`scoring`参数（在本示例中使用`f1_score`作为度量标准）。
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Plotting a receiver operating characteristic
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制接收者操作特征图
- en: '**Receiver operating characteristic** (**ROC**) graphs are useful tools to
    select models for classification based on their performance with respect to the
    FPR and TPR, which are computed by shifting the decision threshold of the classifier.
    The diagonal of a ROC graph can be interpreted as *random guessing*, and classification
    models that fall below the diagonal are considered as worse than random guessing.
    A perfect classifier would fall into the top-left corner of the graph with a TPR
    of 1 and an FPR of 0\. Based on the ROC curve, we can then compute the so-called
    **ROC area under the curve** (**ROC AUC**) to characterize the performance of
    a classification model.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）图是选择基于分类模型性能的有用工具，与FPR和TPR相关。这些指标是通过改变分类器的决策阈值计算得出的。ROC图的对角线可解释为*随机猜测*，而落在对角线以下的分类模型被认为比随机猜测还差。完美的分类器会落在图的左上角，TPR为1，FPR为0。根据ROC曲线，我们可以计算所谓的**ROC曲线下面积**（**ROC
    AUC**），以描述分类模型的性能。'
- en: Similar to ROC curves, we can compute **precision-recall curves** for different
    probability thresholds of a classifier. A function for plotting those precision-recall
    curves is also implemented in scikit-learn and is documented at [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ROC曲线，我们可以计算分类器不同概率阈值下的**精确度-召回率曲线**。在scikit-learn中，还实现了绘制这些精确度-召回率曲线的函数，并在[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)进行了文档化。
- en: 'Executing the following code example, we will plot a ROC curve of a classifier
    that only uses two features from the Breast Cancer Wisconsin dataset to predict
    whether a tumor is benign or malignant. Although we are going to use the same
    logistic regression pipeline that we defined previously, we are only using two
    features this time. This is to make the classification task more challenging for
    the classifier, by withholding useful information contained in the other features,
    so that the resulting ROC curve becomes visually more interesting. For similar
    reasons, we are also reducing the number of folds in the `StratifiedKFold` validator
    to three. The code is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下代码示例，我们将绘制一个ROC曲线，该曲线用于预测乳腺癌威斯康星数据集中的肿瘤是良性还是恶性，仅使用两个特征。尽管我们将使用先前定义的相同逻辑回归管道，但这次只使用两个特征。这样做是为了使分类任务对分类器更具挑战性，因为我们保留了其他特征中的有用信息，从而使结果的ROC曲线更加有趣。出于类似原因，我们还将`StratifiedKFold`验证器中的折数减少到三。代码如下：
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding code example, we used the already familiar `StratifiedKFold`
    class from scikit-learn and calculated the ROC performance of the `LogisticRegression`
    classifier in our `pipe_lr` pipeline using the `roc_curve` function from the `sklearn.metrics`
    module separately for each iteration. Furthermore, we interpolated the average
    ROC curve from the three folds via the `interp` function that we imported from
    SciPy and calculated the area under the curve via the `auc` function. The resulting
    ROC curve indicates that there is a certain degree of variance between the different
    folds, and the average ROC AUC (0.76) falls between a perfect score (1.0) and
    random guessing (0.5):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用了scikit-learn中的已知`StratifiedKFold`类，并分别使用`sklearn.metrics`模块中的`roc_curve`函数计算了`pipe_lr`管道中`LogisticRegression`分类器的ROC性能，然后通过SciPy中的`interp`函数对三个折叠的平均ROC曲线进行了插值，并通过`auc`函数计算了曲线下面积。得到的ROC曲线表明，不同折叠之间存在一定的变化度，而平均ROC
    AUC（0.76）介于完美分数（1.0）和随机猜测（0.5）之间。
- en: '![Chart, line chart  Description automatically generated](img/B17582_06_11.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图  自动生成描述](img/B17582_06_11.png)'
- en: 'Figure 6.11: The ROC plot'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：ROC图
- en: Note that if we are just interested in the ROC AUC score, we could also directly
    import the `roc_auc_score` function from the `sklearn.metrics` submodule, which
    can be used similarly to the other scoring functions (for example, `precision_score`)
    that were introduced in the previous sections.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们只对 ROC AUC 分数感兴趣，我们也可以直接从 `sklearn.metrics` 子模块中导入 `roc_auc_score` 函数，该函数可以类似于前面介绍的其他评分函数（例如
    `precision_score`）进行使用。
- en: 'Reporting the performance of a classifier as the ROC AUC can yield further
    insights into a classifier’s performance with respect to imbalanced samples. However,
    while the accuracy score can be interpreted as a single cutoff point on a ROC
    curve, A. P. Bradley showed that the ROC AUC and accuracy metrics mostly agree
    with each other: *The Use of the Area Under the ROC Curve in the Evaluation of
    Machine Learning Algorithms* by *A. P. Bradley*, *Pattern Recognition*, 30(7):
    1145-1159, 1997, [https://reader.elsevier.com/reader/sd/pii/S0031320396001422](https://reader.elsevier.com/reader/sd/pii/S0031320396001422).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '报告分类器的性能作为 ROC AUC 可以进一步深入了解分类器对于不平衡样本的性能。然而，虽然准确度分数可以解释为 ROC 曲线上的单个截止点，A.P.
    Bradley 显示 ROC AUC 和准确度指标大部分情况下是一致的：*机器学习算法评估中的 ROC 曲线下面积的使用*，作者 A.P. Bradley，*Pattern
    Recognition*，30(7): 1145-1159，1997，[https://reader.elsevier.com/reader/sd/pii/S0031320396001422](https://reader.elsevier.com/reader/sd/pii/S0031320396001422)。'
- en: Scoring metrics for multiclass classification
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类分类的评分指标
- en: 'The scoring metrics that we’ve discussed so far are specific to binary classification
    systems. However, scikit-learn also implements macro and micro averaging methods
    to extend those scoring metrics to multiclass problems via **one-vs.-all** (**OvA**)
    classification. The micro-average is calculated from the individual TPs, TNs,
    FPs, and FNs of the system. For example, the micro-average of the precision score
    in a *k*-class system can be calculated as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的评分指标特定于二元分类系统。然而，scikit-learn 也通过一对所有（**OvA**）分类实现了宏平均和微平均方法，以将这些评分指标扩展到多类问题。微平均是从系统的各个
    TP、TN、FP 和 FN 中计算出来的。例如，在 *k* 类系统中，精度得分的微平均可以计算如下：
- en: '![](img/B17582_06_008.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_008.png)'
- en: 'The macro-average is simply calculated as the average scores of the different
    systems:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 宏平均简单地计算为不同系统的平均得分：
- en: '![](img/B17582_06_009.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_06_009.png)'
- en: Micro-averaging is useful if we want to weight each instance or prediction equally,
    whereas macro-averaging weights all classes equally to evaluate the overall performance
    of a classifier with regard to the most frequent class labels.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Micro-averaging 很有用，如果我们想平等地加权每个实例或预测，而宏平均则权衡所有类别，以评估分类器在最常见的类别标签方面的整体性能。
- en: If we are using binary performance metrics to evaluate multiclass classification
    models in scikit-learn, a normalized or weighted variant of the macro-average
    is used by default. The weighted macro-average is calculated by weighting the
    score of each class label by the number of true instances when calculating the
    average. The weighted macro-average is useful if we are dealing with class imbalances,
    that is, different numbers of instances for each label.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 scikit-learn 中使用二元性能指标来评估多类分类模型，通常默认使用加权宏平均的规范化或加权变体。加权宏平均通过在计算平均值时将每个类别标签的得分按真实实例的数量加权来计算。如果我们处理类别不平衡，即每个标签具有不同数量的实例，加权宏平均非常有用。
- en: 'While the weighted macro-average is the default for multiclass problems in
    scikit-learn, we can specify the averaging method via the `average` parameter
    inside the different scoring functions that we import from the `sklearn.metrics`
    module, for example, the `precision_score` or `make_scorer` functions:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 scikit-learn 中，加权宏平均是多类问题的默认设置，但我们可以通过 `average` 参数在从 `sklearn.metrics`
    模块导入的不同评分函数中指定平均方法，例如 `precision_score` 或 `make_scorer` 函数：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Dealing with class imbalance
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理类别不平衡
- en: We’ve mentioned class imbalances several times throughout this chapter, and
    yet we haven’t actually discussed how to deal with such scenarios appropriately
    if they occur. Class imbalance is a quite common problem when working with real-world
    data—examples from one class or multiple classes are over-represented in a dataset.
    We can think of several domains where this may occur, such as spam filtering,
    fraud detection, or screening for diseases.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章节中多次提到了类别不平衡问题，但实际上并未讨论如何在发生这种情况时适当地处理。在处理真实世界数据时，类别不平衡是一个非常常见的问题——数据集中某一类或多个类的示例数目过多。我们可以想象几个领域可能会出现这种情况，例如垃圾邮件过滤、欺诈检测或疾病筛查。
- en: Imagine that the Breast Cancer Wisconsin dataset that we’ve been working with
    in this chapter consisted of 90 percent healthy patients. In this case, we could
    achieve 90 percent accuracy on the test dataset by just predicting the majority
    class (benign tumor) for all examples, without the help of a supervised machine
    learning algorithm. Thus, training a model on such a dataset that achieves approximately
    90 percent test accuracy would mean our model hasn’t learned anything useful from
    the features provided in this dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在本章节中我们使用的威斯康星州乳腺癌数据集中，90% 的患者是健康的。在这种情况下，我们可以通过只预测多数类别（良性肿瘤）来在测试数据集上达到90%
    的准确率，而不需要有监督的机器学习算法的帮助。因此，在这样的数据集上训练模型，使其在测试集上达到约90% 的准确率，意味着我们的模型并未从提供的特征中学到有用的信息。
- en: 'In this section, we will briefly go over some of the techniques that could
    help with imbalanced datasets. But before we discuss different methods to approach
    this problem, let’s create an imbalanced dataset from our dataset, which originally
    consisted of 357 benign tumors (class `0`) and 212 malignant tumors (class `1`):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍一些技术，可以帮助处理不平衡的数据集。但在讨论解决这个问题的不同方法之前，让我们从我们的数据集中创建一个不平衡的数据集，该数据集最初由
    357 例良性肿瘤（类别 `0`）和 212 例恶性肿瘤（类别 `1`）组成：
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Thus, when we fit classifiers on such datasets, it would make sense to focus
    on other metrics than accuracy when comparing different models, such as precision,
    recall, the ROC curve—whatever we care most about in our application. For instance,
    our priority might be to identify the majority of patients with malignant cancer
    to recommend an additional screening, so recall should be our metric of choice.
    In spam filtering, where we don’t want to label emails as spam if the system is
    not very certain, precision might be a more appropriate metric.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们对这类数据集拟合分类器时，与其比较不同模型的准确率，更有意义的是专注于其他指标，如精确度、召回率、ROC 曲线——这些指标与我们应用场景的关注点密切相关。例如，我们的首要任务可能是识别大多数患有恶性癌症的患者，以建议进行额外的筛查，因此召回率应成为我们选择的度量标准。在垃圾邮件过滤中，如果系统不确定时不希望将邮件标记为垃圾邮件，则精确度可能是更合适的度量标准。
- en: Aside from evaluating machine learning models, class imbalance influences a
    learning algorithm during model fitting itself. Since machine learning algorithms
    typically optimize a reward or loss function that is computed as a sum over the
    training examples that it sees during fitting, the decision rule is likely going
    to be biased toward the majority class.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估机器学习模型外，类别不平衡还会影响模型拟合过程中的学习算法。由于机器学习算法通常优化一个奖励或损失函数，该函数是在拟合过程中对训练样本求和计算得到的，因此决策规则很可能会偏向于多数类别。
- en: In other words, the algorithm implicitly learns a model that optimizes the predictions
    based on the most abundant class in the dataset to minimize the loss or maximize
    the reward during training.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，算法隐式学习一个模型，该模型根据数据集中最丰富的类别优化预测，以在训练期间最小化损失或最大化奖励。
- en: One way to deal with imbalanced class proportions during model fitting is to
    assign a larger penalty to wrong predictions on the minority class. Via scikit-learn,
    adjusting such a penalty is as convenient as setting the `class_weight` parameter
    to `class_weight='balanced'`, which is implemented for most classifiers.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型拟合过程中处理类别不平衡的一种方法是对少数类别的错误预测赋予更大的惩罚。通过 scikit-learn，调整这种惩罚只需将 `class_weight`
    参数设置为 `class_weight='balanced'`，对于大多数分类器都已实现。
- en: Other popular strategies for dealing with class imbalance include upsampling
    the minority class, downsampling the majority class, and the generation of synthetic
    training examples. Unfortunately, there’s no universally best solution or technique
    that works best across different problem domains. Thus, in practice, it is recommended
    to try out different strategies on a given problem, evaluate the results, and
    choose the technique that seems most appropriate.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 处理类别不平衡的其他流行策略包括上采样少数类、下采样多数类以及生成合成训练示例。不幸的是，并不存在适用于所有不同问题域的普遍最佳解决方案或技术。因此，在实践中，建议在给定问题上尝试不同的策略，评估结果，并选择最合适的技术。
- en: 'The scikit-learn library implements a simple `resample` function that can help
    with the upsampling of the minority class by drawing new samples from the dataset
    with replacement. The following code will take the minority class from our imbalanced
    Breast Cancer Wisconsin dataset (here, class `1`) and repeatedly draw new samples
    from it until it contains the same number of examples as class label `0`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 库实现了一个简单的 `resample` 函数，可以通过从数据集中有放回地抽取新样本来帮助上采样少数类。以下代码将从我们不平衡的
    Breast Cancer Wisconsin 数据集中获取少数类（这里是类别 `1`），并重复地抽取新样本，直到它包含与类别 `0` 相同数量的示例：'
- en: '[PRE30]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After resampling, we can then stack the original class `0` samples with the
    upsampled class `1` subset to obtain a balanced dataset as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在重采样之后，我们可以将原始的类别 `0` 样本与上采样的类别 `1` 子集堆叠，以获得一个平衡的数据集，如下所示：
- en: '[PRE31]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Consequently, a majority vote prediction rule would only achieve 50 percent
    accuracy:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用多数投票预测规则只能达到 50% 的准确率：
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Similarly, we could downsample the majority class by removing training examples
    from the dataset. To perform downsampling using the `resample` function, we could
    simply swap the class `1` label with class `0` in the previous code example and
    vice versa.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过从数据集中移除训练示例来对多数类进行下采样。要使用 `resample` 函数执行下采样，我们可以简单地在前面的代码示例中交换类别
    `1` 标签和类别 `0`，反之亦然。
- en: '**Generating new training data to address class imbalance**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成新的训练数据以解决类别不平衡问题**'
- en: 'Another technique for dealing with class imbalance is the generation of synthetic
    training examples, which is beyond the scope of this book. Probably the most widely
    used algorithm for synthetic training data generation is **Synthetic Minority
    Over-sampling Technique** (**SMOTE**), and you can learn more about this technique
    in the original research article by *Nitesh Chawla* and others: *SMOTE: Synthetic
    Minority Over-sampling Technique*, *Journal of Artificial Intelligence Research*,
    16: 321-357, 2002*,* which is available at[https://www.jair.org/index.php/jair/article/view/10302](https://www.jair.org/index.php/jair/article/view/10302).
    It is also highly recommended to check out imbalanced-learn, a Python library
    that is entirely focused on imbalanced datasets, including an implementation of
    SMOTE. You can learn more about imbalanced-learn at [https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '处理类别不平衡的另一种技术是生成合成训练示例，这超出了本书的范围。可能是最广泛使用的合成训练数据生成算法是**Synthetic Minority Over-sampling
    Technique**（**SMOTE**），您可以在*Nitesh Chawla*等人的原始研究文章*SMOTE: Synthetic Minority
    Over-sampling Technique*，*Journal of Artificial Intelligence Research*，16: 321-357，2002年中了解更多信息，链接在[https://www.jair.org/index.php/jair/article/view/10302](https://www.jair.org/index.php/jair/article/view/10302)。同时强烈建议查看
    imbalanced-learn，这是一个完全专注于不平衡数据集的 Python 库，包括 SMOTE 的实现。您可以在[https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)了解更多关于
    imbalanced-learn 的信息。'
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: At the beginning of this chapter, we discussed how to chain different transformation
    techniques and classifiers in convenient model pipelines that help us to train
    and evaluate machine learning models more efficiently. We then used those pipelines
    to perform k-fold cross-validation, one of the essential techniques for model
    selection and evaluation. Using k-fold cross-validation, we plotted learning and
    validation curves to diagnose common problems of learning algorithms, such as
    overfitting and underfitting.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们讨论了如何在便捷的模型管道中串联不同的转换技术和分类器，这帮助我们更有效地训练和评估机器学习模型。然后我们使用这些管道执行了 k 折交叉验证，这是模型选择和评估的基本技术之一。使用
    k 折交叉验证，我们绘制了学习曲线和验证曲线，以诊断学习算法的常见问题，如过拟合和欠拟合。
- en: Using grid search, randomized search, and successive halving, we further fine-tuned
    our model. We then used confusion matrices and various performance metrics to
    evaluate and optimize a model’s performance for specific problem tasks. Finally,
    we concluded this chapter by discussing different methods for dealing with imbalanced
    data, which is a common problem in many real-world applications. Now, you should
    be well equipped with the essential techniques to build supervised machine learning
    models for classification successfully.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步通过网格搜索、随机搜索和逐步缩减法对我们的模型进行了精细调整。然后，我们使用混淆矩阵和各种性能指标来评估和优化模型在特定问题任务中的性能。最后，我们讨论了处理不平衡数据的不同方法，这在许多现实世界的应用中是一个常见问题。现在，您应该已经掌握了构建成功的监督机器学习分类模型所需的基本技术。
- en: 'In the next chapter, we will look at ensemble methods: methods that allow us
    to combine multiple models and classification algorithms to boost the predictive
    performance of a machine learning system even further.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨集成方法：这些方法允许我们结合多个模型和分类算法，进一步提高机器学习系统的预测性能。
- en: Join our book’s Discord space
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 每月与作者进行*问答*活动的书籍Discord工作空间：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
