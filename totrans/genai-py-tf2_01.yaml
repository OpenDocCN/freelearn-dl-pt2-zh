- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: 'An Introduction to Generative AI: "Drawing" Data from Models'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成人工智能简介：“从模型中“勾画”数据
- en: 'In this chapter, we will dive into the various applications of generative models.
    Before that, we will take a step back and examine how exactly generative models
    are different from other types of machine learning. The difference lies with the
    basic units of any machine learning algorithm: probability and the various ways
    we use mathematics to quantify the shape and distribution of data we encounter
    in the world.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨生成模型的各种应用。在此之前，我们将退后一步，详细研究生成模型与其他类型的机器学习的区别。区别在于任何机器学习算法的基本单位：概率以及我们用数学量化我们遇到的数据的形状和分布的各种方法。
- en: 'In the rest of this chapter, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将 covers：
- en: Applications of AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI的应用
- en: Discriminative and generative models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别和生成模型
- en: Implementing generative models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施生成模型
- en: The rules of probability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率的规则
- en: Why use generative models?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用生成模型？
- en: Unique challenges of generative models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型的独特挑战
- en: Applications of AI
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI的应用
- en: In New York City in October 2018, the international auction house Christie's
    sold the **Portrait of Edmond Belamy**(*Figure 1.1*) during the show **Prints
    & Multiples**for $432,500.00\. This sale was remarkable both because the sale
    price was 45 times higher than the initial estimates for the piece, and due to
    the unusual origin of this portrait. Unlike the majority of other artworks sold
    by Christie's since the 18th century, the **Portrait of Edmond Belamy** is not
    painted using oil or watercolors, nor is its creator even human; rather, it is
    an entirely digital image produced by a sophisticated machine learning algorithm.
    The creators—a Paris-based collective named Obvious—used a collection of 15,000
    portraits created between the 14^(th) and 20^(th) centuries to tune an artificial
    neural network model capable of generating aesthetically similar, albeit synthetic,
    images.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年10月的纽约，国际拍卖行佳士得在**印刷品与多重品**展销会上以43.25万美元的价格出售了**埃德蒙·贝拉米的肖像**(*图1.1*)。这次销售之所以引人注目，既因为销售价格比这件作品的初步估计高出45倍，也因为这幅肖像的非同寻常的起源。与佳士得自18世纪以来出售的大多数其他艺术品不同，**埃德蒙·贝拉米的肖像**既不是用油画或水彩画完成的，其创作者甚至不是人类；相反，它是由一个复杂的机器学习算法完全产生的数字图像。创作者——一个名为Obvious的巴黎集体利用了从14世纪到20世纪创作的1.5万幅肖像的集合来调整一个能够生成美学上类似但合成的图像的人工神经网络模型。
- en: '![](img/B16176_01_01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_01.png)'
- en: 'Figure 1.1: The Portrait of Edmond Belamy¹'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：埃德蒙·贝拉米的肖像¹
- en: Portraiture is far from the only area in which machine learning has demonstrated
    astonishing results. Indeed, if you have paid attention to the news in the last
    few years, you have likely seen many stories about the ground-breaking results
    of modern AI systems applied to diverse problems, from the hard sciences to digital
    art. Deep neural network models, such as the one created by Obvious, can now classify
    X-ray images of human anatomy on the level of trained physicians,² beat human
    masters at both classic board games such as Go (an Asian game similar to chess)³
    and multiplayer computer games,⁴ and translate French into English with amazing
    sensitivity to grammatical nuances.⁵
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 画像远非唯一一个展示机器学习惊人成果的领域。事实上，如果你在过去几年关注新闻，你可能已经看到许多关于现代AI系统应用于各种问题的开创性成果的故事，从硬科学到数字艺术。像Obvious创建的深度神经网络模型现在可以在训练有素的医生水平上对人体解剖的X光图像进行分类，²在传统棋类游戏如围棋（一种类似国际象棋的亚洲游戏）³和多人游戏⁴中战胜人类大师，并且对法语翻译成英语时对语法细微差别表现出惊人的敏感性。⁵
- en: Discriminative and generative models
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别模型和生成模型
- en: These other examples of AI differ in an important way from the model that generated
    **The Portrait of Edmond Belamy**. In all of these other applications, the model
    is presented with a set of inputs—data such as English text, images from X-rays,
    or the positions on a gameboard—that is paired with a target output, such as the
    next word in a translated sentence, the diagnostic classification of an X-ray,
    or the next move in a game. Indeed, this is probably the kind of AI model you
    are most familiar with from prior experiences of predictive modeling; they are
    broadly known as **discriminative**models, whose purpose is to create a mapping
    between a set of input variables and a target output. The target output could
    be a set of discrete classes (such as which word in the English language appears
    next in a translation), or a continuous outcome (such as the expected amount of
    money a customer will spend in an online store over the next 12 months).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些人工智能的其他示例与生成**埃德蒙·贝拉米的肖像**的模型在一个重要的方面有所不同。在所有这些其他应用中，模型被呈现一组输入数据，例如英文文本、X射线图像或游戏棋盘上的位置，这些数据与目标输出配对，例如翻译句子中的下一个词、X射线图的诊断分类或游戏中的下一步。事实上，这可能是您在以往预测建模经验中最熟悉的AI模型类型；它们被广泛称为**判别**模型，其目的是在一组输入变量和目标输出之间创建映射。目标输出可以是一组离散类别（例如在翻译中下一个出现的英文单词），也可以是连续结果（例如预期一个客户在接下来的12个月内在在线商店中的消费金额）。
- en: It should be noted that this kind of model, in which data is **labeled** or
    **scored**, represents only half the capabilities of modern machine learning.
    Another class of algorithms, such as the one that generated the artificial portrait
    sold at Christie's, don't compute a score or label from input variables, but rather
    **generate new data**. Unlike discriminative models, the input variables are often
    vectors of numbers that aren't related to real-world values at all, and are often
    even randomly generated. This kind of model—known as a **generative model**—can
    produce complex outputs such as text, music, or images from random noise, and
    is the topic of this book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，这种模型，其中数据被**标记**或**评分**，仅代表了现代机器学习能力的一半。另一类算法，比如生成了在佳士得拍卖会上出售的人造肖像的算法，不会从输入变量中计算得分或标签，而是**生成新数据**。与判别模型不同，输入变量通常是与现实世界值无关的数字向量，甚至经常是随机生成的。这种模型——被称为**生成模型**——可以从随机噪声中产生复杂的输出，例如文本、音乐或图像，并且是本书的主题。
- en: Even if you didn't know it at the time, you have probably seen other instances
    of generative models in the news alongside the discriminative examples given earlier.
    A prominent example is deep fakes, which are videos in which one person's face
    has been systematically replaced with another's by using a neural network to remap
    the pixels.⁶
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您当时不知道，您可能也曾在新闻中看到其他生成模型的实例，与前面提到的判别示例并列。一个突出的例子是深度伪造，这是一种视频，其中一个人的脸被系统地替换为另一个人的脸，通过使用神经网络重新映射像素。⁶
- en: '![](img/B16176_01_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_02.png)'
- en: 'Figure 1.2: A deep fake image⁷'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：一个深度伪造图像⁷
- en: Maybe you have also seen stories about AI models that generate **fake news**,
    which scientists at the firm OpenAI were initially terrified to release to the
    public due to concerns they could be used to create propaganda and misinformation
    online.⁸
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 也许您还看到过关于AI模型生成**虚假新闻**的报道，最初由OpenAI公司的科学家因担心其可能被用于在线制造宣传和误导而感到恐慌。⁸
- en: '![](img/B16176_01_03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_03.png)'
- en: 'Figure 1.3: A chatbot dialogue created using GPT-2⁹'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：使用GPT-2创建的聊天机器人对话⁹
- en: In these and other applications, such as Google's voice assistant Duplex, which
    can make a restaurant reservation by dynamically creating a conversation with
    a human in real time,^(10) or software that can generate original musical compositions,^(11)
    we are surrounded by the outputs of generative AI algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些以及其他应用中，例如谷歌的语音助手Duplex，它可以通过与人类实时动态创建对话来进行餐厅预订^(10)，或者可以生成原创音乐作品的软件^(11)，我们被环绕着生成式人工智能算法的输出。
- en: '![](img/B16176_01_04.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_04.png)'
- en: 'Figure 1.4: Examples of style transfer using Generative Adversarial Networks
    (GANs)^(12)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：使用生成对抗网络（GANs）进行风格迁移的示例^(12)
- en: 'These models are able to handle complex information in a variety of domains:
    creating photorealistic images or stylistic **filters** on pictures (*Figure 1.4*),
    synthetic sound, conversational text, and even rules for optimally playing video
    games. You might ask, where did these models come from? How can I implement them
    myself?We will discuss more on that in the next section.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型能够处理各种领域的复杂信息：创建逼真的图像或照片上的**滤镜** (*图1.4*)，合成声音，对话文本，甚至是最佳玩视频游戏的规则。你可能会问，这些模型从哪里来？我如何能自己实现它们？我们将在下一节中更多地讨论这个问题。
- en: Implementing generative models
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现生成模型
- en: While generative models could theoretically be implemented using a wide variety
    of machine learning algorithms, in practice, they are usually built with deep
    neural networks, which are well suited to capturing complex variations in data
    such as images or language.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成模型理论上可以使用各种机器学习算法来实现，但在实践中，它们通常是通过深度神经网络构建的，这些网络非常适合捕捉图像或语言等数据的复杂变化。
- en: In this book, we will focus on implementing these deep generative models for
    many different applications using **TensorFlow 2.0**. TensorFlow is a C++ framework,
    with APIs in the Python programming language, used to develop and productionize
    deep learning models. It was open sourced by Google in 2013, and has become one
    of the most popular libraries for the research and deployment of neural network
    models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将专注于使用**TensorFlow 2.0**来实现这些深度生成模型的许多不同应用。TensorFlow是一个C++框架，有Python编程语言的API，用于开发和应用深度学习模型。它是谷歌在2013年开源的，并且已经成为研发和部署神经网络模型的最受欢迎的库之一。
- en: With the 2.0 release, much of the boilerplate code that characterized development
    in earlier versions of the library was cleaned up with high-level abstractions,
    allowing us to focus on the model rather than the plumbing of the computations.
    The latest version also introduced the concept of **eager** execution, allowing
    network computations to be run on demand, which will be an important benefit of implementing
    some of our models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2.0版的发布，以前版本中开发的那些样板代码得到了清理，使用了高级抽象层，使我们能够专注于模型而不是计算过程的其它部分。最新版本还引入了**即时执行**的概念，允许网络计算按需运行，这将是我们实现某些模型的重要好处。
- en: In upcoming chapters, you will learn not only the underlying theory behind these
    models, but the practical skills needed to implement them in popular programming
    frameworks. In *Chapter 2*, *Setting up a TensorFlow Lab*, you will learn how
    to set up a cloud environment that will allow you to run TensorFlow in a distributed
    fashion, using the **Kubeflow** framework to catalog your experiments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习不仅是这些模型背后的理论，还有在流行的编程框架中实现它们所需的实际技能。在*第二章*，*建立一个TensorFlow实验室*中，你将学习如何设置一个云环境，以便你可以使用**Kubeflow**框架来分布式运行TensorFlow，并记录你的实验。
- en: Indeed, as I will describe in more detail in *Chapter 3*, *Building Blocks of
    Deep Neural Networks*, since 2006 an explosion of research into **deep** **learning**
    using large neural network models has produced a wide variety of generative modeling
    applications. The first of these was the **restricted Boltzmann machine**, which
    is stacked in multiple layers to create a **deep belief network**. I will describe
    both of these models in *Chapter 4*, *Teaching Networks to Generate Digits*. Later
    innovations included **Variational Autoencoders** (**VAEs**), which can efficiently
    generate complex data samples from random numbers, using techniques that I will
    describe in *Chapter 5*, *Painting Pictures with Neural Networks Using VAEs*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，正如我将在*第三章*，*深度神经网络的构建模块*中更详细地描述的那样，自2006年以来，大规模神经网络模型的**深度学习**研究已经产生了各种各样的生成模型应用。其中第一个是**受限玻尔兹曼机**，它被堆叠在多个层中以创建**深度信念网络**。我将在*第四章*，*教网络生成数字*中描述这两种模型。后来的创新包括**变分自动编码器**（**VAEs**），它们可以有效地从随机数生成复杂的数据样本，我将在*第五章*，*使用VAEs用神经网络绘制图片*中描述这些技术。
- en: We will also cover the algorithm used to create **The Portrait of Edmond Belamy**,
    the GAN, in more detail in *Chapter 6*, *Image Generation with GANs*, of this
    book. Conceptually, the GANmodel creates a competition between two neural networks.
    One (termed the **generator**) produces realistic (or, in the case of the experiments
    by Obvious, artistic) images starting from a set of random numbers and applying
    a mathematical transformation. In a sense, the generator is like an art student,
    producing new paintings from brushstrokes and creative inspiration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The second network, known as the **discriminator**, attempts to classify whether
    a picture comes from a set of real-world images, or whether it was created by
    the generator. Thus, the discriminator acts like a teacher, grading whether the
    student has produced work comparable to the paintings they are attempting to mimic.
    As the generator becomes better at fooling the discriminator, its output becomes
    closer and closer to the historical examples it is designed to copy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many classes of GAN models, with additional variants covered in *Chapter
    7*, *Style Transfer with GANs*, and *Chapter 11*,*Composing Music with Generative
    Models*, in our discussion of advanced models. Another key innovation in generative
    models is in the domain of natural language data. By representing the complex
    interrelationship between words in a sentence in a computationally scalable way,
    the Transformer network and the **Bidirectional Encoder from Transformers** (**BERT**)
    model built on top of it present powerful building blocks to generate textual
    data in applications such as chatbots, which we''ll cover in more detail in *Chapter
    9*,*The Rise of Methods for Text Generation*, and *Chapter 10*,*NLP 2.0: Using
    Transformers to Generate Text*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 12*, *Play Video Games with Generative AI: GAIL*, you will also
    see how models such as GANs and VAEs can be used to generate not just images or
    text, but sets of rules that allow game-playing networks developed with reinforcement
    learning algorithms to process and navigate their environment more efficiently—in essence,
    learning to learn. Generative models are a huge field of research that is constantly
    growing, so unfortunately, we can''t cover every topic in this book. For the interested
    reader, references to further topics will be provided in *Chapter 13*,*Emerging
    Applications in Generative AI*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: To get started with some background information, let's discuss the rules of
    probability.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The rules of probability
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the simplest level, a model, be it for machine learning or a more classical
    method such as linear regression, is a mathematical description of how various
    kinds of data relate to one another.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'In the task of modeling, we usually think about separating the variables of
    our dataset into two broad classes:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent data**, which primarily means **inputs** to a model, are denoted
    by *X*. These could be categorical features (such as a "0" or "1" in six columns
    indicating which of six schools a student attends), continuous (such as the heights
    or test scores of the same students), or ordinal (the rank of a student in the
    class).'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**独立数据**，主要是指模型的 **输入**，用 *X* 表示。这些可以是分类特征（例如某学生所在的六所学校中的“0”或“1”），连续的（例如相同学生的身高或考试成绩），或序数的（班级中学生的排名）。'
- en: '**Dependent data**, conversely, are the outputs of our models, and are denoted
    by *Y*. (Note that in some cases *Y* is a **label** that can be used to condition
    a generative output, such as in a conditional GAN.) As with the independent variables,
    these can be continuous, categorical, or ordinal, and they can be an individual
    element or multidimensional matrix (tensor) for each element of the dataset.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**依赖数据** 相反，则是我们模型的输出，用 *Y* 表示。（请注意，在某些情况下，*Y* 是一个 **标签**，可用于条件产生输出，例如在条件GAN中。）与自变量一样，这些可以是连续的、分类的或序数的，它们可以是数据集每个元素的个体元素或多维矩阵（张量）。'
- en: 'So how can we describe the data in our model using statistics? In other words,
    how can we quantitatively describe what values we are likely to see, and how frequently,
    and which values are more likely to appear together? One way is by asking the
    likelihood of observing a particular value in the data, or the probability of
    that value. For example, if we were to ask what the probability is of observing
    a roll of 4 on a six-sided die, the answer is that, on average, we would observe
    a 4 once every six rolls. We write this as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何使用统计描述我们模型中的数据呢？换句话说，我们如何定量描述我们可能看到的值，以及频率如何，哪些值更有可能一起出现？一种方式是询问数据中观察特定值的可能性，或者该值的概率。例如，如果我们想知道掷六面骰子出现4的概率是多少，答案是平均来说，我们会在六次掷骰子中看到一次4。我们写为：
- en: '*P(X=4) =* *⅙* *= 16.67%*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X=4) =* *⅙* *= 16.67%*'
- en: where *P* denotes *probability of*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P* 表示 *概率为*。
- en: What defines the allowed probability values for a particular dataset? If we
    imagine the set of all possible values of a dataset, such as all values of a die,
    then a probability maps each value to a number between 0 and 1\. The minimum is
    0 because we can't have a negative chance of seeing a result; the most unlikely
    result is that we would never see a particular value, or 0% probability, such
    as rolling a 7 on a six-sided die. Similarly, we can't have greater than 100%
    probability of observing a result, represented by the value 1; an outcome with
    probability 1 is absolutely certain. This set of probability values associated
    with a dataset belong to discrete classes (such as the faces of a die) or an infinite
    set of potential values (such as variations in height or weight). In either case,
    however, these values have to follow certain rules, the **Probability Axioms**
    described by the mathematician Andrey Kolmogorov in 1933:^(13)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 什么定义了特定数据集的允许概率值？如果我们想象数据集所有可能值的集合，比如掷骰子的所有值，那么概率将每个值映射到0和1之间的数。最小值是0，因为我们不可能有看到结果的负概率；最不可能的结果是我们永远不会看到特定值的机会，或者是0%的概率，比如在六面骰子上掷出7。同样，我们也不可能有大于100%的观察结果概率，其值为1；概率为1的结果是绝对确定的。与数据集相关联的这组概率值属于离散类（例如骰子的面）或无限潜在值的集合（例如身高或体重的变化）。不过，在任一情况下，这些值必须遵循某些规则，这些规则是由数学家安德烈·科尔莫戈洛夫在1933年描述的
    **概率公理**。
- en: The probability of an observation (a die role, a particular height, and so on)
    is a non-negative, finite number between 0 and 1.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察的概率（掷骰子点数、特定身高等）是0到1之间的非负有限数。
- en: The probability of *at least one* of the observations in the space of all possible
    observations occurring is 1.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有可能观察空间中至少出现一项观察结果的概率是1。
- en: The joint probability of distinct, mutually exclusive events is the sum of the
    probability of the individual events.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同、互斥事件的联合概率是各个事件概率的总和。
- en: While these rules might seem abstract, you will see in *Chapter 3*, *Building
    Blocks of Deep Neural Networks*, that they have direct relevance to developing
    neural network models. For example, an application of rule 1 is to generate the
    probability between 1 and 0 for a particular outcome in a **softmax** function
    for predicting target classes. Rule 3 is used to normalize these outcomes into
    the range 0-1, under the guarantee that they are mutually distinct predictions
    of a deep neural network (in other words, a real-world image logically can't be
    classified as both a dog and a cat, but rather a dog or a cat, with the probability
    of these two outcomes additive). Finally, the second rule provides the theoretical
    guarantees that we can generate data at all using these models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些规则可能看起来抽象，但你将会在*第三章*，*深度神经网络的基础组件*中看到，它们与开发神经网络模型直接相关。例如，规则1的一个应用是在预测目标类别的**softmax**函数中生成介于1和0之间的概率。规则3用于将这些结果归一化到0-1范围内，保证它们是深度神经网络的相互独立的预测（换句话说，实际世界中的图像逻辑上不能同时被分类为狗和猫，而只能是狗或猫，这两个结果的概率是可加的）。最后，第二条规则提供了我们可以使用这些模型生成数据的理论保证。
- en: 'However, in the context of machine learning and modeling, we are not usually
    interested in just the probability of observing a piece of input data, *X*; we
    instead want to know the **conditional** probability of an outcome, *Y*, given
    the data, *X*. In other words, we want to know how likely a label is for a set
    of data, based on that data. We write this as *the probability of Y given X,*
    or *the probability of Y conditional on X*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习和建模的背景下，我们通常不只是对观察到一条输入数据的概率*X*感兴趣；我们更想知道，基于这些数据，*Y*的**条件**概率是多少。换句话说，我们想知道基于数据对一组数据的标签有多大可能性。我们将此表示为*给定X的Y的概率*，或者*给定X条件下的Y的概率*：
- en: '*P(Y|X)*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Y|X)*'
- en: 'Another question we could ask about *Y* and *X* is how likely they are to occur
    together or their **joint probability**, which can be expressed using the preceding
    conditional probability expression as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以询问关于*Y*和*X*的另一个问题，即它们一起发生或它们的**联合概率**有多大，这可以使用前面的条件概率表达式表示如下：
- en: '*P(X, Y) = P(Y|X)P(X) = P(X|Y)(Y)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X, Y) = P(Y|X)P(X) = P(X|Y)(Y)*'
- en: 'This formula expressed *the probability of X and Y*. In the case of *X* and
    *Y* being completely independent of one another, this is simply their product:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此公式表示*X和Y的概率*。在*X*和*Y*完全独立的情况下，这就是它们的乘积：
- en: '*P(X|Y)P(Y) = P(Y|X)P(X) = P(X)P(Y)*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X|Y)P(Y) = P(Y|X)P(X) = P(X)P(Y)*'
- en: You will see that these expressions become important in our discussion of **complementary
    priors** in *Chapter 4*, *Teaching Networks to Generate Digits*, and the ability
    of **restricted Boltzmann machines** to simulate independent data samples. They
    are also important as building blocks of Bayes' theorem, which we will discuss next.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，这些表达式在我们讨论*第四章*，*教授网络生成数字*中的**补充先验**和**受限玻尔兹曼机**模拟独立数据样本的能力中变得重要。它们也是贝叶斯定理的构建模块，我们将在下一节中讨论。
- en: Discriminative and generative modeling and Bayes' theorem
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别建模和生成建模以及贝叶斯定理
- en: Now let's consider how these rules of conditional and joint probability relate
    to the kinds of predictive models that we build for various machine learning applications.
    In most cases—such as predicting whether an email is fraudulent or the dollar
    amount of the future lifetime value of a customer—we are interested in the conditional
    probability, *P(Y|X=x)*, where *Y* is the set of outcomes we are trying to model,
    *X* represents the input features, and *x* is a particular value of the input
    features. As discussed, this approach is known as **discriminative modeling**.^(14)
    Discriminative modeling attempts to learn a direct mapping between the data, *X*,
    and the outcomes, *Y*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑这些条件和联合概率规则如何与我们为各种机器学习应用构建的预测模型相关联。在大多数情况下——比如预测电子邮件是否欺诈性或客户未来生命周期价值的美元金额——我们对条件概率*P(Y|X=x)*感兴趣，其中*Y*是我们试图建模的结果集，*X*表示输入特征，*x*是输入特征的特定值。如前所述，这种方法被称为**判别建模**。^(14)判别建模试图学习数据*X*与结果*Y*之间的直接映射。
- en: 'Another way to understand discriminative modeling is in the context of Bayes''
    theorem,^(15) which relates the conditional and joint probabilities of a dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解判别建模的方法是在贝叶斯定理的背景下，^(15)它关联了数据集的条件和联合概率：
- en: '*P(Y|X) = P(X|Y)P(Y)/P(X) = P(X, Y)/P(X)*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Y|X) = P(X|Y)P(Y)/P(X) = P(X, Y)/P(X)*'
- en: In Bayes' formula, the expression *P(X|Y)/P(X)* is known as the **likelihood**
    or the supporting evidence that the observation *X* gives to the likelihood of
    observing *Y*. *P(Y)* is the **prior**or the plausibility of the outcome, and
    *P(Y|X)* is the **posterior** or the probability of the outcome given all the
    independent data we have observed related to the outcome thus far. Conceptually,
    Bayes' theorem states that the probability of an outcome is the product of its
    baseline probability and the probability of the input data conditional on this
    outcome.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯公式中，表达式*P(X|Y)/P(X)*被称为**似然**或观察到的*X*给出*Y*观察概率的支持证据。*P(Y)*是**先验**或结果的合理性，*P(Y|X)*是**后验**或给出到目前为止与结果相关的所有独立数据的观察概率。概念上，贝叶斯定理表明结果的概率是其基线概率与此结果的输入数据条件概率的乘积。
- en: The theorem was published two years after the author's death, and in a foreword
    Richard Price described it as a mathematical argument for the existence of God,
    which was perhaps appropriate given that Thomas Bayes served as a reverend during
    his life.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理是作者去世两年后发表的，在前言中理查德·普赖斯描述它为上帝存在的一个数学论证，这或许是适当的，因为托马斯·贝叶斯在生前是一位牧师。
- en: 'In the context of discriminative learning, we can thus see that a discriminative
    model directly computes the posterior; we could have a model of the likelihood
    or prior, but it is not required in this approach. Even though you may not have
    realized it, most of the models you have probably used in the machine learning
    toolkit are discriminative, such as the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在判别学习的背景下，我们可以看到判别模型直接计算后验概率；我们可以有似然或先验模型，但在这种方法中并不需要。即使你可能没有意识到，你可能在机器学习工具包中使用的大多数模型都是判别模型，例如以下模型：
- en: Linear regression
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Random forests
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient-boosted decision trees (GBDT)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDT）
- en: Support vector machines (SVM)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）
- en: The first two (linear and logistic regression) model the outcome, *Y*, conditional
    on the data, *X*, using a normal or Gaussian (linear regression) or sigmoidal
    (logistic regression) probability function. In contrast, the last three have no
    formal probability **model**—they compute a function (an ensemble of trees for
    random forests or GDBT, or an inner product distribution for SVM) that maps *X*
    to *Y*, using a loss or error function to tune those estimates. Given this nonparametric
    nature, some authors have argued that these constitute a separate class of **non-model**
    discriminative algorithms.^(16)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种（线性回归和逻辑回归）是在数据*X*的条件下模型结果*Y*，使用正态或高斯函数（线性回归）或S型函数（逻辑回归）。相比之下，最后三种没有正式的概率**模型**—它们计算将*X*映射到*Y*的函数（一组树用于随机森林或GDBT，或者SVM的内积分布），使用损失或错误函数来调整这些估计值。鉴于这种非参数性质，一些作者认为这构成了一类**非模型**判别算法。(16)
- en: 'In contrast, a **generative model** attempts to learn the joint distribution
    *P(Y, X)* of the labels and the input data. Recall that using the definition of
    joint probability:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**生成模型**试图学习标签和输入数据的联合分布*P(Y, X)*。回想一下通过联合概率的定义：
- en: '*P(X, Y) = P(X|Y)P(Y)*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X, Y) = P(X|Y)P(Y)*'
- en: 'We can rewrite Bayes'' theorem as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将贝叶斯定理重写如下：
- en: '*P(Y|X) = P(X, Y)/P(X)*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Y|X) = P(X, Y)/P(X)*'
- en: 'Instead of learning a direct mapping of *X* to *Y* using *P(Y|X)*, as in the
    discriminative case, our goal is to model the joint probabilities of *X* and *Y*
    using *P(X, Y)*. While we can use the resulting joint distribution of *X* and
    *Y* to compute the posterior, *P(Y|X)*, and learn a **targeted** model, we can
    also use this distribution to sample new instances of the data by either jointly
    sampling new tuples *(x, y)*, or samping new data inputs using a target label,
    *Y*, with the following expression:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与判别情况不同，我们的目标不是使用*P(Y|X)*直接映射*X*到*Y*，而是模拟*X*和*Y*的联合概率*P(X, Y)*。虽然我们可以使用*X*和*Y*的联合分布来计算后验概率*P(Y|X)*并学习一个**目标**模型，但我们也可以使用此分布来通过联合采样新的元组*(x,
    y)*或使用目标标签*Y*采样新的数据输入，使用以下表达式：
- en: '*P(X|Y=y) = P(X, Y)/P(Y)*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X|Y=y) = P(X, Y)/P(Y)*'
- en: 'Examples of generative models include the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的例子包括以下内容：
- en: Naive Bayes classifiers
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Gaussian mixture models
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: Latent Dirichlet Allocation (LDA)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）
- en: Hidden Markov models
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: Deep Boltzmann machines
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度波尔兹曼机
- en: VAEs
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAEs）
- en: GANs
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: Naive Bayes classifiers, though named as a discriminative model, utilize Bayes'
    theorem to learn the joint distribution of *X* and *Y* under the assumption that
    the *X* variables are independent. Similarly, Gaussian mixture models describe
    the likelihood of a data point belonging to one of a group of normal distributions
    using the joint probability of the label and these distributions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器，虽然被称为判别模型，但利用贝叶斯定理来学习*X*和*Y*的联合分布，假设*X*变量是独立的。同样，高斯混合模型描述了数据点属于一组正态分布之一的可能性，使用标签和这些分布的联合概率。
- en: LDA represents a document as the joint probability of a word and a set of underlying
    keyword lists (topics) that are used in a document. Hidden Markov models express
    the joint probability of a state and the next state of data, such as the weather
    on successive days of the week. As you will see in *Chapter 4*, *Teaching Networks
    to Generate Digits*, deep Boltzmann machines learn the joint probability of a
    label and the data vector it is associated with. The VAE and GAN models we will
    cover in *Chapters* 5, 6, 7, and 11 also utilize joint distributions to map between
    complex data types. This mapping allows us to generate data from random vectors
    or transform one kind of data into another.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LDA将文档表示为单词和一组潜在关键字列表（主题）的联合概率，这些关键字列表在文档中使用。隐马尔可夫模型表示数据的状态和下一个状态的联合概率，例如一周中连续几天的天气。正如你将在*第4章*中看到的，*教网络生成数字*，深度波尔兹曼机学习标签和与之相关的数据向量的联合概率。我们将在*第5、6、7和11章*中涵盖的VAE和GAN模型也利用联合分布来映射复杂的数据类型。这种映射允许我们从随机向量生成数据，或者将一种数据转换为另一种。
- en: As already mentioned, another view of generative models is that they allow us
    to generate samples of *X* if we know an outcome, *Y*. In the first four models
    in the previous list, this conditional probability is just a component of the
    model formula, with the posterior estimates still being the ultimate objective.
    However, in the last three examples, which are all deep neural network models,
    learning the conditional of *X* dependent upon a hidden, or **latent**, variable,
    *Z*, is actually the main objective, in order to generate new data samples. Using
    the rich structure allowed by multi-layered neural networks, these models can
    approximate the distribution of complex data types such as images, natural language,
    and sound. Also, instead of being a target value, *Z* is often a random number
    in these applications, serving merely as an input from which to generate a large
    space of hypothetical data points. To the extent we have a label (such as whether
    a generated image should be of a dog or dolphin, or the genre of a generated song),
    the model is *P(X|Y=y, Z=z)*, where the label *Y* controls the generation of data
    that is otherwise unrestricted by the random nature of *Z*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，生成模型的另一个观点是，如果我们知道一个结果*Y*，它们允许我们生成*X*的样本。在前述列表中的前四个模型中，这种条件概率只是模型公式的一个组成部分，而后验估计仍然是最终目标。然而，在最后三个示例中，它们都是深度神经网络模型，学习关于一个隐藏或**潜在**变量*Z*的*X*的条件是实际上的主要目标，以便生成新的数据样本。利用多层神经网络所允许的丰富结构，这些模型可以近似表示复杂数据类型的分布，如图像、自然语言和声音。此外，*Z*不再是目标值，而在这些应用中通常是一个随机数，仅用作从中生成大量假设数据点的输入。在我们有标签的程度上（比如一个生成的图像应该是狗还是海豚，或者一个生成的歌曲的流派），模型就是*P(X|Y=y,
    Z=z)*，其中标签*Y*控制着除了*Z*的随机性外其他数据的生成。
- en: Why use generative models?
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用生成模型？
- en: Now that we have reviewed what generative models are and defined them more formally
    in the language of probability, why would we have a need for such models in the
    first place? What value do they provide in practical applications? To answer this
    question, let's take a brief tour of the topics that we will cover in more detail
    in the rest of this book.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了生成模型的内容，并在概率语言中更正式地定义了它们，为什么我们首先需要这样的模型呢？它们在实际应用中提供了什么价值？为了回答这个问题，让我们简要地浏览一下我们将在本书的其余部分更详细地讨论的主题。
- en: The promise of deep learning
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的承诺
- en: As noted already, many of the models we will survey in the book are deep, multi-level
    neural networks. The last 15 years have seen a renaissance in the development
    of deep learning models for image classification, natural language processing
    and understanding, and reinforcement learning. These advances were enabled by breakthroughs
    in traditional challenges in tuning and optimizing very complex models, combined
    with access to larger datasets, distributed computational power in the cloud,
    and frameworks such as TensorFlow that make it easier to prototype and reproduce
    research.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如已经指出的，我们将在本书中调查的许多模型都是深度的、多级的神经网络。过去15年来，深度学习模型在图像分类、自然语言处理和理解以及强化学习方面取得了复兴。这些进展是由于在调整和优化非常复杂的模型方面的传统挑战的突破，再加上对更大的数据集、分布式计算能力的访问以及诸如TensorFlow这样的框架，使得原型设计和复制研究变得更加容易。
- en: Building a better digit classifier
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建更好的数字分类器
- en: A classic problem used to benchmark algorithms in machine learning and computer
    vision is the task of classifying which handwritten digit from 0-9 is represented
    in a pixelated image from the MNIST dataset.^(17) A large breakthrough on this
    problem occurred in 2006, when researchers at the University of Toronto and the
    National University of Singapore discovered a way to train deep neural networks
    to perform this task.^(18)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在机器学习和计算机视觉中基准算法的一个经典问题是对来自MNIST数据集的像素化图像中表示的0-9之间的哪个手写数字进行分类的任务。^(17) 在这个问题上取得的一个重大突破发生在2006年，当时多伦多大学和新加坡国立大学的研究人员发现了一种训练深度神经网络执行此任务的方法。^(18)
- en: One of their critical observations was that instead of training a network to
    directly predict the most likely digit (*Y*) given an image (*X*), it was more
    effective to first train a network that could **generate images**, and then classify
    them as a second step. In *Chapter 4*, *Teaching Networks to Generate Digits*,
    I will describe how this model improved upon past attempts, and how to create
    your own **restricted Boltzmann machine** and **deep Boltzmann machine** models
    that can generate new MNIST digit images.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的一个关键观察是，与其训练一个网络直接预测给定图像(*X*)的最可能的数字(*Y*)，不如首先训练一个可以**生成图像**的网络，然后作为第二步对它们进行分类。在*第4章*，*教网络生成数字*中，我将描述这个模型是如何改进过去的尝试的，并且如何创建自己的**受限玻尔兹曼机**和**深度玻尔兹曼机**模型，这些模型可以生成新的MNIST数字图像。
- en: Generating images
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成图像
- en: A challenge to generating images such as the **Portrait of Edmond Belamy** with
    the approach used for the MNIST dataset is that frequently, images have no labels
    (such as a digit); rather, we want to map the space of random numbers into a set
    of artificial images using a latent vector, *Z*, as I described earlier in the
    chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MNIST数据集的方法生成像**Edmond Belamy的肖像**这样的图像的一个挑战是，图像通常没有标签（如数字）；相反，我们想要使用一个潜在向量*Z*将随机数空间映射到一组人工图像，就像我在本章中早些时候描述的那样。
- en: A further constraint is that we want to promote **diversity** of these images.
    If we input numbers within a certain range, we would like to know that they generate
    different outputs, and be able to tune the resulting image features. For this
    purpose, VAEs were developed to generate diverse and photorealistic images (*Figure
    1.5*).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是我们希望促进这些图像的**多样性**。如果我们输入在某个范围内的数字，我们希望知道它们生成不同的输出，并且能够调整生成的图像特征。为此，VAE被开发出来生成多样化和逼真的图像（*图1.5*）。
- en: '![](img/B16176_01_05.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_05.png)'
- en: 'Figure 1.5: Sample images from a VAE^(19)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：来自VAE的样本图像^(19)
- en: 'In the context of image classification tasks, being able to generate new images
    can help us increasethe number of examples in an existing dataset, or reduce the
    **bias** if our existing dataset is heavily skewed toward a particular kind of
    photograph. Applications could include generating alternative poses (angles, shades,
    or perspective shots) for product photographs on a fashion e-commerce website
    (*Figure 1.6*):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类任务的背景下，能够生成新图像可以帮助我们增加现有数据集中的示例数量，或者如果我们现有数据集严重偏向于特定类型的照片，则减少**偏差**。应用可能包括为时尚电子商务网站的产品照片生成替代姿势（角度、色调或透视镜头）（*图1.6*）：
- en: '![](img/B16176_01_06.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_01_06.png)'
- en: 'Figure 1.6: Simulating alternative poses with deep generative models^(20)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：使用深度生成模型模拟替代姿势^(20)
- en: Style transfer and image transformation
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风格转移和图像变换
- en: In addition to mapping artificial images to a space of random numbers, we can
    also use generative models to learn a mapping between one kind of image and a
    second. This kind of model can, for example, be used to convert an image of a
    horse into that of a zebra (*Figure 1.7*), create **deep fake videos** in which
    one actor's face has been replaced with another's, or transform a photo into a
    painting (*Figures 1.2* and *1.4*):^(21)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_07.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: CycleGANs apply stripes to horses to generate zebras^(22)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fascinating example of applying generative modeling is a study in which
    lost masterpieces of the artist Pablo Picasso were discovered to have been painted
    over with another image. After X-ray imaging of **The Old Guitarist**and **The
    Crouching Beggar** indicated that earlier images of a woman and a landscape lay
    underneath (*Figure 1.8*),researchers used the other paintings from **Picasso''s
    blue period** or other color photographs (*Figure 1.8*)to train a **neural style
    transfer** model that transforms black-and-white images (the X-ray radiographs
    of the overlying paintings) to the coloration of the original artwork. Then, applying
    this transfer model to the **hidden** images allowed them to reconstruct **colored-in**
    versions of the lost paintings:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_08.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Deep learning was used to color in the X-ray images of the painted-over
    scenes (middle), with color patterns learned from examples (column d) generating
    colorized versions of the lost art (far right)^(23)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these models use the previously mentioned GANs, a type of deep learning
    model proposed in 2014^(24) In addition to changing the contents of an image (as
    in the preceding zebra example), these models can also be used to map one image
    into another, such as paired images (such as dogs and humans with similar facial
    features, as in *Figure 1.9*), or generate textual descriptions from images (*Figure
    1.10*):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_09.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Sim-GAN for mapping human to animal or anime faces^(25)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Caption-GAN for generating descriptions from images^(26)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: We could also condition the properties of the generated images on some auxiliary
    information such as labels, an approach used in the GANGogh algorithm, which synthesizes
    images in the style of different artists by supplying the desired artist as an
    input to the generative model (*Figure 1.4*).^(27) I will describe these applications
    in *Chapter 6*, *Image Generation with GANs*, and *Chapter 7*, *Style Transfer
    with GANs*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Fake news and chatbots
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans have always wanted to talk to machines; the first chatbot, ELIZA,^(28)
    was written at MIT in the 1960s and used a simple program to transform a user's
    input and generate a response, in the mode of a **therapist** who frequently responds
    in the form of a question.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated models can generate entirely novel text, such as Google's
    BERT and GPT-2,^(29 30) which use a unit called a **transformer**. A transformer
    module in a neural network allows a network to propose a new word in the context
    of preceding words in a piece of text, emphasizing those that are more relevant
    in order to generate plausible stretches of language. The BERT model then combines
    transformer units into a powerful multi-dimensional encoding of natural language
    patterns and contextual significance. This approach can be used in document creation
    for **natural language processing** (**NLP**) tasks, or for chatbot dialogue systems
    (*Figure 1.3*).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Sound composition
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sound, like images or text, is a complex, high-dimensional kind of data. Music
    in particular has many complexities: it could involve one or several musicians,
    has a temporal structure, and can be divided into thematically related segments.
    All of these components are incorporated into models such as MuseGAN, as mentioned
    earlier, which uses GANs to generate these various components and synthesize them
    into realistic, yet synthetic, musical tracks. I will describe the implementation
    of MuseGAN and its variants in *Chapter 11*, *Composing Music with Generative
    Models*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The rules of the game
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding applications concern data types we can see, hear, or read. However,
    generative models also have applications to generate rules. This is useful in
    a popular application of deep learning: using algorithms to play board games or
    Atari video games.^(31)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'While these applications have traditionally used **reinforcement learning**
    (**RL**) techniques to train networks to employ the optimal strategy in these
    games, new research has suggested using GANs to propose novel rules as part of
    the training process,^(32) or to generate synthetic data to prime the overall
    learning process.^(33) We will examine both applications in *Chapter 12*, *Play
    Video Games with Generative AI: GAIL*.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Unique challenges of generative models
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the powerful applications that generative models have, what are the major
    challenges in implementing them? As described, most of these models utilize complex
    data, requiring us to fit large models to capture all the nuances of their features
    and distribution. This has implications both for the number of examples that we
    must collect to adequately represent the kind of data we are trying to generate,
    and the computational resources needed to build the model. We will discuss techniques
    in *Chapter 2*, *Setting up a TensorFlow Lab*, to parallelize the training of
    these models using cloud computing frameworks and **graphics processing units**
    (**GPUs**).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'A more subtle problem that comes from having complex data, and the fact that
    we are trying to generate data rather than a numerical label or value, is that
    our notion of model **accuracy** is much more complicated: we cannot simply calculate
    the distance to a single label or scores.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss in *Chapter 5*, *Painting Pictures with Neural Networks Using
    VAEs*, and *Chapter 6*, *Image Generation with GANs*, how deep generative models
    such as VAE and GAN algorithms take different approaches to determine whether
    a generated image is comparable to a real-world image. Finally, as mentioned,
    our models need to allow us to generate both large and **diverse** samples, and
    the various methods we will discuss take different approaches to control the diversity
    of data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed what generative modeling is, and how it fits into
    the landscape of more familiar machine learning methods. I used probability theory
    and Bayes' theorum to describe how these models approach prediction in an opposite
    manner to discriminative learning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed use cases for generative learning, both for specific kinds of data
    and general prediction tasks. Finally, we examined some of the specialized challenges
    that arise from building these models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our practical implementation of these models
    by exploring how to set up a development environment for TensorFlow 2.0 using
    Docker and Kubeflow.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx](https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Baltruschat, I.M., Nickisch, H., Grass, M. et al. (2019). *Comparison of Deep
    Learning Approaches for Multi-Label Chest X-Ray Classification*. Sci Rep 9, 6381\.
    [https://doi.org/10.1038/s41598-019-42294-8](https://doi.org/10.1038/s41598-019-42294-8)
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*AlphaGo* (n.d.). DeepMind. Retrieved April 20, 2021, from [https://deepmind.com/research/case-studies/alphago-the-story-so-far](https://deepmind.com/research/case-studies/alphago-the-story-so-far)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The AlphaStar team (2019, October). *AlphaStar: Grandmaster level in StarCraft
    II using multi-agent reinforcement learning*. DeepMind. [https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-rein)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Devlin, J., Chang, M., Lee, K., Toutanova, K. (2019). *BERT: Pre-training of
    Deep Bidirectional Transformers for Language Understanding*. arXiv. [https://arxiv.org/abs/1810.04805v2](https://arxiv.org/abs/1810.04805v2)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brandon, J. (2018, February 16). *Terrifying high-tech porn: Creepy ''deepfake''
    videos are on the rise*. Fox News. [https://www.foxnews.com/tech/terrifying-high-tech-porn-creepy-deepfake-videos-are-on-the-rise](https://www.foxnews.com/tech/terrifying-high-tech-porn-creepy-deepfake-videos-are-on-the-rise)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://seanbmcgregor.com/DeepfakeDetectionGame.html](https://seanbmcgregor.com/DeepfakeDetectionGame.html)'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Better Language Models and Their Implications*. (February 14, 2019). OpenAI.
    [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-example-1.png](https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-examp)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-example-1.png](https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-examp)'
- en: 'Leviathan Y., Matias Y. (2018, May 8). *Google Duplex: An AI System for Accomplishing
    Real-World Tasks Over the Phone*. Google AI Blog. [https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Leviathan Y., Matias Y. (2018年5月8日)。*Google Duplex：用于电话实现真实世界任务的AI系统*。Google
    AI博客。 [https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)
- en: Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang and Yi-Hsuan Yang. MuseGAN. [https://salu133445.github.io/musegan/](https://salu133445.github.io/musegan/)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang 和 Yi-Hsuan Yang。*MuseGAN*。[https://salu133445.github.io/musegan/](https://salu133445.github.io/musegan/)
- en: '[https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg](https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg)'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg](https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg)'
- en: Kolmogorov A. N., (1956). *Foundations of the Theory of Probability*. (2nd edition).
    Chelsea Publishing Company New York. [https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kolmogorov A. N., (1956). *概率论基础*.（第2版）。纽约：切尔西出版公司。 [https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations)
- en: 'Jebara, Tony., (2004). *Machine Learning: Discriminative and Generative*. Kluwer
    Academic (Springer). [https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jebara, Tony., (2004). *机器学习：判别式与生成式*。Kluwer Academic (Springer)。[https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)
- en: Bayes Thomas, (1763) *LII. An essay towards solving a problem in the doctrine
    of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in
    a letter to John Canton, A. M. F. R. S* Phil. Trans. R. Soc.53370–418\. [https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053](https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bayes Thomas, (1763) *LII. 解决机会命理学问题的尝试*。由已故牧师Bayes先生F.R.S.与约翰坎顿先生的信交流。R. Soc.53370–418\.
    [https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053](https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053)
- en: 'Jebara, Tony., (2004). *Machine Learning: Discriminative and Generative*. Kluwer
    Academic (Springer). [https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jebara, Tony., (2004). *机器学习：判别式与生成式*。Kluwer Academic (Springer)。[https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: G. Hinton, S. Osindero, & Y.-W. Teh. (2005). *A Fast Learning Algorithm for
    Deep Belief Nets*. [www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Hinton, S. Osindero, & Y.-W. Teh. (2005). *用于深度信念网络的快速学习算法*。[www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)
- en: '[https://jaan.io/images/variational-autoencoder-faces.jpg](https://jaan.io/images/variational-autoencoder-faces.jpg)
    and [https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg](https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg)'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://jaan.io/images/variational-autoencoder-faces.jpg](https://jaan.io/images/variational-autoencoder-faces.jpg)
    和 [https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg](https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg)'
- en: Esser, P., Haux, J., Ommer, B., (2019). *Unsupervised Robust Disentangling of
    Latent Characteristics for Image Synthesis*. arXiv. [https://arxiv.org/abs/1910.10223](https://arxiv.org/abs/1910.10223)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Esser, P., Haux, J., Ommer, B., (2019). *用于图像合成的无监督稳健潜在特征分解*. arXiv。[https://arxiv.org/abs/1910.10223](https://arxiv.org/abs/1910.10223)
- en: '*CycleGAN*. TensorFlow Core. Retrieved April 26, 2021, from [https://www.tensorflow.org/tutorials/generative/cyclegan](https://www.tensorflow.org/tutorials/generative/cyclegan)'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*CycleGAN*。TensorFlow Core。2021年4月26日检索自 [https://www.tensorflow.org/tutorials/generative/cyclegan](https://www.tensorflow.org/tutorials/generative/cyclegan)'
- en: '[https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png](https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png)'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png](https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png)'
- en: Bourached, A., Cann, G. (2019). *Raiders of the Lost Art*. arXiv:1909.05677\.
    [https://arxiv.org/pdf/1909.05677.pdf](https://arxiv.org/pdf/1909.05677.pdf)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gorti, S. K., Ma, Jeremy (2018). *Text-to-Image-to-Text Translation using Cycle
    Consistent Adversarial Networks*. arXiv. [https://arxiv.org/abs/1808.04538](https://arxiv.org/abs/1808.04538)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rkjones4, adam-hanna, erincr & rodrigobdz (2020). GANGogh. GitHub repository.
    [https://github.com/rkjones4/GANGogh](https://github.com/rkjones4/GANGogh )
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weizenbaum Joseph. (1976) *Computer and Human Reason*. W. H. Freeman and company.
    [blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf](http://blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Schwartz B., (2019, October 25). *Welcome BERT: Google’s latest search algorithm
    to better understand natural language*. Search Engine Land. [https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-queries-323976](https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-qu)'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Better Language Models and Their Implications*. (2019, February 14). OpenAI.
    [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mnih V., Kavukcuoglu K., Silver D., Graves A., Antonoglou I., Wierstra D., Riedmiller
    M. (2013, January 01). *Playing Atari with Deep Reinforcement Learning*. DeepMind.
    [https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Liu, Yang; Zeng, Yifeng; Chen, Yingke; Tang, Jing; Pan, Yinghui (2019). *Self-Improving
    Generative Adversarial Reinforcement Learning*. AAMS 2019\. [http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p52.pdf](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p52.pdf)
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kasgari, A T Z, Saad, W., Mozaffari, M., Poor, H V (2020). *Experienced Deep
    Reinforcement Learning with Generative Adversarial Networks (GANs) for Model-Free
    Ultra Reliable Low Latency Communication*. arXiv. [https://arxiv.org/abs/1911.03264](https://arxiv.org/abs/1911.03264)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
