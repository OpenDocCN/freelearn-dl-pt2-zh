- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An Introduction to Generative AI: "Drawing" Data from Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will dive into the various applications of generative models.
    Before that, we will take a step back and examine how exactly generative models
    are different from other types of machine learning. The difference lies with the
    basic units of any machine learning algorithm: probability and the various ways
    we use mathematics to quantify the shape and distribution of data we encounter
    in the world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminative and generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rules of probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why use generative models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unique challenges of generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In New York City in October 2018, the international auction house Christie's
    sold the **Portrait of Edmond Belamy**(*Figure 1.1*) during the show **Prints
    & Multiples**for $432,500.00\. This sale was remarkable both because the sale
    price was 45 times higher than the initial estimates for the piece, and due to
    the unusual origin of this portrait. Unlike the majority of other artworks sold
    by Christie's since the 18th century, the **Portrait of Edmond Belamy** is not
    painted using oil or watercolors, nor is its creator even human; rather, it is
    an entirely digital image produced by a sophisticated machine learning algorithm.
    The creators—a Paris-based collective named Obvious—used a collection of 15,000
    portraits created between the 14^(th) and 20^(th) centuries to tune an artificial
    neural network model capable of generating aesthetically similar, albeit synthetic,
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The Portrait of Edmond Belamy¹'
  prefs: []
  type: TYPE_NORMAL
- en: Portraiture is far from the only area in which machine learning has demonstrated
    astonishing results. Indeed, if you have paid attention to the news in the last
    few years, you have likely seen many stories about the ground-breaking results
    of modern AI systems applied to diverse problems, from the hard sciences to digital
    art. Deep neural network models, such as the one created by Obvious, can now classify
    X-ray images of human anatomy on the level of trained physicians,² beat human
    masters at both classic board games such as Go (an Asian game similar to chess)³
    and multiplayer computer games,⁴ and translate French into English with amazing
    sensitivity to grammatical nuances.⁵
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative and generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These other examples of AI differ in an important way from the model that generated
    **The Portrait of Edmond Belamy**. In all of these other applications, the model
    is presented with a set of inputs—data such as English text, images from X-rays,
    or the positions on a gameboard—that is paired with a target output, such as the
    next word in a translated sentence, the diagnostic classification of an X-ray,
    or the next move in a game. Indeed, this is probably the kind of AI model you
    are most familiar with from prior experiences of predictive modeling; they are
    broadly known as **discriminative**models, whose purpose is to create a mapping
    between a set of input variables and a target output. The target output could
    be a set of discrete classes (such as which word in the English language appears
    next in a translation), or a continuous outcome (such as the expected amount of
    money a customer will spend in an online store over the next 12 months).
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that this kind of model, in which data is **labeled** or
    **scored**, represents only half the capabilities of modern machine learning.
    Another class of algorithms, such as the one that generated the artificial portrait
    sold at Christie's, don't compute a score or label from input variables, but rather
    **generate new data**. Unlike discriminative models, the input variables are often
    vectors of numbers that aren't related to real-world values at all, and are often
    even randomly generated. This kind of model—known as a **generative model**—can
    produce complex outputs such as text, music, or images from random noise, and
    is the topic of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you didn't know it at the time, you have probably seen other instances
    of generative models in the news alongside the discriminative examples given earlier.
    A prominent example is deep fakes, which are videos in which one person's face
    has been systematically replaced with another's by using a neural network to remap
    the pixels.⁶
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: A deep fake image⁷'
  prefs: []
  type: TYPE_NORMAL
- en: Maybe you have also seen stories about AI models that generate **fake news**,
    which scientists at the firm OpenAI were initially terrified to release to the
    public due to concerns they could be used to create propaganda and misinformation
    online.⁸
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: A chatbot dialogue created using GPT-2⁹'
  prefs: []
  type: TYPE_NORMAL
- en: In these and other applications, such as Google's voice assistant Duplex, which
    can make a restaurant reservation by dynamically creating a conversation with
    a human in real time,^(10) or software that can generate original musical compositions,^(11)
    we are surrounded by the outputs of generative AI algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Examples of style transfer using Generative Adversarial Networks
    (GANs)^(12)'
  prefs: []
  type: TYPE_NORMAL
- en: 'These models are able to handle complex information in a variety of domains:
    creating photorealistic images or stylistic **filters** on pictures (*Figure 1.4*),
    synthetic sound, conversational text, and even rules for optimally playing video
    games. You might ask, where did these models come from? How can I implement them
    myself?We will discuss more on that in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While generative models could theoretically be implemented using a wide variety
    of machine learning algorithms, in practice, they are usually built with deep
    neural networks, which are well suited to capturing complex variations in data
    such as images or language.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus on implementing these deep generative models for
    many different applications using **TensorFlow 2.0**. TensorFlow is a C++ framework,
    with APIs in the Python programming language, used to develop and productionize
    deep learning models. It was open sourced by Google in 2013, and has become one
    of the most popular libraries for the research and deployment of neural network
    models.
  prefs: []
  type: TYPE_NORMAL
- en: With the 2.0 release, much of the boilerplate code that characterized development
    in earlier versions of the library was cleaned up with high-level abstractions,
    allowing us to focus on the model rather than the plumbing of the computations.
    The latest version also introduced the concept of **eager** execution, allowing
    network computations to be run on demand, which will be an important benefit of implementing
    some of our models.
  prefs: []
  type: TYPE_NORMAL
- en: In upcoming chapters, you will learn not only the underlying theory behind these
    models, but the practical skills needed to implement them in popular programming
    frameworks. In *Chapter 2*, *Setting up a TensorFlow Lab*, you will learn how
    to set up a cloud environment that will allow you to run TensorFlow in a distributed
    fashion, using the **Kubeflow** framework to catalog your experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, as I will describe in more detail in *Chapter 3*, *Building Blocks of
    Deep Neural Networks*, since 2006 an explosion of research into **deep** **learning**
    using large neural network models has produced a wide variety of generative modeling
    applications. The first of these was the **restricted Boltzmann machine**, which
    is stacked in multiple layers to create a **deep belief network**. I will describe
    both of these models in *Chapter 4*, *Teaching Networks to Generate Digits*. Later
    innovations included **Variational Autoencoders** (**VAEs**), which can efficiently
    generate complex data samples from random numbers, using techniques that I will
    describe in *Chapter 5*, *Painting Pictures with Neural Networks Using VAEs*.
  prefs: []
  type: TYPE_NORMAL
- en: We will also cover the algorithm used to create **The Portrait of Edmond Belamy**,
    the GAN, in more detail in *Chapter 6*, *Image Generation with GANs*, of this
    book. Conceptually, the GANmodel creates a competition between two neural networks.
    One (termed the **generator**) produces realistic (or, in the case of the experiments
    by Obvious, artistic) images starting from a set of random numbers and applying
    a mathematical transformation. In a sense, the generator is like an art student,
    producing new paintings from brushstrokes and creative inspiration.
  prefs: []
  type: TYPE_NORMAL
- en: The second network, known as the **discriminator**, attempts to classify whether
    a picture comes from a set of real-world images, or whether it was created by
    the generator. Thus, the discriminator acts like a teacher, grading whether the
    student has produced work comparable to the paintings they are attempting to mimic.
    As the generator becomes better at fooling the discriminator, its output becomes
    closer and closer to the historical examples it is designed to copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many classes of GAN models, with additional variants covered in *Chapter
    7*, *Style Transfer with GANs*, and *Chapter 11*,*Composing Music with Generative
    Models*, in our discussion of advanced models. Another key innovation in generative
    models is in the domain of natural language data. By representing the complex
    interrelationship between words in a sentence in a computationally scalable way,
    the Transformer network and the **Bidirectional Encoder from Transformers** (**BERT**)
    model built on top of it present powerful building blocks to generate textual
    data in applications such as chatbots, which we''ll cover in more detail in *Chapter
    9*,*The Rise of Methods for Text Generation*, and *Chapter 10*,*NLP 2.0: Using
    Transformers to Generate Text*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 12*, *Play Video Games with Generative AI: GAIL*, you will also
    see how models such as GANs and VAEs can be used to generate not just images or
    text, but sets of rules that allow game-playing networks developed with reinforcement
    learning algorithms to process and navigate their environment more efficiently—in essence,
    learning to learn. Generative models are a huge field of research that is constantly
    growing, so unfortunately, we can''t cover every topic in this book. For the interested
    reader, references to further topics will be provided in *Chapter 13*,*Emerging
    Applications in Generative AI*.'
  prefs: []
  type: TYPE_NORMAL
- en: To get started with some background information, let's discuss the rules of
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: The rules of probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the simplest level, a model, be it for machine learning or a more classical
    method such as linear regression, is a mathematical description of how various
    kinds of data relate to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the task of modeling, we usually think about separating the variables of
    our dataset into two broad classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent data**, which primarily means **inputs** to a model, are denoted
    by *X*. These could be categorical features (such as a "0" or "1" in six columns
    indicating which of six schools a student attends), continuous (such as the heights
    or test scores of the same students), or ordinal (the rank of a student in the
    class).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dependent data**, conversely, are the outputs of our models, and are denoted
    by *Y*. (Note that in some cases *Y* is a **label** that can be used to condition
    a generative output, such as in a conditional GAN.) As with the independent variables,
    these can be continuous, categorical, or ordinal, and they can be an individual
    element or multidimensional matrix (tensor) for each element of the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So how can we describe the data in our model using statistics? In other words,
    how can we quantitatively describe what values we are likely to see, and how frequently,
    and which values are more likely to appear together? One way is by asking the
    likelihood of observing a particular value in the data, or the probability of
    that value. For example, if we were to ask what the probability is of observing
    a roll of 4 on a six-sided die, the answer is that, on average, we would observe
    a 4 once every six rolls. We write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X=4) =* *⅙* *= 16.67%*'
  prefs: []
  type: TYPE_NORMAL
- en: where *P* denotes *probability of*.
  prefs: []
  type: TYPE_NORMAL
- en: What defines the allowed probability values for a particular dataset? If we
    imagine the set of all possible values of a dataset, such as all values of a die,
    then a probability maps each value to a number between 0 and 1\. The minimum is
    0 because we can't have a negative chance of seeing a result; the most unlikely
    result is that we would never see a particular value, or 0% probability, such
    as rolling a 7 on a six-sided die. Similarly, we can't have greater than 100%
    probability of observing a result, represented by the value 1; an outcome with
    probability 1 is absolutely certain. This set of probability values associated
    with a dataset belong to discrete classes (such as the faces of a die) or an infinite
    set of potential values (such as variations in height or weight). In either case,
    however, these values have to follow certain rules, the **Probability Axioms**
    described by the mathematician Andrey Kolmogorov in 1933:^(13)
  prefs: []
  type: TYPE_NORMAL
- en: The probability of an observation (a die role, a particular height, and so on)
    is a non-negative, finite number between 0 and 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability of *at least one* of the observations in the space of all possible
    observations occurring is 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The joint probability of distinct, mutually exclusive events is the sum of the
    probability of the individual events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While these rules might seem abstract, you will see in *Chapter 3*, *Building
    Blocks of Deep Neural Networks*, that they have direct relevance to developing
    neural network models. For example, an application of rule 1 is to generate the
    probability between 1 and 0 for a particular outcome in a **softmax** function
    for predicting target classes. Rule 3 is used to normalize these outcomes into
    the range 0-1, under the guarantee that they are mutually distinct predictions
    of a deep neural network (in other words, a real-world image logically can't be
    classified as both a dog and a cat, but rather a dog or a cat, with the probability
    of these two outcomes additive). Finally, the second rule provides the theoretical
    guarantees that we can generate data at all using these models.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in the context of machine learning and modeling, we are not usually
    interested in just the probability of observing a piece of input data, *X*; we
    instead want to know the **conditional** probability of an outcome, *Y*, given
    the data, *X*. In other words, we want to know how likely a label is for a set
    of data, based on that data. We write this as *the probability of Y given X,*
    or *the probability of Y conditional on X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Y|X)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another question we could ask about *Y* and *X* is how likely they are to occur
    together or their **joint probability**, which can be expressed using the preceding
    conditional probability expression as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X, Y) = P(Y|X)P(X) = P(X|Y)(Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula expressed *the probability of X and Y*. In the case of *X* and
    *Y* being completely independent of one another, this is simply their product:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X|Y)P(Y) = P(Y|X)P(X) = P(X)P(Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: You will see that these expressions become important in our discussion of **complementary
    priors** in *Chapter 4*, *Teaching Networks to Generate Digits*, and the ability
    of **restricted Boltzmann machines** to simulate independent data samples. They
    are also important as building blocks of Bayes' theorem, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative and generative modeling and Bayes' theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let's consider how these rules of conditional and joint probability relate
    to the kinds of predictive models that we build for various machine learning applications.
    In most cases—such as predicting whether an email is fraudulent or the dollar
    amount of the future lifetime value of a customer—we are interested in the conditional
    probability, *P(Y|X=x)*, where *Y* is the set of outcomes we are trying to model,
    *X* represents the input features, and *x* is a particular value of the input
    features. As discussed, this approach is known as **discriminative modeling**.^(14)
    Discriminative modeling attempts to learn a direct mapping between the data, *X*,
    and the outcomes, *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to understand discriminative modeling is in the context of Bayes''
    theorem,^(15) which relates the conditional and joint probabilities of a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Y|X) = P(X|Y)P(Y)/P(X) = P(X, Y)/P(X)*'
  prefs: []
  type: TYPE_NORMAL
- en: In Bayes' formula, the expression *P(X|Y)/P(X)* is known as the **likelihood**
    or the supporting evidence that the observation *X* gives to the likelihood of
    observing *Y*. *P(Y)* is the **prior**or the plausibility of the outcome, and
    *P(Y|X)* is the **posterior** or the probability of the outcome given all the
    independent data we have observed related to the outcome thus far. Conceptually,
    Bayes' theorem states that the probability of an outcome is the product of its
    baseline probability and the probability of the input data conditional on this
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The theorem was published two years after the author's death, and in a foreword
    Richard Price described it as a mathematical argument for the existence of God,
    which was perhaps appropriate given that Thomas Bayes served as a reverend during
    his life.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of discriminative learning, we can thus see that a discriminative
    model directly computes the posterior; we could have a model of the likelihood
    or prior, but it is not required in this approach. Even though you may not have
    realized it, most of the models you have probably used in the machine learning
    toolkit are discriminative, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosted decision trees (GBDT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines (SVM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two (linear and logistic regression) model the outcome, *Y*, conditional
    on the data, *X*, using a normal or Gaussian (linear regression) or sigmoidal
    (logistic regression) probability function. In contrast, the last three have no
    formal probability **model**—they compute a function (an ensemble of trees for
    random forests or GDBT, or an inner product distribution for SVM) that maps *X*
    to *Y*, using a loss or error function to tune those estimates. Given this nonparametric
    nature, some authors have argued that these constitute a separate class of **non-model**
    discriminative algorithms.^(16)
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, a **generative model** attempts to learn the joint distribution
    *P(Y, X)* of the labels and the input data. Recall that using the definition of
    joint probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X, Y) = P(X|Y)P(Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite Bayes'' theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Y|X) = P(X, Y)/P(X)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of learning a direct mapping of *X* to *Y* using *P(Y|X)*, as in the
    discriminative case, our goal is to model the joint probabilities of *X* and *Y*
    using *P(X, Y)*. While we can use the resulting joint distribution of *X* and
    *Y* to compute the posterior, *P(Y|X)*, and learn a **targeted** model, we can
    also use this distribution to sample new instances of the data by either jointly
    sampling new tuples *(x, y)*, or samping new data inputs using a target label,
    *Y*, with the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X|Y=y) = P(X, Y)/P(Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of generative models include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden Markov models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Boltzmann machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifiers, though named as a discriminative model, utilize Bayes'
    theorem to learn the joint distribution of *X* and *Y* under the assumption that
    the *X* variables are independent. Similarly, Gaussian mixture models describe
    the likelihood of a data point belonging to one of a group of normal distributions
    using the joint probability of the label and these distributions.
  prefs: []
  type: TYPE_NORMAL
- en: LDA represents a document as the joint probability of a word and a set of underlying
    keyword lists (topics) that are used in a document. Hidden Markov models express
    the joint probability of a state and the next state of data, such as the weather
    on successive days of the week. As you will see in *Chapter 4*, *Teaching Networks
    to Generate Digits*, deep Boltzmann machines learn the joint probability of a
    label and the data vector it is associated with. The VAE and GAN models we will
    cover in *Chapters* 5, 6, 7, and 11 also utilize joint distributions to map between
    complex data types. This mapping allows us to generate data from random vectors
    or transform one kind of data into another.
  prefs: []
  type: TYPE_NORMAL
- en: As already mentioned, another view of generative models is that they allow us
    to generate samples of *X* if we know an outcome, *Y*. In the first four models
    in the previous list, this conditional probability is just a component of the
    model formula, with the posterior estimates still being the ultimate objective.
    However, in the last three examples, which are all deep neural network models,
    learning the conditional of *X* dependent upon a hidden, or **latent**, variable,
    *Z*, is actually the main objective, in order to generate new data samples. Using
    the rich structure allowed by multi-layered neural networks, these models can
    approximate the distribution of complex data types such as images, natural language,
    and sound. Also, instead of being a target value, *Z* is often a random number
    in these applications, serving merely as an input from which to generate a large
    space of hypothetical data points. To the extent we have a label (such as whether
    a generated image should be of a dog or dolphin, or the genre of a generated song),
    the model is *P(X|Y=y, Z=z)*, where the label *Y* controls the generation of data
    that is otherwise unrestricted by the random nature of *Z*.
  prefs: []
  type: TYPE_NORMAL
- en: Why use generative models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have reviewed what generative models are and defined them more formally
    in the language of probability, why would we have a need for such models in the
    first place? What value do they provide in practical applications? To answer this
    question, let's take a brief tour of the topics that we will cover in more detail
    in the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The promise of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As noted already, many of the models we will survey in the book are deep, multi-level
    neural networks. The last 15 years have seen a renaissance in the development
    of deep learning models for image classification, natural language processing
    and understanding, and reinforcement learning. These advances were enabled by breakthroughs
    in traditional challenges in tuning and optimizing very complex models, combined
    with access to larger datasets, distributed computational power in the cloud,
    and frameworks such as TensorFlow that make it easier to prototype and reproduce
    research.
  prefs: []
  type: TYPE_NORMAL
- en: Building a better digit classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classic problem used to benchmark algorithms in machine learning and computer
    vision is the task of classifying which handwritten digit from 0-9 is represented
    in a pixelated image from the MNIST dataset.^(17) A large breakthrough on this
    problem occurred in 2006, when researchers at the University of Toronto and the
    National University of Singapore discovered a way to train deep neural networks
    to perform this task.^(18)
  prefs: []
  type: TYPE_NORMAL
- en: One of their critical observations was that instead of training a network to
    directly predict the most likely digit (*Y*) given an image (*X*), it was more
    effective to first train a network that could **generate images**, and then classify
    them as a second step. In *Chapter 4*, *Teaching Networks to Generate Digits*,
    I will describe how this model improved upon past attempts, and how to create
    your own **restricted Boltzmann machine** and **deep Boltzmann machine** models
    that can generate new MNIST digit images.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A challenge to generating images such as the **Portrait of Edmond Belamy** with
    the approach used for the MNIST dataset is that frequently, images have no labels
    (such as a digit); rather, we want to map the space of random numbers into a set
    of artificial images using a latent vector, *Z*, as I described earlier in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A further constraint is that we want to promote **diversity** of these images.
    If we input numbers within a certain range, we would like to know that they generate
    different outputs, and be able to tune the resulting image features. For this
    purpose, VAEs were developed to generate diverse and photorealistic images (*Figure
    1.5*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Sample images from a VAE^(19)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of image classification tasks, being able to generate new images
    can help us increasethe number of examples in an existing dataset, or reduce the
    **bias** if our existing dataset is heavily skewed toward a particular kind of
    photograph. Applications could include generating alternative poses (angles, shades,
    or perspective shots) for product photographs on a fashion e-commerce website
    (*Figure 1.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Simulating alternative poses with deep generative models^(20)'
  prefs: []
  type: TYPE_NORMAL
- en: Style transfer and image transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to mapping artificial images to a space of random numbers, we can
    also use generative models to learn a mapping between one kind of image and a
    second. This kind of model can, for example, be used to convert an image of a
    horse into that of a zebra (*Figure 1.7*), create **deep fake videos** in which
    one actor's face has been replaced with another's, or transform a photo into a
    painting (*Figures 1.2* and *1.4*):^(21)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: CycleGANs apply stripes to horses to generate zebras^(22)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fascinating example of applying generative modeling is a study in which
    lost masterpieces of the artist Pablo Picasso were discovered to have been painted
    over with another image. After X-ray imaging of **The Old Guitarist**and **The
    Crouching Beggar** indicated that earlier images of a woman and a landscape lay
    underneath (*Figure 1.8*),researchers used the other paintings from **Picasso''s
    blue period** or other color photographs (*Figure 1.8*)to train a **neural style
    transfer** model that transforms black-and-white images (the X-ray radiographs
    of the overlying paintings) to the coloration of the original artwork. Then, applying
    this transfer model to the **hidden** images allowed them to reconstruct **colored-in**
    versions of the lost paintings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Deep learning was used to color in the X-ray images of the painted-over
    scenes (middle), with color patterns learned from examples (column d) generating
    colorized versions of the lost art (far right)^(23)'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these models use the previously mentioned GANs, a type of deep learning
    model proposed in 2014^(24) In addition to changing the contents of an image (as
    in the preceding zebra example), these models can also be used to map one image
    into another, such as paired images (such as dogs and humans with similar facial
    features, as in *Figure 1.9*), or generate textual descriptions from images (*Figure
    1.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Sim-GAN for mapping human to animal or anime faces^(25)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Caption-GAN for generating descriptions from images^(26)'
  prefs: []
  type: TYPE_NORMAL
- en: We could also condition the properties of the generated images on some auxiliary
    information such as labels, an approach used in the GANGogh algorithm, which synthesizes
    images in the style of different artists by supplying the desired artist as an
    input to the generative model (*Figure 1.4*).^(27) I will describe these applications
    in *Chapter 6*, *Image Generation with GANs*, and *Chapter 7*, *Style Transfer
    with GANs*.
  prefs: []
  type: TYPE_NORMAL
- en: Fake news and chatbots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans have always wanted to talk to machines; the first chatbot, ELIZA,^(28)
    was written at MIT in the 1960s and used a simple program to transform a user's
    input and generate a response, in the mode of a **therapist** who frequently responds
    in the form of a question.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated models can generate entirely novel text, such as Google's
    BERT and GPT-2,^(29 30) which use a unit called a **transformer**. A transformer
    module in a neural network allows a network to propose a new word in the context
    of preceding words in a piece of text, emphasizing those that are more relevant
    in order to generate plausible stretches of language. The BERT model then combines
    transformer units into a powerful multi-dimensional encoding of natural language
    patterns and contextual significance. This approach can be used in document creation
    for **natural language processing** (**NLP**) tasks, or for chatbot dialogue systems
    (*Figure 1.3*).
  prefs: []
  type: TYPE_NORMAL
- en: Sound composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sound, like images or text, is a complex, high-dimensional kind of data. Music
    in particular has many complexities: it could involve one or several musicians,
    has a temporal structure, and can be divided into thematically related segments.
    All of these components are incorporated into models such as MuseGAN, as mentioned
    earlier, which uses GANs to generate these various components and synthesize them
    into realistic, yet synthetic, musical tracks. I will describe the implementation
    of MuseGAN and its variants in *Chapter 11*, *Composing Music with Generative
    Models*.'
  prefs: []
  type: TYPE_NORMAL
- en: The rules of the game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding applications concern data types we can see, hear, or read. However,
    generative models also have applications to generate rules. This is useful in
    a popular application of deep learning: using algorithms to play board games or
    Atari video games.^(31)'
  prefs: []
  type: TYPE_NORMAL
- en: 'While these applications have traditionally used **reinforcement learning**
    (**RL**) techniques to train networks to employ the optimal strategy in these
    games, new research has suggested using GANs to propose novel rules as part of
    the training process,^(32) or to generate synthetic data to prime the overall
    learning process.^(33) We will examine both applications in *Chapter 12*, *Play
    Video Games with Generative AI: GAIL*.'
  prefs: []
  type: TYPE_NORMAL
- en: Unique challenges of generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the powerful applications that generative models have, what are the major
    challenges in implementing them? As described, most of these models utilize complex
    data, requiring us to fit large models to capture all the nuances of their features
    and distribution. This has implications both for the number of examples that we
    must collect to adequately represent the kind of data we are trying to generate,
    and the computational resources needed to build the model. We will discuss techniques
    in *Chapter 2*, *Setting up a TensorFlow Lab*, to parallelize the training of
    these models using cloud computing frameworks and **graphics processing units**
    (**GPUs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'A more subtle problem that comes from having complex data, and the fact that
    we are trying to generate data rather than a numerical label or value, is that
    our notion of model **accuracy** is much more complicated: we cannot simply calculate
    the distance to a single label or scores.'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss in *Chapter 5*, *Painting Pictures with Neural Networks Using
    VAEs*, and *Chapter 6*, *Image Generation with GANs*, how deep generative models
    such as VAE and GAN algorithms take different approaches to determine whether
    a generated image is comparable to a real-world image. Finally, as mentioned,
    our models need to allow us to generate both large and **diverse** samples, and
    the various methods we will discuss take different approaches to control the diversity
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed what generative modeling is, and how it fits into
    the landscape of more familiar machine learning methods. I used probability theory
    and Bayes' theorum to describe how these models approach prediction in an opposite
    manner to discriminative learning.
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed use cases for generative learning, both for specific kinds of data
    and general prediction tasks. Finally, we examined some of the specialized challenges
    that arise from building these models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our practical implementation of these models
    by exploring how to set up a development environment for TensorFlow 2.0 using
    Docker and Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx](https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Baltruschat, I.M., Nickisch, H., Grass, M. et al. (2019). *Comparison of Deep
    Learning Approaches for Multi-Label Chest X-Ray Classification*. Sci Rep 9, 6381\.
    [https://doi.org/10.1038/s41598-019-42294-8](https://doi.org/10.1038/s41598-019-42294-8)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*AlphaGo* (n.d.). DeepMind. Retrieved April 20, 2021, from [https://deepmind.com/research/case-studies/alphago-the-story-so-far](https://deepmind.com/research/case-studies/alphago-the-story-so-far)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The AlphaStar team (2019, October). *AlphaStar: Grandmaster level in StarCraft
    II using multi-agent reinforcement learning*. DeepMind. [https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-rein)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Devlin, J., Chang, M., Lee, K., Toutanova, K. (2019). *BERT: Pre-training of
    Deep Bidirectional Transformers for Language Understanding*. arXiv. [https://arxiv.org/abs/1810.04805v2](https://arxiv.org/abs/1810.04805v2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brandon, J. (2018, February 16). *Terrifying high-tech porn: Creepy ''deepfake''
    videos are on the rise*. Fox News. [https://www.foxnews.com/tech/terrifying-high-tech-porn-creepy-deepfake-videos-are-on-the-rise](https://www.foxnews.com/tech/terrifying-high-tech-porn-creepy-deepfake-videos-are-on-the-rise)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://seanbmcgregor.com/DeepfakeDetectionGame.html](https://seanbmcgregor.com/DeepfakeDetectionGame.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Better Language Models and Their Implications*. (February 14, 2019). OpenAI.
    [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-example-1.png](https://devopstar.com/static/2293f764e1538f357dd1c63035ab25b0/d024a/fake-facebook-conversation-examp)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leviathan Y., Matias Y. (2018, May 8). *Google Duplex: An AI System for Accomplishing
    Real-World Tasks Over the Phone*. Google AI Blog. [https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang and Yi-Hsuan Yang. MuseGAN. [https://salu133445.github.io/musegan/](https://salu133445.github.io/musegan/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg](https://neurohive.io/wp-content/uploads/2018/06/neural-style-transfer-example-e1530287419338.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kolmogorov A. N., (1956). *Foundations of the Theory of Probability*. (2nd edition).
    Chelsea Publishing Company New York. [https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jebara, Tony., (2004). *Machine Learning: Discriminative and Generative*. Kluwer
    Academic (Springer). [https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayes Thomas, (1763) *LII. An essay towards solving a problem in the doctrine
    of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in
    a letter to John Canton, A. M. F. R. S* Phil. Trans. R. Soc.53370–418\. [https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053](https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jebara, Tony., (2004). *Machine Learning: Discriminative and Generative*. Kluwer
    Academic (Springer). [https://www.springer.com/gp/book/9781402076473](https://www.springer.com/gp/book/9781402076473)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. Hinton, S. Osindero, & Y.-W. Teh. (2005). *A Fast Learning Algorithm for
    Deep Belief Nets*. [www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://jaan.io/images/variational-autoencoder-faces.jpg](https://jaan.io/images/variational-autoencoder-faces.jpg)
    and [https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg](https://miro.medium.com/max/2880/1*jcCjbdnN4uEowuHfBoqITQ.jpeg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Esser, P., Haux, J., Ommer, B., (2019). *Unsupervised Robust Disentangling of
    Latent Characteristics for Image Synthesis*. arXiv. [https://arxiv.org/abs/1910.10223](https://arxiv.org/abs/1910.10223)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*CycleGAN*. TensorFlow Core. Retrieved April 26, 2021, from [https://www.tensorflow.org/tutorials/generative/cyclegan](https://www.tensorflow.org/tutorials/generative/cyclegan)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png](https://www.tensorflow.org/tutorials/generative/images/horse2zebra_2.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bourached, A., Cann, G. (2019). *Raiders of the Lost Art*. arXiv:1909.05677\.
    [https://arxiv.org/pdf/1909.05677.pdf](https://arxiv.org/pdf/1909.05677.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y. (2014). *Generative Adversarial Networks*. arXiv.
    [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gorti, S. K., Ma, Jeremy (2018). *Text-to-Image-to-Text Translation using Cycle
    Consistent Adversarial Networks*. arXiv. [https://arxiv.org/abs/1808.04538](https://arxiv.org/abs/1808.04538)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rkjones4, adam-hanna, erincr & rodrigobdz (2020). GANGogh. GitHub repository.
    [https://github.com/rkjones4/GANGogh](https://github.com/rkjones4/GANGogh )
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weizenbaum Joseph. (1976) *Computer and Human Reason*. W. H. Freeman and company.
    [blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf](http://blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Schwartz B., (2019, October 25). *Welcome BERT: Google’s latest search algorithm
    to better understand natural language*. Search Engine Land. [https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-queries-323976](https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-qu)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Better Language Models and Their Implications*. (2019, February 14). OpenAI.
    [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mnih V., Kavukcuoglu K., Silver D., Graves A., Antonoglou I., Wierstra D., Riedmiller
    M. (2013, January 01). *Playing Atari with Deep Reinforcement Learning*. DeepMind.
    [https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Liu, Yang; Zeng, Yifeng; Chen, Yingke; Tang, Jing; Pan, Yinghui (2019). *Self-Improving
    Generative Adversarial Reinforcement Learning*. AAMS 2019\. [http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p52.pdf](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p52.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kasgari, A T Z, Saad, W., Mozaffari, M., Poor, H V (2020). *Experienced Deep
    Reinforcement Learning with Generative Adversarial Networks (GANs) for Model-Free
    Ultra Reliable Low Latency Communication*. arXiv. [https://arxiv.org/abs/1911.03264](https://arxiv.org/abs/1911.03264)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
