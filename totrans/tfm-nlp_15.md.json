["```py\n#Uncomment the following command to display the list of pre-installed modules \n#!pip list -v \n```", "```py\n!pip install transformers\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests \n```", "```py\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw) \n```", "```py\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224') \n```", "```py\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\",predicted_class_idx,\": \", model.config.id2label[predicted_class_idx]) \n```", "```py\nPredicted class: 285 :  Egyptian cat \n```", "```py\n    {0: 'tench, Tinca tinca',1: 'goldfish, Carassius auratus', 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',3: 'tiger shark, Galeocerdo cuvieri',...,999: 'toilet tissue, toilet paper, bathroom tissue'} \n    ```", "```py\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): PatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      ) \n    ```", "```py\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git \n```", "```py\nimport os\nimport clip\nimport torch\nfrom torchvision.datasets import CIFAR100 \n```", "```py\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device) \n```", "```py\n# Download the dataset\ncifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False) \n```", "```py\n# Prepare the inputs\nimage, class_id = cifar100[index]\nimage_input = preprocess(image).unsqueeze(0).to(device)\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device) \n```", "```py\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nplt.imshow(image) \n```", "```py\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs) \n```", "```py\n# Pick the top 5 most similar labels for the image\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(5)\n# Print the result\nprint(\"\\nTop predictions:\\n\")\nfor value, index in zip(values, indices):\n    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\") \n```", "```py\nTop predictions:\n            lion: 96.34%\n           tiger: 1.04%\n           camel: 0.28%\n      lawn_mower: 0.26%\n         leopard: 0.26% \n```", "```py\ncifar100.classes \n```", "```py\n[...,'kangaroo','keyboard','lamp','lawn_mower','leopard','lion',\n 'lizard', ...] \n```", "```py\nCLIP(\n  (visual): VisionTransformer(\n    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        ) \n```", "```py\n(transformer): Transformer(\n    (resblocks): Sequential(\n      (0): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      ) \n```", "```py\n!pip install DALL-E \n```", "```py\nimport io\nimport os, sys\nimport requests\nimport PIL\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom dall_e import map_pixels, unmap_pixels, load_model\nfrom IPython.display import display, display_markdown\ntarget_image_size = 256\ndef download_image(url):\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return PIL.Image.open(io.BytesIO(resp.content))\ndef preprocess(img):\n    s = min(img.size)\n\n    if s < target_image_size:\n        raise ValueError(f'min dim for image {s} < {target_image_size}')\n\n    r = target_image_size / s\n    s = (round(r * img.size[1]), round(r * img.size[0]))\n    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n    img = TF.center_crop(img, output_size=2 * [target_image_size])\n    img = torch.unsqueeze(T.ToTensor()(img), 0)\n    return map_pixels(img) \n```", "```py\n# This can be changed to a GPU, e.g. 'cuda:0'.\ndev = torch.device('cpu')\n# For faster load times, download these files locally and use the local paths instead.\nenc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", dev)\ndec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", dev) \n```", "```py\nx=preprocess(download_image('https://github.com/Denis2054/AI_Educational/blob/master/mycat.jpg?raw=true')) \n```", "```py\ndisplay_markdown('Original image:')\ndisplay(T.ToPILImage(mode='RGB')(x[0])) \n```", "```py\nimport torch.nn.functional as F\nz_logits = enc(x)\nz = torch.argmax(z_logits, axis=1)\nz = F.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()\nx_stats = dec(z).float()\nx_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\nx_rec = T.ToPILImage(mode='RGB')(x_rec[0])\ndisplay_markdown('Reconstructed image:')\ndisplay(x_rec) \n```"]