["```py\n{`\"question\"`: \"is windows movie maker part of windows essentials\"\n`\"passage\"`: \"Windows Movie Maker -- Windows Movie Maker (formerly known as Windows Live Movie Maker in Windows 7) is a discontinued video editing software by Microsoft. It is a part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\", \"idx\": 2, \"label\": true} \n```", "```py\n{`\"premise\"`: \"The Susweca. It means ''dragonfly'' in Sioux, you know. Did I ever tell you that's where Paul and I met?\"\n`\"hypothesis\"`: \"Susweca is where she and Paul met,\"\n`\"label\"`: `\"entailment\"`, \"idx\": 77} \n```", "```py\n\"Text\": \"text\": \"The rally took place on October 17, the shooting on February 29\\. Again, standard filmmaking techniques are interpreted as smooth distortion: \\\"Moore works by depriving you of context and guiding your mind to fill the vacuum -- with completely false ideas. It is brilliantly, if unethically, done.\\\" As noted above, the \\\"from my cold dead hands\\\" part is simply Moore's way to introduce Heston. Did anyone but Moore's critics view it as anything else? He certainly does not \\\"attribute it to a speech where it was not uttered\\\" and, as noted above, doing so twice would make no sense whatsoever if Moore was the mastermind deceiver that his critics claim he is. Concerning the Georgetown Hoya interview where Heston was asked about Rolland, you write: \\\"There is no indication that [Heston] recognized Kayla Rolland's case.\\\" This is naive to the extreme -- Heston would not be president of the NRA if he was not kept up to date on the most prominent cases of gun violence. Even if he did not respond to that part of the interview, he certainly knew about the case at that point. Regarding the NRA website excerpt about the case and the highlighting of the phrase \\\"48 hours after Kayla Rolland is pronounced dead\\\": This is one valid criticism, but far from the deliberate distortion you make it out to be; rather, it is an example for how the facts can sometimes be easy to miss with Moore's fast pace editing. The reason the sentence is highlighted is not to deceive the viewer into believing that Heston hurried to Flint to immediately hold a rally there (as will become quite obvious), but simply to highlight the first mention of the name \\\"Kayla Rolland\\\" in the text, which is in this paragraph. \" \n```", "```py\n`\"question\"`: \"When was Kayla Rolland shot?\"\n`\"answers\"`:\n[{\"text\": \"February 17\", \"idx\": 168, \"label\": 0}, \n`{\"text\": \"February 29\", \"idx\": 169, \"label\": 1},` \n{\"text\": \"October 29\", \"idx\": 170, \"label\": 0},\n{\"text\": \"October 17\", \"idx\": 171, \"label\": 0},\n{\"text\": \"February 17\", \"idx\": 172, \"label\": 0}], \"idx\": 26},\n`{\"question\"`: \"Who was president of the NRA on February 29?\", \n`\"answers\": [{\"text\": \"Charleton Heston\", \"idx\": 173, \"label\": 1}`,\n{\"text\": \"Moore\", \"idx\": 174, \"label\": 0},\n{\"text\": \"George Hoya\", \"idx\": 175, \"label\": 0},\n{\"text\": \"Rolland\", \"idx\": 176, \"label\": 0},\n{\"text\": \"Hoya\", \"idx\": 177, \"label\": 0}, {\"text\": \"Kayla\", \"idx\": 178, \"label\": 0}], \"idx\": 27}, \n```", "```py\n`\"source\"`: \"Daily mail\"\nA passage contains the text and indications as to where the entities are located.\nA passage begins with the text:\n`\"passage\"`: {\n    `\"text\"`: \"A Peruvian tribe once revered by the Inca's for their fierce hunting skills and formidable warriors are clinging on to their traditional existence in the coca growing valleys of South America, sharing their land with drug traffickers, rebels and illegal loggers. Ashaninka Indians are the largest group of indigenous people in the mountainous nation's Amazon region, but their settlements are so sparse that they now make up less than one per cent of Peru's 30 million population. Ever since they battled rival tribes for territory and food during native rule in the rainforests of South America, the Ashaninka have rarely known peace.\\n@highlight\\nThe Ashaninka tribe once shared the Amazon with the like of the Incas hundreds of years ago\\n@highlight\\nThey have been forced to share their land after years of conflict forced rebels and drug dealers into the forest\\n@highlight\\n. Despite settling in valleys rich with valuable coca, they live a poor pre-industrial existence\", \n```", "```py\n `\"entities\": [{\"start\": 2,\"end\": 9}, …,\"start\": 711,\"end\": 715}]` \n```", "```py\n{`\"``query\"`: \"Innocence of youth: Many of the `@placeholder's` younger generations have turned their backs on tribal life and moved to the cities where living conditions are better\", \n`\"answers\"`:[{\"start\":263,\"end\":271,\"text\":\"Ashaninka\"},{\"start\":601,\"end\":609,\"text\":\"Ashaninka\"},{\"start\":651,\"end\":659,\"text\":\"Ashaninka\"}],\"idx\":9}],\"idx\":3} \n```", "```py\n{`\"premise\"`: \"U.S. crude settled $1.32 lower at $42.83 a barrel.\", \n`\"hypothesis\"`: \"Crude the light American lowered to the closing 1.32 dollars, to 42.83 dollars the barrel.\", \"label\": `\"not_entailment\"`, >\"idx\": 19} \n```", "```py\n \"word\": `\"place\"` \n```", "```py\n \"sentence1\": \"Do you want to come over to my place later?\",\n  \"sentence2\": \"A political system with no `place` for the less prominent groups.\" \n```", "```py\n \"idx\": 0,\n  \"label\": false,\n  \"start1\": 31,\n  \"start2\": 27,\n  \"end1\": 36,\n  \"end2\": 32, \n```", "```py\n{`\"text\"`: >\"I poured water from the bottle into the cup until it was full.\",\nThe WSC ask the model to find the target pronoun token number 10 starting at 0: \n`\"target\"`: {`\"span2_index\"`: 10, \nThen it asks the model to determine if \"it\" refers to \"the cup\" or not: \n`\"span1_index\"`: 7,\n`\"span1_text\"`: \"the cup\", \n`\"span2_text\"`: \"it\"}, \nFor sample index #4, the label is true:\n\"idx\": 4, `\"label\"`: true} \n```", "```py\n#@title Loading the Dataset\n#source of dataset : https://nyu-mll.github.io/CoLA/\ndf = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\ndf.shape \n```", "```py\n#@title Loading the Hugging Face Bert Uncased Base Model \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) \n```", "```py\n#@title SST-2 Binary Classification\nfrom transformers import pipeline\nnlp = pipeline(\"sentiment-analysis\")\nprint(nlp(\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start.\"),\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start.\")\nprint(nlp(\"Effective but too-tepid biopic.\"),\"Effective but too-tepid biopic.\") \n```", "```py\n[{'label': 'POSITIVE', 'score': 0.999825656414032}] If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\n[{'label': 'NEGATIVE', 'score': 0.9974064230918884}] Effective but too-tepid biopic. \n```", "```py\n#@title Sequence Classification : paraphrase classification\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nclasses = [\"not paraphrase\", \"is paraphrase\"]\nsequence_A = \"The DVD-CCA then appealed to the state Supreme Court.\"\nsequence_B = \"The DVD CCA appealed that decision to the U.S. Supreme Court.\"\nparaphrase = tokenizer.encode_plus(sequence_A, sequence_B, return_tensors=\"tf\")\nparaphrase_classification_logits = model(paraphrase)[0]\nparaphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\nprint(sequence_B, \"should be a paraphrase\")\nfor i in range(len(classes)):\n    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\") \n```", "```py\nThe DVD CCA appealed that decision to the U.S. Supreme Court. should be a paraphrase\nnot paraphrase: 8.0%\nis paraphrase: 92.0% \n```", "```py\n#@title Winograd\nfrom transformers import pipeline\ntranslator = pipeline(\"translation_en_to_fr\")\nprint(translator(\"The car could not go in the garage because it was too big.\", max_length=40)) \n```", "```py\n[{'translation_text': \"La voiture ne pouvait pas aller dans le garage parce qu'elle était trop grosse.\"}] \n```"]