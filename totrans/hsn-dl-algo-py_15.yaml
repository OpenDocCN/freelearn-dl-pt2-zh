- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the answers to the questions mentioned at the end of each
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 - Introduction to Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The success of machine learning lies in the right set of features. Feature engineering
    plays a crucial role in machine learning. If we handcraft the right set of features
    to predict a certain outcome, then the machine learning algorithms can perform
    well, but finding and engineering the right set of features is not an easy task.
    With deep learning, we don't have to handcraft such features. Since deep **artificial
    neural networks** (**ANNs**) employ several layers, they learn the complex intrinsic
    features and multi-level abstract representation of the data by itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is basically due to the structure of An ANN. ANNs consist of some *n* number
    of layers to perform any computation. We can build an ANN with several layers, where
    each layer is responsible for learning the intricate patterns in the data. Due
    to the computational advancements, we can build a network even with hundreds or
    thousands of layers deep. Since the ANN uses deep layers to perform learning,
    we call it deep learning, and when an ANN uses deep layers to learn, we call it
    a deep network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The activation function is used to introduce non-linearity to the neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we feed any negative input to the ReLU function, it converts them into
    zero. The snag for being zero for all negative values is a problem called **dying
    ReLU.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The whole process of moving from the input layer to the output layer to predict
    output is known as **forward propagation**. During this propagation, the inputs
    are multiplied by their respective weights on each layer and an activation function
    is applied on top of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The whole process of backpropagating the network from the output layer to the
    input layer and updating the weights of the network using gradient descent to
    minimize the loss is called **backpropagation**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient checking is basically used for debugging the gradient descent algorithm
    and to validate that we have a correct implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 2 - Getting to Know TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every computation in TensorFlow is represented by a computational graph. It
    consists of several nodes and edges, where nodes are the mathematical operations,
    such as addition, multiplication, and so on, and edges are the tensors. A computational
    graph is very efficient in optimizing resources and it also promotes distributed
    computing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A computational graph with the operations on the node and tensors to its edges
    will only be created, and in order to execute the graph, we use a TensorFlow session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A TensorFlow session can be created using `tf.Session()`, and it will allocate
    the memory for storing the current value of the variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variables are the containers used to store values. Variables will be used as
    input to several other operations in the computational graph. We can think of placeholders
    as variables, where we only define the type and dimension, but will not assign
    the value. Values for the placeholders will be fed at runtime. We feed the data
    to the computational graphs using placeholders. Placeholders are defined with
    no values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorBoard is TensorFlow's visualization tool that can be used to visualize
    the computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it would become confusing when we have to debug the model.
    As we can visualize the computational graph in TensorBoard, we can easily understand,
    debug and optimize such complex models. It also supports sharing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scoping is used to reduce complexity and helps us to better understand the model
    by grouping the related nodes together. Having a name scope helps us in grouping
    similar operations in a graph. It comes in handy when we are building a complex
    architecture. Scoping can be created using  `tf.name_scope()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operations, eager execution follows the imperative programming paradigm, where
    any operations can be performed immediately without having to create a graph,
    just like we do in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3 - Gradient Descent and Its Variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike gradient descent, in SGD, in order to update the parameter, we don't
    have to iterate through all the data points in our training set. Instead, we just
    iterate through a single data point. That is, unlike gradient descent, we don't
    have to wait to update the parameter of the model after iterating all the data
    points in our training set. We just update the parameters of the model after iterating
    through every single data point in our training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In mini-batch gradient descent, instead of updating the parameters after iterating
    each training sample, we update the parameters after iterating some batches of
    data points. Let's say the batch size is 50, which means that we update the parameter
    of the model after iterating through 50 data points, instead of updating the parameter
    after iterating through each individual data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing mini-batch gradient descent with momentum helps us to reduce oscillations
    in the gradient steps and attain  convergence faster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fundamental motivation behind the Nesterov momentum is that instead of calculating
    the gradient at the current position, we calculate gradients at the position where
    the momentum would take us to, and we call this position the lookahead position.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Adagrad, we set the learning rate to a small value when the past gradients
    value is high and to a high value when the past gradient value is less. So, our
    learning rate value changes according to the past gradients updates of the parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The update equation of Adadelta is given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4a9abe88-0464-4cfd-b112-d0527d05d0e3.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '![](img/09292ed8-827a-4781-b662-a2a9badd2afa.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'RMSProp is introduced to combat the decaying learning rate problem of Adagrad.
    So, in RMSProp, we compute the exponentially decaying running average of gradients
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/09c28ff5-e6dd-46dc-9d3e-92ddfbb3f98a.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Instead of a taking the sum of the square of all the past gradients, we use
    this running average of gradients. So, our update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/203b8439-2352-4af4-ae38-25bb9dbd3ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The update equation of Adam is given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6a8ec672-c917-4536-9927-9fc0673d3a95.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Chapter 4 - Generating Song Lyrics Using an RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A normal feedforward neural network predicts output only based on the current
    input, but an RNN predicts output based on the current input and also the previous
    hidden state, which acts as a memory and stores the contextual information (input)
    that the network has seen so far.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The hidden state, ![](img/30768371-ef5d-487e-94f5-deb5b16bd89c.png), at a time
    step, ![](img/ba5147ce-cc42-49f7-9abd-23780a0ae4c5.png), can be computed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/68c3c600-0c4e-4759-976f-ff8385d33ef7.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In other words, this is *hidden state at a time step, t = tanh([input to hidden
    layer weight x input] + [hidden to hidden layer weight x previous hidden state])*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various natural language processing (NLP) tasks, such as language
    translation, sentiment analysis, text generation, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While backpropagating the RNN, we multiply the weights and derivative of the *tanh* function
    at every time step. When we multiply smaller numbers at every step while moving
    backward, our gradient becomes infinitesimally small and leads to a number that the
    computer can't handle; this is called the **vanishing gradient problem. **
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we initialize the weights of the network to a very large number, the gradients
    will become very large at every step. While backpropagating, we multiply a large
    number together at every time step, and it leads to infinity. This is called the exploding
    gradient problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use gradient clipping to bypass the exploding gradient problem. In this method,
    we normalize the gradients according to a vector norm (say, *L2*) and clip the
    gradient value to a certain range. For instance, if we set the threshold as 0.7,
    then we keep the gradients in the -0.7 to +0.7 range. If the gradient value exceeds
    -0.7, then we change it to -0.7, and similarly, if it exceeds 0.7, then we change
    it to +0.7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different types of RNN architectures include one-to-one, one-to-many, many-to-one,
    and many-to-many, and they are used for various applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 5 - Improvements to the RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**A Long Short-Term Memory** (**LSTM**) cell is a variant of an RNN that resolves
    the vanishing gradient problem by using a special structure called **gates**.
    Gates keep the information in the memory as long as it is required. They learn
    what information to keep and what information to discard from the memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTM consists of three types of gates, namely, the forget gate, the input gate,
    and the output gate. The forget gate is responsible for deciding what information
    should be removed from the cell state (memory). The input gate is responsible
    for deciding what information should be stored in the cell state. The output gate
    is responsible for deciding what information should be taken from the cell state
    to give as an output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell state is also called internal memory where all the information will
    be stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While backpropagating the LSTM network, we need to update too many parameters
    on every iteration. This increases our training time. So, we introduce the **Gated
    Recurrent Units** (**GRU**) cell, which acts as a simplified version of the LSTM
    cell. Unlike LSTM, the GRU cell has only two gates and one hidden state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a bidirectional RNN, we have two different layers of hidden units. Both of
    these layers connect from the input to the output layer. In one layer, the hidden
    states are shared from left to right and in another layer, it is shared from right
    to left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Deep RNN computes the hidden state by taking the previous hidden state and
    also the previous layer's output as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder learns the representation (embeddings) of the given input sentence. Once
    the encoder learns the embedding, it sends the embedding to the decoder. The decoder takes
    this embedding (a thought vector) as input and tries to construct a target sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the input sentence is long, the context vector does not capture the whole
    meaning of the sentence, since it is just the hidden state from the final time
    step. So, instead of taking the last hidden state as a context vector and using
    it for the decoder with an attention mechanism, we take the sum of all the hidden
    states from the encoder and use it as a context vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 6 - Demystifying Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The different layers of CNN include convolution, pooling, and fully connected
    layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We slide over the input matrix with the filter matrix by one pixel and perform
    the convolution operation. But we can not only slide over the input matrix by
    one pixel-we can also slide over the input matrix by any number of pixels. The
    number of pixels we slide over the input matrix by the filter matrix is called **stride**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the convolution operation, we slide over the input matrix with a filter
    matrix. But in some cases, the filter does not perfectly fit the input matrix.
    That is, there exists a situation that when we move our filter matrix by two pixels,
    it reaches the border and the filter does not fit the input matrix, that is, some
    part of our filter matrix is outside the input matrix. In this case, we perform
    padding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pooling layer reduces spatial dimensions by keeping only the important features.
    The different types of pooling operation include max pooling, average pooling,
    and sum pooling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: VGGNet is one of the most popularly used CNN architectures. It was invented
    by the **V****isual Geometry Group** (**VGG**) at the University of Oxford. The
    architecture of the VGG network consists of convolutional layers followed by a
    pooling layer. It uses 3 x 3 convolution and 2 x 2 pooling throughout the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With factorized convolution, we break down a convolutional layer with a larger
    filter size into a stack of convolutional layers, with a smaller filter size.
    So, in the inception block, a convolutional layer with a 5 x 5 filter can be broken
    down into two convolutional layers with 3 x 3 filters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like the CNN, the Capsule network checks the presence of certain features to
    classify the image, but apart from detecting the features, it will also check
    the spatial relationship between them- that is, it learns the hierarchy of the
    features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Capsule networks, apart from calculating probabilities, we also need
    to preserve the direction of the vectors, so we use a different activation function,
    called the squash function. It is given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c4f40149-bfd6-4192-8ce9-91ec5a7f3570.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Chapter 7 - Learning Text Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the **continuous bag-of-words** (**CBOW**) model, we try to predict the target
    word given the context word, and in the skip-gram model, we try to predict the
    context word given the target word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss function of the CBOW model is given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0f91ced9-1243-460a-ae4c-4472584dfbdd.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: When we have millions of words in the vocabulary, we need to perform numerous
    weight updates until we predict the correct target word. It is time-consuming
    and also not an efficient method. So, instead of doing this, we mark the correct
    target word as a positive class and sample a few words from the vocabulary and
    mark it as a negative class, and this is called negative sampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PV-DM is similar to a continuous bag of words model, where we try to predict
    the target word given a context word. In PV-DM, along with word vectors, we introduce
    one more vector, called the paragraph vector. As the name suggests, the paragraph
    vector learns the vector representation of the whole paragraph and it captures
    the subject of the paragraph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The role of an encoder is to map the sentence to a vector and the role of the
    decoder is to generate the surrounding sentences; that is the previous and following
    sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In QuickThoughts is an interesting algorithm for learning the sentence embeddings.
    In quick-thoughts, we try to learn whether a given sentence is related to the
    candidate sentence. So, instead of using a decoder, we use a classifier to learn
    whether a given input sentence is related to the candidate sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 8 - Generating Images Using GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discriminative models learn to find the decision boundary that separates the
    classes in an optimal way, while generative models learn about the characteristics
    of each class. That is, discriminative models predict the labels conditioned on
    the input, ![](img/fa2c1387-f56c-43c8-ba63-aa0745db68bf.png) , whereas generative
    models learn the joint probability distribution, ![](img/808d0d6e-6c0f-467c-99ff-fbdbad2a1bec.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generator learns the distribution of images in our dataset. It learns the
    distribution of handwritten digits in our training set. We feed random noise to
    the generator and it will convert the random noise into a new handwritten digit
    similar to the one in our training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the one generated by the generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss function for the discriminator is given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d1d307f2-b89d-409e-9c8f-fa91a9ce7b85.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The generator loss function is given as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/d0409175-8fed-4dd4-9993-d98cc92f5a16.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: DCGAN extends the design of GANs with convolutional networks. That is, we replace
    with feedforward network in the generator and discriminator with the **Convolutional
    Neural Network** (**CNN**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Kullback-Leibler** (**KL**) divergence is one of the most popularly used
    measures for determining how one probability distribution diverges from the other.
    Let''s say we have two discrete probability distributions, ![](img/f6b015ff-4278-4357-80cd-7715c2e4eede.png) and ![](img/d2cfa544-9d66-44fc-8ed4-e0aaad37e531.png),
    then the KL divergence can be expressed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/942f50b8-21d4-4d9c-953d-fd4f85433903.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The Wasserstein distance, also known as the **Earth Movers** (**EM**) distance,
    is one of the most popularly used distance measures in the optimal transport problems
    where we need to move things from one configuration to another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Lipschitz continuous function is a function that must be continuous and almost
    differentiable everywhere. So, for any function to be a Lipschitz continuous, the
    absolute value of a slope of the function’s graph cannot be more than a constant, ![](img/142295a8-5a99-45a2-aa9c-9c93d45cb141.png).
    This constant, ![](img/dd78343f-755e-4dc4-8b7a-c6e1bf61e24d.png), is called the **Lipschitz** **constant**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 9 - Learning More about GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike vanilla GANs, **CGAN**, is a condition to both the generator and the
    discriminator. This condition tells the GAN what image we are expecting our generator
    to generate. So, both of our components—the discriminator and the generator—act
    upon this condition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code, *c,* is basically interpretable disentangled information. Assuming
    we have some MNIST data, then, code, *c1,* implies the digit label, code, *c2,* implies
    the width, *c3,* implies the stroke of the digit, and so on. We collectively represent
    them by the term *c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mutual information between two random variables tells us the amount of information
    we can obtain from one random variable through another. Mutual information between
    two random variables *x* and *y* can be given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/92cb416a-7c5f-42f4-b722-0c5d821902ed.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: It is basically the difference between the entropy of *y* and the conditional
    entropy of *y* given *x*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code, *c,* gives us the interpretable disentangled information about the
    image. So, we try to find *c* given the image? However, we can't do this easily
    since we don't know the posterior ![](img/e4c1fc8a-4494-49ee-8cf1-8edc71964f0a.png),
    so, we use an auxiliary distribution ![](img/217553a6-2287-4da1-9533-0a9efb2cbdc5.png) to
    learn *c*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The adversarial loss alone does not ensure the proper mapping of the images.
    For instance, a generator can map the images from the source domain to a random
    permutation of images in the target domain that matches the target distribution.
    So, to avoid this, we introduce an additional loss called **cycle consistent lo****ss**.
    It enforces both of the generators *G* and *F* to be cycle consistent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have two generators: ![](img/c2c631a6-2bd4-430f-abf3-1d380dc23ab9.png) and ![](img/b1ae509c-6617-4c92-9ff8-6ac60ce2a981.png). The
    role of ![](img/7d1b5a82-0587-4169-ac1b-afa3b92ef3d7.png) is to learn the mapping
    from ![](img/8c5e6e65-1fd7-4cf4-ac68-309a44cfdeed.png) to ![](img/1e118f8d-61c2-44c1-93d6-3f8e3acb5c94.png) and the
    role of the generator, ![](img/d7ca0a60-3cba-423c-a7a2-11fe6f9d46ef.png), is to
    learn the mapping from y to ![](img/f6b9941a-a0a8-47a0-800d-45e9dca7477d.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stack GANs convert text descriptions into pictures in two stages. In the first
    stage, artists draw primitive shapes and create a basic outline that forms an
    initial version of the image. In the next stage, they enhance the image by making
    it more realistic and appealing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 10 - Reconstructing Inputs Using Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders are unsupervised learning algorithms. Unlike other algorithms,
    autoencoders learn to reconstruct the input, that is, an autoencoder takes the
    input and learns to reproduce the input as an output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can define our loss function as a difference between the actual input and
    reconstructed input as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d26a0352-fd59-400a-b30a-2952b38698d3.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Here, ![](img/f722eddd-2fe5-4e87-829a-d6e3027f1f6a.png) is the number of training
    samples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Convolutional Autoencoder** (**CAE**) that uses a convolutional network instead
    of a vanilla neural network. In the vanilla autoencoders, encoders and decoders
    are basically a feedforward network. But in CAEs, they are basically convolutional
    networks. This means the encoder consists of convolutional layers and the decoder
    consists of transposed convolutional layers, instead of a raw feedforward network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Denoising Autoencoders** (**DAE**) are another small variant of the autoencoder.
    They are mainly used to remove noise from the image, audio, and other inputs.
    So, we feed the corrupted input to the autoencoder and it learns to reconstruct
    the original uncorrupted input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The average activation of the ![](img/a03ad7b2-084e-4041-9430-0b9e428343c4.png) neuron
    in the hidden layer, ![](img/42defd68-1a60-4a28-a297-1aeb213e426d.png) ,over the
    whole training set can be calculated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9bdd477e-fde0-46e3-87bf-2cbc09d72f75.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The loss function of contractive autoencoders can be mathematically represented
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1605d5b1-7e46-457a-9838-f376edb5fedc.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The first term represents the reconstruction error and the second term represents
    the penalty term or the regularizer, and it is basically the** Frobenius** **norm** of
    the **Jacobian matrix**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Frobenius norm, also called the **Hilbert-Schmidt norm**, of a matrix is
    defined as the square root of the sum of the absolute square of its elements. A
    matrix comprising a partial derivative of the vector-valued function is called
    the **Jacobian matrix**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 11 - Exploring Few-Shot Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning from a few data points is called **few-shot** **learning** or **k-shot
    learning**, where *k* specifies the number of data points in each of the classes
    in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need our models to learn from just a few data points. In order to attain
    this, we train them in the same way; that is, we train the model on very few data
    points. Say we have a dataset, ![](img/5abaf867-7d9b-45be-b0cf-72874cfa28a4.png): we sample
    a few data points from each of the classes present in our dataset and we call
    it **support set**. Similarly, we sample some different data points from each
    of the classes and call it **query set**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Siamese networks basically consist of two symmetrical neural networks both sharing
    the same weights and architecture and both joined together at the end using some
    energy function, ![](img/9b56618e-e6cf-4f0c-bacf-466a3e978113.png). The objective
    of our Siamese network is to learn whether the two inputs are similar or dissimilar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The energy function, ![](img/94ce18a5-9eb9-4e23-b8e5-3fbf3905d9ba.png), which
    will give us a similarity between the two inputs. It can be expressed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/31107e05-9bbf-4a23-8411-3700f468bb4d.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Since the goal of the Siamese network is not to perform a classification task
    but to understand the similarity between the two input values, we use the contrastive
    loss function. It can be expressed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7f260d60-d456-43c2-8b8c-0933b522ec98.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Prototypical networks are yet another simple, efficient, and popularly used
    few-shot learning algorithm. The basic idea of the prototypical network is to
    create a prototypical representation of each class and classify a query point
    (new point) based on the distance between the class prototype and the query point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Relation networks consist of two important functions: an embedding function
    denoted by ![](img/a82c6d54-9d40-4f2d-ad3f-20cce11d148e.png) and the relation
    function denoted by ![](img/326bb923-f76d-4493-842b-c4a07d54f7e4.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
