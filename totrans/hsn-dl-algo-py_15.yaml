- en: Assessments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: The following are the answers to the questions mentioned at the end of each
    chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每章末尾提到的问题的答案。
- en: Chapter 1 - Introduction to Deep Learning
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章 - 深度学习简介
- en: The success of machine learning lies in the right set of features. Feature engineering
    plays a crucial role in machine learning. If we handcraft the right set of features
    to predict a certain outcome, then the machine learning algorithms can perform
    well, but finding and engineering the right set of features is not an easy task.
    With deep learning, we don't have to handcraft such features. Since deep **artificial
    neural networks** (**ANNs**) employ several layers, they learn the complex intrinsic
    features and multi-level abstract representation of the data by itself.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习的成功在于正确的特征集合。特征工程在机器学习中扮演了关键角色。如果我们手工设计了正确的特征集合来预测某种结果，那么机器学习算法可以表现良好，但是找到和设计出正确的特征集合并不是一件容易的任务。有了深度学习，我们不需要手工设计这样的特征。由于深度人工神经网络（ANNs）使用了多层，它们自己学习数据的复杂内在特征和多级抽象表示。
- en: It is basically due to the structure of An ANN. ANNs consist of some *n* number
    of layers to perform any computation. We can build an ANN with several layers, where
    each layer is responsible for learning the intricate patterns in the data. Due
    to the computational advancements, we can build a network even with hundreds or
    thousands of layers deep. Since the ANN uses deep layers to perform learning,
    we call it deep learning, and when an ANN uses deep layers to learn, we call it
    a deep network.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这基本上是由于 ANN 的结构。ANN 由一些*n*个层组成，以执行任何计算。我们可以构建一个有多层的ANN，其中每一层负责学习数据中的复杂模式。由于计算技术的进步，我们甚至可以构建深层次的网络，拥有数百甚至数千层。由于ANN使用深层进行学习，我们称其为深度学习；当ANN使用深层进行学习时，我们称其为深度网络。
- en: The activation function is used to introduce non-linearity to the neural networks.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数用于向神经网络引入非线性。
- en: When we feed any negative input to the ReLU function, it converts them into
    zero. The snag for being zero for all negative values is a problem called **dying
    ReLU.**
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们向ReLU函数输入任何负值时，它会将它们转换为零。对于所有负值变为零的问题称为**dying ReLU**。
- en: The whole process of moving from the input layer to the output layer to predict
    output is known as **forward propagation**. During this propagation, the inputs
    are multiplied by their respective weights on each layer and an activation function
    is applied on top of them.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入层到输出层的整个预测输出过程称为**前向传播**。在这个传播过程中，输入会在每一层被其相应的权重乘以，并在其上应用激活函数。
- en: The whole process of backpropagating the network from the output layer to the
    input layer and updating the weights of the network using gradient descent to
    minimize the loss is called **backpropagation**.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输出层向输入层反向传播网络，并使用梯度下降更新网络权重以最小化损失的整个过程称为**反向传播**。
- en: Gradient checking is basically used for debugging the gradient descent algorithm
    and to validate that we have a correct implementation.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度检查基本上用于调试梯度下降算法，并验证我们是否有正确的实现。
- en: Chapter 2 - Getting to Know TensorFlow
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 - 了解 TensorFlow
- en: Every computation in TensorFlow is represented by a computational graph. It
    consists of several nodes and edges, where nodes are the mathematical operations,
    such as addition, multiplication, and so on, and edges are the tensors. A computational
    graph is very efficient in optimizing resources and it also promotes distributed
    computing.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 中的每个计算都由计算图表示。它由多个节点和边组成，其中节点是数学操作，如加法、乘法等，边是张量。计算图在优化资源方面非常高效，也促进了分布式计算。
- en: A computational graph with the operations on the node and tensors to its edges
    will only be created, and in order to execute the graph, we use a TensorFlow session.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个计算图包含了节点上的操作和其边上的张量，只有创建了这个图，我们才能使用 TensorFlow 会话来执行它。
- en: A TensorFlow session can be created using `tf.Session()`, and it will allocate
    the memory for storing the current value of the variable.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`tf.Session()`来创建 TensorFlow 会话，并且它将分配内存以存储变量的当前值。
- en: Variables are the containers used to store values. Variables will be used as
    input to several other operations in the computational graph. We can think of placeholders
    as variables, where we only define the type and dimension, but will not assign
    the value. Values for the placeholders will be fed at runtime. We feed the data
    to the computational graphs using placeholders. Placeholders are defined with
    no values.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量是用来存储值的容器。变量将作为计算图中多个操作的输入。我们可以将占位符视为变量，其中我们只定义类型和维度，但不分配值。占位符的值将在运行时提供。我们通过占位符将数据馈送给计算图。占位符被定义为没有值。
- en: TensorBoard is TensorFlow's visualization tool that can be used to visualize
    the computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it would become confusing when we have to debug the model.
    As we can visualize the computational graph in TensorBoard, we can easily understand,
    debug and optimize such complex models. It also supports sharing.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard 是 TensorFlow 的可视化工具，可以用来可视化计算图。它还可以用来绘制各种定量指标和几个中间计算的结果。当我们训练一个非常深的神经网络时，如果我们不得不调试模型，情况可能变得混乱。通过在
    TensorBoard 中可视化计算图，我们可以轻松理解、调试和优化这样复杂的模型。它还支持共享。
- en: Scoping is used to reduce complexity and helps us to better understand the model
    by grouping the related nodes together. Having a name scope helps us in grouping
    similar operations in a graph. It comes in handy when we are building a complex
    architecture. Scoping can be created using  `tf.name_scope()`.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作用域用于减少复杂性，并通过将相关节点分组来帮助我们更好地理解模型。在图中具有名称作用域有助于我们组织类似操作。当我们构建复杂的架构时，这非常方便。作用域可以使用
    `tf.name_scope()` 创建。
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operations, eager execution follows the imperative programming paradigm, where
    any operations can be performed immediately without having to create a graph,
    just like we do in Python.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 中的急切执行更符合 Python 风格，并允许快速原型设计。与图模式不同，我们无需每次执行操作时都构建一个图表，急切执行遵循命令式编程范例，可以立即执行任何操作，就像在
    Python 中一样。
- en: Chapter 3 - Gradient Descent and Its Variants
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 - 梯度下降及其变体
- en: Unlike gradient descent, in SGD, in order to update the parameter, we don't
    have to iterate through all the data points in our training set. Instead, we just
    iterate through a single data point. That is, unlike gradient descent, we don't
    have to wait to update the parameter of the model after iterating all the data
    points in our training set. We just update the parameters of the model after iterating
    through every single data point in our training set.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与梯度下降不同，在 SGD 中，为了更新参数，我们不必遍历训练集中的所有数据点。相反，我们只需遍历单个数据点。也就是说，与梯度下降不同，在遍历训练集中的所有数据点之后等待更新模型参数是不必要的。我们只需在遍历训练集中的每个单一数据点之后更新模型的参数。
- en: In mini-batch gradient descent, instead of updating the parameters after iterating
    each training sample, we update the parameters after iterating some batches of
    data points. Let's say the batch size is 50, which means that we update the parameter
    of the model after iterating through 50 data points, instead of updating the parameter
    after iterating through each individual data point.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在小批量梯度下降中，我们不是在遍历每个训练样本后更新参数，而是在遍历一些数据点批次后更新参数。假设批量大小为 50，这意味着我们在遍历 50 个数据点后更新模型的参数，而不是在遍历每个单独数据点后更新参数。
- en: Performing mini-batch gradient descent with momentum helps us to reduce oscillations
    in the gradient steps and attain  convergence faster.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用动量执行小批量梯度下降有助于减少梯度步骤中的振荡，并更快地达到收敛。
- en: The fundamental motivation behind the Nesterov momentum is that instead of calculating
    the gradient at the current position, we calculate gradients at the position where
    the momentum would take us to, and we call this position the lookahead position.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nesterov 动量背后的基本动机是，我们不是在当前位置计算梯度，而是在动量将我们带到的位置计算梯度，我们称这个位置为前瞻位置。
- en: In Adagrad, we set the learning rate to a small value when the past gradients
    value is high and to a high value when the past gradient value is less. So, our
    learning rate value changes according to the past gradients updates of the parameter.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Adagrad中，当过去梯度值较高时，我们将学习率设置为较小的值，当过去梯度值较小时，我们将其设置为较高的值。因此，我们的学习率值根据参数过去梯度的更新而改变。
- en: 'The update equation of Adadelta is given as follows:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adadelta的更新方程如下：
- en: '![](img/4a9abe88-0464-4cfd-b112-d0527d05d0e3.png)'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/4a9abe88-0464-4cfd-b112-d0527d05d0e3.png)'
- en: '![](img/09292ed8-827a-4781-b662-a2a9badd2afa.png)'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/09292ed8-827a-4781-b662-a2a9badd2afa.png)'
- en: 'RMSProp is introduced to combat the decaying learning rate problem of Adagrad.
    So, in RMSProp, we compute the exponentially decaying running average of gradients
    as follows:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RMSProp是为了解决Adagrad的学习率衰减问题而引入的。因此，在RMSProp中，我们计算梯度的指数衰减运行平均值如下：
- en: '![](img/09c28ff5-e6dd-46dc-9d3e-92ddfbb3f98a.png)'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/09c28ff5-e6dd-46dc-9d3e-92ddfbb3f98a.png)'
- en: 'Instead of a taking the sum of the square of all the past gradients, we use
    this running average of gradients. So, our update equation becomes the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是采用过去所有梯度的平方和，而是使用这些梯度的运行平均值。因此，我们的更新方程如下：
- en: '![](img/203b8439-2352-4af4-ae38-25bb9dbd3ed9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/203b8439-2352-4af4-ae38-25bb9dbd3ed9.png)'
- en: 'The update equation of Adam is given as follows:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adam的更新方程如下：
- en: '![](img/6a8ec672-c917-4536-9927-9fc0673d3a95.png)'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/6a8ec672-c917-4536-9927-9fc0673d3a95.png)'
- en: Chapter 4 - Generating Song Lyrics Using an RNN
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 - 使用RNN生成歌词
- en: A normal feedforward neural network predicts output only based on the current
    input, but an RNN predicts output based on the current input and also the previous
    hidden state, which acts as a memory and stores the contextual information (input)
    that the network has seen so far.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个普通的前馈神经网络仅基于当前输入预测输出，但是循环神经网络基于当前输入和前一个隐藏状态预测输出，前者充当内存并存储到目前为止网络所见的上下文信息（输入）。
- en: 'The hidden state, ![](img/30768371-ef5d-487e-94f5-deb5b16bd89c.png), at a time
    step, ![](img/ba5147ce-cc42-49f7-9abd-23780a0ae4c5.png), can be computed as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步长为![](img/ba5147ce-cc42-49f7-9abd-23780a0ae4c5.png)时，隐藏状态![](img/30768371-ef5d-487e-94f5-deb5b16bd89c.png)可以计算如下：
- en: '![](img/68c3c600-0c4e-4759-976f-ff8385d33ef7.png)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/68c3c600-0c4e-4759-976f-ff8385d33ef7.png)'
- en: In other words, this is *hidden state at a time step, t = tanh([input to hidden
    layer weight x input] + [hidden to hidden layer weight x previous hidden state])*.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 换句话说，这是一个*时间步长t时的隐藏状态，tanh([输入到隐藏层权重 x 输入] + [隐藏到隐藏层权重 x 上一个隐藏状态])*。
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various natural language processing (NLP) tasks, such as language
    translation, sentiment analysis, text generation, and so on.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN广泛应用于涉及序列数据的用例，如时间序列、文本、音频、语音、视频、天气等。它们在各种自然语言处理（NLP）任务中得到广泛应用，如语言翻译、情感分析、文本生成等。
- en: While backpropagating the RNN, we multiply the weights and derivative of the *tanh* function
    at every time step. When we multiply smaller numbers at every step while moving
    backward, our gradient becomes infinitesimally small and leads to a number that the
    computer can't handle; this is called the **vanishing gradient problem. **
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播RNN时，我们在每个时间步长乘以权重和*tanh*函数的导数。当我们在向后移动时在每一步乘以较小的数字时，我们的梯度变得微小，导致计算机无法处理的数值；这就是所谓的梯度消失问题。
- en: When we initialize the weights of the network to a very large number, the gradients
    will become very large at every step. While backpropagating, we multiply a large
    number together at every time step, and it leads to infinity. This is called the exploding
    gradient problem.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们将网络的权重初始化为非常大的数时，梯度将在每一步变得非常大。在反向传播时，我们在每个时间步长都乘以一个大数，这会导致梯度变为无穷大。这就是所谓的梯度爆炸问题。
- en: We use gradient clipping to bypass the exploding gradient problem. In this method,
    we normalize the gradients according to a vector norm (say, *L2*) and clip the
    gradient value to a certain range. For instance, if we set the threshold as 0.7,
    then we keep the gradients in the -0.7 to +0.7 range. If the gradient value exceeds
    -0.7, then we change it to -0.7, and similarly, if it exceeds 0.7, then we change
    it to +0.7.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用梯度裁剪来规避梯度爆炸问题。在这种方法中，我们根据向量范数（比如，*L2*）对梯度进行归一化，并将梯度值裁剪到一定范围内。例如，如果我们将阈值设为0.7，那么我们将梯度保持在-0.7到+0.7的范围内。如果梯度值超过-0.7，则将其更改为-0.7；同样地，如果超过了0.7，则将其更改为+0.7。
- en: Different types of RNN architectures include one-to-one, one-to-many, many-to-one,
    and many-to-many, and they are used for various applications.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同类型的RNN架构包括一对一、一对多、多对一和多对多，并且它们用于各种应用。
- en: Chapter 5 - Improvements to the RNN
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 - RNN的改进
- en: '**A Long Short-Term Memory** (**LSTM**) cell is a variant of an RNN that resolves
    the vanishing gradient problem by using a special structure called **gates**.
    Gates keep the information in the memory as long as it is required. They learn
    what information to keep and what information to discard from the memory.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）单元是RNN的一种变体，通过使用称为**门**的特殊结构解决了梯度消失问题。门控制信息在记忆中保持所需的时间。它们学习保留哪些信息和丢弃哪些信息。'
- en: LSTM consists of three types of gates, namely, the forget gate, the input gate,
    and the output gate. The forget gate is responsible for deciding what information
    should be removed from the cell state (memory). The input gate is responsible
    for deciding what information should be stored in the cell state. The output gate
    is responsible for deciding what information should be taken from the cell state
    to give as an output.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM由三种类型的门组成，即遗忘门、输入门和输出门。遗忘门负责决定从细胞状态（记忆）中移除哪些信息。输入门负责决定将哪些信息存储在细胞状态中。输出门负责决定从细胞状态中提取哪些信息作为输出。
- en: The cell state is also called internal memory where all the information will
    be stored.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞状态也称为内部记忆，所有信息都将存储在这里。
- en: While backpropagating the LSTM network, we need to update too many parameters
    on every iteration. This increases our training time. So, we introduce the **Gated
    Recurrent Units** (**GRU**) cell, which acts as a simplified version of the LSTM
    cell. Unlike LSTM, the GRU cell has only two gates and one hidden state.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播LSTM网络时，我们需要在每次迭代中更新太多的参数。这增加了我们的训练时间。因此，我们引入了作为LSTM单元简化版本的**门控循环单元**（**GRU**）单元。与LSTM不同，GRU单元只有两个门和一个隐藏状态。
- en: In a bidirectional RNN, we have two different layers of hidden units. Both of
    these layers connect from the input to the output layer. In one layer, the hidden
    states are shared from left to right and in another layer, it is shared from right
    to left.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在双向RNN中，我们有两个不同的隐藏单元层。这两个层从输入连接到输出层。在一个层中，隐藏状态从左到右共享，在另一个层中，从右到左共享。
- en: A Deep RNN computes the hidden state by taking the previous hidden state and
    also the previous layer's output as input.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度RNN通过使用前一个隐藏状态和前一层的输出来计算隐藏状态。
- en: The encoder learns the representation (embeddings) of the given input sentence. Once
    the encoder learns the embedding, it sends the embedding to the decoder. The decoder takes
    this embedding (a thought vector) as input and tries to construct a target sentence.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器学习给定输入句子的表示（嵌入）。一旦编码器学习到嵌入，它将嵌入发送给解码器。解码器将这个嵌入（思维向量）作为输入，并尝试构建目标句子。
- en: When the input sentence is long, the context vector does not capture the whole
    meaning of the sentence, since it is just the hidden state from the final time
    step. So, instead of taking the last hidden state as a context vector and using
    it for the decoder with an attention mechanism, we take the sum of all the hidden
    states from the encoder and use it as a context vector.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当输入句子很长时，上下文向量并不能捕获整个句子的完整含义，因为它只是最终时间步的隐藏状态。因此，我们不是将最后的隐藏状态作为上下文向量并在解码器中使用注意力机制，而是取所有隐藏状态的总和作为上下文向量。
- en: Chapter 6 - Demystifying Convolutional Networks
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 - 解密卷积网络
- en: The different layers of CNN include convolution, pooling, and fully connected
    layers.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN的不同层包括卷积、池化和全连接层。
- en: We slide over the input matrix with the filter matrix by one pixel and perform
    the convolution operation. But we can not only slide over the input matrix by
    one pixel-we can also slide over the input matrix by any number of pixels. The
    number of pixels we slide over the input matrix by the filter matrix is called **stride**.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过一个像素滑动输入矩阵和过滤矩阵，并执行卷积操作。但我们不仅可以通过一个像素滑动输入矩阵，还可以通过任意数量的像素滑动输入矩阵。我们通过过滤矩阵滑动输入矩阵的像素数称为**步长**。
- en: With the convolution operation, we slide over the input matrix with a filter
    matrix. But in some cases, the filter does not perfectly fit the input matrix.
    That is, there exists a situation that when we move our filter matrix by two pixels,
    it reaches the border and the filter does not fit the input matrix, that is, some
    part of our filter matrix is outside the input matrix. In this case, we perform
    padding.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在卷积操作中，我们用一个过滤器矩阵滑动在输入矩阵上。但在某些情况下，过滤器不能完全适应输入矩阵。也就是说，当我们将我们的过滤器矩阵移动两个像素时，它会到达边界，过滤器不适合输入矩阵，也就是说，我们的过滤器矩阵的某些部分在输入矩阵之外。在这种情况下，我们进行填充。
- en: The pooling layer reduces spatial dimensions by keeping only the important features.
    The different types of pooling operation include max pooling, average pooling,
    and sum pooling.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化层通过保留重要特征来减少空间维度。不同类型的池化操作包括最大池化、平均池化和总和池化。
- en: VGGNet is one of the most popularly used CNN architectures. It was invented
    by the **V****isual Geometry Group** (**VGG**) at the University of Oxford. The
    architecture of the VGG network consists of convolutional layers followed by a
    pooling layer. It uses 3 x 3 convolution and 2 x 2 pooling throughout the network.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VGGNet是最广泛使用的CNN架构之一。它由牛津大学的**视觉几何组**（**VGG**）发明。VGG网络的架构由卷积层和池化层组成。它在整个网络中使用3
    x 3卷积和2 x 2池化。
- en: With factorized convolution, we break down a convolutional layer with a larger
    filter size into a stack of convolutional layers, with a smaller filter size.
    So, in the inception block, a convolutional layer with a 5 x 5 filter can be broken
    down into two convolutional layers with 3 x 3 filters.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分解卷积层，我们将具有较大过滤器尺寸的卷积层分解为具有较小过滤器尺寸的一堆卷积层。因此，在inception块中，具有5 x 5过滤器的卷积层可以分解为两个具有3
    x 3过滤器的卷积层。
- en: Like the CNN, the Capsule network checks the presence of certain features to
    classify the image, but apart from detecting the features, it will also check
    the spatial relationship between them- that is, it learns the hierarchy of the
    features.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于CNN，胶囊网络检查某些特征的存在以对图像进行分类，但除了检测特征外，它还会检查它们之间的空间关系 - 也就是说，它学习特征的层次结构。
- en: 'In the Capsule networks, apart from calculating probabilities, we also need
    to preserve the direction of the vectors, so we use a different activation function,
    called the squash function. It is given as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在胶囊网络中，除了计算概率之外，我们还需要保留向量的方向，因此我们使用一种称为压缩函数的不同激活函数。它如下所示：
- en: '![](img/c4f40149-bfd6-4192-8ce9-91ec5a7f3570.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/c4f40149-bfd6-4192-8ce9-91ec5a7f3570.png)'
- en: Chapter 7 - Learning Text Representations
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 - 学习文本表示
- en: In the **continuous bag-of-words** (**CBOW**) model, we try to predict the target
    word given the context word, and in the skip-gram model, we try to predict the
    context word given the target word.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**连续词袋**（**CBOW**）模型中，我们试图预测给定上下文词的目标词，而在skip-gram模型中，我们试图预测给定目标词的上下文词。
- en: 'The loss function of the CBOW model is given as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CBOW模型的损失函数如下所示：
- en: '![](img/0f91ced9-1243-460a-ae4c-4472584dfbdd.png)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/0f91ced9-1243-460a-ae4c-4472584dfbdd.png)'
- en: When we have millions of words in the vocabulary, we need to perform numerous
    weight updates until we predict the correct target word. It is time-consuming
    and also not an efficient method. So, instead of doing this, we mark the correct
    target word as a positive class and sample a few words from the vocabulary and
    mark it as a negative class, and this is called negative sampling
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的词汇表中有数百万个单词时，我们需要执行大量的权重更新，直到预测正确的目标词。这是耗时且不高效的方法。因此，我们不是这样做，而是将正确的目标词标记为正类，并从词汇表中随机抽取几个词并标记为负类，这被称为负采样。
- en: PV-DM is similar to a continuous bag of words model, where we try to predict
    the target word given a context word. In PV-DM, along with word vectors, we introduce
    one more vector, called the paragraph vector. As the name suggests, the paragraph
    vector learns the vector representation of the whole paragraph and it captures
    the subject of the paragraph.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PV-DM类似于连续词袋模型，其中我们试图预测给定上下文词的目标词。在PV-DM中，除了词向量外，我们引入了另一个向量，称为段落向量。顾名思义，段落向量学习整个段落的向量表示，并捕捉段落的主题。
- en: The role of an encoder is to map the sentence to a vector and the role of the
    decoder is to generate the surrounding sentences; that is the previous and following
    sentences.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器的作用是将句子映射到向量，解码器的作用是生成周围的句子；即前面和后面的句子。
- en: In QuickThoughts is an interesting algorithm for learning the sentence embeddings.
    In quick-thoughts, we try to learn whether a given sentence is related to the
    candidate sentence. So, instead of using a decoder, we use a classifier to learn
    whether a given input sentence is related to the candidate sentence.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 QuickThoughts 中，有一个有趣的算法用于学习句子嵌入。在 quick-thoughts 中，我们试图学习一个给定句子是否与候选句子相关。因此，我们使用分类器而不是解码器来学习是否一个给定的输入句子与候选句子相关。
- en: Chapter 8 - Generating Images Using GANs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章 - 使用 GAN 生成图像
- en: Discriminative models learn to find the decision boundary that separates the
    classes in an optimal way, while generative models learn about the characteristics
    of each class. That is, discriminative models predict the labels conditioned on
    the input, ![](img/fa2c1387-f56c-43c8-ba63-aa0745db68bf.png) , whereas generative
    models learn the joint probability distribution, ![](img/808d0d6e-6c0f-467c-99ff-fbdbad2a1bec.png).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 辨别模型学习如何以最佳方式找到分隔类别的决策边界，而生成模型则学习每个类别的特征。也就是说，辨别模型预测输入条件下的标签， ![](img/fa2c1387-f56c-43c8-ba63-aa0745db68bf.png)，而生成模型学习联合概率分布， ![](img/808d0d6e-6c0f-467c-99ff-fbdbad2a1bec.png)。
- en: The generator learns the distribution of images in our dataset. It learns the
    distribution of handwritten digits in our training set. We feed random noise to
    the generator and it will convert the random noise into a new handwritten digit
    similar to the one in our training set.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器学习数据集中图像的分布。它学习训练集中手写数字的分布。我们向生成器输入随机噪声，它将把随机噪声转换成与训练集中类似的新手写数字。
- en: The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the one generated by the generator.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器的目标是执行分类任务。给定一幅图像，它将其分类为真实或虚假；也就是说，图像是来自训练集还是生成器生成的。
- en: 'The loss function for the discriminator is given as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器的损失函数如下所示：
- en: '![](img/d1d307f2-b89d-409e-9c8f-fa91a9ce7b85.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/d1d307f2-b89d-409e-9c8f-fa91a9ce7b85.png)'
- en: 'The generator loss function is given as follows:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下所示：
- en: '![](img/d0409175-8fed-4dd4-9993-d98cc92f5a16.png)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/d0409175-8fed-4dd4-9993-d98cc92f5a16.png)'
- en: DCGAN extends the design of GANs with convolutional networks. That is, we replace
    with feedforward network in the generator and discriminator with the **Convolutional
    Neural Network** (**CNN**).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DCGAN 使用卷积网络扩展了 GAN 的设计。也就是说，我们用卷积神经网络替换了生成器和判别器中的前馈网络，**卷积神经网络**（**CNN**）。
- en: 'The **Kullback-Leibler** (**KL**) divergence is one of the most popularly used
    measures for determining how one probability distribution diverges from the other.
    Let''s say we have two discrete probability distributions, ![](img/f6b015ff-4278-4357-80cd-7715c2e4eede.png) and ![](img/d2cfa544-9d66-44fc-8ed4-e0aaad37e531.png),
    then the KL divergence can be expressed as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler**（**KL**）散度是用于确定一个概率分布与另一个概率分布之间差异的最常用度量之一。假设我们有两个离散概率分布， ![](img/f6b015ff-4278-4357-80cd-7715c2e4eede.png) 和 ![](img/d2cfa544-9d66-44fc-8ed4-e0aaad37e531.png)，那么
    KL 散度可以表达如下：'
- en: '![](img/942f50b8-21d4-4d9c-953d-fd4f85433903.png)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/942f50b8-21d4-4d9c-953d-fd4f85433903.png)'
- en: The Wasserstein distance, also known as the **Earth Movers** (**EM**) distance,
    is one of the most popularly used distance measures in the optimal transport problems
    where we need to move things from one configuration to another.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 瓦瑟斯坦距离，也被称为**地球移动距离**（**EM**）距离，在最优传输问题中是最常用的距离度量之一，用于从一种配置移动物品到另一种配置。
- en: A Lipschitz continuous function is a function that must be continuous and almost
    differentiable everywhere. So, for any function to be a Lipschitz continuous, the
    absolute value of a slope of the function’s graph cannot be more than a constant, ![](img/142295a8-5a99-45a2-aa9c-9c93d45cb141.png).
    This constant, ![](img/dd78343f-755e-4dc4-8b7a-c6e1bf61e24d.png), is called the **Lipschitz** **constant**.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利普希茨连续函数是一种必须处处连续且几乎处处可微的函数。因此，对于任何利普希茨连续函数，函数图形斜率的绝对值不能超过一个常数， ![](img/142295a8-5a99-45a2-aa9c-9c93d45cb141.png)。这个常数， ![](img/dd78343f-755e-4dc4-8b7a-c6e1bf61e24d.png)，称为**利普希茨常数**。
- en: Chapter 9 - Learning More about GANs
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章 - 深入学习 GAN
- en: Unlike vanilla GANs, **CGAN**, is a condition to both the generator and the
    discriminator. This condition tells the GAN what image we are expecting our generator
    to generate. So, both of our components—the discriminator and the generator—act
    upon this condition.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与普通 GAN 不同，**条件生成对抗网络（CGAN）**对生成器和鉴别器都施加条件。这个条件告诉 GAN 我们期望生成器生成什么图像。因此，我们的两个组件——鉴别器和生成器——都基于这个条件进行操作。
- en: The code, *c,* is basically interpretable disentangled information. Assuming
    we have some MNIST data, then, code, *c1,* implies the digit label, code, *c2,* implies
    the width, *c3,* implies the stroke of the digit, and so on. We collectively represent
    them by the term *c*.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码 *c* 基本上是可解释的解缠信息。假设我们有一些 MNIST 数据，那么，代码 *c1* 表示数字标签，代码 *c2* 表示数字的宽度，代码 *c3*
    表示数字的笔画等。我们用术语 *c* 统称它们。
- en: 'Mutual information between two random variables tells us the amount of information
    we can obtain from one random variable through another. Mutual information between
    two random variables *x* and *y* can be given as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个随机变量之间的互信息告诉我们通过一个随机变量可以从另一个随机变量获取的信息量。随机变量 *x* 和 *y* 之间的互信息可以表示如下：
- en: '![](img/92cb416a-7c5f-42f4-b722-0c5d821902ed.png)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/92cb416a-7c5f-42f4-b722-0c5d821902ed.png)'
- en: It is basically the difference between the entropy of *y* and the conditional
    entropy of *y* given *x*.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它基本上是 *y* 的熵和在给定 *x* 的条件下 *y* 的条件熵之间的差异。
- en: The code, *c,* gives us the interpretable disentangled information about the
    image. So, we try to find *c* given the image? However, we can't do this easily
    since we don't know the posterior ![](img/e4c1fc8a-4494-49ee-8cf1-8edc71964f0a.png),
    so, we use an auxiliary distribution ![](img/217553a6-2287-4da1-9533-0a9efb2cbdc5.png) to
    learn *c*.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码 *c* 给出了关于图像的可解释的解缠信息。因此，我们试图在不知道后验分布 ![](img/e4c1fc8a-4494-49ee-8cf1-8edc71964f0a.png)
    的情况下找到 *c*，因此，我们使用辅助分布 ![](img/217553a6-2287-4da1-9533-0a9efb2cbdc5.png) 来学习 *c*。
- en: The adversarial loss alone does not ensure the proper mapping of the images.
    For instance, a generator can map the images from the source domain to a random
    permutation of images in the target domain that matches the target distribution.
    So, to avoid this, we introduce an additional loss called **cycle consistent lo****ss**.
    It enforces both of the generators *G* and *F* to be cycle consistent.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅仅靠对抗损失并不能确保图像的正确映射。例如，生成器可以将源域中的图像映射到与目标域中的目标分布相匹配的随机排列图像中。因此，为了避免这种情况，我们引入了一种额外的损失，称为**循环一致性损失**。它强制生成器
    *G* 和 *F* 都保持循环一致。
- en: We have two generators: ![](img/c2c631a6-2bd4-430f-abf3-1d380dc23ab9.png) and ![](img/b1ae509c-6617-4c92-9ff8-6ac60ce2a981.png). The
    role of ![](img/7d1b5a82-0587-4169-ac1b-afa3b92ef3d7.png) is to learn the mapping
    from ![](img/8c5e6e65-1fd7-4cf4-ac68-309a44cfdeed.png) to ![](img/1e118f8d-61c2-44c1-93d6-3f8e3acb5c94.png) and the
    role of the generator, ![](img/d7ca0a60-3cba-423c-a7a2-11fe6f9d46ef.png), is to
    learn the mapping from y to ![](img/f6b9941a-a0a8-47a0-800d-45e9dca7477d.png).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有两个生成器：![](img/c2c631a6-2bd4-430f-abf3-1d380dc23ab9.png) 和 ![](img/b1ae509c-6617-4c92-9ff8-6ac60ce2a981.png)。![](img/7d1b5a82-0587-4169-ac1b-afa3b92ef3d7.png)
    的作用是学习从 ![](img/8c5e6e65-1fd7-4cf4-ac68-309a44cfdeed.png) 到 ![](img/1e118f8d-61c2-44c1-93d6-3f8e3acb5c94.png)
    的映射，生成器 ![](img/d7ca0a60-3cba-423c-a7a2-11fe6f9d46ef.png) 的作用是学习从 y 到 ![](img/f6b9941a-a0a8-47a0-800d-45e9dca7477d.png)
    的映射。
- en: Stack GANs convert text descriptions into pictures in two stages. In the first
    stage, artists draw primitive shapes and create a basic outline that forms an
    initial version of the image. In the next stage, they enhance the image by making
    it more realistic and appealing.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Stack GANs 将文本描述转换为图像分为两个阶段。在第一阶段，艺术家绘制基本形状，并创建形成图像初始版本的基本轮廓。在下一个阶段，他们通过使图像更加真实和吸引人来增强图像。
- en: Chapter 10 - Reconstructing Inputs Using Autoencoders
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章 - 使用自编码器重建输入
- en: Autoencoders are unsupervised learning algorithms. Unlike other algorithms,
    autoencoders learn to reconstruct the input, that is, an autoencoder takes the
    input and learns to reproduce the input as an output.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器是一种无监督学习算法。与其他算法不同，自编码器学习重建输入，也就是说，自编码器接受输入并学习以输出形式复制输入。
- en: 'We can define our loss function as a difference between the actual input and
    reconstructed input as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将我们的损失函数定义为实际输入和重构输入之间的差异，如下所示：
- en: '![](img/d26a0352-fd59-400a-b30a-2952b38698d3.png)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/d26a0352-fd59-400a-b30a-2952b38698d3.png)'
- en: Here, ![](img/f722eddd-2fe5-4e87-829a-d6e3027f1f6a.png) is the number of training
    samples.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，![](img/f722eddd-2fe5-4e87-829a-d6e3027f1f6a.png) 是训练样本的数量。
- en: '**Convolutional Autoencoder** (**CAE**) that uses a convolutional network instead
    of a vanilla neural network. In the vanilla autoencoders, encoders and decoders
    are basically a feedforward network. But in CAEs, they are basically convolutional
    networks. This means the encoder consists of convolutional layers and the decoder
    consists of transposed convolutional layers, instead of a raw feedforward network.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积自编码器**（**CAE**）使用卷积网络而不是普通的神经网络。在普通自编码器中，编码器和解码器基本上是前馈网络。但在CAE中，它们基本上是卷积网络。这意味着编码器由卷积层组成，解码器由转置卷积层组成，而不是原始的前馈网络。'
- en: '**Denoising Autoencoders** (**DAE**) are another small variant of the autoencoder.
    They are mainly used to remove noise from the image, audio, and other inputs.
    So, we feed the corrupted input to the autoencoder and it learns to reconstruct
    the original uncorrupted input.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**去噪自编码器**（**DAE**）是自编码器的另一个小变体。它们主要用于从图像、音频和其他输入中去除噪声。因此，我们将损坏的输入提供给自编码器，它学习重构原始的未损坏输入。'
- en: 'The average activation of the ![](img/a03ad7b2-084e-4041-9430-0b9e428343c4.png) neuron
    in the hidden layer, ![](img/42defd68-1a60-4a28-a297-1aeb213e426d.png) ,over the
    whole training set can be calculated as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层中的神经元![](img/a03ad7b2-084e-4041-9430-0b9e428343c4.png)的平均激活可以计算为整个训练集上的以下值：
- en: '![](img/9bdd477e-fde0-46e3-87bf-2cbc09d72f75.png)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/9bdd477e-fde0-46e3-87bf-2cbc09d72f75.png)'
- en: 'The loss function of contractive autoencoders can be mathematically represented
    as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗自编码器的损失函数可以数学表示如下：
- en: '![](img/1605d5b1-7e46-457a-9838-f376edb5fedc.png)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/1605d5b1-7e46-457a-9838-f376edb5fedc.png)'
- en: The first term represents the reconstruction error and the second term represents
    the penalty term or the regularizer, and it is basically the** Frobenius** **norm** of
    the **Jacobian matrix**.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一项代表重构误差，第二项代表惩罚项或正则化器，基本上是**Jacobian矩阵**的**Frobenius范数**。
- en: The Frobenius norm, also called the **Hilbert-Schmidt norm**, of a matrix is
    defined as the square root of the sum of the absolute square of its elements. A
    matrix comprising a partial derivative of the vector-valued function is called
    the **Jacobian matrix**.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵的Frobenius范数，也称为**Hilbert-Schmidt范数**，定义为其元素的绝对平方和的平方根。由向量值函数的偏导组成的矩阵称为**Jacobian矩阵**。
- en: Chapter 11 - Exploring Few-Shot Learning Algorithms
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 - 探索少样本学习算法
- en: Learning from a few data points is called **few-shot** **learning** or **k-shot
    learning**, where *k* specifies the number of data points in each of the classes
    in the dataset.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从少量数据点中学习称为**少样本学习**或**k-样本学习**，其中*k*指定数据集中每个类中的数据点数。
- en: We need our models to learn from just a few data points. In order to attain
    this, we train them in the same way; that is, we train the model on very few data
    points. Say we have a dataset, ![](img/5abaf867-7d9b-45be-b0cf-72874cfa28a4.png): we sample
    a few data points from each of the classes present in our dataset and we call
    it **support set**. Similarly, we sample some different data points from each
    of the classes and call it **query set**.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要我们的模型仅从少量数据点中学习。为了实现这一点，我们以相同的方式训练它们；也就是说，我们在很少的数据点上训练模型。假设我们有一个数据集，![](img/5abaf867-7d9b-45be-b0cf-72874cfa28a4.png)：我们从每个类别中取样少量数据点，并称之为**支持集**。类似地，我们从每个类别中取样一些不同的数据点，并称之为**查询集**。
- en: Siamese networks basically consist of two symmetrical neural networks both sharing
    the same weights and architecture and both joined together at the end using some
    energy function, ![](img/9b56618e-e6cf-4f0c-bacf-466a3e978113.png). The objective
    of our Siamese network is to learn whether the two inputs are similar or dissimilar.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Siamese网络基本上由两个对称的神经网络组成，两者共享相同的权重和架构，并在末端使用某种能量函数![](img/9b56618e-e6cf-4f0c-bacf-466a3e978113.png)连接在一起。我们Siamese网络的目标是学习两个输入是否相似或不相似。
- en: 'The energy function, ![](img/94ce18a5-9eb9-4e23-b8e5-3fbf3905d9ba.png), which
    will give us a similarity between the two inputs. It can be expressed as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 能量函数![](img/94ce18a5-9eb9-4e23-b8e5-3fbf3905d9ba.png)，它将为我们提供两个输入之间的相似度。它可以表达如下：
- en: '![](img/31107e05-9bbf-4a23-8411-3700f468bb4d.png)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/31107e05-9bbf-4a23-8411-3700f468bb4d.png)'
- en: 'Since the goal of the Siamese network is not to perform a classification task
    but to understand the similarity between the two input values, we use the contrastive
    loss function. It can be expressed as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Siamese网络的目标不是执行分类任务，而是理解两个输入值之间的相似性，因此我们使用对比损失函数。它可以表达如下：
- en: '![](img/7f260d60-d456-43c2-8b8c-0933b522ec98.png)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/7f260d60-d456-43c2-8b8c-0933b522ec98.png)'
- en: Prototypical networks are yet another simple, efficient, and popularly used
    few-shot learning algorithm. The basic idea of the prototypical network is to
    create a prototypical representation of each class and classify a query point
    (new point) based on the distance between the class prototype and the query point.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原型网络是另一种简单、高效且广泛使用的少样本学习算法。原型网络的基本思想是为每个类别创建一个原型表示，并根据类别原型与查询点（新点）之间的距离对查询点进行分类。
- en: 'Relation networks consist of two important functions: an embedding function
    denoted by ![](img/a82c6d54-9d40-4f2d-ad3f-20cce11d148e.png) and the relation
    function denoted by ![](img/326bb923-f76d-4493-842b-c4a07d54f7e4.png).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关系网络由两个重要函数组成：一个嵌入函数，用 ![](img/a82c6d54-9d40-4f2d-ad3f-20cce11d148e.png) 表示，和一个关系函数，用 ![](img/326bb923-f76d-4493-842b-c4a07d54f7e4.png) 表示。
