- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring Deep Learning Endpoints in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the difference in development and production settings, it is difficult
    to assure the performance of **deep learning** (**DL**) models once they are deployed.
    If any difference exists in model behavior, it must be captured within a reasonable
    time; otherwise, it can affect downstream applications in negative ways.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our goal is to explain existing solutions for monitoring DL
    model behavior in production. We will start by clearly describing the benefit
    of monitoring and what it takes to keep the overall system running in a stable
    manner. Then, we will discuss popular tools for monitoring DL models and alerting.
    Out of the various tools we introduce, we will get our hands dirty with **CloudWatch**.
    We will start with the basics of CloudWatch and discuss how to integrate CloudWatch
    into endpoints running on **SageMaker** and **Elastic Kubernetes Service** (**EKS**)
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to DL endpoint monitoring in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring using CloudWatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring a SageMaker endpoint using CloudWatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring an EKS endpoint using CloudWatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the supplemental material for this chapter from this book’s
    GitHub repository: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_12](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_12)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to DL endpoint monitoring in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start our chapter by describing the benefits of DL model monitoring
    for a deployed endpoint. Ideally, we should analyze information related to incoming
    data, outgoing data, model metrics, and traffic. A system that monitors the listed
    data can provide us with the following benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, *the input and output information for the model can be persisted in
    a data storage solution (for example, a Simple Storage Service (S3) bucket) for
    understanding data distributions*. Detailed analysis of the incoming data and
    predictions can help in identifying potential improvements for the downstream
    process. For example, monitoring the incoming data can help us in identifying
    bias in model predictions. Models can be biased toward specific feature groups
    while handling incoming requests. This information can guide us on what we should
    be considering when we are training a new model for the following deployment.
    Another benefit comes from the model’s explainability. The reasoning behind a
    model predictions needs to be explained for business purposes or legal purposes.
    This involves the techniques we have described in [*Chapter 9*](B18522_09.xhtml#_idTextAnchor187),
    *Scaling a Deep Learning Pipeline*.
  prefs: []
  type: TYPE_NORMAL
- en: Another key metric we should be tracking is the **throughput** of the endpoint,
    which can help us improve user satisfaction. *A model’s behavior may change depending
    on the volume of incoming requests and the computational power of the underlying
    machines*. We can monitor inference latency with respect to incoming traffic to
    build stable and efficient inference endpoints for users.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, monitoring for DL models can be categorized into two areas:
    **endpoint monitoring** and **model monitoring**. In the former area, we aim to
    collect data related to endpoint latency and throughput of the target endpoint.
    The latter area is focused on improving model performance; we need to collect
    incoming data, predictions, and model performances, as well as inference latency.
    While many use cases of model monitoring are achieved in an online fashion on
    a running endpoint, it is also applied during the training and validation process
    in an offline fashion with the goal of understanding the model''s behavior prior
    to deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will introduce popular tools for monitoring DL
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring tools for monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tools for monitoring can be mostly categorized into two groups, depending on
    what they are designed for: **monitoring tools** and **alerting tools**. Covering
    all tools explicitly is out of the scope of this book; however, we will introduce
    a few of them briefly to explain the benefits that monitoring and alerting tools
    aim to provide. Please note that the boundary is often unclear, and some tools
    may be built to support both features.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into monitoring tools first.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prometheus** is an open-source monitoring and alerting tool ([https://prometheus.io](https://prometheus.io/)).
    Prometheus stores data delivered from the application in local storage. It uses
    a time-series database for storing, aggregating, and retrieving metrics, which
    aligns well with the nature of monitoring tasks. Interacting with Prometheus involves
    using **Prometheus Query Language** (**PromQL**) ([https://prometheus.io/docs/prometheus/latest/querying/basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)).
    Prometheus is designed to process metrics such as **central processing unit**
    (**CPU**) usage, memory usage, and latency. Additionally, custom metrics such
    as model performance or distribution of incoming and outgoing data can be ingested
    for monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: CloudWatch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CloudWatch** is a monitoring and observability service designed by **Amazon
    Web Services** (**AWS**) ([https://aws.amazon.com/cloudwatch](https://aws.amazon.com/cloudwatch/)).
    CloudWatch is easy to set up compared to setting up a dedicated Prometheus service,
    as it handles data storage management behind the scenes. By default, most AWS
    services such as AWS Lambda and EKS clusters use CloudWatch to persist metrics
    for further analysis. Also, CloudWatch can support alerting users through emails
    or Slack messages for unusual changes from the monitored metric. For example,
    you can set a threshold for a metric and get notified if it goes above or below
    the predefined threshold. Details of the alerting feature can be found at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Grafana** is a popular tool designed for visualizing metrics collected from
    monitoring tools ([https://grafana.com](https://grafana.com/)). Metrics data from
    CloudWatch or AWS-managed Prometheus can be read by Grafana for visualization.
    For a complete description of these configurations, we recommend you to read [https://grafana.com/docs/grafana/latest/datasources/aws-cloudwatch](https://grafana.com/docs/grafana/latest/datasources/aws-cloudwatch/)
    and [https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-onboard-query-standalone-grafana.html](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-onboard-query-standalone-grafana.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the popular proprietary solutions is **Datadog** ([https://www.datadoghq.com](https://www.datadoghq.com/)).
    This tool provides a wide variety of monitoring features: log monitoring, application
    performance monitoring, network traffic monitoring, and real-time user monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Clarify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker has a built-in support for monitoring endpoints created from SageMaker,
    **SageMaker Clarify** ([https://aws.amazon.com/sagemaker/clarify](https://aws.amazon.com/sagemaker/clarify/)).
    SageMaker Clarify comes with a **software development kit** (**SDK**) which helps
    understand the performance of the model and its bias in predictions. Details of
    SageMaker Clarify can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will introduce alerting tools.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring tools for alerting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *incident* is an event that requires a follow-up action, such as a failed
    job or a build. While monitoring tools can capture unusual changes, they often
    lack incident management and automation for the responding process. Alerting tools
    close this gap by providing many of these features out of the box. Therefore,
    many companies often integrate explicit alerting tools to respond to incidents
    in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will introduce the two most popular alerting tools: PagerDuty
    and Dynatrace.'
  prefs: []
  type: TYPE_NORMAL
- en: PagerDuty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a tool for alerting and managing the **incident response** (**IR**) process,
    many compa[nies integrate **PagerDuty**](https://www.pagerduty.com/) ([https://www.pagerduty.com](https://www.pagerduty.com/)).
    On top of the basic alerting feature, PagerDuty supports assigning priorities
    to incidents based on their type and severity. PagerDuty can read data from several
    popular monitoring software such [as Prometheus and Datadog (https://aws.amazon.com/blogs/mt/using-amazon-managed-service-for-prometheus-alert-manager-to-re](https://aws.amazon.com/blogs/mt/using-amazon-managed-service-for-prometheus-alert-manager-to-receive-alerts-with-pagerduty/)ceive-alerts-with-pagerduty).
    It can also be integrated with CloudWatch with minimal code changes ([https://support.pagerduty.com/docs/aws-cloudwatch-integration-guide](https://support.pagerduty.com/docs/aws-cloudwatch-integration-guide)).
  prefs: []
  type: TYPE_NORMAL
- en: Dynatrace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Dynatrace** is another proprietary tool for monitoring entire clusters or
    ne[tworks and alerting incide](https://www.dynatrace.com/)nts ([https://www.dynatrace.com](https://www.dynatrace.com/)).
    Information related to resource usage, traffic, and response time of running processes
    can be easily monitored. Dynatrace has a unique alerting system based on alerting
    profiles. These profiles define how the system delivers notifications across the
    organization. Dynatrace has built-in push notifications, but it can be integrated
    with other systems that provide a notification feature, such as Slack and PagerDuty.'
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Monitoring information related to incoming data, outgoing data, model metrics,
    and traffic volumes for an endpoint allows us to understand the behavior of our
    endpoint and helps us in identifying potential improvements.
  prefs: []
  type: TYPE_NORMAL
- en: b. Prometheus is an open sourced monitoring and alerting system that can be
    used for monitoring metrics of a DL endpoint. CloudWatch is a monitoring service
    from AWS designed for logging a set of data and tracking unusual changes from
    incoming and outgoing traffic.
  prefs: []
  type: TYPE_NORMAL
- en: c. PagerDuty is a popular alerting tool that handles the complete life cycle
    of an incident.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at why we need monitoring for a DL endpoint and provided
    a list of tools available. In the remaining sections of this chapter, we will
    look in detail at CloudWatch, the most common monitoring tool, as it is integrated
    well into most services within AWS (for example, SageMaker).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring using CloudWatch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will introduce a few key concepts in CloudWatch: logs, metrics, alarms,
    and dashboards. **CloudWatch** persists ingested data in the form of logs or metrics
    organized by timestamps. As the name suggests, *logs* refer to text data emitted
    throughout the lifetime of a program. On the other hand, *metrics* represent organized
    numeric data such as CPU or memory utilization. Since metrics are stored in an
    organized matter, CloudWatch supports aggregating metrics and creating histograms
    from collected data. An *alarm* can be set up to alert if unusual changes are
    reported for the target metric. Also, a *dashboard* can be set up to get an intuitive
    view of selected metrics and raised alarms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will describe how to log metric data using a CloudWatch
    service client from the `boto3` library. The metric data is structured as a dictionary
    and consists of metric names, dimensions, and values. The idea of dimensions is
    to capture factual information about the metric. For example, a metric name city
    can have a value of New York City. Then, dimensions can capture specific information
    such as hourly counts of fire accidents or burglaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we first create a `cloudwatch` service client
    for CloudWatch using the `boto3.client` function. This instance will allow us
    to communicate with CloudWatch from a Python environment. The key method for logging
    a set of data is `put_metric_data`. This function `put_metric_data` method from
    the CloudWatch client instance takes in `MetricData` (the target metric data to
    ingest into CloudWatch: `data_metrics`) and `Namespace` (container for the metric
    data: ''`ECOMMERCE/Revenue`''). Data from different namespaces is managed separately
    to support efficient aggregation.'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the `data_metrics` metric data contains a field `MetricName`
    of `gross_merchandise_value` with the value of `900000.0`. The unit for `gross_merchandise_value`
    is defined as `None`. Additionally, we are providing the number of goods sold
    (`num_goods_sold`) as additional dimension information.
  prefs: []
  type: TYPE_NORMAL
- en: For a complete description [of CloudWatch concepts, please refer to https://docs.aws.amazon.com/AmazonCloudWatch/la](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html)test/monitoring/cloudwatch_concepts.html.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. CloudWatch persists ingested data in the form of logs or metrics organized
    by timestamps. It supports setting up an alarm for unusual changes and provides
    effective visualization through dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: b. Logging a metric to CloudWatch can be easily achieved using the `boto3` library.
    It provides a service client for CloudWatch that supports logging through the
    `put_metric_data` function.
  prefs: []
  type: TYPE_NORMAL
- en: While logging for CloudWatch can be done explicitly as described in this section,
    SageMaker provides built-in logging features for some of the out-of-the-box metrics.
    Let’s take a closer look at them.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring a SageMaker endpoint using CloudWatch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Being an end-to-end service for **machine learning**, SageMaker is one of the
    main tools that we use to implement various steps of a DL project. In this section,
    we will describe the last missing piece: monitoring an endpoint created with SageMaker.
    First, we will explain how you can set up CloudWatch-based monitoring for training
    where metrics are reported in batches offline. Next, we will discuss how to monitor
    a live endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippets in this section are designed to run on SageMaker Studio.
    Therefore, we first need to define an AWS **Identity and Access Management** (**IAM**)
    role and a session object. Let’s have a look at the first code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, the `get_execution_role` function provides the
    IAM role for the notebook. `role_exec`. `sagemaker.session` provides a SageMaker
    `sag_sess` SageMaker session object required for the job configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring a model throughout the training process in SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logging during model training involves SageMaker’s `Estimator` class. It
    can process printed messages using `regex` expressions and store them as metrics.
    You can see an example here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we create `estimator`, which is an `Estimator`
    instance for training. Explanations for most of the parameters can be found in
    [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient Model Training*. The
    additional parameter we are defining in this example is `metric_definitions`.
    We are passing in `reg_pattern_metrics`, which defines a set of `Train_error=(.*?)`
    and `Valid_error=(.*?)`, training and evaluation logs. Texts that match the given
    patterns will be persisted as metrics in CloudWatch. For the complete details
    of offline metrics recording throughout model training using the `Estimator` class,
    please refer to [https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).
    We want to mention that specific training job metrics (such as memory, CPU, **graphics
    processing unit** (**GPU**), and disk utilization) are automatically logged, and
    you can monitor them either through CloudWatch or SageMaker console.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring a live inference endpoint from SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will describe SageMaker’s CloudWatch-based monitoring feature
    for an endpoint. In the following code snippet, we are presenting a sample `inference.py`
    script with an `output_handler` function. This file is assigned for an `entry_point`
    parameter of SageMaker’s `Model` or `Estimator` class to define additional pre-
    and postprocessing logic. Details of `inference.py` can be found in [*Chapter
    9*](B18522_09.xhtml#_idTextAnchor187), *Scaling a Deep Learning Pipeline*. The
    `output_handler` function is designed to process model predictions and log metric
    data using the `print` function. The printed messages get stored as logs in CloudWatch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding inference code, we first get a model prediction (`results`)
    and construct a dictionary for metric data (`data_metrics`). The dictionary already
    has a `MetricName` value of `model_name` and a dimension named `classify`. The
    model prediction will be specified for the `classify` dimension. SageMaker will
    collect printed metric data and ingest it to CloudWatch. A sample approach to
    continuous model m[onitoring for quality drift is described online at https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/model_quality/model_quality_churn_sdk.html)_monitor/model_quality/model_quality_churn_sdk.html.
    This page nicely explains how you can leverage CloudWatch in such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. The `Estimator` class from SageMaker provides built-in support for CloudWatch-based
    monitoring during training. You need to pass a set of regex patterns to the `metric_definitions`
    parameter when constructing an instance.
  prefs: []
  type: TYPE_NORMAL
- en: b. Printed messages from a SageMaker endpoint get stored as CloudWatch logs.
    Therefore, we can achieve monitoring by logging metric data through an `entry_point`
    script.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explained how SageMaker supports CloudWatch-based monitoring.
    Let’s look at how EKS supports monitoring for inference endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring an EKS endpoint using CloudWatch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with SageMaker, we have described EKS-based endpoints in [*Chapter 9*](B18522_09.xhtml#_idTextAnchor187),
    *Scaling a Deep Learning Pipeline*. In this section, we describe CloudWatch-based
    monitoring available for EKS. First, we will learn how EKS metrics from the container
    can be logged for monitoring. Next, we will explain how to log model-related metrics
    from an EKS inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first look at how to set up CloudWatch for monitoring an EKS cluster.
    The simplest approach is to install a CloudWatch agent in the container. Additionally,
    you can install **Fluent Bit**, an open [source tool tha](http://www.fluentbit.io)t
    further enhances the logging process ([www.fluentbit.io](http://www.fluentbit.io)).
    For a complete explanation of CloudWatch agents and Fluent Bit, please read [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html).
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to persist the default metrics sent by the EKS control plane.
    This can be easily enabled from the EKS web console ([https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html](https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html)).
    The complete list of metrics emitted from the EKS control plane can be found at
    [https://aws.github.io/aws-eks-best-practices/reliability/docs/controlplane](https://aws.github.io/aws-eks-best-practices/reliability/docs/controlplane/).
    For example, if you are interested in logging latency-related metrics, you can
    use `apiserver_request_duration_seconds*`.
  prefs: []
  type: TYPE_NORMAL
- en: To log model-related metrics during model inference, you need to instantiate
    `boto3`’s CloudWatch service client within the code and log them explicitly. The
    code snippet included in the previous section, *Monitoring a SageMaker endpoint
    using CloudWatch*, should be a good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Logging endpoint-related metrics from an EKS cluster can be achieved by using
    a CloudWatch agent or persisting default metrics sent by the EKS control plane.
  prefs: []
  type: TYPE_NORMAL
- en: b. Model-related metrics need to be logged explicitly using the `boto3` library.
  prefs: []
  type: TYPE_NORMAL
- en: As the last topic of this section, we explained how to log various metrics to
    CloudWatch from an EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal in this chapter was to explain why you need to monitor an endpoint
    running a DL model and to introduce popular tools in this domain. The tools we
    introduced in this chapter are designed for monitoring a set of information from
    an endpoint and alerting an incident when there are sudden changes to the monitored
    metrics. The tools that we covered are CloudWatch, Prometheus, Grafana, Datadog,
    SageMaker Clarify, PagerDuty, and Dynatrace. For completeness, we looked at how
    CloudWatch can be integrated into SageMaker and EKS for monitoring an endpoint
    as well as model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, as the last chapter of this book, we will explore the process
    of evaluating a completed project and discussing potential improvements.
  prefs: []
  type: TYPE_NORMAL
