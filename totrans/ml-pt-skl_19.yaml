- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning for Decision Making in Complex Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we focused on supervised and unsupervised machine
    learning. We also learned how to leverage artificial neural networks and deep
    learning to tackle problems encountered with these types of machine learning.
    As you’ll recall, supervised learning focuses on predicting a category label or
    continuous value from a given input feature vector. Unsupervised learning focuses
    on extracting patterns from data, making it useful for data compression (*Chapter
    5*, *Compressing Data via Dimensionality Reduction*), clustering (*Chapter 10*,
    *Working with Unlabeled Data – Clustering Analysis*), or approximating the training
    set distribution for generating new data (*Chapter 17*, *Generative Adversarial
    Networks for Synthesizing New Data*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we turn our attention to a separate category of machine learning,
    **reinforcement learning** (**RL**), which is different from the previous categories
    as it is focused on learning *a series of actions* for optimizing an overall reward—for
    example, winning at a game of chess. In summary, this chapter will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the basics of RL, getting familiar with agent/environment interactions,
    and understanding how the reward process works, in order to help make decisions
    in complex environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing different categories of RL problems, model-based and model-free
    learning tasks, Monte Carlo, and temporal difference learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a Q-learning algorithm in a tabular format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding function approximation for solving RL problems, and combining
    RL with deep learning by implementing a *deep* Q-learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL is a complex and vast area of research, and this chapter focuses on the fundamentals.
    As this chapter serves as an introduction, and to keep our attention on the important
    methods and algorithms, we will work mainly with basic examples that illustrate
    the main concepts. However, toward the end of this chapter, we will go over a
    more challenging example and utilize deep learning architectures for a particular
    RL approach known as deep Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction – learning from experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will first introduce the concept of RL as a branch of machine
    learning and see its major differences compared with other tasks of machine learning.
    After that, we will cover the fundamental components of an RL system. Then, we
    will see the RL mathematical formulation based on the Markov decision process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until this point, this book has primarily focused on *supervised* and *unsupervised*
    learning. Recall that in *supervised* learning, we rely on labeled training examples,
    which are provided by a supervisor or a human expert, and the goal is to train
    a model that can generalize well to unseen, unlabeled test examples. This means
    that the supervised learning model should learn to assign the same labels or values
    to a given input example as the supervisor human expert. On the other hand, in
    *unsupervised* learning, the goal is to learn or capture the underlying structure
    of a dataset, such as in clustering and dimensionality reduction methods; or learning
    how to generate new, synthetic training examples with a similar underlying distribution.
    RL is substantially different from supervised and unsupervised learning, and so
    RL is often regarded as the “third category of machine learning.”
  prefs: []
  type: TYPE_NORMAL
- en: The key element that distinguishes RL from other subtasks of machine learning,
    such as supervised and unsupervised learning, is that RL is centered around the
    concept of *learning by interaction*. This means that in RL, the model learns
    from interactions with an environment to maximize a *reward function*.
  prefs: []
  type: TYPE_NORMAL
- en: While maximizing a reward function is related to the concept of minimizing the
    loss function in supervised learning, the *correct* labels for learning a series
    of actions are not known or defined upfront in RL—instead, they need to be learned
    through interactions with the environment to achieve a certain desired outcome—such
    as winning at a game. With RL, the model (also called an **agent**) interacts
    with its environment, and by doing so generates a sequence of interactions that
    are together called an **episode**. Through these interactions, the agent collects
    a series of rewards determined by the environment. These rewards can be positive
    or negative, and sometimes they are not disclosed to the agent until the end of
    an episode.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that we want to teach a computer to play the game of chess
    and win against human players. The labels (rewards) for each individual chess
    move made by the computer are not known until the end of the game, because during
    the game itself, we don’t know whether a particular move will result in winning
    or losing that game. Only right at the end of the game is the feedback determined.
    That feedback would likely be a positive reward given if the computer won the
    game because the agent had achieved the overall desired outcome; and vice versa,
    a negative reward would likely be given if the computer had lost the game.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, considering the example of playing chess, the input is the current
    configuration, for instance, the arrangement of the individual chess pieces on
    the board. Given the large number of possible inputs (the states of the system),
    it is impossible to label each configuration or state as positive or negative.
    Therefore, to define a learning process, we provide rewards (or penalties) at
    the end of each game, when we know whether we reached the desired outcome—whether
    we won the game or not.
  prefs: []
  type: TYPE_NORMAL
- en: This is the essence of RL. In RL, we cannot or do not teach an agent, computer,
    or robot *how* to do things; we can only specify *what* we want the agent to achieve.
    Then, based on the outcome of a particular trial, we can determine rewards depending
    on the agent’s success or failure. This makes RL very attractive for decision
    making in complex environments, especially when the problem-solving task requires
    a series of steps, which are unknown, or hard to explain, or hard to define.
  prefs: []
  type: TYPE_NORMAL
- en: Besides applications in games and robotics, examples of RL can also be found
    in nature. For example, training a dog involves RL—we hand out rewards (treats)
    to the dog when it performs certain desirable actions. Or consider a medical dog
    that is trained to warn its partner of an oncoming seizure. In this case, we do
    not know the exact mechanism by which the dog is able to detect an oncoming seizure,
    and we certainly wouldn’t be able to define a series of steps to learn seizure
    detection, even if we had precise knowledge of this mechanism. However, we can
    reward the dog with a treat if it successfully detects a seizure to *reinforce*
    this behavior!
  prefs: []
  type: TYPE_NORMAL
- en: While RL provides a powerful framework for learning an arbitrary series of actions
    to achieve a certain goal, please do keep in mind that RL is still a relatively
    young and active area of research with many unresolved challenges. One aspect
    that makes training RL models particularly challenging is that the consequent
    model inputs depend on actions taken previously. This can lead to all sorts of
    problems, and usually results in unstable learning behavior. Also, this sequence-dependence
    in RL creates a so-called *delayed effect*, which means that the action taken
    at a time step *t* may result in a future reward appearing some arbitrary number
    of steps later.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the agent-environment interface of a reinforcement learning system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In all examples of RL, we can find two distinct entities: an agent and an environment.
    Formally, an **agent** is defined as an entity that learns how to make decisions
    and interacts with its surrounding environment by taking an action. In return,
    as a consequence of taking an action, the agent receives observations and a reward
    signal as governed by the environment. The **environment** is anything that falls
    outside the agent. The environment communicates with the agent and determines
    the reward signal for the agent’s action as well as its observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **reward signal** is the feedback that the agent receives from interacting
    with the environment, which is usually provided in the form of a scalar value
    and can be either positive or negative. The purpose of the reward is to tell the
    agent how well it has performed. The frequency at which the agent receives the
    reward depends on the given task or problem. For example, in the game of chess,
    the reward would be determined after a full game based on the outcome of all the
    moves: a win or a loss. On the other hand, we could define a maze such that the
    reward is determined after each time step. In such a maze, the agent then tries
    to maximize its accumulated rewards over its lifetime—where lifetime describes
    the duration of an episode.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 19.1* illustrates the interactions and communication between the agent
    and the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.1: The interaction between the agent and its environment'
  prefs: []
  type: TYPE_NORMAL
- en: The state of the agent, as illustrated in *Figure 19.1*, is the set of all of
    its variables (1). For example, in the case of a robot drone, these variables
    could include the drone’s current position (longitude, latitude, and altitude),
    the drone’s remaining battery life, the speed of each fan, and so forth. At each
    time step, the agent interacts with the environment through a set of available
    actions **A**[t] (2). Based on the action taken by the agent denoted by **A**[t],
    while it is at state **S**[t], the agent will receive a reward signal **R**[t][+1]
    (3), and its state will become **S**[t][+1] (4).
  prefs: []
  type: TYPE_NORMAL
- en: During the learning process, the agent must try different actions (**exploration**)
    so that it can progressively learn which actions to prefer and perform more often
    (**exploitation**) in order to maximize the total, cumulative reward. To understand
    this concept, let’s consider a very simple example where a new computer science
    graduate with a focus on software engineering is wondering whether to start working
    at a company (exploitation) or to pursue a master’s or Ph.D. degree to learn more
    about data science and machine learning (exploration). In general, exploitation
    will result in choosing actions with a greater short-term reward, whereas exploration
    can potentially result in greater total rewards in the long run. The tradeoff
    between exploration and exploitation has been studied extensively, and yet, there
    is no universal answer to this decision-making dilemma.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical foundations of RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into some practical examples and start training an RL model,
    which we will be doing later in this chapter, let’s first understand some of the
    theoretical foundations of RL. The following sections will begin by first examining
    the mathematical formulation of **Markov decision processes**, episodic versus
    continuing tasks, some key RL terminology, and dynamic programming using the **Bellman
    equation**. Let’s start with Markov decision processes.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, the type of problems that RL deals with are typically formulated
    as **Markov decision processes** (**MDPs**). The standard approach for solving
    MDP problems is by using dynamic programming, but RL offers some key advantages
    over dynamic programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic programming**'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming refers to a set of computer algorithms and programming methods
    that was developed by Richard Bellman in the 1950s. In a sense, dynamic programming
    is about recursive problem solving—solving relatively complicated problems by
    breaking them down into smaller subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between recursion and dynamic programming is that dynamic
    programming stores the results of subproblems (usually as a dictionary or other
    form of lookup table) so that they can be accessed in constant time (instead of
    recalculating them) if they are encountered again in future.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of some famous problems in computer science that are solved by dynamic
    programming include sequence alignment and computing the shortest path from point
    A to point B.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming is not a feasible approach, however, when the size of states
    (that is, the number of possible configurations) is relatively large. In such
    cases, RL is considered a much more efficient and practical alternative approach
    for solving MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical formulation of Markov decision processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The types of problems that require learning an interactive and sequential decision-making
    process, where the decision at time step *t* affects the subsequent situations,
    are mathematically formalized as MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the agent/environment interactions in RL, if we denote the agent’s
    starting state as *S*[0], the interactions between the agent and the environment
    result in a sequence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{*S*[0], *A*[0], *R*[1]},    {*S*[1], *A*[1], *R*[2]},    {*S*[2], *A*[2], *R*[3]},    ...'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the braces serve only as a visual aid. Here, *S*[t] and *A*[t] stand
    for the state and the action taken at time step *t*. *R*[t][+1] denotes the reward
    received from the environment after performing action *A*[t]. Note that *S*[t],
    *R*[t][+1], and *A*[t] are time-dependent random variables that take values from
    predefined finite sets denoted by ![](img/B17582_19_001.png), ![](img/B17582_19_002.png),
    and ![](img/B17582_19_003.png), respectively. In an MDP, these time-dependent
    random variables, *S*[t] and *R*[t][+1], have probability distributions that only
    depend on their values at the preceding time step, *t* – 1\. The probability distribution
    for *S*[t][+1] = *s*′ and *R*[t][+1] = *r* can be written as a conditional probability
    over the preceding state (*S*[t]) and taken action (*A*[t]) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_004.png)'
  prefs: []
  type: TYPE_IMG
- en: This probability distribution completely defines the **dynamics of the environment**
    (or model of the environment) because, based on this distribution, all transition
    probabilities of the environment can be computed. Therefore, the environment dynamics
    are a central criterion for categorizing different RL methods. The types of RL
    methods that require a model of the environment or try to learn a model of the
    environment (that is, the environment dynamics) are called *model-based* methods,
    as opposed to *model-free* methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-free and model-based RL**'
  prefs: []
  type: TYPE_NORMAL
- en: When the probability ![](img/B17582_19_005.png) is known, then the learning
    task can be solved with dynamic programming. But when the dynamics of the environment
    are not known, as is the case in many real-world problems, then we would need
    to acquire a large number of samples by interacting with the environment to compensate
    for the unknown environment dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main approaches for dealing with this problem are the model-free **Monte
    Carlo** (**MC**) and **temporal difference** (**TD**) methods. The following chart
    displays the two main categories and the branches of each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.2: The different models to use based on the environment dynamics'
  prefs: []
  type: TYPE_NORMAL
- en: We will cover these different approaches and their branches from theory to practical
    algorithms in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The environment dynamics can be considered deterministic if particular actions
    for given states are always or never taken, that is, ![](img/B17582_19_006.png).
    Otherwise, in the more general case, the environment would have stochastic behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sense of this stochastic behavior, let’s consider the probability of
    observing the future state *S*[t][+1] = *s*′ conditioned on the current state
    *S*[t] = *s* and the performed action *A*[t] = *a*. This is denoted by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be computed as a marginal probability by taking the sum over all possible
    rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_008.png)'
  prefs: []
  type: TYPE_IMG
- en: This probability is called **state-transition probability**. Based on the state-transition
    probability, if the environment dynamics are deterministic, then it means that
    when the agent takes action *A*[t] = *a* at state *S*[t] = *s*, the transition
    to the next state, *S*[t][+1] = *s*′, will be 100 percent certain, that is, ![](img/B17582_19_009.png).
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of a Markov process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Markov process can be represented as a directed cyclic graph in which the
    nodes in the graph represent the different states of the environment. The edges
    of the graph (that is, the connections between the nodes) represent the transition
    probabilities between the states.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider a student deciding between three different situations:
    (A) studying for an exam at home, (B) playing video games at home, or (C) studying
    at the library. Furthermore, there is a terminal state (T) for going to sleep.
    The decisions are made every hour, and after making a decision, the student will
    remain in a chosen situation for that particular hour. Then, assume that when
    staying at home (state A), there is a 50 percent likelihood that the student switches
    the activity to playing video games. On the other hand, when the student is at
    state B (playing video games), there is a relatively high chance (80 percent)
    that the student will continue playing video games in the subsequent hours.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamics of the student’s behavior is shown as a Markov process in *Figure
    19.3*, which includes a cyclic graph and a transition table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.3: The Markov process of the student'
  prefs: []
  type: TYPE_NORMAL
- en: The values on the edges of the graph represent the transition probabilities
    of the student’s behavior, and their values are also shown in the table to the
    right. When considering the rows in the table, please note that the transition
    probabilities coming out of each state (node) always sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Episodic versus continuing tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the agent interacts with the environment, the sequence of observations or
    states forms a trajectory. There are two types of trajectories. If an agent’s
    trajectory can be divided into subparts such that each starts at time *t* = 0
    and ends in a terminal state *S*[T] (at *t* = *T*), the task is called an *episodic
    task*.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the trajectory is infinitely continuous without a terminal
    state, the task is called a *continuing task*.
  prefs: []
  type: TYPE_NORMAL
- en: The task related to a learning agent for the game of chess is an episodic task,
    whereas a cleaning robot that is keeping a house tidy is typically performing
    a continuing task. In this chapter, we only consider episodic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In episodic tasks, an **episode** is a sequence or trajectory that an agent
    takes from a starting state, *S*[0], to a terminal state, *S*[T]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S*[0], *A*[0], *R*[1], *S*[1], *A*[1], *R*[2], ..., *S*[t], *A*[t], *R*[t][+1], ..., *S*[t][–1], *A*[t][–1], *R*[t], *S*[t]'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Markov process shown in *Figure 19.3*, which depicts the task of a
    student studying for an exam, we may encounter episodes like the following three
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RL terminology: return, policy, and value function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let’s define some additional RL-specific terminology that we will need
    for the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The return
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The so-called *return* at time *t* is the cumulated reward obtained from the
    entire duration of an episode. Recall that *R*[t][+1] = *r* is the *immediate
    reward* obtained after performing an action, *A*[t], at time *t*; the *subsequent*
    rewards are *R*[t][+2], *R*[t][+3], and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The return at time *t* can then be calculated from the immediate reward as
    well as the subsequent ones, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_011.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17582_03_064.png) is the *discount factor* in range [0, 1]. The
    parameter ![](img/B17582_03_064.png) indicates how much the future rewards are
    “worth” at the current moment (time *t*). Note that by setting ![](img/B17582_19_014.png),
    we would imply that we do not care about future rewards. In this case, the return
    will be equal to the immediate reward, ignoring the subsequent rewards after *t* + 1,
    and the agent will be short-sighted. On the other hand, if ![](img/B17582_19_015.png),
    the return will be the unweighted sum of all subsequent rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, note that the equation for the return can be expressed in a simpler
    way by using *recursion* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_016.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that the return at time *t* is equal to the immediate reward *r*
    plus the discounted future return at time *t* + 1\. This is a very important property,
    which facilitates the computations of the return.
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition behind the discount factor**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an understanding of the discount factor, consider *Figure 19.4*, showing
    the value of earning a $100 bill today compared to earning it in a year from now.
    Under certain economic situations, like inflation, earning this $100 bill right
    now could be worth more than earning it in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.4: An example of a discount factor based on the value of a $100 bill
    over time'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we say that if this bill is worth $100 right now, then it would be
    worth $90 in a year with a discount factor ![](img/B17582_19_017.png).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compute the return at different time steps for the episodes in our previous
    student example. Assume ![](img/B17582_19_018.png) and that the only reward given
    is based on the result of the exam (+1 for passing the exam, and –1 for failing
    it). The rewards for intermediate time steps are 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_019.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_020.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_021.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '![](img/B17582_19_022.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_023.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_19_024.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_025.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_026.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_027.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_028.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17582_19_029.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B17582_19_030.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: We leave the computation of the returns for the third episode as an exercise
    for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *policy* typically denoted by ![](img/B17582_19_031.png) is a function that
    determines the next action to take, which can be either deterministic or stochastic
    (that is, the probability for taking the next action). A stochastic policy then
    has a probability distribution over actions that an agent can take at a given
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_032.png)'
  prefs: []
  type: TYPE_IMG
- en: During the learning process, the policy may change as the agent gains more experience.
    For example, the agent may start from a random policy, where the probability of
    all actions is uniform; meanwhile, the agent will hopefully learn to optimize
    its policy toward reaching the optimal policy. The *optimal policy* ![](img/B17582_19_033.png)
    is the policy that yields the highest return.
  prefs: []
  type: TYPE_NORMAL
- en: Value function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *value function*, also referred to as the *state-value function*, measures
    the *goodness* of each state—in other words, how good or bad it is to be in a
    particular state. Note that the criterion for goodness is based on the return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, based on the return *G*[t], we define the value function of state *s*
    as the expected return (the average return over all possible episodes) after *following
    policy* ![](img/B17582_19_034.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_035.png)'
  prefs: []
  type: TYPE_IMG
- en: In an actual implementation, we usually estimate the value function using lookup
    tables, so we do not have to recompute it multiple times. (This is the dynamic
    programming aspect.) For example, in practice, when we estimate the value function
    using such tabular methods, we store all the state values in a table denoted by
    *V*(*s*). In a Python implementation, this could be a list or a NumPy array whose
    indices refer to different states; or, it could be a Python dictionary, where
    the dictionary keys map the states to the respective values.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we can also define a value for each state-action pair, which is called
    the *action-value function* and is denoted by ![](img/B17582_19_036.png). The
    action-value function refers to the expected return *G*[t] when the agent is at
    state *S*[t] = *s* and takes action *A*[t] = *a*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extending the definition of the state-value function to state-action pairs,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_037.png)'
  prefs: []
  type: TYPE_IMG
- en: This is similar to referring to the optimal policy as ![](img/B17582_19_038.png),
    ![](img/B17582_19_039.png), and ![](img/B17582_19_040.png) also denote the optimal
    state-value and action-value functions.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the value function is an essential component of RL methods. We will
    cover different ways of calculating and estimating the state-value function and
    action-value function later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**The difference between the reward, return, and value function**'
  prefs: []
  type: TYPE_NORMAL
- en: The *reward* is a consequence of the agent taking an action given the current
    state of the environment. In other words, the reward is a signal that the agent
    receives when performing an action to transition from one state to the next. However,
    remember that not every action yields a positive or negative reward—think back
    to our chess example, where a positive reward is only received upon winning the
    game, and the reward for all intermediate actions is zero.
  prefs: []
  type: TYPE_NORMAL
- en: A state itself has a certain value, which we assign to it, to measure how good
    or bad this state is—this is where the *value function* comes into play. Typically,
    the states with a “high” or “good” value are those states that have a high expected
    *return* and will likely yield a high reward given a particular policy.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s consider a chess-playing computer once more. A positive reward
    may only be given at the end of the game if the computer wins the game. There
    is no (positive) reward if the computer loses the game. Now, imagine the computer
    performs a particular chess move that captures the opponent’s queen without any
    negative consequences for the computer. Since the computer only receives a reward
    for winning the game, it does not get an immediate reward by making this move
    that captures the opponent’s queen. However, the new state (the state of the board
    after capturing the queen) may have a high value, which may yield a reward (if
    the game is won afterward). Intuitively, we can say that the high value associated
    with capturing the opponent’s queen is associated with the fact that capturing
    the queen often results in winning the game—and thus the high expected return,
    or value. However, note that capturing the opponent’s queen does not always lead
    to winning the game; hence, the agent is likely to receive a positive reward,
    but it is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the return is the weighted sum of rewards for an entire episode, which
    would be equal to the discounted final reward in our chess example (since there
    is only one reward). The value function is the expectation over all possible episodes,
    which basically computes how “valuable” it is, on average, to make a certain move.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move directly ahead into some RL algorithms, let’s briefly go over
    the derivation for the Bellman equation, which we can use to implement the policy
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming using the Bellman equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Bellman equation is one of the central elements of many RL algorithms. The
    Bellman equation simplifies the computation of the value function, such that rather
    than summing over multiple time steps, it uses a recursion that is similar to
    the recursion for computing the return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the recursive equation for the total return ![](img/B17582_19_041.png),
    we can rewrite the value function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_042.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the immediate reward *r* is taken out of the expectation since it
    is a constant and known quantity at time *t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the action-value function, we could write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the environment dynamics to compute the expectation by summing all
    the probabilities of the next state *s*′ and the corresponding rewards *r*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can see that expectation of the return, ![](img/B17582_19_045.png),
    is essentially the state-value function ![](img/B17582_19_046.png). So, we can
    write ![](img/B17582_19_047.png) as a function of ![](img/B17582_19_046.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_049.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called the **Bellman equation**, which relates the value function for
    a state, *s*, to the value function of its subsequent state, *s*′. This greatly
    simplifies the computation of the value function because it eliminates the iterative
    loop along the time axis.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover a series of learning algorithms. We will start
    with dynamic programming, which assumes that the transition dynamics—or the environment
    dynamics, that is, ![](img/B17582_19_050.png)—are known. However, in most RL problems,
    this is not the case. To work around the unknown environment dynamics, RL techniques
    were developed that learn through interacting with the environment. These techniques
    include **Monte Carlo** (**MC**), **temporal difference** (**TD**) learning, and
    the increasingly popular Q-learning and deep Q-learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 19.5* describes the course of advancing RL algorithms, from dynamic
    programming to Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.5: Different types of RL algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections of this chapter, we will step through each of these
    RL algorithms. We will start with dynamic programming, before moving on to MC,
    and finally on to TD and its branches of on-policy **SARSA** (**state–action–reward–state–action**)
    and off-policy Q-learning. We will also move into deep Q-learning while we build
    some practical models.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will focus on solving RL problems under the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: We have full knowledge of the environment dynamics; that is, all transition
    probabilities ![](img/B17582_19_051.png)—are known.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent’s state has the Markov property, which means that the next action
    and reward depend only on the current state and the choice of action we make at
    this moment or current time step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mathematical formulation for RL problems using a **Markov decision process**
    (**MDP**) was introduced earlier in this chapter. If you need a refresher, please
    refer to the section entitled *The mathematical formulation of Markov decision
    processes*, which introduced the formal definition of the value function ![](img/B17582_19_052.png)
    following the policy ![](img/B17582_19_053.png), and the Bellman equation, which
    was derived using the environment dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: We should emphasize that dynamic programming is not a practical approach for
    solving RL problems. The problem with using dynamic programming is that it assumes
    full knowledge of the environment dynamics, which is usually unreasonable or impractical
    for most real-world applications. However, from an educational standpoint, dynamic
    programming helps with introducing RL in a simple fashion and motivates the use
    of more advanced and complicated RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main objectives via the tasks described in the following subsections:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the true state-value function, ![](img/B17582_19_054.png); this task
    is also known as the prediction task and is accomplished with *policy evaluation*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the optimal value function, ![](img/B17582_19_055.png), which is accomplished
    via *generalized policy iteration*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policy evaluation – predicting the value function with dynamic programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the Bellman equation, we can compute the value function for an arbitrary
    policy ![](img/B17582_19_034.png) with dynamic programming when the environment
    dynamics are known. For computing this value function, we can adapt an iterative
    solution, where we start from ![](img/B17582_19_057.png), which is initialized
    to zero values for each state. Then, at each iteration *i* + 1, we update the
    values for each state based on the Bellman equation, which, in turn, is based
    on the values of states from a previous iteration, *i*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_058.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be shown that as the iterations increase to infinity, ![](img/B17582_19_059.png)
    converges to the true state-value function, ![](img/B17582_19_060.png).
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice here that we do not need to interact with the environment. The
    reason for this is that we already know the environment dynamics accurately. As
    a result, we can leverage this information and estimate the value function easily.
  prefs: []
  type: TYPE_NORMAL
- en: After computing the value function, an obvious question is how that value function
    can be useful for us if our policy is still a random policy. The answer is that
    we can actually use this computed ![](img/B17582_19_061.png) to improve our policy,
    as we will see next.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the policy using the estimated value function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have computed the value function ![](img/B17582_19_062.png) by
    following the existing policy, ![](img/B17582_19_063.png), we want to use ![](img/B17582_19_052.png)
    and improve the existing policy, ![](img/B17582_19_065.png). This means that we
    want to find a new policy, ![](img/B17582_19_066.png), that, for each state, *s*,
    following ![](img/B17582_19_066.png), would yield higher or at least equal value
    than using the current policy, ![](img/B17582_19_063.png). In mathematical terms,
    we can express this objective for the improved policy, ![](img/B17582_19_066.png),
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_070.png)'
  prefs: []
  type: TYPE_IMG
- en: First, recall that a policy, ![](img/B17582_19_063.png), determines the probability
    of choosing each action, *a*, while the agent is at state *s*. Now, in order to
    find ![](img/B17582_19_072.png) that always has a better or equal value for each
    state, we first compute the action-value function, ![](img/B17582_19_073.png),
    for each state, *s*, and action, *a*, based on the computed state value using
    the value function ![](img/B17582_19_074.png). We iterate through all the states,
    and for each state, *s*, we compare the value of the next state, *s*′, that would
    occur if action *a* was selected.
  prefs: []
  type: TYPE_NORMAL
- en: After we have obtained the highest state value by evaluating all state-action
    pairs via ![](img/B17582_19_075.png), we can compare the corresponding action
    with the action selected by the current policy. If the action suggested by the
    current policy (that is, ![](img/B17582_19_076.png)) is different than the action
    suggested by the action-value function (that is, ![](img/B17582_19_077.png)),
    then we can update the policy by reassigning the probabilities of actions to match
    the action that gives the highest action value, ![](img/B17582_19_078.png). This
    is called the *policy improvement* algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the policy improvement algorithm described in the previous subsection,
    it can be shown that the policy improvement will strictly yield a better policy,
    unless the current policy is already optimal (which means ![](img/B17582_19_079.png)
    for each ![](img/B17582_19_080.png)). Therefore, if we iteratively perform policy
    evaluation followed by policy improvement, we are guaranteed to find the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this technique is referred to as **generalized policy iteration**
    (**GPI**), which is common among many RL methods. We will use the GPI in later
    sections of this chapter for the MC and TD learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We saw that by repeating the policy evaluation (compute ![](img/B17582_19_074.png)
    and ![](img/B17582_19_082.png)) and policy improvement (finding ![](img/B17582_19_083.png)
    such that ![](img/B17582_19_084.png)), we can reach the optimal policy. However,
    it can be more efficient if we combine the two tasks of policy evaluation and
    policy improvement into a single step. The following equation updates the value
    function for iteration *i* + 1 (denoted by ![](img/B17582_19_085.png)) based on
    the action that maximizes the weighted sum of the next state value and its immediate
    reward (![](img/B17582_19_086.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_087.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the updated value for ![](img/B17582_19_088.png) is maximized
    by choosing the best action out of all possible actions, whereas in policy evaluation,
    the updated value was using the weighted sum over all actions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notation for tabular estimates of the state-value and action-value functions**'
  prefs: []
  type: TYPE_NORMAL
- en: In most RL literature and textbooks, the lowercase ![](img/B17582_19_089.png)
    and ![](img/B17582_19_090.png) are used to refer to the true state-value and true
    action-value functions, respectively, as mathematical functions.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, for practical implementations, these value functions are defined
    as lookup tables. The tabular estimates of these value functions are denoted by
    ![](img/B17582_19_091.png) and ![](img/B17582_19_092.png). We will also use this
    notation in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning with Monte Carlo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous section, dynamic programming relies on a simplistic
    assumption that the environment’s dynamics are fully known. Moving away from the
    dynamic programming approach, we now assume that we do not have any knowledge
    about the environment dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: That is, we do not know the state-transition probabilities of the environment,
    and instead, we want the agent to learn through *interacting* with the environment.
    Using MC methods, the learning process is based on the so-called *simulated experience*.
  prefs: []
  type: TYPE_NORMAL
- en: For MC-based RL, we define an agent class that follows a probabilistic policy,
    ![](img/B17582_19_034.png), and based on this policy, our agent takes an action
    at each step. This results in a simulated episode.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we defined the state-value function, such that the value of a state
    indicates the expected return from that state. In dynamic programming, this computation
    relied on the knowledge of the environment dynamics, that is, ![](img/B17582_19_094.png).
  prefs: []
  type: TYPE_NORMAL
- en: However, from now on, we will develop algorithms that do not require the environment
    dynamics. MC-based methods solve this problem by generating simulated episodes
    where an agent interacts with the environment. From these simulated episodes,
    we will be able to compute the average return for each state visited in that simulated
    episode.
  prefs: []
  type: TYPE_NORMAL
- en: State-value function estimation using MC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After generating a set of episodes, for each state, *s*, the set of episodes
    that all pass through state *s* is considered for calculating the value of state
    *s*. Let’s assume that a lookup table is used for obtaining the value corresponding
    to the value function, ![](img/B17582_19_095.png). MC updates for estimating the
    value function are based on the total return obtained in that episode starting
    from the first time that state *s* is visited. This algorithm is called *first-visit
    Monte Carlo* value prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Action-value function estimation using MC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the environment dynamics are known, we can easily infer the action-value
    function from a state-value function by looking one step ahead to find the action
    that gives the maximum value, as was shown in the *Dynamic programming* section.
    However, this is not feasible if the environment dynamics are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this issue, we can extend the algorithm for estimating the first-visit
    MC state-value prediction. For instance, we can compute the *estimated* return
    for each state-action pair using the action-value function. To obtain this estimated
    return, we consider visits to each state-action pair (*s*, *a*), which refers
    to visiting state *s* and taking action *a*.
  prefs: []
  type: TYPE_NORMAL
- en: However, a problem arises since some actions may never be selected, resulting
    in insufficient exploration. There are a few ways to resolve this. The simplest
    approach is called *exploratory start*, which assumes that every state-action
    pair has a non-zero probability at the beginning of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach for dealing with this lack-of-exploration issue is called the
    ![](img/B17582_19_096.png)-*greedy policy*, which will be discussed in the next
    section on policy improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Finding an optimal policy using MC control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*MC control* refers to the optimization procedure for improving a policy. Similar
    to the policy iteration approach in the previous section (*Dynamic programming*),
    we can repeatedly alternate between policy evaluation and policy improvement until
    we reach the optimal policy. So, starting from a random policy, ![](img/B17582_19_097.png),
    the process of alternating between policy evaluation and policy improvement can
    be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_098.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy improvement – computing the greedy policy from the action-value function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given an action-value function, *q*(*s*, *a*), we can generate a greedy (deterministic)
    policy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_099.png)'
  prefs: []
  type: TYPE_IMG
- en: To avoid the lack-of-exploration problem, and to consider the non-visited state-action
    pairs as discussed earlier, we can let the non-optimal actions have a small chance
    (![](img/B17582_19_096.png)) to be chosen. This is called the ![](img/B17582_19_096.png)-greedy
    policy, according to which, all non-optimal actions at state *s* have a minimal
    ![](img/B17582_19_102.png) probability of being selected (instead of 0), and the
    optimal action has a probability of ![](img/B17582_19_103.png) (instead of 1).
  prefs: []
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have seen two fundamental RL techniques—dynamic programming and MC-based
    learning. Recall that dynamic programming relies on the complete and accurate
    knowledge of the environment dynamics. The MC-based method, on the other hand,
    learns by simulated experience. In this section, we will now introduce a third
    RL method called *TD learning*, which can be considered as an improvement or extension
    of the MC-based RL approach.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the MC technique, TD learning is also based on learning by experience
    and, therefore, does not require any knowledge of environment dynamics and transition
    probabilities. The main difference between the TD and MC techniques is that in
    MC, we have to wait until the end of the episode to be able to calculate the total
    return.
  prefs: []
  type: TYPE_NORMAL
- en: However, in TD learning, we can leverage some of the learned properties to update
    the estimated values before reaching the end of the episode. This is called *bootstrapping*
    (in the context of RL, the term *bootstrapping* is not to be confused with the
    bootstrap estimates we used in *Chapter 7*, *Combining Different Models for Ensemble
    Learning*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the dynamic programming approach and MC-based learning, we will
    consider two tasks: estimating the value function (which is also called value
    prediction) and improving the policy (which is also called the control task).'
  prefs: []
  type: TYPE_NORMAL
- en: TD prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s first revisit the value prediction by MC. At the end of each episode,
    we are able to estimate the return, *G*[t], for each time step *t*. Therefore,
    we can update our estimates for the visited states as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_104.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *G*[t] is used as the *target return* to update the estimated values,
    and (*G*[t] – *V*(*S*[t])) is a *correction* term added to our current estimate
    of the value *V*(*S*[t]). The value ![](img/B17582_15_030.png) is a hyperparameter
    denoting the learning rate, which is kept constant during learning.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in MC, the correction term uses the *actual* return, *G*[t], which
    is not known until the end of the episode. To clarify this, we can rename the
    actual return, *G*[t], to *G*[t][:][T], where the subscript *t*:*T* indicates
    that this is the return obtained at time step *t* while considering all the events
    that occurred from time step *t* until the final time step, *T*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TD learning, we replace the actual return, *G*[t][:][T], with a new target
    return, *G*[t][:][t][+1], which significantly simplifies the updates for the value
    function, *V*(*S*[t]). The update formula based on TD learning is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_106.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the target return, ![](img/B17582_19_107.png), is using the observed reward,
    *R*[t][+1], and the estimated value of the next immediate step. Notice the difference
    between MC and TD. In MC, *G*[t][:][T] is not available until the end of the episode,
    so we should execute as many steps as needed to get there. On the contrary, in
    TD, we only need to go one step ahead to get the target return. This is also known
    as TD(0).
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the TD(0) algorithm can be generalized to the so-called *n-step
    TD* algorithm, which incorporates more future steps—more precisely, the weighted
    sum of *n* future steps. If we define *n* = 1, then the n-step TD procedure is
    identical to TD(0), which was described in the previous paragraph. If ![](img/B17582_19_108.png),
    however, the n-step TD algorithm will be the same as the MC algorithm. The update
    rule for n-step TD is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_109.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And *G*[t][:][t][+][n] is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_110.png)'
  prefs: []
  type: TYPE_IMG
- en: '**MC versus TD: which method converges faster?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the precise answer to this question is still unknown, in practice, it
    is empirically shown that TD can converge faster than MC. If you are interested,
    you can find more details on the convergences of MC and TD in the book entitled
    *Reinforcement Learning: An Introduction*, by *Richard S. Sutton* and *Andrew
    G. Barto*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have covered the prediction task using the TD algorithm, we can
    move on to the control task. We will cover two algorithms for TD control: an *on-policy*
    control and an *off-policy* control. In both cases, we use the GPI that was used
    in both the dynamic programming and MC algorithms. In on-policy TD control, the
    value function is updated based on the actions from the same policy that the agent
    is following; while in an off-policy algorithm, the value function is updated
    based on actions outside the current policy.'
  prefs: []
  type: TYPE_NORMAL
- en: On-policy TD control (SARSA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For simplicity, we only consider the one-step TD algorithm, or TD(0). However,
    the on-policy TD control algorithm can be readily generalized to *n*-step TD.
    We will start by extending the prediction formula for defining the state-value
    function to describe the action-value function. To do this, we use a lookup table,
    that is, a tabular 2D array, *Q*(*S*[t], *A*[t]), which represents the action-value
    function for each state-action pair. In this case, we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_111.png)'
  prefs: []
  type: TYPE_IMG
- en: This algorithm is often called SARSA, referring to the quintuple (*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1])
    that is used in the update formula.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous sections describing the dynamic programming and MC
    algorithms, we can use the GPI framework, and starting from the random policy,
    we can repeatedly estimate the action-value function for the current policy and
    then optimize the policy using the ![](img/B17582_19_112.png)-greedy policy based
    on the current action-value function.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy TD control (Q-learning)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw when using the previous on-policy TD control algorithm that how we estimate
    the action-value function is based on the policy that is used in the simulated
    episode. After updating the action-value function, a separate step for policy
    improvement is performed by taking the action that has the higher value.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative (and better) approach is to combine these two steps. In other
    words, imagine the agent is following policy ![](img/B17582_19_063.png), generating
    an episode with the current transition quintuple (*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1]).
    Instead of updating the action-value function using the action value of *A*[t][+1]
    that is taken by the agent, we can find the best action even if it is not actually
    chosen by the agent following the current policy. (That’s why this is considered
    an *off-policy* algorithm.)
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we can modify the update rule to consider the maximum Q-value by
    varying different actions in the next immediate state. The modified equation for
    updating the Q-values is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_114.png)'
  prefs: []
  type: TYPE_IMG
- en: We encourage you to compare the update rule here with that of the SARSA algorithm.
    As you can see, we find the best action in the next state, *S*[t][+1], and use
    that in the correction term to update our estimate of *Q*(*S*[t], *A*[t]).
  prefs: []
  type: TYPE_NORMAL
- en: To put these materials into perspective, in the next section, we will see how
    to implement the Q-learning algorithm for solving a *grid world problem*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first RL algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the implementation of the Q-learning algorithm
    to solve a *grid world problem* (a grid world is a two-dimensional, cell-based
    environment where the agent moves in four directions to collect as much reward
    as possible). To do this, we use the OpenAI Gym toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the OpenAI Gym toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI Gym is a specialized toolkit for facilitating the development of RL
    models. OpenAI Gym comes with several predefined environments. Some basic examples
    are CartPole and MountainCar, where the tasks are to balance a pole and to move
    a car up a hill, respectively, as the names suggest. There are also many advanced
    robotics environments for training a robot to fetch, push, and reach for items
    on a bench or training a robotic hand to orient blocks, balls, or pens. Moreover,
    OpenAI Gym provides a convenient, unified framework for developing new environments.
    More information can be found on its official website: [https://gym.openai.com/](https://gym.openai.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow the OpenAI Gym code examples in the next sections, you need to install
    the `gym` library (at the time of writing, version 0.20.0 was used), which can
    be easily done using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you need additional help with the installation, please refer to the official
    installation guide at [https://gym.openai.com/docs/#installation](https://gym.openai.com/docs/#installation).
  prefs: []
  type: TYPE_NORMAL
- en: Working with the existing environments in OpenAI Gym
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For practice with the Gym environments, let’s create an environment from `CartPole-v1`,
    which already exists in OpenAI Gym. In this example environment, there is a pole
    attached to a cart that can move horizontally, as shown in *Figure 19.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.6: The CartPole example in Gym'
  prefs: []
  type: TYPE_NORMAL
- en: The movements of the pole are governed by the laws of physics, and the goal
    for RL agents is to learn how to move the cart to stabilize the pole and prevent
    it from tipping over to either side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at some properties of the CartPole environment in the context
    of RL, such as its state (or observation) space, action space, and how to execute
    an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we created an environment for the CartPole problem.
    The observation space for this environment is `Box(4,)` (with float values from
    `-inf` to `inf`), which represents a four-dimensional space corresponding to four
    real-valued numbers: the position of the cart, the cart’s velocity, the angle
    of the pole, and the velocity of the tip of the pole. The action space is a discrete
    space, `Discrete(2)`, with two choices: pushing the cart either to the left or
    to the right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment object, `env`, that we previously created by calling `gym.make(''CartPole-v1'')`
    has a `reset()` method that we can use to reinitialize an environment prior to
    each episode. Calling the `reset()` method will basically set the pole’s starting
    state (*S*[0]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The values in the array returned by the `env.reset()` method call mean that
    the initial position of the cart is –0.039, with a velocity –0.008, and the angle
    of the pole is 0.033 radians, while the angular velocity of its tip is –0.021\.
    Upon calling the `reset()` method, these values are initialized with random values
    with uniform distribution in the range [–0.05, 0.05].
  prefs: []
  type: TYPE_NORMAL
- en: 'After resetting the environment, we can interact with the environment by choosing
    an action and executing it by passing the action to the `step()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Via the previous two commands, `env.step(action=0)` and `env.step(action=1)`,
    we pushed the cart to the left (`action=0`) and then to the right (`action=1`),
    respectively. Based on the selected action, the cart and its pole can move as
    governed by the laws of physics. Every time we call `env.step()`, it returns a
    tuple consisting of four elements:'
  prefs: []
  type: TYPE_NORMAL
- en: An array for the new state (or observations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reward (a scalar value of type `float`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A termination flag (`True` or `False`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python dictionary containing auxiliary information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `env` object also has a `render()` method, which we can execute after each
    step (or a series of steps) to visualize the environment and the movements of
    the pole and cart, through time.
  prefs: []
  type: TYPE_NORMAL
- en: The episode terminates when the angle of the pole becomes larger than 12 degrees
    (from either side) with respect to an imaginary vertical axis, or when the position
    of the cart is more than 2.4 units from the center position. The reward defined
    in this example is to maximize the time the cart and pole are stabilized within
    the valid regions—in other words, the total reward (that is, return) can be maximized
    by maximizing the length of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: A grid world example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After introducing the CartPole environment as a warm-up exercise for working
    with the OpenAI Gym toolkit, we will now switch to a different environment. We
    will work with a grid world example, which is a simplistic environment with *m*
    rows and *n* columns. Considering *m* = 5 and *n* = 6, we can summarize this environment
    as shown in *Figure 19.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.7: An example of a grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this environment, there are 30 different possible states. Four of these
    states are terminal states: a pot of gold at state 16 and three traps at states
    10, 15, and 22\. Landing in any of these four terminal states will end the episode,
    but with a difference between the gold and trap states. Landing on the gold state
    yields a positive reward, +1, whereas moving the agent onto one of the trap states
    yields a negative reward, –1\. All other states have a reward of 0\. The agent
    always starts from state 0\. Therefore, every time we reset the environment, the
    agent will go back to state 0\. The action space consists of four directions:
    move up, down, left, and right.'
  prefs: []
  type: TYPE_NORMAL
- en: When the agent is at the outer boundary of the grid, selecting an action that
    would result in leaving the grid will not change the state.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how to implement this environment in Python using the OpenAI
    Gym package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the grid world environment in OpenAI Gym
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For experimenting with the grid world environment via OpenAI Gym, using a script
    editor or IDE rather than executing the code interactively is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a new Python script named `gridworld_env.py` and then proceed
    by importing the necessary packages and two helper functions that we define for
    building the visualization of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: To render the environments for visualization purposes, the OpenAI Gym library
    uses the pyglet library and provides wrapper classes and functions for our convenience.
    We will use these wrapper classes for visualizing the grid world environment in
    the following code example. More details about these wrapper classes can be found
    at [https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/envs/classic_control/rendering.py](https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/envs/classic_control/rendering.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example uses those wrapper classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Using Gym 0.22 or newer**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `gym` is currently undergoing some internal restructuring. In version
    0.22 and newer, you may have to update the previous code example (from `gridworld_env.py`)
    and replace the following line
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For more details, please refer to the code repository at [https://github.com/rasbt/machine-learning-book/tree/main/ch19](https://github.com/rasbt/machine-learning-book/tree/main/ch19)
  prefs: []
  type: TYPE_NORMAL
- en: The first helper function, `get_coords()`, returns the coordinates of the geometric
    shapes that we will use to annotate the grid world environment, such as a triangle
    to display the gold or circles to display the traps. The list of coordinates is
    passed to `draw_object()`, which decides to draw a circle, a triangle, or a polygon
    based on the length of the input list of coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the grid world environment. In the same file (`gridworld_env.py`),
    we define a class named `GridWorldEnv`, which inherits from OpenAI Gym’s `DiscreteEnv`
    class. The most important function of this class is the constructor method, `__init__()`,
    where we define the action space, specify the role of each action, and determine
    the terminal states (gold as well as traps) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code implements the grid world environment, from which we can create instances
    of this environment. We can then interact with it in a manner similar to that
    in the CartPole example. The implemented class, `GridWorldEnv`, inherits methods
    such as `reset()` for resetting the state and `step()` for executing an action.
    The details of the implementation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We defined the four different actions using lambda functions: `move_up()`,
    `move_down()`, `move_left()`, and `move_right()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy array `isd` holds the probabilities of the starting states so that
    a random state will be selected based on this distribution when the `reset()`
    method (from the parent class) is called. Since we always start from state 0 (the
    lower-left corner of the grid world), we set the probability of state 0 to 1.0
    and the probabilities of all other 29 states to 0.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transition probabilities, defined in the Python dictionary `P` determine
    the probabilities of moving from one state to another state when an action is
    selected. This allows us to have a probabilistic environment where taking an action
    could have different outcomes based on the stochasticity of the environment. For
    simplicity, we just use a single outcome, which is to change the state in the
    direction of the selected action. Finally, these transition probabilities will
    be used by the `env.step()` function to determine the next state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, the `_build_display()` function will set up the initial visualization
    of the environment, and the `render()` function will show the movements of the
    agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that during the learning process, we do not know about the transition probabilities,
    and the goal is to learn by interacting with the environment. Therefore, we do
    not have access to `P` outside the class definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can test this implementation by creating a new environment and visualizing
    a random episode by taking random actions at each state. Include the following
    code at the end of the same Python script (`gridworld_env.py`) and then execute
    the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the script, you should see a visualization of the grid world
    environment as depicted in *Figure 19.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant capture d’écran  Description générée automatiquement](img/17582_19_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.8: A visualization of our grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: Solving the grid world problem with Q-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After focusing on the theory and the development process of RL algorithms, as
    well as setting up the environment via the OpenAI Gym toolkit, we will now implement
    the currently most popular RL algorithm, Q-learning. For this, we will use the
    grid world example that we already implemented in the script `gridworld_env.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we create a new script and name it `agent.py`. In this `agent.py` script,
    we define an agent for interacting with the environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `__init__()` constructor sets up various hyperparameters, such as the learning
    rate, discount factor (![](img/B17582_03_056.png)), and the parameters for the
    ![](img/B17582_19_096.png)-greedy policy. Initially, we start with a high value
    of ![](img/B17582_19_096.png), but the `_adjust_epsilon()` method reduces it until
    it reaches the minimum value, ![](img/B17582_19_118.png). The `choose_action()`
    method chooses an action based on the ![](img/B17582_19_119.png)-greedy policy
    as follows. A random uniform number is selected to determine whether the action
    should be selected randomly or otherwise, based on the action-value function.
    The `_learn()` method implements the update rule for the Q-learning algorithm.
    It receives a tuple for each transition, which consists of the current state (*s*),
    selected action (*a*), observed reward (*r*), next state (*s*′), as well as a
    flag to determine whether the end of the episode has been reached. The target
    value is equal to the observed reward (*r*) if this is flagged as end-of-episode;
    otherwise, the target is ![](img/B17582_19_120.png).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for our next step, we create a new script, `qlearning.py`, to put everything
    together and train the agent using the Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we define a function, `run_qlearning()`, that implements
    the Q-learning algorithm, simulating an episode by calling the `_choose_action()`
    method of the agent and executing the environment. Then, the transition tuple
    is passed to the `_learn()` method of the agent to update the action-value function.
    In addition, for monitoring the learning process, we also store the final reward
    of each episode (which could be –1 or +1), as well as the length of episodes (the
    number of moves taken by the agent from the start of the episode until the end).
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of rewards and the number of moves is then plotted using the `plot_learning_history()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this script will run the Q-learning program for 50 episodes. The
    behavior of the agent will be visualized, and you can see that at the beginning
    of the learning process, the agent mostly ends up in the trap states. But over
    time, it learns from its failures and eventually finds the gold state (for instance,
    the first time in episode 7). *Figure 19.9* shows the agent’s number of moves
    and rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.9: The agent’s number of moves and rewards'
  prefs: []
  type: TYPE_NORMAL
- en: The plotted learning history shown in the previous figure indicates that the
    agent, after 30 episodes, learns a short path to get to the gold state. As a result,
    the lengths of the episodes after the 30th episode are more or less the same,
    with minor deviations due to the ![](img/B17582_19_112.png)-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: A glance at deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous code, we saw an implementation of the popular Q-learning algorithm
    for the grid world example. This example consisted of a discrete state space of
    size 30, where it was sufficient to store the Q-values in a Python dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: However, we should note that sometimes the number of states can get very large,
    possibly almost infinitely large. Also, we may be dealing with a continuous state
    space instead of working with discrete states. Moreover, some states may not be
    visited at all during training, which can be problematic when generalizing the
    agent to deal with such unseen states later.
  prefs: []
  type: TYPE_NORMAL
- en: To address these problems, instead of representing the value function in a tabular
    format like *V*(*S*[t]), or *Q*(*S*[t], *A*[t]), for the action-value function,
    we use a *function approximation* approach. Here, we define a parametric function,
    *v*[w](*x*[s]), that can learn to approximate the true value function, that is,
    ![](img/B17582_19_122.png), where *x*[s] is a set of input features (or “featurized”
    states).
  prefs: []
  type: TYPE_NORMAL
- en: 'When the approximator function, *q*[w](*x*[s], *a*), is a **deep neural network**
    (**DNN**), the resulting model is called a **deep Q-network** (**DQN**). For training
    a DQN model, the weights are updated according to the Q-learning algorithm. An
    example of a DQN model is shown in *Figure 19.10*, where the states are represented
    as features passed to the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.10: An example of a DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can train a DQN using the *deep Q-learning* algorithm.
    Overall, the main approach is very similar to the tabular Q-learning method. The
    main difference is that we now have a multilayer NN that computes the action values.
  prefs: []
  type: TYPE_NORMAL
- en: Training a DQN model according to the Q-learning algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the procedure for training a DQN model using the
    Q-learning algorithm. The deep Q-learning approach requires us to make some modifications
    to our previously implemented standard Q-learning approach.
  prefs: []
  type: TYPE_NORMAL
- en: One such modification is in the agent’s `choose_action()` method, which, in
    the code of the previous section for Q-learning, was simply accessing the action
    values stored in a dictionary. Now, this function should be changed to perform
    a forward pass of the NN model for computing the action values.
  prefs: []
  type: TYPE_NORMAL
- en: The other modifications needed for the deep Q-learning algorithm are described
    in the following two subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Replay memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the previous tabular method for Q-learning, we could update the values
    for specific state-action pairs without affecting the values of others. However,
    now that we approximate *q*(*s*, *a*) with an NN model, updating the weights for
    a state-action pair will likely affect the output of other states as well. When
    training NNs using stochastic gradient descent for a supervised task (for example,
    a classification task), we use multiple epochs to iterate through the training
    data multiple times until it converges.
  prefs: []
  type: TYPE_NORMAL
- en: This is not feasible in Q-learning, since the episodes will change during the
    training and, as a result, some states that were visited in the early stages of
    training will become less likely to be visited later.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, another problem is that when we train an NN, we assume that the
    training examples are **IID** (**independent and identically distributed**). However,
    the samples taken from an episode of the agent are not IID, as they form a sequence
    of transitions.
  prefs: []
  type: TYPE_NORMAL
- en: To solve these issues, as the agent interacts with the environment and generates
    a transition quintuple *q*[w](*x*[s], *a*), we store a large (but finite) number
    of such transitions in a memory buffer, often called *replay memory*. After each
    new interaction (that is, the agent selects an action and executes it in the environment),
    the resulting new transition quintuple is appended to the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the size of the memory bounded, the oldest transition will be removed
    from the memory (for example, if it is a Python list, we can use the `pop(0)`
    method to remove the first element of the list). Then, a mini-batch of examples
    is randomly selected from the memory buffer, which will be used for computing
    the loss and updating the network parameters. *Figure 19.11* illustrates the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.11: The replay memory process'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing the replay memory**'
  prefs: []
  type: TYPE_NORMAL
- en: The replay memory can be implemented using a Python list, where every time we
    add a new element to the list, we need to check the size of the list and call
    `pop(0)` if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use the `deque` data structure from the Python `collections`
    library, which allows us to specify an optional argument, `max_len`. By specifying
    the `max_len` argument, we will have a bounded deque. Therefore, when the object
    is full, appending a new element results in automatically removing an element
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is more efficient than using a Python list, since removing the
    first element of a list using `pop(0)` has O(n) complexity, while the deque’s
    runtime complexity is O(1). You can learn more about the deque implementation
    from the official documentation that is available at [https://docs.python.org/3.9/library/collections.html#collections.deque](https://docs.python.org/3.9/library/collections.html#collections.deque).
  prefs: []
  type: TYPE_NORMAL
- en: Determining the target values for computing the loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another required change from the tabular Q-learning method is how to adapt the
    update rule for training the DQN model parameters. Recall that a transition quintuple,
    *T*, stored in the batch of examples, contains ![](img/B17582_19_123.png).
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 19.12*, we perform two forward passes of the DQN model.
    The first forward pass uses the features of the current state (*x*[s]). Then,
    the second forward pass uses the features of the next state (![](img/B17582_19_124.png)).
    As a result, we will obtain the estimated action values, ![](img/B17582_19_125.png)
    and ![](img/B17582_19_126.png), from the first and second forward pass, respectively.
    (Here, this ![](img/B17582_19_127.png) notation means a vector of Q-values for
    all actions in ![](img/B17582_19_128.png).) From the transition quintuple, we
    know that action *a* is selected by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, according to the Q-learning algorithm, we need to update the action
    value corresponding to the state-action pair (*x*[s], *a*) with the scalar target
    value ![](img/B17582_19_129.png). Instead of forming a scalar target value, we
    will create a target action-value vector that retains the action values for other
    actions, ![](img/B17582_19_130.png), as shown in *Figure 19.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.12: Determining the target value using the DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We treat this as a regression problem, using the following three quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: The currently predicted values, ![](img/B17582_19_131.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value vector as described
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard **mean squared error** (**MSE**) loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, the losses will be zero for every action except for *a*. Finally,
    the computed loss will be backpropagated to update the network parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep Q-learning algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we will use all these techniques to implement a deep Q-learning algorithm.
    This time, we use the CartPole environment from the OpenAI Gym environment that
    we introduced earlier. Recall that the CartPole environment has a continuous state
    space of size 4\. In the following code, we define a class, `DQNAgent`, that builds
    the model and specifies various hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class has two additional methods compared to the previous agent that was
    based on tabular Q-learning. The `remember()` method will append a new transition
    quintuple to the memory buffer, and the `replay()` method will create a mini-batch
    of example transitions and pass that to the `_learn()` method for updating the
    network’s weight parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, with the following code, we train the model for 200 episodes, and
    at the end visualize the learning history using the `plot_learning_history()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the agent for 200 episodes, we see that the agent indeed learned
    to increase the total rewards over time, as shown in *Figure 19.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.13: The agent’s rewards increased over time'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the total rewards obtained in an episode are equal to the amount of
    time that the agent is able to balance the pole. The learning history plotted
    in this figure shows that after about 30 episodes, the agent learns how to balance
    the pole and hold it for more than 200 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter and book summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the essential concepts in RL, starting from the
    very foundations, and how RL can support decision making in complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned about agent-environment interactions and **Markov decision processes**
    (**MDPs**), and we considered three main approaches for solving RL problems: dynamic
    programming, MC learning, and TD learning. We discussed the fact that the dynamic
    programming algorithm assumes that the full knowledge of environment dynamics
    is available, an assumption that is not typically true for most real-world problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we saw how the MC- and TD-based algorithms learn by allowing an agent
    to interact with the environment and generate a simulated experience. After discussing
    the underlying theory, we implemented the Q-learning algorithm as an off-policy
    subcategory of the TD algorithm for solving the grid world example. Finally, we
    covered the concept of function approximation and deep Q-learning in particular,
    which can be used for problems with large or continuous state spaces.
  prefs: []
  type: TYPE_NORMAL
- en: We hope you enjoyed this last chapter of *Python Machine Learning* and our exciting
    tour of machine learning and deep learning. Throughout the journey of this book,
    we’ve covered the essential topics that this field has to offer, and you should
    now be well equipped to put those techniques into action to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We started our journey in *Chapter 1*, *Giving Computers the Ability to Learn
    from Data*, with a brief overview of the different types of learning tasks: supervised
    learning, reinforcement learning, and unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed several different learning algorithms that you can use for
    classification, starting with simple single-layer NNs in *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: We continued to discuss advanced classification algorithms in *Chapter 3*, *A
    Tour of Machine Learning Classifiers Using Scikit-Learn*, and we learned about
    the most important aspects of a machine learning pipeline in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, and *Chapter 5*, *Compressing Data
    via Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that even the most advanced algorithm is limited by the information
    in the training data that it gets to learn from. So, in *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we learned about
    the best practices to build and evaluate predictive models, which is another important
    aspect in machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: If one single learning algorithm does not achieve the performance we desire,
    it can sometimes be helpful to create an ensemble of experts to make a prediction.
    We explored this in *Chapter 7*, *Combining Different Models for Ensemble Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in *Chapter 8*, *Applying Machine Learning to Sentiment Analysis*, we
    applied machine learning to analyze one of the most popular and interesting forms
    of data in the modern age, which is dominated by social media platforms on the
    internet—text documents.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, our focus was on algorithms for classification, which is
    probably the most popular application of machine learning. However, this is not
    where our journey ended! In *Chapter 9*, *Predicting Continuous Target Variables
    with Regression Analysis*, we explored several algorithms for regression analysis
    to predict continuous target variables.
  prefs: []
  type: TYPE_NORMAL
- en: Another exciting subfield of machine learning is clustering analysis, which
    can help us find hidden structures in the data, even if our training data does
    not come with the right answers to learn from. We worked with this in *Chapter
    10*, *Working with Unlabeled Data – Clustering Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: We then shifted our attention to one of the most exciting algorithms in the
    whole machine learning field—artificial neural networks. We started by implementing
    a multilayer perceptron from scratch with NumPy in *Chapter 11*, *Implementing
    a Multilayer Artificial Neural Network from Scratch*.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of PyTorch for deep learning became obvious in *Chapter 12*, *Parallelizing
    Neural Network Training with PyTorch*, where we used PyTorch to facilitate the
    process of building NN models, worked with PyTorch `Dataset` objects, and learned
    how to apply preprocessing steps to a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We delved deeper into the mechanics of PyTorch in *Chapter 13*, *Going Deeper
    – The Mechanics of PyTorch*, and discussed the different aspects and mechanics
    of PyTorch, including tensor objects, computing gradients of a computation, as
    well as the neural network module, `torch.nn`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 14*, *Classifying Images with Deep Convolutional Neural Networks*,
    we dived into convolutional neural networks, which are widely used in computer
    vision at the moment, due to their great performance in image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we learned about sequence modeling using RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 16*, *Transformers – Improving Natural Language Processing with
    Attention Mechanisms*, we introduced the attention mechanism to address one of
    the weaknesses of RNNs, that is, remembering previous input elements when dealing
    with long sequences. We then explored various kinds of transformer architectures,
    which are deep learning architectures that are centered around the self-attention
    mechanism and constitute the state of the art for creating large-scale language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 17*, *Generative Adversarial Networks for Synthesizing New Data*,
    we saw how to generate new images using GANs and, along the way, we also learned
    about autoencoders, batch normalization, transposed convolution, and Wasserstein
    GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Previous chapters were centered around tabular datasets as well as text and
    image data. In *Chapter 18*, *Graph Neural Networks for Capturing Dependencies
    in Graph Structured Data*, we focused on deep learning for graph-structured data,
    which is commonly used data representation for social networks and molecules (chemical
    compounds). Moreover, we learned about so-called graph neural networks, which
    are deep neural networks that are compatible with such data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in this chapter, we covered a separate category of machine learning
    tasks and saw how to develop algorithms that learn by interacting with their environment
    through a reward process.
  prefs: []
  type: TYPE_NORMAL
- en: While a comprehensive study of deep learning is well beyond the scope of this
    book, we hope that we’ve kindled your interest enough to follow the most recent
    advancements in this field of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re considering a career in machine learning, or you just want to keep
    up to date with the current advancements in this field, we can recommend that
    you keep an eye on the recent literature published in this field. The following
    are some resources that we find particularly useful:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A subreddit and community dedicated to learning machine learning: [https://www.reddit.com/r/learnmachinelearning/](https://www.reddit.com/r/learnmachinelearning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A daily updated list of the latest machine learning manuscripts uploaded to
    the arXiv preprint server: [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A paper recommendation engine built on top of arXiv: [http://www.arxiv-sanity.com](http://www.arxiv-sanity.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, you can find out what we, the authors, are up to at these sites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebastian Raschka: [https://sebastianraschka.com](https://sebastianraschka.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hayden Liu: [https://www.mlexample.com/](https://www.mlexample.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vahid Mirjalili: [http://vahidmirjalili.com](http://vahidmirjalili.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re always welcome to contact us if you have any questions about this book
    or if you need some general tips on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
