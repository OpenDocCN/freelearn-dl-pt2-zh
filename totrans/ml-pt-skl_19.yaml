- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Reinforcement Learning for Decision Making in Complex Environments
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策复杂环境中的强化学习
- en: In the previous chapters, we focused on supervised and unsupervised machine
    learning. We also learned how to leverage artificial neural networks and deep
    learning to tackle problems encountered with these types of machine learning.
    As you’ll recall, supervised learning focuses on predicting a category label or
    continuous value from a given input feature vector. Unsupervised learning focuses
    on extracting patterns from data, making it useful for data compression (*Chapter
    5*, *Compressing Data via Dimensionality Reduction*), clustering (*Chapter 10*,
    *Working with Unlabeled Data – Clustering Analysis*), or approximating the training
    set distribution for generating new data (*Chapter 17*, *Generative Adversarial
    Networks for Synthesizing New Data*).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们专注于监督学习和无监督学习。我们还学习了如何利用人工神经网络和深度学习来解决这些类型机器学习所遇到的问题。回想一下，监督学习专注于从给定输入特征向量预测类别标签或连续值。无监督学习专注于从数据中提取模式，使其在数据压缩（*第5章*，*通过降维压缩数据*）、聚类（*第10章*，*使用无标签数据进行聚类分析*）或近似训练集分布以生成新数据（*第17章*，*生成对抗网络用于合成新数据*）等方面非常有用。
- en: 'In this chapter, we turn our attention to a separate category of machine learning,
    **reinforcement learning** (**RL**), which is different from the previous categories
    as it is focused on learning *a series of actions* for optimizing an overall reward—for
    example, winning at a game of chess. In summary, this chapter will cover the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将注意力转向另一类机器学习，**强化学习**（**RL**），与以往的类别不同，它侧重于学习*一系列动作*以优化整体奖励，例如，在国际象棋游戏中取得胜利。总之，本章将涵盖以下主题：
- en: Learning the basics of RL, getting familiar with agent/environment interactions,
    and understanding how the reward process works, in order to help make decisions
    in complex environments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习RL的基础知识，熟悉代理与环境的交互，理解奖励过程如何工作，以帮助在复杂环境中做出决策
- en: Introducing different categories of RL problems, model-based and model-free
    learning tasks, Monte Carlo, and temporal difference learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入RL问题的不同类别，基于模型和无模型学习任务，蒙特卡洛和时序差分学习算法
- en: Implementing a Q-learning algorithm in a tabular format
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表格格式中实现Q学习算法
- en: Understanding function approximation for solving RL problems, and combining
    RL with deep learning by implementing a *deep* Q-learning algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解用于解决RL问题的函数逼近，通过实现*深度* Q学习算法结合RL与深度学习
- en: RL is a complex and vast area of research, and this chapter focuses on the fundamentals.
    As this chapter serves as an introduction, and to keep our attention on the important
    methods and algorithms, we will work mainly with basic examples that illustrate
    the main concepts. However, toward the end of this chapter, we will go over a
    more challenging example and utilize deep learning architectures for a particular
    RL approach known as deep Q-learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RL是一个复杂而广阔的研究领域，本章侧重于基础知识。由于本章作为介绍，为了集中注意力于重要的方法和算法，我们将主要使用能够说明主要概念的基础示例。然而，在本章末尾，我们将介绍一个更具挑战性的例子，并利用深度学习架构来实现一种称为深度Q学习的特定RL方法。
- en: Introduction – learning from experience
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍——从经验中学习
- en: In this section, we will first introduce the concept of RL as a branch of machine
    learning and see its major differences compared with other tasks of machine learning.
    After that, we will cover the fundamental components of an RL system. Then, we
    will see the RL mathematical formulation based on the Markov decision process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍RL作为机器学习的一个分支的概念，并看到它与其他机器学习任务相比的主要区别。之后，我们将覆盖RL系统的基本组成部分。然后，我们将基于马尔可夫决策过程看RL的数学公式化。
- en: Understanding reinforcement learning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解强化学习
- en: Until this point, this book has primarily focused on *supervised* and *unsupervised*
    learning. Recall that in *supervised* learning, we rely on labeled training examples,
    which are provided by a supervisor or a human expert, and the goal is to train
    a model that can generalize well to unseen, unlabeled test examples. This means
    that the supervised learning model should learn to assign the same labels or values
    to a given input example as the supervisor human expert. On the other hand, in
    *unsupervised* learning, the goal is to learn or capture the underlying structure
    of a dataset, such as in clustering and dimensionality reduction methods; or learning
    how to generate new, synthetic training examples with a similar underlying distribution.
    RL is substantially different from supervised and unsupervised learning, and so
    RL is often regarded as the “third category of machine learning.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书主要集中在*监督*和*无监督*学习上。回顾一下，在*监督*学习中，我们依赖于由监督员或人类专家提供的标记训练样本，目标是训练一个能够很好地推广到未见过的未标记测试样本的模型。这意味着监督学习模型应该学会为给定的输入示例分配与监督人类专家相同的标签或值。另一方面，在*无监督*学习中，目标是学习或捕获数据集的潜在结构，例如在聚类和降维方法中；或者学习如何生成具有类似潜在分布的新的合成训练样本。强化学习与监督和无监督学习显著不同，因此强化学习通常被视为“机器学习的第三类别”。
- en: The key element that distinguishes RL from other subtasks of machine learning,
    such as supervised and unsupervised learning, is that RL is centered around the
    concept of *learning by interaction*. This means that in RL, the model learns
    from interactions with an environment to maximize a *reward function*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 区分强化学习与监督学习、无监督学习等其他机器学习子任务的关键元素是，强化学习围绕*通过交互学习*的概念。这意味着在强化学习中，模型通过与环境的交互学习来最大化一个*奖励函数*。
- en: While maximizing a reward function is related to the concept of minimizing the
    loss function in supervised learning, the *correct* labels for learning a series
    of actions are not known or defined upfront in RL—instead, they need to be learned
    through interactions with the environment to achieve a certain desired outcome—such
    as winning at a game. With RL, the model (also called an **agent**) interacts
    with its environment, and by doing so generates a sequence of interactions that
    are together called an **episode**. Through these interactions, the agent collects
    a series of rewards determined by the environment. These rewards can be positive
    or negative, and sometimes they are not disclosed to the agent until the end of
    an episode.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大化奖励函数与监督学习中最小化损失函数的概念相关联时，用于学习一系列动作的*正确*标签在强化学习中并不事先已知或定义——相反，它们需要通过与环境的交互学习以达到某种期望的结果——比如在游戏中取得胜利。通过强化学习，模型（也称为**代理**）与其环境交互，从而生成一系列称为**回合（episode）**的交互序列。通过这些交互，代理收集由环境确定的一系列奖励。这些奖励可以是正面的也可以是负面的，并且有时候直到回合结束后才会向代理披露。
- en: For example, imagine that we want to teach a computer to play the game of chess
    and win against human players. The labels (rewards) for each individual chess
    move made by the computer are not known until the end of the game, because during
    the game itself, we don’t know whether a particular move will result in winning
    or losing that game. Only right at the end of the game is the feedback determined.
    That feedback would likely be a positive reward given if the computer won the
    game because the agent had achieved the overall desired outcome; and vice versa,
    a negative reward would likely be given if the computer had lost the game.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象我们想教计算机玩国际象棋并赢得人类玩家。计算机所做的每一步棋的标签（奖励）直到游戏结束都是未知的，因为在游戏过程中，我们不知道特定的一步棋是否会导致赢得或失去该游戏。直到游戏结束时才确定反馈。如果计算机赢得了比赛，那么反馈很可能是正面奖励，因为代理已经实现了整体期望的结果；反之，如果计算机输了比赛，那么很可能会给出负面奖励。
- en: Furthermore, considering the example of playing chess, the input is the current
    configuration, for instance, the arrangement of the individual chess pieces on
    the board. Given the large number of possible inputs (the states of the system),
    it is impossible to label each configuration or state as positive or negative.
    Therefore, to define a learning process, we provide rewards (or penalties) at
    the end of each game, when we know whether we reached the desired outcome—whether
    we won the game or not.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑下象棋的例子，输入是当前的配置，例如棋盘上个别棋子的排列方式。考虑到可能的输入（系统状态的状态）的数量巨大，不可能标记每个配置或状态为正面或负面。因此，为了定义学习过程，我们在每局比赛结束时提供奖励（或惩罚），当我们知道是否达到了期望的结果——无论是赢得比赛还是没有。
- en: This is the essence of RL. In RL, we cannot or do not teach an agent, computer,
    or robot *how* to do things; we can only specify *what* we want the agent to achieve.
    Then, based on the outcome of a particular trial, we can determine rewards depending
    on the agent’s success or failure. This makes RL very attractive for decision
    making in complex environments, especially when the problem-solving task requires
    a series of steps, which are unknown, or hard to explain, or hard to define.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是强化学习的精髓。在强化学习中，我们不能或不会教导一个代理、计算机或机器人*如何*做事情；我们只能明确*我们希望*代理实现的目标是什么。然后，基于特定试验的结果，我们可以根据代理的成功或失败来确定奖励。这使得强化学习在复杂环境中作出决策特别具有吸引力，特别是当问题解决任务需要一系列步骤时，这些步骤是未知的，或者难以解释，或者难以定义。
- en: Besides applications in games and robotics, examples of RL can also be found
    in nature. For example, training a dog involves RL—we hand out rewards (treats)
    to the dog when it performs certain desirable actions. Or consider a medical dog
    that is trained to warn its partner of an oncoming seizure. In this case, we do
    not know the exact mechanism by which the dog is able to detect an oncoming seizure,
    and we certainly wouldn’t be able to define a series of steps to learn seizure
    detection, even if we had precise knowledge of this mechanism. However, we can
    reward the dog with a treat if it successfully detects a seizure to *reinforce*
    this behavior!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在游戏和机器人技术中的应用之外，强化学习的例子也可以在自然界中找到。例如，训练一只狗就涉及到强化学习——当它执行某些理想的动作时，我们给它奖励（零食）。或者考虑一个训练有素的医疗狗，它被训练来警告它的伴侣即将发作癫痫。在这种情况下，我们不知道狗能够检测癫痫发作的确切机制，即使我们对这一机制有精确的了解，我们也无法定义一系列步骤来学习癫痫检测。然而，如果它成功检测到癫痫，我们可以给它一块零食来*强化*这种行为！
- en: While RL provides a powerful framework for learning an arbitrary series of actions
    to achieve a certain goal, please do keep in mind that RL is still a relatively
    young and active area of research with many unresolved challenges. One aspect
    that makes training RL models particularly challenging is that the consequent
    model inputs depend on actions taken previously. This can lead to all sorts of
    problems, and usually results in unstable learning behavior. Also, this sequence-dependence
    in RL creates a so-called *delayed effect*, which means that the action taken
    at a time step *t* may result in a future reward appearing some arbitrary number
    of steps later.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习为学习实现某个目标的任意系列动作提供了一个强大的框架，请记住强化学习仍然是一个相对年轻且活跃的研究领域，有许多未解决的挑战。使训练强化学习模型特别具有挑战性的一个方面是，后续模型输入取决于先前采取的行动。这可能导致各种问题，并通常导致不稳定的学习行为。此外，强化学习中的这种序列依赖性产生了所谓的*延迟效应*，这意味着在时间步骤*t*上采取的行动可能导致未来几个步骤后出现奖励。
- en: Defining the agent-environment interface of a reinforcement learning system
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义强化学习系统的代理-环境接口
- en: 'In all examples of RL, we can find two distinct entities: an agent and an environment.
    Formally, an **agent** is defined as an entity that learns how to make decisions
    and interacts with its surrounding environment by taking an action. In return,
    as a consequence of taking an action, the agent receives observations and a reward
    signal as governed by the environment. The **environment** is anything that falls
    outside the agent. The environment communicates with the agent and determines
    the reward signal for the agent’s action as well as its observations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有强化学习的例子中，我们可以找到两个不同的实体：一个代理和一个环境。正式地说，**代理**被定义为一个学习如何做出决策并通过采取行动与其周围环境交互的实体。作为采取行动的后果，代理接收来自环境的观察和奖励信号。**环境**是指代理之外的任何东西。环境与代理进行通信，并确定代理行动的奖励信号以及其观察结果。
- en: 'The **reward signal** is the feedback that the agent receives from interacting
    with the environment, which is usually provided in the form of a scalar value
    and can be either positive or negative. The purpose of the reward is to tell the
    agent how well it has performed. The frequency at which the agent receives the
    reward depends on the given task or problem. For example, in the game of chess,
    the reward would be determined after a full game based on the outcome of all the
    moves: a win or a loss. On the other hand, we could define a maze such that the
    reward is determined after each time step. In such a maze, the agent then tries
    to maximize its accumulated rewards over its lifetime—where lifetime describes
    the duration of an episode.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励信号**是代理从与环境交互中接收到的反馈，通常以标量形式提供，可以是正或负。奖励的目的是告诉代理其表现如何。代理接收奖励的频率取决于给定的任务或问题。例如，在国际象棋游戏中，奖励将在整个游戏后根据所有移动的结果确定：胜利或失败。另一方面，我们可以定义一个迷宫，使得奖励在每个时间步骤后确定。在这样的迷宫中，代理尝试最大化其终身内的累积奖励——其中终身描述了一个事件的持续时间。'
- en: '*Figure 19.1* illustrates the interactions and communication between the agent
    and the environment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.1*说明了代理与环境之间的交互和通信：'
- en: '![](img/17582_19_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_01.png)'
- en: 'Figure 19.1: The interaction between the agent and its environment'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1：代理与其环境之间的互动
- en: The state of the agent, as illustrated in *Figure 19.1*, is the set of all of
    its variables (1). For example, in the case of a robot drone, these variables
    could include the drone’s current position (longitude, latitude, and altitude),
    the drone’s remaining battery life, the speed of each fan, and so forth. At each
    time step, the agent interacts with the environment through a set of available
    actions **A**[t] (2). Based on the action taken by the agent denoted by **A**[t],
    while it is at state **S**[t], the agent will receive a reward signal **R**[t][+1]
    (3), and its state will become **S**[t][+1] (4).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的状态，如*图19.1*所示，是其所有变量的集合（1）。例如，在机器人无人机的情况下，这些变量可能包括无人机的当前位置（经度、纬度和高度）、无人机剩余的电池寿命、每个风扇的速度等。在每个时间步骤，代理通过一组可用的动作**A**[t]（2）与环境进行交互。基于代理在状态**S**[t]时采取的动作**A**[t]，代理将收到奖励信号**R**[t][+1]（3），并且其状态将变为**S**[t][+1]（4）。
- en: During the learning process, the agent must try different actions (**exploration**)
    so that it can progressively learn which actions to prefer and perform more often
    (**exploitation**) in order to maximize the total, cumulative reward. To understand
    this concept, let’s consider a very simple example where a new computer science
    graduate with a focus on software engineering is wondering whether to start working
    at a company (exploitation) or to pursue a master’s or Ph.D. degree to learn more
    about data science and machine learning (exploration). In general, exploitation
    will result in choosing actions with a greater short-term reward, whereas exploration
    can potentially result in greater total rewards in the long run. The tradeoff
    between exploration and exploitation has been studied extensively, and yet, there
    is no universal answer to this decision-making dilemma.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，代理必须尝试不同的动作（**探索**），以便逐步学习哪些动作更值得偏好和更频繁执行（**利用**），以最大化总累积奖励。为了理解这个概念，让我们考虑一个非常简单的例子：一个以软件工程为重点的新计算机科学研究生正在考虑是开始在一家公司工作（利用）还是继续攻读硕士或博士学位，学习更多关于数据科学和机器学习的知识（探索）。一般来说，利用将导致选择具有更大短期奖励的动作，而探索则可能在长期内实现更大的总奖励。探索和利用之间的权衡已被广泛研究，然而，在这种决策过程中没有普遍适用的答案。
- en: The theoretical foundations of RL
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的理论基础
- en: Before we jump into some practical examples and start training an RL model,
    which we will be doing later in this chapter, let’s first understand some of the
    theoretical foundations of RL. The following sections will begin by first examining
    the mathematical formulation of **Markov decision processes**, episodic versus
    continuing tasks, some key RL terminology, and dynamic programming using the **Bellman
    equation**. Let’s start with Markov decision processes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入一些实际示例并开始训练强化学习模型之前（我们将在本章稍后进行），让我们先理解一些强化学习的理论基础。接下来的部分将首先讨论马尔可夫决策过程的数学表达式、阶段性与持续性任务、一些关键的强化学习术语，以及使用**贝尔曼方程**进行动态规划。让我们从马尔可夫决策过程开始。
- en: Markov decision processes
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In general, the type of problems that RL deals with are typically formulated
    as **Markov decision processes** (**MDPs**). The standard approach for solving
    MDP problems is by using dynamic programming, but RL offers some key advantages
    over dynamic programming.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，RL处理的问题类型通常被形式化为Markov决策过程（MDPs）。解决MDP问题的标准方法是使用动态规划，但是RL相比动态规划具有一些关键优势。
- en: '**Dynamic programming**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**'
- en: Dynamic programming refers to a set of computer algorithms and programming methods
    that was developed by Richard Bellman in the 1950s. In a sense, dynamic programming
    is about recursive problem solving—solving relatively complicated problems by
    breaking them down into smaller subproblems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划指的是一组计算机算法和编程方法，由Richard Bellman在1950年代开发。从某种意义上说，动态规划是关于递归问题解决——通过将相对复杂的问题分解为更小的子问题来解决。
- en: The key difference between recursion and dynamic programming is that dynamic
    programming stores the results of subproblems (usually as a dictionary or other
    form of lookup table) so that they can be accessed in constant time (instead of
    recalculating them) if they are encountered again in future.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 递归与动态规划的关键区别在于，动态规划存储子问题的结果（通常作为字典或其他形式的查找表），这样如果将来再次遇到这些子问题，可以在常数时间内访问结果，而不是重新计算它们。
- en: Examples of some famous problems in computer science that are solved by dynamic
    programming include sequence alignment and computing the shortest path from point
    A to point B.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一些著名的计算机科学问题的例子，这些问题通过动态规划解决，包括序列比对和计算从点A到点B的最短路径。
- en: Dynamic programming is not a feasible approach, however, when the size of states
    (that is, the number of possible configurations) is relatively large. In such
    cases, RL is considered a much more efficient and practical alternative approach
    for solving MDPs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当状态的大小（即可能配置的数量）相对较大时，动态规划并不是一种可行的方法。在这种情况下，RL被认为是解决MDPs更高效和实际的替代方法。
- en: The mathematical formulation of Markov decision processes
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Markov决策过程的数学表述
- en: The types of problems that require learning an interactive and sequential decision-making
    process, where the decision at time step *t* affects the subsequent situations,
    are mathematically formalized as MDPs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 需要学习互动和顺序决策过程的问题类型，其中时间步*t*的决策会影响后续情况，数学上形式化为MDPs。
- en: 'In the case of the agent/environment interactions in RL, if we denote the agent’s
    starting state as *S*[0], the interactions between the agent and the environment
    result in a sequence as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中的代理/环境交互中，如果我们将代理的起始状态表示为*S*[0]，代理与环境的交互会导致以下序列：
- en: '{*S*[0], *A*[0], *R*[1]},    {*S*[1], *A*[1], *R*[2]},    {*S*[2], *A*[2], *R*[3]},    ...'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '{*S*[0], *A*[0], *R*[1]}，{*S*[1], *A*[1], *R*[2]}，{*S*[2], *A*[2], *R*[3]}，...'
- en: 'Note that the braces serve only as a visual aid. Here, *S*[t] and *A*[t] stand
    for the state and the action taken at time step *t*. *R*[t][+1] denotes the reward
    received from the environment after performing action *A*[t]. Note that *S*[t],
    *R*[t][+1], and *A*[t] are time-dependent random variables that take values from
    predefined finite sets denoted by ![](img/B17582_19_001.png), ![](img/B17582_19_002.png),
    and ![](img/B17582_19_003.png), respectively. In an MDP, these time-dependent
    random variables, *S*[t] and *R*[t][+1], have probability distributions that only
    depend on their values at the preceding time step, *t* – 1\. The probability distribution
    for *S*[t][+1] = *s*′ and *R*[t][+1] = *r* can be written as a conditional probability
    over the preceding state (*S*[t]) and taken action (*A*[t]) as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，大括号仅作为视觉辅助。这里，*S*[t]和*A*[t]代表时间步*t*处的状态和采取的动作。*R*[t][+1]表示执行动作*A*[t]后从环境获得的奖励。注意，*S*[t]、*R*[t][+1]和*A*[t]是依赖时间的随机变量，它们的取值来自预定义的有限集合，分别用![](img/B17582_19_001.png)、![](img/B17582_19_002.png)和![](img/B17582_19_003.png)表示。在MDP中，这些时间依赖的随机变量*S*[t]和*R*[t][+1]的概率分布仅依赖于它们在前一时间步*t* – 1的值。*S*[t][+1] = *s*′和*R*[t][+1] = *r*的概率分布可以写成关于前一状态（*S*[t]）和采取的动作（*A*[t]）的条件概率如下：
- en: '![](img/B17582_19_004.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_004.png)'
- en: This probability distribution completely defines the **dynamics of the environment**
    (or model of the environment) because, based on this distribution, all transition
    probabilities of the environment can be computed. Therefore, the environment dynamics
    are a central criterion for categorizing different RL methods. The types of RL
    methods that require a model of the environment or try to learn a model of the
    environment (that is, the environment dynamics) are called *model-based* methods,
    as opposed to *model-free* methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概率分布完全定义了环境的**动态**（或环境模型），因为基于这种分布，可以计算环境的所有转移概率。因此，环境动态是对不同RL方法进行分类的一个核心标准。那些需要环境模型或试图学习环境模型（即环境动态）的RL方法被称为**基于模型**的方法，与**无模型**方法相对。
- en: '**Model-free and model-based RL**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于模型和无模型RL**'
- en: When the probability ![](img/B17582_19_005.png) is known, then the learning
    task can be solved with dynamic programming. But when the dynamics of the environment
    are not known, as is the case in many real-world problems, then we would need
    to acquire a large number of samples by interacting with the environment to compensate
    for the unknown environment dynamics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当已知概率 ![](img/B17582_19_005.png) 时，可以使用动态规划来解决学习任务。但是在许多现实世界问题中，由于环境动态未知，我们需要通过与环境交互来获取大量样本以补偿这种不确定性。
- en: 'Two main approaches for dealing with this problem are the model-free **Monte
    Carlo** (**MC**) and **temporal difference** (**TD**) methods. The following chart
    displays the two main categories and the branches of each method:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此问题的两种主要方法是**蒙特卡洛**（**MC**）和**时间差分**（**TD**）方法。以下图表展示了这两种方法的主要类别和各自的分支：
- en: '![](img/17582_19_02.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_02.png)'
- en: 'Figure 19.2: The different models to use based on the environment dynamics'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2：基于环境动态使用的不同模型
- en: We will cover these different approaches and their branches from theory to practical
    algorithms in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从理论到实际算法覆盖这些不同方法及其分支。
- en: The environment dynamics can be considered deterministic if particular actions
    for given states are always or never taken, that is, ![](img/B17582_19_006.png).
    Otherwise, in the more general case, the environment would have stochastic behavior.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定状态下特定的行动总是或从不被执行，那么可以认为环境动态是确定性的，即，![](img/B17582_19_006.png)。否则，在更一般的情况下，环境将表现出随机性。
- en: 'To make sense of this stochastic behavior, let’s consider the probability of
    observing the future state *S*[t][+1] = *s*′ conditioned on the current state
    *S*[t] = *s* and the performed action *A*[t] = *a*. This is denoted by:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这种随机行为，让我们考虑在当前状态 *S*[t] = *s* 和执行的动作 *A*[t] = *a* 条件下观察未来状态 *S*[t][+1] =
    *s*′ 的概率。这可以表示为：
- en: '![](img/B17582_19_007.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_007.png)'
- en: 'It can be computed as a marginal probability by taking the sum over all possible
    rewards:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对所有可能的奖励进行求和，可以计算其作为边际概率的值：
- en: '![](img/B17582_19_008.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_008.png)'
- en: This probability is called **state-transition probability**. Based on the state-transition
    probability, if the environment dynamics are deterministic, then it means that
    when the agent takes action *A*[t] = *a* at state *S*[t] = *s*, the transition
    to the next state, *S*[t][+1] = *s*′, will be 100 percent certain, that is, ![](img/B17582_19_009.png).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概率称为**状态转移概率**。基于状态转移概率，如果环境动态是确定性的，那么当智能体在状态 *S*[t] = *s* 采取行动 *A*[t] = *a*
    时，到达下一个状态 *S*[t][+1] = *s*′ 将是100%确定的，即，![](img/B17582_19_009.png)。
- en: Visualization of a Markov process
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫过程的可视化
- en: A Markov process can be represented as a directed cyclic graph in which the
    nodes in the graph represent the different states of the environment. The edges
    of the graph (that is, the connections between the nodes) represent the transition
    probabilities between the states.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫过程可以表示为一个有向循环图，其中图中的节点表示环境的不同状态。图的边（即节点之间的连接）表示状态之间的转移概率。
- en: 'For example, let’s consider a student deciding between three different situations:
    (A) studying for an exam at home, (B) playing video games at home, or (C) studying
    at the library. Furthermore, there is a terminal state (T) for going to sleep.
    The decisions are made every hour, and after making a decision, the student will
    remain in a chosen situation for that particular hour. Then, assume that when
    staying at home (state A), there is a 50 percent likelihood that the student switches
    the activity to playing video games. On the other hand, when the student is at
    state B (playing video games), there is a relatively high chance (80 percent)
    that the student will continue playing video games in the subsequent hours.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个学生在三种不同情况之间做出决定：（A）在家里复习考试，（B）在家里玩视频游戏，或者（C）在图书馆学习。此外，还有一个终止状态（T），即睡觉。每小时做出决定，并且做出决定后，学生将在选择的情况下度过那个特定小时。然后，假设当留在家里（状态A）时，有50%的可能性学生会切换到玩视频游戏。另一方面，当学生处于状态B（玩视频游戏）时，有较高的概率（80%）会在接下来的小时继续玩视频游戏。
- en: 'The dynamics of the student’s behavior is shown as a Markov process in *Figure
    19.3*, which includes a cyclic graph and a transition table:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 学生行为的动态被展示为图19.3中的马尔可夫过程，包括一个循环图和一个转移表：
- en: '![](img/17582_19_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_03.png)'
- en: 'Figure 19.3: The Markov process of the student'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.3：学生的马尔可夫过程
- en: The values on the edges of the graph represent the transition probabilities
    of the student’s behavior, and their values are also shown in the table to the
    right. When considering the rows in the table, please note that the transition
    probabilities coming out of each state (node) always sum to 1.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图中边缘上的值表示学生行为的转移概率，并且它们的值也显示在右侧的表格中。在考虑表中的行时，请注意每个状态（节点）出来的转移概率总是等于1。
- en: Episodic versus continuing tasks
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情节性任务与持续任务
- en: As the agent interacts with the environment, the sequence of observations or
    states forms a trajectory. There are two types of trajectories. If an agent’s
    trajectory can be divided into subparts such that each starts at time *t* = 0
    and ends in a terminal state *S*[T] (at *t* = *T*), the task is called an *episodic
    task*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理与环境互动时，观察或状态的序列形成一个轨迹。有两种类型的轨迹。如果一个代理的轨迹可以分成子部分，每个部分从时间*t* = 0开始，并在终止状态*S*[T]（在时间*t*
    = *T*）结束，那么这个任务被称为*情节性任务*。
- en: On the other hand, if the trajectory is infinitely continuous without a terminal
    state, the task is called a *continuing task*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果轨迹是无限连续的，没有终止状态，那么这个任务被称为*持续任务*。
- en: The task related to a learning agent for the game of chess is an episodic task,
    whereas a cleaning robot that is keeping a house tidy is typically performing
    a continuing task. In this chapter, we only consider episodic tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与国际象棋学习代理相关的任务是一个情节性任务，而保持房屋整洁的清洁机器人通常执行持续任务。在本章中，我们仅考虑情节性任务。
- en: 'In episodic tasks, an **episode** is a sequence or trajectory that an agent
    takes from a starting state, *S*[0], to a terminal state, *S*[T]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在情节任务中，一个**情节**是一个从起始状态*S*[0]到终止状态*S*[T]的序列或轨迹：
- en: '*S*[0], *A*[0], *R*[1], *S*[1], *A*[1], *R*[2], ..., *S*[t], *A*[t], *R*[t][+1], ..., *S*[t][–1], *A*[t][–1], *R*[t], *S*[t]'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*S*[0]、*A*[0]、*R*[1]、*S*[1]、*A*[1]、*R*[2]、...、*S*[t]、*A*[t]、*R*[t][+1]、...、*S*[t][–1]、*A*[t][–1]、*R*[t]、*S*[t]'
- en: 'For the Markov process shown in *Figure 19.3*, which depicts the task of a
    student studying for an exam, we may encounter episodes like the following three
    examples:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图19.3中展示的马尔可夫过程，描述了学生复习考试的任务，我们可能会遇到以下三个示例：
- en: '![](img/B17582_19_010.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_010.png)'
- en: 'RL terminology: return, policy, and value function'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RL术语：返回值、策略和价值函数
- en: Next, let’s define some additional RL-specific terminology that we will need
    for the remainder of this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义本章余下部分所需的一些额外的RL专有术语。
- en: The return
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 返回值
- en: The so-called *return* at time *t* is the cumulated reward obtained from the
    entire duration of an episode. Recall that *R*[t][+1] = *r* is the *immediate
    reward* obtained after performing an action, *A*[t], at time *t*; the *subsequent*
    rewards are *R*[t][+2], *R*[t][+3], and so forth.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的时间*t*时的*返回值*是从整个情节中获得的累积奖励。回想一下，*R*[t][+1] = *r* 是在时间*t*执行动作*A*[t]后获得的*即时奖励*；随后的奖励是*R*[t][+2]、*R*[t][+3]等等。
- en: 'The return at time *t* can then be calculated from the immediate reward as
    well as the subsequent ones, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以根据即时奖励以及随后的奖励计算时间*t*时的返回值，如下所示：
- en: '![](img/B17582_19_011.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_011.png)'
- en: Here, ![](img/B17582_03_064.png) is the *discount factor* in range [0, 1]. The
    parameter ![](img/B17582_03_064.png) indicates how much the future rewards are
    “worth” at the current moment (time *t*). Note that by setting ![](img/B17582_19_014.png),
    we would imply that we do not care about future rewards. In this case, the return
    will be equal to the immediate reward, ignoring the subsequent rewards after *t* + 1,
    and the agent will be short-sighted. On the other hand, if ![](img/B17582_19_015.png),
    the return will be the unweighted sum of all subsequent rewards.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_03_064.png)是范围为[0, 1]的*折现因子*。参数![](img/B17582_03_064.png)表示在当前时刻（时间*t*）未来回报的“价值”。请注意，通过设置![](img/B17582_19_014.png)，我们将意味着我们不关心未来回报。在这种情况下，回报将等于即时奖励，忽略*t*
    + 1之后的后续回报，并且代理将显得短视。另一方面，如果![](img/B17582_19_015.png)，回报将是所有后续回报的未加权和。
- en: 'Moreover, note that the equation for the return can be expressed in a simpler
    way by using *recursion* as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，可以通过使用*递归*将回报的方程简化如下：
- en: '![](img/B17582_19_016.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_016.png)'
- en: This means that the return at time *t* is equal to the immediate reward *r*
    plus the discounted future return at time *t* + 1\. This is a very important property,
    which facilitates the computations of the return.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着时间*t*的回报等于即时奖励*r*加上时间*t* + 1的折现未来回报。这是一个非常重要的属性，有助于计算回报。
- en: '**Intuition behind the discount factor**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**折现因子背后的直觉**'
- en: 'To get an understanding of the discount factor, consider *Figure 19.4*, showing
    the value of earning a $100 bill today compared to earning it in a year from now.
    Under certain economic situations, like inflation, earning this $100 bill right
    now could be worth more than earning it in the future:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解折现因子，请考虑*图19.4*，显示了今天赚取100美元与一年后赚取100美元的价值对比。在某些经济情况下，如通货膨胀，现在赚取这100美元可能比将来赚取它更有价值：
- en: '![](img/17582_19_04.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_04.png)'
- en: 'Figure 19.4: An example of a discount factor based on the value of a $100 bill
    over time'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.4：基于100美元的价值随时间变化的折现因子示例
- en: Therefore, we say that if this bill is worth $100 right now, then it would be
    worth $90 in a year with a discount factor ![](img/B17582_19_017.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，如果这张钞票现在值100美元，那么带有折现因子![](img/B17582_19_017.png)的一年后将值90美元。
- en: Let’s compute the return at different time steps for the episodes in our previous
    student example. Assume ![](img/B17582_19_018.png) and that the only reward given
    is based on the result of the exam (+1 for passing the exam, and –1 for failing
    it). The rewards for intermediate time steps are 0.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算我们先前学生示例中不同时间步骤的回报。假设![](img/B17582_19_018.png)且唯一的奖励基于考试结果（通过考试+1，未通过-1）。中间时间步骤的奖励为0。
- en: '![](img/B17582_19_019.png):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B17582_19_019.png)：'
- en: '![](img/B17582_19_020.png)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_020.png)'
- en: '![](img/B17582_19_021.png)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_19_021.png)'
- en: '![](img/B17582_19_022.png)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_022.png)'
- en: '![](img/B17582_19_023.png)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_023.png)'
- en: '...'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '![](img/B17582_19_024.png)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_024.png)'
- en: '![](img/B17582_19_025.png)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_025.png)'
- en: '![](img/B17582_19_026.png):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B17582_19_026.png)：'
- en: '![](img/B17582_19_027.png)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_027.png)'
- en: '![](img/B17582_19_028.png)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_028.png)'
- en: '...'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '![](img/B17582_19_029.png)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_029.png)'
- en: '![](img/B17582_19_030.png)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_19_030.png)'
- en: We leave the computation of the returns for the third episode as an exercise
    for the reader.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们把第三个情节的回报计算留给读者作为练习。
- en: Policy
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: 'A *policy* typically denoted by ![](img/B17582_19_031.png) is a function that
    determines the next action to take, which can be either deterministic or stochastic
    (that is, the probability for taking the next action). A stochastic policy then
    has a probability distribution over actions that an agent can take at a given
    state:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通常由![](img/B17582_19_031.png)表示的*策略*是一个确定下一步动作的函数，可以是确定性或随机性（即，采取下一步动作的概率）。随机策略则在给定状态下有动作的概率分布：
- en: '![](img/B17582_19_032.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_032.png)'
- en: During the learning process, the policy may change as the agent gains more experience.
    For example, the agent may start from a random policy, where the probability of
    all actions is uniform; meanwhile, the agent will hopefully learn to optimize
    its policy toward reaching the optimal policy. The *optimal policy* ![](img/B17582_19_033.png)
    is the policy that yields the highest return.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，随着代理获得更多经验，策略可能会发生变化。例如，代理可以从随机策略开始，其中所有动作的概率都是均匀的；同时，代理希望学习优化其策略以达到最优策略。*最优策略*
    ![](img/B17582_19_033.png) 是能够获得最高回报的策略。
- en: Value function
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值函数
- en: The *value function*, also referred to as the *state-value function*, measures
    the *goodness* of each state—in other words, how good or bad it is to be in a
    particular state. Note that the criterion for goodness is based on the return.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*值函数*，也称为*状态值函数*，衡量每个状态的*优劣*，换句话说，处于特定状态的好坏程度。请注意，优良性的标准基于回报。'
- en: 'Now, based on the return *G*[t], we define the value function of state *s*
    as the expected return (the average return over all possible episodes) after *following
    policy* ![](img/B17582_19_034.png):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于回报 *G*[t]，我们定义状态 *s* 的值函数为在遵循策略 ![](img/B17582_19_034.png) 后的期望回报（所有可能的情节的平均回报）：
- en: '![](img/B17582_19_035.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_035.png)'
- en: In an actual implementation, we usually estimate the value function using lookup
    tables, so we do not have to recompute it multiple times. (This is the dynamic
    programming aspect.) For example, in practice, when we estimate the value function
    using such tabular methods, we store all the state values in a table denoted by
    *V*(*s*). In a Python implementation, this could be a list or a NumPy array whose
    indices refer to different states; or, it could be a Python dictionary, where
    the dictionary keys map the states to the respective values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，我们通常使用查找表来估算值函数，这样就不必多次重新计算它。（这是动态规划的一个方面。）例如，在实践中，当我们使用这种表格方法估算值函数时，我们将所有状态值存储在一个名为
    *V*(*s*) 的表中。在Python实现中，这可以是一个列表或NumPy数组，其索引引用不同的状态；或者，它可以是一个Python字典，其中字典键将状态映射到相应的值。
- en: Moreover, we can also define a value for each state-action pair, which is called
    the *action-value function* and is denoted by ![](img/B17582_19_036.png). The
    action-value function refers to the expected return *G*[t] when the agent is at
    state *S*[t] = *s* and takes action *A*[t] = *a*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以为每个状态-动作对定义一个值，称为*动作值函数*，用 ![](img/B17582_19_036.png) 表示。动作值函数指的是当代理处于状态
    *S*[t] = *s* 并采取动作 *A*[t] = *a* 时的预期回报 *G*[t]。
- en: 'Extending the definition of the state-value function to state-action pairs,
    we get the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态值函数的定义扩展到状态-动作对，我们得到以下内容：
- en: '![](img/B17582_19_037.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_037.png)'
- en: This is similar to referring to the optimal policy as ![](img/B17582_19_038.png),
    ![](img/B17582_19_039.png), and ![](img/B17582_19_040.png) also denote the optimal
    state-value and action-value functions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于将最优策略称为 ![](img/B17582_19_038.png), ![](img/B17582_19_039.png) 和 ![](img/B17582_19_040.png)，它们也表示最优状态值函数和动作值函数。
- en: Estimating the value function is an essential component of RL methods. We will
    cover different ways of calculating and estimating the state-value function and
    action-value function later in this chapter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 估算值函数是强化学习方法的一个重要组成部分。我们将在本章后面介绍计算和估算状态值函数和动作值函数的不同方法。
- en: '**The difference between the reward, return, and value function**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励、回报和值函数之间的区别**'
- en: The *reward* is a consequence of the agent taking an action given the current
    state of the environment. In other words, the reward is a signal that the agent
    receives when performing an action to transition from one state to the next. However,
    remember that not every action yields a positive or negative reward—think back
    to our chess example, where a positive reward is only received upon winning the
    game, and the reward for all intermediate actions is zero.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*奖励*是代理根据环境当前状态执行动作而获得的后果。换句话说，奖励是代理在执行动作以从一个状态过渡到下一个状态时收到的信号。但请记住，并非每个动作都会产生正面或负面的奖励——回想一下我们的象棋示例，在那里只有赢得比赛才会获得正面奖励，所有中间动作的奖励都为零。'
- en: A state itself has a certain value, which we assign to it, to measure how good
    or bad this state is—this is where the *value function* comes into play. Typically,
    the states with a “high” or “good” value are those states that have a high expected
    *return* and will likely yield a high reward given a particular policy.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个状态本身具有一定的价值，我们将其分配给它，以衡量这个状态是好是坏—这就是*价值函数*发挥作用的地方。通常，具有“高”或“好”价值的状态是那些有高期望*回报*并且可能在特定策略下产生高奖励的状态。
- en: For example, let’s consider a chess-playing computer once more. A positive reward
    may only be given at the end of the game if the computer wins the game. There
    is no (positive) reward if the computer loses the game. Now, imagine the computer
    performs a particular chess move that captures the opponent’s queen without any
    negative consequences for the computer. Since the computer only receives a reward
    for winning the game, it does not get an immediate reward by making this move
    that captures the opponent’s queen. However, the new state (the state of the board
    after capturing the queen) may have a high value, which may yield a reward (if
    the game is won afterward). Intuitively, we can say that the high value associated
    with capturing the opponent’s queen is associated with the fact that capturing
    the queen often results in winning the game—and thus the high expected return,
    or value. However, note that capturing the opponent’s queen does not always lead
    to winning the game; hence, the agent is likely to receive a positive reward,
    but it is not guaranteed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们再次考虑下象棋电脑。如果电脑赢得了比赛，最终才会得到正面奖励。如果电脑输掉比赛，则不会有（正面）奖励。现在，想象一下，电脑执行了一个特定的象棋移动，捕获了对手的皇后，而对电脑没有任何负面影响。由于电脑只有在赢得比赛时才会得到奖励，所以它不会因为捕获对手的皇后而立即获得奖励。然而，新状态（捕获皇后后的棋盘状态）可能具有很高的价值，这可能会产生奖励（如果之后赢得了比赛）。直觉上，我们可以说与捕获对手皇后相关联的高价值与捕获皇后通常导致赢得比赛—因此高期望回报或价值相关。然而，请注意，捕获对手的皇后并不总是导致赢得比赛；因此，代理可能会收到正面奖励，但不能保证。
- en: In short, the return is the weighted sum of rewards for an entire episode, which
    would be equal to the discounted final reward in our chess example (since there
    is only one reward). The value function is the expectation over all possible episodes,
    which basically computes how “valuable” it is, on average, to make a certain move.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，回报是整个情节中奖励的加权和，在我们的象棋示例中，这将等于折现的最终奖励（因为只有一个奖励）。价值函数是所有可能情节的期望，基本上计算某个移动平均来看，做出某个特定移动是多么“有价值”。
- en: Before we move directly ahead into some RL algorithms, let’s briefly go over
    the derivation for the Bellman equation, which we can use to implement the policy
    evaluation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们直接进入一些强化学习算法之前，让我们简要回顾一下贝尔曼方程的推导，这可以用来实现策略评估。
- en: Dynamic programming using the Bellman equation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程的动态规划
- en: The Bellman equation is one of the central elements of many RL algorithms. The
    Bellman equation simplifies the computation of the value function, such that rather
    than summing over multiple time steps, it uses a recursion that is similar to
    the recursion for computing the return.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是许多强化学习算法的核心要素之一。贝尔曼方程简化了价值函数的计算，不需要对多个时间步长进行求和，而是使用类似于计算回报的递归。
- en: 'Based on the recursive equation for the total return ![](img/B17582_19_041.png),
    we can rewrite the value function as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于总回报的递归方程，![](img/B17582_19_041.png)，我们可以将价值函数重写如下：
- en: '![](img/B17582_19_042.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_042.png)'
- en: Notice that the immediate reward *r* is taken out of the expectation since it
    is a constant and known quantity at time *t*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于它是时间 *t* 的常量和已知数量，所以立即奖励 *r* 被从期望中除去。
- en: 'Similarly, for the action-value function, we could write:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于动作值函数，我们可以写成：
- en: '![](img/B17582_19_043.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_043.png)'
- en: 'We can use the environment dynamics to compute the expectation by summing all
    the probabilities of the next state *s*′ and the corresponding rewards *r*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用环境动态来计算期望，通过对下一个状态 *s*′ 的所有可能性和相应的奖励 *r* 的概率进行求和：
- en: '![](img/B17582_19_044.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_044.png)'
- en: 'Now, we can see that expectation of the return, ![](img/B17582_19_045.png),
    is essentially the state-value function ![](img/B17582_19_046.png). So, we can
    write ![](img/B17582_19_047.png) as a function of ![](img/B17582_19_046.png):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到回报的期望，![](img/B17582_19_045.png)，本质上是状态值函数 ![](img/B17582_19_046.png)。因此，我们可以将
    ![](img/B17582_19_047.png) 写成关于 ![](img/B17582_19_046.png) 的函数：
- en: '![](img/B17582_19_049.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_049.png)'
- en: This is called the **Bellman equation**, which relates the value function for
    a state, *s*, to the value function of its subsequent state, *s*′. This greatly
    simplifies the computation of the value function because it eliminates the iterative
    loop along the time axis.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的**贝尔曼方程**，它将一个状态*s*的价值函数与其后续状态*s*'的价值函数相关联。这极大地简化了价值函数的计算，因为它消除了沿时间轴的迭代循环。
- en: Reinforcement learning algorithms
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: In this section, we will cover a series of learning algorithms. We will start
    with dynamic programming, which assumes that the transition dynamics—or the environment
    dynamics, that is, ![](img/B17582_19_050.png)—are known. However, in most RL problems,
    this is not the case. To work around the unknown environment dynamics, RL techniques
    were developed that learn through interacting with the environment. These techniques
    include **Monte Carlo** (**MC**), **temporal difference** (**TD**) learning, and
    the increasingly popular Q-learning and deep Q-learning approaches.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖一系列学习算法。我们将从动态规划开始，它假设转移动态或环境动态即![](img/B17582_19_050.png)是已知的。然而，在大多数RL问题中，这并非如此。为了解决未知的环境动态，开发出了通过与环境交互学习的RL技术。这些技术包括**蒙特卡洛**（**MC**）、**时序差分**（**TD**）学习，以及越来越流行的Q学习和深度Q学习方法。
- en: '*Figure 19.5* describes the course of advancing RL algorithms, from dynamic
    programming to Q-learning:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.5*描述了RL算法的进展过程，从动态规划到Q学习：'
- en: '![](img/17582_19_05.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_05.png)'
- en: 'Figure 19.5: Different types of RL algorithms'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.5：不同类型的RL算法
- en: In the following sections of this chapter, we will step through each of these
    RL algorithms. We will start with dynamic programming, before moving on to MC,
    and finally on to TD and its branches of on-policy **SARSA** (**state–action–reward–state–action**)
    and off-policy Q-learning. We will also move into deep Q-learning while we build
    some practical models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分，我们将逐步介绍每个强化学习算法。我们将从动态规划开始，然后转向MC，最后到TD及其分支，包括基于策略的**SARSA**（**状态-动作-奖励-状态-动作**）和离策略的Q学习。同时，我们还将进入深度Q学习，构建一些实用模型。
- en: Dynamic programming
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态规划
- en: 'In this section, we will focus on solving RL problems under the following assumptions:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于解决以下假设下的RL问题：
- en: We have full knowledge of the environment dynamics; that is, all transition
    probabilities ![](img/B17582_19_051.png)—are known.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对环境动态有全面的了解；也就是说，所有转移概率![](img/B17582_19_051.png)——都是已知的。
- en: The agent’s state has the Markov property, which means that the next action
    and reward depend only on the current state and the choice of action we make at
    this moment or current time step.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理的状态具有马尔可夫性质，这意味着下一个动作和奖励仅依赖于当前状态及我们在此时刻或当前时间步骤所做的动作选择。
- en: The mathematical formulation for RL problems using a **Markov decision process**
    (**MDP**) was introduced earlier in this chapter. If you need a refresher, please
    refer to the section entitled *The mathematical formulation of Markov decision
    processes*, which introduced the formal definition of the value function ![](img/B17582_19_052.png)
    following the policy ![](img/B17582_19_053.png), and the Bellman equation, which
    was derived using the environment dynamics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的数学表述使用**马尔可夫决策过程**（**MDP**）在本章前面已经介绍过。如果您需要复习，请参阅名为*马尔可夫决策过程的数学表述*的部分，该部分介绍了在策略![](img/B17582_19_053.png)下，价值函数的正式定义![](img/B17582_19_052.png)，以及使用环境动态导出的贝尔曼方程。
- en: We should emphasize that dynamic programming is not a practical approach for
    solving RL problems. The problem with using dynamic programming is that it assumes
    full knowledge of the environment dynamics, which is usually unreasonable or impractical
    for most real-world applications. However, from an educational standpoint, dynamic
    programming helps with introducing RL in a simple fashion and motivates the use
    of more advanced and complicated RL algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调，动态规划并不是解决RL问题的实用方法。使用动态规划的问题在于它假设对环境动态有全面了解，而这通常对大多数实际应用来说是不合理或不可行的。然而，从教育的角度来看，动态规划有助于以简单的方式介绍RL，并激发使用更先进和复杂的RL算法。
- en: 'There are two main objectives via the tasks described in the following subsections:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过下面小节描述的任务，有两个主要目标：
- en: Obtain the true state-value function, ![](img/B17582_19_054.png); this task
    is also known as the prediction task and is accomplished with *policy evaluation*.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得真实的状态值函数![](img/B17582_19_054.png)；这项任务也被称为预测任务，并且是通过*策略评估*完成的。
- en: Find the optimal value function, ![](img/B17582_19_055.png), which is accomplished
    via *generalized policy iteration*.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到最优值函数，![](img/B17582_19_055.png)，这是通过*广义策略迭代*完成的。
- en: Policy evaluation – predicting the value function with dynamic programming
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略评估 – 用动态规划预测值函数
- en: 'Based on the Bellman equation, we can compute the value function for an arbitrary
    policy ![](img/B17582_19_034.png) with dynamic programming when the environment
    dynamics are known. For computing this value function, we can adapt an iterative
    solution, where we start from ![](img/B17582_19_057.png), which is initialized
    to zero values for each state. Then, at each iteration *i* + 1, we update the
    values for each state based on the Bellman equation, which, in turn, is based
    on the values of states from a previous iteration, *i*, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝尔曼方程，当环境动态已知时，我们可以用动态规划计算任意策略![](img/B17582_19_034.png)的值函数。为了计算这个值函数，我们可以采用迭代解法，其中我们从![](img/B17582_19_057.png)开始，它对于每个状态都初始化为零值。然后，在每次迭代*i* + 1中，我们根据贝尔曼方程更新每个状态的值，这反过来又基于前一次迭代*i*中状态的值，如下所示：
- en: '![](img/B17582_19_058.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_058.png)'
- en: It can be shown that as the iterations increase to infinity, ![](img/B17582_19_059.png)
    converges to the true state-value function, ![](img/B17582_19_060.png).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，随着迭代次数无限增加，![](img/B17582_19_059.png)收敛到真实的状态值函数![](img/B17582_19_060.png)。
- en: Also, notice here that we do not need to interact with the environment. The
    reason for this is that we already know the environment dynamics accurately. As
    a result, we can leverage this information and estimate the value function easily.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意到这里我们不需要与环境交互。这是因为我们已经准确地了解了环境动态。因此，我们可以利用这些信息并轻松估计值函数。
- en: After computing the value function, an obvious question is how that value function
    can be useful for us if our policy is still a random policy. The answer is that
    we can actually use this computed ![](img/B17582_19_061.png) to improve our policy,
    as we will see next.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算值函数后，一个显而易见的问题是，如果我们的策略仍然是随机策略，那么该值函数如何对我们有用。答案是，我们实际上可以利用这个计算得到的![](img/B17582_19_061.png)来改进我们的策略，接下来我们将看到。
- en: Improving the policy using the estimated value function
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过估计值函数改进策略
- en: 'Now that we have computed the value function ![](img/B17582_19_062.png) by
    following the existing policy, ![](img/B17582_19_063.png), we want to use ![](img/B17582_19_052.png)
    and improve the existing policy, ![](img/B17582_19_065.png). This means that we
    want to find a new policy, ![](img/B17582_19_066.png), that, for each state, *s*,
    following ![](img/B17582_19_066.png), would yield higher or at least equal value
    than using the current policy, ![](img/B17582_19_063.png). In mathematical terms,
    we can express this objective for the improved policy, ![](img/B17582_19_066.png),
    as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过遵循现有的策略![](img/B17582_19_063.png)计算了值函数![](img/B17582_19_062.png)，我们想要使用![](img/B17582_19_052.png)来改进现有的策略![](img/B17582_19_065.png)。这意味着我们想要找到一个新的策略![](img/B17582_19_066.png)，对于每个状态*s*，遵循![](img/B17582_19_066.png)，将产生比使用当前策略![](img/B17582_19_063.png)更高或至少相等的值。用数学术语来说，我们可以表达这个改进策略![](img/B17582_19_066.png)的目标为：
- en: '![](img/B17582_19_070.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_070.png)'
- en: First, recall that a policy, ![](img/B17582_19_063.png), determines the probability
    of choosing each action, *a*, while the agent is at state *s*. Now, in order to
    find ![](img/B17582_19_072.png) that always has a better or equal value for each
    state, we first compute the action-value function, ![](img/B17582_19_073.png),
    for each state, *s*, and action, *a*, based on the computed state value using
    the value function ![](img/B17582_19_074.png). We iterate through all the states,
    and for each state, *s*, we compare the value of the next state, *s*′, that would
    occur if action *a* was selected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回想一下，策略![](img/B17582_19_063.png)决定了在状态*s*时选择每个动作*a*的概率。现在，为了找到一个![](img/B17582_19_072.png)，它在每个状态上都具有更好或至少相等的值，我们首先计算基于值函数![](img/B17582_19_074.png)的每个状态*s*和动作*a*的动作值函数![](img/B17582_19_073.png)。我们遍历所有状态，并且对于每个状态*s*，我们比较如果选择动作*a*会发生的下一个状态*s*′的值。
- en: After we have obtained the highest state value by evaluating all state-action
    pairs via ![](img/B17582_19_075.png), we can compare the corresponding action
    with the action selected by the current policy. If the action suggested by the
    current policy (that is, ![](img/B17582_19_076.png)) is different than the action
    suggested by the action-value function (that is, ![](img/B17582_19_077.png)),
    then we can update the policy by reassigning the probabilities of actions to match
    the action that gives the highest action value, ![](img/B17582_19_078.png). This
    is called the *policy improvement* algorithm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 ![](img/B17582_19_075.png) 评估所有状态-动作对并得到最高状态值之后，我们可以将相应动作与当前政策选择的动作进行比较。如果当前政策建议的动作（即
    ![](img/B17582_19_076.png)）与动作值函数建议的动作（即 ![](img/B17582_19_077.png)）不同，则可以通过重新分配动作的概率来更新政策，以匹配提供最高动作值的动作，即
    ![](img/B17582_19_078.png)。这被称为*政策改进*算法。
- en: Policy iteration
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 政策迭代
- en: Using the policy improvement algorithm described in the previous subsection,
    it can be shown that the policy improvement will strictly yield a better policy,
    unless the current policy is already optimal (which means ![](img/B17582_19_079.png)
    for each ![](img/B17582_19_080.png)). Therefore, if we iteratively perform policy
    evaluation followed by policy improvement, we are guaranteed to find the optimal
    policy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前文描述的政策改进算法可以证明，政策改进将严格产生更好的政策，除非当前政策已经是最优的（这意味着对每个 ![](img/B17582_19_080.png)，都有
    ![](img/B17582_19_079.png)）。因此，如果我们迭代执行政策评估然后执行政策改进，我们将保证找到最优政策。
- en: Note that this technique is referred to as **generalized policy iteration**
    (**GPI**), which is common among many RL methods. We will use the GPI in later
    sections of this chapter for the MC and TD learning methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种技术被称为**广义策略迭代**（**GPI**），在许多强化学习方法中很常见。我们将在本章的后续部分中使用 GPI 进行 MC 和 TD 学习方法。
- en: Value iteration
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'We saw that by repeating the policy evaluation (compute ![](img/B17582_19_074.png)
    and ![](img/B17582_19_082.png)) and policy improvement (finding ![](img/B17582_19_083.png)
    such that ![](img/B17582_19_084.png)), we can reach the optimal policy. However,
    it can be more efficient if we combine the two tasks of policy evaluation and
    policy improvement into a single step. The following equation updates the value
    function for iteration *i* + 1 (denoted by ![](img/B17582_19_085.png)) based on
    the action that maximizes the weighted sum of the next state value and its immediate
    reward (![](img/B17582_19_086.png)):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复政策评估（计算 ![](img/B17582_19_074.png) 和 ![](img/B17582_19_082.png)）和政策改进（找到
    ![](img/B17582_19_083.png) 以使 ![](img/B17582_19_084.png)），我们可以得到最优政策。然而，如果将政策评估和政策改进两个任务合并为一步会更加高效。以下方程根据最大化下一个状态值及其即时奖励的加权和更新了第
    *i* + 1 次迭代的值函数（表示为 ![](img/B17582_19_085.png)）：
- en: '![](img/B17582_19_087.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_087.png)'
- en: In this case, the updated value for ![](img/B17582_19_088.png) is maximized
    by choosing the best action out of all possible actions, whereas in policy evaluation,
    the updated value was using the weighted sum over all actions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，通过选择所有可能动作中的最佳动作来最大化 ![](img/B17582_19_088.png) 的更新值，而在政策评估中，更新值是通过所有动作的加权和来计算的。
- en: '**Notation for tabular estimates of the state-value and action-value functions**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态值和动作值函数的表格估计符号**'
- en: In most RL literature and textbooks, the lowercase ![](img/B17582_19_089.png)
    and ![](img/B17582_19_090.png) are used to refer to the true state-value and true
    action-value functions, respectively, as mathematical functions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数强化学习文献和教科书中，小写字母 ![](img/B17582_19_089.png) 和 ![](img/B17582_19_090.png)
    被用来指代真实的状态值函数和动作值函数，作为数学函数。
- en: Meanwhile, for practical implementations, these value functions are defined
    as lookup tables. The tabular estimates of these value functions are denoted by
    ![](img/B17582_19_091.png) and ![](img/B17582_19_092.png). We will also use this
    notation in this chapter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，对于实际实现，这些值函数被定义为查找表。这些值函数的表格估计表示为 ![](img/B17582_19_091.png) 和 ![](img/B17582_19_092.png)。在本章节中我们也将使用这种符号。
- en: Reinforcement learning with Monte Carlo
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛强化学习
- en: As we saw in the previous section, dynamic programming relies on a simplistic
    assumption that the environment’s dynamics are fully known. Moving away from the
    dynamic programming approach, we now assume that we do not have any knowledge
    about the environment dynamics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: That is, we do not know the state-transition probabilities of the environment,
    and instead, we want the agent to learn through *interacting* with the environment.
    Using MC methods, the learning process is based on the so-called *simulated experience*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: For MC-based RL, we define an agent class that follows a probabilistic policy,
    ![](img/B17582_19_034.png), and based on this policy, our agent takes an action
    at each step. This results in a simulated episode.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we defined the state-value function, such that the value of a state
    indicates the expected return from that state. In dynamic programming, this computation
    relied on the knowledge of the environment dynamics, that is, ![](img/B17582_19_094.png).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: However, from now on, we will develop algorithms that do not require the environment
    dynamics. MC-based methods solve this problem by generating simulated episodes
    where an agent interacts with the environment. From these simulated episodes,
    we will be able to compute the average return for each state visited in that simulated
    episode.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: State-value function estimation using MC
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After generating a set of episodes, for each state, *s*, the set of episodes
    that all pass through state *s* is considered for calculating the value of state
    *s*. Let’s assume that a lookup table is used for obtaining the value corresponding
    to the value function, ![](img/B17582_19_095.png). MC updates for estimating the
    value function are based on the total return obtained in that episode starting
    from the first time that state *s* is visited. This algorithm is called *first-visit
    Monte Carlo* value prediction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Action-value function estimation using MC
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the environment dynamics are known, we can easily infer the action-value
    function from a state-value function by looking one step ahead to find the action
    that gives the maximum value, as was shown in the *Dynamic programming* section.
    However, this is not feasible if the environment dynamics are unknown.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: To solve this issue, we can extend the algorithm for estimating the first-visit
    MC state-value prediction. For instance, we can compute the *estimated* return
    for each state-action pair using the action-value function. To obtain this estimated
    return, we consider visits to each state-action pair (*s*, *a*), which refers
    to visiting state *s* and taking action *a*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: However, a problem arises since some actions may never be selected, resulting
    in insufficient exploration. There are a few ways to resolve this. The simplest
    approach is called *exploratory start*, which assumes that every state-action
    pair has a non-zero probability at the beginning of the episode.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Another approach for dealing with this lack-of-exploration issue is called the
    ![](img/B17582_19_096.png)-*greedy policy*, which will be discussed in the next
    section on policy improvement.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 处理探索不足问题的另一种方法称为 ![](img/B17582_19_096.png)-贪婪策略，将在下一节关于策略改进中讨论。
- en: Finding an optimal policy using MC control
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MC 控制找到最优策略
- en: '*MC control* refers to the optimization procedure for improving a policy. Similar
    to the policy iteration approach in the previous section (*Dynamic programming*),
    we can repeatedly alternate between policy evaluation and policy improvement until
    we reach the optimal policy. So, starting from a random policy, ![](img/B17582_19_097.png),
    the process of alternating between policy evaluation and policy improvement can
    be illustrated as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*MC 控制* 指的是改进策略的优化过程。与上一节的策略迭代方法（*动态规划*）类似，我们可以反复进行策略评估和策略改进，直到达到最优策略。因此，从随机策略开始，![](img/B17582_19_097.png)，交替进行策略评估和策略改进的过程可以如下所示：'
- en: '![](img/B17582_19_098.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_098.png)'
- en: Policy improvement – computing the greedy policy from the action-value function
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略改进 – 从动作值函数计算贪婪策略
- en: 'Given an action-value function, *q*(*s*, *a*), we can generate a greedy (deterministic)
    policy as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 给定动作值函数 *q*(*s*, *a*)，我们可以按以下方式生成贪婪（确定性）策略：
- en: '![](img/B17582_19_099.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_099.png)'
- en: To avoid the lack-of-exploration problem, and to consider the non-visited state-action
    pairs as discussed earlier, we can let the non-optimal actions have a small chance
    (![](img/B17582_19_096.png)) to be chosen. This is called the ![](img/B17582_19_096.png)-greedy
    policy, according to which, all non-optimal actions at state *s* have a minimal
    ![](img/B17582_19_102.png) probability of being selected (instead of 0), and the
    optimal action has a probability of ![](img/B17582_19_103.png) (instead of 1).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免探索不足问题，并考虑之前讨论过的未访问状态-动作对，我们可以让非最优动作有一定几率 (![](img/B17582_19_096.png)) 被选择。这被称为
    ![](img/B17582_19_096.png)-贪婪策略，根据这个策略，状态 *s* 下所有非最优动作的选择概率最小为 ![](img/B17582_19_102.png)（而非0），最优动作的选择概率为
    ![](img/B17582_19_103.png)（而非1）。
- en: Temporal difference learning
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间差分学习
- en: So far, we have seen two fundamental RL techniques—dynamic programming and MC-based
    learning. Recall that dynamic programming relies on the complete and accurate
    knowledge of the environment dynamics. The MC-based method, on the other hand,
    learns by simulated experience. In this section, we will now introduce a third
    RL method called *TD learning*, which can be considered as an improvement or extension
    of the MC-based RL approach.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了两种基本的强化学习技术—动态规划和基于蒙特卡洛的学习。回想一下，动态规划依赖于对环境动态的完整和准确的知识。另一方面，基于蒙特卡洛的方法通过模拟体验学习。在本节中，我们现在将介绍第三种强化学习方法称为
    *TD 学习*，它可以被视为基于蒙特卡洛的强化学习方法的改进或扩展。
- en: Similar to the MC technique, TD learning is also based on learning by experience
    and, therefore, does not require any knowledge of environment dynamics and transition
    probabilities. The main difference between the TD and MC techniques is that in
    MC, we have to wait until the end of the episode to be able to calculate the total
    return.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 MC 技术，TD 学习也是基于经验学习的，因此不需要了解环境动态和转移概率。TD 和 MC 技术的主要区别在于，在 MC 中，我们必须等到回合结束才能计算总回报。
- en: However, in TD learning, we can leverage some of the learned properties to update
    the estimated values before reaching the end of the episode. This is called *bootstrapping*
    (in the context of RL, the term *bootstrapping* is not to be confused with the
    bootstrap estimates we used in *Chapter 7*, *Combining Different Models for Ensemble
    Learning*).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 TD 学习中，我们可以利用一些学到的属性在到达该回合结束之前更新估计值。这被称为 *自举*（在强化学习的背景下，自举这一术语与我们在 *第7章*
    *集成学习* 中使用的自举估计方法不同）。
- en: 'Similar to the dynamic programming approach and MC-based learning, we will
    consider two tasks: estimating the value function (which is also called value
    prediction) and improving the policy (which is also called the control task).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于动态规划方法和基于蒙特卡洛的学习，我们将考虑两个任务：估计值函数（也称为值预测）和改进策略（也称为控制任务）。
- en: TD prediction
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TD 预测
- en: 'Let’s first revisit the value prediction by MC. At the end of each episode,
    we are able to estimate the return, *G*[t], for each time step *t*. Therefore,
    we can update our estimates for the visited states as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_104.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Here, *G*[t] is used as the *target return* to update the estimated values,
    and (*G*[t] – *V*(*S*[t])) is a *correction* term added to our current estimate
    of the value *V*(*S*[t]). The value ![](img/B17582_15_030.png) is a hyperparameter
    denoting the learning rate, which is kept constant during learning.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in MC, the correction term uses the *actual* return, *G*[t], which
    is not known until the end of the episode. To clarify this, we can rename the
    actual return, *G*[t], to *G*[t][:][T], where the subscript *t*:*T* indicates
    that this is the return obtained at time step *t* while considering all the events
    that occurred from time step *t* until the final time step, *T*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'In TD learning, we replace the actual return, *G*[t][:][T], with a new target
    return, *G*[t][:][t][+1], which significantly simplifies the updates for the value
    function, *V*(*S*[t]). The update formula based on TD learning is as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_106.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Here, the target return, ![](img/B17582_19_107.png), is using the observed reward,
    *R*[t][+1], and the estimated value of the next immediate step. Notice the difference
    between MC and TD. In MC, *G*[t][:][T] is not available until the end of the episode,
    so we should execute as many steps as needed to get there. On the contrary, in
    TD, we only need to go one step ahead to get the target return. This is also known
    as TD(0).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the TD(0) algorithm can be generalized to the so-called *n-step
    TD* algorithm, which incorporates more future steps—more precisely, the weighted
    sum of *n* future steps. If we define *n* = 1, then the n-step TD procedure is
    identical to TD(0), which was described in the previous paragraph. If ![](img/B17582_19_108.png),
    however, the n-step TD algorithm will be the same as the MC algorithm. The update
    rule for n-step TD is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_109.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'And *G*[t][:][t][+][n] is defined as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_19_110.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: '**MC versus TD: which method converges faster?**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'While the precise answer to this question is still unknown, in practice, it
    is empirically shown that TD can converge faster than MC. If you are interested,
    you can find more details on the convergences of MC and TD in the book entitled
    *Reinforcement Learning: An Introduction*, by *Richard S. Sutton* and *Andrew
    G. Barto*.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have covered the prediction task using the TD algorithm, we can
    move on to the control task. We will cover two algorithms for TD control: an *on-policy*
    control and an *off-policy* control. In both cases, we use the GPI that was used
    in both the dynamic programming and MC algorithms. In on-policy TD control, the
    value function is updated based on the actions from the same policy that the agent
    is following; while in an off-policy algorithm, the value function is updated
    based on actions outside the current policy.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了使用TD算法进行预测任务，我们可以继续进行控制任务。我们将涵盖两种TD控制算法：一种是*在策略*控制，另一种是*离策略*控制。在两种情况下，我们使用了GPI，这在动态规划和MC算法中都有使用过。在在策略TD控制中，根据代理正在遵循的策略更新值函数；而在离策略算法中，值函数是根据当前策略之外的动作进行更新的。
- en: On-policy TD control (SARSA)
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在策略TD控制（SARSA）
- en: 'For simplicity, we only consider the one-step TD algorithm, or TD(0). However,
    the on-policy TD control algorithm can be readily generalized to *n*-step TD.
    We will start by extending the prediction formula for defining the state-value
    function to describe the action-value function. To do this, we use a lookup table,
    that is, a tabular 2D array, *Q*(*S*[t], *A*[t]), which represents the action-value
    function for each state-action pair. In this case, we will have the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们仅考虑一步TD算法，或者TD(0)。然而，在策略TD控制算法可以轻松推广到*n*-步TD。我们将从扩展预测公式开始，以描述状态值函数，进而描述动作值函数。为此，我们使用一个查找表，即一个二维表格，*Q*(*S*[t], *A*[t])，它表示每个状态-动作对的动作值函数。在这种情况下，我们将会有以下情况：
- en: '![](img/B17582_19_111.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_111.png)'
- en: This algorithm is often called SARSA, referring to the quintuple (*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1])
    that is used in the update formula.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法通常称为SARSA，指的是用于更新公式的五元组（*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1]）。
- en: As we saw in the previous sections describing the dynamic programming and MC
    algorithms, we can use the GPI framework, and starting from the random policy,
    we can repeatedly estimate the action-value function for the current policy and
    then optimize the policy using the ![](img/B17582_19_112.png)-greedy policy based
    on the current action-value function.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在描述动态规划和MC算法的前几节中看到的那样，我们可以使用GPI框架，从随机策略开始，重复估计当前策略的动作值函数，然后使用基于当前动作值函数的![](img/B17582_19_112.png)-贪婪策略来优化策略。
- en: Off-policy TD control (Q-learning)
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离策略TD控制（Q学习）
- en: We saw when using the previous on-policy TD control algorithm that how we estimate
    the action-value function is based on the policy that is used in the simulated
    episode. After updating the action-value function, a separate step for policy
    improvement is performed by taking the action that has the higher value.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，当使用前述的在策略TD控制算法时，如何估计动作值函数是基于模拟回合中使用的策略的。在更新动作值函数后，通过采取值更高的动作进行策略改进的一个独立步骤也会被执行。
- en: An alternative (and better) approach is to combine these two steps. In other
    words, imagine the agent is following policy ![](img/B17582_19_063.png), generating
    an episode with the current transition quintuple (*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1]).
    Instead of updating the action-value function using the action value of *A*[t][+1]
    that is taken by the agent, we can find the best action even if it is not actually
    chosen by the agent following the current policy. (That’s why this is considered
    an *off-policy* algorithm.)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种（更好的）方法是将这两个步骤结合起来。换句话说，想象一下，代理正在遵循策略![](img/B17582_19_063.png)，生成一个包含当前转移五元组（*S*[t], *A*[t], *R*[t][+1], *S*[t][+1], *A*[t][+1]）的回合。与其使用代理实际选择的*A*[t][+1]的动作值来更新动作值函数，我们可以找到最佳动作，即使该动作并未由当前策略选择的代理实际执行。（这就是为什么这被认为是一个离策略算法。）
- en: 'To do this, we can modify the update rule to consider the maximum Q-value by
    varying different actions in the next immediate state. The modified equation for
    updating the Q-values is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以修改更新规则，以考虑在下一个即时状态中通过不同动作变化的最大Q值。更新Q值的修改方程如下：
- en: '![](img/B17582_19_114.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_19_114.png)'
- en: We encourage you to compare the update rule here with that of the SARSA algorithm.
    As you can see, we find the best action in the next state, *S*[t][+1], and use
    that in the correction term to update our estimate of *Q*(*S*[t], *A*[t]).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您比较这里的更新规则与SARSA算法的更新规则。正如您所见，我们在下一个状态*S*[t][+1]中找到最佳动作，并将其用于修正项，以更新我们对*Q*(*S*[t], *A*[t])的估计。
- en: To put these materials into perspective, in the next section, we will see how
    to implement the Q-learning algorithm for solving a *grid world problem*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这些材料更具体，下一节我们将看看如何实现 Q-learning 算法来解决*网格世界问题*。
- en: Implementing our first RL algorithm
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的第一个 RL 算法
- en: In this section, we will cover the implementation of the Q-learning algorithm
    to solve a *grid world problem* (a grid world is a two-dimensional, cell-based
    environment where the agent moves in four directions to collect as much reward
    as possible). To do this, we use the OpenAI Gym toolkit.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍实现 Q-learning 算法来解决*网格世界问题*（网格世界是一个二维的、基于单元格的环境，代理在其中以四个方向移动以尽可能多地收集奖励）。为此，我们使用
    OpenAI Gym 工具包。
- en: Introducing the OpenAI Gym toolkit
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入 OpenAI Gym 工具包
- en: 'OpenAI Gym is a specialized toolkit for facilitating the development of RL
    models. OpenAI Gym comes with several predefined environments. Some basic examples
    are CartPole and MountainCar, where the tasks are to balance a pole and to move
    a car up a hill, respectively, as the names suggest. There are also many advanced
    robotics environments for training a robot to fetch, push, and reach for items
    on a bench or training a robotic hand to orient blocks, balls, or pens. Moreover,
    OpenAI Gym provides a convenient, unified framework for developing new environments.
    More information can be found on its official website: [https://gym.openai.com/](https://gym.openai.com/).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个专门用于促进 RL 模型开发的工具包。OpenAI Gym 预装了几个预定义的环境。一些基本示例是 CartPole 和 MountainCar，如它们的名称所示，任务是平衡杆和将车开上山。还有许多用于训练机器人从桌子上获取、推动和抓取物品或训练机器手操作方块、球或笔的高级机器人学环境。此外，OpenAI
    Gym 提供了一个便利的统一框架来开发新的环境。更多信息可以在其官方网站上找到：[https://gym.openai.com/](https://gym.openai.com/)。
- en: 'To follow the OpenAI Gym code examples in the next sections, you need to install
    the `gym` library (at the time of writing, version 0.20.0 was used), which can
    be easily done using `pip`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要在接下来的章节中使用 OpenAI Gym 的代码示例，您需要安装`gym`库（在撰写本文时使用的是版本0.20.0），可以通过`pip`轻松完成：
- en: '[PRE0]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you need additional help with the installation, please refer to the official
    installation guide at [https://gym.openai.com/docs/#installation](https://gym.openai.com/docs/#installation).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要安装方面的额外帮助，请参考官方安装指南：[https://gym.openai.com/docs/#installation](https://gym.openai.com/docs/#installation)。
- en: Working with the existing environments in OpenAI Gym
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用现有的 OpenAI Gym 环境
- en: 'For practice with the Gym environments, let’s create an environment from `CartPole-v1`,
    which already exists in OpenAI Gym. In this example environment, there is a pole
    attached to a cart that can move horizontally, as shown in *Figure 19.6*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习 Gym 环境，让我们从`CartPole-v1`中创建一个环境，这个环境已经存在于 OpenAI Gym 中。在这个示例环境中，有一个杆连接到一个可以水平移动的小车，如*图
    19.6*所示：
- en: '![](img/17582_19_06.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_06.png)'
- en: 'Figure 19.6: The CartPole example in Gym'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.6：Gym 中的 CartPole 示例
- en: The movements of the pole are governed by the laws of physics, and the goal
    for RL agents is to learn how to move the cart to stabilize the pole and prevent
    it from tipping over to either side.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 杆的运动受物理定律控制，RL 代理的目标是学会如何移动小车以稳定杆，并防止其向任一方向倾斜。
- en: 'Now, let’s look at some properties of the CartPole environment in the context
    of RL, such as its state (or observation) space, action space, and how to execute
    an action:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 RL 的背景下看一些 CartPole 环境的属性，例如其状态（或观察）空间、动作空间以及如何执行动作：
- en: '[PRE1]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we created an environment for the CartPole problem.
    The observation space for this environment is `Box(4,)` (with float values from
    `-inf` to `inf`), which represents a four-dimensional space corresponding to four
    real-valued numbers: the position of the cart, the cart’s velocity, the angle
    of the pole, and the velocity of the tip of the pole. The action space is a discrete
    space, `Discrete(2)`, with two choices: pushing the cart either to the left or
    to the right.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们为 CartPole 问题创建了一个环境。该环境的观察空间是`Box(4,)`（具有从`-inf`到`inf`的浮点值），表示四维空间对应四个实数：小车的位置、小车的速度、杆的角度和杆顶的速度。动作空间是离散空间，`Discrete(2)`，有两种选择：将小车向左推或向右推。
- en: 'The environment object, `env`, that we previously created by calling `gym.make(''CartPole-v1'')`
    has a `reset()` method that we can use to reinitialize an environment prior to
    each episode. Calling the `reset()` method will basically set the pole’s starting
    state (*S*[0]):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The values in the array returned by the `env.reset()` method call mean that
    the initial position of the cart is –0.039, with a velocity –0.008, and the angle
    of the pole is 0.033 radians, while the angular velocity of its tip is –0.021\.
    Upon calling the `reset()` method, these values are initialized with random values
    with uniform distribution in the range [–0.05, 0.05].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'After resetting the environment, we can interact with the environment by choosing
    an action and executing it by passing the action to the `step()` method:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Via the previous two commands, `env.step(action=0)` and `env.step(action=1)`,
    we pushed the cart to the left (`action=0`) and then to the right (`action=1`),
    respectively. Based on the selected action, the cart and its pole can move as
    governed by the laws of physics. Every time we call `env.step()`, it returns a
    tuple consisting of four elements:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: An array for the new state (or observations)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reward (a scalar value of type `float`)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A termination flag (`True` or `False`)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python dictionary containing auxiliary information
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `env` object also has a `render()` method, which we can execute after each
    step (or a series of steps) to visualize the environment and the movements of
    the pole and cart, through time.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The episode terminates when the angle of the pole becomes larger than 12 degrees
    (from either side) with respect to an imaginary vertical axis, or when the position
    of the cart is more than 2.4 units from the center position. The reward defined
    in this example is to maximize the time the cart and pole are stabilized within
    the valid regions—in other words, the total reward (that is, return) can be maximized
    by maximizing the length of the episode.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: A grid world example
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After introducing the CartPole environment as a warm-up exercise for working
    with the OpenAI Gym toolkit, we will now switch to a different environment. We
    will work with a grid world example, which is a simplistic environment with *m*
    rows and *n* columns. Considering *m* = 5 and *n* = 6, we can summarize this environment
    as shown in *Figure 19.7*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_07.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.7: An example of a grid world environment'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'In this environment, there are 30 different possible states. Four of these
    states are terminal states: a pot of gold at state 16 and three traps at states
    10, 15, and 22\. Landing in any of these four terminal states will end the episode,
    but with a difference between the gold and trap states. Landing on the gold state
    yields a positive reward, +1, whereas moving the agent onto one of the trap states
    yields a negative reward, –1\. All other states have a reward of 0\. The agent
    always starts from state 0\. Therefore, every time we reset the environment, the
    agent will go back to state 0\. The action space consists of four directions:
    move up, down, left, and right.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: When the agent is at the outer boundary of the grid, selecting an action that
    would result in leaving the grid will not change the state.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how to implement this environment in Python using the OpenAI
    Gym package.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the grid world environment in OpenAI Gym
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For experimenting with the grid world environment via OpenAI Gym, using a script
    editor or IDE rather than executing the code interactively is highly recommended.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a new Python script named `gridworld_env.py` and then proceed
    by importing the necessary packages and two helper functions that we define for
    building the visualization of the environment.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: To render the environments for visualization purposes, the OpenAI Gym library
    uses the pyglet library and provides wrapper classes and functions for our convenience.
    We will use these wrapper classes for visualizing the grid world environment in
    the following code example. More details about these wrapper classes can be found
    at [https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/envs/classic_control/rendering.py](https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/envs/classic_control/rendering.py).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example uses those wrapper classes:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Using Gym 0.22 or newer**'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Note that `gym` is currently undergoing some internal restructuring. In version
    0.22 and newer, you may have to update the previous code example (from `gridworld_env.py`)
    and replace the following line
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'with the following code:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For more details, please refer to the code repository at [https://github.com/rasbt/machine-learning-book/tree/main/ch19](https://github.com/rasbt/machine-learning-book/tree/main/ch19)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The first helper function, `get_coords()`, returns the coordinates of the geometric
    shapes that we will use to annotate the grid world environment, such as a triangle
    to display the gold or circles to display the traps. The list of coordinates is
    passed to `draw_object()`, which decides to draw a circle, a triangle, or a polygon
    based on the length of the input list of coordinates.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the grid world environment. In the same file (`gridworld_env.py`),
    we define a class named `GridWorldEnv`, which inherits from OpenAI Gym’s `DiscreteEnv`
    class. The most important function of this class is the constructor method, `__init__()`,
    where we define the action space, specify the role of each action, and determine
    the terminal states (gold as well as traps) as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义网格世界环境。在同一个文件（`gridworld_env.py`）中，我们定义了一个名为`GridWorldEnv`的类，该类继承自OpenAI
    Gym的`DiscreteEnv`类。该类最重要的函数是构造方法`__init__()`，在这里我们定义了动作空间，指定了每个动作的作用，并确定了终止状态（金色以及陷阱）如下：
- en: '[PRE7]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code implements the grid world environment, from which we can create instances
    of this environment. We can then interact with it in a manner similar to that
    in the CartPole example. The implemented class, `GridWorldEnv`, inherits methods
    such as `reset()` for resetting the state and `step()` for executing an action.
    The details of the implementation are as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码实现了网格世界环境，我们可以从中创建环境的实例。然后，我们可以像在CartPole示例中那样与其进行交互。实现的`GridWorldEnv`类继承了诸如`reset()`用于重置状态和`step()`用于执行动作等方法。实现的细节如下：
- en: 'We defined the four different actions using lambda functions: `move_up()`,
    `move_down()`, `move_left()`, and `move_right()`.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用lambda函数定义了四种不同的动作：`move_up()`，`move_down()`，`move_left()`和`move_right()`。
- en: The NumPy array `isd` holds the probabilities of the starting states so that
    a random state will be selected based on this distribution when the `reset()`
    method (from the parent class) is called. Since we always start from state 0 (the
    lower-left corner of the grid world), we set the probability of state 0 to 1.0
    and the probabilities of all other 29 states to 0.0.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy数组`isd`保存了起始状态的概率，因此当调用`reset()`方法（来自父类）时，将根据这个分布随机选择一个起始状态。由于我们始终从状态0开始（网格世界的左下角），我们将状态0的概率设置为1.0，将其他29个状态的概率设置为0.0。
- en: The transition probabilities, defined in the Python dictionary `P` determine
    the probabilities of moving from one state to another state when an action is
    selected. This allows us to have a probabilistic environment where taking an action
    could have different outcomes based on the stochasticity of the environment. For
    simplicity, we just use a single outcome, which is to change the state in the
    direction of the selected action. Finally, these transition probabilities will
    be used by the `env.step()` function to determine the next state.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python字典`P`中定义的过渡概率确定了在选择动作时从一个状态移动到另一个状态的概率。这使我们可以拥有一个概率性环境，其中采取动作可能基于环境的随机性具有不同的结果。为简单起见，我们只使用单一结果，即沿着选择的动作方向改变状态。最后，这些过渡概率将被`env.step()`函数用于确定下一个状态。
- en: Furthermore, the `_build_display()` function will set up the initial visualization
    of the environment, and the `render()` function will show the movements of the
    agent.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，`_build_display()`函数将设置环境的初始可视化，而`render()`函数将显示代理的移动。
- en: Note that during the learning process, we do not know about the transition probabilities,
    and the goal is to learn by interacting with the environment. Therefore, we do
    not have access to `P` outside the class definition.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在学习过程中，我们不知道过渡概率，并且目标是通过与环境的交互进行学习。因此，在类定义之外，我们无法访问`P`。
- en: 'Now, we can test this implementation by creating a new environment and visualizing
    a random episode by taking random actions at each state. Include the following
    code at the end of the same Python script (`gridworld_env.py`) and then execute
    the script:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过创建一个新的环境并在每个状态采取随机动作来可视化一个随机回合来测试这个实现。将以下代码包含在同一Python脚本（`gridworld_env.py`）的末尾，然后执行脚本：
- en: '[PRE8]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the script, you should see a visualization of the grid world
    environment as depicted in *Figure 19.8*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 执行脚本后，您应该能看到网格世界环境的可视化，如*图19.8*所示：
- en: '![Une image contenant capture d’écran  Description générée automatiquement](img/17582_19_08.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant capture d’écran  Description générée automatiquement](img/17582_19_08.png)'
- en: 'Figure 19.8: A visualization of our grid world environment'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.8：我们网格世界环境的可视化
- en: Solving the grid world problem with Q-learning
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Q学习解决网格世界问题
- en: After focusing on the theory and the development process of RL algorithms, as
    well as setting up the environment via the OpenAI Gym toolkit, we will now implement
    the currently most popular RL algorithm, Q-learning. For this, we will use the
    grid world example that we already implemented in the script `gridworld_env.py`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在专注于强化学习算法的理论和开发过程，并通过OpenAI Gym工具包设置环境之后，我们现在将实现当前最流行的强化学习算法，Q-learning。为此，我们将使用已在脚本`gridworld_env.py`中实现的网格世界示例。
- en: 'Now, we create a new script and name it `agent.py`. In this `agent.py` script,
    we define an agent for interacting with the environment as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个新的脚本并命名为`agent.py`。在这个`agent.py`脚本中，我们定义了一个与环境交互的代理如下：
- en: '[PRE9]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `__init__()` constructor sets up various hyperparameters, such as the learning
    rate, discount factor (![](img/B17582_03_056.png)), and the parameters for the
    ![](img/B17582_19_096.png)-greedy policy. Initially, we start with a high value
    of ![](img/B17582_19_096.png), but the `_adjust_epsilon()` method reduces it until
    it reaches the minimum value, ![](img/B17582_19_118.png). The `choose_action()`
    method chooses an action based on the ![](img/B17582_19_119.png)-greedy policy
    as follows. A random uniform number is selected to determine whether the action
    should be selected randomly or otherwise, based on the action-value function.
    The `_learn()` method implements the update rule for the Q-learning algorithm.
    It receives a tuple for each transition, which consists of the current state (*s*),
    selected action (*a*), observed reward (*r*), next state (*s*′), as well as a
    flag to determine whether the end of the episode has been reached. The target
    value is equal to the observed reward (*r*) if this is flagged as end-of-episode;
    otherwise, the target is ![](img/B17582_19_120.png).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__()`构造函数设置了各种超参数，如学习率、折扣因子（![](img/B17582_03_056.png)）、以及![](img/B17582_19_096.png)-贪婪策略的参数。最初，我们从较高的![](img/B17582_19_096.png)值开始，但`_adjust_epsilon()`方法会将其减少直至达到最小值![](img/B17582_19_118.png)。`choose_action()`方法根据![](img/B17582_19_119.png)-贪婪策略选择动作如下。通过选择随机均匀数确定动作是否应该随机选择，否则基于动作值函数选择。`_learn()`方法实现了Q-learning算法的更新规则。它为每个转换接收一个元组，其中包括当前状态(*s*)、选择的动作(*a*)、观察到的奖励(*r*)、下一个状态(*s*′)，以及一个标志，用于确定是否已到达一集的结束。如果标记为一集的结束，则目标值等于观察到的奖励(*r*)；否则目标值为![](img/B17582_19_120.png)。'
- en: Finally, for our next step, we create a new script, `qlearning.py`, to put everything
    together and train the agent using the Q-learning algorithm.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为我们的下一步，我们创建一个新的脚本，名为`qlearning.py`，将所有内容整合在一起，并使用Q-learning算法训练代理。
- en: In the following code, we define a function, `run_qlearning()`, that implements
    the Q-learning algorithm, simulating an episode by calling the `_choose_action()`
    method of the agent and executing the environment. Then, the transition tuple
    is passed to the `_learn()` method of the agent to update the action-value function.
    In addition, for monitoring the learning process, we also store the final reward
    of each episode (which could be –1 or +1), as well as the length of episodes (the
    number of moves taken by the agent from the start of the episode until the end).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们定义了一个函数`run_qlearning()`，实现了Q-learning算法，通过调用代理的`_choose_action()`方法模拟一个回合，并执行环境。然后，将转换元组传递给代理的`_learn()`方法以更新动作值函数。此外，为了监控学习过程，我们还存储了每个回合的最终奖励（可能为-1或+1），以及回合的长度（代理从开始到结束所采取的步数）。
- en: 'The list of rewards and the number of moves is then plotted using the `plot_learning_history()`
    function:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励列表和步数将使用`plot_learning_history()`函数绘制：
- en: '[PRE10]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Executing this script will run the Q-learning program for 50 episodes. The
    behavior of the agent will be visualized, and you can see that at the beginning
    of the learning process, the agent mostly ends up in the trap states. But over
    time, it learns from its failures and eventually finds the gold state (for instance,
    the first time in episode 7). *Figure 19.9* shows the agent’s number of moves
    and rewards:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此脚本将运行50个回合的Q-learning程序。代理的行为将被可视化，您可以看到在学习过程的开始阶段，代理大部分时间都会陷入陷阱状态。但随着时间的推移，它从失败中学习，最终找到黄金状态（例如，在第7个回合第一次）。*图19.9*显示了代理的步数和奖励：
- en: '![](img/17582_19_09.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_09.png)'
- en: 'Figure 19.9: The agent’s number of moves and rewards'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.9：代理的步数和奖励
- en: The plotted learning history shown in the previous figure indicates that the
    agent, after 30 episodes, learns a short path to get to the gold state. As a result,
    the lengths of the episodes after the 30th episode are more or less the same,
    with minor deviations due to the ![](img/B17582_19_112.png)-greedy policy.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: A glance at deep Q-learning
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous code, we saw an implementation of the popular Q-learning algorithm
    for the grid world example. This example consisted of a discrete state space of
    size 30, where it was sufficient to store the Q-values in a Python dictionary.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: However, we should note that sometimes the number of states can get very large,
    possibly almost infinitely large. Also, we may be dealing with a continuous state
    space instead of working with discrete states. Moreover, some states may not be
    visited at all during training, which can be problematic when generalizing the
    agent to deal with such unseen states later.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: To address these problems, instead of representing the value function in a tabular
    format like *V*(*S*[t]), or *Q*(*S*[t], *A*[t]), for the action-value function,
    we use a *function approximation* approach. Here, we define a parametric function,
    *v*[w](*x*[s]), that can learn to approximate the true value function, that is,
    ![](img/B17582_19_122.png), where *x*[s] is a set of input features (or “featurized”
    states).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'When the approximator function, *q*[w](*x*[s], *a*), is a **deep neural network**
    (**DNN**), the resulting model is called a **deep Q-network** (**DQN**). For training
    a DQN model, the weights are updated according to the Q-learning algorithm. An
    example of a DQN model is shown in *Figure 19.10*, where the states are represented
    as features passed to the first layer:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_10.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.10: An example of a DQN'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can train a DQN using the *deep Q-learning* algorithm.
    Overall, the main approach is very similar to the tabular Q-learning method. The
    main difference is that we now have a multilayer NN that computes the action values.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Training a DQN model according to the Q-learning algorithm
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the procedure for training a DQN model using the
    Q-learning algorithm. The deep Q-learning approach requires us to make some modifications
    to our previously implemented standard Q-learning approach.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: One such modification is in the agent’s `choose_action()` method, which, in
    the code of the previous section for Q-learning, was simply accessing the action
    values stored in a dictionary. Now, this function should be changed to perform
    a forward pass of the NN model for computing the action values.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: The other modifications needed for the deep Q-learning algorithm are described
    in the following two subsections.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Replay memory
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the previous tabular method for Q-learning, we could update the values
    for specific state-action pairs without affecting the values of others. However,
    now that we approximate *q*(*s*, *a*) with an NN model, updating the weights for
    a state-action pair will likely affect the output of other states as well. When
    training NNs using stochastic gradient descent for a supervised task (for example,
    a classification task), we use multiple epochs to iterate through the training
    data multiple times until it converges.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先前的Q学习表格方法，我们可以更新特定状态-动作对的值而不影响其他值。然而，现在我们用NN模型来近似*q*(*s*, *a*），更新状态-动作对的权重可能会影响其他状态的输出。当使用随机梯度下降训练NN进行监督任务（例如分类任务）时，我们使用多个epochs多次迭代训练数据直到收敛。
- en: This is not feasible in Q-learning, since the episodes will change during the
    training and, as a result, some states that were visited in the early stages of
    training will become less likely to be visited later.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Q学习中是不可行的，因为在训练过程中，剧集会发生变化，结果是一些在训练早期访问的状态在后期变得不太可能再被访问。
- en: Furthermore, another problem is that when we train an NN, we assume that the
    training examples are **IID** (**independent and identically distributed**). However,
    the samples taken from an episode of the agent are not IID, as they form a sequence
    of transitions.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，另一个问题是，在训练NN时，我们假设训练样本是**IID**（独立同分布的）。然而，从代理的一个剧集中取样得到的样本并不是IID，因为它们形成一个过渡序列。
- en: To solve these issues, as the agent interacts with the environment and generates
    a transition quintuple *q*[w](*x*[s], *a*), we store a large (but finite) number
    of such transitions in a memory buffer, often called *replay memory*. After each
    new interaction (that is, the agent selects an action and executes it in the environment),
    the resulting new transition quintuple is appended to the memory.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，当代理与环境交互并生成一个过渡五元组*q*[w](*x*[s], *a*）时，我们将大量（但有限）这样的过渡存储在内存缓冲区中，通常称为*重放记忆*。每次新的交互之后（即代理选择一个动作并在环境中执行它），新的过渡五元组将被追加到内存中。
- en: 'To keep the size of the memory bounded, the oldest transition will be removed
    from the memory (for example, if it is a Python list, we can use the `pop(0)`
    method to remove the first element of the list). Then, a mini-batch of examples
    is randomly selected from the memory buffer, which will be used for computing
    the loss and updating the network parameters. *Figure 19.11* illustrates the process:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持内存大小有界，最老的过渡将从内存中移除（例如，如果它是一个Python列表，我们可以使用`pop(0)`方法来移除列表的第一个元素）。然后，从内存缓冲区中随机选择一小批示例，用于计算损失和更新网络参数。*图19.11*说明了这个过程：
- en: '![](img/17582_19_11.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17582_19_11.png)'
- en: 'Figure 19.11: The replay memory process'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.11：重放记忆过程
- en: '**Implementing the replay memory**'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现重放记忆**'
- en: The replay memory can be implemented using a Python list, where every time we
    add a new element to the list, we need to check the size of the list and call
    `pop(0)` if needed.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python列表实现重放记忆，每次向列表添加新元素时，需要检查列表的大小，并在需要时调用`pop(0)`。
- en: Alternatively, we can use the `deque` data structure from the Python `collections`
    library, which allows us to specify an optional argument, `max_len`. By specifying
    the `max_len` argument, we will have a bounded deque. Therefore, when the object
    is full, appending a new element results in automatically removing an element
    from it.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用Python `collections`库中的`deque`数据结构，这样我们可以指定一个可选参数`max_len`。通过指定`max_len`参数，我们将拥有一个有界的deque。因此，当对象满时，追加新元素会自动移除其中的一个元素。
- en: Note that this is more efficient than using a Python list, since removing the
    first element of a list using `pop(0)` has O(n) complexity, while the deque’s
    runtime complexity is O(1). You can learn more about the deque implementation
    from the official documentation that is available at [https://docs.python.org/3.9/library/collections.html#collections.deque](https://docs.python.org/3.9/library/collections.html#collections.deque).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这比使用Python列表更高效，因为使用`pop(0)`来移除列表的第一个元素的复杂度是O(n)，而deque的运行时复杂度是O(1)。您可以从官方文档中了解更多关于deque实现的信息，网址是[https://docs.python.org/3.9/library/collections.html#collections.deque](https://docs.python.org/3.9/library/collections.html#collections.deque)。
- en: Determining the target values for computing the loss
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定用于计算损失的目标值
- en: Another required change from the tabular Q-learning method is how to adapt the
    update rule for training the DQN model parameters. Recall that a transition quintuple,
    *T*, stored in the batch of examples, contains ![](img/B17582_19_123.png).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 19.12*, we perform two forward passes of the DQN model.
    The first forward pass uses the features of the current state (*x*[s]). Then,
    the second forward pass uses the features of the next state (![](img/B17582_19_124.png)).
    As a result, we will obtain the estimated action values, ![](img/B17582_19_125.png)
    and ![](img/B17582_19_126.png), from the first and second forward pass, respectively.
    (Here, this ![](img/B17582_19_127.png) notation means a vector of Q-values for
    all actions in ![](img/B17582_19_128.png).) From the transition quintuple, we
    know that action *a* is selected by the agent.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, according to the Q-learning algorithm, we need to update the action
    value corresponding to the state-action pair (*x*[s], *a*) with the scalar target
    value ![](img/B17582_19_129.png). Instead of forming a scalar target value, we
    will create a target action-value vector that retains the action values for other
    actions, ![](img/B17582_19_130.png), as shown in *Figure 19.12*:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_12.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.12: Determining the target value using the DQN'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'We treat this as a regression problem, using the following three quantities:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The currently predicted values, ![](img/B17582_19_131.png)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value vector as described
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard **mean squared error** (**MSE**) loss function
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, the losses will be zero for every action except for *a*. Finally,
    the computed loss will be backpropagated to update the network parameters.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep Q-learning algorithm
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we will use all these techniques to implement a deep Q-learning algorithm.
    This time, we use the CartPole environment from the OpenAI Gym environment that
    we introduced earlier. Recall that the CartPole environment has a continuous state
    space of size 4\. In the following code, we define a class, `DQNAgent`, that builds
    the model and specifies various hyperparameters.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'This class has two additional methods compared to the previous agent that was
    based on tabular Q-learning. The `remember()` method will append a new transition
    quintuple to the memory buffer, and the `replay()` method will create a mini-batch
    of example transitions and pass that to the `_learn()` method for updating the
    network’s weight parameters:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, with the following code, we train the model for 200 episodes, and
    at the end visualize the learning history using the `plot_learning_history()`
    function:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After training the agent for 200 episodes, we see that the agent indeed learned
    to increase the total rewards over time, as shown in *Figure 19.13*:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17582_19_13.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.13: The agent’s rewards increased over time'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Note that the total rewards obtained in an episode are equal to the amount of
    time that the agent is able to balance the pole. The learning history plotted
    in this figure shows that after about 30 episodes, the agent learns how to balance
    the pole and hold it for more than 200 time steps.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Chapter and book summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the essential concepts in RL, starting from the
    very foundations, and how RL can support decision making in complex environments.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned about agent-environment interactions and **Markov decision processes**
    (**MDPs**), and we considered three main approaches for solving RL problems: dynamic
    programming, MC learning, and TD learning. We discussed the fact that the dynamic
    programming algorithm assumes that the full knowledge of environment dynamics
    is available, an assumption that is not typically true for most real-world problems.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Then, we saw how the MC- and TD-based algorithms learn by allowing an agent
    to interact with the environment and generate a simulated experience. After discussing
    the underlying theory, we implemented the Q-learning algorithm as an off-policy
    subcategory of the TD algorithm for solving the grid world example. Finally, we
    covered the concept of function approximation and deep Q-learning in particular,
    which can be used for problems with large or continuous state spaces.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: We hope you enjoyed this last chapter of *Python Machine Learning* and our exciting
    tour of machine learning and deep learning. Throughout the journey of this book,
    we’ve covered the essential topics that this field has to offer, and you should
    now be well equipped to put those techniques into action to solve real-world problems.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'We started our journey in *Chapter 1*, *Giving Computers the Ability to Learn
    from Data*, with a brief overview of the different types of learning tasks: supervised
    learning, reinforcement learning, and unsupervised learning.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed several different learning algorithms that you can use for
    classification, starting with simple single-layer NNs in *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: We continued to discuss advanced classification algorithms in *Chapter 3*, *A
    Tour of Machine Learning Classifiers Using Scikit-Learn*, and we learned about
    the most important aspects of a machine learning pipeline in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, and *Chapter 5*, *Compressing Data
    via Dimensionality Reduction*.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Remember that even the most advanced algorithm is limited by the information
    in the training data that it gets to learn from. So, in *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we learned about
    the best practices to build and evaluate predictive models, which is another important
    aspect in machine learning applications.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: If one single learning algorithm does not achieve the performance we desire,
    it can sometimes be helpful to create an ensemble of experts to make a prediction.
    We explored this in *Chapter 7*, *Combining Different Models for Ensemble Learning*.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Then, in *Chapter 8*, *Applying Machine Learning to Sentiment Analysis*, we
    applied machine learning to analyze one of the most popular and interesting forms
    of data in the modern age, which is dominated by social media platforms on the
    internet—text documents.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, our focus was on algorithms for classification, which is
    probably the most popular application of machine learning. However, this is not
    where our journey ended! In *Chapter 9*, *Predicting Continuous Target Variables
    with Regression Analysis*, we explored several algorithms for regression analysis
    to predict continuous target variables.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Another exciting subfield of machine learning is clustering analysis, which
    can help us find hidden structures in the data, even if our training data does
    not come with the right answers to learn from. We worked with this in *Chapter
    10*, *Working with Unlabeled Data – Clustering Analysis*.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: We then shifted our attention to one of the most exciting algorithms in the
    whole machine learning field—artificial neural networks. We started by implementing
    a multilayer perceptron from scratch with NumPy in *Chapter 11*, *Implementing
    a Multilayer Artificial Neural Network from Scratch*.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of PyTorch for deep learning became obvious in *Chapter 12*, *Parallelizing
    Neural Network Training with PyTorch*, where we used PyTorch to facilitate the
    process of building NN models, worked with PyTorch `Dataset` objects, and learned
    how to apply preprocessing steps to a dataset.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We delved deeper into the mechanics of PyTorch in *Chapter 13*, *Going Deeper
    – The Mechanics of PyTorch*, and discussed the different aspects and mechanics
    of PyTorch, including tensor objects, computing gradients of a computation, as
    well as the neural network module, `torch.nn`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 14*, *Classifying Images with Deep Convolutional Neural Networks*,
    we dived into convolutional neural networks, which are widely used in computer
    vision at the moment, due to their great performance in image classification tasks.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we learned about sequence modeling using RNNs.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 16*, *Transformers – Improving Natural Language Processing with
    Attention Mechanisms*, we introduced the attention mechanism to address one of
    the weaknesses of RNNs, that is, remembering previous input elements when dealing
    with long sequences. We then explored various kinds of transformer architectures,
    which are deep learning architectures that are centered around the self-attention
    mechanism and constitute the state of the art for creating large-scale language
    models.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 17*, *Generative Adversarial Networks for Synthesizing New Data*,
    we saw how to generate new images using GANs and, along the way, we also learned
    about autoencoders, batch normalization, transposed convolution, and Wasserstein
    GANs.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Previous chapters were centered around tabular datasets as well as text and
    image data. In *Chapter 18*, *Graph Neural Networks for Capturing Dependencies
    in Graph Structured Data*, we focused on deep learning for graph-structured data,
    which is commonly used data representation for social networks and molecules (chemical
    compounds). Moreover, we learned about so-called graph neural networks, which
    are deep neural networks that are compatible with such data.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in this chapter, we covered a separate category of machine learning
    tasks and saw how to develop algorithms that learn by interacting with their environment
    through a reward process.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: While a comprehensive study of deep learning is well beyond the scope of this
    book, we hope that we’ve kindled your interest enough to follow the most recent
    advancements in this field of deep learning.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re considering a career in machine learning, or you just want to keep
    up to date with the current advancements in this field, we can recommend that
    you keep an eye on the recent literature published in this field. The following
    are some resources that we find particularly useful:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'A subreddit and community dedicated to learning machine learning: [https://www.reddit.com/r/learnmachinelearning/](https://www.reddit.com/r/learnmachinelearning/)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A daily updated list of the latest machine learning manuscripts uploaded to
    the arXiv preprint server: [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A paper recommendation engine built on top of arXiv: [http://www.arxiv-sanity.com](http://www.arxiv-sanity.com)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, you can find out what we, the authors, are up to at these sites:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebastian Raschka: [https://sebastianraschka.com](https://sebastianraschka.com)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hayden Liu: [https://www.mlexample.com/](https://www.mlexample.com/)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vahid Mirjalili: [http://vahidmirjalili.com](http://vahidmirjalili.com)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’re always welcome to contact us if you have any questions about this book
    or if you need some general tips on machine learning.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
