- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling a Deep Learning Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) opens many possibilities in **deep learning**
    (**DL**) model deployments. In this chapter, we will introduce the two most popular
    services designed for deploying a DL model as an inference endpoint: **Elastic
    Kubernetes Service** (**EKS**) and **SageMaker**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first half, we will describe the EKS-based approach. First, we will discuss
    how to create inference endpoints for **TensorFlow** (**TF**) and PyTorch models
    and deploy them using EKS. We will also introduce the **Elastic Inference** (**EI**)
    accelerator, which can increase the throughput while reducing the cost. EKS clusters
    have pods that host the inference endpoints as web servers. As the last topic
    for EKS-based deployment, we will introduce how the pods can be scaled horizontally
    for the dynamic incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half, we will introduce SageMaker-based deployment. We will discuss
    how to create inference endpoints for TF, PyTorch, and ONNX models. Additionally,
    the endpoints will be optimized using **Amazon SageMaker Neo** and EI accelerators.
    Then, we will set up automatic scaling for the inference endpoints running on
    SageMaker. Finally, we will wrap up this chapter by describing how to host multiple
    models in a single SageMaker inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing using Elastic Kubernetes Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferencing using SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the supplemental material for this chapter from this book’s
    GitHub repository at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9).
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing using Elastic Kubernetes Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EKS is designed to provide Kubernetes clusters for application deployment by
    simplifying the complex cluster management process ([https://aws.amazon.com/eks](https://aws.amazon.com/eks)).
    The detailed steps for creating an EKS cluster can be found at [https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html).
    In general, an EKS cluster is used to deploy any web service application and scale
    it as necessary. The inference endpoint on EKS is just a web service application
    that handles model inference requests. In this section, you will learn how to
    host a DL model inference endpoint on EKS.
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster has a control plane and a set of nodes. The control plane
    makes scheduling and scaling decisions based on the volume of incoming traffic.
    With scheduling, the control plane manages which node runs a job at a given point
    in time. With scaling, the control plane increases or decreases the size of the
    pod based on the volume of traffic coming into the endpoints. EKS manages these
    components behind the scenes so that you can focus on hosting your services efficiently
    and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: This section begins by describing how to set up an EKS cluster. Then, we will
    describe how to create endpoints using TF and PyTorch to handle model inference
    requests on an EKS cluster. Next, we will discuss the EI accelerator, which improves
    the inference performance, along with cost reduction. Finally, we will introduce
    a way to scale the services dynamically based on the volume of incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing an EKS cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of model deployment based on EKS is to create a pod of appropriate
    hardware resources. In this section, we will use the GPU Docker images recommended
    by AWS ([https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)).
    These standard images are already registered and available on **Elastic Container
    Registry** (**ECR**), which provides a secure, scalable, and reliable registry
    for Docker images ([https://aws.amazon.com/ecr](https://aws.amazon.com/ecr)).
    Next, we should apply the NVIDIA device plugin to the container. This plugin enables
    **machine learning** (**ML**) operations to exploit the underlying hardware to
    achieve lower latency. For more details on the NVIDIA device plugin, we recommend
    reading [https://github.com/awslabs/aws-virtual-gpu-device-plugin](https://github.com/awslabs/aws-virtual-gpu-device-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we will use `kubectl`, the `kubectl`, you need
    to provide a YAML file that consists of information about clusters, users, namespaces,
    and authentication mechanisms ([https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig)).
    The most popular operation is `kubectl apply`, which creates or modifies resources
    in an EKS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding use case, the `kubectl apply` command applies the NVIDIA device
    plugin according to the specification specified in the YAML file to the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring EKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A YAML file is used to configure both the machines that make up the Kubernetes
    cluster and the application running within the cluster. The configurations in
    the YAML file can be broken down into two parts based on their type: *deployment*
    and *service*. The deployment part controls the application running within the
    pod. In this section, it will be used to create an endpoint from DL models. In
    the EKS context, a set of applications running on one or more pods of a Kubernetes
    cluster is called a service. The service part creates and configures the service
    on the cluster. Throughout the service part, we will create a unique URL for the
    service that external connections can use and configure load balancing for incoming
    traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When managing an EKS cluster, namespaces can be useful as they isolate a group
    of resources within the cluster. To create a namespace, you can simply use the
    `kubectl create namespace` terminal command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, we constructed the `tf-inference` namespace for the
    inference endpoints and services that we will be creating in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an inference endpoint using the TensorFlow model on EKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will describe an EKS configuration file (`tf.yaml`) designed
    to host an inference endpoint using a TF model. The endpoint is created by *TensorFlow
    Service*, a system designed for deploying a TF model ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)).
    Since our main focus is on EKS configurations, we will simply assume that a trained
    TF model is already available on S3 as a `.pb` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s look at the `Deployment` part of the configuration, which handles
    the endpoint creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the `Deployment` part of the configuration starts with `kind:
    Deployment`. In this first part of the configuration, we provide some metadata
    about the endpoint and define the system settings by filling in the `spec` section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important configurations for the endpoint are specified under `template`.
    We will create an endpoint that can be accessed using **HyperText Transfer Protocol**
    (**HTTP**) requests, as well as **Remote Procedure Call** (**gRPC**) requests.
    HTTP is the most basic transfer data protocol for web clients and servers. Built
    on top of HTTP, gRPC is an open source protocol for sending requests and receiving
    responses in binary format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the `template` section, we specify an ECR image to use (`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04`),
    the command to create the TF inference endpoint (`command: /usr/bin/tensorflow_model_server`),
    the arguments for TF serving (`args`), and the ports configuration for containers
    (`ports`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The TF serving arguments contains the model’s name (`--model_name=saved_model`),
    the location of the model on S3 (`--model_base_path=s3://mybucket/models`), the
    ports for HTTP access (`--rest_api_port=8500`), and the ports for gRPC access
    (`--port=9000`). The two `ContainerPort` configurations under `ports` are used
    to expose the endpoints to external connections (`containerPort: 8500` and `containerPort:
    9000`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at the second part of the YAML file – that is, the configurations
    for `Service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Service` part of the configuration starts with `kind: Service`. Under
    the `name: http-tf-serving` section, we have `port: 8500`, which refers to the
    port that the TF serving web server is listening to inside the pods for HTTP requests.
    `targetPort` specifies the port that the pods use to expose the corresponding
    port. We have another set of ports configuration for gRPC under the `name: grpc-tf-serving`
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: To apply the configuration to the underlying cluster, you can simply provide
    this YAML file to the `kubectl apply` command.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create an endpoint for a PyTorch model on EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an inference endpoint using a PyTorch model on EKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn how to create a PyTorch model inference endpoint
    on EKS. First, we would like to introduce *TorchServe*, an open source model serving
    framework for PyTorch ([https://pytorch.org/serve](https://pytorch.org/serve)).
    It is designed to simplify the process of PyTorch model deployment at scale. EKS
    configurations for PyTorch model deployment are very similar to what we have described
    for deploying a TF model in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a PyTorch model `.pth` file needs to be converted into a `.mar` file,
    which is the format required by TorchServe ([https://github.com/pytorch/serve/blob/master/model-archiver/README.md](https://github.com/pytorch/serve/blob/master/model-archiver/README.md)).
    The conversion can be achieved using the `torch-model-archiver` package. TorchServe
    and `torch-model-archiver` can be downloaded and installed through `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The conversion, when using the `torch-model-archiver` command, is shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `torch-model-archiver` command takes in `model-name`
    (the name of the output `.mar` file, which is `archived_model`), `version` (PyTorch
    version 1.0), `serialized-file` (the input PyTorch `.pth` file, which is `model.pth`),
    and `handler` (the name of the file that defines TorchServe inference logic; that
    is, `run_inference`, which indicates the file named `run_inference.py`). The command
    will generate an `archived_model.mar` file, which will be uploaded to an S3 bucket
    for endpoint hosting through EKS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another command we would like to introduce before discussing EKS configuration
    is `mxnet-model-server`. This command is available in a DLAMI instance, allowing
    you to host a web server that runs PyTorch inference for the incoming requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the `mxnet-model-server` command, with the `start`
    parameter, creates an endpoint for the model provided through the `models` parameter.
    As you can see, the `models` parameter points to the location of the model on
    S3 (`archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar`).
    The input arguments for the model are specified in the `/home/model-server/config.properties`
    file, which is passed to the command through the `mms-config` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will discuss how the `Deployment` part of the EKS configuration must
    be filled in. Every component can stay similar to the version for the TF model.
    The main difference comes from the `template` section, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are using a different Docker image that has PyTorch
    installed (`image: "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04"`).
    The configuration takes in the `mxnet-model-server` command to create an inference
    endpoint. The port we will be using for this endpoint is `8080`. The only change
    we made for the `Service` part can be found in the `Ports` section; we must ensure
    that an external port is assigned and connected to port `8080` – that is, the
    port that the endpoint is hosted on. Again, you can use the `kubectl apply` command
    to apply the changes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe how to interact with the endpoint hosted
    by the EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating with an endpoint on EKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have an endpoint running, we will explain how you can send a request
    and retrieve an inference result. First, we need to identify the IP address of
    the service using `kubectl get services`, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will return a list of services and their external IP
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we will make use of the `tf-inference` service we created
    in the *Creating an inference endpoint using the TensorFlow model on EKS* section.
    From the sample output of `kubectl get services`, we can see that the service
    is running with an external IP address of `104.198.xxx.xx`. To access the service
    via HTTP, you need to append the port for HTTP to the IP address: `http://104.198.xxx.xx:8500`.
    If you are interested in creating an explicit URL for the IP address, please go
    to [https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster](https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To send a prediction request to the endpoint and receive an inference result,
    you need to make a POST-typed HTTP request. If you want to send a request from
    the terminal, you can use the `curl` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, we are sending JSON data (`demo_input.json`) to the
    endpoint (`http://104.198.xxx.xx:8500/v1/models/demo:predict`). The input JSON
    file, `demo_input.json`, consists of the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The response data we will receive from the endpoint also consists of JSON data
    that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A detailed explanation of the input and output JSON data structures can be
    found in the official documentation: [https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in using gRPC instead of HTTP, you can find the details
    at [https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework](https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework).
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully created an endpoint for your model that
    your application can access over the network. Next, we will introduce Amazon EI
    accelerator, which can reduce the inference latency and EKS costs.
  prefs: []
  type: TYPE_NORMAL
- en: Improving EKS endpoint performance using Amazon Elastic Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will describe how to create an EKS cluster with the EI accelerator,
    a low-cost GPU-powered acceleration. The EI accelerator can be linked to Amazon
    EC2 and Sagemaker instances or `eia2.*`-typed instances. The complete description
    of eia2.* instances can be found at [https://aws.amazon.com/machine-learning/elastic-inference/pricing](https://aws.amazon.com/machine-learning/elastic-inference/pricing).
  prefs: []
  type: TYPE_NORMAL
- en: To make the most out of AWS resources, you also need to compile your model using
    *AWS Neuron* ([https://aws.amazon.com/machine-learning/neuron](https://aws.amazon.com/machine-learning/neuron)).
    The advantage of Neuron models comes from the fact that they can utilize Amazon
    EC2 Inf1 instances. These types of machines consist of *AWS Inferentia*, a custom
    chip designed by AWS for ML in the cloud ([https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia)).
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Neuron SDK is pre-installed in AWS DL containers and **Amazon Machine
    Images** (**AMI**). In this section, we will focus on TF models. However, PyTorch
    model compilation goes through the same process. The detailed steps for TF can
    be found at [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html)
    and the steps for PyTorch can be found at [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling a TF model into a Neuron model can be achieved by using `tf.neuron.saved_model.compile`
    function of TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For this function, we simply need to provide where the input model is located
    (`tf_model_dir`) and where we want to store the output Neuron model (`neuron_model_dir`).
    Just as we upload a TF model to an S3 bucket for endpoint creation, we need to
    move the Neuron model to an S3 bucket as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the changes you need to make to the EKS configuration only need to be
    done in the `template` section of the `Deployment` part. The following code snippet
    describes the updated sections of the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we notice from the preceding configuration is that it is very
    similar to the one we described in the *Creating an inference endpoint using the
    TensorFlow model on EKS* section. The difference mainly comes from the `image`,
    `command`, and `args` sections. First, we need to use a DL container with AWS
    Neuron and TensorFlow Serving applications (`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04`).
    Next, the entry point script for the model artifact file is passed through the
    `command` key: `/usr/local/bin/entrypoint.sh`. The entry point script is used
    to start the web server using `args`. To create an endpoint from a Neuron model,
    we must specify the S3 bucket where the target Neuron model is stored as a `model_base_path`
    parameter (`--model_base_path=s3://mybucket/neuron_model/`).'
  prefs: []
  type: TYPE_NORMAL
- en: To apply the changes to the cluster, you can simply pass the updated YAML file
    to the `kubectl apply` command.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will look at the autoscaling feature of EKS to increase the stability
    of the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing EKS cluster dynamically using autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An EKS cluster can automatically adjust the size of the cluster based on the
    volume of traffic. The idea of horizontal pod autoscaling is to scale up the number
    of running applications by increasing the number of pods as the number of incoming
    requests increases. Similarly, some pods will be freed up when the volume of the
    incoming traffic decreases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an application has been deployed through the `kubectl apply` command,
    autoscaling can be set up using the `kubectl autoscale` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding example, the `kubectl autoscale` command takes in
    the name of the application specified in the `Deployment` part of the YAML file,
    `cpu-percent` (the cut-off CPU percentage that is used to scale up or down the
    cluster size), `min` (the minimum number of pods to keep), and `max` (the maximum
    number of pods to spin up). To summarize, the example command will run the service
    using 1 to 10 pods, depending on the volume of the traffic, keeping the CPU usage
    at 60%.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. EKS is designed to provide Kubernetes clusters for application deployment
    by simplifying the complex cluster management for dynamic traffic.
  prefs: []
  type: TYPE_NORMAL
- en: b. A YAML file is used to configure both the machines that make up the Kubernetes
    cluster and the application running within the cluster. The two parts of the configuration,
    `Deployment` and `Service`, control the application running within the pod and
    configure the service for the underlying target cluster, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: c. It is possible to create and host inference endpoints using TF and PyTorch
    models on an EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: d. By exploiting the EI accelerator with a model compiled using AWS Neuron,
    it is possible to improve the inference latency while saving the operating cost
    of the EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: b. An EKS cluster can be configured to resize itself dynamically based on the
    volume of the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we discussed EKS-based DL model deployment for TF and PyTorch
    models. We described how the AWS Neuron model and the EI accelerator can be used
    to improve service performance. Finally, we covered autoscaling to utilize the
    available resources more effectively. In the next section, we will look at another
    AWS service for hosting inference endpoints: SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing using SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to create an endpoint using SageMaker instead
    of the EKS cluster. First, we will describe framework-independent ways of creating
    inference endpoints (the `Model` class). Then, we will look at creating TF endpoints
    using `TensorFlowModel` and the TF-specific `Estimator` class. The next section
    will focus on endpoint creation for PyTorch models using the `PyTorchModel` class
    and the PyTorch-specific `Estimator` class. Furthermore, we will introduce how
    to build an endpoint from an ONNX model. At this point, we should have a service
    running model prediction for incoming requests. After that, we will describe how
    to improve the quality of a service using *AWS SageMaker Neo* and the EI accelerator.
    Finally, we will cover autoscaling and describe how to host multiple models on
    a single endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: As described in the *Utilizing SageMaker for ETL* section in [*Chapter 5*](B18522_05.xhtml#_idTextAnchor106),
    *Data Preparation in the Cloud*, SageMaker provides a built-in notebook environment
    called SageMaker Studio. The code snippets we have included in this section are
    meant to be executed in this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an inference endpoint using the Model class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, SageMaker provides three different classes for endpoint creation.
    The most basic one is the `Model` class, which supports models from various DL
    frameworks. The other option is to use a framework-specific `Model` class. The
    last option is to use the `Estimator` class. In this section, we will look at
    the first option, which is the `Model` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into the endpoint creation process, we need to make sure the
    necessary components have been prepared appropriately; the right IAM role must
    be configured for SageMaker, and the trained model should be available on S3\.
    The IAM role can be prepared in the notebook as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the IAM access role and default bucket have been set
    up. To load the current IAM role of the SageMaker notebook, you can use the `sagemaker.get_execution_role`
    function. To create a SageMaker session, you need to create an instance for the
    `Session` class. The `default_bucket` method of the `Session` instance will create
    a default bucket with its name in `sagemaker-{region}-{aws-account-id}` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before uploading the model to an S3 bucket, the model needs to be compressed
    as a `.tar` file. The following code snippet describes how to compress the model
    and upload the compressed model to the target bucket within the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, the compression is performed using the `tarfile`
    library. The `upload_data` method of the `Session` instance is used to upload
    the compiled model to the S3 bucket linked with the SageMaker session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to create an instance of the `Model` class. In this particular
    example, we will assume that the model has been trained with TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the constructor of the `Model` class takes in
    `model_data` (the S3 path where the compressed model file is located), `framework_version`
    (a version of TF), and `role` (the IAM role for the notebook). The `deploy` method
    of the `Model` instance handles the actual endpoint creation. It takes in `initial_instance_count`
    (the number of instances to start the endpoint with) and `instance_type` (the
    EC2 instance type to use).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can provide a defined `image` and drop `framework_version`.
    In this case, the endpoint will be created with the Docker image specified for
    the `image` parameter. It should be pointing at an image on ECR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will discuss how to trigger a model inference from the notebook using
    the created endpoint. The `deploy` method will return a `Predictor` instance.
    As shown in the following code snippet, you can achieve this through the `predict`
    function of the `Predictor` instance. All you need to pass to this function is
    some JSON data representing the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `predict` function, `results`, consists of JSON data that,
    in our example, looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `predict` function supports data of different formats such as JSON, CSV,
    and multidimensional array. If you need to use a type other than JSON, you can
    refer to [https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option for triggering model inference is to use the `SageMaker.Client`
    class from the `boto3` library. The `SageMaker.Client` class is a low-level client
    representing Amazon SageMaker Service. In the following code snippet, we are creating
    an instance of `SageMaker.Client` and demonstrating how to access the endpoint
    using the `invoke_endpoint` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code snippet, the `invoke_endpoint` method takes in
    `EndpointName` (the name of the endpoint; that is, `run_model_prediction`), `ContentType`
    (the type of the input data; that is, `"text/csv"`), and `Body` (the input data
    for model prediction; that is, `payload`).
  prefs: []
  type: TYPE_NORMAL
- en: In reality, many companies utilize Amazon API Gateway ([https://aws.amazon.com/api-gateway](https://aws.amazon.com/api-gateway))
    and AWS Lambda ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda))
    along with SageMaker endpoints, to communicate with the deployed model in a serverless
    architecture. For the detailed setup, please refer to [https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explain framework-specific approaches to creating an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a TensorFlow inference endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will describe a `Model` class designed specifically for
    TF – the `TensorFlowModel` class. Then, we will explain how to use the TF-specific
    `Estimator` class for endpoint creation. The complete versions of the code snippets
    in this section can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a TensorFlow inference endpoint using the TensorFlowModel class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `TensorFlowModel` class is a `Model` class that is designed for TF models.
    As shown in the following code snippet, the class can be imported from the `sagemaker.tensorflow`
    module and its usage is identical to the `Model` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of the `TensorFlowModel` class takes in the same parameters
    as the constructor of the `Model` class: the S3 path of the uploaded model (`model_s3_path`),
    the TF framework version (`Tf_framework_version`), and the IAM role for SageMaker
    (`role`). In addition, you can provide a Python script for pre- and post-processing
    the input and output of the model inference by providing `entry_point`. In this
    case, the script needs to be named `inference.py`. For more details, please refer
    to [https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing).'
  prefs: []
  type: TYPE_NORMAL
- en: Being a child class of `Model`, `TensorFlowModel` also provides a `Predictor`
    instance through the `deploy` method. Its usage is identical to what we described
    in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to deploy your model using the `Estimator` class, which
    we have already introduced for the model training on SageMaker in [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133),
    *Efficient Model Training*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a TensorFlow inference endpoint using the Estimator class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As introduced in the *Training a TensorFlow model using SageMaker* section
    of [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient Model Training*,
    SageMaker provides the `Estimator` class, which supports model training on SageMaker.
    The same class can be used to create and deploy an inference endpoint. In the
    following code snippet, we are making use of the `Estimator` class that’s been
    designed for TF, `sagemaker.tensorflow.estimator.TensorFlow`, to train a TF model
    and deploy an endpoint using a trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, the `sagemaker.tensorflow.estimator.TensorFlow`
    class takes in the following parameters: `entry_point` (the script that handles
    the training; that is, `"tf-train.py"`), `instance_count` (the number of instances
    to use; that is, `1`), `instance_type` (the type of the instance; that is, `"ml.c4.xlarge"`),
    `framework_version` (a PyTorch version; that is, `"2.2"`), and `py_version` (a
    Python version; that is, `"py37"`). The `fit` method of the `Estimator` instance
    performs the model training. The key method for creating and deploying an endpoint
    is the `deploy` method, which creates and hosts an endpoint for the model it trained
    based on the conditions provided: the `initial_instance_count` (`1`) instances
    of `instance_type` (`"ml.c5.xlarge"`). The `deploy` method of the `Estimator`
    class returns a `Predictor` instance as in the case of the `Model` class.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explained how to create an endpoint for a TF model on SageMaker.
    In the next section, we will look at how SageMaker supports PyTorch models.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a PyTorch inference endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is designed to cover different ways of creating and hosting an
    endpoint from a PyTorch model on SageMaker. First, we will introduce a `Model`
    class designed for PyTorch models: the `PyTorchModel` class. Then, we will describe
    an `Estimator` class for the PyTorch model. The complete implementations for the
    code snippets in this section can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a PyTorch inference endpoint using the PyTorchModel class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the `TensorFlowModel` class, there exists a `Model` class designed
    specifically for a PyTorch model, `PyTorchModel`. It can be instantiated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code snippet, the constructor takes in `entry_point`,
    which defines custom pre- and post-processing logic for the data, `source_dir`
    (the S3 path of the entry point script), `role` (the IAM role for SageMaker),
    `model_data` (the S3 path of the model), `framework_version` (the version of PyTorch),
    and `py_version` (the version of Python).
  prefs: []
  type: TYPE_NORMAL
- en: Since the `PyTorchModel` class inherits the `Model` class, it provides the `deploy`
    function, which creates and deploys an endpoint, as described in the *Setting
    up a PyTorch inference endpoint using the Model class* section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce an `Estimator` class designed for PyTorch models.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a PyTorch inference endpoint using the Estimator class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If a trained PyTorch model is not available, the `sagemaker.pytorch.estimator.PyTorch`
    class can be used to train and deploy a model. The training can be achieved with
    the `fit` method, as described in the *Training a PyTorch model using SageMaker*
    section of [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient Model Training*.
    Being an `Estimator` class, the `sagemaker.pytorch.estimator.PyTorch` class provides
    the same features as `sagemaker.tensorflow.estimator.TensorFlow`, which we covered
    in the *Setting up a TensorFlow inference endpoint using the Estimator class*
    section. In the following code snippet, we are creating an `Estimator` instance
    for a PyTorch model, training the model, and creating an endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code snippet, the constructor of `sagemaker.pytorch.estimator.PyTorch`
    takes in the same set of parameters as the `Estimator` class designed for TF:
    `entry_point` (the script that handles the training; that is, `"pytorch-train.py"`),
    `instance_count` (the number of instances to use; that is, `1`), `instance_type`
    (the type of the EC2 instance; that is, `"ml.c4.xlarge"`), `framework_version`
    (the PyTorch version; that is, `"1.11.0"`), and `py_version` (the Python version;
    that is, `"py37"`). The model training (the `fit` method) and deployment (the
    `deploy` method) are achieved the same way as in the previous example in the *Setting
    up a TensorFlow inference endpoint using the Estimator class* section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we covered how to deploy a PyTorch model in two different
    ways: using the `PyTorchModel` class and using the `Estimator` class. Next, we
    will learn how to create an endpoint for an ONNX model on SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an inference endpoint from an ONNX model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, [*Chapter 8*](B18522_08.xhtml#_idTextAnchor175),
    *Simplifying Deep Learning Model Deployment*, DL models are often transformed
    into **open neural network exchange** (**ONNX**) models for deployment. In this
    section, we will describe how to deploy an ONNX model on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most standard approach is to use the base `Model` class. As mentioned in
    the *Setting up a TensorFlow inference endpoint using the Model class* section,
    the `Model` class supports DL models of various types. Fortunately, it provides
    built-in support for ONNX models as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have a trained ONNX model on S3\. The key in the
    `Model` instance creation comes from `framework="onnx"`. We also need to provide
    an ONNX framework version to `framework_version`. In this example, we are using
    the ONNX framework version 1.4.0\. Everything else is almost identical to the
    previous examples. Again, the `deploy` function is designed for creating and deploying
    an endpoint; a `Predictor` instance will be returned for model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also common to use the `TensorFlowModel` and `PyTorchModel` classes for
    creating an endpoint from an ONNX model. The following code snippet demonstrates
    such use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippets are self-explanatory. Both classes take in a ONNX
    model path (`model_data`), an inference script (`entry_point`), an IAM role (`role`),
    a Python version (`py_version`), and versions for each framework (`framework_version`).
    Like how the `Model` class deploys an endpoint, the `deploy` method will create
    and host an endpoint from each model.
  prefs: []
  type: TYPE_NORMAL
- en: While endpoints allow us to get the model predictions at any point in time for
    dynamic input data, there are cases where you need to perform inference on the
    whole input data stored on an S3 bucket instead of feeding each of them one by
    one. Therefore, we will look at how we can leverage Batch Transform for this requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Handling prediction requests in batches using Batch Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the Batch Transform feature of SageMaker ([https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html))
    to run inference on a large dataset in one queue. Using the `sagemaker.transformer.Transformer`
    class, you can perform model prediction in batches for any dataset on S3 without
    a persistent endpoint. The details are included in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the `sagemaker.transformer.Transformer` class
    takes in `base_transformer_job_name` (a job name for the transformer job), `model_name`
    (the name of the model that holds the inference pipeline), `max_payload` (the
    maximum payload in MB allowed), `instance_count` (the number of EC2 instances
    to start with), `instance_type` (the type of EC2 instance), and `output_path`
    (an S3 path where the output will be stored). The `transformer` method will trigger
    the model prediction on the dataset specified. It takes in the following parameters:
    `input_location` (the S3 path where the input data is located), `content_type`
    (the content of the input data; that is, `"text/csv"`), and `split_type` (this
    controls how to split the input data; `"Line"` is used to feed each line of the
    data as an individual input to the model). In reality, many companies also utilize
    SageMaker processing jobs ([https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html))
    to perform batch inference, but we will not talk about this in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at how SageMaker supports hosting an inference endpoint
    for handling live prediction requests and running model predictions in batches
    for a static dataset available on S3\. In the next section, we will describe how
    to use **AWS SageMaker Neo** to further improve the inference latency of the deployed
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving SageMaker endpoint performance using AWS SageMaker Neo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explain how SageMaker can further improve the performance
    of the application by exploiting the underlying hardware resources (EC2 instances
    or mobile devices). The idea is to compile the trained DL model using **AWS SageMaker
    Neo** ([https://aws.amazon.com/sagemaker/neo](https://aws.amazon.com/sagemaker/neo)).
    After the compilation, the generated Neo model can utilize the underlying device
    better, thus reducing the inference latency. AWS SageMaker Neo supports models
    of different frameworks (TF, PyTorch, MxNet, and ONNX) and various types of hardware
    (OS, chip, architecture, and accelerator). The complete list of supported resources
    can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Neo model generation can be achieved using the `compile` method of the `Model`
    class. The `compile` method returns an `Estimator` instance that supports endpoint
    creation. Let’s look at the following example for the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we start with a `Model` instance called `sm_model`.
    We trigger the `compile` method to compile the loaded model into a Neo model.
    The following list describes the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`target_instance_family`: The EC2 instance type that the model will be optimized
    for'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape`: The input data shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`job_name`: The name of the compilation job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`role`: The IAM role of the compiled model output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework`: A DL framework such as TF or PyTorch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework_version`: The version of the framework to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_path`: The output S3 path where the compiled model will be stored'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Estimator` instance consists of a `deploy` function that creates the endpoint.
    The output is a `Predictor` instance that you can use to run the model prediction.
    In the preceding example, we optimized our model to perform the best on instances
    of the `ml_c5` type.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will describe how to integrate the EI accelerator into the endpoints
    running on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Improving SageMaker endpoint performance using Amazon Elastic Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Improving EKS endpoint performance using Amazon Elastic Inference* section,
    we described how an EI accelerator can reduce the operating cost for an inference
    endpoint while improving the inference latency by exploiting the available GPU
    devices. In this section, we will cover EI accelerator integration for SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary change is fairly simple; you just need to provide `accelerator_type`
    when triggering the `deploy` method of a `Model` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `deploy` method creates an endpoint for the given
    `Model` instance. To attach an EI accelerator to the endpoint, you need to specify
    the type of accelerator you want (`accelerator_type`) on top of the default parameters
    (`initial_instance_count` and `instance_type`). For the complete description of
    using EI for the SageMaker endpoint, please look at [https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will look at the autoscaling feature of SageMaker,
    which allows us to handle the changes in the incoming traffic better.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing SageMaker endpoints dynamically using autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to how the EKS cluster supports autoscaling to automatically scale
    up or down the endpoints based on the changes in the traffic, SageMaker also provides
    the autoscaling feature. Configuring autoscaling involves configuring the scaling
    policy, which defines when the scaling takes place and how many resources are
    created and destroyed at the time of scaling. The scaling policy for the SageMaker
    endpoint can be configured from the SageMaker web console. The following steps
    describe how you can configure autoscaling for the inference endpoints created
    from a SageMaker notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the SageMaker web console, [https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/),
    and click **Endpoints** under **Inference** in the navigation panel on the left-hand
    side. You may need to provide your credentials to log in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you must choose the endpoint name you want to configure. Under the **Endpoint
    runtime** settings, choose the model variant that requires the configuration.
    This feature allows you to deploy multiple versions of a model in a single endpoint,
    spinning up one container per version. The details on this feature can be found
    at [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the **Endpoint runtime** settings, select **Configure auto scaling**.
    This will take you to the **Configure variant automatic scaling** page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The Configure variant automatic scaling page of the SageMaker
    web console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – The Configure variant automatic scaling page of the SageMaker web
    console
  prefs: []
  type: TYPE_NORMAL
- en: Type the minimum number of instances to maintain in the **Minimum instance count**
    field. The minimum value is 1\. This value defines the minimum instance number
    that will be kept at all times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type the maximum number of instances of the scaling policy to maintain in the
    **Maximum instance count** field. This value defines the maximum number of instances
    allowed at peak traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in the **SageMakerVariantInvocationsPerInstance** field. Each endpoint
    can have multiple models (or model versions) deployed in a single endpoint hosted
    across one or more EC2 instances. **SageMakerVariantInvocationsPerInstance** defines
    the maximum number of invocations allowed per minute for each model variant. This
    value is used for load balancing. Details on calculating the right number for
    this field can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in the scale-in cooldown and scale-out cooldown. These indicate how long
    SageMaker will wait before it checks for another round of scaling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Disable scale in** checkbox. During an increase in traffic, more
    instances are started as part of the scale-out process. But these instances can
    be quickly deleted during the scale-in process if the traffic slows down right
    after the increase. To avoid a newly created instance from being released as soon
    as it gets created, this checkbox must be selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Save** button to apply the configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scaling will be applied to the selected model variant as soon as you click
    the **Save** button. SageMaker will increase and decrease the number of instances
    based on the incoming traffic. For more details on auto-scaling, please take a
    look at [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html).
  prefs: []
  type: TYPE_NORMAL
- en: As the last topic for SageMaker-based endpoints, we will describe how to deploy
    multiple models through a single endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting multiple models on a single SageMaker inference endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker supports deploying multiple models on a single endpoint through **Multimodal
    Endpoints** (**MME**). There are a couple of things you must keep in mind before
    setting up MME. First, it’s recommended to set up multiple endpoints if you want
    to keep the low latency. Second, the container can only deploy models from the
    same DL framework. For those who are interested in hosting models from different
    frameworks, we recommend reading [https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html).
    MEE works best when the models are similar in size and expected to perform with
    similar latencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how to set up MME:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the SageMaker web console at [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)
    with your AWS credentials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Models** under the **Inference** section of the left navigation panel.
    Then, click the **Create Model** button at the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a value for the **Model Name** field. This will be used to uniquely identify
    the target model in the context of SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an IAM role with the **AmazonSageMakerFullAccess** IAM policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the **Container definition** section, choose the **Multiple models**
    option and provide the location of the inference code image and the location of
    the model artifacts (see *Figure 9.2*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The Multi-modal endpoint configuration page of the SageMaker
    web console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – The Multi-modal endpoint configuration page of the SageMaker web
    console
  prefs: []
  type: TYPE_NORMAL
- en: The former field is used to deploy your models with a custom Docker image ([https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)).
    In this field, you should provide the image registry path where the images are
    located within Amazon ECR. The latter field specifies the S3 path where the model
    artifacts reside.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, fill in the **Container host name** field. This specifies details
    about the host where the inference code image will be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the **Create Model** button at the end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once SageMaker has been configured with MME, we can test the endpoint using
    `SageMaker.Client` from the `boto3` library as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `invoke_endpoint` function of the `SageMaker.Client`
    instance sends a request to the created endpoint. The `invoke_endpoint` function
    takes in `EndpointName` (the name of the created endpoint), `ContentType` (the
    type of data in the request body), `TargetModel` (the compressed model file in
    `.tar.gz` format; this is used to specify the target model which the request will
    be invoking), and `Body` (the input data in `ContentType`). The `response` variable
    that’s returned from the call consists of the prediction results. For the complete
    description of communicating with the endpoints, please look at [https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html](https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html).
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: 'a. SageMaker supports endpoint creation through its built-in `Model` class
    and the `Estimator` class. These classes support models that have been trained
    with various DL frameworks, including TF, PyTorch, and ONNX. `Model` classes designed
    specifically for TF and PyTorch frameworks also exist: `TensorFlowModel` and `PyTorchModel`.'
  prefs: []
  type: TYPE_NORMAL
- en: b. Once a model has been compiled using AWS SageMaker Neo, the model can exploit
    the underlying hardware resources better, demonstrating greater inference performance.
  prefs: []
  type: TYPE_NORMAL
- en: c. SageMaker can be configured to use an EI accelerator, reducing the operating
    cost for inference endpoints while improving the inference latency.
  prefs: []
  type: TYPE_NORMAL
- en: d. SageMaker includes an autoscaling feature that scales the endpoints up and
    down dynamically based on the volume of incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: e. SageMaker supports deploying multiple models on a single endpoint through
    MME.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we have described various features that SageMaker provides
    for deploying a DL model as an inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we described the two most popular AWS services designed for
    deploying a DL model as an inference endpoint: EKS and SageMaker. For both options,
    we started with the simplest setting: creating an inference endpoint from TF,
    PyTorch, or ONNX models. Then, we explained how to improve the performance of
    an inference endpoint using the EI accelerator, AWS Neuron, and AWS SageMaker
    Neo. We also covered how to set up autoscaling to handle the changes in the traffic
    more effectively. Finally, we discussed the MME feature of SageMaker that is used
    to host multiple models on a single inference endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at various model compression techniques:
    network quantization, weight sharing, network pruning, knowledge distillation,
    and network architecture search. These techniques will increase the inference
    efficiency even further.'
  prefs: []
  type: TYPE_NORMAL
