- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Deepfakes with GANs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GANs 进行 deepfakes
- en: Manipulating videos and photographs to edit artifacts has been in practice for
    quite a long time. If you have seen movies like *Forrest Gump* or *Fast and Furious
    7*, chances are you did not even notice that the scenes with John F. Kennedy or
    Paul Walker in their respective movies were fake and edited into the movies as required.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对视频和照片进行编辑以编辑文物已经有很长一段时间了。如果你看过《阿甘正传》或《速度与激情7》，很有可能你甚至没有注意到这些电影中约翰·F·肯尼迪或保罗·沃克的场景是假的，是根据需要编辑到电影中的。
- en: You may recall one particular scene from the movie *Forrest Gump*, where Gump
    meets John F. Kennedy. The scene was created using complex visual effects and
    archival footage to ensure high-quality results. Hollywood studios, spy agencies
    from across the world, and media outlets have been making use of editing tools
    such as Photoshop, After Effects, and complex custom visual effects/CGI (computer generated
    imagery) pipelines to come up with such compelling results. While the results
    have been more or less believable in most instances, it takes a huge amount of
    manual effort and time to edit each and every detail, such as scene lighting,
    face, eyes, and lip movements, as well as shadows, for every frame of the scene.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得电影《阿甘正传》中的一个场景，阿甘会见约翰·F·肯尼迪。该场景是使用复杂的视觉效果和档案素材创建的，以确保高质量的结果。好莱坞制片厂、世界各地的间谍机构和媒体机构一直在利用诸如
    Photoshop、After Effects 和复杂的自定义视觉效果/CGI（计算机生成图像）流水线等编辑工具来获得如此引人入胜的结果。虽然在大多数情况下结果或多或少是可信的，但要编辑每一个细节，如场景光线、面部、眼睛和唇部运动以及每一帧的阴影，需要大量的人工工作和时间。
- en: Along the same lines, there is a high chance you might have come across a Buzzfeed video¹
    where former US president Barack Obama says "Killmonger was right" (Killmonger
    is one of Marvel Cinematic Universe's villains). While obviously fake, the video
    does seem real in terms of its visual and audio aspects. There are a number of
    other examples where prominent personalities can be seen making comments they
    would usually not.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在 BuzzFeed 的一个视频中，你很有可能见过前美国总统巴拉克·奥巴马说“基尔蒙格是对的”（基尔蒙格是漫威电影宇宙的一个反派角色）。虽然显然是假的，但从视觉和音频方面看，视频似乎是真实的。还有许多其他例子，突出人物可以被看到发表他们通常不会说的评论。
- en: Keeping ethics aside, there is one major difference between Gump meeting John
    F. Kennedy and Barack Obama talking about Killmonger. As mentioned earlier, the
    former is the result of painstaking manual work done using complex visual effects/CGI.
    The latter, on the other hand, is the result of a technology called **deepfakes**.
    A portmanteau of the words *deep learning* and *fake*, *deepfake* is a broad term
    used to describe AI-enabled technology that is used to generate the examples we
    discussed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不考虑伦理问题，阿甘见到约翰·F·肯尼迪和巴拉克·奥巴马谈论基尔蒙格之间有一个主要区别。如前所述，前者是通过使用复杂的视觉效果/CGI进行的繁琐手工工作的结果。而后者，则是一种名为**deepfakes**的技术的结果。*deep
    learning*和*fake*的混成词，*deepfake*是一个广泛的术语，用于描述生成我们讨论的示例的 AI 能力技术。
- en: 'In this chapter, we will cover different concepts, architectures, and components
    associated with deepfakes. We will focus on the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖与 deepfakes 相关的不同概念、架构和组件。我们将重点关注以下主题：
- en: Overview of the deepfakes technological landscape
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度伪造技术景观概览
- en: 'The different forms of deepfaking: replacement, re-enactment, and editing'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deepfaking 的不同形式：替换、重新演绎和编辑
- en: Key features leveraged by different architectures
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同架构利用的关键特性
- en: A high-level deepfakes workflow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级 deepfakes 工作流程
- en: Swapping faces using autoencoders
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动编码器交换面孔
- en: Re-enacting Obama's face movements using pix2pix
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pix2pix 重新演绎奥巴马的面部动作
- en: Challenges and ethical issues
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战和道德问题
- en: A brief discussion of off-the-shelf implementations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于现成实现的简要讨论
- en: We will cover the internal workings of different GAN architectures and key contributions
    that have enabled deepfakes. We will also build and train these architectures
    from scratch to get a better understanding of them. Deepfakes are not limited
    to videos or photographs, but are also used to generate fake text (news articles,
    books) and even audio (voice clips, phone calls). In this chapter, we will focus
    on videos/images only and the term *deepfakes* refers to related use cases, unless stated
    otherwise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍不同GAN架构的内部工作原理和使deepfakes成为可能的主要贡献。我们还将从头构建和训练这些架构，以更好地理解它们。Deepfakes不仅限于视频或照片，还用于生成假文本（新闻文章，书籍）甚至语音（语音片段，电话录音）。在本章中，我们只关注视频/图像，术语*deepfakes*指相关用例，除非另有说明。
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中呈现的所有代码片段都可以直接在Google Colab中运行。出于空间原因，依赖项的导入语句未包含在内，但读者可以参考GitHub存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)。
- en: Let's begin with an overview of deepfakes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从deepfakes的概述开始。
- en: Deepfakes overview
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Deepfakes概述
- en: Deepfakes is an all-encompassing term representing content generated using artificial
    intelligence (in particular, deep learning) that seems realistic and authentic
    to a human being. The generation of fake content or manipulation of existing content
    to suit the needs and agenda of the entities involved is not new. In the introduction,
    we discussed a few movies where CGI and painstaking manual effort helped in generating
    realistic results. With advancements in deep learning and, more specifically,
    generative models, it is becoming increasingly difficult to differentiate between
    what is real and what is fake.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Deepfakes是一个总括性术语，代表使用人工智能（特别是深度学习）生成的内容，对人类看来真实和可信。生成假内容或操纵现有内容以适应参与方的需求和议程并不是什么新鲜事。在前言中，我们讨论了一些通过CGI和费力的手工努力来生成逼真结果的电影。随着深度学习和更具体地说，生成模型的进步，越来越难区分真实与虚假。
- en: '**Generative Adversarial Networks** (**GANs**) have played a very important
    role in this space by enabling the generation of sharp, high-quality images and
    videos. Works such as [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com),
    based on StyleGAN, have really pushed the boundaries in terms of the generation
    of high-quality realistic content. A number of other key architectures (some of
    which we discussed in *Chapter 6*, *Image Generation with GANs*, and *Chapter
    7*, *Style Transfer with GANs*) have become key building blocks for different
    deepfake setups.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）在这一领域发挥了非常重要的作用，使得能够生成清晰、高质量的图像和视频。诸如[https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)等基于StyleGAN的作品真正推动了生成高质量逼真内容的界限。其他一些关键架构（我们在*第6章*，*使用GAN生成图像*和*第7章*，*使用GAN进行风格转换*中讨论过）已成为不同deepfake设置的重要构建基块。'
- en: Deepfakes have a number of applications, which can be categorized into creative,
    productive, and unethical or malicious use cases. The following are a few examples
    that highlight the different use cases of deepfakes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Deepfakes有许多应用，可以分类为创造性、生产性和不道德或恶意的用例。以下是几个例子，突出了deepfakes的不同用例。
- en: 'Creative and productive use cases:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创造性和生产性用例：
- en: '**Recreating history and famous personalities**: There are a number of historical
    figures we would love to interact with and learn from. With the ability to manipulate
    and generate realistic content, deepfakes are just the right technology for such
    use cases. A large-scale experiment of this type was developed to bring famous
    surrealist painter Salvador Dali back to life. The Dali Museum, in collaboration
    with the ad agency GS&P, developed an exhibition entitled Dali Lives.² The exhibition
    used archival footage and interviews to train a deepfake setup on thousands of
    hours of videos. The final outcome was a re-enactment of Dali''s voice and facial
    expressions. Visitors to the museum were greeted by Dali, who then shared his
    life''s stories with them. Toward the end, Dali even proposed a selfie with the
    visitors, and the output photographs were realistic selfies indeed.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重现历史和著名人物**：有许多历史人物，我们很乐意与之互动并学习。有能力操纵和生成逼真内容的深度伪造技术正是这种用例所需的技术。一项大规模的此类实验是为了让著名超现实主义画家萨尔瓦多·达利重生。达利博物馆与广告公司
    GS&P 合作，开发了一个名为《达利永生》的展览。² 该展览利用存档的视频素材和访谈来训练一个深度伪造设置，观看者被达利所迎接，然后与他分享了他的一生故事。最后，达利甚至提出与观众自拍，输出的照片确实是逼真的自拍照。'
- en: '**Movie translation**: With the likes of Netflix becoming the norm these days,
    viewers are watching far more cross-lingual content than ever before. While subtitles
    and manual dubbing are viable options, they leave a lot to be desired. With deepfakes,
    using AI to autogenerate dubbed translations of any video is easier than ever.
    The social initiative known as *Malaria Must Die* created a powerful campaign
    leveraging a similar technique to help David Beckham, a famous footballer, speak
    in nine different languages to help spread awareness.³ Similarly, deepfakes have
    been used by a political party in India, where a candidate is seen speaking in
    different languages as part of his election campaign.⁴'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电影翻译**：随着 Netflix 等平台变得越来越普遍，观众们观看跨语言内容的次数比以往任何时候都要多。虽然字幕和手动配音是可行的选择，但它们还有很多需要改进的地方。利用深度伪造技术，使用人工智能自动生成任何视频的配音翻译比以往任何时候都更加容易。被称为
    *疟疾必须消灭* 的社会倡议利用了类似的技术，创建了一个强大的运动，帮助著名足球运动员大卫·贝克汉姆以九种不同的语言传播意识。³ 同样地，深度伪造技术已经被印度的一个政党使用，其中候选人在竞选活动中被看到使用不同的语言发言。⁴'
- en: '**Fashion**: Making use of GANs and other generative models to create new styles
    and fashion content is not new. With deepfakes, researchers, bloggers, and fashion
    houses are taking the fashion industry to new levels. We now have AI-generated
    digital models that are adorning new fashion line-ups and help in reducing costs.
    This technology is even being used to create renderings of models personalized
    to mimic a buyer''s body type, to improve the chances of a purchase.⁵'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时尚**：利用 GANs 和其他生成模型创建新的风格和时尚内容并不是什么新鲜事。随着深度伪造技术的出现，研究人员、博客作者和时尚品牌将时尚产业推向了新的高度。现在我们有了由人工智能生成的数字模特，她们穿着新的时尚系列，并帮助降低成本。这项技术甚至被用来创建可以个性化模仿买家体型的模特渲染，以提高购买的机会。⁵'
- en: '**Video game characters**: Video games have improved a lot over the years,
    with many modern games presenting cinema class graphics. Traditionally, human
    actors have been leveraged to create characters within such games. However, there
    is now a growing trend of using deepfakes and related technologies to develop
    characters and storylines. The developers of the game *Call of Duty* released
    a trailer showing former US president Ronald Reagan playing one of the characters
    in the game.⁶'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频游戏角色**：多年来，视频游戏已经取得了很大进步，许多现代游戏展现了电影级的图形。传统上，人类演员被利用来在这些游戏中扮演角色。然而，现在有一个越来越流行的趋势，就是利用深度伪造和相关技术来开发角色和故事情节。游戏
    *使命召唤* 的开发者发布了一段预告片，展示了前美国总统罗纳德·里根在游戏中扮演一个角色。⁶'
- en: '**Stock images**: Marketing flyers, advertisements, and official documents
    sometimes require certain individuals to be placed alongside the rest of the content.
    Traditionally, actual actors and models have been used. There are also stock image
    services that license such content for commercial use. With works such as [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com),
    it is now very easy to generate a new face or personality as per our requirements,
    without any actual actors or models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存图像**：营销传单、广告和官方文件有时需要把某些人放在其他内容旁边。传统上，使用实际的演员和模特。也有库存图像服务授权此类内容进行商业使用。有了[https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)这样的作品，现在非常容易根据我们的需求生成一个新的面孔或个性，而不需要任何实际的演员或模特。'
- en: 'Malicious use cases:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意使用案例：
- en: '**Pornography**: The ability to generate fake content as per our requirements
    has grave consequences. Indeed, deepfakes came into the public eye when, in 2017,
    a notorious fake pornographic video was posted by a Reddit user with a celebrity''s
    face swapped on.⁷ After this, there have been whole communities working toward
    generating such fake videos, which can be very damaging to the public image of
    the people they depict.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**色情内容**：根据我们的需求生成虚假内容具有严重后果。事实上，deepfakes引起公众注意时，是因为在2017年，一位Reddit用户发布了一个臭名昭著的伪造色情视频，视频中一位名人的面孔被替换了⁷。此后，已经有整个社区致力于生成这样的虚假视频，这对他们描绘的人物的公众形象可能造成非常严重的破坏。'
- en: '**Impersonation**: We''ve already discussed a fake video of former US president
    Barack Obama talking about a number of topics and things he would usually avoid.
    Creating such videos to impersonate public figures, politicians, and so on can
    have huge consequences.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冒充**：我们已经讨论过一个前美国总统巴拉克·奥巴马演讲的伪造视频，他谈论了许多他通常会避免的话题和事物。制作这样的视频来冒充公众人物、政治家等可能会产生巨大的后果。'
- en: Deepfakes entail realistic looking content that can be categorized into a number
    of subcategories. In the next section, we will present a discussion on the different
    categories to better understand the overall landscape.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Deepfakes包括类似真实的内容，可以被归类为多个子类别。在下一节中，我们将讨论不同类别，以更好地理解整体情况。
- en: Modes of operation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作模式
- en: Generating believable fake content requires taking care of multiple aspects
    to ensure that the results are as authentic as possible. A typical deepfake setup
    requires a **source**, **a target**, and **the generated content**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成可信的虚假内容需要注意多个方面，以确保结果尽可能地真实。典型的deepfake设置需要一个**源**、**一个目标**和**生成的内容**。
- en: The source, denoted with subscript *s*, is the driver identity that controls
    the required output
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源（用下标*s*表示）是控制所需输出的驱动身份。
- en: The target, denoted with subscript *t*, is the identity being faked
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标（用下标*t*表示）是正在伪造的身份。
- en: The generated content, denoted with subscript *g*, is the result following the
    transformation of the source to the target.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的内容（用下标*g*表示）是通过将源转换为目标得到的结果。
- en: Now that we have some basic terminology in place, let's dive deeper and understand
    different ways of generating fake content.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些基本术语，让我们深入了解生成虚假内容的不同方式。
- en: Replacement
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换
- en: 'This is the most widely used form of generating fake content. The aim is to
    replace specific content of the target (*x*[t]) with that from the source (*x*[s]).
    Face replacement has been an active area of research for quite some time now.
    *Figure 8.1* shows Donald Trump''s face being replaced with Nicolas Cage''s. The
    figure displays both source (*x*[s]) and target (*x*[t]) identities, while the
    generated content (*x*[g]) is shown in the last column:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成虚假内容的最常用形式。目的是用源（*x*[s]）的特定内容替换目标（*x*[t]）的内容。面部替换已经是一个长期以来的研究领域。*图8.1*显示了唐纳德·特朗普的面孔被尼古拉斯·凯奇的面孔替换的例子。图中展示了源（*x*[s]）和目标（*x*[t]）的身份，而生成的内容（*x*[g]）显示在最后一列：
- en: '![](img/B16176_08_01.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_01.png)'
- en: 'Figure 8.1: Face replacement⁸'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：面部替换⁸
- en: 'Replacement techniques can be broadly categorized into:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 替换技术可以大致分为以下几类：
- en: '**Transfer**: This is a basic form of replacement where the content of *x*[s]
    (for example, the face in the case of face replacement) is transferred to *x*[t].
    The transfer method is mostly leveraged in a coarse context, in other words, the
    replacement is not as clean or smooth as one would expect. For example, for clothes
    shopping, users might be interested in visualizing themselves in different outfits.
    Such applications can afford to leave out very detailed information yet still
    give users the required experience.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转移**：这是一种基本的替换形式，其中*x*[s]的内容（例如，脸部替换的情况下）转移到*x*[t]。转移方法在粗略的上下文中大多被利用，换句话说，替换并不如人们所期望的那样干净或平滑。例如，对于购物服装，用户可能有兴趣在不同的服装中进行可视化。这类应用可以省略非常详细的信息但仍然为用户提供所需的体验。'
- en: '**Swap**: This is a slightly more sophisticated type of replacement where the
    transfer to *x*[t] is guided by certain characteristics of *x*[t] itself. For
    instance, in *Figure 8.1*, the bottom row shows Nicolas Cage''s face getting swapped
    onto Donald Trump''s face. The replacement image maintains the characteristics
    of Trump''s (the target image''s) hair, pose, and so on.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交换**：这是一种略微复杂一点的替换类型，其中对*x*[t]的转移受到*x*[t]本身特定特征的引导。例如，在*图8.1*中，底部一行显示了尼古拉斯·凯奇的脸被换到唐纳德·特朗普的脸上。替换图像保持了特朗普（目标图像）的头发、姿势等特征。'
- en: The replacement mode, despite sounding trivial, is not so simple, since any
    models need to focus on a number of factors relating to image lighting, skin colors,
    occlusions, and shadows. The handling of some of these aspects will be discussed
    in later sections of this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 替换模式，尽管听起来很琐碎，但并不简单，因为任何模型都需要关注与图像照明、肤色、遮挡和阴影相关的许多因素。本章后面的部分将讨论其中一些方面的处理。
- en: Re-enactment
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新表演
- en: 'Replacement methods yield impressive results, but the generated content leaves
    scope for improvement. Re-enactment methods are utilized to capture characteristics
    such as the pose, expression, and gaze of the target to improve upon the believability
    of the generated content. Re-enactment techniques focus on the following aspects
    to improve the quality of the fake content:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 替换方法产生令人印象深刻的结果，但生成的内容仍有改进的空间。再演绎方法被用来捕捉目标的姿势、表情和凝视等特征，以改进生成内容的可信度。再演绎技术侧重于以下方面以提高虚假内容的质量：
- en: '**Gaze**: The aim is to focus on the eyes and the position of the eyelids.
    Techniques in this area try to re-enact the generated output''s gaze based on
    the source''s eye movements/gaze. This is useful in improving photographs or maintaining
    eye contact in videos.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**凝视**：重点是关注眼睛和眼皮的位置。该领域的技术试图根据源的眼部运动/凝视重新演绎生成输出的凝视。这对于改进照片或保持视频中的眼神联系非常有用。'
- en: '**Mouth**: Re-enacting lips and the mouth region of a face improves the believability
    of the generated content. In this case, the mouth movements of *x*[t] are conditioned
    on the mouth movements of *x*[s]. In certain cases, the source input *x*[s] could
    be speech or other audio. Mouth re-enactment methods are also called Bol methods.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**口部**：重新演绎面部的嘴唇和口部区域可以提高生成内容的可信度。在这种情况下，*x*[t]的口部运动取决于*x*[s]的口部运动。在某些情况下，源输入*x*[s]可能是语音或其他音频。口部重新演绎方法也被称为Bol方法。'
- en: '**Expression**: This is a more generic form of re-enactment that often includes
    other re-enactment aspects such as the eyes, mouth, and pose. These are used to
    drive the expression of *x*[t] on the basis of *x*[s].'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表情**：这是再演绎的一种更通用形式，通常包括其他再演绎方面，如眼睛、嘴巴和姿势。它们被用来根据*x*[s]驱动*x*[t]的表达。'
- en: '**Pose**: Pose re-enactments, for both the head and the body, are all-encompassing
    methods that consider the positioning of the head and the whole body. In this
    case as well, the source drives the target and yields more believable results.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**姿势**：姿势再现，无论是头部还是身体，都是一种全面考虑头部和整个身体定位的方法。在这种情况下，源驱动目标，并产生更有信服力的结果。'
- en: These re-enactments are better depicted in *Figure 8.2*, where we have the source
    (*x*[s]) and target (*x*[t]) shown on the left of the figure. The right side of
    the figure shows how different aspects of the source impact the generated content.
    Please note that *Figure 8.2* is for illustrative purposes only and the results
    are not mere copy and paste editing of the target content. We will see more evolved
    examples as we progress through the chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些再现在 *图8.2* 中得到了更好的描述，在图的左侧我们有源（*x*[s]）和目标（*x*[t]），图的右侧显示了源的不同方面如何影响生成的内容。请注意，*图8.2*
    仅用于举例说明，结果并不仅仅是目标内容的简单复制粘贴。随着我们在本章中的深入，我们还会看到更进化的例子。
- en: '![](img/B16176_08_02.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_02.png)'
- en: 'Figure 8.2: Re-enactment methods. Impacted regions are highlighted for each
    re-enactment'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.2: 再现方法。受影响的区域在每次再现中都有所突出'
- en: Specific regions that are the focus of different types of re-enactments have
    been highlighted specifically in *Figure 8.2*. As mentioned earlier, it is quite
    apparent that expression re-enactments encompass the eye and mouth regions as
    well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 特定地区虚拟不同类型的再现在 *图8.2* 中被特别突出。 正如前面提到的，很显然表情再现包括眼部和口部区域。
- en: Editing
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编辑
- en: 'Deepfakes do not necessarily concern replacement or re-enactment. Another application
    of deepfakes is to add, remove, or alter certain aspects of the target entity
    to serve specific objectives. Editing could involve manipulation of clothing,
    age, ethnicity, gender, hair, and so on. A few possible edits are depicted in
    the following figure:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '深伪造并不一定涉及替换或再现。深伪造的另一个应用是为了实现特定目标而添加、删除或更改目标实体的某些方面。编辑可能涉及对服装、年龄、种族、性别、头发等进行操纵。以下图示了一些可能的编辑:'
- en: '![](img/B16176_08_03.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_03.png)'
- en: 'Figure 8.3: Deepfakes in Edit mode. The left image is the base input for transformation.
    The right image depicts three different edits: hair, spectacles, and age.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.3: 编辑模式下的深伪造。左图是变换的基础输入。右图展示了三种不同的编辑: 头发、眼镜和年龄。'
- en: The edits on the right-hand side of *Figure 8.3* showcase how certain attributes
    of the input image can be transformed to generate fake content. There are a number
    of benign use cases that are either for fun (apps such as **FaceApp** and **REFACE**)
    or have commercial value (eyewear and cosmetics brands). Yet there are a number
    of malicious applications (pornography, fake identities, and so on) that undermine
    and raise questions about the use of such tools.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.3* 右侧的编辑展示了如何将输入图像的某些属性转换为生成虚假内容。有许多良性的用例，要么是为了好玩（如**FaceApp**和**REFACE**这样的应用程序），要么具有商业价值（眼镜和化妆品品牌）。然而，也有许多恶意的应用程序（色情作品、虚假身份等），这些恶意应用程序削弱并引发了对此类工具的使用引发的问题。'
- en: We have covered the basics of the different modes of generating fake content
    and discussed the major areas of focus for each of the modes. In the next section,
    we will discuss what features play a role in training such models and how we leverage
    them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了生成虚假内容的不同模式的基础知识，并讨论了每种模式的主要关注领域。在下一节中，我们将讨论在训练此类模型中发挥作用的特征以及我们如何利用它们。
- en: Key feature set
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键特征集
- en: The human face and body are key entities in this task of fake content generation.
    While deep learning architectures usually do not require hand-crafted features,
    a little nudge goes a long way when complex entities are involved. Particularly
    when dealing with the human face, apart from detecting the overall face in a given
    image or video, a deepfake solution also needs to focus on the eyes, mouth, and
    other features. We discussed different modes of operation in the previous section,
    where we highlighted the importance of different sections of a face and their
    impact on improving the believability of the fake content generated.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸和身体是虚假内容生成任务中的关键实体。尽管深度学习架构通常不需要手工制作的特征，在处理复杂实体时，小小的推动会产生深远影响。特别是在处理人脸时，除了在给定图像或视频中检测整体脸部之外，深伪造解决方案还需要关注眼睛、嘴巴和其他特征。在上一节中，我们讨论了不同的操作模式，强调了脸部不同部分的重要性以及它们对改善所生成虚假内容的可信度的影响。
- en: 'In this section, we will briefly cover a few important features leveraged by
    different deepfake solutions. These are:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将简要介绍一些不同深伪造解决方案利用的重要特征。这些特征包括:'
- en: Facial Action Coding System (FACS)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部行为编码系统 (FACS)
- en: 3D Morphable Model (3DMM)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三维可塑模型 (3DMM)
- en: Facial landmarks
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部标志
- en: We will also undertake a couple of hands-on exercises to better understand these
    feature sets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将进行一些实际操作，以更好地理解这些特征集。
- en: Facial Action Coding System (FACS)
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部动作编码系统（FACS）
- en: Developed by Carl-Herman Hjortsjö in 1969, and later adopted and refined by
    Ekamn et al. in 1978, Facial Action Coding System, or FACS, is an anatomy-based
    system for understanding facial movements. It is one of the most extensive and
    accurate coding systems for analyzing facial muscles to understand expressions
    and emotions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 胡尔特绍在1969年开发了FACS，并在1978年由埃克曼等人采用和完善。面部动作编码系统，或FACS，是一种基于解剖学的系统，用于理解面部运动。它是用于分析面部肌肉以理解表情和情绪的最详尽和准确的编码系统之一。
- en: '*Figure 8.4* depicts a few specific muscle actions and their associated meanings.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.4*描述了一些特定的肌肉动作及其相关含义。'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_08_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![一个手机截图  自动生成的描述](img/B16176_08_04.png)'
- en: 'Figure 8.4: A sample set of action marking using FACS'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：使用FACS进行动作标记的样本集
- en: FACS consists of a detailed manual that is used by human coders to manually
    code each facial expression. The muscular activities are grouped into what are
    called Action Units, or AUs. These AUs represent muscular activities corresponding
    to facial expressions. A few sample AUs are described in *Figure 8.4*, pointing
    to the movement of eyebrows, lips, and other parts of the face.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: FACS包括一份详细的手册，由人类编码员用来手动编码每个面部表情。肌肉活动被分为被称为行动单元（AU）的组。这些AU代表与面部表情对应的肌肉活动。 *图8.4*
    描述了一些示例AU，指向眉毛、嘴唇和面部其他部分的运动。
- en: 'Although the original FACS system required human coders, there are automated
    systems now available to computationally determine the correct AUs. Works such
    as the following leverage automated AUs to generate realistic results:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初的FACS系统需要人工编码员，现在已经有自动化系统可计算确定正确的AU。像下面这样的作品利用自动的AU生成逼真的结果：
- en: '*GANimation: Anatomically-aware* *Facial Animation from a Single Image*⁹'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GANimation:解剖学感知的* *来自单个图像的面部动画*⁹'
- en: '*High-Resolution* *Face Swapping for Visual Effects*^(10)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视觉效果的高分辨率* *人脸换装*^(10)'
- en: '*3D* *Guided Fine-Grained Face Manipulation*^(11)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*3D* *引导的细粒度人脸操作*^(11)'
- en: Even though FACS provides a fine-grained understanding of a given face's expressions,
    the complexity of the overall system limits its usage outside of professional
    animation/CGI/VFX studios.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FACS可以对给定面部表情提供细粒度的理解，但整个系统的复杂性限制了它在专业动画/CGI/VFX工作室之外的使用。
- en: 3D Morphable Model
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D可塑模型
- en: 3D Morphable Model, or 3DMM for short, is a method of inferring a complete 3D
    facial surface from a 2D image. Originally proposed by Blanz, Vetter, et al. in
    their work entitled *A Morphable Model for the Synthesis of 3D Faces*,^(12) this
    is a powerful statistical method that can model human face shape and texture along
    with pose and illumination.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 三维可塑模型，简称3DMM，是一种从2D图像推断完整三维面部表面的方法。最初由Blanz、Vetter等人在其名为*用于合成3D面部的可塑模型*的作品中提出^(12)，这是一种强大的统计方法，可以模拟人脸形状和质地以及姿势和照明。
- en: The technique works by transforming the input image into a face mesh. The face
    mesh consists of vertices and edges that determine the shape and texture of each
    section of the face. The mesh helps in parameterizing the pose and expressions
    with a set of vectors and matrices. These vectors, or the 3D reconstruction itself,
    can then be used as input features for our fake content generation models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术通过将输入图像转换为面部网格来工作。面部网格由确定面部每个部分的形状和质地的顶点和边组成。网格有助于使用一组向量和矩阵对姿势和表情进行参数化。然后，这些向量或3D重建本身可以用作我们的伪造内容生成模型的输入特征。
- en: Facial landmarks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部标记
- en: FACS and 3DMM-based features are highly accurate and expressive in terms of
    defining the characteristics of a human face (and body in general). Yet these
    methods are computationally expensive and sometimes even require human intervention
    (for example, FACS coding) for proper results. Facial landmarks are another feature
    set which are simple yet powerful, and are being used by a number of recent works
    to achieve state-of-the-art results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 基于FACS和3DMM的特征在定义人脸（以及总体身体）特征方面具有高度准确性和表现力。然而，这些方法在计算方面是昂贵的，有时甚至需要人类干预（例如，FACS编码）才能得到良好的结果。面部标记是另一种特征集，简单而强大，并且被一些最近的作品使用以取得最先进的结果。
- en: Facial landmarks are a list of important facial features, such as the nose,
    eyebrows, mouth, and the corners of the eyes. The goal is the detection of these
    key features using some form of a regression model. The most common method is
    to leverage a predefined set of positions on the face or body that can be efficiently
    tracked using trained models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'A facial landmark detection task can be broken down into the following two-step
    approach:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The first step involves localization of a face (or faces) in a given image
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second step goes granular to identify key facial structures of the identified
    face(s)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two steps can be thought of as special cases of shape prediction. There
    are a couple of different methods we can use to detect facial landmarks as features
    for the task of fake content generation. In the following subsections, we will
    cover three of the most widely used methods: OpenCV, dlib, and MTCNN.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection using OpenCV
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV is a computer vision library aimed at handling real-time tasks. It is
    one of the most popular and widely used libraries, with wrappers available in
    a number of languages, Python included. It consists of a number of extensions
    and contrib-packages, such as the ones for face detection, text manipulation,
    and image processing. These packages enhance its overall capabilities.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection can be performed using OpenCV in a few different ways.
    One of the ways is to leverage Haar Cascade filters, which make use of histograms
    followed by an SVM for object detection. OpenCV also supports a DNN-based method
    of performing the same task.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection using dlib
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dlib is another cross-platform library which provides more or less similar functionality
    to OpenCV. The major advantage dlib offers over OpenCV is a list of pretrained
    detectors for faces as well as landmarks. Before we get onto the implementation
    details, let's learn a bit more about the landmark features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks are granular details on a given face. Even though each face
    is unique, there are certain attributes that help us to identify a given shape
    as a face. This precise list of common traits is codified into what is called
    the **68-coordinate** or **68-point system**. This point system was devised for
    annotating the iBUG-300W dataset. This dataset forms the basis of a number of
    landmark detectors available through dlib. Each feature is given a specific index
    (out of 68) and has its own (*x*, *y*) coordinates. The 68 indices are indicated
    in *Figure 8.5*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_08_05.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The 68-point annotations from the iBUG-300W dataset'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in the figure, each index corresponds to a specific coordinate and
    a set of indices mark a facial landmark. For instance, indices 28-31 correspond
    to the nose bridge and the detectors try to detect and predict the corresponding
    coordinates for those indices.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up dlib is a bit of an involved process, especially if you are on a
    Windows machine. Refer to setup guides such as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 dlib 是一个有点复杂的过程，特别是如果你使用的是 Windows 机器。请参考以下设置指南：
- en: '[https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/](https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/](https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/)'
- en: '[https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117f](https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117f](https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117)'
- en: 'Let''s now leverage this 68-coordinate system of facial landmarks to develop
    a short demo application for detecting facial features. We will make use of pretrained
    detectors from dlib and OpenCV to build this demo. The following snippet shows
    how a few lines of code can help us identify different facial landmarks easily:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用这个 68 个面部地标的坐标系来开发一个简短的演示应用程序，用于检测面部特征。我们将利用 dlib 和 OpenCV 中的预训练检测器来构建这个演示。以下代码片段显示了如何用几行代码轻松地识别不同的面部地标：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code takes in an image of a face as input, converts it to grayscale,
    and marks the aforementioned 68 points onto the face using a dlib detector and
    predictor. Once we have these functions ready, we can execute the overall script.
    The script pops open a video capture window. The video output is overlaid with
    facial landmarks, as shown in *Figure 8.6*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码接受面部图像作为输入，将其转换为灰度，并使用 dlib 检测器和预测器在脸上标出上述 68 个点。一旦我们准备好这些功能，就可以执行整个脚本。该脚本弹出一个视频捕获窗口。视频输出叠加了面部地标，如*图
    8.6*所示：
- en: '![](img/B16176_08_06.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_06.png)'
- en: 'Figure 8.6: A sample video capture with facial landmark detection using pretrained
    detectors'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：使用预训练检测器进行面部地标检测的样本视频捕获
- en: As you can see, the pretrained facial landmark detector seems to be doing a
    great job. With a few lines of code, we were able to get specific facial features.
    In the later sections of the chapter, we will leverage these features for training
    our own deepfake architectures.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所看到的，预训练的面部地标检测器似乎做得很好。通过几行代码，我们能够获得具体的面部特征。在本章的后续部分，我们将利用这些特征来训练我们自己的深度伪造架构。
- en: Facial landmark detection using MTCNN
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MTCNN 进行面部地标检测
- en: There are a number of alternatives to OpenCV and dlib for face and facial landmark
    detection tasks. One of the most prominent and well performing ones is called
    **MTCNN**, short for **Multi-Task Cascaded Convolutional Networks**. Developed
    by Zhang, Zhang et al.,^(13) MTCNN is a complex deep learning architecture consisting
    of three cascaded networks. Together, these three networks help with the tasks
    of face and landmark identification. A discussion of the details of MTCNN are
    beyond the scope of this book, but we will briefly talk about its salient aspects
    and build a quick demo. Interested readers are requested to go through the original
    cited work for details.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于面部和面部地标检测任务，OpenCV 和 dlib 有很多替代方案。其中最突出、表现最好的之一叫做**MTCNN**，全称**多任务级联卷积网络**。由张,
    张等^(13)开发，MTCNN 是一个由三个级联网络组成的复杂深度学习架构。这三个网络共同帮助完成面部和地标识别的任务。对于 MTCNN 的详细讨论超出了本书的范围，但我们将简要介绍其显著的方面并构建一个快速演示。有兴趣的读者请阅读原始引用的工作了解详情。
- en: The MTCNN setup, as mentioned earlier, makes use of three cascaded networks
    called P-Net, R-Net, and O-Net. Without going into much detail, the setup first
    builds a pyramid of the input image, i.e. the input image is scaled to different
    resolutions. The Proposal-Net, or P-Net, then takes these as input and outputs
    a number of potential bounding boxes that might contain a face. With some pre-processing
    steps in between, the Refine-Net, or R-Net, then refines the results by narrowing
    them down to the most probable bounding boxes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，MTCNN 设置使用了三个级联网络，称为 P-Net、R-Net 和 O-Net。不多说，设置首先建立了输入图像的金字塔，即将输入图像缩放到不同的分辨率。然后，提议网络（P-Net）将其作为输入，并输出可能包含面部的潜在边界框。在中间进行一些预处理步骤后，精化网络（R-Net）通过将其缩小到最可能的边界框来精化结果。
- en: The final output is generated by Output-Net, or O-Net. O-Net outputs the final
    bounding boxes containing faces, along with landmark coordinates for the eyes,
    nose, and mouth.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出由 Output-Net 或 O-Net 生成。O-Net 输出包含面部的最终边界框，以及眼睛、鼻子和嘴巴的地标坐标。
- en: 'Let''s now try out this state-of-the-art architecture to identify faces and
    corresponding landmarks. Luckily for us, MTCNN is available as a pip package,
    which is straightforward to use. In the following code listing, we will build
    a utility function to leverage MTCNN for our required tasks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一下这个最先进的架构来识别脸部和相应的标志。幸运的是，MTCNN可以作为一个pip软件包，非常容易使用。在下面的代码清单中，我们将构建一个实用函数来利用MTCNN进行我们所需的任务：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As showcased in the code listing, the predictions from the MTCNN detector outputs
    two items for each detected face – the bounding box for the face and five coordinates
    for each facial landmark. Using these outputs, we can leverage OpenCV to add markers
    on the input image to visualize the predictions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如代码清单所展示的，MTCNN检测器的预测为每个检测到的脸部输出两个项目 - 脸部的边界框和每个面部标志的五个坐标。利用这些输出，我们可以利用OpenCV在输入图像上添加标记以可视化预测。
- en: '*Figure 8.7* depicts the sample output from this exercise.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.7*展示了这次练习的样本输出。'
- en: '![](img/B16176_08_07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_07.png)'
- en: 'Figure 8.7: MTCNN-based face and facial landmark detection'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：基于MTCNN的人脸和面部标志检测
- en: As shown in the figure, MTCNN seems to have detected all the faces in the image
    along with the facial landmarks properly. With a few lines of code, we were able
    to use a state-of-the-art complex deep learning network to quickly generate the
    required outputs. Similar to the dlib/OpenCV exercise in the previous section,
    we can leverage MTCNN to identify key features which can be used as inputs for
    our fake content generation models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图所示，MTCNN似乎已经正确地检测出图像中的所有面部以及面部标志。我们只需几行代码，就能够使用最先进的复杂深度学习网络快速生成所需的输出。类似于上一节中的dlib/OpenCV练习，我们可以利用MTCNN识别关键特征，这些特征可以用作我们虚假内容生成模型的输入。
- en: Another easy-to-use deep learning-based library for face detection and recognition
    is called `face_recognition`. This is a pip-installable package that provides
    straightforward APIs for both the tasks. For the task of face recognition (where
    the primary aim is to identify a person apart from just detecting a face), it
    makes use of VGGFace. VGGFace is a deep learning architecture developed by the
    Visual Geometry Group at Oxford University. It makes use of a VGG-style backbone
    to extract facial features. These features can then be leveraged for similarity
    matching. We will make use of this package in later sections of the chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个易于使用的基于深度学习的人脸检测和识别库名为`face_recognition`。这是一个可以通过pip进行安装的软件包，为这两项任务提供了直接的API。对于人脸识别的任务（主要目的是识别一个人而不仅仅是检测脸部），它使用了VGGFace。VGGFace是牛津大学视觉几何组开发的深度学习架构。它使用了VGG风格的主干来提取面部特征。这些特征然后可以用于相似性匹配。我们将在本章的后续部分使用这个软件包。
- en: Now that we have developed an understanding of different modes, along with different
    ways of identifying and extracting relevant features, let's get started with building
    a few such architectures of our own from scratch. In the coming sections, we will
    discuss a high-level flow for building a deepfake model and common architectures
    employed for this purpose, followed by hands-on training of a couple of these
    from scratch.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同模式以及识别和提取相关特征的不同方式，让我们开始从头开始构建自己的一些这样的架构。在接下来的部分中，我们将讨论构建深伪造模型的高级流程以及为此目的使用的常见架构，接着从头开始操作训练其中几个。
- en: High-level workflow
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级工作流程
- en: Fake content generation is a complex task consisting of a number of components
    and steps that help in generating believable content. While this space is seeing
    quite a lot of research and hacks that improve the overall results, the setup
    can largely be explained using a few common building blocks. In this section,
    we will discuss a common high-level flow that describes how a deepfake setup uses
    data to train and generate fake content. We will also touch upon a few common
    architectures used in a number of works as basic building blocks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假内容生成是一个复杂的任务，包含许多组件和步骤来帮助生成可信的内容。虽然这个领域正在经历很多改进整体结果的研究和技巧，但整体设置主要可以用一些常见的构造块来解释。在本节中，我们将讨论一个描述深伪造设置如何使用数据来训练和生成虚假内容的常见高级流程。我们还将简要介绍一些在许多作品中用作基础构造块的常见架构。
- en: 'As discussed earlier, a deepfake setup requires a source identity (*x*[s])
    which drives the target identity (*x*[t]) to generate fake content (*x*[g]). To
    understand the high-level flow, we will continue with this notation, along with
    the concepts related to the key feature set discussed in the previous section.
    The steps are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，深度伪造设置需要一个源身份（*x*[s]），它驱动目标身份（*x*[t]）生成虚假内容（*x*[g]）。为了理解高级流程，我们将继续使用这个符号，以及与前一节讨论的关键特征集相关的概念。步骤如下：
- en: '**Input processing**'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入处理**'
- en: The input image (*x*[s] or *x*[t]) is processed using a face detector that identifies
    and crops the face
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用识别和裁剪面孔的面部检测器处理输入图像（*x*[s]或*x*[t]）。
- en: The cropped face is then used to extract intermediate representations or features
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用裁剪的面孔来提取中间表示或特征。
- en: '**Generation**'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**'
- en: The intermediate representation along with a driving signal (*x*[s] or another
    face) is used to generate a new face
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间表示（*x*[s]或另一张脸）与驱动信号一起用于生成新的面孔。
- en: '**Blending**'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**'
- en: A blending function then merges the generated face into the target as cleanly
    as possible
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，混合函数将生成的面孔尽可能清晰地合并到目标中。
- en: 'Respective works employ additional interim or post-processing steps to improve
    the overall results. *Figure 8.8* depicts the main steps in detail:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的工作采用额外的中间或后处理步骤来改善整体结果。*图8.8*详细描述了主要步骤：
- en: '![](img/B16176_08_08.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_08.png)'
- en: 'Figure 8.8: High-level flow for creating deepfakes'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：创建深度伪造的高级流程
- en: As shown in the figure, we use a photograph of Nicolas Cage as input and transform
    him into a fake photograph resembling Donald Trump. The key components used for
    each of these steps could be any of the various components presented so far in
    the chapter. For instance, the face crop step could leverage either dlib or MTCNN,
    and similarly, the key features used for the generation process could be either
    FACS AUs, facial landmarks, or the 3DMM vectors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们使用尼古拉斯·凯奇的照片作为输入，并将其转换为一个类似于唐纳德·特朗普的假照片。每个步骤中使用的关键组件可以是本章中已经介绍过的各种组件中的任何一个。例如，面部裁剪步骤可以利用dlib或MTCNN，同样，用于生成过程的关键特征可以是FACS
    AUs、面部标志或3DMM向量中的任何一个。
- en: So far, we have covered aspects related to face cropping and key features which
    can be used in this fake content generation process. The next step in this process
    of deepfakes is the final output image, or video generation. Generative modeling
    is something we have covered in quite some depth in previous chapters, from variational
    autoencoders to different types of GANs. For the task of fake content generation,
    we will build upon these architectures. Readers should note that the deepfakes
    task is a special case, or rather a restricted use case, of different models we have
    covered in these previous chapters.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了与面部裁剪和关键特征相关的方面，这些特征可以在这个虚假内容生成过程中使用。深度伪造的下一步是最终输出图像或视频的生成。生成建模是我们在以前的章节中已经相当深入讨论过的内容，从变分自编码器到不同类型的GANs。对于虚假内容生成的任务，我们将建立在这些架构之上。读者应该注意，deepfakes
    任务是我们在这些先前章节中涵盖的不同模型的特殊情况，或者说是受限制的用例。
- en: Let's now have a look at some of the most common architectures that are leveraged
    by different deepfake works.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下一些不同 deepfake 作品中常用的一些架构。
- en: Common architectures
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见架构
- en: Most deepfake setups leverage known architectures with certain tweaks as building
    blocks for generating fake content. We have discussed most of these architectures
    in detail in *Chapters* *4*, *5*, *6*, and *7*. The following is a brief reiteration
    of the most commonly leveraged architectures for generating images or videos.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 deepfake 设置利用已知的架构并进行一定的调整，作为生成虚假内容的构建模块。我们在*第4、5、6和7章*中详细讨论了大多数这些架构。以下是用于生成图像或视频的最常用架构的简要重述。
- en: Encoder-Decoder (ED)
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器（ED）
- en: 'An encoder-decoder architecture consists of two components, an encoder and
    a decoder. The encoder component consists of a sequence of layers that start from
    the actual higher dimensional input, such as an image. The encoder then narrows
    the input down to a lower dimensional space or a vector representation, rightly
    termed **bottleneck features**. The decoder component takes the bottleneck features
    and decodes or transforms them to a different or the same vector space as the
    input. A typical ED architecture is depicted here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构包括两个组件，编码器和解码器。编码器组件由一系列层组成，从实际的高维输入（如图像）开始。然后将输入缩小到较低维度空间或向量表示，称为**瓶颈特征**。解码器组件需要瓶颈特征，并将其解码或转换为不同或相同的向量空间作为输入。典型的ED架构如下所示：
- en: '![A picture containing monitor, screen, clock  Description automatically generated](img/B16176_08_09.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![包含显示器、屏幕、钟表的图片  自动生成描述](img/B16176_08_09.png)'
- en: 'Figure 8.9: A typical encoder-decoder architecture'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：典型的编码器-解码器架构
- en: A special case of ED architecture is called an **autoencoder**. An autoencoder
    takes the input, transforms it into bottleneck features, and then reconstructs
    the original input as the output. Such networks are useful in learning feature
    representation on inputs. Another variant of ED architecture is called the **variational
    autoencoder**, or VAE. A VAE learns the posterior distribution of the decoder
    given the input space. We have seen in *Chapter 5*, *Painting Pictures with Neural
    Networks using VAEs*, how VAEs are better at learning and untangling representations,
    and better at generating content overall (as compared to autoencoders).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器解码器架构的特殊情况被称为**自编码器**。自编码器将输入转换为瓶颈特征，然后将原始输入重构为输出。这样的网络在学习输入特征表示上很有用。编码器-解码器架构的另一种变体称为**变分自编码器**，或者VAE。VAE学习给定输入空间的解码器的后验分布。在*第5章*，*使用VAE的神经网络绘画图像*中，我们看到VAE在学习和解开表示方面更好，并且在总体上生成内容更好（与自编码器相比）。
- en: Generative Adversarial Networks (GANs)
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: GANs are implicit density modeling networks which have been used to generate
    very high-quality outputs in recent works. Without going into much detail, a GAN
    setup consists of two competing models, a generator and a discriminator. The generator
    is tasked with generating real-looking content based on a driving signal (noise
    vector, conditional inputs, and so on). The discriminator, on the other hand,
    is tasked with distinguishing fake from real. The two networks play a minimax
    game until we achieve an equilibrium state, with the generator being able to generate
    good enough samples to fool the discriminator.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是隐式密度建模网络，在最近的研究中被用来生成非常高质量的输出。不详细介绍，GAN的设置包括两个竞争模型，一个生成器和一个鉴别器。生成器的任务是根据驱动信号（噪音向量，条件输入等）生成看起来真实的内容。另一方面，鉴别器的任务是区分假的和真的。两个网络进行最小最大博弈，直到达到均衡状态，生成器能够生成足够好的样本来愚弄鉴别器。
- en: 'A typical GAN is depicted in *Figure 8.10*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的GAN如*图8.10*所示：
- en: '![A picture containing drawing  Description automatically generated](img/B16176_08_10.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![包含绘画的图片  自动生成描述](img/B16176_08_10.png)'
- en: 'Figure 8.10: A typical GAN architecture'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：典型的GAN架构
- en: GANs are effective at generating high-quality outputs, and they have been the
    subject of significant research over the years. Improvements have led to some
    really powerful variants that have pushed the boundaries even further. Two of
    the most widely used variants in the context of deepfakes are **CycleGAN** and
    **pix2pix**. Both architectures were designed for image-to-image translation tasks.
    Pix2pix is a paired translation network, while CycleGAN does not require any pairing
    of the training samples. The effectiveness and simplicity of both these architectures
    make them perfect candidates for the task of deepfakes. We discussed both these
    architectures in detail in *Chapter 7*, *Style Transfer with GANs*; we encourage
    you to take a quick look at the previous chapter for a better understanding of
    the remaining sections in this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 在生成高质量输出方面非常有效，并且多年来一直是重要研究的对象。改进已经带来了一些真正强大的变体，使其进一步推动了边界。在深度伪造的上下文中，两个最广泛使用的变体是**CycleGAN**和**pix2pix**。两种架构都是为图像到图像的转换任务设计的。Pix2pix
    是一个配对翻译网络，而 CycleGAN 不需要对训练样本进行任何配对。这两种架构的有效性和简单性使它们成为深度伪造任务的完美候选者。我们在*第7 章*，*使用
    GANs 进行风格转移*中详细讨论了这两种架构；我们鼓励你快速浏览前一章，以更好地理解本章剩余部分。
- en: We have covered all the required building blocks in fair detail so far. Let's
    now leverage this understanding to implement a couple of deepfake setups from
    scratch.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经相当详细地涵盖了所有所需的基本组件。现在让我们利用这种理解，从头开始实现几个深度伪造的设置。
- en: Replacement using autoencoders
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动编码器进行替换
- en: Deepfakes are an interesting and powerful use of technology that is both useful
    and dangerous. In previous sections, we discussed different modes of operations
    and key features that can be leveraged, as well as common architectures. We also
    briefly touched upon the high-level flow of different tasks required to achieve
    the end results. In this section, we will focus on developing a face swapping
    setup using an autoencoder as our backbone architecture. Let's get started.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造是技术的一个有趣而强大的应用，既有用又危险。在前几节中，我们讨论了可以利用的不同操作模式和关键特性，以及常见的架构。我们还简要涉及了实现最终结果所需的不同任务的高层流程。在这一节中，我们将专注于使用自动编码器作为我们的主要架构来开发面部交换设置。让我们开始吧。
- en: Task definition
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务定义
- en: The aim of this exercise is to develop a face swapping setup. As discussed earlier,
    face swapping is a type of replacement mode operation in the context of deepfake
    terminology. In this setup, we will focus on transforming Nicolas Cage (a Hollywood
    actor) into Donald J. Trump (former US president). In the upcoming sections, we
    will present each sub-task necessary for the preparation of data, training our
    models, and finally, the generation of swapped fake output images.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习的目标是开发一个面部交换设置。如前所述，面部交换是深度伪造术语下的一种替换模式操作。在此设置中，我们将专注于将好莱坞演员尼古拉斯·凯奇 (Nicolas
    Cage) 变身为前美国总统唐纳德·J·特朗普 (Donald J. Trump)。在接下来的章节中，我们将介绍为准备数据、训练模型以及最终生成交换的假图片而需要完成的每个子任务。
- en: Dataset preparation
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集准备
- en: The first and foremost task is data preparation. Since the aim is to develop
    a face swapper for Nicolas Cage and Donald Trump, we need datasets containing
    images of each of them. This task of data collection itself can be time-consuming
    and challenging for a number of reasons. Firstly, photographs could be restricted
    by licensing and privacy issues. Secondly, it is challenging to find good quality
    datasets that are publicly available. Finally, there is the challenge associated
    with identifying specific faces in photographs, as there could be multiple faces
    in a given photograph belonging to different people.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首要任务是数据准备。由于目标是为尼古拉斯·凯奇和唐纳德·特朗普开发一个面部交换器，我们需要包含每个人图像的数据集。出于许多原因，这个数据收集任务本身可能非常耗时和具有挑战性。首先，照片可能受到许可和隐私问题的限制。其次，公开可用的高质量数据集很难找到。最后，在照片中识别特定面孔也是一个具有挑战性的任务，因为在给定照片中可能存在属于不同人的多张面孔。
- en: 'For copyright reasons, we cannot publish the training datasets that have been
    used to obtain the exact output in this chapter, as they have been scraped from
    a variety of online sources. However, websites that might prove useful for obtaining
    similar datasets are:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于版权原因，我们无法发布用于获取本章精确输出的训练数据集，因为它们是从各种在线来源中获取的。但可能会对获取类似数据集有用的网站是：
- en: '[https://github.com/deepfakes/faceswap/](https://github.com/deepfakes/faceswap/)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/deepfakes/faceswap/](https://github.com/deepfakes/faceswap/)'
- en: '[http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/](http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/](http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/)'
- en: '[https://www.kaggle.com/c/deepfake-detection-challenge/overview](https://www.kaggle.com/c/deepfake-detection-challenge/overview)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/c/deepfake-detection-challenge/overview](https://www.kaggle.com/c/deepfake-detection-challenge/overview)'
- en: 'Assuming we already have the raw datasets collected, we can proceed to the
    next set of tasks: face detection and identification.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经收集了原始数据集，我们可以继续进行下一组任务：面部检测和识别。
- en: 'The first task is to define an entity class to hold face-related objects. We
    need such a class as we will need to pass images, extracted faces, and face landmarks,
    as well as transformations, through the pipeline. We define a class, `DetectedFace`,
    as shown in the following code snippet:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是定义一个实体类来保存与面部相关的对象。我们需要这样一个类，因为我们需要通过管道传递图像、提取的脸部和面部标志，以及变换。我们定义一个类，`DetectedFace`，如下面的代码片段所示：
- en: '[PRE2]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Abstracting these frequently used properties into an object (class) enables
    us to reduce the number of individual parameters needed to pass between different
    utilities. We discussed the `face_recognition` library in the *Key feature set*
    section. We will leverage the pose prediction model from this library to predict
    face locations using dlib''s `shape_predictor`. The following snippet instantiates
    the predictor objects:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些经常使用的属性抽象成一个对象（类）使我们能够减少在不同的实用程序之间需要传递的单独参数的数量。我们在*关键特征集*部分讨论了`face_recognition`库。我们将利用这个库中的姿势预测模型来使用dlib的`shape_predictor`来预测面部位置。下面的代码片段实例化了预测器对象：
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method takes an image as input and generates `DetectedFace` objects as
    outputs. Readers should note that we are yielding objects of type `DetectedFace`.
    The `yield` keyword ensures lazy execution, meaning objects are created when required.
    This ensures smaller memory requirements. The `DetectedFace` object, on the other
    hand, abstracts the extracted face and corresponding landmarks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法接受图像作为输入，并生成`DetectedFace`对象作为输出。读者应该注意，我们正在产生`DetectedFace`类型的对象。`yield`关键字确保了延迟执行，这意味着在需要时创建对象。这可以确保较小的内存需求。另一方面，`DetectedFace`对象将提取的面部和相应的标志抽象化。
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We iterate the generator object returned by the `detect_faces` method to visualize
    all the identified faces. In the following snippet, we perform this visualization:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迭代由`detect_faces`方法返回的生成器对象，以可视化所有识别到的面部。在下面的代码片段中，我们执行这个可视化：
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The sample image used for face identification and extraction is showcased in
    the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 用于面部识别和提取的示例图像如下图所示：
- en: '![](img/B16176_08_11.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_11.png)'
- en: 'Figure 8.11: Sample image for face identification and extraction'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：用于面部识别和提取的示例图像
- en: 'As visible in *Figure 8.11*, there are two faces corresponding to Donald Trump
    and Narendra Modi. The extracted faces using the `detect_faces` utility method
    are shown in *Figure 8.12*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图8.11*所示，有两张脸对应于唐纳德·特朗普和纳伦德拉·莫迪。使用`detect_faces`实用方法提取的面部显示在*图8.12*中：
- en: '![A close up of a person  Description automatically generated with low confidence](img/B16176_08_12.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![一张人的特写 描述是由低置信度自动生成的](img/B16176_08_12.png)'
- en: 'Figure 8.12: Extracted faces from sample image'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：从示例图像中提取的面部
- en: '[PRE8]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, the `FaceFilter` class requires a reference image as
    an input. This is intuitive; this reference serves as a ground truth to compare
    against in order to confirm whether or not we've found the right face. As mentioned
    earlier, the `face_recognition` package makes use of VGGFace to generate encoding
    for any image. We do this for the reference image and extract a vector representation
    for it. The `check` function in the `FaceFilter` class is then used to perform
    a similarity check (using metrics such as Euclidean distance or cosine similarity)
    between any new image and the reference image. If the similarity is below a certain
    threshold, it returns `False`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`FaceFilter`类需要一个参考图像作为输入。这是直观的；这个参考图像用作对比的基准，以确认我们是否找到了正确的面部。正如前面提到的，`face_recognition`包使用VGGFace为任何图像生成编码。我们对参考图像执行这个操作，并提取其矢量表示。然后在`FaceFilter`类中使用`check`函数来执行新图像与参考图像之间的相似性检查（使用欧氏距离或余弦相似性等度量）。如果相似度低于一定阈值，则返回`False`。
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The sample image, reference image, and recognized face are shown in *Figure
    8.13*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 示例图像、参考图像和识别的面部显示在*图8.13*中：
- en: '![](img/B16176_08_13.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_13.png)'
- en: 'Figure 8.13: Sample image, reference image, matched face, and unmatched face'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13：示例图像，参考图像，匹配的脸和不匹配的脸
- en: As shown in the figure, our `FaceFilter` class was able to identify which face
    belongs to Donald Trump and which doesn't. This is extremely useful for creating
    our datasets.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图中所示，我们的`FaceFilter`类能够识别哪个脸属于唐纳德·特朗普，哪个脸不属于他。这对于创建我们的数据集非常有用。
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `extract` method in the `Extract` class takes in the whole image, along
    with the `DetectFace` object, as input. It also takes in a size parameter to resize
    the images to the required dimensions. We make use of cv2''s `warpAffine` and
    skimage''s `transform` methods to perform the alignment. Interested readers are
    requested to check out the official documentation of these libraries for more
    details. For now, we can consider these as helper functions that allow us to extract
    and align the detected faces. *Figure 8.14* shows the output after alignment:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`Extract`类中的`extract`方法将整个图像以及`DetectFace`对象作为输入。它还接受一个大小参数，以将图像调整为所需的尺寸。我们利用了cv2的`warpAffine`和skimage的`transform`方法来执行对齐。感兴趣的读者请查阅这些库的官方文档以获取更多详情。目前，我们可以将这些视为帮助函数，允许我们提取和对齐检测到的面部。*图8.14*显示了对齐后的输出：'
- en: '![](img/B16176_08_14.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_14.png)'
- en: 'Figure 8.14: Transformation from the input image to the extracted face and
    finally, the aligned face'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14：从输入图像到提取的面部，最后到对齐的面部的变换
- en: The transformation depicted in the figure highlights subtle differences between
    the raw extracted face and the aligned face. This transformation works for any
    face pose and helps align faces for better results.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图中所示的变换突显了原始提取的面部与对齐后的面部之间的细微差别。这个变换适用于任何面部姿势，并有助于对齐面部以获得更好的结果。
- en: 'Now that we understand the step-by-step tasks, let''s put everything in order
    to generate the required datasets. The following snippet combines all these steps
    into a single method for ease of use:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了逐步任务，让我们将所有内容整理起来以生成所需的数据集。以下代码片段将所有这些步骤合并为一个单一方法以便使用：
- en: '[PRE14]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We next use the `get_faces` method to write a high-level function which takes
    the raw images as input, along with other required objects, to extract and dump
    the relevant faces into an output directory. This is shown in the following snippet:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`get_faces`方法编写一个高级函数，该函数以原始图像作为输入，以及其他所需对象，来提取并将相关面部转储到输出目录。如下所示：
- en: '[PRE15]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We use `create_face_dataset` to scan through raw images of Donald Trump and
    Nicolas Cage to create the required datasets for us.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`create_face_dataset`扫描唐纳德·特朗普和尼古拉斯·凯奇的原始图像，为我们创建所需的数据集。
- en: Autoencoder architecture
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器架构
- en: We prepared our datasets for both Donald Trump and Nicolas Cage using the tools
    presented in the previous section. Let's now work toward a model architecture
    that learns the task of face swapping.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用前一节中提供的工具为唐纳德·特朗普和尼古拉斯·凯奇准备了数据集。现在让我们朝着一个学习人脸交换任务的模型架构努力。
- en: We presented a few common architectures in earlier sections of the book. The
    encoder-decoder setup is one such setup widely used for deepfake tasks. For our
    current task of face swapping, we will develop an autoencoder setup to learn and
    swap faces. As has been the norm, we will make use of TensorFlow and Keras to
    prepare the required models.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的前几节介绍了一些常见的架构。编码器-解码器设置是其中一个广泛用于深度伪造任务的设置。对于我们当前的人脸交换任务，我们将开发一个自编码器设置来学习和交换面部。一如既往，我们将利用
    TensorFlow 和 Keras 来准备所需的模型。
- en: 'Before we get onto actual architecture code, let''s briefly recap how this
    setup works. A typical autoencoder has two components, an encoder and a decoder.
    The encoder takes an image as input and compresses it down to a lower dimensional
    space. This compressed representation is called an embedding, or bottleneck features.
    The decoder works in the reverse manner. It takes the embedding vector as input
    and tries to reconstruct the image as output. In short, an autoencoder can be
    described as:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入实际架构代码之前，让我们简要回顾一下这个设置是如何工作的。一个典型的自编码器有两个组件，编码器和解码器。编码器将图像作为输入并将其压缩到一个较低维度的空间。这个压缩表示被称为嵌入，或者瓶颈特征。解码器以相反的方式工作。它以嵌入向量作为输入，并试图将图像重建为输出。简而言之，自编码器可以描述为：
- en: '![](img/B16176_08_001.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_001.png)'
- en: The autoencoder takes *x* as input and tries to generate a reconstruction ![](img/B16176_08_002.png)
    such that ![](img/B16176_08_003.png).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器以*x*作为输入，并尝试生成一个重建![](img/B16176_08_002.png)，使得![](img/B16176_08_003.png)。
- en: 'With this brief overview of the autoencoder architecture, let''s get started
    with developing the required functions for both encoders and decoders. The following
    snippet shows a function that creates downsampling blocks for the encoder part:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对自编码器架构的简要概述，让我们开始为两个编码器和解码器开发所需的函数。以下片段显示了用于编码器部分创建下采样块的函数：
- en: '[PRE16]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The downsampling block makes use of a two-dimensional convolutional layer followed
    by leaky ReLU activation. The encoder will make use of multiple such repeating
    blocks followed by fully connected and reshaping layers. We finally make use of
    an upsampling block to transform the output into an 8x8 image with 512 channels.
    The following snippet shows the upsampling block:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样块使用一个二维卷积层，后跟泄漏的ReLU激活。编码器将利用多个这样的重复块，然后是全连接和重塑层。最后，我们使用上采样块将输出转换为具有512个通道的8x8图像。以下片段显示了上采样块：
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The upsampling block is composed of a two-dimensional convolution, `LeakyReLU`,
    and finally an `UpSampling2D` layer. We use both the downsampling and upsampling
    blocks to create the encoder architecture, which is presented in the following
    snippet:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样块由二维卷积，`LeakyReLU`，最后是一个`UpSampling2D`层组成。我们使用上采样块和下采样块来创建编码器架构，如下片段所示：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The decoder, on the other hand, has a simpler setup. We make use of a few upsampling
    blocks followed by a convolutional layer to reconstruct the input image as its
    output. The following snippet shows the function for the decoder:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，解码器具有更简单的设置。我们使用几个上采样块，然后是一个卷积层来重构输入图像作为其输出。以下片段显示了解码器的函数：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For our task of face swapping, we develop two autoencoders, one for each identity,
    in other words, one for Donald Trump and one for Nicolas Cage. The only trick
    is that both autoencoders share the same encoder. Yes, this architectural setup
    requires us to develop two autoencoders with specific decoders but a common encoder.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的人脸交换任务，我们开发了两个自编码器，一个用于每个身份，换句话说，一个用于唐纳德·特朗普，一个用于尼古拉斯·凯奇。唯一的技巧是两个自编码器共享相同的编码器。是的，这种架构设置要求我们开发具有特定解码器但共同编码器的两个自编码器。
- en: 'This trick works for a few simple reasons. Let''s discuss this a bit more.
    Let''s assume we have two autoencoders, Autoencoder-A and Autoencoder-B, composed
    of a common encoder but the respective decoders of Decoder-A and Decoder-B. This
    setup is depicted in the following figure:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧之所以有效是因为有几个简单的原因。我们稍微讨论一下这一点。假设我们有两个自编码器，Autoencoder-A和Autoencoder-B，由一个公共编码器组成，但分别具有Decoder-A和Decoder-B的解码器。这个设置如下图所示：
- en: '![](img/B16176_08_15.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_15.png)'
- en: 'Figure 8.15: Replacement using autoencoders'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15：使用自编码器进行替换
- en: 'The details regarding how this setup works are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个设置如何工作的详细信息如下：
- en: Both autoencoders learn to reconstruct their respective inputs as they train
    using backpropagation.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个自编码器在训练过程中使用反向传播学习重构它们各自的输入。
- en: Each autoencoder tries to minimize the reconstruction error. In our case, we
    will make use of **mean absolute error** (**MAE**) as our metric.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个自编码器都试图最小化重构误差。在我们的情况下，我们将使用**平均绝对误差**（**MAE**）作为我们的度量标准。
- en: Since both autoencoders have the same encoder, the encoder learns to understand
    both kinds of faces and transforms them into the embedding space.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于两个自编码器具有相同的编码器，编码器学习理解两种类型的人脸并将它们转换为嵌入空间。
- en: Transforming the input images by aligning and warping the faces ensures that
    the encoder is able to learn representations of both kinds of faces.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对输入图像进行对齐和变形，确保编码器能够学习两种类型人脸的表示。
- en: The respective decoders, on the other hand, are trained to make use of the embeddings
    to reconstruct the images.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，相应的解码器经过训练，利用嵌入来重构图像。
- en: 'Once both the encoders are trained to our satisfaction, we proceed to face
    swapping. Let''s consider the scenario where we''re swapping the face of person
    B onto the face of person A:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦两个编码器都训练得令我们满意，我们就进行人脸交换。让我们考虑这样一个场景：我们要将人物B的脸换到人物A的脸上：
- en: We start with an image of person B. The input is encoded into a lower dimensional
    space by the encoder. Now, in place of using the decoder for B, we swap it with
    the decoder of A itself, i.e. Decoder-A. This is essentially Autoencoder-A with
    input from the dataset of person B.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从一个人物B的图像开始。输入由编码器编码为低维空间。现在，我们不再使用B的解码器，而是将其与A的解码器交换，即Decoder-A。这本质上是使用了来自人物B数据集的Autoencoder-A的输入。
- en: Face swapping using B as input for Autoencoder-A works because Autoencoder-A
    treats the face of B as if it were a warped version of face A itself (due to the
    common encoder in place).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 B 作为 Autoencoder-A 的输入进行面部交换是可行的，因为 Autoencoder-A 将 B 的面部视为 A 本身的扭曲版本（因为存在公共编码器）。
- en: Thus, Decoder-A of Autoencoder-A generates an output image which seems like
    a look-alike of A with the characteristics of B.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，Autoencoder-A 的解码器生成一个看起来像 A 的外观，但具有 B 的特征的输出图像。
- en: 'Let''s leverage this understanding to create the required autoencoders. The
    following snippet presents autoencoders for both types of faces:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这一理解来创建所需的自动编码器。以下代码片段展示了两种类型面孔的自动编码器：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have two autoencoders that take 3 channel input images, each 64x64 in size.
    The encoder transforms these images into embeddings of size 8x8x512, while the
    decoder uses these embeddings to reconstruct output images of shape 64x64x3\.
    In the next section, we'll train these autoencoders.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个接受 3 通道输入图像的自动编码器，每个图像大小为 64x64。编码器将这些图像转换为大小为 8x8x512 的嵌入，而解码器使用这些嵌入来重建形状为
    64x64x3 的输出图像。在下一节中，我们将训练这些自动编码器。
- en: Training our own face swapper
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练我们自己的面部交换程序
- en: Now that we have both the autoencoders in place, we need to prepare a custom
    training loop to train both networks together. However, before we get to the training
    loop, there are a few other utilities we need to define.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了自动编码器，我们需要准备一个自定义训练循环来一起训练这两个网络。然而，在进行训练循环之前，我们需要定义一些其他实用程序。
- en: 'The input datasets we have created for both personalities contain their faces
    under different lighting conditions, face positions, and other settings. Yet these
    cannot be exhaustive in nature. To ensure that we capture a larger variation of
    each type of face, we''ll make use of a few augmentation methods. The following
    snippet presents a function that applies random transformations to an input image:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为两种个性创建的输入数据集包含它们在不同的光照条件、面部位置和其他设置下的面部。然而这些并不是穷尽的。为了确保我们捕获每种面孔的更大变化，我们将使用一些增强方法。以下代码片段展示了一个向输入图像施加随机变换的函数：
- en: '[PRE21]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `random_transform` function helps us to generate different warps of the
    same input face. This method ensures that we have enough variation for training
    our networks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_transform` 函数帮助我们生成同一输入面部的不同扭曲。这种方法确保我们有足够的变化来训练我们的网络。'
- en: 'The next function required is a batch generator. Since we are dealing with
    images and large networks, it is imperative that we keep in mind the resource
    requirements. We make use of lazy execution utilities such as `yield` to keep
    memory/GPU requirements as low as possible. The following snippet shows our batch
    generator for the training process:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要的函数是批量生成器。由于我们处理图像和大型网络，务必记住资源要求。我们利用诸如`yield`这样的延迟执行实用程序来尽可能保持内存/GPU要求低。以下代码片段显示了我们训练过程的批量生成器：
- en: '[PRE22]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now that we have the batch generator along with the augmentation functions,
    let''s prepare a training loop. This is shown in the following snippet:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了批量生成器和增强函数，让我们准备一个训练循环。下面是展示的：
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We train both the autoencoders for about 10,000 steps, or until the loss stabilizes.
    We utilize a batch size of 64 and save checkpoint weights every 100 epochs. Readers
    are free to play around with these parameters depending on their infrastructure
    setup.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练两个自动编码器大约 10,000 步，或者直到损失稳定。我们使用批量大小为 64 并且每 100 个周期保存检查点权重。读者可根据其基础架构设置自由调整这些参数。
- en: Results and limitations
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果和局限性
- en: We have now trained the respective autoencoders for Nicolas Cage (Autoencoder-A)
    and Donald Trump (Autoencoder-B). The final step is to transform Nicolas Cage
    into Donald Trump. We described the steps earlier; we will use the autoencoder
    for Donald Trump with the input as Nicolas Cage, thereby generating an output
    that seems like a Nicolas Cage version of Donald Trump.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为尼古拉斯·凯奇（自动编码器-A）和唐纳德·特朗普（自动编码器-B）分别训练了对应的自动编码器。最后一步是将尼古拉斯·凯奇转换为唐纳德·特朗普。我们之前描述了这些步骤；我们将使用唐纳德·特朗普的自动编码器，输入为尼古拉斯·凯奇，从而生成一个看起来像尼古拉斯·凯奇版本的唐纳德·特朗普的输出。
- en: 'But before we get to the task of output generation, we require a few additional
    utilities. We discussed an additional step called **blending**. This step is performed
    post-output generation to ensure that the generated replacement and the original
    face seamlessly combine into a single image. Refer back to *Figure 8.8* for a
    visual reminder of the concept of blending. For our task, we prepare a blending
    class called `Convert`. The class is presented in the following snippet:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 但在进入输出生成任务之前，我们需要一些额外的实用程序。我们讨论了一个称为**混合**的额外步骤。这一步是在输出生成后执行的，以确保生成的替换和原始面孔无缝地融合成一幅图像。回头看*图8.8*，对于混合概念的视觉提醒。对于我们的任务，我们准备了一个名为`Convert`的混合类。该类在以下片段中呈现：
- en: '[PRE24]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `patch_image` method relies on a few utility functions also defined in
    the class, `get_new_face`, `apply_new_face` and `get_image_mask`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`patch_image`方法依赖于该类中还定义的几个实用函数，`get_new_face`、`apply_new_face`和`get_image_mask`：'
- en: '[PRE25]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This class takes in multiple parameters to improve the blending results. The
    parameters control aspects such as the size of the blurring kernel, the type of
    patch (such as rectangular or polygon), and the erosion kernel size. The class
    also takes the encoder as input. The class method `patch_image` does its magic
    with the help of transformation functions from the cv2 library and the parameters
    we set during instantiation. We use the following `convert` function to process
    each input face of type A and transform it into type B:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该类接收多个参数以改善混合结果。这些参数控制诸如模糊核大小、补丁类型（如矩形或多边形）和侵蚀核大小等方面。该类还接收编码器作为输入。类方法`patch_image`在实例化期间使用来自cv2库的变换函数和我们设置的参数来进行其神奇操作。我们使用以下`convert`函数来处理每个A类型输入面孔并将其转换为B类型：
- en: '[PRE26]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The generated outputs are depicted in the following figure:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下图所示：
- en: '![](img/B16176_08_16.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_16.png)'
- en: 'Figure 8.16: Nicolas Cage transformed as Donald Trump'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.16: 尼古拉斯·凯奇变成唐纳德·特朗普'
- en: The swapped output faces are encouraging, but not as seamless as we would have
    expected. Still, we can see that the model has learned to identify and swap the
    right portions of the face. The blending step has also tried to match the skin
    tone, face pose, and other aspects to make the results as realistic as possible.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 交换后的输出面孔令人鼓舞，但并不像我们预期的那样无缝。但我们可以看到模型已学会识别和交换面部的正确部分。混合步骤还尝试匹配肤色、面部姿势和其他方面，使结果尽可能真实。
- en: 'This seems like a good start, but leaves a lot for improvement. Here are a
    few limitations of our setup:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个良好的开始，但留下了很多需要改进的地方。以下是我们设置的几个限制：
- en: The quality of the swapped output is directly related to the capabilities of
    the trained autoencoders. Since there is no component to track how realistic the
    reconstructed outputs are, it is difficult to nudge the autoencoders in the right
    direction. Using a GAN would be a possible enhancement to provide a positive feedback
    to the overall training process.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交换输出的质量直接与训练的自动编码器的能力相关。由于没有组件来跟踪重建输出的真实性，很难将自动编码器引导到正确的方向。使用GAN可能是提供正面反馈以增强整个训练过程的一种可能的增强。
- en: The outputs are a bit blurry. This is due to the difference in the actual input
    resolution and the generated output (64x64). Another reason for the blurry output
    is the use of MAE as a simple loss function. Research has shown that composite
    and complex losses help improve final output quality.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出有点模糊。这是由于实际输入分辨率和生成的输出（64x64）之间的差异。造成模糊输出的另一个原因是使用MAE作为简单的损失函数。研究表明，复合和复杂的损失有助于提高最终的输出质量。
- en: The limited dataset is another reason for restricted output quality. We leveraged
    augmentation techniques to work around the limitation, but it is not a substitute
    for larger datasets.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的数据集是输出质量受限的另一个原因。我们利用增强技术来解决限制，但这并不能替代更大的数据集。
- en: In this section, we developed a face swapping deepfake architecture from scratch.
    We went through a step-by-step approach to understand every component and step
    in the overall pipeline that swaps Nicolas Cage with Donald Trump. In the next
    section, we will use a more complex setup to try our hand at a different mode
    of operation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从零开始开发了一个人脸交换deepfake架构。我们以逐步的方式来理解将尼古拉斯·凯奇与唐纳德·特朗普交换的整体流程中的每个组件和步骤。在下一节中，我们将使用更复杂的设置来尝试不同的操作模式。
- en: 'The code presented in this section is based on the original deepfake work as
    well as a simplified implementation of Olivier Valery''s code, which is available
    on GitHub at the following link: [https://github.com/OValery16/swap-face](https://github.com/OValery16/swap-face).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码基于原始deepfake工作以及Olivier Valery的代码的简化实现，该代码可在GitHub的以下链接找到：[https://github.com/OValery16/swap-face](https://github.com/OValery16/swap-face)。
- en: Now that we have trained our own face swapper, illustrating the replacement
    mode of operation, we can move onto the re-enactment mode.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了自己的人脸交换器，演示了替换模式的操作，我们可以继续进行再现模式。
- en: Re-enactment using pix2pix
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pix2pix进行再现
- en: Re-enactment is another mode of operation for the deepfakes setup. It is supposedly
    better at generating believable fake content compared to the replacement mode.
    In earlier sections, we discussed different techniques used to perform re-enactment,
    i.e. by focusing on gaze, expressions, the mouth, and so on.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 再现是深度假像设置的另一种操作模式。与替换模式相比，它据说更擅长生成可信的虚假内容。在前面的章节中，我们讨论了执行再现的不同技术，即通过关注凝视、表情、嘴巴等。
- en: We also discussed image-to-image translation architectures in *Chapter 7*, *Style
    Transfer with GANs*. Particularly, we discussed in detail how the pix2pix GAN
    is a powerful architecture which enables paired translation tasks. In this section,
    we will leverage the pix2pix GAN to develop a face re-enactment setup from scratch.
    We will work toward building a network where we can use our own face, mouth, and
    expressions to control Barack Obama's (former US president) face. We will go through
    each and every step, starting right from preparing the dataset, to defining the
    pix2pix architecture, to finally generating the output re-enactment. Let's get started.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在*第7章*，*使用GAN进行风格转移*中讨论了图像到图像翻译体系结构。特别是，我们详细讨论了pix2pix GAN是一种强大的架构，可以实现成对翻译任务。在本节中，我们将利用pix2pix
    GAN从零开始开发一个人脸再现设置。我们将努力构建一个网络，我们可以使用我们自己的面部、嘴巴和表情来控制巴拉克·奥巴马（前美国总统）的面部。我们将逐步进行每一步，从准备数据集开始，到定义pix2pix架构，最后生成输出再现。让我们开始吧。
- en: Dataset preparation
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集准备
- en: We will be using the pix2pix GAN as the backbone network for our current task
    of re-enactment. While pix2pix is a powerful network that trains with very few
    training samples, there is a restriction that requires the training samples to
    be paired. In this section, we will use this restriction to our advantage.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pix2pix GAN作为我们当前再现任务的骨干网络。虽然pix2pix是一个训练样本非常少的强大网络，但有一个限制，即需要训练样本成对出现。在本节中，我们将利用这个限制来达到我们的目的。
- en: Since the aim is to analyze a target face and control it using a source face,
    we can leverage what is common between faces to develop a dataset for our use
    case. The common characteristics between different faces are the presence of facial
    landmarks and their positioning. In the *Key feature set* section, we discussed
    how simple and easy it is to build a facial landmark detection module using libraries
    such as dlib, cv2, and MTCNN.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标是分析目标面孔并使用源面孔进行控制，我们可以利用不同面孔之间的共同之处来为我们的用例开发数据集。不同面孔之间的共同特征是面部地标的存在和位置。在*关键特征集*部分，我们讨论了如何使用诸如dlib、cv2和MTCNN等库构建简单易用的面部地标检测模块。
- en: For our current use case, we will prepare paired training samples consisting
    of pairs of landmarks and their corresponding images/photographs. For generating
    re-enacted content, we can then simply extract facial landmarks of the source
    face/controlling entity and use pix2pix to generate high quality actual output
    based on the target person.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们当前的用例，我们将准备成对的训练样本，包括一对地标和其对应的图像/照片。要生成再现内容，我们只需提取源脸部/控制实体的面部地标，然后使用pix2pix基于目标人物生成高质量的实际输出。
- en: In our case, the source/controlling personality could be you or any other person,
    while the target personality is Barack Obama.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，源/控制人格可以是您或任何其他人，而目标人格是巴拉克·奥巴马。
- en: To prepare our dataset, we will extract frames and corresponding landmarks of
    each frame from a video. Since we want to train our network to be able to generate
    high-quality coloured output images based on landmark inputs, we need a video
    of Barack Obama. You could download this from various different sources on the
    internet. Please note that this exercise is again for academic and educational
    purposes only. Kindly use any videos carefully and with caution.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating a paired dataset of landmark and video frames is a straightforward
    application of the code snippets given in the *Facial landmarks* section. To avoid
    repetition, we leave this as an exercise for the reader. Please note that complete
    code is available in the code repository for this book. We generated close to
    400 paired samples from one of the speeches of Barack Obama. *Figure 8.17* presents
    a few of these samples:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, different, person  Description automatically
    generated](img/B16176_08_17.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Paired training samples consisting of facial landmarks and corresponding
    video frames'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: We can see how the landmarks capture the position of the head along with the
    movement of the lips, eyes, and other facial landmarks. We are thus able to generate
    a paired training dataset in almost no time. Let's now move on to network setup
    and training.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Pix2pix GAN setup and training
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed the pix2pix architecture along with its sub-components and objective
    functions in detail in *Chapter 7*, *Style Transfer with GANs*. In this section,
    we will briefly touch upon them for the sake of completeness.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Please note that we are reusing the utility functions prepared as part of *Chapter
    7*, *Style Transfer with GANs*. Unlike the generator, which has a specific setup,
    the discriminator network for pix2pix is a fairly straightforward implementation.
    We present the discriminator network in the following snippet:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use these two functions to prepare our generator, discriminator, and GAN
    network objects. The objects are created as shown in the following snippet:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The training loop is straightforward; we make use of the three network objects
    (discriminator, generator, and the overall GAN model) and alternately train the
    generator and discriminator. Note that the facial landmarks dataset is used as
    input, while the video frames are output for this training process. The training
    loop is presented in the following snippet:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*Figures 8.18* and *8.19* showcase the training progress of this setup:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_18.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Training progress for pix2pix GAN for face re-enactment (epoch
    1)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_19.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: Training progress for the pix2pix GAN for face re-enactment (epoch
    40)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figures, the model is able to capture key facial
    features and their positioning, along with the background details. In the initial
    iterations (*Figure 8.18*), the model seems to be having difficulty in generating
    the mouth region, but as the training progresses, it learns to fill it with the
    right set of details (*Figure 8.19*).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的图中所看到的，模型能够捕捉关键的面部特征及其位置，以及背景细节。在初始迭代中（*图8.18*），模型似乎在生成嘴部区域方面遇到了困难，但随着训练的进行，它学会了用正确的一组细节填充它（*图8.19*）。
- en: Now that we have our GAN trained for the required task, let's perform some re-enactments
    in the next section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为所需任务训练了我们的 GAN，让我们在下一节中进行一些再现。
- en: Results and limitations
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果和局限性
- en: In the chapter so far, we have dealt mostly with images or photographs as input.
    Since the pix2pix GAN is a very efficient implementation, it can be leveraged
    to generate outputs in near-real time. This capability therefore implies that
    we can use such a trained model to perform re-enactments using a live video feed.
    In other words, we can use a live video feed of ourselves to re-enact Barack Obama's
    face movements and expressions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本章中主要处理图像或照片作为输入。由于 pix2pix GAN 是一个非常高效的实现，它可以用来在几乎实时地生成输出。因此，这种能力意味着我们可以使用这样一个训练好的模型来使用实时视频来进行再现。换句话说，我们可以使用自己的实时视频来再现巴拉克·奥巴马的面部动作和表情。
- en: '[PRE33]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: These functions help extract and draw facial landmarks from a given frame, and
    use those landmarks to generate output of colored frames using the pix2pix GAN.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数帮助从给定帧中提取并绘制面部标志，并使用这些标志来使用 pix2pix GAN 生成彩色帧的输出。
- en: 'The next step is to use these functions to process the live video feed and
    generate re-enacted output samples. This manipulation is fast enough to enhance
    the believability of the fake content. The following snippet presents the manipulation
    loop:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用这些函数处理实时视频并生成再现的输出样本。这种操作足够快速，以增强虚假内容的可信度。以下代码段展示了操作循环：
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/B16176_08_20.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_08_20.png)'
- en: 'Figure 8.20: Re-enactment using live video as the source and Obama as the target
    using the pix2pix GAN'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.20: 使用实时视频作为源，奥巴马作为目标，使用 pix2pix GAN 进行再现'
- en: '*Figure 8.20* presents how seamlessly the overall setup works. We are able
    to capture a live video, convert it into facial landmarks, and then generate re-enactments
    using the pix2pix GAN. It is apparent how, in the live video, there are no objects
    in the background, but our network is able to generate the American flag correctly.
    The samples also showcase how the model is able to capture expressions and head
    tilt nicely.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.20*展示了整体设置如何无缝工作。我们能够捕捉到实时视频，将其转换为面部标志，然后使用 pix2pix GAN 生成再现。在实时视频中，背景中没有物体，但我们的网络能够正确生成美国国旗。样本还展示了模型如何很好地捕捉表情和头部倾斜。'
- en: 'Though the results are encouraging, they are far from being perceived as real
    or believable. The following are a few limitations associated with the approach
    we discussed in this section:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果令人鼓舞，但远远不能被认为是真实或可信的。以下是我们在本节讨论的方法中所涉及的一些限制：
- en: The outputs in *Figure 8.20* are a bit fuzzy. They turn completely blank or
    incomprehensible if the head is tilted quite a lot or if the person in the live
    video feed is too close or too far from the camera. This issue is mostly because
    the pix2pix GAN has learned the relative size and position of facial landmarks
    with respect to the training dataset. This can be improved by performing face
    alignment and using tighter crops for both the input and inference stages.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图8.20*中的输出有点模糊。如果头部倾斜得太多，或者直播视频中的人离摄像头太近或太远，它们会完全变成空白或难以理解。这个问题主要是因为 pix2pix
    GAN 学会了相对大小和位置的面部标志物，相对于训练数据集。通过进行面部对齐并在输入和推理阶段使用更紧凑的裁剪，可以改善这一问题。'
- en: The model's generated content is highly dependent upon the training data. Since
    our training dataset is derived from a speech, there is limited head movement
    and very limited facial expression. Thus, if you try to move the head a bit too
    much or present an expression that isn't in the training dataset, the model makes
    a very poor guess. A larger dataset with more variability can help fix this issue.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型生成的内容高度依赖于训练数据。由于我们的训练数据集来自演讲，因此头部移动有限，面部表情也非常有限。因此，如果你试图移动头部太多或展示训练数据集中不存在的表情，模型会做出非常糟糕的猜测。更大的数据集和更多的变化可以帮助解决这个问题。
- en: We have seen how a powerful image-to-image translation GAN architecture can
    be reused for the task of re-enactment.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一个强大的图像到图像翻译GAN架构可以被重新用于再现任务。
- en: In the last two sections, we covered a number of interesting hands-on exercises
    to develop replacement and re-enactment architectures from scratch. We discussed
    some of the issues with our setup and how we could improve upon them. In the following
    section, we will discuss some of the challenges associated with deepfake systems.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们介绍了一些有趣的从零开始开发替换和再现架构的实际练习。我们讨论了我们的设置中的一些问题以及如何改进它们。在接下来的一节中，我们将讨论与深度伪造系统相关的一些挑战。
- en: Challenges
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: In this section, we will discuss some of the common challenges associated with
    deepfake architectures, beginning with a brief discussion on the ethical issues
    associated with this technology.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论与深度伪造架构相关的一些常见挑战，首先简要讨论与这项技术相关的道德问题。
- en: Ethical issues
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 道德问题
- en: Even though generating fake content is not a new concept, the word "deepfake"
    came into the limelight in 2017 when a Reddit user by the name u/deepfakes posted
    fake pornographic videos with celebrity faces superimposed on them using deep
    learning. The quality of the content and the ease with which the user was able
    to generate them created huge uproar on news channels across the globe. Soon,
    u/deepfakes released an easy-to-setup application called **FakeApp** that enabled
    users to generate such content with very little knowledge of how deep learning
    works. This led to a number of fake videos and objectionable content. This, in
    turn, helped people gain traction on issues associated with identity theft, impersonation,
    fake news, and so on.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成虚假内容并不是一个新概念，但“deepfake”一词在2017年成为众所关注的焦点，那时Reddit用户u/deepfakes发布了用深度学习将名人面孔叠加在色情视频上的虚假视频。这些视频的质量和用户能够轻松生成它们的方式在全球新闻频道上掀起了轩然大波。很快，u/deepfakes发布了一个名为**FakeApp**的易于设置的应用程序，使用户能够在对深度学习工作原理几乎一无所知的情况下生成此类内容。这导致了大量虚假视频和令人反感的内容。结果，人们开始对涉及身份盗用、冒充、假新闻等问题产生关注。
- en: Soon, interest picked up within the academic community, which not only helped
    to improve the technology but also insisted on its ethical use. While there are
    malicious and objectionable content creators making use of these techniques, a
    number of industry and research projects are underway to detect such fake content,
    such as Microsoft's deepfake detection tool and Deepware.^(14 15)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，学术界对此产生了浓厚兴趣，这不仅有助于改善技术，还坚持其道德使用。尽管一些恶意和令人反感的内容创作者利用这些技术，但也有许多工业和研究项目正在进行中，以检测此类虚假内容，如微软的深度伪造检测工具和Deepware。^(14
    15)
- en: Technical challenges
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术挑战
- en: 'Ethical issues aside, let''s also discuss a few challenges that are quite apparent
    for a typical deepfake setup: generalization, occlusions, and temporal issues.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管道德问题暂且不提，让我们也讨论一下典型深度伪造设置中明显存在的一些挑战：泛化、遮挡和时间问题。
- en: Generalization
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泛化
- en: Deepfake architectures are generative models at their core, which are highly
    dependent on the training dataset used. These architectures typically also require
    huge amounts of training samples, which could be hard to get, especially for the
    target (or victim in the case of malicious use). Another issue is the paired training
    setup. Typically, a model trained for one source and target pair is not so easy
    to use for another pair of source and target personalities.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造架构本质上是生成式模型，高度依赖于使用的训练数据集。这些架构通常也需要大量的训练样本，这可能很难获得，特别是对于目标（或在恶意使用的情况下的受害者）而言。另一个问题是配对的训练设置。通常针对一个源和目标配对训练的模型不易用于另一对源和目标人物。
- en: Work on efficient architectures that can train with smaller amounts of training
    data is an active area of research. The development of CycleGAN and other unpaired
    translation architectures is also helping in overcoming the paired training bottleneck.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的一个活跃领域是致力于全新高效架构的开发，这些架构能够使用更少的训练数据。CycleGAN和其他无配对翻译架构的发展也有助于克服配对训练的瓶颈。
- en: Occlusions
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遮挡
- en: The source or target inputs might have artifacts around them that obstruct certain
    features. This could be due to hand movements, hair, eyewear, or other objects.
    Another type of occlusion occurs due to dynamic changes in the mouth and eye region.
    This can lead to inconsistent facial features or weird cropped imagery. Certain
    works are focusing on avoiding such issues by making use of segmentation, in-painting,
    and other related techniques. One example of such a work is *First Order Motion
    Model for Image Generation* by Siarohin et al.^(16)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 源或目标输入可能围绕它们存在妨碍某些特征的残留物。这可能是由于手部运动、头发、眼镜或其他物体造成的。另一种遮挡发生在口部和眼部区域的动态变化上。这可以导致不一致的面部特征或奇怪的裁剪图像。某些作品正在致力于通过使用分割、修补和其他相关技术来避免这些问题。其中一项作品的例子是
    Siarohin 等人的*First Order Motion Model for Image Generation*^(16)
- en: Temporal issues
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间问题
- en: 'Deepfake architectures work on a frame-by-frame basis (when it comes to video
    inputs). This results in jitter, flickering, or complete incoherence between subsequent
    frames. We saw an example of this with the re-enactment exercise using the pix2pix
    GAN in the previous section. The model is unable to generate coherent output for
    unseen scenarios. To improve upon this, some researchers are trying to use RNNs
    (recurrent neural networks) with GANs to generate coherent outputs. Examples of
    this include:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造架构基于逐帧处理（对于视频输入）。这导致了视频帧之间的抖动、闪烁，或者完全不连贯。我们在上一节使用 pix2pix GAN 进行再现练习时看到了一个例子。该模型无法为未见过的场景生成连贯的输出。为了改进这一点，一些研究人员正尝试使用带有
    GANs 的 RNNs（循环神经网络）来生成连贯的输出。这方面的例子包括：
- en: '*MoCoGAN: Decomposing Motion and Content for Video Generation*^(17)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MoCoGAN：分解运动和内容以进行视频生成*^(17)'
- en: '*Video-to-Video Synthesis*^(18)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Video-to-Video Synthesis*^(18)'
- en: Off-the-shelf implementations
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现成的实现
- en: In this chapter, we covered a step-by-step approach to developing two different
    deepfake architectures for replacement and re-enactment. Although the implementations
    are easy to understand and execute, they require quite a bit of understanding
    and resources to generate high-quality results.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种逐步开发用于替换和再现的两种不同深度伪造架构的方法。尽管这些实现易于理解和执行，但它们需要相当多的理解和资源来生成高质量的结果。
- en: Since the release of u/deepfakes' content in 2017, a number of open source implementations
    have come out to simplify the use of this technology. While dangerous, most of
    these projects highlight the ethical implications and caution developers and users
    in general against the malicious adoption of such projects. While it is beyond
    the scope of this chapter, we list a few well-designed and popular implementations
    in this section. Readers are encouraged to go through specific projects for more
    details.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年发布 u/deepfakes 内容以来，已经推出了许多开源实现以简化这项技术的使用。尽管危险，这些项目大多强调了道德意义和警告开发人员以及普通用户不要恶意采用这些项目。虽然这超出了本章的范围，但我们在本节列举了一些设计良好并受欢迎的实现。鼓励读者查看特定项目以获取更多细节。
- en: '**FaceSwap**^(19) The developers of this project claim this implementation
    is close to the original implementation by u/deepfakes, with enhancements over
    the years to improve output content quality. This project provides a detailed
    documentation and step-by-step guide for preparing the training dataset and generating
    the fake content. They also share pretrained networks for speeding up the training
    process. This project has a graphical interface for completely novice users.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FaceSwap**^(19) 该项目的开发人员声称这一实现接近 u/deepfakes 的原始实现，并经过多年的改进以提高输出内容质量。该项目提供了详细的文档和逐步指南，用于准备训练数据集和生成虚假内容。他们还分享了用于加速培训过程的预训练网络。该项目拥有一个图形界面，适合完全新手用户。'
- en: '**DeepFaceLab**^(20) This is one of the most extensive, detailed, and popular
    deepfakes projects available on the internet. This project is based on the recent
    paper with the same name presented in May 2020\. The project consists of a detailed
    user guide, video tutorials, a very mature GUI, pretrained models, Colab notebooks,
    datasets, and even Docker images for quick deployment.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepFaceLab**^(20) 这是互联网上最全面、详细和受欢迎的深伪造项目之一。该项目基于2020年5月提出的同名论文。该项目包括详细的用户指南、视频教程、非常成熟的
    GUI、预训练模型、Colab 笔记本、数据集，甚至是用于快速部署的 Docker 镜像。'
- en: '**FaceSwap-GAN**^(21) A simple, yet effective, implementation using an ED+GAN
    setup. This project provides utilities and ready-to-use notebooks for quickly
    training your own models. The project also provides pretrained models for direct
    use or transfer learning.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FaceSwap-GAN**^(21) 采用了一种简单而有效的技术，使用了ED+GAN设置。该项目提供了用于快速训练自己模型的实用程序和现成的笔记本。该项目还提供了预训练模型，供直接使用或迁移学习。'
- en: There are a number of Android and iOS apps that work along the same lines and
    lower the entry barrier to a bare minimum. Today, anybody with a smartphone or
    a little understanding of technical concepts can use or train such setups with
    ease.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多Android和iOS应用程序都以同样的方式工作，并将入门门槛降到最低。如今，几乎任何拥有智能手机或一点技术概念的人都可以轻松使用或训练这些设置。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Deepfakes are a complicated subject both ethically and technically. In this
    chapter, we discussed the deepfake technology in general to start with. We presented
    an overview of what deepfakes are all about and briefly touched upon a number
    of productive as well as malicious use cases. We presented a detailed discussion
    on different modes of operation of different deepfake setups and how each of these
    impacts the overall believability of generated content. While deepfakes is an
    all-encompassing term associated with videos, images, audio, text, and so on,
    we focused on visual use cases only in this chapter.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '*Deepfakes*在伦理和技术上都是一个复杂的课题。在本章中，我们首先讨论了deepfake技术总体情况。我们概述了deepfakes的内容以及简要介绍了一些有益的和恶意的使用案例。我们详细讨论了不同deepfake设置的不同操作模式以及这些操作模式如何影响生成内容的整体可信度。虽然deepfakes是一个与视频、图像、音频、文本等等相关的全面术语，但在本章中我们只关注视觉使用案例。'
- en: Given our scope, we discussed various feature sets leveraged by different works
    in this space. In particular, we discussed the Facial Action Coding System (FACS),
    3D Morphable Models (3DMM), and facial landmarks. We also discussed how we can
    perform facial landmark detection using libraries such as dlib and MTCNN. We then
    presented a high-level flow of tasks to be performed for a deepfakes pipeline.
    In conjunction with this, we discussed a few common architectures that are widely
    used to develop such systems.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的范围内，我们讨论了在这一领域中不同作品中利用的各种特征集。特别是，我们讨论了面部表情编码系统（FACS）、3D可塑模型（3DMM）和面部标志。我们还讨论了如何使用诸如dlib和MTCNN之类的库进行面部标志检测。然后，我们介绍了一种要在deepfakes管道中执行的高级任务流程。与此同时，我们还讨论了一些开发这种系统所广泛使用的常见架构。
- en: The second part of the chapter leveraged this understanding to present two hands-on
    exercises to develop deepfake pipelines from scratch. We first worked toward developing
    an autoencoder-based face swapping architecture. Through this exercise, we worked
    through a step by step approach for preparing the dataset, training the networks,
    and finally generating the swapped outputs. The second exercise involved using
    the pix2pix GAN to perform re-enactment using live video as the source and Barack
    Obama as the target. We discussed issues and ways of overcoming the issues we
    faced with each of these implementations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二部分利用这一理解，提出了两个实际操作的练习，从零开始开发deepfake管道。我们首先致力于开发基于自动编码器的人脸交换架构。通过这项练习，我们逐步进行了准备数据集、训练网络，并最终生成交换输出的步骤。第二个练习涉及使用pix2pix
    GAN执行重新演绎，将实时视频用作源，将巴拉克·奥巴马用作目标。我们讨论了每个实施的问题以及克服这些问题的方法。
- en: In the final section, we presented a discussion about the ethical issues and
    challenges associated with deepfake architectures. We also touched on a few popular
    off-the-shelf projects that allow anyone and everyone with a computer or a smartphone
    to generate fake content.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们提出了有关deepfake架构相关的伦理问题和挑战的讨论。我们还简要介绍了一些流行的现成项目，这些项目允许任何拥有计算机或智能手机的人生成虚假内容。
- en: We covered a lot of ground in this chapter and worked on some very exciting
    use cases. It is important that we reiterate how vital it is to be careful when
    we are using technology as powerful as this. The implications and consequences
    could be very dangerous for the entities involved, so we should be mindful of
    how this knowledge is used.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中涵盖了很多内容，并且涉及一些非常令人兴奋的使用案例。重要的是要再次强调，当我们使用这样强大的技术时，要非常小心。所涉及的影响和后果对涉及的实体可能非常危险，因此我们应该注意如何使用这些知识。
- en: While this chapter focused on visual aspects, we will shift gears and move on
    to text in the next two chapters. The Natural Language Processing space is brimming
    with some exciting research and use cases. We will focus on some of the path-breaking
    textual generative works next. Stay tuned.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章主要关注视觉方面，但我们将转变方向，接下来的两章将讨论文本内容。自然语言处理领域涌现了一些激动人心的研究和应用案例。我们将重点关注一些开创性的文本生成作品。敬请期待。
- en: References
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: BuzzFeedVideo. (2018, April 17). *You Won't Believe What Obama Says In This
    Video! ;)* [Video]. YouTube. [https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo](https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo)
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BuzzFeedVideo. (2018年4月17日)。 *你绝对不会相信这个视频中奥巴马说了什么！;)* [视频]. YouTube。 [https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo](https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo)
- en: Lee, D. (2019, May 10). *Deepfake Salvador Dalí takes selfies with museum visitors*.
    The Verge. [https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum](https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum)
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lee, D. (2019年5月10日)。 *Deepfake萨尔瓦多·达利与博物馆参观者自拍*. The Verge. [https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum](https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum)
- en: Malaria Must Die. (2020). *A World Without Malaria*. Malaria Must Die. [https://malariamustdie.com/](https://malariamustdie.com/)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Malaria Must Die. (2020年)。 *没有疟疾的世界*. Malaria Must Die. [https://malariamustdie.com/](https://malariamustdie.com/)
- en: Lyons, K. (2020, February 18). *An Indian politician used AI to translate his
    speech into other languages to reach more voters*. The Verge. [https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections](https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lyons, K. (2020年2月18日)。 *一名印度政治家使用AI将演讲翻译成其他语言，以吸引更多选民*. The Verge. [https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections](https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections)
- en: Dietmar, J. (2019, May 21). *GANs And Deepfakes Could Revolutionize The Fashion
    Industry*. Forbes. [https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-fashion-industry/?sh=2502d4163d17](https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dietmar, J. (2019年5月21日)。 *GAN和深假技术可能会引领时尚业的革命*。福布斯。 [https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-fashion-industry/?sh=2502d4163d17](https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the)
- en: 'Statt, N. (2020, August 27). *Ronald Reagan sends you to do war crimes in the
    latest Call of Duty: Black Ops Cold War trailer*. The Verge. [https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ronald-reagan](https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ro)'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Statt, N. (2020年8月27日)。 *罗纳德·里根在最新的使命召唤：黑色行动冷战预告片中派你去犯战争罪*。The Verge。 [https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ronald-reagan](https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ro)
- en: Cole, S. (2017, December 11). *AI-Assisted Fake Porn Is Here and We’re All Fucked*.
    Vice. [https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn](https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn)
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cole, S. (2017年12月11日)。 *AI辅助的深假色情已经出现，我们都完蛋了*。Vice。 [https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn](https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn)
- en: dfaker & czfhhh. (2020). df. GitHub repository. [https://github.com/dfaker/df](https://github.com/dfaker/df)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dfaker & czfhhh. (2020年). df. GitHub 代码库. [https://github.com/dfaker/df](https://github.com/dfaker/df)
- en: 'Pumarola, A., Agudo, A., Martinez, A.M., Sanfeliu, A., & Moreno-Noguer, F.
    (2018). *GANimation: Anatomically-aware Facial Animation from a Single Image*.
    ECCV 2018\. [https://arxiv.org/abs/1807.09251](https://arxiv.org/abs/1807.09251)'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pumarola, A., Agudo, A., Martinez, A.M., Sanfeliu, A., & Moreno-Noguer, F.
    (2018年). *GANimation: 来自单个图像的解剖学感知面部动画*。ECCV 2018。[https://arxiv.org/abs/1807.09251](https://arxiv.org/abs/1807.09251)'
- en: Naruniec, J., Helminger, L., Schroers, C., & Weber, R.M. (2020). *High-Resolution
    Neural Face Swapping for Visual Effects. Eurographics Symposium on Rendering 2020*.
    [https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Neural-Face-Swapping-for-Visual-Effects.pdf](https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Ne)
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Naruniec, J., Helminger, L., Schroers, C., & Weber, R.M. (2020年). *用于视觉效果的高分辨率神经面部交换。Eurographics渲染研讨会2020*。[https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Neural-Face-Swapping-for-Visual-Effects.pdf](https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Ne)
- en: Geng, Z., Cao, C., & Tulyakov, S. (2019). *3D Guided Fine-Grained Face Manipulation*.
    arXiv. [https://arxiv.org/abs/1902.08900](https://arxiv.org/abs/1902.08900)
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Blanz, V., & Vetter, T. (1999). *A morphable model for the synthesis of 3D
    faces*. SIGGRAPH ''99: Proceedings of the 26th annual conference on Computer graphics
    and interactive techniques. 187-194\. [https://cseweb.ucsd.edu/~ravir/6998/papers/p187-blanz.pdf](https://cseweb.ucsd.edu/~ravir/6998/papers/p187-blanz.pdf)'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaipeng, Z., Zhang, Z., Li, Z., & Qiao, Y. (2016). *Joint Face Detection and
    Alignment using Multi-task Cascaded Convolutional Networks*. IEEE Signal Processing
    Letters (SPL), vol. 23, no. 10, pp. 1499-1503, 2016\. [https://kpzhang93.github.io/MTCNN_face_detection_alignment/](https://kpzhang93.github.io/MTCNN_face_detection_alignment/)
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Burt, T., & Horvitz, E. (2020, September 1). *New Steps to Combat Disinformation*.
    Microsoft blog. [https://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authenticator/](https://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authen)
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deepware. (2021). *Deepware - Scan & Detect Deepfake Videos With a Simple tool*.
    [https://deepware.ai/](https://deepware.ai/)
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Siarohin, A., Lathuiliere, S., Tulyakov, S., Ricci, E., & Sebe, N. (2019). *First
    Order Motion Model for Image Animation*. NeurIPS 2019\. [https://aliaksandrsiarohin.github.io/first-order-model-website/](https://aliaksandrsiarohin.github.io/first-order-model-website/)
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tulyakov, S., Liu, M-Y., Yang, X., & Kautz, J. (2017). *MoCoGAN: Decomposing
    Motion and Content for Video Generation*. arXiv. [https://arxiv.org/abs/1707.04993](https://arxiv.org/abs/1707.04993)'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wang, T-C., Liu, M-Y., Zhu, J-Y., Liu, G., Tao, A., Kautz, J., Catanzaro, B.
    (2018). *Video-to-Video Synthesis*. NeurIPS, 2018\. [https://arxiv.org/abs/1808.06601](https://arxiv.org/abs/1808.06601)
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: torzdf & 77 other contributors. (2021). faceswap. GitHub repository. [https://github.com/Deepfakes/faceswap](https://github.com/Deepfakes/faceswap)
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 20\. iperov & 18 other contributors. (2021). DeepFaceLab. GitHub repository.
    [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab)
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: shaoanlu, silky, clarle, & Ja1r0\. (2019). faceswap-GAN. GitHub repository.
    [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN)
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
