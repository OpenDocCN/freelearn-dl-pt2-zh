- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deepfakes with GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manipulating videos and photographs to edit artifacts has been in practice for
    quite a long time. If you have seen movies like *Forrest Gump* or *Fast and Furious
    7*, chances are you did not even notice that the scenes with John F. Kennedy or
    Paul Walker in their respective movies were fake and edited into the movies as required.
  prefs: []
  type: TYPE_NORMAL
- en: You may recall one particular scene from the movie *Forrest Gump*, where Gump
    meets John F. Kennedy. The scene was created using complex visual effects and
    archival footage to ensure high-quality results. Hollywood studios, spy agencies
    from across the world, and media outlets have been making use of editing tools
    such as Photoshop, After Effects, and complex custom visual effects/CGI (computer generated
    imagery) pipelines to come up with such compelling results. While the results
    have been more or less believable in most instances, it takes a huge amount of
    manual effort and time to edit each and every detail, such as scene lighting,
    face, eyes, and lip movements, as well as shadows, for every frame of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Along the same lines, there is a high chance you might have come across a Buzzfeed video¹
    where former US president Barack Obama says "Killmonger was right" (Killmonger
    is one of Marvel Cinematic Universe's villains). While obviously fake, the video
    does seem real in terms of its visual and audio aspects. There are a number of
    other examples where prominent personalities can be seen making comments they
    would usually not.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping ethics aside, there is one major difference between Gump meeting John
    F. Kennedy and Barack Obama talking about Killmonger. As mentioned earlier, the
    former is the result of painstaking manual work done using complex visual effects/CGI.
    The latter, on the other hand, is the result of a technology called **deepfakes**.
    A portmanteau of the words *deep learning* and *fake*, *deepfake* is a broad term
    used to describe AI-enabled technology that is used to generate the examples we
    discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover different concepts, architectures, and components
    associated with deepfakes. We will focus on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the deepfakes technological landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The different forms of deepfaking: replacement, re-enactment, and editing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key features leveraged by different architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level deepfakes workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping faces using autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-enacting Obama's face movements using pix2pix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and ethical issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief discussion of off-the-shelf implementations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover the internal workings of different GAN architectures and key contributions
    that have enabled deepfakes. We will also build and train these architectures
    from scratch to get a better understanding of them. Deepfakes are not limited
    to videos or photographs, but are also used to generate fake text (news articles,
    books) and even audio (voice clips, phone calls). In this chapter, we will focus
    on videos/images only and the term *deepfakes* refers to related use cases, unless stated
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin with an overview of deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deepfakes is an all-encompassing term representing content generated using artificial
    intelligence (in particular, deep learning) that seems realistic and authentic
    to a human being. The generation of fake content or manipulation of existing content
    to suit the needs and agenda of the entities involved is not new. In the introduction,
    we discussed a few movies where CGI and painstaking manual effort helped in generating
    realistic results. With advancements in deep learning and, more specifically,
    generative models, it is becoming increasingly difficult to differentiate between
    what is real and what is fake.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) have played a very important
    role in this space by enabling the generation of sharp, high-quality images and
    videos. Works such as [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com),
    based on StyleGAN, have really pushed the boundaries in terms of the generation
    of high-quality realistic content. A number of other key architectures (some of
    which we discussed in *Chapter 6*, *Image Generation with GANs*, and *Chapter
    7*, *Style Transfer with GANs*) have become key building blocks for different
    deepfake setups.'
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes have a number of applications, which can be categorized into creative,
    productive, and unethical or malicious use cases. The following are a few examples
    that highlight the different use cases of deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creative and productive use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recreating history and famous personalities**: There are a number of historical
    figures we would love to interact with and learn from. With the ability to manipulate
    and generate realistic content, deepfakes are just the right technology for such
    use cases. A large-scale experiment of this type was developed to bring famous
    surrealist painter Salvador Dali back to life. The Dali Museum, in collaboration
    with the ad agency GS&P, developed an exhibition entitled Dali Lives.² The exhibition
    used archival footage and interviews to train a deepfake setup on thousands of
    hours of videos. The final outcome was a re-enactment of Dali''s voice and facial
    expressions. Visitors to the museum were greeted by Dali, who then shared his
    life''s stories with them. Toward the end, Dali even proposed a selfie with the
    visitors, and the output photographs were realistic selfies indeed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Movie translation**: With the likes of Netflix becoming the norm these days,
    viewers are watching far more cross-lingual content than ever before. While subtitles
    and manual dubbing are viable options, they leave a lot to be desired. With deepfakes,
    using AI to autogenerate dubbed translations of any video is easier than ever.
    The social initiative known as *Malaria Must Die* created a powerful campaign
    leveraging a similar technique to help David Beckham, a famous footballer, speak
    in nine different languages to help spread awareness.³ Similarly, deepfakes have
    been used by a political party in India, where a candidate is seen speaking in
    different languages as part of his election campaign.⁴'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fashion**: Making use of GANs and other generative models to create new styles
    and fashion content is not new. With deepfakes, researchers, bloggers, and fashion
    houses are taking the fashion industry to new levels. We now have AI-generated
    digital models that are adorning new fashion line-ups and help in reducing costs.
    This technology is even being used to create renderings of models personalized
    to mimic a buyer''s body type, to improve the chances of a purchase.⁵'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video game characters**: Video games have improved a lot over the years,
    with many modern games presenting cinema class graphics. Traditionally, human
    actors have been leveraged to create characters within such games. However, there
    is now a growing trend of using deepfakes and related technologies to develop
    characters and storylines. The developers of the game *Call of Duty* released
    a trailer showing former US president Ronald Reagan playing one of the characters
    in the game.⁶'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stock images**: Marketing flyers, advertisements, and official documents
    sometimes require certain individuals to be placed alongside the rest of the content.
    Traditionally, actual actors and models have been used. There are also stock image
    services that license such content for commercial use. With works such as [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com),
    it is now very easy to generate a new face or personality as per our requirements,
    without any actual actors or models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malicious use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pornography**: The ability to generate fake content as per our requirements
    has grave consequences. Indeed, deepfakes came into the public eye when, in 2017,
    a notorious fake pornographic video was posted by a Reddit user with a celebrity''s
    face swapped on.⁷ After this, there have been whole communities working toward
    generating such fake videos, which can be very damaging to the public image of
    the people they depict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impersonation**: We''ve already discussed a fake video of former US president
    Barack Obama talking about a number of topics and things he would usually avoid.
    Creating such videos to impersonate public figures, politicians, and so on can
    have huge consequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deepfakes entail realistic looking content that can be categorized into a number
    of subcategories. In the next section, we will present a discussion on the different
    categories to better understand the overall landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Modes of operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating believable fake content requires taking care of multiple aspects
    to ensure that the results are as authentic as possible. A typical deepfake setup
    requires a **source**, **a target**, and **the generated content**.
  prefs: []
  type: TYPE_NORMAL
- en: The source, denoted with subscript *s*, is the driver identity that controls
    the required output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target, denoted with subscript *t*, is the identity being faked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated content, denoted with subscript *g*, is the result following the
    transformation of the source to the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have some basic terminology in place, let's dive deeper and understand
    different ways of generating fake content.
  prefs: []
  type: TYPE_NORMAL
- en: Replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most widely used form of generating fake content. The aim is to
    replace specific content of the target (*x*[t]) with that from the source (*x*[s]).
    Face replacement has been an active area of research for quite some time now.
    *Figure 8.1* shows Donald Trump''s face being replaced with Nicolas Cage''s. The
    figure displays both source (*x*[s]) and target (*x*[t]) identities, while the
    generated content (*x*[g]) is shown in the last column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Face replacement⁸'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replacement techniques can be broadly categorized into:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer**: This is a basic form of replacement where the content of *x*[s]
    (for example, the face in the case of face replacement) is transferred to *x*[t].
    The transfer method is mostly leveraged in a coarse context, in other words, the
    replacement is not as clean or smooth as one would expect. For example, for clothes
    shopping, users might be interested in visualizing themselves in different outfits.
    Such applications can afford to leave out very detailed information yet still
    give users the required experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swap**: This is a slightly more sophisticated type of replacement where the
    transfer to *x*[t] is guided by certain characteristics of *x*[t] itself. For
    instance, in *Figure 8.1*, the bottom row shows Nicolas Cage''s face getting swapped
    onto Donald Trump''s face. The replacement image maintains the characteristics
    of Trump''s (the target image''s) hair, pose, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The replacement mode, despite sounding trivial, is not so simple, since any
    models need to focus on a number of factors relating to image lighting, skin colors,
    occlusions, and shadows. The handling of some of these aspects will be discussed
    in later sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Re-enactment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Replacement methods yield impressive results, but the generated content leaves
    scope for improvement. Re-enactment methods are utilized to capture characteristics
    such as the pose, expression, and gaze of the target to improve upon the believability
    of the generated content. Re-enactment techniques focus on the following aspects
    to improve the quality of the fake content:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaze**: The aim is to focus on the eyes and the position of the eyelids.
    Techniques in this area try to re-enact the generated output''s gaze based on
    the source''s eye movements/gaze. This is useful in improving photographs or maintaining
    eye contact in videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mouth**: Re-enacting lips and the mouth region of a face improves the believability
    of the generated content. In this case, the mouth movements of *x*[t] are conditioned
    on the mouth movements of *x*[s]. In certain cases, the source input *x*[s] could
    be speech or other audio. Mouth re-enactment methods are also called Bol methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expression**: This is a more generic form of re-enactment that often includes
    other re-enactment aspects such as the eyes, mouth, and pose. These are used to
    drive the expression of *x*[t] on the basis of *x*[s].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pose**: Pose re-enactments, for both the head and the body, are all-encompassing
    methods that consider the positioning of the head and the whole body. In this
    case as well, the source drives the target and yields more believable results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These re-enactments are better depicted in *Figure 8.2*, where we have the source
    (*x*[s]) and target (*x*[t]) shown on the left of the figure. The right side of
    the figure shows how different aspects of the source impact the generated content.
    Please note that *Figure 8.2* is for illustrative purposes only and the results
    are not mere copy and paste editing of the target content. We will see more evolved
    examples as we progress through the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Re-enactment methods. Impacted regions are highlighted for each
    re-enactment'
  prefs: []
  type: TYPE_NORMAL
- en: Specific regions that are the focus of different types of re-enactments have
    been highlighted specifically in *Figure 8.2*. As mentioned earlier, it is quite
    apparent that expression re-enactments encompass the eye and mouth regions as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Editing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deepfakes do not necessarily concern replacement or re-enactment. Another application
    of deepfakes is to add, remove, or alter certain aspects of the target entity
    to serve specific objectives. Editing could involve manipulation of clothing,
    age, ethnicity, gender, hair, and so on. A few possible edits are depicted in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Deepfakes in Edit mode. The left image is the base input for transformation.
    The right image depicts three different edits: hair, spectacles, and age.'
  prefs: []
  type: TYPE_NORMAL
- en: The edits on the right-hand side of *Figure 8.3* showcase how certain attributes
    of the input image can be transformed to generate fake content. There are a number
    of benign use cases that are either for fun (apps such as **FaceApp** and **REFACE**)
    or have commercial value (eyewear and cosmetics brands). Yet there are a number
    of malicious applications (pornography, fake identities, and so on) that undermine
    and raise questions about the use of such tools.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the basics of the different modes of generating fake content
    and discussed the major areas of focus for each of the modes. In the next section,
    we will discuss what features play a role in training such models and how we leverage
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Key feature set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The human face and body are key entities in this task of fake content generation.
    While deep learning architectures usually do not require hand-crafted features,
    a little nudge goes a long way when complex entities are involved. Particularly
    when dealing with the human face, apart from detecting the overall face in a given
    image or video, a deepfake solution also needs to focus on the eyes, mouth, and
    other features. We discussed different modes of operation in the previous section,
    where we highlighted the importance of different sections of a face and their
    impact on improving the believability of the fake content generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will briefly cover a few important features leveraged by
    different deepfake solutions. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Facial Action Coding System (FACS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D Morphable Model (3DMM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial landmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also undertake a couple of hands-on exercises to better understand these
    feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: Facial Action Coding System (FACS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Developed by Carl-Herman Hjortsjö in 1969, and later adopted and refined by
    Ekamn et al. in 1978, Facial Action Coding System, or FACS, is an anatomy-based
    system for understanding facial movements. It is one of the most extensive and
    accurate coding systems for analyzing facial muscles to understand expressions
    and emotions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.4* depicts a few specific muscle actions and their associated meanings.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A sample set of action marking using FACS'
  prefs: []
  type: TYPE_NORMAL
- en: FACS consists of a detailed manual that is used by human coders to manually
    code each facial expression. The muscular activities are grouped into what are
    called Action Units, or AUs. These AUs represent muscular activities corresponding
    to facial expressions. A few sample AUs are described in *Figure 8.4*, pointing
    to the movement of eyebrows, lips, and other parts of the face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the original FACS system required human coders, there are automated
    systems now available to computationally determine the correct AUs. Works such
    as the following leverage automated AUs to generate realistic results:'
  prefs: []
  type: TYPE_NORMAL
- en: '*GANimation: Anatomically-aware* *Facial Animation from a Single Image*⁹'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High-Resolution* *Face Swapping for Visual Effects*^(10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*3D* *Guided Fine-Grained Face Manipulation*^(11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though FACS provides a fine-grained understanding of a given face's expressions,
    the complexity of the overall system limits its usage outside of professional
    animation/CGI/VFX studios.
  prefs: []
  type: TYPE_NORMAL
- en: 3D Morphable Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D Morphable Model, or 3DMM for short, is a method of inferring a complete 3D
    facial surface from a 2D image. Originally proposed by Blanz, Vetter, et al. in
    their work entitled *A Morphable Model for the Synthesis of 3D Faces*,^(12) this
    is a powerful statistical method that can model human face shape and texture along
    with pose and illumination.
  prefs: []
  type: TYPE_NORMAL
- en: The technique works by transforming the input image into a face mesh. The face
    mesh consists of vertices and edges that determine the shape and texture of each
    section of the face. The mesh helps in parameterizing the pose and expressions
    with a set of vectors and matrices. These vectors, or the 3D reconstruction itself,
    can then be used as input features for our fake content generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FACS and 3DMM-based features are highly accurate and expressive in terms of
    defining the characteristics of a human face (and body in general). Yet these
    methods are computationally expensive and sometimes even require human intervention
    (for example, FACS coding) for proper results. Facial landmarks are another feature
    set which are simple yet powerful, and are being used by a number of recent works
    to achieve state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks are a list of important facial features, such as the nose,
    eyebrows, mouth, and the corners of the eyes. The goal is the detection of these
    key features using some form of a regression model. The most common method is
    to leverage a predefined set of positions on the face or body that can be efficiently
    tracked using trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A facial landmark detection task can be broken down into the following two-step
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step involves localization of a face (or faces) in a given image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second step goes granular to identify key facial structures of the identified
    face(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two steps can be thought of as special cases of shape prediction. There
    are a couple of different methods we can use to detect facial landmarks as features
    for the task of fake content generation. In the following subsections, we will
    cover three of the most widely used methods: OpenCV, dlib, and MTCNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection using OpenCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV is a computer vision library aimed at handling real-time tasks. It is
    one of the most popular and widely used libraries, with wrappers available in
    a number of languages, Python included. It consists of a number of extensions
    and contrib-packages, such as the ones for face detection, text manipulation,
    and image processing. These packages enhance its overall capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection can be performed using OpenCV in a few different ways.
    One of the ways is to leverage Haar Cascade filters, which make use of histograms
    followed by an SVM for object detection. OpenCV also supports a DNN-based method
    of performing the same task.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection using dlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dlib is another cross-platform library which provides more or less similar functionality
    to OpenCV. The major advantage dlib offers over OpenCV is a list of pretrained
    detectors for faces as well as landmarks. Before we get onto the implementation
    details, let's learn a bit more about the landmark features.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks are granular details on a given face. Even though each face
    is unique, there are certain attributes that help us to identify a given shape
    as a face. This precise list of common traits is codified into what is called
    the **68-coordinate** or **68-point system**. This point system was devised for
    annotating the iBUG-300W dataset. This dataset forms the basis of a number of
    landmark detectors available through dlib. Each feature is given a specific index
    (out of 68) and has its own (*x*, *y*) coordinates. The 68 indices are indicated
    in *Figure 8.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The 68-point annotations from the iBUG-300W dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in the figure, each index corresponds to a specific coordinate and
    a set of indices mark a facial landmark. For instance, indices 28-31 correspond
    to the nose bridge and the detectors try to detect and predict the corresponding
    coordinates for those indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up dlib is a bit of an involved process, especially if you are on a
    Windows machine. Refer to setup guides such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/](https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117f](https://medium.com/analytics-vidhya/how-to-install-dlib-library-for-python-in-windows-10-57348ba1117)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now leverage this 68-coordinate system of facial landmarks to develop
    a short demo application for detecting facial features. We will make use of pretrained
    detectors from dlib and OpenCV to build this demo. The following snippet shows
    how a few lines of code can help us identify different facial landmarks easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code takes in an image of a face as input, converts it to grayscale,
    and marks the aforementioned 68 points onto the face using a dlib detector and
    predictor. Once we have these functions ready, we can execute the overall script.
    The script pops open a video capture window. The video output is overlaid with
    facial landmarks, as shown in *Figure 8.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: A sample video capture with facial landmark detection using pretrained
    detectors'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the pretrained facial landmark detector seems to be doing a
    great job. With a few lines of code, we were able to get specific facial features.
    In the later sections of the chapter, we will leverage these features for training
    our own deepfake architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmark detection using MTCNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a number of alternatives to OpenCV and dlib for face and facial landmark
    detection tasks. One of the most prominent and well performing ones is called
    **MTCNN**, short for **Multi-Task Cascaded Convolutional Networks**. Developed
    by Zhang, Zhang et al.,^(13) MTCNN is a complex deep learning architecture consisting
    of three cascaded networks. Together, these three networks help with the tasks
    of face and landmark identification. A discussion of the details of MTCNN are
    beyond the scope of this book, but we will briefly talk about its salient aspects
    and build a quick demo. Interested readers are requested to go through the original
    cited work for details.
  prefs: []
  type: TYPE_NORMAL
- en: The MTCNN setup, as mentioned earlier, makes use of three cascaded networks
    called P-Net, R-Net, and O-Net. Without going into much detail, the setup first
    builds a pyramid of the input image, i.e. the input image is scaled to different
    resolutions. The Proposal-Net, or P-Net, then takes these as input and outputs
    a number of potential bounding boxes that might contain a face. With some pre-processing
    steps in between, the Refine-Net, or R-Net, then refines the results by narrowing
    them down to the most probable bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: The final output is generated by Output-Net, or O-Net. O-Net outputs the final
    bounding boxes containing faces, along with landmark coordinates for the eyes,
    nose, and mouth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try out this state-of-the-art architecture to identify faces and
    corresponding landmarks. Luckily for us, MTCNN is available as a pip package,
    which is straightforward to use. In the following code listing, we will build
    a utility function to leverage MTCNN for our required tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As showcased in the code listing, the predictions from the MTCNN detector outputs
    two items for each detected face – the bounding box for the face and five coordinates
    for each facial landmark. Using these outputs, we can leverage OpenCV to add markers
    on the input image to visualize the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.7* depicts the sample output from this exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: MTCNN-based face and facial landmark detection'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure, MTCNN seems to have detected all the faces in the image
    along with the facial landmarks properly. With a few lines of code, we were able
    to use a state-of-the-art complex deep learning network to quickly generate the
    required outputs. Similar to the dlib/OpenCV exercise in the previous section,
    we can leverage MTCNN to identify key features which can be used as inputs for
    our fake content generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Another easy-to-use deep learning-based library for face detection and recognition
    is called `face_recognition`. This is a pip-installable package that provides
    straightforward APIs for both the tasks. For the task of face recognition (where
    the primary aim is to identify a person apart from just detecting a face), it
    makes use of VGGFace. VGGFace is a deep learning architecture developed by the
    Visual Geometry Group at Oxford University. It makes use of a VGG-style backbone
    to extract facial features. These features can then be leveraged for similarity
    matching. We will make use of this package in later sections of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have developed an understanding of different modes, along with different
    ways of identifying and extracting relevant features, let's get started with building
    a few such architectures of our own from scratch. In the coming sections, we will
    discuss a high-level flow for building a deepfake model and common architectures
    employed for this purpose, followed by hands-on training of a couple of these
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: High-level workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fake content generation is a complex task consisting of a number of components
    and steps that help in generating believable content. While this space is seeing
    quite a lot of research and hacks that improve the overall results, the setup
    can largely be explained using a few common building blocks. In this section,
    we will discuss a common high-level flow that describes how a deepfake setup uses
    data to train and generate fake content. We will also touch upon a few common
    architectures used in a number of works as basic building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed earlier, a deepfake setup requires a source identity (*x*[s])
    which drives the target identity (*x*[t]) to generate fake content (*x*[g]). To
    understand the high-level flow, we will continue with this notation, along with
    the concepts related to the key feature set discussed in the previous section.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input processing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input image (*x*[s] or *x*[t]) is processed using a face detector that identifies
    and crops the face
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The cropped face is then used to extract intermediate representations or features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intermediate representation along with a driving signal (*x*[s] or another
    face) is used to generate a new face
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blending**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A blending function then merges the generated face into the target as cleanly
    as possible
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Respective works employ additional interim or post-processing steps to improve
    the overall results. *Figure 8.8* depicts the main steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: High-level flow for creating deepfakes'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure, we use a photograph of Nicolas Cage as input and transform
    him into a fake photograph resembling Donald Trump. The key components used for
    each of these steps could be any of the various components presented so far in
    the chapter. For instance, the face crop step could leverage either dlib or MTCNN,
    and similarly, the key features used for the generation process could be either
    FACS AUs, facial landmarks, or the 3DMM vectors.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered aspects related to face cropping and key features which
    can be used in this fake content generation process. The next step in this process
    of deepfakes is the final output image, or video generation. Generative modeling
    is something we have covered in quite some depth in previous chapters, from variational
    autoencoders to different types of GANs. For the task of fake content generation,
    we will build upon these architectures. Readers should note that the deepfakes
    task is a special case, or rather a restricted use case, of different models we have
    covered in these previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now have a look at some of the most common architectures that are leveraged
    by different deepfake works.
  prefs: []
  type: TYPE_NORMAL
- en: Common architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most deepfake setups leverage known architectures with certain tweaks as building
    blocks for generating fake content. We have discussed most of these architectures
    in detail in *Chapters* *4*, *5*, *6*, and *7*. The following is a brief reiteration
    of the most commonly leveraged architectures for generating images or videos.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-Decoder (ED)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An encoder-decoder architecture consists of two components, an encoder and
    a decoder. The encoder component consists of a sequence of layers that start from
    the actual higher dimensional input, such as an image. The encoder then narrows
    the input down to a lower dimensional space or a vector representation, rightly
    termed **bottleneck features**. The decoder component takes the bottleneck features
    and decodes or transforms them to a different or the same vector space as the
    input. A typical ED architecture is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing monitor, screen, clock  Description automatically generated](img/B16176_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: A typical encoder-decoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: A special case of ED architecture is called an **autoencoder**. An autoencoder
    takes the input, transforms it into bottleneck features, and then reconstructs
    the original input as the output. Such networks are useful in learning feature
    representation on inputs. Another variant of ED architecture is called the **variational
    autoencoder**, or VAE. A VAE learns the posterior distribution of the decoder
    given the input space. We have seen in *Chapter 5*, *Painting Pictures with Neural
    Networks using VAEs*, how VAEs are better at learning and untangling representations,
    and better at generating content overall (as compared to autoencoders).
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GANs are implicit density modeling networks which have been used to generate
    very high-quality outputs in recent works. Without going into much detail, a GAN
    setup consists of two competing models, a generator and a discriminator. The generator
    is tasked with generating real-looking content based on a driving signal (noise
    vector, conditional inputs, and so on). The discriminator, on the other hand,
    is tasked with distinguishing fake from real. The two networks play a minimax
    game until we achieve an equilibrium state, with the generator being able to generate
    good enough samples to fool the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical GAN is depicted in *Figure 8.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing drawing  Description automatically generated](img/B16176_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: A typical GAN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: GANs are effective at generating high-quality outputs, and they have been the
    subject of significant research over the years. Improvements have led to some
    really powerful variants that have pushed the boundaries even further. Two of
    the most widely used variants in the context of deepfakes are **CycleGAN** and
    **pix2pix**. Both architectures were designed for image-to-image translation tasks.
    Pix2pix is a paired translation network, while CycleGAN does not require any pairing
    of the training samples. The effectiveness and simplicity of both these architectures
    make them perfect candidates for the task of deepfakes. We discussed both these
    architectures in detail in *Chapter 7*, *Style Transfer with GANs*; we encourage
    you to take a quick look at the previous chapter for a better understanding of
    the remaining sections in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered all the required building blocks in fair detail so far. Let's
    now leverage this understanding to implement a couple of deepfake setups from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Replacement using autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deepfakes are an interesting and powerful use of technology that is both useful
    and dangerous. In previous sections, we discussed different modes of operations
    and key features that can be leveraged, as well as common architectures. We also
    briefly touched upon the high-level flow of different tasks required to achieve
    the end results. In this section, we will focus on developing a face swapping
    setup using an autoencoder as our backbone architecture. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Task definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aim of this exercise is to develop a face swapping setup. As discussed earlier,
    face swapping is a type of replacement mode operation in the context of deepfake
    terminology. In this setup, we will focus on transforming Nicolas Cage (a Hollywood
    actor) into Donald J. Trump (former US president). In the upcoming sections, we
    will present each sub-task necessary for the preparation of data, training our
    models, and finally, the generation of swapped fake output images.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first and foremost task is data preparation. Since the aim is to develop
    a face swapper for Nicolas Cage and Donald Trump, we need datasets containing
    images of each of them. This task of data collection itself can be time-consuming
    and challenging for a number of reasons. Firstly, photographs could be restricted
    by licensing and privacy issues. Secondly, it is challenging to find good quality
    datasets that are publicly available. Finally, there is the challenge associated
    with identifying specific faces in photographs, as there could be multiple faces
    in a given photograph belonging to different people.
  prefs: []
  type: TYPE_NORMAL
- en: 'For copyright reasons, we cannot publish the training datasets that have been
    used to obtain the exact output in this chapter, as they have been scraped from
    a variety of online sources. However, websites that might prove useful for obtaining
    similar datasets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/deepfakes/faceswap/](https://github.com/deepfakes/faceswap/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/](http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/c/deepfake-detection-challenge/overview](https://www.kaggle.com/c/deepfake-detection-challenge/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assuming we already have the raw datasets collected, we can proceed to the
    next set of tasks: face detection and identification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first task is to define an entity class to hold face-related objects. We
    need such a class as we will need to pass images, extracted faces, and face landmarks,
    as well as transformations, through the pipeline. We define a class, `DetectedFace`,
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Abstracting these frequently used properties into an object (class) enables
    us to reduce the number of individual parameters needed to pass between different
    utilities. We discussed the `face_recognition` library in the *Key feature set*
    section. We will leverage the pose prediction model from this library to predict
    face locations using dlib''s `shape_predictor`. The following snippet instantiates
    the predictor objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This method takes an image as input and generates `DetectedFace` objects as
    outputs. Readers should note that we are yielding objects of type `DetectedFace`.
    The `yield` keyword ensures lazy execution, meaning objects are created when required.
    This ensures smaller memory requirements. The `DetectedFace` object, on the other
    hand, abstracts the extracted face and corresponding landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate the generator object returned by the `detect_faces` method to visualize
    all the identified faces. In the following snippet, we perform this visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample image used for face identification and extraction is showcased in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Sample image for face identification and extraction'
  prefs: []
  type: TYPE_NORMAL
- en: 'As visible in *Figure 8.11*, there are two faces corresponding to Donald Trump
    and Narendra Modi. The extracted faces using the `detect_faces` utility method
    are shown in *Figure 8.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a person  Description automatically generated with low confidence](img/B16176_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Extracted faces from sample image'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `FaceFilter` class requires a reference image as
    an input. This is intuitive; this reference serves as a ground truth to compare
    against in order to confirm whether or not we've found the right face. As mentioned
    earlier, the `face_recognition` package makes use of VGGFace to generate encoding
    for any image. We do this for the reference image and extract a vector representation
    for it. The `check` function in the `FaceFilter` class is then used to perform
    a similarity check (using metrics such as Euclidean distance or cosine similarity)
    between any new image and the reference image. If the similarity is below a certain
    threshold, it returns `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample image, reference image, and recognized face are shown in *Figure
    8.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Sample image, reference image, matched face, and unmatched face'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the figure, our `FaceFilter` class was able to identify which face
    belongs to Donald Trump and which doesn't. This is extremely useful for creating
    our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `extract` method in the `Extract` class takes in the whole image, along
    with the `DetectFace` object, as input. It also takes in a size parameter to resize
    the images to the required dimensions. We make use of cv2''s `warpAffine` and
    skimage''s `transform` methods to perform the alignment. Interested readers are
    requested to check out the official documentation of these libraries for more
    details. For now, we can consider these as helper functions that allow us to extract
    and align the detected faces. *Figure 8.14* shows the output after alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: Transformation from the input image to the extracted face and
    finally, the aligned face'
  prefs: []
  type: TYPE_NORMAL
- en: The transformation depicted in the figure highlights subtle differences between
    the raw extracted face and the aligned face. This transformation works for any
    face pose and helps align faces for better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the step-by-step tasks, let''s put everything in order
    to generate the required datasets. The following snippet combines all these steps
    into a single method for ease of use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We next use the `get_faces` method to write a high-level function which takes
    the raw images as input, along with other required objects, to extract and dump
    the relevant faces into an output directory. This is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We use `create_face_dataset` to scan through raw images of Donald Trump and
    Nicolas Cage to create the required datasets for us.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We prepared our datasets for both Donald Trump and Nicolas Cage using the tools
    presented in the previous section. Let's now work toward a model architecture
    that learns the task of face swapping.
  prefs: []
  type: TYPE_NORMAL
- en: We presented a few common architectures in earlier sections of the book. The
    encoder-decoder setup is one such setup widely used for deepfake tasks. For our
    current task of face swapping, we will develop an autoencoder setup to learn and
    swap faces. As has been the norm, we will make use of TensorFlow and Keras to
    prepare the required models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get onto actual architecture code, let''s briefly recap how this
    setup works. A typical autoencoder has two components, an encoder and a decoder.
    The encoder takes an image as input and compresses it down to a lower dimensional
    space. This compressed representation is called an embedding, or bottleneck features.
    The decoder works in the reverse manner. It takes the embedding vector as input
    and tries to reconstruct the image as output. In short, an autoencoder can be
    described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The autoencoder takes *x* as input and tries to generate a reconstruction ![](img/B16176_08_002.png)
    such that ![](img/B16176_08_003.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'With this brief overview of the autoencoder architecture, let''s get started
    with developing the required functions for both encoders and decoders. The following
    snippet shows a function that creates downsampling blocks for the encoder part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The downsampling block makes use of a two-dimensional convolutional layer followed
    by leaky ReLU activation. The encoder will make use of multiple such repeating
    blocks followed by fully connected and reshaping layers. We finally make use of
    an upsampling block to transform the output into an 8x8 image with 512 channels.
    The following snippet shows the upsampling block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The upsampling block is composed of a two-dimensional convolution, `LeakyReLU`,
    and finally an `UpSampling2D` layer. We use both the downsampling and upsampling
    blocks to create the encoder architecture, which is presented in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder, on the other hand, has a simpler setup. We make use of a few upsampling
    blocks followed by a convolutional layer to reconstruct the input image as its
    output. The following snippet shows the function for the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For our task of face swapping, we develop two autoencoders, one for each identity,
    in other words, one for Donald Trump and one for Nicolas Cage. The only trick
    is that both autoencoders share the same encoder. Yes, this architectural setup
    requires us to develop two autoencoders with specific decoders but a common encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'This trick works for a few simple reasons. Let''s discuss this a bit more.
    Let''s assume we have two autoencoders, Autoencoder-A and Autoencoder-B, composed
    of a common encoder but the respective decoders of Decoder-A and Decoder-B. This
    setup is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: Replacement using autoencoders'
  prefs: []
  type: TYPE_NORMAL
- en: 'The details regarding how this setup works are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Both autoencoders learn to reconstruct their respective inputs as they train
    using backpropagation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each autoencoder tries to minimize the reconstruction error. In our case, we
    will make use of **mean absolute error** (**MAE**) as our metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since both autoencoders have the same encoder, the encoder learns to understand
    both kinds of faces and transforms them into the embedding space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the input images by aligning and warping the faces ensures that
    the encoder is able to learn representations of both kinds of faces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The respective decoders, on the other hand, are trained to make use of the embeddings
    to reconstruct the images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once both the encoders are trained to our satisfaction, we proceed to face
    swapping. Let''s consider the scenario where we''re swapping the face of person
    B onto the face of person A:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with an image of person B. The input is encoded into a lower dimensional
    space by the encoder. Now, in place of using the decoder for B, we swap it with
    the decoder of A itself, i.e. Decoder-A. This is essentially Autoencoder-A with
    input from the dataset of person B.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face swapping using B as input for Autoencoder-A works because Autoencoder-A
    treats the face of B as if it were a warped version of face A itself (due to the
    common encoder in place).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, Decoder-A of Autoencoder-A generates an output image which seems like
    a look-alike of A with the characteristics of B.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s leverage this understanding to create the required autoencoders. The
    following snippet presents autoencoders for both types of faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We have two autoencoders that take 3 channel input images, each 64x64 in size.
    The encoder transforms these images into embeddings of size 8x8x512, while the
    decoder uses these embeddings to reconstruct output images of shape 64x64x3\.
    In the next section, we'll train these autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Training our own face swapper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have both the autoencoders in place, we need to prepare a custom
    training loop to train both networks together. However, before we get to the training
    loop, there are a few other utilities we need to define.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input datasets we have created for both personalities contain their faces
    under different lighting conditions, face positions, and other settings. Yet these
    cannot be exhaustive in nature. To ensure that we capture a larger variation of
    each type of face, we''ll make use of a few augmentation methods. The following
    snippet presents a function that applies random transformations to an input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `random_transform` function helps us to generate different warps of the
    same input face. This method ensures that we have enough variation for training
    our networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function required is a batch generator. Since we are dealing with
    images and large networks, it is imperative that we keep in mind the resource
    requirements. We make use of lazy execution utilities such as `yield` to keep
    memory/GPU requirements as low as possible. The following snippet shows our batch
    generator for the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the batch generator along with the augmentation functions,
    let''s prepare a training loop. This is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We train both the autoencoders for about 10,000 steps, or until the loss stabilizes.
    We utilize a batch size of 64 and save checkpoint weights every 100 epochs. Readers
    are free to play around with these parameters depending on their infrastructure
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: Results and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now trained the respective autoencoders for Nicolas Cage (Autoencoder-A)
    and Donald Trump (Autoencoder-B). The final step is to transform Nicolas Cage
    into Donald Trump. We described the steps earlier; we will use the autoencoder
    for Donald Trump with the input as Nicolas Cage, thereby generating an output
    that seems like a Nicolas Cage version of Donald Trump.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we get to the task of output generation, we require a few additional
    utilities. We discussed an additional step called **blending**. This step is performed
    post-output generation to ensure that the generated replacement and the original
    face seamlessly combine into a single image. Refer back to *Figure 8.8* for a
    visual reminder of the concept of blending. For our task, we prepare a blending
    class called `Convert`. The class is presented in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `patch_image` method relies on a few utility functions also defined in
    the class, `get_new_face`, `apply_new_face` and `get_image_mask`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This class takes in multiple parameters to improve the blending results. The
    parameters control aspects such as the size of the blurring kernel, the type of
    patch (such as rectangular or polygon), and the erosion kernel size. The class
    also takes the encoder as input. The class method `patch_image` does its magic
    with the help of transformation functions from the cv2 library and the parameters
    we set during instantiation. We use the following `convert` function to process
    each input face of type A and transform it into type B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated outputs are depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: Nicolas Cage transformed as Donald Trump'
  prefs: []
  type: TYPE_NORMAL
- en: The swapped output faces are encouraging, but not as seamless as we would have
    expected. Still, we can see that the model has learned to identify and swap the
    right portions of the face. The blending step has also tried to match the skin
    tone, face pose, and other aspects to make the results as realistic as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This seems like a good start, but leaves a lot for improvement. Here are a
    few limitations of our setup:'
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the swapped output is directly related to the capabilities of
    the trained autoencoders. Since there is no component to track how realistic the
    reconstructed outputs are, it is difficult to nudge the autoencoders in the right
    direction. Using a GAN would be a possible enhancement to provide a positive feedback
    to the overall training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs are a bit blurry. This is due to the difference in the actual input
    resolution and the generated output (64x64). Another reason for the blurry output
    is the use of MAE as a simple loss function. Research has shown that composite
    and complex losses help improve final output quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limited dataset is another reason for restricted output quality. We leveraged
    augmentation techniques to work around the limitation, but it is not a substitute
    for larger datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we developed a face swapping deepfake architecture from scratch.
    We went through a step-by-step approach to understand every component and step
    in the overall pipeline that swaps Nicolas Cage with Donald Trump. In the next
    section, we will use a more complex setup to try our hand at a different mode
    of operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code presented in this section is based on the original deepfake work as
    well as a simplified implementation of Olivier Valery''s code, which is available
    on GitHub at the following link: [https://github.com/OValery16/swap-face](https://github.com/OValery16/swap-face).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have trained our own face swapper, illustrating the replacement
    mode of operation, we can move onto the re-enactment mode.
  prefs: []
  type: TYPE_NORMAL
- en: Re-enactment using pix2pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Re-enactment is another mode of operation for the deepfakes setup. It is supposedly
    better at generating believable fake content compared to the replacement mode.
    In earlier sections, we discussed different techniques used to perform re-enactment,
    i.e. by focusing on gaze, expressions, the mouth, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed image-to-image translation architectures in *Chapter 7*, *Style
    Transfer with GANs*. Particularly, we discussed in detail how the pix2pix GAN
    is a powerful architecture which enables paired translation tasks. In this section,
    we will leverage the pix2pix GAN to develop a face re-enactment setup from scratch.
    We will work toward building a network where we can use our own face, mouth, and
    expressions to control Barack Obama's (former US president) face. We will go through
    each and every step, starting right from preparing the dataset, to defining the
    pix2pix architecture, to finally generating the output re-enactment. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the pix2pix GAN as the backbone network for our current task
    of re-enactment. While pix2pix is a powerful network that trains with very few
    training samples, there is a restriction that requires the training samples to
    be paired. In this section, we will use this restriction to our advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Since the aim is to analyze a target face and control it using a source face,
    we can leverage what is common between faces to develop a dataset for our use
    case. The common characteristics between different faces are the presence of facial
    landmarks and their positioning. In the *Key feature set* section, we discussed
    how simple and easy it is to build a facial landmark detection module using libraries
    such as dlib, cv2, and MTCNN.
  prefs: []
  type: TYPE_NORMAL
- en: For our current use case, we will prepare paired training samples consisting
    of pairs of landmarks and their corresponding images/photographs. For generating
    re-enacted content, we can then simply extract facial landmarks of the source
    face/controlling entity and use pix2pix to generate high quality actual output
    based on the target person.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the source/controlling personality could be you or any other person,
    while the target personality is Barack Obama.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare our dataset, we will extract frames and corresponding landmarks of
    each frame from a video. Since we want to train our network to be able to generate
    high-quality coloured output images based on landmark inputs, we need a video
    of Barack Obama. You could download this from various different sources on the
    internet. Please note that this exercise is again for academic and educational
    purposes only. Kindly use any videos carefully and with caution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating a paired dataset of landmark and video frames is a straightforward
    application of the code snippets given in the *Facial landmarks* section. To avoid
    repetition, we leave this as an exercise for the reader. Please note that complete
    code is available in the code repository for this book. We generated close to
    400 paired samples from one of the speeches of Barack Obama. *Figure 8.17* presents
    a few of these samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, different, person  Description automatically
    generated](img/B16176_08_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Paired training samples consisting of facial landmarks and corresponding
    video frames'
  prefs: []
  type: TYPE_NORMAL
- en: We can see how the landmarks capture the position of the head along with the
    movement of the lips, eyes, and other facial landmarks. We are thus able to generate
    a paired training dataset in almost no time. Let's now move on to network setup
    and training.
  prefs: []
  type: TYPE_NORMAL
- en: Pix2pix GAN setup and training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed the pix2pix architecture along with its sub-components and objective
    functions in detail in *Chapter 7*, *Style Transfer with GANs*. In this section,
    we will briefly touch upon them for the sake of completeness.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that we are reusing the utility functions prepared as part of *Chapter
    7*, *Style Transfer with GANs*. Unlike the generator, which has a specific setup,
    the discriminator network for pix2pix is a fairly straightforward implementation.
    We present the discriminator network in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We use these two functions to prepare our generator, discriminator, and GAN
    network objects. The objects are created as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The training loop is straightforward; we make use of the three network objects
    (discriminator, generator, and the overall GAN model) and alternately train the
    generator and discriminator. Note that the facial landmarks dataset is used as
    input, while the video frames are output for this training process. The training
    loop is presented in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '*Figures 8.18* and *8.19* showcase the training progress of this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Training progress for pix2pix GAN for face re-enactment (epoch
    1)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_08_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: Training progress for the pix2pix GAN for face re-enactment (epoch
    40)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figures, the model is able to capture key facial
    features and their positioning, along with the background details. In the initial
    iterations (*Figure 8.18*), the model seems to be having difficulty in generating
    the mouth region, but as the training progresses, it learns to fill it with the
    right set of details (*Figure 8.19*).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our GAN trained for the required task, let's perform some re-enactments
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Results and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the chapter so far, we have dealt mostly with images or photographs as input.
    Since the pix2pix GAN is a very efficient implementation, it can be leveraged
    to generate outputs in near-real time. This capability therefore implies that
    we can use such a trained model to perform re-enactments using a live video feed.
    In other words, we can use a live video feed of ourselves to re-enact Barack Obama's
    face movements and expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: These functions help extract and draw facial landmarks from a given frame, and
    use those landmarks to generate output of colored frames using the pix2pix GAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to use these functions to process the live video feed and
    generate re-enacted output samples. This manipulation is fast enough to enhance
    the believability of the fake content. The following snippet presents the manipulation
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16176_08_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.20: Re-enactment using live video as the source and Obama as the target
    using the pix2pix GAN'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.20* presents how seamlessly the overall setup works. We are able
    to capture a live video, convert it into facial landmarks, and then generate re-enactments
    using the pix2pix GAN. It is apparent how, in the live video, there are no objects
    in the background, but our network is able to generate the American flag correctly.
    The samples also showcase how the model is able to capture expressions and head
    tilt nicely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the results are encouraging, they are far from being perceived as real
    or believable. The following are a few limitations associated with the approach
    we discussed in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs in *Figure 8.20* are a bit fuzzy. They turn completely blank or
    incomprehensible if the head is tilted quite a lot or if the person in the live
    video feed is too close or too far from the camera. This issue is mostly because
    the pix2pix GAN has learned the relative size and position of facial landmarks
    with respect to the training dataset. This can be improved by performing face
    alignment and using tighter crops for both the input and inference stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model's generated content is highly dependent upon the training data. Since
    our training dataset is derived from a speech, there is limited head movement
    and very limited facial expression. Thus, if you try to move the head a bit too
    much or present an expression that isn't in the training dataset, the model makes
    a very poor guess. A larger dataset with more variability can help fix this issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen how a powerful image-to-image translation GAN architecture can
    be reused for the task of re-enactment.
  prefs: []
  type: TYPE_NORMAL
- en: In the last two sections, we covered a number of interesting hands-on exercises
    to develop replacement and re-enactment architectures from scratch. We discussed
    some of the issues with our setup and how we could improve upon them. In the following
    section, we will discuss some of the challenges associated with deepfake systems.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some of the common challenges associated with
    deepfake architectures, beginning with a brief discussion on the ethical issues
    associated with this technology.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though generating fake content is not a new concept, the word "deepfake"
    came into the limelight in 2017 when a Reddit user by the name u/deepfakes posted
    fake pornographic videos with celebrity faces superimposed on them using deep
    learning. The quality of the content and the ease with which the user was able
    to generate them created huge uproar on news channels across the globe. Soon,
    u/deepfakes released an easy-to-setup application called **FakeApp** that enabled
    users to generate such content with very little knowledge of how deep learning
    works. This led to a number of fake videos and objectionable content. This, in
    turn, helped people gain traction on issues associated with identity theft, impersonation,
    fake news, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Soon, interest picked up within the academic community, which not only helped
    to improve the technology but also insisted on its ethical use. While there are
    malicious and objectionable content creators making use of these techniques, a
    number of industry and research projects are underway to detect such fake content,
    such as Microsoft's deepfake detection tool and Deepware.^(14 15)
  prefs: []
  type: TYPE_NORMAL
- en: Technical challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ethical issues aside, let''s also discuss a few challenges that are quite apparent
    for a typical deepfake setup: generalization, occlusions, and temporal issues.'
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deepfake architectures are generative models at their core, which are highly
    dependent on the training dataset used. These architectures typically also require
    huge amounts of training samples, which could be hard to get, especially for the
    target (or victim in the case of malicious use). Another issue is the paired training
    setup. Typically, a model trained for one source and target pair is not so easy
    to use for another pair of source and target personalities.
  prefs: []
  type: TYPE_NORMAL
- en: Work on efficient architectures that can train with smaller amounts of training
    data is an active area of research. The development of CycleGAN and other unpaired
    translation architectures is also helping in overcoming the paired training bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Occlusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source or target inputs might have artifacts around them that obstruct certain
    features. This could be due to hand movements, hair, eyewear, or other objects.
    Another type of occlusion occurs due to dynamic changes in the mouth and eye region.
    This can lead to inconsistent facial features or weird cropped imagery. Certain
    works are focusing on avoiding such issues by making use of segmentation, in-painting,
    and other related techniques. One example of such a work is *First Order Motion
    Model for Image Generation* by Siarohin et al.^(16)
  prefs: []
  type: TYPE_NORMAL
- en: Temporal issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deepfake architectures work on a frame-by-frame basis (when it comes to video
    inputs). This results in jitter, flickering, or complete incoherence between subsequent
    frames. We saw an example of this with the re-enactment exercise using the pix2pix
    GAN in the previous section. The model is unable to generate coherent output for
    unseen scenarios. To improve upon this, some researchers are trying to use RNNs
    (recurrent neural networks) with GANs to generate coherent outputs. Examples of
    this include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MoCoGAN: Decomposing Motion and Content for Video Generation*^(17)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video-to-Video Synthesis*^(18)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-the-shelf implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a step-by-step approach to developing two different
    deepfake architectures for replacement and re-enactment. Although the implementations
    are easy to understand and execute, they require quite a bit of understanding
    and resources to generate high-quality results.
  prefs: []
  type: TYPE_NORMAL
- en: Since the release of u/deepfakes' content in 2017, a number of open source implementations
    have come out to simplify the use of this technology. While dangerous, most of
    these projects highlight the ethical implications and caution developers and users
    in general against the malicious adoption of such projects. While it is beyond
    the scope of this chapter, we list a few well-designed and popular implementations
    in this section. Readers are encouraged to go through specific projects for more
    details.
  prefs: []
  type: TYPE_NORMAL
- en: '**FaceSwap**^(19) The developers of this project claim this implementation
    is close to the original implementation by u/deepfakes, with enhancements over
    the years to improve output content quality. This project provides a detailed
    documentation and step-by-step guide for preparing the training dataset and generating
    the fake content. They also share pretrained networks for speeding up the training
    process. This project has a graphical interface for completely novice users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepFaceLab**^(20) This is one of the most extensive, detailed, and popular
    deepfakes projects available on the internet. This project is based on the recent
    paper with the same name presented in May 2020\. The project consists of a detailed
    user guide, video tutorials, a very mature GUI, pretrained models, Colab notebooks,
    datasets, and even Docker images for quick deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FaceSwap-GAN**^(21) A simple, yet effective, implementation using an ED+GAN
    setup. This project provides utilities and ready-to-use notebooks for quickly
    training your own models. The project also provides pretrained models for direct
    use or transfer learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of Android and iOS apps that work along the same lines and
    lower the entry barrier to a bare minimum. Today, anybody with a smartphone or
    a little understanding of technical concepts can use or train such setups with
    ease.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deepfakes are a complicated subject both ethically and technically. In this
    chapter, we discussed the deepfake technology in general to start with. We presented
    an overview of what deepfakes are all about and briefly touched upon a number
    of productive as well as malicious use cases. We presented a detailed discussion
    on different modes of operation of different deepfake setups and how each of these
    impacts the overall believability of generated content. While deepfakes is an
    all-encompassing term associated with videos, images, audio, text, and so on,
    we focused on visual use cases only in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Given our scope, we discussed various feature sets leveraged by different works
    in this space. In particular, we discussed the Facial Action Coding System (FACS),
    3D Morphable Models (3DMM), and facial landmarks. We also discussed how we can
    perform facial landmark detection using libraries such as dlib and MTCNN. We then
    presented a high-level flow of tasks to be performed for a deepfakes pipeline.
    In conjunction with this, we discussed a few common architectures that are widely
    used to develop such systems.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the chapter leveraged this understanding to present two hands-on
    exercises to develop deepfake pipelines from scratch. We first worked toward developing
    an autoencoder-based face swapping architecture. Through this exercise, we worked
    through a step by step approach for preparing the dataset, training the networks,
    and finally generating the swapped outputs. The second exercise involved using
    the pix2pix GAN to perform re-enactment using live video as the source and Barack
    Obama as the target. We discussed issues and ways of overcoming the issues we
    faced with each of these implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section, we presented a discussion about the ethical issues and
    challenges associated with deepfake architectures. We also touched on a few popular
    off-the-shelf projects that allow anyone and everyone with a computer or a smartphone
    to generate fake content.
  prefs: []
  type: TYPE_NORMAL
- en: We covered a lot of ground in this chapter and worked on some very exciting
    use cases. It is important that we reiterate how vital it is to be careful when
    we are using technology as powerful as this. The implications and consequences
    could be very dangerous for the entities involved, so we should be mindful of
    how this knowledge is used.
  prefs: []
  type: TYPE_NORMAL
- en: While this chapter focused on visual aspects, we will shift gears and move on
    to text in the next two chapters. The Natural Language Processing space is brimming
    with some exciting research and use cases. We will focus on some of the path-breaking
    textual generative works next. Stay tuned.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BuzzFeedVideo. (2018, April 17). *You Won't Believe What Obama Says In This
    Video! ;)* [Video]. YouTube. [https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo](https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lee, D. (2019, May 10). *Deepfake Salvador Dalí takes selfies with museum visitors*.
    The Verge. [https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum](https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Malaria Must Die. (2020). *A World Without Malaria*. Malaria Must Die. [https://malariamustdie.com/](https://malariamustdie.com/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lyons, K. (2020, February 18). *An Indian politician used AI to translate his
    speech into other languages to reach more voters*. The Verge. [https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections](https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dietmar, J. (2019, May 21). *GANs And Deepfakes Could Revolutionize The Fashion
    Industry*. Forbes. [https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-fashion-industry/?sh=2502d4163d17](https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statt, N. (2020, August 27). *Ronald Reagan sends you to do war crimes in the
    latest Call of Duty: Black Ops Cold War trailer*. The Verge. [https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ronald-reagan](https://www.theverge.com/2020/8/27/21403879/call-of-duty-black-ops-cold-war-gamescom-2020-trailer-ro)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cole, S. (2017, December 11). *AI-Assisted Fake Porn Is Here and We’re All Fucked*.
    Vice. [https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn](https://www.vice.com/en/article/gydydm/gal-gadot-fake-ai-porn)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dfaker & czfhhh. (2020). df. GitHub repository. [https://github.com/dfaker/df](https://github.com/dfaker/df)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pumarola, A., Agudo, A., Martinez, A.M., Sanfeliu, A., & Moreno-Noguer, F.
    (2018). *GANimation: Anatomically-aware Facial Animation from a Single Image*.
    ECCV 2018\. [https://arxiv.org/abs/1807.09251](https://arxiv.org/abs/1807.09251)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naruniec, J., Helminger, L., Schroers, C., & Weber, R.M. (2020). *High-Resolution
    Neural Face Swapping for Visual Effects. Eurographics Symposium on Rendering 2020*.
    [https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Neural-Face-Swapping-for-Visual-Effects.pdf](https://s3.amazonaws.com/disney-research-data/wp-content/uploads/2020/06/18013325/High-Resolution-Ne)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geng, Z., Cao, C., & Tulyakov, S. (2019). *3D Guided Fine-Grained Face Manipulation*.
    arXiv. [https://arxiv.org/abs/1902.08900](https://arxiv.org/abs/1902.08900)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Blanz, V., & Vetter, T. (1999). *A morphable model for the synthesis of 3D
    faces*. SIGGRAPH ''99: Proceedings of the 26th annual conference on Computer graphics
    and interactive techniques. 187-194\. [https://cseweb.ucsd.edu/~ravir/6998/papers/p187-blanz.pdf](https://cseweb.ucsd.edu/~ravir/6998/papers/p187-blanz.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaipeng, Z., Zhang, Z., Li, Z., & Qiao, Y. (2016). *Joint Face Detection and
    Alignment using Multi-task Cascaded Convolutional Networks*. IEEE Signal Processing
    Letters (SPL), vol. 23, no. 10, pp. 1499-1503, 2016\. [https://kpzhang93.github.io/MTCNN_face_detection_alignment/](https://kpzhang93.github.io/MTCNN_face_detection_alignment/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Burt, T., & Horvitz, E. (2020, September 1). *New Steps to Combat Disinformation*.
    Microsoft blog. [https://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authenticator/](https://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authen)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deepware. (2021). *Deepware - Scan & Detect Deepfake Videos With a Simple tool*.
    [https://deepware.ai/](https://deepware.ai/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Siarohin, A., Lathuiliere, S., Tulyakov, S., Ricci, E., & Sebe, N. (2019). *First
    Order Motion Model for Image Animation*. NeurIPS 2019\. [https://aliaksandrsiarohin.github.io/first-order-model-website/](https://aliaksandrsiarohin.github.io/first-order-model-website/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tulyakov, S., Liu, M-Y., Yang, X., & Kautz, J. (2017). *MoCoGAN: Decomposing
    Motion and Content for Video Generation*. arXiv. [https://arxiv.org/abs/1707.04993](https://arxiv.org/abs/1707.04993)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wang, T-C., Liu, M-Y., Zhu, J-Y., Liu, G., Tao, A., Kautz, J., Catanzaro, B.
    (2018). *Video-to-Video Synthesis*. NeurIPS, 2018\. [https://arxiv.org/abs/1808.06601](https://arxiv.org/abs/1808.06601)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: torzdf & 77 other contributors. (2021). faceswap. GitHub repository. [https://github.com/Deepfakes/faceswap](https://github.com/Deepfakes/faceswap)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 20\. iperov & 18 other contributors. (2021). DeepFaceLab. GitHub repository.
    [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: shaoanlu, silky, clarle, & Ja1r0\. (2019). faceswap-GAN. GitHub repository.
    [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
