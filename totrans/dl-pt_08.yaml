- en: Modern Network Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we explored how deep learning algorithms can be used to
    create artistic images, create new images based on existing datasets, and generate
    text. In this chapter, we will introduce you to different network architectures
    that power modern computer vision applications and natural language systems. Some
    of the architectures that we will look at in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important things that we do when the deep learning model fails to
    learn is to add more layers to the model. As you add layers the model accuracy
    improves and then starts saturating. It starts degrading as you keep on adding
    more layers. Adding more layers beyond a certain number will add certain challenges,
    such as vanishing or exploding gradients, which is partially solved by carefully
    initializing weights and introducing intermediate normalizing layers. Modern architectures,
    such as **residual network** (**ResNet**) and Inception, try to solve this problem
    by introducing different techniques, such as residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ResNet solves these problems by explicitly letting the layers in the network
    fit a residual mapping by adding a shortcut connection. The following image shows how
    ResNet works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/458fcc47-ed90-46e3-aa8d-15d3a1c6b36b.png)'
  prefs: []
  type: TYPE_IMG
- en: In all the networks we have seen, we try to find a function that maps the input
    (*x*) to its output (*H(x)*) by stacking different layers. But the authors of
    ResNet proposed a fix; instead of trying to learn an underlying mapping from *x*
    to *H(x)*, we learn the difference between the two, or the residual. Then, to
    calculate *H(x)*, we can just add the residual to the input. Say the residual
    is *F(x) = H(x) - x*; instead of trying to learn *H(x)* directly, we try to learn
    *F(x) + x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each ResNet block consists of a series of layers, and a shortcut connection
    adding the input of the block to the output of the block. The add operation is
    performed element-wise and the inputs and outputs need to be of the same size.
    If they are of a different size, then we can use paddings. The following code
    demonstrates what a simple ResNet block would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `ResNetBasicBlock` contains an `init` method which initializes all the different
    layers, such as the convolution layer, batch normalization, and ReLU layers. The
    `forward` method is almost similar to what we have seen up until now, except that
    the input is being added back to the layer's output just before it is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch `torchvision` package provides an out-of-the-box ResNet model with
    different layers. Some of the different models available are:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also use any of these models for transfer learning. The `torchvision`
    instance enables us to simply create one of these models and use them. We have
    done this a couple of times in the book, and the following code is a refresher
    for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows what a 34-layer ResNet model would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e36d8826-ab65-403c-aec4-affe9b965fa4.png)'
  prefs: []
  type: TYPE_IMG
- en: 34 layer ResNet model
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this network consists of multiple ResNet blocks. There have
    been experiments where teams have tried models as deep as 1,000 layers. For most
    real-world use cases, my personal recommendation would be to start with a smaller
    network. Another key advantage of these modern networks is that they need very
    few parameters compared to models such as VGG, as they avoid using fully connected
    layers that need lots of parameters to train. Another popular architecture that
    is being used to solve problems in the computer vision field is **Inception**.
    Before moving on to Inception architecture, let''s train a ResNet model on the
    `Dogs vs. Cats` dataset. We will use the data that we used in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*,
    Deep Learning for Computer Vision,* and will quickly train a model based on features
    calculated from ResNet. As usual, we will follow these steps to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating PyTorch datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating loaders for training and validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the ResNet model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract convolutional features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a simple linear model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once done, we are going to repeat this step for Inception and DenseNet. At the
    end, we will also explore the ensembling technique, where we combine these powerful
    models to build a new model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating PyTorch datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We create a transformation object containing all the basic transformations
    required and use the `ImageFolder` to load the images from the data directory
    that we created in [Chapter](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml) *[5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    Deep Learning for Computer Vision*. In the following code, we create the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By now, most of the preceding code will be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating loaders for training and validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use PyTorch loaders to load the data provided by the dataset in the form
    of batches, along with all the advantages, such as shuffling the data and using
    multi-threads, to speed up the process. The following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We need to maintain the exact sequence of the data while calculating the pre-convoluted
    features. When we allow the data to be shuffled, we will not be able to maintain
    the labels. So, ensure the `shuffle` is `False`, otherwise the required logic
    needs to be handled inside the code.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ResNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the layers of the `resnet34` pretrained model, we create a PyTorch sequential
    model by discarding the last linear layer. We will use this trained model for
    extracting features from our images. The following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we created a `resnet34` model available in `torchvision`
    models. In the following line, we pick all the ResNet layers, excluding the last
    layer, and create a new model using `nn.Sequential`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `nn.Sequential` instance allows us to quickly create a model using a bunch
    of PyTorch layers. Once the model is created, do not forget to set the `requires_grad`
    parameter to `False`, as this will allow PyTorch not to maintain any space for
    holding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting convolutional features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We pass the data from the train and validation data loaders through the model
    and store the results of the model in a list for further computation. By calculating
    the pre-convoluted features, we can save a lot of time in training the model,
    as we will not be calculating these features in every iteration. In the following
    code, we calculate the pre-convulted features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once we calculate the pre-convoluted features, we need to create a custom dataset
    that can pick data from our pre-convoluted features. Let's create a custom dataset
    and loader for the pre-convoluted features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen how to create a PyTorch dataset. It should be a subclass
    of the `torch.utils.data` dataset class and should implement the `__getitem__(self,
    index)` and `__len__(self)` methods, which return the length of the data in the
    dataset. In the following code, we implement a custom dataset for the pre-convoluted
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the custom dataset class is created, creating a data loader for the pre-convoluted
    features is straightforward, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to create a simple linear model that can map the pre-convoluted
    features to the corresponding categories.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple linear model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a simple linear model that will map the pre-convoluted features
    to the respective categories. In this case, the number of categories is two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are good to train our new model and validate the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same `fit` function that we have been using from [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*,
    Deep Learning for Computer Vision*. I am not including that here, to save space.
    The following code snippet contains functionality to train the model and shows
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the results, the model achieves a 98% training accuracy and
    97% validation accuracy. Let's understand another modern architecture and how
    to use it for calculating pre-convoluted features and use them to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most of the deep learning algorithms we have seen for computer vision models,
    we either pick up a convolution layer with a filter size of 1 x 1, 3 x 3, 5 x
    5, 7 x 7, or a map pooling layer. The Inception module combines convolutions of
    different filter sizes and concatenates all the outputs together. The following
    image makes the Inception model clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6b94415-ec97-49ff-8c59-775631d0ad41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: https://arxiv.org/pdf/1409.4842.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this Inception block image, the convolution of different sizes is applied
    to the input, and the outputs of all these layers are concatenated. This is the
    simplest version of an Inception module. There is another variant of an Inception
    block where we pass the input through a 1 x 1 convolution before passing it through
    3 x 3 and 5 x 5 convolutions. A 1 x 1 convolution is used for dimensionality reduction.
    It helps in solving computational bottlenecks. A 1 x 1 convolution looks at one
    value at a time and across the channels. For example, using a 10 x 1 x 1 filter
    on an input size of 100 x 64 x 64 would result in 10 x 64 x 64\. The following
    figure shows the Inception block with dimensionality reductions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d91e925-80cd-4aa7-b94c-26083e8f2534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: https://arxiv.org/pdf/1409.4842.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at a PyTorch example of what the preceding Inception block
    would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code contains two classes, `BasicConv2d` and `InceptionBasicBlock`.
    `BasicConv2d` acts like a custom layer which applies a two-dimensional convolution
    layer, batch normalization, and a ReLU layer to the input that is passed through.
    It is good practice to create a new layer when we have a repeating code structure,
    to make the code look elegant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `InceptionBasicBlock` implements what we have in the second Inception figure.
    Let''s go through each smaller snippet and try to understand how it is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code transforms the input by applying a 1 x 1 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block followed by a 5 x 5 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block followed by a 3 x 3 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we apply an average pool along with a 1 x 1 convolution
    block, and at the end, we concatenate all the results together. An Inception network
    would consist of several Inception blocks. The following image shows what an Inception
    architecture would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dd6a8d2-c984-49d7-87d9-57cab172b60c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inception architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision` package has an Inception network which can be used in the
    same way we used the ResNet network. There were many improvements made to the
    initial Inception block, and the current implementation available from PyTorch
    is Inception v3\. Let''s look at how we can use the Inception v3 model from `torchvision`
    to calculate pre-computed features. We will not go through the data loading process
    as we will be using the same data loaders from the previous ResNet section. We
    will look at the following important topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Inception model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting convolutional features using `register_forward_hook`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new dataset for the convoluted features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a fully connected model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an Inception model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Inception v3 model has two branches, each of which generates an output,
    and in the original model training, we would merge the losses as we did for style
    transfer. As of now we are interested in using only one branch to calculate pre-convoluted
    features using Inception. Getting into the details of this is outside the scope
    of the book. If you are interested in knowing more about how it works, then going
    through the paper and the source code ([https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py))
    of the Inception model would help. We can disable one of the branches by setting
    the `aux_logits` parameter to `False`. The following code explains how to create
    a model and set the `aux_logits` parameter to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Extracting the convolution features from the Inception model is not straightforward,
    as with ResNet, so we will use the `register_forward_hook` to extract the activations.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting convolutional features using register_forward_hook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the same techniques that we used to calculate activations
    for style transfer. The following is the `LayerActivations` class with some minor
    modifications, as we are interested in extracting only outputs of a particular
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from the `hook` function, the rest of the code is similar to what we
    have used for style transfer. As we are capturing the outputs of all the images
    and storing them, we will not be able to hold the data on **graphics processing
    unit** (**GPU**) memory. So we extract the tensors from GPU to CPU and just store
    the tensors instead of `Variable`. We are converting it back to tensors as the
    data loaders will work only with tensors. In the following code, we use the objects
    of `LayerActivations` to extract the output of the Inception model at the last
    layer, excluding the average pooling layer, dropout and the linear layer. We are
    skipping the average pooling layer to avoid losing useful information in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Let's create the datasets and loaders required for the new convoluted features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new dataset for the convoluted features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the same `FeaturesDataset` class to create the new dataset and data
    loaders. In the following code, we create the datasets and the loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a new model to train on the pre-convoluted features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully connected model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simple model may end in overfitting, so let''s include dropout in the model.
    Dropout will help avoid overfitting. In the following code, we are creating our
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is created, we can train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the same fit and training logic as seen in the previous ResNet and other
    examples. We will just look at the training code and the results from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the results, the Inception model achieves 99% accuracy on the training
    and 97.8% accuracy on the validation dataset. As we are pre-computing and holding
    all the features in the memory, it takes less than a few minutes to train the
    models. If you are running out of memory when you run the program on your machine,
    then you may need to avoid holding the features in the memory.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at another interesting architecture, DenseNet, which has become
    very popular in the last year.
  prefs: []
  type: TYPE_NORMAL
- en: Densely connected convolutional networks – DenseNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the successful and popular architectures, such as ResNet and Inception,
    have shown the importance of deeper and wider networks. ResNet uses shortcut connections
    to build deeper networks. DenseNet takes it to a new level by introducing connections
    from each layer to all other subsequent layers, that is a layer where one could
    receive all the feature maps from the previous layers. Symbolically, it would
    look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28672c69-1d67-410d-8e49-63158824882c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure describes what a five-layer dense block would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7f3e5fa-b3b5-49e3-968f-6423cba0e915.png)'
  prefs: []
  type: TYPE_IMG
- en: Image source: https://arxiv.org/abs/1608.06993
  prefs: []
  type: TYPE_NORMAL
- en: There is a DenseNet implementation of torchvision ([https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)).
    Let's look at two major functionalities, `_DenseBlock` and `_DenseLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: DenseBlock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the code for `DenseBlock` and then walk through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`DenseBlock` is a sequential module where we add layers in a sequential order.
    Based on the number of layers (`num_layers`) in the block, we add that number
    of `_Denselayer` objects along with a name to it. All the magic is happening inside
    the `DenseLayer`. Let''s look at what goes on inside the `DenseLayer`.'
  prefs: []
  type: TYPE_NORMAL
- en: DenseLayer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One good way to learn how a particular network works is to look at the source
    code. PyTorch has a very clean implementation and most of the time is easily readable.
    Let''s look at the `DenseLayer` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you are new to inheritance in Python, then the preceding code may not look
    intuitive. The `_DenseLayer` is a subclass of `nn.Sequential`; let's look at what
    goes on inside each method.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` method, we add all the layers that the input data needs to
    be passed to. It is quite similar to all the other network architectures we have
    seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The magic happens in the `forward` method. We pass the input to the `forward`
    method of the `super` class, which is `nn.Sequential`. Let''s look at what happens
    in the `forward` method of the sequential class ([https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The input is passed through all the layers that were previously added to the
    sequential block and the output is concatenated to the input. The process is repeated
    for the required number of layers in a block.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the understanding of how a `DenseNet` block works, let''s explore how
    we can use DenseNet for calculating pre-convoluted features and building a classifier
    model on top of it. At a high level, the DenseNet implementation is similar to
    the VGG implementation. The DenseNet implementation also has a features module,
    which contains all the dense blocks, and a classifier module, which contains the
    fully connected model. We will be going through the following steps to build the
    model. We will be skipping most of the part that is similar to what we have seen
    for Inception and ResNet, such as creating the data loader and datasets. Also,
    we will discuss the following steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DenseNet model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting DenseNet features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a dataset and loaders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a fully connected model and train
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now, most of the code will be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DenseNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Torchvision has a pretrained DenseNet model with different layer options (121,
    169, 201, 161). We have chosen the model with `121` layers. As discussed, the
    DenseNet has two modules: features (containing the dense blocks), and classifier
    (fully connected block). As we are using DenseNet as an image feature extractor,
    we will only use the feature module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let's extract the DenseNet features from the images.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting DenseNet features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is quite similar to what we did for Inception, except we are not using `register_forward_hook`
    to extract features. The following code shows how the DenseNet features are extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is similar to what we have seen for Inception and ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset and loaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same `FeaturesDataset` class that we created for ResNet and
    use it to create data loaders for the `train` and `validation` dataset in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Time to create the model and train it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully connected model and train
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use a simple linear model, similar to what we used in ResNet and Inception.
    The following code shows the network architecture which we will be using to train
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same `fit` method to train the preceding model. The following
    code snippet shows the training code, along with the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The preceding algorithm was able to achieve a maximum training accuracy of 99%,
    and 99% validation accuracy. Your results could change as the `validation` dataset
    you create may have different images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages of DenseNet are:'
  prefs: []
  type: TYPE_NORMAL
- en: It substantially reduces the number of parameters required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It alleviates the vanishing gradient problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages feature reuse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this next section, we will explore how we can build a model that combines
    the advantage of the convoluted features computed, using the different models
    of ResNet, Inception, and DenseNet.
  prefs: []
  type: TYPE_NORMAL
- en: Model ensembling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There could be times when we would need to try to combine multiple models to
    build a very powerful model. There are many techniques that can be used for building
    an ensemble model. In this section, we will learn how to combine outputs using
    the features generated by three different models (ResNet, Inception, and DenseNet)
    to build a powerful model. We will be using the same dataset that we used for
    other examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture for the ensemble model would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/203dd83d-e8f6-4786-9e55-a5904274c72d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This image shows what we are going to do in the ensemble model, which can be
    summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create three models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the image features using the created models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a custom dataset which returns features of all the three models along
    with the labels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create model similar to the architecture in the preceding figure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and validate the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's explore each of the steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create all the three required models, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now we have all the models, let's extract the features from the images.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the image features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we combine all the logic that we have seen individually for the algorithms
    in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: By now, we have created image features using all the models. If you are facing
    memory issues, then you can either remove one of the models, or stop storing the
    features in the memory, which could be slow to train. If you are running this
    on a CUDA instance, then you can go for a more powerful instance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom dataset along with data loaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will not be able to use the `FeaturesDataset` class as it is, since it was
    developed to pick from the output of only one model. So, the following implementation
    contains minor changes to the `FeaturesDataset` class to accommodate all the three
    different generated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have made changes to the `__init__` method to store all the features generated
    from different models, and the `__getitem__` method to retrieve the features and
    label of an image. Using the `FeatureDataset` class, we created dataset instances
    for both training and validation data. Once the dataset is created, we can use
    the same data loader for batching data, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Creating an ensembling model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to create a model that is similar to the architecture diagram show
    previously. The following code implements this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we create three linear layers that take features generated
    from different models. We sum up all the outputs from these three linear layers
    and pass them on to another linear layer, which maps them to the required categories.
    To prevent the model from overfitting, we have used dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to make some minor changes to the `fit` method to accommodate the three
    input-values generated from the data loader. The following code implements the
    new `fit` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the previous code, most of it remains the same, except that
    the loader returns three inputs and one label. So, we make changes in the function,
    which is self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The ensemble model achieves a 99.6% training accuracy and a validation accuracy
    of 99.3%. Though ensemble models are powerful, they are computationally expensive.
    They are good techniques to use when you are solving problems in competitions
    such as Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almost all the deep learning algorithms we have seen in the book are good at
    learning how to map training data to their corresponding labels. We cannot use
    them directly for tasks where the model needs to learn from a sequence and generate
    another sequence or an image. Some of the example applications are:'
  prefs: []
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation (seq2img)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these problems can be seen as some form of sequence-to-sequence mapping,
    and these can be solved using a family of architectures called **encoder–decoder
    architectures**. In this section, we will learn about the intuition behind these
    architectures. We will not be looking at the implementation of these networks,
    as they need to be studied in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, an encoder–decoder architecture would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ec104a4-761f-4cdd-aac0-21e12af8a215.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An encoder is usually a **recurrent neural network** (**RNN**) (for sequential
    data) or a **Convolution Neural Network** (**CNN**) (for images) that takes in
    an image or a sequence and converts it into a fixed length vector which encodes
    all the information. The decoder is another RNN or CNN, which learns to decode
    the vector generated by the encoder and generates a new sequence of data. The
    following image shows how the encoder–decoder architecture looks for an image
    captioning system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdb83743-f9c2-489e-af1e-c611523bbfb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder-decoder architecture for image captioning system
  prefs: []
  type: TYPE_NORMAL
- en: Image source: https://arxiv.org/pdf/1411.4555.pdf
  prefs: []
  type: TYPE_NORMAL
- en: Let's look in more detail at what happens inside an encoder and a decoder architecture
    for an image captioning system.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For an image captioning system, we will preferably use a trained architecture,
    such as ResNet or Inception, for extracting features from the image. Like we did
    for the ensemble model, we can output a fixed vector length by using a linear
    layer, and then make that linear layer trainable.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decoder is a **Long Short-Term Memory** (**LSTM**) layer which will generate
    a caption for an image. To build a simple model, we can just pass the encoder
    embedding as input to the LSTM only once. But it could be quite challenging for
    the decoder to learn; instead, it is common practice to provide the encoder embedding
    at every step of the decoder. Intuitively, a decoder learns to generate a sequence
    of text that best describes the caption of a given image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored some modern architectures, such as ResNet, Inception,
    and DenseNet. We also explored how we can use these models for transfer learning
    and ensembling, and introduced the encoder–decoder architecture, which powers
    a lot of systems, such as language translation systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will arrive at a conclusion of what we have achieved
    in our learning journey through the book, as well as discuss where can you go
    from here. We will visit a plethora of resources on PyTorch and some cool deep
    learning projects that have been created or are undergoing research using PyTorch.
  prefs: []
  type: TYPE_NORMAL
