- en: Modern Network Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代网络架构
- en: 'In the last chapter, we explored how deep learning algorithms can be used to
    create artistic images, create new images based on existing datasets, and generate
    text. In this chapter, we will introduce you to different network architectures
    that power modern computer vision applications and natural language systems. Some
    of the architectures that we will look at in this chapter are:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们探讨了如何使用深度学习算法创建艺术图像，基于现有数据集创建新图像以及生成文本。在本章中，我们将介绍驱动现代计算机视觉应用和自然语言系统的不同网络架构。本章我们将看到的一些架构包括：
- en: ResNet
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: Inception
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception
- en: DenseNet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: Encoder-decoder architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: Modern network architectures
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代网络架构
- en: One of the important things that we do when the deep learning model fails to
    learn is to add more layers to the model. As you add layers the model accuracy
    improves and then starts saturating. It starts degrading as you keep on adding
    more layers. Adding more layers beyond a certain number will add certain challenges,
    such as vanishing or exploding gradients, which is partially solved by carefully
    initializing weights and introducing intermediate normalizing layers. Modern architectures,
    such as **residual network** (**ResNet**) and Inception, try to solve this problem
    by introducing different techniques, such as residual connections.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度学习模型学习失败时，我们通常会向模型中添加更多的层。随着层的增加，模型的准确性会提高，然后开始饱和。继续添加更多层之后，准确性会开始下降。超过一定数量的层会引入一些挑战，比如梯度消失或爆炸问题，这部分可以通过仔细初始化权重和引入中间归一化层来部分解决。现代架构，如**残差网络**（**ResNet**）和
    Inception，尝试通过引入不同的技术，如残差连接，来解决这些问题。
- en: ResNet
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet
- en: 'ResNet solves these problems by explicitly letting the layers in the network
    fit a residual mapping by adding a shortcut connection. The following image shows how
    ResNet works:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 通过显式让网络中的层适应一个残差映射来解决这些问题，通过添加快捷连接。下图展示了 ResNet 的工作原理：
- en: '![](img/458fcc47-ed90-46e3-aa8d-15d3a1c6b36b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/458fcc47-ed90-46e3-aa8d-15d3a1c6b36b.png)'
- en: In all the networks we have seen, we try to find a function that maps the input
    (*x*) to its output (*H(x)*) by stacking different layers. But the authors of
    ResNet proposed a fix; instead of trying to learn an underlying mapping from *x*
    to *H(x)*, we learn the difference between the two, or the residual. Then, to
    calculate *H(x)*, we can just add the residual to the input. Say the residual
    is *F(x) = H(x) - x*; instead of trying to learn *H(x)* directly, we try to learn
    *F(x) + x*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到的所有网络中，我们试图通过堆叠不同的层找到一个将输入（*x*）映射到其输出（*H(x)*）的函数。但是 ResNet 的作者提出了一个修正方法；不再试图学习从
    *x* 到 *H(x)* 的基础映射，而是学习两者之间的差异，或者残差。然后，为了计算 *H(x)*，我们可以将残差简单地加到输入上。假设残差为 *F(x)
    = H(x) - x*；与其直接学习 *H(x)*，我们尝试学习 *F(x) + x*。
- en: 'Each ResNet block consists of a series of layers, and a shortcut connection
    adding the input of the block to the output of the block. The add operation is
    performed element-wise and the inputs and outputs need to be of the same size.
    If they are of a different size, then we can use paddings. The following code
    demonstrates what a simple ResNet block would look like:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 ResNet 块由一系列层组成，并通过快捷连接将块的输入添加到块的输出。加法操作是逐元素进行的，输入和输出需要具有相同的大小。如果它们大小不同，我们可以使用填充。以下代码展示了一个简单的
    ResNet 块是如何工作的：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `ResNetBasicBlock` contains an `init` method which initializes all the different
    layers, such as the convolution layer, batch normalization, and ReLU layers. The
    `forward` method is almost similar to what we have seen up until now, except that
    the input is being added back to the layer's output just before it is returned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResNetBasicBlock` 包含一个 `init` 方法，用于初始化所有不同的层，如卷积层、批标准化层和 ReLU 层。`forward`
    方法与我们之前看到的几乎相同，唯一不同的是在返回之前将输入重新添加到层的输出中。'
- en: 'The PyTorch `torchvision` package provides an out-of-the-box ResNet model with
    different layers. Some of the different models available are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的 `torchvision` 包提供了一个带有不同层的即用型 ResNet 模型。一些可用的不同模型包括：
- en: ResNet-18
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-18
- en: ResNet-34
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-34
- en: ResNet-50
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-50
- en: ResNet-101
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-101
- en: ResNet-152
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-152
- en: 'We can also use any of these models for transfer learning. The `torchvision`
    instance enables us to simply create one of these models and use them. We have
    done this a couple of times in the book, and the following code is a refresher
    for that:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用这些模型中的任何一个进行迁移学习。`torchvision` 实例使我们能够简单地创建这些模型并使用它们。我们在书中已经做过几次，以下代码是对此的一次复习：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following figure shows what a 34-layer ResNet model would look like:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了34层ResNet模型的结构：
- en: '![](img/e36d8826-ab65-403c-aec4-affe9b965fa4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e36d8826-ab65-403c-aec4-affe9b965fa4.png)'
- en: 34 layer ResNet model
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 34层的ResNet模型
- en: 'We can see how this network consists of multiple ResNet blocks. There have
    been experiments where teams have tried models as deep as 1,000 layers. For most
    real-world use cases, my personal recommendation would be to start with a smaller
    network. Another key advantage of these modern networks is that they need very
    few parameters compared to models such as VGG, as they avoid using fully connected
    layers that need lots of parameters to train. Another popular architecture that
    is being used to solve problems in the computer vision field is **Inception**.
    Before moving on to Inception architecture, let''s train a ResNet model on the
    `Dogs vs. Cats` dataset. We will use the data that we used in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*,
    Deep Learning for Computer Vision,* and will quickly train a model based on features
    calculated from ResNet. As usual, we will follow these steps to train the model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个网络由多个ResNet块组成。有些团队进行了实验，尝试了深达1000层的模型。对于大多数实际应用场景，我个人推荐从一个较小的网络开始。这些现代网络的另一个关键优势是，它们与需要大量参数训练的模型（如VGG）相比，需要很少的参数，因为它们避免使用全连接层。在计算机视觉领域解决问题时，另一种流行的架构是**Inception**。在继续研究Inception架构之前，让我们在`Dogs
    vs. Cats`数据集上训练一个ResNet模型。我们将使用我们在[第5章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*深度学习计算机视觉*中使用的数据，并基于从ResNet计算的特征快速训练一个模型。像往常一样，我们将按照以下步骤训练模型：
- en: Creating PyTorch datasets
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建PyTorch数据集
- en: Creating loaders for training and validation
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于训练和验证的加载器
- en: Creating the ResNet model
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建ResNet模型
- en: Extract convolutional features
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取卷积特征
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个自定义的PyTorch数据集类，用于预处理的特征和加载器
- en: Creating a simple linear model
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个简单的线性模型
- en: Training and validating the model
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Once done, we are going to repeat this step for Inception and DenseNet. At the
    end, we will also explore the ensembling technique, where we combine these powerful
    models to build a new model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们将对Inception和DenseNet重复此步骤。最后，我们还将探讨集成技术，在其中结合这些强大的模型来构建一个新模型。
- en: Creating PyTorch datasets
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建PyTorch数据集
- en: 'We create a transformation object containing all the basic transformations
    required and use the `ImageFolder` to load the images from the data directory
    that we created in [Chapter](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml) *[5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    Deep Learning for Computer Vision*. In the following code, we create the datasets:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个包含所有基本变换的变换对象，并使用`ImageFolder`从我们在[章节](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*[5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)中创建的数据目录中加载图像，深度学习计算机视觉。在以下代码中，我们创建数据集：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By now, most of the preceding code will be self-explanatory.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，前面的大部分代码应该是不言自明的。
- en: Creating loaders for training and validation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于训练和验证的加载器
- en: 'We use PyTorch loaders to load the data provided by the dataset in the form
    of batches, along with all the advantages, such as shuffling the data and using
    multi-threads, to speed up the process. The following code demonstrates this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch加载器以批次形式提供数据集中的数据，同时使用所有优势，如数据洗牌和多线程，以加快处理速度。以下代码演示了这一点：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We need to maintain the exact sequence of the data while calculating the pre-convoluted
    features. When we allow the data to be shuffled, we will not be able to maintain
    the labels. So, ensure the `shuffle` is `False`, otherwise the required logic
    needs to be handled inside the code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算预处理特征时，我们需要保持数据的确切顺序。当我们允许数据被洗牌时，我们将无法保持标签的顺序。因此，请确保`shuffle`参数为`False`，否则需要在代码中处理所需的逻辑。
- en: Creating a ResNet model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个ResNet模型
- en: 'Using the layers of the `resnet34` pretrained model, we create a PyTorch sequential
    model by discarding the last linear layer. We will use this trained model for
    extracting features from our images. The following code demonstrates this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`resnet34`预训练模型的层，通过丢弃最后一个线性层创建PyTorch序列模型。我们将使用这个训练好的模型从我们的图像中提取特征。以下代码演示了这一点：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code, we created a `resnet34` model available in `torchvision`
    models. In the following line, we pick all the ResNet layers, excluding the last
    layer, and create a new model using `nn.Sequential`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了一个在`torchvision`模型中可用的`resnet34`模型。在下面的代码中，我们挑选所有的ResNet层，但排除最后一层，并使用`nn.Sequential`创建一个新模型：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `nn.Sequential` instance allows us to quickly create a model using a bunch
    of PyTorch layers. Once the model is created, do not forget to set the `requires_grad`
    parameter to `False`, as this will allow PyTorch not to maintain any space for
    holding gradients.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Sequential`实例允许我们快速创建一个使用一堆PyTorch层的模型。一旦模型创建完毕，不要忘记将`requires_grad`参数设置为`False`，这将允许PyTorch不维护任何用于保存梯度的空间。'
- en: Extracting convolutional features
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取卷积特征
- en: 'We pass the data from the train and validation data loaders through the model
    and store the results of the model in a list for further computation. By calculating
    the pre-convoluted features, we can save a lot of time in training the model,
    as we will not be calculating these features in every iteration. In the following
    code, we calculate the pre-convulted features:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过模型将训练和验证数据加载器传递，并将模型的结果存储在列表中以供进一步计算。通过计算预卷积特征，我们可以在训练模型时节省大量时间，因为我们不会在每次迭代中计算这些特征。在下面的代码中，我们计算预卷积特征：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once we calculate the pre-convoluted features, we need to create a custom dataset
    that can pick data from our pre-convoluted features. Let's create a custom dataset
    and loader for the pre-convoluted features.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了预卷积特征，我们需要创建一个能够从我们的预卷积特征中挑选数据的自定义数据集。让我们为预卷积特征创建一个自定义数据集和加载器。
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为预卷积特征创建自定义的PyTorch数据集类和加载器
- en: 'We have already seen how to create a PyTorch dataset. It should be a subclass
    of the `torch.utils.data` dataset class and should implement the `__getitem__(self,
    index)` and `__len__(self)` methods, which return the length of the data in the
    dataset. In the following code, we implement a custom dataset for the pre-convoluted
    features:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过如何创建PyTorch数据集。它应该是`torch.utils.data`数据集类的子类，并且应该实现`__getitem__(self,
    index)`和`__len__(self)`方法，这些方法返回数据集中的数据长度。在下面的代码中，我们为预卷积特征实现一个自定义数据集：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the custom dataset class is created, creating a data loader for the pre-convoluted
    features is straightforward, as shown in the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义数据集类之后，创建预卷积特征的数据加载器就很简单了，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we need to create a simple linear model that can map the pre-convoluted
    features to the corresponding categories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建一个简单的线性模型，它可以将预卷积特征映射到相应的类别。
- en: Creating a simple linear model
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的线性模型
- en: 'We will create a simple linear model that will map the pre-convoluted features
    to the respective categories. In this case, the number of categories is two:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个简单的线性模型，将预卷积特征映射到相应的类别。在这种情况下，类别的数量为两个：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we are good to train our new model and validate the dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练我们的新模型并验证数据集。
- en: Training and validating the model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'We will use the same `fit` function that we have been using from [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)*,
    Deep Learning for Computer Vision*. I am not including that here, to save space.
    The following code snippet contains functionality to train the model and shows
    the results:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的`fit`函数，该函数我们已经在《第5章》*计算机视觉的深度学习*中使用过。我没有在这里包含它，以节省空间。以下代码片段包含了训练模型和显示结果的功能：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果如下：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we can see from the results, the model achieves a 98% training accuracy and
    97% validation accuracy. Let's understand another modern architecture and how
    to use it for calculating pre-convoluted features and use them to train a model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从结果中看到的那样，模型达到了98%的训练精度和97%的验证精度。让我们了解另一种现代架构及其如何用于计算预卷积特征并用它们来训练模型。
- en: Inception
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception
- en: 'In most of the deep learning algorithms we have seen for computer vision models,
    we either pick up a convolution layer with a filter size of 1 x 1, 3 x 3, 5 x
    5, 7 x 7, or a map pooling layer. The Inception module combines convolutions of
    different filter sizes and concatenates all the outputs together. The following
    image makes the Inception model clearer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到的大多数计算机视觉模型的深度学习算法中，我们会选择使用卷积层，其滤波器大小为 1 x 1、3 x 3、5 x 5、7 x 7 或映射池化层。Inception
    模块结合了不同滤波器大小的卷积，并将所有输出串联在一起。下图使 Inception 模型更清晰：
- en: '![](img/f6b94415-ec97-49ff-8c59-775631d0ad41.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6b94415-ec97-49ff-8c59-775631d0ad41.png)'
- en: 'Image source: https://arxiv.org/pdf/1409.4842.pdf'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://arxiv.org/pdf/1409.4842.pdf
- en: 'In this Inception block image, the convolution of different sizes is applied
    to the input, and the outputs of all these layers are concatenated. This is the
    simplest version of an Inception module. There is another variant of an Inception
    block where we pass the input through a 1 x 1 convolution before passing it through
    3 x 3 and 5 x 5 convolutions. A 1 x 1 convolution is used for dimensionality reduction.
    It helps in solving computational bottlenecks. A 1 x 1 convolution looks at one
    value at a time and across the channels. For example, using a 10 x 1 x 1 filter
    on an input size of 100 x 64 x 64 would result in 10 x 64 x 64\. The following
    figure shows the Inception block with dimensionality reductions:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 Inception 块图像中，应用了不同尺寸的卷积到输入上，并将所有这些层的输出串联起来。这是一个 Inception 模块的最简单版本。还有另一种
    Inception 块的变体，我们在通过 3 x 3 和 5 x 5 卷积之前会先通过 1 x 1 卷积来减少维度。1 x 1 卷积用于解决计算瓶颈问题。1
    x 1 卷积一次查看一个值，并跨通道进行。例如，在输入大小为 100 x 64 x 64 的情况下，使用 10 x 1 x 1 的滤波器将导致 10 x 64
    x 64 的输出。以下图展示了具有降维的 Inception 块：
- en: '![](img/3d91e925-80cd-4aa7-b94c-26083e8f2534.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d91e925-80cd-4aa7-b94c-26083e8f2534.png)'
- en: 'Image source: https://arxiv.org/pdf/1409.4842.pdf'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://arxiv.org/pdf/1409.4842.pdf
- en: 'Now, let''s look at a PyTorch example of what the preceding Inception block
    would look like:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个 PyTorch 示例，展示前述 Inception 块的外观：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code contains two classes, `BasicConv2d` and `InceptionBasicBlock`.
    `BasicConv2d` acts like a custom layer which applies a two-dimensional convolution
    layer, batch normalization, and a ReLU layer to the input that is passed through.
    It is good practice to create a new layer when we have a repeating code structure,
    to make the code look elegant.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码包含两个类，`BasicConv2d` 和 `InceptionBasicBlock`。`BasicConv2d` 作为一个自定义层，将二维卷积层、批归一化和
    ReLU 层应用于传递的输入上。当我们有重复的代码结构时，创建一个新层是良好的做法，使代码看起来更优雅。
- en: 'The `InceptionBasicBlock` implements what we have in the second Inception figure.
    Let''s go through each smaller snippet and try to understand how it is implemented:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`InceptionBasicBlock` 实现了我们在第二个 Inception 图中看到的内容。让我们逐个查看每个较小的片段，并尝试理解它们的实现方式：'
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code transforms the input by applying a 1 x 1 convolution block:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码通过应用一个 1 x 1 卷积块来转换输入：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block followed by a 5 x 5 convolution block:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们通过应用一个 1 x 1 卷积块后跟一个 5 x 5 卷积块来转换输入：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block followed by a 3 x 3 convolution block:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们通过应用一个 1 x 1 卷积块后跟一个 3 x 3 卷积块来转换输入：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code, we apply an average pool along with a 1 x 1 convolution
    block, and at the end, we concatenate all the results together. An Inception network
    would consist of several Inception blocks. The following image shows what an Inception
    architecture would look like:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们应用了平均池化以及一个 1 x 1 卷积块，在最后，我们将所有结果串联在一起。一个 Inception 网络将由多个 Inception
    块组成。下图展示了 Inception 架构的外观：
- en: '![](img/8dd6a8d2-c984-49d7-87d9-57cab172b60c.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dd6a8d2-c984-49d7-87d9-57cab172b60c.jpg)'
- en: Inception architecture
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 架构
- en: 'The `torchvision` package has an Inception network which can be used in the
    same way we used the ResNet network. There were many improvements made to the
    initial Inception block, and the current implementation available from PyTorch
    is Inception v3\. Let''s look at how we can use the Inception v3 model from `torchvision`
    to calculate pre-computed features. We will not go through the data loading process
    as we will be using the same data loaders from the previous ResNet section. We
    will look at the following important topics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision` 包含一个 Inception 网络，可以像我们使用 ResNet 网络一样使用。对初始 Inception 块进行了许多改进，PyTorch
    提供的当前实现是 Inception v3。让我们看看如何使用 `torchvision` 中的 Inception v3 模型来计算预计算特征。我们将不会详细介绍数据加载过程，因为我们将使用之前
    ResNet 部分的相同数据加载器。我们将关注以下重要主题：'
- en: Creating an Inception model
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 Inception 模型
- en: Extracting convolutional features using `register_forward_hook`
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `register_forward_hook` 提取卷积特征
- en: Creating a new dataset for the convoluted features
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为卷积特征创建新数据集
- en: Creating a fully connected model
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建全连接模型
- en: Training and validating the model
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Creating an Inception model
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Inception 模型
- en: 'The Inception v3 model has two branches, each of which generates an output,
    and in the original model training, we would merge the losses as we did for style
    transfer. As of now we are interested in using only one branch to calculate pre-convoluted
    features using Inception. Getting into the details of this is outside the scope
    of the book. If you are interested in knowing more about how it works, then going
    through the paper and the source code ([https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py))
    of the Inception model would help. We can disable one of the branches by setting
    the `aux_logits` parameter to `False`. The following code explains how to create
    a model and set the `aux_logits` parameter to `False`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3 模型有两个分支，每个分支生成一个输出，在原始模型训练中，我们会像样式迁移那样合并损失。目前我们只关心使用一个分支来计算使用 Inception
    的预卷积特征。深入了解这一点超出了本书的范围。如果你有兴趣了解更多工作原理，阅读论文和 Inception 模型的源代码（[https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py)）将有所帮助。我们可以通过将
    `aux_logits` 参数设置为 `False` 来禁用其中一个分支。以下代码解释了如何创建模型并将 `aux_logits` 参数设置为 `False`：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Extracting the convolution features from the Inception model is not straightforward,
    as with ResNet, so we will use the `register_forward_hook` to extract the activations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Inception 模型中提取卷积特征并不像 ResNet 那样直接，因此我们将使用 `register_forward_hook` 来提取激活值。
- en: Extracting convolutional features using register_forward_hook
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `register_forward_hook` 提取卷积特征
- en: 'We will be using the same techniques that we used to calculate activations
    for style transfer. The following is the `LayerActivations` class with some minor
    modifications, as we are interested in extracting only outputs of a particular
    layer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与计算样式迁移激活值相同的技术。以下是 `LayerActivations` 类的一些小修改，因为我们只关注提取特定层的输出：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Apart from the `hook` function, the rest of the code is similar to what we
    have used for style transfer. As we are capturing the outputs of all the images
    and storing them, we will not be able to hold the data on **graphics processing
    unit** (**GPU**) memory. So we extract the tensors from GPU to CPU and just store
    the tensors instead of `Variable`. We are converting it back to tensors as the
    data loaders will work only with tensors. In the following code, we use the objects
    of `LayerActivations` to extract the output of the Inception model at the last
    layer, excluding the average pooling layer, dropout and the linear layer. We are
    skipping the average pooling layer to avoid losing useful information in the data:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `hook` 函数外，其余代码与我们用于样式迁移的代码类似。由于我们正在捕获所有图像的输出并存储它们，我们将无法在 **图形处理单元** (**GPU**)
    内存中保留数据。因此，我们将从 GPU 中提取张量到 CPU，并仅存储张量而不是 `Variable`。我们将其转换回张量，因为数据加载器只能处理张量。在以下代码中，我们使用
    `LayerActivations` 对象来提取 Inception 模型在最后一层的输出，跳过平均池化层、dropout 和线性层。我们跳过平均池化层是为了避免在数据中丢失有用信息：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's create the datasets and loaders required for the new convoluted features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建新的数据集和加载器，以便获取新的卷积特征。
- en: Creating a new dataset for the convoluted features
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为卷积特征创建新数据集
- en: 'We can use the same `FeaturesDataset` class to create the new dataset and data
    loaders. In the following code, we create the datasets and the loaders:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的 `FeaturesDataset` 类来创建新的数据集和数据加载器。在以下代码中，我们创建数据集和加载器：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let's create a new model to train on the pre-convoluted features.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的模型来在预卷积特征上进行训练。
- en: Creating a fully connected model
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个完全连接的模型
- en: 'A simple model may end in overfitting, so let''s include dropout in the model.
    Dropout will help avoid overfitting. In the following code, we are creating our
    model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的模型可能会导致过拟合，因此让我们在模型中包含 dropout。Dropout 可以帮助避免过拟合。在以下代码中，我们正在创建我们的模型：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the model is created, we can train the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，我们可以对模型进行训练。
- en: Training and validating the model
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'We use the same fit and training logic as seen in the previous ResNet and other
    examples. We will just look at the training code and the results from it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前的 ResNet 和其他示例中相同的拟合和训练逻辑。我们只会查看训练代码和其结果：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Looking at the results, the Inception model achieves 99% accuracy on the training
    and 97.8% accuracy on the validation dataset. As we are pre-computing and holding
    all the features in the memory, it takes less than a few minutes to train the
    models. If you are running out of memory when you run the program on your machine,
    then you may need to avoid holding the features in the memory.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，Inception 模型在训练集上达到了 99% 的准确率，在验证集上达到了 97.8% 的准确率。由于我们预先计算并将所有特征保存在内存中，训练模型只需不到几分钟的时间。如果在运行程序时出现内存不足的情况，则可能需要避免将特征保存在内存中。
- en: We will look at another interesting architecture, DenseNet, which has become
    very popular in the last year.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看另一个有趣的架构 DenseNet，在过去一年中变得非常流行。
- en: Densely connected convolutional networks – DenseNet
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集连接的卷积网络 – DenseNet
- en: 'Some of the successful and popular architectures, such as ResNet and Inception,
    have shown the importance of deeper and wider networks. ResNet uses shortcut connections
    to build deeper networks. DenseNet takes it to a new level by introducing connections
    from each layer to all other subsequent layers, that is a layer where one could
    receive all the feature maps from the previous layers. Symbolically, it would
    look like the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一些成功和流行的架构，如 ResNet 和 Inception，展示了更深更宽网络的重要性。ResNet 使用快捷连接来构建更深的网络。DenseNet
    将其提升到一个新水平，通过引入从每一层到所有后续层的连接，即一个层可以接收来自前几层的所有特征图。符号上看，它可能是这样的：
- en: '![](img/28672c69-1d67-410d-8e49-63158824882c.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28672c69-1d67-410d-8e49-63158824882c.png)'
- en: 'The following figure describes what a five-layer dense block would look like:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了一个五层密集块的结构：
- en: '![](img/f7f3e5fa-b3b5-49e3-968f-6423cba0e915.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7f3e5fa-b3b5-49e3-968f-6423cba0e915.png)'
- en: Image source: https://arxiv.org/abs/1608.06993
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://arxiv.org/abs/1608.06993
- en: There is a DenseNet implementation of torchvision ([https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)).
    Let's look at two major functionalities, `_DenseBlock` and `_DenseLayer`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: torchvision 中有 DenseNet 的实现（[https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)）。让我们看一下两个主要功能，`_DenseBlock`
    和 `_DenseLayer`。
- en: DenseBlock
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DenseBlock
- en: 'Let''s look at the code for `DenseBlock` and then walk through it:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 `DenseBlock` 的代码，然后逐步分析它：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`DenseBlock` is a sequential module where we add layers in a sequential order.
    Based on the number of layers (`num_layers`) in the block, we add that number
    of `_Denselayer` objects along with a name to it. All the magic is happening inside
    the `DenseLayer`. Let''s look at what goes on inside the `DenseLayer`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`DenseBlock` 是一个顺序模块，我们按顺序添加层。根据块中的层数（`num_layers`），我们添加相应数量的 `_DenseLayer`
    对象以及一个名称。所有的魔法都发生在 `DenseLayer` 内部。让我们看看 `DenseLayer` 内部发生了什么。'
- en: DenseLayer
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DenseLayer
- en: 'One good way to learn how a particular network works is to look at the source
    code. PyTorch has a very clean implementation and most of the time is easily readable.
    Let''s look at the `DenseLayer` implementation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 学习一个特定网络如何工作的一个好方法是查看源代码。PyTorch 实现非常清晰，大多数情况下易于阅读。让我们来看一下 `DenseLayer` 的实现：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you are new to inheritance in Python, then the preceding code may not look
    intuitive. The `_DenseLayer` is a subclass of `nn.Sequential`; let's look at what
    goes on inside each method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 Python 中的继承不熟悉，那么前面的代码可能看起来不直观。`_DenseLayer` 是 `nn.Sequential` 的子类；让我们看看每个方法内部发生了什么。
- en: In the `__init__` method, we add all the layers that the input data needs to
    be passed to. It is quite similar to all the other network architectures we have
    seen.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在`__init__`方法中，我们添加所有需要传递给输入数据的层。这与我们看到的所有其他网络架构非常相似。
- en: 'The magic happens in the `forward` method. We pass the input to the `forward`
    method of the `super` class, which is `nn.Sequential`. Let''s look at what happens
    in the `forward` method of the sequential class ([https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py)):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 魔法发生在`forward`方法中。我们将输入传递给`super`类的`forward`方法，即`nn.Sequential`的方法。让我们看看顺序类的`forward`方法中发生了什么（[https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py)）：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The input is passed through all the layers that were previously added to the
    sequential block and the output is concatenated to the input. The process is repeated
    for the required number of layers in a block.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通过之前添加到顺序块中的所有层，并将输出连接到输入。该过程在块中所需数量的层中重复进行。
- en: 'With the understanding of how a `DenseNet` block works, let''s explore how
    we can use DenseNet for calculating pre-convoluted features and building a classifier
    model on top of it. At a high level, the DenseNet implementation is similar to
    the VGG implementation. The DenseNet implementation also has a features module,
    which contains all the dense blocks, and a classifier module, which contains the
    fully connected model. We will be going through the following steps to build the
    model. We will be skipping most of the part that is similar to what we have seen
    for Inception and ResNet, such as creating the data loader and datasets. Also,
    we will discuss the following steps in detail:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了`DenseNet`块的工作原理后，让我们探索如何使用DenseNet计算预卷积特征并在其上构建分类器模型。在高层次上，DenseNet的实现类似于VGG的实现。DenseNet实现还有一个特征模块，其中包含所有的密集块，以及一个分类器模块，其中包含全连接模型。我们将按照以下步骤构建模型。我们将跳过与Inception和ResNet相似的大部分内容，例如创建数据加载器和数据集。同时，我们将详细讨论以下步骤：
- en: Creating a DenseNet model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个DenseNet模型
- en: Extracting DenseNet features
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取DenseNet特征
- en: Creating a dataset and loaders
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个数据集和加载器
- en: Creating a fully connected model and train
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个全连接模型并训练
- en: By now, most of the code will be self-explanatory.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大部分代码都将是不言自明的。
- en: Creating a DenseNet model
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个DenseNet模型
- en: 'Torchvision has a pretrained DenseNet model with different layer options (121,
    169, 201, 161). We have chosen the model with `121` layers. As discussed, the
    DenseNet has two modules: features (containing the dense blocks), and classifier
    (fully connected block). As we are using DenseNet as an image feature extractor,
    we will only use the feature module:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Torchvision有一个预训练的DenseNet模型，具有不同的层次选项（121、169、201、161）。我们选择了具有`121`层的模型。正如讨论的那样，DenseNet有两个模块：特征（包含密集块）和分类器（全连接块）。由于我们正在使用DenseNet作为图像特征提取器，我们只会使用特征模块：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let's extract the DenseNet features from the images.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从图像中提取DenseNet特征。
- en: Extracting DenseNet features
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取DenseNet特征
- en: 'It is quite similar to what we did for Inception, except we are not using `register_forward_hook`
    to extract features. The following code shows how the DenseNet features are extracted:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们为Inception所做的相似，只是我们没有使用`register_forward_hook`来提取特征。以下代码展示了如何提取DenseNet特征：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code is similar to what we have seen for Inception and ResNet.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码与我们之前看到的Inception和ResNet类似。
- en: Creating a dataset and loaders
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个数据集和加载器
- en: 'We will use the same `FeaturesDataset` class that we created for ResNet and
    use it to create data loaders for the `train` and `validation` dataset in the
    following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们为ResNet创建的`FeaturesDataset`类，并在以下代码中使用它来为`train`和`validation`数据集创建数据加载器：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Time to create the model and train it.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候创建模型并训练了。
- en: Creating a fully connected model and train
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个全连接模型并训练
- en: 'We will use a simple linear model, similar to what we used in ResNet and Inception.
    The following code shows the network architecture which we will be using to train
    the model:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的线性模型，类似于我们在ResNet和Inception中使用的模型。以下代码展示了我们将用来训练模型的网络架构：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will use the same `fit` method to train the preceding model. The following
    code snippet shows the training code, along with the results:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的`fit`方法来训练前面的模型。以下代码片段显示了训练代码及其结果：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The result of the preceding code is:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果是：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding algorithm was able to achieve a maximum training accuracy of 99%,
    and 99% validation accuracy. Your results could change as the `validation` dataset
    you create may have different images.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 前述算法能够达到99%的最大训练精度和99%的验证精度。由于您创建的`validation`数据集可能包含不同的图像，因此您的结果可能会有所不同。
- en: 'Some of the advantages of DenseNet are:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet的一些优点包括：
- en: It substantially reduces the number of parameters required
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它大大减少了所需的参数数量
- en: It alleviates the vanishing gradient problem
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它缓解了梯度消失问题
- en: It encourages feature reuse
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它鼓励特征重用
- en: In this next section, we will explore how we can build a model that combines
    the advantage of the convoluted features computed, using the different models
    of ResNet, Inception, and DenseNet.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨如何构建一个结合使用ResNet、Inception和DenseNet不同模型计算的卷积特征优势的模型。
- en: Model ensembling
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型集成
- en: There could be times when we would need to try to combine multiple models to
    build a very powerful model. There are many techniques that can be used for building
    an ensemble model. In this section, we will learn how to combine outputs using
    the features generated by three different models (ResNet, Inception, and DenseNet)
    to build a powerful model. We will be using the same dataset that we used for
    other examples in this chapter.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要尝试将多个模型组合在一起构建一个非常强大的模型。有许多技术可以用于构建集成模型。在本节中，我们将学习如何使用由三种不同模型（ResNet、Inception和DenseNet）生成的特征来结合输出，从而构建一个强大的模型。我们将使用本章中其他示例中使用的同一数据集。
- en: 'The architecture for the ensemble model would look like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的架构将如下所示：
- en: '![](img/203dd83d-e8f6-4786-9e55-a5904274c72d.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/203dd83d-e8f6-4786-9e55-a5904274c72d.png)'
- en: 'This image shows what we are going to do in the ensemble model, which can be
    summarized in the following steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了我们在集成模型中要做的事情，可以总结为以下步骤：
- en: Create three models
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个模型
- en: Extract the image features using the created models
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用创建的模型提取图像特征
- en: Create a custom dataset which returns features of all the three models along
    with the labels
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自定义数据集，该数据集返回所有三个模型的特征以及标签
- en: Create model similar to the architecture in the preceding figure
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建类似于前面图中架构的模型
- en: Train and validate the model
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Let's explore each of the steps in detail.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每个步骤。
- en: Creating models
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'Let''s create all the three required models, as shown in the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按以下代码创建所有三个所需的模型：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now we have all the models, let's extract the features from the images.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有模型，让我们从图像中提取特征。
- en: Extracting the image features
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取图像特征
- en: 'Here, we combine all the logic that we have seen individually for the algorithms
    in the chapter:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将本章中各算法的各自逻辑组合起来：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: By now, we have created image features using all the models. If you are facing
    memory issues, then you can either remove one of the models, or stop storing the
    features in the memory, which could be slow to train. If you are running this
    on a CUDA instance, then you can go for a more powerful instance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用所有模型创建了图像特征。如果你遇到内存问题，那么可以删除其中一个模型，或者停止在内存中存储特征，这可能会导致训练速度变慢。如果你在运行这个过程时使用的是CUDA实例，那么可以选择更强大的实例。
- en: Creating a custom dataset along with data loaders
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个自定义数据集以及数据加载器
- en: 'We will not be able to use the `FeaturesDataset` class as it is, since it was
    developed to pick from the output of only one model. So, the following implementation
    contains minor changes to the `FeaturesDataset` class to accommodate all the three
    different generated features:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`FeaturesDataset`类仅开发用于从一个模型的输出中进行选择，所以我们将无法直接使用它。因此，以下实现对`FeaturesDataset`类进行了微小的更改，以适应所有三个不同生成特征的情况：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have made changes to the `__init__` method to store all the features generated
    from different models, and the `__getitem__` method to retrieve the features and
    label of an image. Using the `FeatureDataset` class, we created dataset instances
    for both training and validation data. Once the dataset is created, we can use
    the same data loader for batching data, as shown in the following code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对`__init__`方法进行了更改，以存储来自不同模型生成的所有特征，并对`__getitem__`方法进行了更改，以检索图像的特征和标签。使用`FeatureDataset`类，我们为训练和验证数据创建了数据集实例。一旦数据集创建完成，我们可以使用相同的数据加载器批处理数据，如以下代码所示：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Creating an ensembling model
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集成模型
- en: 'We need to create a model that is similar to the architecture diagram show
    previously. The following code implements this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个与之前展示的架构图类似的模型。以下代码实现了这一点：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code, we create three linear layers that take features generated
    from different models. We sum up all the outputs from these three linear layers
    and pass them on to another linear layer, which maps them to the required categories.
    To prevent the model from overfitting, we have used dropouts.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了三个线性层，这些线性层接收从不同模型生成的特征。我们将这三个线性层的所有输出相加，并将它们传递给另一个线性层，将它们映射到所需的类别。为了防止模型过拟合，我们使用了dropout。
- en: Training and validating the model
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'We need to make some minor changes to the `fit` method to accommodate the three
    input-values generated from the data loader. The following code implements the
    new `fit` function:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对`fit`方法进行一些小的更改，以适应从数据加载器生成的三个输入值。以下代码实现了新的`fit`函数：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see from the previous code, most of it remains the same, except that
    the loader returns three inputs and one label. So, we make changes in the function,
    which is self-explanatory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的代码中看到的，大部分代码保持不变，只是加载器返回了三个输入和一个标签。因此，我们对功能进行了更改，这是不言自明的。
- en: 'The following code shows the training code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了训练代码：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果如下：
- en: '[PRE39]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The ensemble model achieves a 99.6% training accuracy and a validation accuracy
    of 99.3%. Though ensemble models are powerful, they are computationally expensive.
    They are good techniques to use when you are solving problems in competitions
    such as Kaggle.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型达到了99.6%的训练精度和99.3%的验证精度。虽然集成模型功能强大，但计算开销大。它们在解决如Kaggle竞赛中的问题时是很好的技术。
- en: Encoder-decoder architecture
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'Almost all the deep learning algorithms we have seen in the book are good at
    learning how to map training data to their corresponding labels. We cannot use
    them directly for tasks where the model needs to learn from a sequence and generate
    another sequence or an image. Some of the example applications are:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中看到的几乎所有深度学习算法都擅长学习如何将训练数据映射到其相应的标签。我们不能直接将它们用于需要模型从序列学习并生成另一个序列或图像的任务。一些示例应用包括：
- en: Language translation
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Image captioning
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕
- en: Image generation (seq2img)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成（seq2img）
- en: Speech recognition
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Question answering
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Most of these problems can be seen as some form of sequence-to-sequence mapping,
    and these can be solved using a family of architectures called **encoder–decoder
    architectures**. In this section, we will learn about the intuition behind these
    architectures. We will not be looking at the implementation of these networks,
    as they need to be studied in more detail.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题大多可以看作是某种形式的序列到序列映射，可以使用一系列称为**编码器-解码器架构**的体系结构来解决。在本节中，我们将了解这些架构背后的直觉。我们不会看这些网络的实现，因为它们需要更详细的学习。
- en: 'At a high level, an encoder–decoder architecture would look like the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，编码器-解码器架构看起来如下所示：
- en: '![](img/6ec104a4-761f-4cdd-aac0-21e12af8a215.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ec104a4-761f-4cdd-aac0-21e12af8a215.png)'
- en: 'An encoder is usually a **recurrent neural network** (**RNN**) (for sequential
    data) or a **Convolution Neural Network** (**CNN**) (for images) that takes in
    an image or a sequence and converts it into a fixed length vector which encodes
    all the information. The decoder is another RNN or CNN, which learns to decode
    the vector generated by the encoder and generates a new sequence of data. The
    following image shows how the encoder–decoder architecture looks for an image
    captioning system:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器通常是一个**递归神经网络**（**RNN**）（用于序列数据）或**卷积神经网络**（**CNN**）（用于图像），它接收图像或序列并将其转换为一个固定长度的向量，该向量编码了所有信息。解码器是另一个RNN或CNN，它学习解码编码器生成的向量，并生成新的数据序列。下图展示了用于图像字幕系统的编码器-解码器架构的外观：
- en: '![](img/bdb83743-f9c2-489e-af1e-c611523bbfb3.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdb83743-f9c2-489e-af1e-c611523bbfb3.png)'
- en: Encoder-decoder architecture for image captioning system
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕系统的编码器-解码器架构
- en: Image source: https://arxiv.org/pdf/1411.4555.pdf
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：https://arxiv.org/pdf/1411.4555.pdf
- en: Let's look in more detail at what happens inside an encoder and a decoder architecture
    for an image captioning system.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一看图像字幕系统中编码器和解码器架构的内部情况。
- en: Encoder
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: For an image captioning system, we will preferably use a trained architecture,
    such as ResNet or Inception, for extracting features from the image. Like we did
    for the ensemble model, we can output a fixed vector length by using a linear
    layer, and then make that linear layer trainable.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像字幕系统，我们通常会使用训练好的架构，比如ResNet或Inception，从图像中提取特征。就像我们对集成模型所做的那样，我们可以通过使用一个线性层输出固定长度的向量，然后使该线性层可训练。
- en: Decoder
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: Decoder is a **Long Short-Term Memory** (**LSTM**) layer which will generate
    a caption for an image. To build a simple model, we can just pass the encoder
    embedding as input to the LSTM only once. But it could be quite challenging for
    the decoder to learn; instead, it is common practice to provide the encoder embedding
    at every step of the decoder. Intuitively, a decoder learns to generate a sequence
    of text that best describes the caption of a given image.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是一个**长短期记忆**（**LSTM**）层，用于为图像生成字幕。为了构建一个简单的模型，我们可以只将编码器嵌入作为LSTM的输入传递一次。但是解码器要学习起来可能会很有挑战性；因此，常见做法是在解码器的每一步中提供编码器嵌入。直观地说，解码器学习生成一系列最佳描述给定图像字幕的文本序列。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored some modern architectures, such as ResNet, Inception,
    and DenseNet. We also explored how we can use these models for transfer learning
    and ensembling, and introduced the encoder–decoder architecture, which powers
    a lot of systems, such as language translation systems.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一些现代架构，如ResNet、Inception和DenseNet。我们还探讨了如何使用这些模型进行迁移学习和集成学习，并介绍了编码器-解码器架构，该架构驱动着许多系统，如语言翻译系统。
- en: In the next chapter, we will arrive at a conclusion of what we have achieved
    in our learning journey through the book, as well as discuss where can you go
    from here. We will visit a plethora of resources on PyTorch and some cool deep
    learning projects that have been created or are undergoing research using PyTorch.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将总结在书籍学习旅程中取得的成就，同时讨论你接下来可以从哪里继续前行。我们将探讨关于PyTorch的大量资源以及一些正在使用PyTorch进行研究的酷炫深度学习项目。
