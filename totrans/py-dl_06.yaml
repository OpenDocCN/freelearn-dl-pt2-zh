- en: Chapter 6. Recurrent Neural Networks and Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The neural network architectures we discussed in the previous chapters take
    in fixed sized input and provide fixed sized output. Even the convolutional networks
    used in image recognition ([Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*) are flattened into a fixed
    output vector. This chapter will lift us from this constraint by introducing **Recurrent
    Neural Networks** (**RNNs**). RNNs help us deal with sequences of variable length
    by defining a recurrence relation over these sequences, hence the name.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to process arbitrary sequences of input makes RNNs applicable for
    tasks such as language modeling (see section on *Language Modelling*) or speech
    recognition (see section on *Speech Recognition*). In fact, in theory, RNNs can
    be applied to any problem since it has been proven that they are Turing-Complete
    [1]. This means that theoretically, they can simulate any program that a regular
    computer would not be able to compute. As an example of this, Google DeepMind
    has proposed a model named "Neural Turing Machines", which can learn how to execute
    simple algorithms, such as sorting [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build and train a simple RNN, based on a toy problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem of vanishing and exploding gradients in RNN training and how to
    solve them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LSTM model for long-term memory learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling and how RNNs can be applied to this problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to applying deep learning to speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RNNs get their name because they recurrently apply the same function over a
    sequence. An RNN can be written as a recurrence relation defined by this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here *S*[t] —the state at step *t*—is computed by the function *f* from the
    state in the previous step, that is *t-1*, and an input *X*[t] at the current
    step. This recurrence relation defines how the state evolves step by step over
    the sequence via a feedback loop over previous states, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure from [3]
  prefs: []
  type: TYPE_NORMAL
- en: 'Left: Visual illustration of the RNN recurrence relation: *S* *[t]* *= S* *[t-1]*
    ** W + X* *[t]* ** U*. The final output will be *o* *[t]* *= V*S* *[t]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right: RNN states recurrently unfolded over the sequence *t-* *1, t, t+1*.
    Note that the parameters U, V, and W are shared between all the steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here *f* can be any differentiable function. For example, a basic RNN is defined
    by the following recurrence relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here *W* defines a linear transformation from state to state, and *U* is a linear
    transformation from input to state. The *tanh* function can be replaced by other
    transformations, such as logit, tanh, or ReLU. This relation is illustrated in
    the following figure where *O*[t] is the output generated by the network.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in word-level language modeling, the input *X* will be a sequence
    of words encoded in input vectors *(X* *[1]* *… X* *[t]* *…)*. The state *S* will
    be a sequence of state vectors *(S* *[1]* *… S* *[t]* *… )*. And the output *O*
    will be a sequence of probability vectors *(O* *[1]* *… O* *[t]* *… )* of the
    next words in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in an RNN, each state is dependent on all previous computations
    via this recurrence relation. An important implication of this is that RNNs have
    memory over time because the states *S* contain information based on the previous
    steps. In theory, RNNs can remember information for an arbitrarily long period
    of time, but in practice, they are limited to look back only a few steps. We will
    address this issue in more detail in section on *Vanishing* *and exploding gradients*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because RNNs are not limited to processing input of fixed size, they really
    expand the possibilities of what we can compute with neural networks, such as
    sequences of different lengths or images of varied sizes. The next figure visually
    illustrates some combinations of sequences we can make. Here''s a brief note on
    these combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-to****-one**: This is non-sequential processing, such as feedforward
    neural networks and convolutional neural networks. Note that there isn''t much
    difference between a feedforward network and applying an RNN to a single time
    step. An example of one-to-one processing is the image classification from chapter
    (See [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-to-many**: This generates a sequence based on a single input, for example,
    caption generation from an image [4].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-one**: This outputs a single result based on a sequence, for example,
    sentiment classification from text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to****-many indirect**: A sequence is encoded into a state vector, after
    which this state vector is decoded into a new sequence, for example, language
    translation [5], [6].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-many direct**: This outputs a result for each input step, for example,
    frame phoneme labeling in speech recognition (see the *Speech recognition* section).![Recurrent
    neural networks](img/00223.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image from [7]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RNNs expand the possibilities of what we can compute with neural networks—Red:
    input X, Green: states S, Blue: outputs O.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RNN — how to implement and train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we briefly discussed what RNNs are and what problems
    they can solve. Let''s dive into the details of an RNN and how to train it with
    the help of a very simple toy example: counting ones in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this problem, we will teach the most basic RNN how to count the number of
    ones in the input and output the result at the end of the sequence. We will show
    an implementation of this network in Python and NumPy. An example of input and
    output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The network we are going to train is a very basic one and is illustrated in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN — how to implement and train](img/00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Basic RNN for counting ones in the input
  prefs: []
  type: TYPE_NORMAL
- en: 'The network will have only two parameters: an input weight *U* and a recurrence
    weight *W*. The output weight *V* is set to 1 so we just read out the last state
    as the output *y*. The recurrence relation defined by this network is *S* *[t]*
    *= S* *[t-1]* ** W + X* *[t]* ** U*. Note that this is a linear model since we
    don''t apply a nonlinear function in this formula. This function can be defined
    in terms of code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Because 3 is the number we want to output and there are three ones, a good
    solution to this is to just get the sum of the input across the sequence. If we
    set *U=1*, then whenever an input is received, we will get its full value. If
    we set *W=1*, then the value we would accumulate will never decay. So for this
    example, we would get the desired output: 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the training and implementation of this neural network will be
    interesting, as we will see in the rest of this section. So let's see how we could
    get this result through backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The backpropagation through time algorithm is the typical algorithm we use to
    train recurrent networks [8]. The name already implies that it is based on the
    backpropagation algorithm we discussed in [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: If you understand regular back-propagation, then backpropagation through time
    is not too difficult to understand. The main difference is that the recurrent
    network needs to be unfolded through time for a certain number of time steps.
    This unfolding is illustrated in the preceding figure (*Basic RNN for counting
    ones in the* *input*). Once the unfolding is complete, we end up with a model
    that is quite similar to a regular multilayer feedforward network. The only differences
    are that each layer has multiple input (the previous state, which is *S* *[t-1]*),
    and the current input (*X* *[t]*) and the parameters (here *U* and *W*) are shared
    between each layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass unwraps the RNN along the sequence and builds up a stack of
    activities for each step. The forward step with a batch of input sequences *X*
    can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After this forward step, we have the resulting activations, represented by
    *S*, for each step and each sample in the batch. Because we want to output more
    or less continuous output (sum of all ones), we use the mean squared error cost
    function to define our output cost with respect to the targets and output `y`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our forward step and cost function, we can define how the gradient
    is propagated backward. First, we need to get the gradient of the output `y` with
    respect to the cost function (*??/?y*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this gradient, we can propagate it backward through the stack
    of activities we built during the forward step. This backward pass pops activities
    off the stack to accumulate the error derivatives at each time step. The recurrence
    relation to propagate this gradient through the network can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation through time](img/00225.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of the parameters are accumulated with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation through time](img/00226.jpeg)![Backpropagation through time](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following implementation, the gradients for `U` and `W` are accumulated
    during `gU` and `gW`, respectively, during the backward step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now try to use gradient descent to optimize our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There is an issue though. Notice that if you try to run this code, the final
    parameters *U* and *W* tend to end up as **Not a Number** (**NaN**). Let's try
    to investigate what happened by plotting the parameter updates over an error surface,
    as shown in the following figure. Notice that the parameters slowly move toward
    the optimum (*U=W=1*) until it overshoots and hits approximately (*U=W=1.5*).
    At this point, the gradient values just blow up and make the parameter values
    jump outside the plot. This problem is known as exploding gradients. The next
    section will explain why this happens in detail and how to prevent it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Backpropagation through time](img/00228.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Parameter updates plotted on an error surface via a gradient descent. The error
    surface is plotted on a logarithmic color scale
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs can be harder to train than feedforward or convolutional networks. Some
    difficulties arise due to the recurrent nature of the RNN where the same weight
    matrix is used to compute all the state updates [9], [10].
  prefs: []
  type: TYPE_NORMAL
- en: The end of the last section, the preceding figure, illustrated the exploding
    gradient, which brings RNN training to an unstable state due to the blowing up
    of long-term components. Besides the exploding gradient problem, there is also
    the vanishing gradient problem where the opposite happens. Long-term components
    go to zero exponentially fast, and the model is unable to learn from temporally
    distant events. In this section, we will explain both the problems in detail and
    also how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both exploding and vanishing gradients arise from the fact that the recurrence
    relation that propagates the gradient backward through time forms a geometric
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing and exploding gradients](img/00229.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In our simple linear RNN, the gradient grows exponentially if *|W| > 1*.This
    is known as the exploding gradient (for example, 50 time steps over W=1.5 is *W**[50]*
    *= 1.5**^(50)* *˜≈6 * 10**⁸*). The gradient shrinks exponentially if *|W**| <
    1*; this is known as the vanishing gradient (for example, 20 time steps over *W=0.6*
    is *W**[20]* *= 0.6**^(20)* *˜≈3*10**^(-5)*). If the weight parameter *W* is a
    matrix instead of a scalar, this exploding or vanishing gradient is related to
    the largest eigenvalue (ρ) of *W* (also known as a spectral radius). It is sufficient
    for ρ < *1* for the gradients to vanish, and it is necessary for ρ > *1* for them
    to explode.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure visually illustrates the concept of exploding gradients.
    What happens is that the cost surface we are training on is highly unstable. Using
    small steps, we might move to a stable part of the cost function, where the gradient
    is low, and suddenly hit upon a jump in cost and a corresponding huge gradient.
    Because this gradient is so huge, it will have a big effect on our parameters.
    They will end up in a place on the cost surface that is far from where they originally
    were. This makes gradient descent learning unstable and even impossible in some
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing and exploding gradients](img/00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of an exploding gradient [11]
  prefs: []
  type: TYPE_NORMAL
- en: 'We can counter the effects of exploding gradients by controlling the size our
    gradients can grow to. Some examples of solutions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient clipping, where we threshold the maximum value a gradient can get [11].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second order optimization (Newton's method), where we model the curvature of
    the cost function. Modeling the curvature allows us to take big steps in low-curvature
    scenarios and small steps in high-curvature scenarios. For computational reasons,
    typically only an approximation of the second order gradient is used [12].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization methods, such as momentum [13] or RmsProp that rely less on the
    local gradient [14].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, we can retrain our network that wasn't able to converge (refer
    the preceding figure of *Illustration of an exploding gradient*) with the help
    of Rprop [15]. Rprop is a momentum-like method that only uses the sign of the
    gradient to update the momentum parameters, and it is thus not affected by exploding
    gradients. If we run Rprop optimization, we can see that the training converges
    in the following figure. Notice that while the training starts in a high gradient
    region (*U=-1.5, W=2*), it converges fast until it finds the optimum at (*U* *=W=1*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing and exploding gradients](img/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Parameter updates via Rprop plotted on an error surface. Error surface is plotted
    on a logarithmic scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vanishing gradient problem is the inverse of the exploding gradient problem.
    The gradient decays exponentially over the number of steps. This means that the
    gradients in earlier states become extremely small, and the ability to retain
    the history of these states vanishes. The small gradients from earlier time steps
    are outcompeted by the larger gradients from more recent time steps. Hochreiter
    and Schmidhuber [16] describe this as follows: *Backpropagation through time is
    too sensitive to recent* *distractions*.'
  prefs: []
  type: TYPE_NORMAL
- en: This problem is more difficult to detect because the network will still learn
    and output something (unlike in the exploding gradients case). It just won't be
    able to learn long-term dependencies. People have tried to tackle this problem
    with solutions similar to the ones we have for exploding gradients, such as second
    order optimization or momentum. These solutions are far from perfect, and learning
    long-term dependencies with simple RNNs is still very difficult. Thankfully, there
    is a clever solution to the vanishing gradient problem that uses a special architecture
    made up of memory cells. We will discuss this architecture in detail in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Long short term memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In theory, simple RNNs are capable of learning long-term dependencies, but in
    practice, due to the vanishing gradient problem, they only seem to limit themselves
    to learning short-term dependencies. Hochreiter and Schmidhuber studied this problem
    extensively and came up with a solution called **Long Short** **Term Memory**
    (**LSTM**) [16]. LSTMs can handle long-term dependencies due to a specially crafted
    memory cell. They work so well that most of the current accomplishments in training
    RNNs on a variety of problems are due to the use of LSTMs. In this section, we
    will explore how this memory cell works and how it solves the vanishing gradient
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of LSTM is the cell state, of which the information can only be
    explicitly written in or removed so that the cell state stays constant if there
    is no outside interference. This cell state for time t is illustrated as *c* *[t]*
    in the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM cell state can only be altered by specific gates, which are a way
    to let information pass through. These gates are composed of a logistic sigmoid
    function and element-wise multiplication. Because the logistic function only outputs
    values between 0 and 1, the multiplication can only reduce the value running through
    the gate. A typical LSTM is composed of three gates: a forget gate, input gate,
    and output gate. These are all illustrated as *f*, *i*, and *o*, in the following
    figure. Note that the cell state, input, and output are all vectors, so the LSTM
    can hold a combination of different information blocks at each time step. Next,
    we will describe the workings of each gate in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00232.jpeg)![Long short term memory](img/00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: LSTM cell
  prefs: []
  type: TYPE_NORMAL
- en: '*x**[t]**, c**[t]**, h**[t]* are the input, cell state, and LSTM output at
    time *t*, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The first gate in LSTM is the forget gate; it is called so because it decides
    whether we want to erase the cell state or not. This gate was not in the original
    LSTM proposed by Hochreiter; it was proposed by Gers and others [17]. The forget
    gate bases its decision on the previous output *h* *[t-1]* and current input *x**[t]*
    *.* It combines this information and squashes them by a logistic function so that
    it outputs a number between 0 and 1 for each block of the cell's vector. Because
    of the element-wise multiplication with the cell, an output of 0 erases a specific
    cell block completely and an output of 1 leaves all of the information in that
    cell blocked. This means that the LSTM can get rid of irrelevant information in
    its cell state vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00234.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next gate decides what new information is going to be added to the memory
    cell. This is done in two parts. The first part decides whether information is
    going to be added. As in the input gate, its bases it decision on *h**[t-1]* and
    *x**[t]* and outputs 0 or 1 through the logistic function available for each cell
    block of the cell''s vector. An output of 0 means that no information is added
    to that cell block''s memory. As a result, the LSTM can store specific pieces
    of information in its cell state vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00235.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The input to be added, *a* *[t]*, is derived from the previous output (*h*
    *[t-1]*) and the current input (*x**[t]*) and is transformed via a *tanh* function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The forget and input gates completely decide the new cell by adding the old
    cell state with the new information to add:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00237.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last gate decides what the output is going to be. The output gate takes
    *h* *[t-1]* and *x**[t]* as input and outputs 0 or 1 through the logistic function
    available for each cell block''s memory. An output of 0 means that the cell block
    doesn''t output any information, while an output of 1 means that the full cell
    block''s memory is transferred to the output of the cell. The LSTM can thus output
    specific blocks of information from its cell state vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00238.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final value outputted is the cell''s memory transferred by a *tanh* function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00239.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because all these formulas are derivable, we can chain LSTM cells together just
    like we chain simple RNN states together and train the network via backpropagation
    through time.
  prefs: []
  type: TYPE_NORMAL
- en: Now how does the LSTM protect us from vanishing gradients? Notice that the cell
    state is copied identically from step to step if the forget gate is 1 and the
    input gate is 0\. Only the forget gate can completely erase the cell's memory.
    As a result, memory can remain unchanged over a long period of time. Also, notice
    that the input is a tanh activation added to the current cell's memory; this means
    that the cell memory doesn't blow up and is quite stable.
  prefs: []
  type: TYPE_NORMAL
- en: How the LSTM is unrolled in practice is illustrated in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the value of 4.2 is given to the network as input; the input gate
    is set to 1 so the complete value is stored. Then for the next two time steps,
    the forget gate is set to 1\. So the entire information is kept throughout these
    steps and no new information is being added because the input gates are set to
    0\. Finally, the output gate is set to 1, and 4.2 is outputted and remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory](img/00240.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Unrolling LSTM through time [18]
  prefs: []
  type: TYPE_NORMAL
- en: While the LSTM network described in the preceding diagram is a typical LSTM
    version used in most applications, there are many variants of LSTM networks that
    combine different gates in different orders [19]. Getting into all these different
    architectures is out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of language models is to compute a probability of a sequence of words.
    They are crucial to a lot of different applications, such as speech recognition,
    optical character recognition, machine translation, and spelling correction. For
    example, in American English, the two phrases *wreck a nice beach* and *recognize
    speech* are almost identical in pronunciation, but their respective meanings are
    completely different from each other. A good language model can distinguish which
    phrase is most likely correct, based on the context of the conversation. This
    section will provide an overview of word- and character-level language models
    and how RNNs can be used to build them.
  prefs: []
  type: TYPE_NORMAL
- en: Word-based models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A word-based language model defines a probability distribution over sequences
    of words. Given a sequence of words of length *m*, it assigns a probability *P(w*
    *[1]* *, ... , w* *[m]* *)* to the full sequence of words. The application of
    these probabilities are two-fold. We can use them to estimate the likelihood of
    different phrases in natural language processing applications. Or, we can use
    them generatively to generate new text.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The inference of the probability of a long sequence, say *w* *[1]* *, ...,
    w* *[m]*, is typically infeasible. Calculating the joint probability of *P(w*
    *[1]* *, ... , w* *[m]* *)* would be done by applying the following chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![N-grams](img/00241.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Especially the probability of the later words given the earlier words would
    be difficult to estimate from the data. This is why this joint probability is
    typically approximated by an independence assumption that the *i*^(th) word is
    only dependent on the *n-1* previous words. We only model the joint probabilities
    of *n* sequential words called n-grams. Note that n-grams can be used to refer
    to other sequences of length *n*, such as *n* characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inference of the joint distribution is approximated via n-gram models that
    split up the joint distribution in multiple independent parts. Note that n-grams
    are combinations of multiple sequential words, where *n* is the number of sequential
    words. For example, in the phrase *the quick brown fox*, we have the following
    n-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1-gram**: "The," "quick," "brown," and "fox" (also known as unigram)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-grams**: "The quick," "quick brown," and "brown fox" (also known as bigram)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3-grams**: "The quick brown" and "quick brown fox" (also known as trigram)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4-grams**: "The quick brown fox"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now if we have a huge corpus of text, we can find all the n-grams up until
    a certain *n* (typically 2 to 4) and count the occurrence of each n-gram in that
    corpus. From these counts, we can estimate the probabilities of the last word
    of each n-gram, given the previous *n-1* words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1-gram**:![N-grams](img/00242.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-gram**: ![N-grams](img/00243.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n-gram**: ![N-grams](img/00244.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independence assumption that the *i*^(th) word is only dependent on the
    previous *n**-1* words can now be used to approximate the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for a unigram, we can approximate the joint distribution by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![N-grams](img/00245.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a trigram, we can approximate the joint distribution by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![N-grams](img/00246.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that based on the vocabulary size, the number of n-grams grows exponentially
    with *n*. For example, if a small vocabulary contains 100 words, then the number
    of possible 5-grams would be *100**⁵* *= 10,000,000,000* different 5-grams. In
    comparison, the entire works of Shakespeare contain around *30,000* different
    words, illustrating the infeasibility of using n-grams with a large *n*. Not only
    is there the issue of storing all the probabilities, we would also need a very
    large text corpus to create decent n-gram probability estimations for larger values
    of *n*. This problem is what is known as the curse of dimensionality. When the
    number of possible input variables (words) increases, the number of different
    combinations of these input values increases exponentially. This curse of dimensionality
    arises when the learning algorithm needs at least one example per relevant combination
    of values, which is the case in n-gram modeling. The larger our *n*, the better
    we can approximate the original distribution and the more data we would need to
    make good estimations of the n-gram probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Neural language models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we illustrated the curse of dimensionality when modeling
    text with n-grams. The number of n-grams we need to count grows exponentially
    with *n* and with the number of words in the vocabulary. One way to overcome this
    curse is by learning a lower dimensional, distributed representation of the words
    [20]. This distributed representation is created by learning an embedding function
    that transforms the space of words into a lower dimensional space of word embeddings,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural language models](img/00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: V-words from the vocabulary are transformed into one-hot encoding vectors of
    size V (each word is encoded uniquely). The embedding function then transforms
    this V-dimensional space into a distributed representation of size D (here D=4).
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the learned embedding function learns semantic information
    about the words. It associates each word in the vocabulary with a continuous-valued
    vector representation, the word embedding. Each word corresponds to a point in
    this embedding space where different dimensions correspond to the grammatical
    or semantic properties of these words. The goal is to ensure that the words close
    to each other in this embedding space should have similar meanings. This way,
    the information that some words are semantically similar can be exploited by the
    language model. For example, it might learn that "fox" and "cat" are semantically
    related and that both "the quick brown fox" and "the quick brown cat" are valid
    phrases. A sequence of words can then be transformed into a sequence of embedding
    vectors that capture the characteristics of these words.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to model the language model via a neural network and learn this
    embedding function implicitly. We can learn a neural network that given a sequence
    of *n-1* words (*w**[t-n+1]* *, …, w**[t-1]*) tries to output the probability
    distribution of the next word, that is, *w**[t]*. The network is made up of different
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding layer takes the one-hot representation of the word *w* *[i]*
    and converts it into its embedding by multiplying it with the embedding matrix
    *C*. This computation can be efficiently implemented by a table lookup. The embedding
    matrix *C* is shared over all the words, so all words use the same embedding function.
    *C* is represented by a *V * D* matrix, where *V* is the size of the vocabulary
    and *D* the size of the embedding. The resulting embeddings are concatenated into
    a hidden layer; after this, a bias *b* and a nonlinear function, such as *tanh*,
    can be applied. The output of the hidden layer is thus represented by the function
    *z = tanh(concat(w* *[t-n+1]* *, …, w* *[t-1]* *) + b)*. From the hidden layer,
    we can now output the probability distribution of the next word *w* *[t]* by multiplying
    the hidden layer with *U*. This maps the hidden layer to the word space, adding
    a bias *b* and applying the softmax function to get a probability distribution.
    The final layer computes *softmax(z*U +b)*. This network is illustrated in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural language models](img/00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A neural network language model that outputs the probability distribution of
    the word w[t], given the words *w**[t-1]* *... w**[t-n+1]*. *C* is the embedding
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: This model simultaneously learns an embedding of all the words in the vocabulary
    and a model of the probability function for sequences of words. It is able to
    generalize this probability function to sequences of words not seen during training,
    thanks to these distributed representations. A specific combination of words in
    the test set might not be seen in the training set, but a sequence with similar
    embedding features is much more likely to be seen during training.
  prefs: []
  type: TYPE_NORMAL
- en: A 2D projection of some word embeddings is illustrated in the following figure.
    It can be seen that words that are semantically close are also close to each other
    in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural language models](img/00249.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Related words in a 2D embedding space are close to each other in this space
    [21]
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings can be trained unsupervised on a large corpus of text data.
    This way, they are able to capture general semantic information between words.
    The resulting embeddings can now be used to improve the performance of other tasks
    where there might not be a lot of labeled data available. For example, a classifier
    trying to classify the sentiment of an article might be trained on using previously
    learned word embeddings, instead of one-hot encoding vectors. This way, the semantic
    information of the words becomes readily available for the sentiment classifier.
    Because of this, a lot of research has gone into creating better word embeddings
    without focusing on learning a probability function over sequences of words. For
    example, a popular word embedding model is word2vec [22], [23].
  prefs: []
  type: TYPE_NORMAL
- en: A surprising result is that these word embeddings can capture analogies between
    words as differences. It might, for example, capture that the difference between
    the embedding of "woman" and "man" encodes the gender and that this difference
    is the same in other gender-related words such as "queen" and "king."
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural language models](img/00250.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Word embeddings can capture semantic differences between words [24]
  prefs: []
  type: TYPE_NORMAL
- en: embed(woman) - embed(man) ? embed(aunt) - embed(uncle)
  prefs: []
  type: TYPE_NORMAL
- en: embed(woman) - embed(man) ? embed(queen) - embed(king)
  prefs: []
  type: TYPE_NORMAL
- en: While the previous feedforward network language model can overcome the curse
    of dimensionality of modeling a large vocabulary input, it is still limited to
    only modeling fixed length word sequences. To overcome this issue, we can use
    RNNs to build an RNN language model that is not limited by fixed length word sequences
    [25]. These RNN-based models can then not only cluster similar words in the input
    embedding, but can also cluster similar histories in the recurrent state vector.
  prefs: []
  type: TYPE_NORMAL
- en: One issue with these word-based models is computing the output probabilities,
    *P(w* *[i]* *| context)*, of each word in the vocabulary. We get these output
    probabilities by using a softmax over all word activations. For a small vocabulary
    *V* of *50,000* words, this would need a *|S| * |V|* output matrix, where *|V|*
    is the size of the vocabulary and *|S|* the size of the state vector. This matrix
    is huge and would grow even more when we increase our vocabulary. And because
    softmax normalizes the activation of a single word by a combination of all other
    activations, we need to compute each activation to get the probability of a single
    word. Both illustrate the difficulty of computing the softmax over a large vocabulary;
    a lot of parameters are needed to model the linear transformation before the softmax,
    and the softmax itself is computationally intensive.
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to overcome this issue, for example, by modeling the softmax
    function as a binary tree, essentially only needing *log(|V|)* computations the
    calculate to final output probability of a single word [26].
  prefs: []
  type: TYPE_NORMAL
- en: Instead of going into these workarounds in detail, let's check out another variant
    of language modeling that is not affected by these large vocabulary issues.
  prefs: []
  type: TYPE_NORMAL
- en: Character-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, language modeling is performed at the word level, where the distribution
    is over a fixed vocabulary of *|* *V|* words. Vocabularies in realistic tasks,
    such as the language models used in speech recognition, often exceed *100,000*
    words. This huge dimensionality makes modeling the output distribution very challenging.
    Furthermore, these word level models are quite limited when it comes to modeling
    text data that contains non-word strings, such as multidigit numbers or words
    that were never part of the training data (out-of-vocabulary words).
  prefs: []
  type: TYPE_NORMAL
- en: A class of models that can overcome these issues is called a character-level
    language model [27]. These models model the distribution over sequences of characters
    instead of words, thus allowing you to compute probabilities over a much smaller
    vocabulary. The vocabulary here comprises all the possible characters in our text
    corpus. There is a downside to these models, though. By modeling the sequence
    of characters instead of words, we need to model much longer sequences to capture
    the same information over time. To capture these long-term dependencies, let's
    use an LSTM RNN language model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following part of this section will go into detail on how to implement
    a character-level LSTM in Tensorflow and how to train it on Leo Tolstoy''s *War
    and Peace*. This LSTM will model the probability of the next character, given
    the previously seen characters: *P(c* *[t]* *| c* *[t-1]* *... c* *[t-n]* *)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Because the full text is too long to train a network with **back-propagation**
    **through time** (**BPTT**), we will use a batched variant called truncated BPTT.
    In this method, we will divide the training data into batches of fixed sequence
    length and train the network batch by batch. Because the batches will follow up
    with each other, we can use the final state of the last batch as the initial state
    in the next batch. This way, we can exploit the information stored in the state
    without having to do a full backpropagation through the full input text. Next,
    we will describe how to read these batches and feed them into the network.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and reading data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train a good language model, we need a lot of data. For our example, we will
    learn about a model based on the English translation of Leo Tolstoy's "War and
    peace." This book contains more than *500,000* words, making it the perfect candidate
    for our small example. Since it's in the public domain, "War and peace" can be
    downloaded as plain text for free from Project Gutenberg. As part of preprocessing,
    we will remove the Gutenberg license, book information, and table of contents.
    Next, we will strip out newlines in the middle of sentences and reduce the maximum
    number of consecutive newlines allowed to two.
  prefs: []
  type: TYPE_NORMAL
- en: 'To feed the data into the network, we will have to convert it into a numerical
    format. Each character will be associated with an integer. In our example, we
    will extract a total of 98 different characters from the text corpus. Next, we
    will extract input and targets. For each input character, we will predict the
    next character. Because we are training with truncated BPTT, we will make all
    the batches follow up on each other to exploit the continuity of the sequence.
    The process of converting the text into a list of indices and splitting it up
    in to batches of input and targets is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing and reading data](img/00251.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Converting text into input and target batches of integer labels with length
    5\. Note that batches follow on to each other.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network we will train will be a two-layer LSTM network with 512 cells in
    each layer. We will train this network with truncated BPTT, so we will need to
    store the state between batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define placeholders for our input and targets. The first
    dimension of both the input and targets is the batch size, the number of examples
    processed in parallel. The second dimension will be the dimension along the text
    sequence. Both these placeholders take batches of sequences where the characters
    are represented by their index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To feed the characters to the network, we need to transform them into a vector.
    We will transform them into one-hot encoding, which means that each character
    is going to be transformed into a vector with length equal to the size of the
    number of different characters in the dataset. This vector will be all zeros,
    except the cell that corresponds to its index, which will be set to 1\. This can
    be done easily in TensorFlow with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define our multilayer LSTM architecture. First we need to define
    the LSTM cells for each layer (`lstm_sizes` is a list of sizes for each layer,
    for example (512, 512), in our case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we wrap these cells in a single multilayer RNN cell using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To store the state between the batches, we need to get the initial state of
    the network and wrap it in the variable to be stored. Note that for computational
    reasons, TensorFlow stores LSTM states in a tuple of two separate tensors (`c`
    and `h` from the *Long Short Term* *Memory* section ). We can flatten this nested
    data structure with the `flatten` method, wrap each tensor in a variable, and
    repack it as the original structure with the `pack_sequence``_as` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the initial state defined as a variable, we can start unrolling
    the network through time. TensorFlow provides the `dynamic_rnn` method that does
    this unrolling dynamically as per the sequence length of the input. This method
    will return a tuple consisting of a tensor representing the LSTM output and the
    final state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to store the final state as the initial state for the next batch.
    We use the variable `assign` method to store each final state in the right initial
    state variable. The `control_dependencies` method is used to force that the state
    update to run before we return the LSTM output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the logit output from the final LSTM output, we need to apply a linear
    transformation to the output so it can have *batch* *size * sequence length *
    number of symbols* as its dimensions. Before we apply this linear transformation,
    we need to flatten the output to a matrix of the size number of *outputs ** *number
    of output features*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then define and apply the linear transformation with a weight matrix
    *W* and bias *b* to get the logits, apply the softmax function, and reshape it
    to a tensor of the size *batch size ** *sequence length * number of characters*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![LSTM network](img/00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: LSTM character language model unfolded
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have defined the input, targets, and architecture of our network,
    let's define how to train it. The first step in training is defining a loss function
    that we want to minimize. This loss function describes the cost of outputting
    a wrong sequence of characters, given the input and targets. Because we are predicting
    the next character considering the previous characters, it is a classification
    problem and we will use cross-entropy loss. We do this by using the `sparse_softmax_cross_`
    `entropy_with_logits` TensorFlow function. This function takes the logit output
    of the network as input (before the softmax) and targets as class labels and computes
    the cross-entropy loss of each output with respect to its target. To reduce the
    loss over the full sequence and all the batches, we take the mean value of all
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we flatten the targets to a one-dimensional vector first to make
    them compatible with the flattened logit output from our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have this loss function defined, it is possible to define the training
    operation in TensorFlow that will optimize our network of input and target batches.
    To execute the optimization, we will use the Adam optimizer; this helps stabilize
    gradient updates. The Adam optimizer is just a specific way of performing gradient
    descent in a more controlled way [28]. We will also clip the gradients to prevent
    exploding gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Having defined all the TensorFlow operations required for training, we can
    now start with the optimization in mini batches. If `data_feeder` is a generator
    that returns consecutive batches of input and targets, then we can train these
    batches by iteratively feeding in the input and target batches. We reset the initial
    state every 100 mini batches so that the network learns how to deal with the initial
    states in the beginning of sequences. You can save the model with a TensorFlow
    saver to reload it for sampling later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once our model is trained, we might want to sample the sequences from this
    model to generate text. We can initialize our sampling architecture with the same
    code we used for training the model, but we''d need to set `batch_size` to `1`
    and `sequence_length` to `None`. This way, we can generate a single string and
    sample sequences of different lengths. We can then initialize the parameters of
    the model with the parameters saved after training. To start with the sampling,
    we feed in an initial string (`prime_string`) to prime the state of the network.
    After this string is fed in, we can sample the next character based on the output
    distribution of the softmax function. We can then feed in this sampled character
    and get the output distribution for the next one. This process can be continued
    for a number of steps until a string of a specified size is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Example training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have our code for training and sampling, we can train the network
    on Leo Tolstoy's *War and Peace* and sample what the network has learned every
    couple of batch iterations. Let's prime the network with the phrase "*She was
    born in the year*" and see how it completes it during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'After 500 batches, we get this result: *She was born in the year sive but*
    *us eret tuke Toffhin e feale shoud pille saky doctonas* *laft the comssing hinder
    to gam the droved at ay* *vime*. The network has already picked up some distribution
    of characters and has come up with things that look like words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After 5,000 batches, the network picks up a lot of different words and names:
    "*She was born in the* *year he had meaningly many of Seffer Zsites. Now in* *his
    crownchy-destruction, eccention, was formed a wolf of Veakov* *one also because
    he was congrary, that he suddenly had* *first did not reply."* It still invents
    plausible looking words likes "congrary" and "eccention".'
  prefs: []
  type: TYPE_NORMAL
- en: 'After 50,000 batches, the network outputs the following text: *She was born
    in the year* *1813\. At last the sky may behave the Moscow house* *there was a
    splendid chance that had to be passed* *the Rostóvs'', all the times: sat retiring,
    showed them to* *confure the sovereigns."* The network seems to have figured out
    that a year number is a very plausible words to follow up our prime string. Short
    strings of words seem to make sense, but the sentences on their own don''t make
    sense yet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After 500,000 batches, we stop the training and the network outputs this: "*She
    was born in* *the year 1806, when he entered his thought on the* *words of his
    name. The commune would not sacrifice him* *: "What is this?" asked Natásha. "Do
    you remember?""*. We can see that the network is now trying to make sentences,
    but the sentences are not coherent with each other. What is remarkable is that
    it models small conversations in full sentences at the end, including quotes and
    punctuation.'
  prefs: []
  type: TYPE_NORMAL
- en: While not perfect, it is remarkable how the RNN language model is able to generate
    coherent phrases of text. We would like to encourage you at this point to experiment
    with different architectures, increase the size of the LSTM layers, put a third
    LSTM layer in the network, download more text data from the Internet, and see
    how much you can improve the current model.
  prefs: []
  type: TYPE_NORMAL
- en: The language models we have discussed so far are used in many different applications,
    ranging from speech recognition to creating intelligent chat bots that are able
    to build a conversation with a user. In the next section, we will briefly discuss
    deep learning speech recognition models in which language models play an important
    part.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we saw how RNNs can be used to learn patterns of many
    different time sequences. In this section, we will look at how these models can
    be used for the problem of recognizing and understanding speech. We will give
    a brief overview of the speech recognition pipeline and provide a high-level view
    of how we can use neural networks in each part of the pipeline. In order to know
    more about the methods discussed in this section, we would like you to refer to
    the references.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Speech recognition tries to find a transcription of the most probable word
    sequence considering the acoustic observations provided; this is represented by
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*transcription = argmax(* *P(words | audio features))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This probability function is typically modeled in different parts (note that
    the normalizing term P (audio features) is usually ignored):'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (words | audio features) = P (audio* *features | words) * P (words)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= P (audio features | phonemes) * P (phonemes* *| words) * P (words)*'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**What are phonemes?**'
  prefs: []
  type: TYPE_NORMAL
- en: Phonemes are a basic unit of sound that define the pronunciation of words. For
    example, the word "bat" is composed of three phonemes `/b/`, `/ae/`, and `/t/`.
    Each phoneme is tied to a specific sound. Spoken English consists of around 44
    phonemes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these probability functions will be modeled by different parts of the
    recognition system. A typical speech recognition pipeline takes in an audio signal
    and performs preprocessing and feature extraction. The features are then used
    in an acoustic model that tries to learn how to distinguish between different
    sounds and phonemes: *P (audio features | phonemes)*. These phonemes are then
    matched to characters or words with the help of pronunciation dictionaries: *P(phonemes
    | words)*. The probabilities of the words extracted from the audio signal are
    then combined with the probabilities of a language model, *P(words)*. The most
    likely sequence is then found via a decoding search step that searches for the
    most likely sequence(see *Decoding* section). A high-level overview of this speech
    recognition pipeline is described in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Speech recognition pipeline](img/00253.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overview of a typical speech recognition pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Large, real-world vocabulary speech recognition pipelines are based on this
    same pipeline; however, they use a lot of tricks and heuristics in each step to
    make the problem tractable. While these details are out of the scope of this section,
    there is open source software available—Kaldi [29]—that allows you to train a
    speech recognition system with advanced pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will briefly describe each of the steps in this standard
    pipeline and how deep learning can help improve these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Speech as input data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Speech is a type of sound that typically conveys information. It is a vibration
    that propagates through a medium, such as air. If these vibrations are between
    20 Hz and 20 kHz, they are audible to humans. These vibrations can be captured
    and converted into a digital signal so that they can be used in audio signal processing
    on computers. They are typically captured by a microphone after which the continuous
    signal is sampled at discrete samples. A typical sample rate is 44.1 kHz, which
    means that the amplitude of the incoming audio signal is measured 44,100 times
    per second. Note that this is around twice the maximum human hearing frequency.
    A sampled recording of someone saying "hello world" is plotted in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Speech as input data](img/00254.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Speech signal of someone saying "hello world" in the time domain
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recording of the audio signal in the preceding figure is recorded over 1.2
    seconds. To digitize the audio, it is sampled 44,100 times per second (44.1 kHz).
    This means that roughly 50,000 amplitude samples were taken for this 1.2-second
    audio signal.
  prefs: []
  type: TYPE_NORMAL
- en: For only a small example, these are a lot of points over the time dimension.
    To reduce the size of the input data, these audio signals are typically preprocessed
    to reduce the number of time steps before feeding them into speech recognition
    algorithms. A typical transformation transforms a signal to a spectrogram, which
    is a representation of how the frequencies in the signal change over time, see
    the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: This spectral transformation is done by dividing the time signal in overlapping
    windows and taking the Fourier transform of each of these windows. The Fourier
    transform decomposes a signal over time into frequencies that make up the signal
    [30]. The resulting frequencies responses are compressed into fixed frequency
    bins. This array of frequency bins is also known as a filter banks. A filter bank
    is a collection of filters that separate out the signal in multiple frequency
    bands.
  prefs: []
  type: TYPE_NORMAL
- en: Say the previous "hello world" recording is divided into overlapping windows
    of 25 ms with a stride of 10 ms. The resulting windows are then transformed into
    a frequency space with the help of a windowed Fourier transform. This means that
    the amplitude information for each time step is transformed into amplitude information
    for each frequency. The final frequencies are mapped to 40 frequency bins according
    to a logarithmic scale, also known as the Mel scale. The resulting filter bank
    spectrogram is shown in the following figure . This transformation resulted in
    reducing the time dimension from 50,000 to 118 samples, where each sample is a
    vector of size 40.
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing](img/00255.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mel spectrum of speech signal from the previous figure
  prefs: []
  type: TYPE_NORMAL
- en: Especially in older speech recognition systems, these Mel-scale filter banks
    are even more processed by decorrelation to remove linear dependencies. Typically,
    this is done by taking a **discrete** **cosine transform** (**DCT**) of the logarithm
    of the filter banks. This DCT is a variant of the Fourier transform. This signal
    transformation is also known as **Mel Frequency Cepstral** **Coefficients** (**MFCC**).
  prefs: []
  type: TYPE_NORMAL
- en: More recently, deep learning methods, such as convolutional neural networks,
    have learned some of these preprocessing steps [31], [32].
  prefs: []
  type: TYPE_NORMAL
- en: Acoustic model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In speech recognition, we want to output the words being spoken as text. This
    can be done by learning a time-dependent model that takes in a sequence of audio
    features, as described in the previous section, and outputs a sequential distribution
    of possible words being spoken. This model is called the acoustic model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acoustic model tries to model the likelihood that a sequence of audio features
    was generated by a sequence of words or phonemes: *P (audio* *features | words)
    = P (audio features | phonemes) * P (phonemes | words)*.'
  prefs: []
  type: TYPE_NORMAL
- en: A typical speech recognition acoustic model, before deep learning became popular,
    would use **hidden Markov models** (**HMMs**) to model the temporal variability
    of speech signals [33], [34]. Each HMM state emits a mixture of Gaussians to model
    the spectral features of the audio signal. The emitted Gaussians form a **Gaussian
    mixture model** (**GMM**), and they determine how well each HMM state fits in
    a short window of acoustic features. HMMs are used to model the sequential structure
    of data, while GMMs model the local structure of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: The HMM assumes that successive frames are independent given the hidden state
    of the HMM. Because of this strong conditional independence assumption, the acoustic
    features are typically decorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in using deep learning in speech recognition is to replace GMMs
    with **deep neural networks** (**DNN**) [35]. DNNs take a window of feature vectors
    as input and output the posterior probabilities of the HMM states: *P (HMM state
    | audio features)*.'
  prefs: []
  type: TYPE_NORMAL
- en: The networks used in this step are typically pretrained as a general model on
    a window of spectral features. Usually, **deep** **belief networks** (**DBN**)
    are used to pretrain these networks. The generative pretraining creates many layers
    of feature detectors of increased complexity. Once generative pretraining is finished,
    the network is discriminatively fine-tuned to classify the correct HMM states,
    based on the spectral features. HMMs in these hybrid models are used to align
    the segment classifications provided by the DNNs to a temporal classification
    of the full label sequence. These DNN-HMM models have been shown to achieve better
    phone recognition than GMM-HMM models [36].
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes how RNNs can be used to model sequential data. The problem
    with the straightforward application of RNNs on speech recognition is that the
    labels of the training data need to be perfectly aligned with the input. If the
    data isn't aligned well, then the input to output mapping will contain too much
    of noise for the network to learn anything. Some early attempts tried to model
    the sequential context of the acoustic features by using hybrid RNN-HMM models,
    where the RNNs would model the emission probabilities of the HMM models, much
    in the same way that DBNs are used [37].
  prefs: []
  type: TYPE_NORMAL
- en: Later experiments tried to train LSTMs (see section on *Long* *Short Term Memory*)
    to output the posterior probability of the phonemes at a given frame [38].
  prefs: []
  type: TYPE_NORMAL
- en: The next step in speech recognition would be to get rid of the necessity of
    having aligned labeled data and removing the need for hybrid HMM models.
  prefs: []
  type: TYPE_NORMAL
- en: CTC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard RNN objective functions are defined independently for each sequence
    step, each step outputs its own independent label classification. This means that
    training data must be perfectly aligned with the target labels. However, a global
    objective function that maximizes the probability of a full correct labeling can
    be devised. The idea is to interpret the network outputs as a conditional probability
    distribution over all possible labeling sequences, given the full input sequence.
    The network can then be used as a classifier by searching for the most probable
    labeling given the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Connectionist** **Temporal Classification** (**CTC**) is an objective function
    that defines a distribution over all the alignments with all the output sequences
    [39]. It tries to optimize the overall edit distance between the output sequence
    and the target sequence. This edit distance is the minimum number of insertions,
    substitutions, and deletions required to change the output labeling to target
    labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: A CTC network has a softmax output layer for each step. This softmax function
    outputs label distributions for each possible label plus an extra blank symbol
    (Ø). This extra blank symbol represents that there is no relevant label at that
    time step. The CTC network will thus output label predictions at any point in
    the input sequence. The output is then translated into a sequence labeling by
    removing all the blanks and repeated labels from the paths. This corresponds to
    outputting a new label when the network switches from predicting no label to predicting
    a label or from predicting one label to another. For example, "ØaaØabØØ" gets
    translated into "aab". This has as effect that only the overall sequence of labels
    has to be correct, thus removing the need for aligned data.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this reduction means that multiple output sequences can be reduced to
    the same output labeling. To find the most likely output labeling, we have to
    add all the paths that correspond to that labeling. The task of searching for
    this most probable output labeling is known as decoding (see the *Decoding* section).
  prefs: []
  type: TYPE_NORMAL
- en: An example of such a labeling in speech recognition could be outputting a sequence
    of phonemes, given a sequence of acoustic features. The CTC objective's function,
    built on top of an LSTM, has been to give state-of-the-art results on acoustic
    modeling and to remove the need of using HMMs to model temporal variability [40],
    [41].
  prefs: []
  type: TYPE_NORMAL
- en: Attention-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative to using the CTC sequence to sequence a model is an attention-based
    model [42]. These attention models have the ability to dynamically pay attention
    to parts of the input sequence. This allows them to automatically search for relevant
    parts of the input signal to predict the right phoneme, without having to have
    an explicit segmentation of the parts.
  prefs: []
  type: TYPE_NORMAL
- en: These attention-based sequence models are made up of an RNN that decodes a representation
    of the input into a sequence of labels, which are phonemes in this case. In practice,
    the input representation will be generated by a model that encodes the input sequence
    into a suitable representation. The first network is called the decoder network,
    while the latter is called the encoder network [43].
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is guided by an attention model that focuses each step of the decoder
    on an attention window over encoded input. The attention model can be driven by
    a combination of context (what it is focusing on) or location-based information
    (where it is focusing on). The decoder can then use the previous information and
    the information from the attention window to output the next label (phoneme).
  prefs: []
  type: TYPE_NORMAL
- en: Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we model the phoneme distribution with the acoustic model and train a
    language model (see the *Language Modelling* section), we can combine them together
    with a pronunciation dictionary to get a probability function of words over audio
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P (words | audio features) = P (audio features | phonemes* *) * P (phonemes
    | words) * P (words)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This probability function doesn''t give us the final transcript yet; we still
    need to perform a search over the distribution of the word sequence to find the
    most likely transcription. This search process is called decoding. All possible
    paths of decoding can be illustrated in a lattice data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decoding](img/00256.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A pruned word lattice [44]
  prefs: []
  type: TYPE_NORMAL
- en: The most likely word sequence, given a sequence of audio features, is found
    by searching through all the possible word sequences [33]. A popular search algorithm
    based on dynamic programming that guarantees it could find the most likely sequence
    is the Viterbi algorithm [45]. This algorithm is a breadth-first search algorithm
    that is mostly associated with finding the most likely sequence of states in an
    HMM.
  prefs: []
  type: TYPE_NORMAL
- en: For large vocabulary speech recognition, the Viterbi algorithm becomes intractable
    for practical use. So in practice, heuristic search algorithms, such as beam search,
    are used to try and find the most likely sequence. The beam search heuristic only
    keeps the n-best solutions during the search and assumes that all the rest don't
    lead to the most likely sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Many different decoding algorithms exist [46] and the problem of finding the
    best transcription from the probability function is mostly seen as unsolved.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to conclude this chapter by mentioning end-to-end techniques. Deep learning
    methods, such as CTC [47], [48] and attention based models [49], have allowed
    us to learn the full speech recognition pipeline in an end-to-end fashion. They
    do so without modeling phonemes explicitly. This means that these end-to-end models
    will learn acoustic and language models in one single model and directly output
    a distribution over words. These models illustrate the power of deep learning
    by combining everything in one model; with this, the model becomes conceptually
    easier to understand. We speculate that this will lead to speech recognition being
    recognized as a solved problem in the next few years.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the beginning of this chapter, we learned what RNNs are, how to train them,
    what problems might occur during training, and how to solve these problems. In
    the second part, we described the problem of language modeling and how RNNs help
    us solve some of the difficulties in modeling languages. The third section brought
    this information together in the form of a practical example on how to train a
    character-level language model to generate text based on Leo Tolstoy's *War and*
    *Peace*. The last section gave a brief overview of how deep learning, and especially
    RNNs, can be applied to the problem of speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The RNNs discussed in this chapter are very powerful methods that have been
    very promising when it comes to a lot of tasks, such as language modeling and
    speech recognition. They are especially suited for modeling sequential problems
    where they could discover patterns over sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Siegelmann, H.T. (1995). "Computation Beyond the Turing Limit". Science.
    238 (28): 632–637\. URL: [http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf](http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Alex Graves and Greg Wayne and Ivo Danihelka (2014). "Neural Turing Machines".
    CoRR URL: [https://arxiv.org/pdf/1410.5401v2.pdf](https://arxiv.org/pdf/1410.5401v2.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yann LeCun, Yoshua Bengio & Geoffrey Hinton (2015). "Deep Learning". Nature
    521\. URL: [http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan (2014).
    "Show and Tell: {A} Neural Image Caption Generator". CoRR. URL: [https://arxiv.org/pdf/1411.4555v2.pdf](https://arxiv.org/pdf/1411.4555v2.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Kyunghyun Cho et al. (2014). "Learning Phrase Representations using RNN
    Encoder-Decoder for Statistical Machine Translation". CoRR. URL: [https://arxiv.org/pdf/1406.1078v3.pdf](https://arxiv.org/pdf/1406.1078v3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Ilya Sutskever et al. (2014). "Sequence to Sequence Learning with Neural
    Networks". NIPS''14\. URL: [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Andrej Karpathy (2015). "The Unreasonable Effectiveness of Recurrent Neural
    Networks". URL: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Paul J. Werbos (1990). "Backpropagation Through Time: What It Does and
    How to Do It" Proceedings of the IEEE. URL: [http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Razvan Pascanu and Tomas Mikolov and Yoshua Bengio. (2012). "Understanding
    the exploding gradient problem". URL: [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Yoshua Bengio et al. (1994). "Learning long-term dependencies with gradient
    descent is difficult". URL: [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Razvan Pascanu and Tomas Mikolov and Yoshua Bengio. (2012). "Understanding
    the exploding gradient problem". URL: [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] James Martens, Ilya Sutskever. (2011). "Learning Recurrent Neural Networks
    with Hessian-Free Optimization". URL: [http://www.icml-2011.org/papers/532_icmlpaper.pdf](http://www.icml-2011.org/papers/532_icmlpaper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Ilya Sutskever et al. (2013). "On the importance of initialization and
    momentum in deep learning". URL: [http://proceedings.mlr.press/v28/sutskever13.pdf](http://proceedings.mlr.press/v28/sutskever13.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Geoffrey Hinton & Tijmen Tieleman. (2014) "Neural Networks for Machine
    Learning - Lecture 6a - Overview of mini-batch gradient descent". URL: [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Martin Riedmiller und Heinrich Braun (1992). "Rprop - A Fast Adaptive
    Learning Algorithm" URL: [http://axon.cs.byu.edu/~martinez/classes/678/Papers/riedmiller92rprop.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/riedmiller92rprop.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Sepp Hochreiter and Jurgen Schmidhuber (1997). "Long Short-Term Memory".
    URL: [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Gers et al. (2000) "Learning to Forget: Continual Prediction with LSTM"
    URL: [https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf](https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Nikhil Buduma (2015) "A Deep Dive into Recurrent Neural Nets" URL: [http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Klaus Greff et al. (2015). "LSTM: A Search Space Odyssey". URL: [https://arxiv.org/pdf/1503.04069v1.pdf](https://arxiv.org/pdf/1503.04069v1.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yoshua Bengio et al. (2003). "A Neural Probabilistic Language Model".
    URL: [https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf](https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Christopher Olah (2014) "Deep Learning, NLP, and Representations". URL:
    [http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Tomas Mikolov et al. (2013) "Distributed Representations of Words and
    Phrases and their Compositionality". URL: [http://papers.nips.cc/paper/5021-distributedrepresentations-of-words-and-phrases-and-theircompositionality.pdf](http://papers.nips.cc/paper/5021-distributedrepresentations-of-words-and-phrases-and-theircompositionality.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Tomas Mikolov et al. (2013). "Efficient Estimation of Word Representations
    in Vector Space". URL: [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Tomas Mikolov et al. (2013). "Linguistic Regularities in Continuous Space
    Word Representations". URL: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Thomas Mikolov et al. (2010) "Recurrent neural network based language
    model". URL: [http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Frederic Morin and Yoshua Bengio (2005). "Hierarchical probabilistic neural
    network language model". URL: [http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Alex Graves (2013). "Generating Sequences With Recurrent Neural Networks".
    URL: [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Diederik P. Kingma and Jimmy Ba (2014). "Adam: A Method for Stochastic
    Optimization". URL: [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Daniel Povey et al. (2011) "The Kaldi Speech Recognition Toolkit". URL:
    [http://kaldi-asr.org/](http://kaldi-asr.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Hagit Shatkay. (1995). "The Fourier Transform - A Primer". URL: [https://pdfs.semanticscholar.org/fe79/085198a13f7bd7ee95393dcb82e715537add.pdf](https://pdfs.semanticscholar.org/fe79/085198a13f7bd7ee95393dcb82e715537add.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Dimitri Palaz et al. (2015). "Analysis of CNN-based Speech Recognition
    System using Raw Speech as Input". URL: [https://ronan.collobert.com/pub/matos/2015_cnnspeech_interspeech](https://ronan.collobert.com/pub/matos/2015_cnnspeech_interspeech)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Yedid Hoshen et al. (2015) "Speech Acoustic Modeling from Raw Multichannel
    Waveforms". URL: [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Mark Gales and Steve Young. (2007). "The Application of Hidden Markov
    Models in Speech Recognition". URL: [http://mi.eng.cam.ac.uk/~mjfg/mjfg_NOW.pdf](http://mi.eng.cam.ac.uk/~mjfg/mjfg_NOW.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L.R. Rabiner. (1989). "A tutorial on hidden Markov models and selected
    applications in speech recognition". URL: [http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Abdel-rahman Mohamed et al. (2011). "Acoustic Modeling Using Deep Belief
    Networks". URL: [http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf](http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Geoffrey Hinton et al. (2012) "Deep Neural Networks for Acoustic Modeling
    in Speech Recognition". URL: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Tony Robinson et al. (1996) "The Use of Recurrent Neural Networks in Continuous
    Speech Recognition". URL: [http://www.cstr.ed.ac.uk/downloads/publications/1996/rnn4csr96.pdf](http://www.cstr.ed.ac.uk/downloads/publications/1996/rnn4csr96.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Graves A, Schmidhuber J. (2005) "Framewise phoneme classification with
    bidirectional LSTM and other neural network architectures.". URL: [https://www.cs.toronto.edu/~graves/nn_2005.pdf](https://www.cs.toronto.edu/~graves/nn_2005.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Alex Graves et al. (2006). "Connectionist Temporal Classification: Labelling
    Unsegmented Sequence Data with Recurrent Neural Networks". URL: [http://www.cs.toronto.edu/~graves/icml_2006.pdf](http://www.cs.toronto.edu/~graves/icml_2006.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Alex Graves et al. (2013) "Speech Recognition with Deep Recurrent Neural
    Networks". URL: [https://arxiv.org/pdf/1303.5778.pdf](https://arxiv.org/pdf/1303.5778.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Dario Amodei et al. (2015). "Deep Speech 2: End-to-End Speech Recognition
    in English and Mandarin". URL: [https://arxiv.org/pdf/1512.02595.pdf](https://arxiv.org/pdf/1512.02595.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jan Chorowski et al. (2015). "Attention-Based Models for Speech Recognition",
    URL: [https://arxiv.org/pdf/1506.07503.pdf](https://arxiv.org/pdf/1506.07503.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Dzmitry Bahdanau et al. (2015) "Neural Machine Translation by Jointly
    Learning to Align and Translate" URL: [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] The Institute for Signal and Information Processing. "Lattice tools".
    URL: [https://www.isip.piconepress.com/projects/speech/software/legacy/lattice_tools/](https://www.isip.piconepress.com/projects/speech/software/legacy/lattice_tools/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G.D. Forney. (1973). "The viterbi algorithm". URL: [http://www.systems.caltech.edu/EE/Courses/EE127/EE127A/handout/ForneyViterbi.pdf](http://www.systems.caltech.edu/EE/Courses/EE127/EE127A/handout/ForneyViterbi.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Xavier L. Aubert (2002). "An overview of decoding techniques for large
    vocabulary continuous speech recognition". URL: [http://www.cs.cmu.edu/afs/cs/user/tbergkir/www/11711fa16/aubert_asr_decoding.pdf](http://www.cs.cmu.edu/afs/cs/user/tbergkir/www/11711fa16/aubert_asr_decoding.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Alex Graves and Navdeep Jaitly. (2014). "Towards End-To-End Speech Recognition
    with Recurrent Neural Networks" URL: [http://proceedings.mlr.press/v32/graves14.pdf](http://proceedings.mlr.press/v32/graves14.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Awni Hannun. (2014) "Deep Speech: Scaling up end-to-end speech recognition".
    URL: [https://arxiv.org/pdf/1412.5567.pdf](https://arxiv.org/pdf/1412.5567.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] William Chan (2015). "Listen, Attend and Spell" URL: [https://arxiv.org/pdf/1508.01211.pdf](https://arxiv.org/pdf/1508.01211.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
