- en: Getting to Know TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about TensorFlow, which is one of the most popularly
    used deep learning libraries. Throughout this book, we will be using TensorFlow
    to build deep learning models from scratch. So, in this chapter, we will get the
    hang of TensorFlow and its functionalities. We will also learn about TensorBoard,
    which is a visualization tool provided by TensorFlow used for visualizing models.
    Moving on, we will learn how to build our first neural network, using TensorFlow
    to perform handwritten digit classification. Following that, we will learn about
    TensorFlow 2.0, which is the latest version of TensorFlow. We will learn how TensorFlow
    2.0 differs from its previous versions and how it uses Keras as its high-level
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational graphs and sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten digit classification in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Math operations in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow 2.0 and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source software library from Google, which is extensively
    used for numerical computation. It is one of the most popularly used libraries
    for building deep learning models. It is highly scalable and runs on multiple
    platforms, such as Windows, Linux, macOS, and Android. It was originally developed
    by the researchers and engineers of the Google Brain team.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow supports execution on everything, including CPUs, GPUs, and TPUs,
    which are tensor processing units, and on mobile and embedded platforms. Due to
    its flexible architecture and ease of deployment, it has become a popular choice
    of library among many researchers and scientists for building deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, every computation is represented by a data flow graph, also known
    as a **computational graph**, where a node represents operations, such as addition
    or multiplication, and an edge represents tensors. Data flow graphs can also be
    shared and executed on many different platforms. TensorFlow provides a visualization
    tool, called TensorBoard, for visualizing data flow graphs.
  prefs: []
  type: TYPE_NORMAL
- en: A **tensor** is just a multidimensional array. So, when we say TensorFlow, it
    is literally a flow of multidimensional arrays (tensors) in a computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install TensorFlow easily through `pip` by just typing the following
    command in your Terminal. We will install TensorFlow 1.13.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the successful installation of TensorFlow by running the following
    simple `Hello TensorFlow!` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding program should print `Hello TensorFlow!`. If you get any errors,
    then you probably have not installed TensorFlow correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computational graphs and sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have learned, every computation in TensorFlow is represented by a computational
    graph. They consist of several nodes and edges, where nodes are mathematical operations,
    such as addition and multiplication, and edges are tensors. Computational graphs
    are very efficient at optimizing resources and promote distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: A computational graph consists of several TensorFlow operations, arranged in
    a graph of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a basic addition operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The computational graph for the preceding code would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/583ee1fd-e4fe-4000-90df-72303f077c33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A computational graph helps us to understand the network architecture when
    we work on building a really complex neural network. For instance, let''s consider
    a simple layer, ![](img/dc6ada10-965d-445e-8de0-c874c351c6a6.png). Its computational
    graph would be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfe1fcb5-392c-4f41-a8a7-d6d8c616eb93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two types of dependency in the computational graph, called direct
    and indirect dependency. Say we have the `b` node, the input of which is dependent
    on the output of the `a` node; this type of dependency is called **direct dependency**,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When the `b` node doesn''t depend on the `a` node for its input, it is called
    **indirect dependency**, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if we can understand these dependencies, we can distribute the independent
    computations in the available resources and reduce the computation time. Whenever
    we import TensorFlow, a default graph is created automatically and all of the
    nodes we create are associated with the default graph. We can also create our
    own graphs instead of using the default graph, and this is useful when building
    multiple models that do not depend on one another in one file. A TensorFlow graph
    can be created using `tf.Graph()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If we want to clear the default graph (that is, if we want to clear the previously
    defined variables and operations), then we can do that using `tf.reset_default_graph()`.
  prefs: []
  type: TYPE_NORMAL
- en: Sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A computational graph with operations on its nodes and tensors to its edges
    will be created, and in order to execute the graph, we use a TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: 'A TensorFlow session can be created using `tf.Session()`, as shown in the following
    code, and it will allocate memory for storing the current value of the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After creating the session, we can execute our graph, using the `sess.run()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Every computation in TensorFlow is represented by a computational graph, so
    we need to run a computational graph for everything. That is, in order to compute
    anything on TensorFlow, we need to create a TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following code to multiply two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Instead of printing `9`, the preceding code will print a TensorFlow object,
    `Tensor("Mul:0", shape=(), dtype=int32)`.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, whenever we import TensorFlow, a default computational
    graph will automatically be created and all nodes will get attached to the graph.
    Hence, when we print `a`, it just returns the TensorFlow object because the value
    for `a` is not computed yet, as the computation graph has not been executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to execute the graph, we need to initialize and run the TensorFlow
    session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will print `9`.
  prefs: []
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders are fundamental elements of TensorFlow.
    However, there is always confusion between these three. Let's look at each element,
    one by one, and learn the difference between them.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Variables are containers used to store values. Variables are used as input
    to several other operations in a computational graph. A variable can be created
    using the `tf.Variable()` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a variable called `W`, using `tf.Variable()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we create a variable, `W`, by randomly
    drawing values from a normal distribution with a standard deviation of `0.35`.
  prefs: []
  type: TYPE_NORMAL
- en: What is that parameter called `name` in `tf.Variable()`?
  prefs: []
  type: TYPE_NORMAL
- en: It is used to set the name of the variable in the computational graph. So, in
    the preceding code, Python saves the variable as `W` but in the TensorFlow graph,
    it will be saved as `weights`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also initialize a new variable with a value from another variable using
    `initialized_value()`. For instance, if we want to create a new variable called
    `weights_2`, using a value from the previously defined `weights` variable, it
    can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: However, after defining a variable, we need to initialize all of the variables
    in the computational graph. That can be done using `tf.global_variables_initializer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we create a session, first, we run the initialization operation, which
    will initialize all of the defined variables, and only then can we run the other
    operations, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can also create a TensorFlow variable using `tf.get_variable()`. It takes
    the three important parameters, which are `name`, `shape`, and `initializer`.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike `tf.Variable()`, we cannot pass the value directly to `tf.get_variable()`;
    instead, we use `initializer`. There are several initializers available for initializing
    values. For example, `tf.constant_initializer(value)` initializes the variable
    with a constant value, and `tf.random_normal_initializer(mean, stddev)` initializes
    the variable by drawing values from random normal distribution with a specified
    mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variables created using `tf.Variable()` cannot be shared, and every time we
    call `tf.Variable()`, it will create a new variable. But `tf.get_variable()` checks
    the computational graph for an existing variable with the specified parameter.
    If the variable already exists, then it will be reused; otherwise, a new variable
    will be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So, the preceding code checks whether there is any variable already existing
    with the given parameters. If yes, then it will reuse it; otherwise, it will create
    a new variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are reusing variables using `tf.get_variable()`, in order to avoid
    name conflicts, we use `tf.variable_scope`, as shown in the following code. A
    variable scope is basically a name-scoping technique that just adds a prefix to
    the variable within the scope to avoid the naming clash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you print `a.name` and `b.name`, then it will return the same name, which
    is `scope/x:0`. As you can see, we specified the `reuse=True` parameter in the
    variable scope named `scope`, which implies that the variables can be shared.
    If we don't set `reuse = True`, then it will give an error saying that the variable
    already exists.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use `tf.get_variable()` rather than `tf.Variable()`, because
    `tf.get_variable`, allows you to share variables, and it will make the code refactoring
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Constants, unlike variables, cannot have their values changed. That is, constants
    are immutable. Once they are assigned values, they cannot be changed throughout.
    We can create constants using the `tf.constant()`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders and feed dictionaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can think of placeholders as variables, where we only define the type and
    dimension, but do not assign the value. Values for the placeholders will be fed
    at runtime. We feed the data to the computational graphs using placeholders. Placeholders
    are defined with no values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A placeholder can be defined using `tf.placeholder()`. It takes an optional
    argument called `shape`, which denotes the dimensions of the data. If `shape`
    is set to `None`, then we can feed data of any size at runtime. A placeholder
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To put it in simple terms, we use `tf.Variable` to store the data and `tf.placeholder`
    for feeding the external data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simple example to better understand placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If we run the preceding code, then it will return an error because we are trying
    to compute `y`, where `y= x+3` and `x` is a placeholder whose value is not assigned.
    As we have learned, values for the placeholders will be assigned at runtime. We
    assign the values of the placeholder using the `feed_dict` parameter. The `feed_dict`
    parameter is basically the dictionary where the key represents the name of the
    placeholder, and the value represents the value of the placeholder.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following code, we set `feed_dict = {x:5}`, which implies
    that the value for the `x` placeholder is `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code returns `8.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to use multiple values for `x`? As we have not defined any
    shapes for the placeholders, it takes any number of values, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s say we define the shape of `x` as `[None,2]`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that `x` can take a matrix of any rows but with `2` columns, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Introducing TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is TensorFlow's visualization tool, which can be used to visualize
    a computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it becomes confusing when we have to debug the network. So,
    if we can visualize the computational graph in TensorBoard, we can easily understand
    such complex models, debug them, and optimize them. TensorBoard also supports
    sharing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, the TensorBoard panel consists of several
    tabs—SCALARS, IMAGES, AUDIO, GRAPHS, DISTRIBUTIONS, HISTOGRAMS, and EMBEDDINGS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdbe9a38-d2d4-464f-ba46-67bf44e48477.png)'
  prefs: []
  type: TYPE_IMG
- en: The tabs are pretty self-explanatory. The SCALARS tab shows useful information
    about the scalar variables we use in our program. For example, it shows how the
    value of a scalar variable called loss changes over several iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The GRAPHS tab shows the computational graph. The DISTRIBUTIONS and HISTOGRAMS
    tabs show the distribution of a variable. For example, our model's weight distribution
    and histogram can be seen under these tabs. The **EMBEDDINGS** tab is used for
    visualizing high-dimensional vectors, such as word embeddings (we will learn about
    this in detail in [Chapter 7](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml), *Learning
    Text Representations*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a basic computational graph and visualize it in TensorBoard. Let''s
    say we have four variables, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s multiply `x` and `y` and `a` and `b` and save them as `prod1` and `prod2`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `prod1` and `prod2` and store them in `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can visualize all of these operations in TensorBoard. In order to visualize
    in TensorBoard, we first need to save our event files. It can be done using `tf.summary.FileWriter()`.
    It takes two important parameters, `logdir` and `graph`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, `logdir` specifies the directory where we want to store
    the graph, and `graph` specifies which graph we want to store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `graphs` is the directory where we are storing our event
    file, and `sess.graph` specifies the current graph in our TensorFlow session.
    So, we are storing the current graph in the TensorFlow session in the `graphs`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start TensorBoard, go to your Terminal, locate the working directory, and
    type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `logdir` parameter indicates the directory where the event file is stored
    and `port` is the port number. Once you run the preceding command, open your browser
    and type `http://localhost:8000/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the TensorBoard panel, under the GRAPHS tab, you can see the computational
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b63f2545-f298-46d0-a6fd-9cc136644f54.png)'
  prefs: []
  type: TYPE_IMG
- en: As you may notice, all of the operations we have defined are clearly shown in
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a name scope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scoping is used to reduce complexity and helps us to better understand a model
    by grouping related nodes together. Having a name scope helps us to group similar
    operations in a graph. It comes in handy when we are building a complex architecture.
    Scoping can be created using `tf.name_scope()`. In the previous example, we performed
    two operations, `Product` and `sum`. We can simply group them into two different
    name scopes as `Product` and `sum`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how `prod1` and `prod2` perform multiplication
    and compute the result. We''ll define a name scope called `Product`, and group
    the `prod1` and `prod2` operations, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the name scope for `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the file in the `graphs` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the graph in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice, now, we have only two nodes, **sum** and **Product**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa3246f8-34bc-4496-a021-d8e191e5a085.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we double-click on the nodes, we can see how the computation is happening.
    As you can see, the **prod1** and **prod2** nodes are grouped under the **Product**
    scope, and their results are sent to the **sum** node, where they will be added.
    You can see how the **prod1** and **prod2** nodes compute their value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d11d7a2b-389d-497b-bc80-9bd9c9e9bdb4.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding example is just a simple example. When we are working on a complex
    project with a lot of operations, name scoping helps us to group similar operations
    together and enables us to understand the computational graph better.
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digit classification using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Putting together all the concepts we have learned so far, we will see how we
    can use TensorFlow to build a neural network to recognize handwritten digits.
    If you have been playing around with deep learning of late, then you must have
    come across the MNIST dataset. It has been called the *hello world* of deep learning.
    It consists of 55,000 data points of handwritten digits (0 to 9).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how we can use our neural network to recognize
    these handwritten digits, and we will get the hang of TensorFlow and TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the required libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a first step, let''s import all of the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the dataset, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `data/mnist` implies the location where we store the
    MNIST dataset, and `one_hot=True` implies that we are one-hot encoding the labels
    (0 to 9).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see what we have in our data by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We have `55000` images in the training set, each image is of size `784`, and
    we have `10` labels, which are actually 0 to 9\. Similarly, we have `10000` images
    in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll plot an input image to see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, our input image looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52193d70-1696-4ae6-8bfa-ed05915c159c.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the number of neurons in each layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll build a four-layer neural network with three hidden layers and one output
    layer. As the size of the input image is `784`, we set `num_input` to `784`, and
    since we have 10 handwritten digits (0 to 9), we set `10` neurons in the output
    layer. We define the number of neurons in each layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Defining placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have learned, we first need to define the placeholders for `input` and
    `output`. Values for the placeholders will be fed in at runtime through `feed_dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Since we have a four-layer network, we have four weights and four biases. We
    initialize our weights by drawing values from the truncated normal distribution
    with a standard deviation of `0.1`. Remember, the dimensions of the weights matrix
    should be a *numb**er of neurons in the previous layer* x *a number of neurons
    in the current layer*. For instance, the dimension of weight matrix `w3` should
    be the *number of neurons in hidden layer 2* x *the number of neurons in hidden
    layer 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We often define all of the weights in a dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the bias should be the number of neurons in the current layer.
    For instance, the dimension of the `b2` bias is the number of neurons in hidden
    layer 2\. We set the bias value as a constant; `0.1` in all of the layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we''ll define the forward propagation operation. We''ll use ReLU activations
    in all layers. In the last layers, we''ll apply `sigmoid` activation, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Computing loss and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we''ll define our loss function. We''ll use softmax cross-entropy as
    our loss function. TensorFlow provides the `tf.nn.softmax_cross_entropy_with_logits()`
    function for computing softmax cross-entropy loss. It takes two parameters as
    inputs, `logits` and `labels`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `logits` parameter specifies the `logits` predicted by our network; for
    example, `y_hat`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels` parameter specifies the actual labels; for example, true labels,
    `Y`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We take the mean of the `loss` function using `tf.reduce_mean()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to minimize the loss using backpropagation. Don''t worry! We don''t
    have to calculate the derivatives of all the weights manually. Instead, we can
    use TensorFlow''s optimizer. In this section, we the use the Adam optimizer. It
    is a variant of the gradient descent optimization technique we learned about in
    [Chapter 1](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml), *Introduction to Deep
    Learning*. In [Chapter 3](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml), *Gradient
    Descent and Its Variants*, we will dive into the details and see how exactly the
    Adam optimizer and several other optimizers work. For now, let''s say we use the
    Adam optimizer as our backpropagation algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Computing accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We calculate the accuracy of our model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `y_hat` parameter denotes the predicted probability for each class of our
    model. Since we have `10` classes, we will have `10` probabilities. If the probability
    is high at position `7`, then it means that our network predicts the input image
    as digit `7` with high probability. The `tf.argmax()` function returns the index
    of the largest value. Thus, `tf.argmax(y_hat,1)` gives the index where the probability
    is high. Thus, if the probability is high at index `7`, then it returns `7`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Y` parameter denotes the actual labels, and they are the one-hot encoded
    values. That is, it consists of zeros everywhere except at the position of the
    actual image, where it consists of `1`. For instance, if the input image is `7`,
    then `Y` has 0 at all indices except at index `7`, where it has `1`. Thus, `tf.argmax(Y,1)`
    returns `7` because that is where we have a high value, `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, `tf.argmax(y_hat,1)` gives the predicted digit, and `tf.argmax(Y,1)` gives
    us the actual digit.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.equal(x, y)` function takes `x` and `y` as inputs and returns the truth
    value of *(x == y)* element-wise. Thus, `correct_pred = tf.equal(predicted_digit,actual_digit)`
    consists of `True` where the actual and predicted digits are the same, and `False`
    where the actual and predicted digits are not the same. We convert the Boolean
    values in `correct_pred` into float values using TensorFlow's cast operation,
    `tf.cast(correct_pred, tf.float32)`. After converting them into float values,
    we take the average using `tf.reduce_mean()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, `tf.reduce_mean(tf.cast(correct_pred, tf.float32))` gives us the average
    correct predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Creating summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also visualize how the loss and accuracy of our model changes during
    several iterations in TensorBoard. So, we use `tf.summary()` to get the summary
    of the variable. Since the loss and accuracy are scalar variables, we use `tf.summary.scalar()`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we merge all of the summaries we use in our graph, using `tf.summary.merge_all()`.
    We do this because when we have many summaries, running and storing them would
    become inefficient, so we run them once in our session instead of running multiple
    times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it is time to train our model. As we have learned, first, we need to initialize
    all of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the batch size, number of iterations, and learning rate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the event files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model for a number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a batch of data according to the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Print `loss` and `accuracy` for every 100^(th) iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice from the following output, the loss decreases and the accuracy
    increases over various training iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing graphs in TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training, we can visualize our computational graph in TensorBoard, as
    shown in the following diagram. As you can see, our **Model** takes input, weights,
    and biases as input and returns the output. We compute Loss and Accuracy based
    on the output of the model. We minimize the loss by calculating gradients and
    updating weights. We can observe all of this in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d073d2d4-a592-4e2a-871a-c3f4956612dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we double-click and expand the **Model**, we can see that we have three
    hidden layers and one output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7d8d3c-94b9-40f4-a951-3edb199393fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can double-click and see every node. For instance, if we open
    **weights**, we can see how the four weights are initialized using truncated normal
    distribution, and how it is updated using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/662ac908-27ec-4570-88c8-0cad3354fb23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have learned, the computational graph helps us to understand what is
    happening on each node. We can see how the accuracy is being calculated by double-clicking
    on the **Accuracy** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d48fcb84-8a10-4716-a8a3-69006100ce30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember that we also stored the summary of our `loss` and `accuracy` variables.
    We can find them under the SCALARS tab in TensorBoard, as shown in the following
    screenshot. We can see how that loss decreases over iterations, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fa0158f-52d0-433f-9aa1-451efad738bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows how accuracy increases over iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b81c6aa3-29d8-4988-a7ec-a916fe0727a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Introducing eager execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operations, eager execution follows the imperative programming paradigm, where
    any operations can be performed immediately, without having to create a graph,
    just like we do in Python. Hence, with eager execution, we can say goodbye to
    sessions and placeholders. It also makes the debugging process easier with an
    immediate runtime error, unlike the graph mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the graph mode, to compute anything, we run the session. As
    shown in the following code, to evaluate the value of `z`, we have to run the
    TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'With eager execution, we don''t need to create a session; we can simply compute
    `z`, just like we do in Python. In order to enable eager execution, just call
    the `tf.enable_eager_execution()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get the output value, we can print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Math operations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will explore some of the operations in TensorFlow using the eager execution
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Let's start with some basic arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `tf.add` to add two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf.subtract` function is used for finding the difference between two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf.multiply` function is used for multiplying two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide two numbers using `tf.divide`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The dot product can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s find the index of the minimum and maximum elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The index of the minimum value is computed using `tf.argmin()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The index of the maximum value is computed using `tf.argmax()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following code to find the squared difference between `x` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Let's try typecasting; that is, converting from one data type into another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the type of `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We can convert the type of `x`, which is `tf.int32`, into `tf.float32` using
    `tf.cast`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the `x` type. It will be `tf.float32`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the matrices row-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to concatenate the matrices column-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Stack the `x` matrix using the `stack` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let'' see how to perform the `reduce_mean` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean value of `x`; that is, *(1.0 + 5.0 + 2.0 + 3.0) / 4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean across the row; that is, *(1.0+5.0)/2, (2.0+3.0)/2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean across the column; that is, *(1.0+5.0)/2.0, (2.0+3.0)/2.0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw random values from the probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the softmax probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll look at how to compute the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `square` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The gradients can be computed for the preceding `square` function using `tf.GradientTape`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: More TensorFlow operations are available in the Notebook on GitHub at [http://bit.ly/2YSYbYu](http://bit.ly/2YSYbYu).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is a lot more than this. We will learn about various important functionalities
    of TensorFlow as we move on through this book.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow 2.0 has got some really cool features. It sets the eager execution
    mode by default. It provides a simplified workflow and uses Keras as the main
    API for building deep learning models. It is also backward compatible with TensorFlow
    1.x versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install TensorFlow 2.0, open your Terminal and type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras
    works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Bonjour Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is another popularly used deep learning library. It was developed by François
    Chollet at Google. It is well known for its fast prototyping, and it makes model
    building simple. It is a high-level library, meaning that it does not perform
    any low-level operations on its own, such as convolution. It uses a backend engine
    for doing that, such as TensorFlow. The Keras API is available in `tf.keras`,
    and TensorFlow 2.0 uses it as the primary API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a model in Keras involves four important steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is defining the model. Keras provides two different APIs to
    define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The sequential API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functional API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a sequential model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a sequential model, we stack each layer, one above another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s define our model as a `Sequential()` model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the first layer, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `Dense` implies a fully connected layer, `input_dim`
    implies the dimension of our input, and `activation` specifies the activation
    function that we use. We can stack up as many layers as we want, one above another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the next layer with the `relu` activation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the output layer with the `sigmoid` activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The final code block of the sequential model is shown as follows. As you can
    see, the Keras code is much simpler than the TensorFlow code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Defining a functional model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack of one above another. A functional
    model comes in handy when creating complex models, such as directed acyclic graphs,
    models with multiple input values, multiple output values, and shared layers.
    Now, we will see how to define a functional model in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define the input dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll define our first fully connected layer with `10` neurons and `relu`
    activations, using the `Dense` class, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined `layer1`, but where is the input to `layer1` coming from? We need
    to specify the input to `layer1` in a bracket notation at the end, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the next layer, `layer2`, with `13` neurons and `relu` activation.
    The input to `layer2` comes from `layer1`, so that is added in the bracket at
    the end, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the output layer with the `sigmoid` activation function.
    Input to the output layer comes from `layer2`, so that is added in parentheses
    at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining all of the layers, we define the model using a `Model` class,
    where we need to specify `inputs` and `outputs`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for the functional model is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have defined the model, the next step is to compile it. In this
    phase, we set up how the model should learn. We define three parameters when compiling
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `optimizer` parameter: This defines the optimization algorithm we want
    to use; for example, the gradient descent, in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `loss` parameter: This is the objective function that we are trying to
    minimize; for example, the mean squared error or cross-entropy loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `metrics` parameter: This is the metric through which we want to assess
    the model''s performance; for example, `accuracy`. We can also specify more than
    one metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the following code to compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We defined and also compiled the model. Now, we will train the model. Training
    the model can be done using the `fit` function. We specify our features, `x`;
    labels, `y`; the number of `epochs` we want to train; and the `batch_size`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training the model, we will evaluate the model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also evaluate the model on the same train set, and that will help us
    to understand the training accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: MNIST digit classification using TensorFlow 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how we can perform MNIST handwritten digit classification,
    using TensorFlow 2.0\. It requires only a few lines of code compared to TensorFlow
    1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just
    need to add `tf.keras` to the Keras code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a train and test set with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the train and test sets by dividing the values of `x` by the maximum
    value of `x`; that is, `255.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the sequential model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add layers to the model. We use a three-layer network with the
    `relu` function and `softmax` in the final layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model by running the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Writing code with the Keras API is that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Should we use Keras or TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that TensorFlow 2.0 uses Keras as a high-level API. Using a
    high-level API enables rapid prototyping. But we can't use high-level APIs when
    we want to build a model on a low level, or if we want to build something that
    a high-level API doesn't provide.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, writing code from scratch strengthens our knowledge of
    algorithms and helps us to understand and learn concepts better than directly
    diving into high-level APIs. That's why, in this book, we will code most of the
    algorithms from scratch using TensorFlow, without using high-level APIs such as
    Keras. We will be using TensorFlow version 1.13.1.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off this chapter by learning about TensorFlow and how it uses computational
    graphs. We learned that every computation in TensorFlow is represented by a computational
    graph, which consists of several nodes and edges, where nodes are mathematical
    operations, such as addition and multiplication, and edges are tensors.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that variables are containers used to store values and they are used
    as input to several other operations in a computational graph. Later, we learned
    that placeholders are like variables, where we only define the type and dimension
    but will not assign the values, and values for the placeholders will be fed at
    runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we learned about TensorBoard, which is TensorFlow's visualization
    tool and can be used to visualize a computational graph. It can also be used to
    plot various quantitative metrics and the results of several intermediate calculations.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about eager execution, which is more Pythonic, and allows for
    rapid prototyping. We understood that, unlike the graph mode, where we need to
    construct a graph every time to perform any operations, eager execution follows
    the imperative programming paradigm, where any operations can be performed immediately,
    without having to create a graph, just like we do in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about gradient descent and the variants of
    gradient descent algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assess your knowledge about TensorFlow by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a computational graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are sessions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we create a session in TensorFlow?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between variables and placeholders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need TensorBoard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the name scope and how is it created?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is eager execution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can learn more about TensorFlow by checking out the official documentation
    at [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials).
  prefs: []
  type: TYPE_NORMAL
