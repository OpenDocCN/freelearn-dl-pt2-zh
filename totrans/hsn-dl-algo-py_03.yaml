- en: Getting to Know TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认识TensorFlow
- en: In this chapter, we will learn about TensorFlow, which is one of the most popularly
    used deep learning libraries. Throughout this book, we will be using TensorFlow
    to build deep learning models from scratch. So, in this chapter, we will get the
    hang of TensorFlow and its functionalities. We will also learn about TensorBoard,
    which is a visualization tool provided by TensorFlow used for visualizing models.
    Moving on, we will learn how to build our first neural network, using TensorFlow
    to perform handwritten digit classification. Following that, we will learn about
    TensorFlow 2.0, which is the latest version of TensorFlow. We will learn how TensorFlow
    2.0 differs from its previous versions and how it uses Keras as its high-level
    API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习TensorFlow，这是最流行的深度学习库之一。在本书中，我们将使用TensorFlow从头开始构建深度学习模型。因此，在本章中，我们将了解TensorFlow及其功能。我们还将学习TensorFlow提供的用于模型可视化的工具TensorBoard。接下来，我们将学习如何使用TensorFlow执行手写数字分类，以构建我们的第一个神经网络。随后，我们将了解TensorFlow
    2.0，这是TensorFlow的最新版本。我们将学习TensorFlow 2.0与其早期版本的区别，以及它如何使用Keras作为其高级API。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: TensorFlow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Computational graphs and sessions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图和会话
- en: Variables, constants, and placeholders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: TensorBoard
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard
- en: Handwritten digit classification in TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow中的手写数字分类
- en: Math operations in TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow中的数学运算
- en: TensorFlow 2.0 and Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0和Keras
- en: What is TensorFlow?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow是什么？
- en: TensorFlow is an open source software library from Google, which is extensively
    used for numerical computation. It is one of the most popularly used libraries
    for building deep learning models. It is highly scalable and runs on multiple
    platforms, such as Windows, Linux, macOS, and Android. It was originally developed
    by the researchers and engineers of the Google Brain team.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是来自Google的开源软件库，广泛用于数值计算。它是构建深度学习模型的最流行的库之一。它高度可扩展，可以运行在多个平台上，如Windows、Linux、macOS和Android。最初由Google
    Brain团队的研究人员和工程师开发。
- en: TensorFlow supports execution on everything, including CPUs, GPUs, and TPUs,
    which are tensor processing units, and on mobile and embedded platforms. Due to
    its flexible architecture and ease of deployment, it has become a popular choice
    of library among many researchers and scientists for building deep learning models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持在包括CPU、GPU和TPU（张量处理单元）在内的所有设备上执行，还支持移动和嵌入式平台。由于其灵活的架构和易于部署，它已成为许多研究人员和科学家构建深度学习模型的流行选择。
- en: In TensorFlow, every computation is represented by a data flow graph, also known
    as a **computational graph**, where a node represents operations, such as addition
    or multiplication, and an edge represents tensors. Data flow graphs can also be
    shared and executed on many different platforms. TensorFlow provides a visualization
    tool, called TensorBoard, for visualizing data flow graphs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，每个计算都由数据流图表示，也称为**计算图**，其中节点表示操作，如加法或乘法，边表示张量。数据流图也可以在许多不同的平台上共享和执行。TensorFlow提供了一种称为TensorBoard的可视化工具，用于可视化数据流图。
- en: A **tensor** is just a multidimensional array. So, when we say TensorFlow, it
    is literally a flow of multidimensional arrays (tensors) in a computation graph.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量**只是一个多维数组。因此，当我们说TensorFlow时，它实际上是在计算图中流动的多维数组（张量）。'
- en: 'You can install TensorFlow easily through `pip` by just typing the following
    command in your Terminal. We will install TensorFlow 1.13.1:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在终端中输入以下命令轻松地通过`pip`安装TensorFlow。我们将安装TensorFlow 1.13.1：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can check the successful installation of TensorFlow by running the following
    simple `Hello TensorFlow!` program:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下简单的`Hello TensorFlow!`程序来检查TensorFlow的成功安装：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding program should print `Hello TensorFlow!`. If you get any errors,
    then you probably have not installed TensorFlow correctly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的程序应该打印出`Hello TensorFlow!`。如果出现任何错误，那么您可能没有正确安装TensorFlow。
- en: Understanding computational graphs and sessions
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解计算图和会话
- en: As we have learned, every computation in TensorFlow is represented by a computational
    graph. They consist of several nodes and edges, where nodes are mathematical operations,
    such as addition and multiplication, and edges are tensors. Computational graphs
    are very efficient at optimizing resources and promote distributed computing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学到的，TensorFlow中的每个计算都由计算图表示。它们由多个节点和边缘组成，其中节点是数学操作，如加法和乘法，边缘是张量。计算图非常有效地优化资源并促进分布式计算。
- en: A computational graph consists of several TensorFlow operations, arranged in
    a graph of nodes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图由几个TensorFlow操作组成，排列成节点图。
- en: 'Let''s consider a basic addition operation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个基本的加法操作：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The computational graph for the preceding code would look like the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的计算图将如下所示：
- en: '![](img/583ee1fd-e4fe-4000-90df-72303f077c33.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/583ee1fd-e4fe-4000-90df-72303f077c33.png)'
- en: 'A computational graph helps us to understand the network architecture when
    we work on building a really complex neural network. For instance, let''s consider
    a simple layer, ![](img/dc6ada10-965d-445e-8de0-c874c351c6a6.png). Its computational
    graph would be represented as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在构建一个非常复杂的神经网络时，计算图帮助我们理解网络架构。例如，让我们考虑一个简单的层，![](img/dc6ada10-965d-445e-8de0-c874c351c6a6.png)。其计算图将表示如下：
- en: '![](img/cfe1fcb5-392c-4f41-a8a7-d6d8c616eb93.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfe1fcb5-392c-4f41-a8a7-d6d8c616eb93.png)'
- en: 'There are two types of dependency in the computational graph, called direct
    and indirect dependency. Say we have the `b` node, the input of which is dependent
    on the output of the `a` node; this type of dependency is called **direct dependency**,
    as shown in the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图中有两种依赖类型，称为直接和间接依赖。假设我们有`b`节点，其输入依赖于`a`节点的输出；这种依赖称为**直接依赖**，如下所示的代码：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When the `b` node doesn''t depend on the `a` node for its input, it is called
    **indirect dependency**, as shown in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当`b`节点的输入不依赖于`a`节点时，这被称为**间接依赖**，如下所示的代码：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, if we can understand these dependencies, we can distribute the independent
    computations in the available resources and reduce the computation time. Whenever
    we import TensorFlow, a default graph is created automatically and all of the
    nodes we create are associated with the default graph. We can also create our
    own graphs instead of using the default graph, and this is useful when building
    multiple models that do not depend on one another in one file. A TensorFlow graph
    can be created using `tf.Graph()`, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能理解这些依赖关系，我们就能在可用资源中分配独立的计算，并减少计算时间。每当我们导入TensorFlow时，会自动创建一个默认图，并且我们创建的所有节点都与默认图相关联。我们还可以创建自己的图而不是使用默认图，当在一个文件中构建多个不相互依赖的模型时，这非常有用。可以使用`tf.Graph()`创建TensorFlow图，如下所示：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If we want to clear the default graph (that is, if we want to clear the previously
    defined variables and operations), then we can do that using `tf.reset_default_graph()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要清除默认图（即清除先前定义的变量和操作），可以使用`tf.reset_default_graph()`。
- en: Sessions
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 会话
- en: A computational graph with operations on its nodes and tensors to its edges
    will be created, and in order to execute the graph, we use a TensorFlow session.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将创建一个包含节点和边缘张量的计算图，为了执行该图，我们使用TensorFlow会话。
- en: 'A TensorFlow session can be created using `tf.Session()`, as shown in the following
    code, and it will allocate memory for storing the current value of the variable:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tf.Session()`创建TensorFlow会话，如下所示的代码，它将分配内存以存储变量的当前值：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After creating the session, we can execute our graph, using the `sess.run()`
    method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建会话后，我们可以使用`sess.run()`方法执行我们的图。
- en: Every computation in TensorFlow is represented by a computational graph, so
    we need to run a computational graph for everything. That is, in order to compute
    anything on TensorFlow, we need to create a TensorFlow session.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的每个计算都由计算图表示，因此我们需要为所有事物运行计算图。也就是说，为了在TensorFlow上计算任何内容，我们需要创建一个TensorFlow会话。
- en: 'Let''s execute the following code to multiply two numbers:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下代码来执行两个数字的乘法：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Instead of printing `9`, the preceding code will print a TensorFlow object,
    `Tensor("Mul:0", shape=(), dtype=int32)`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是打印`9`，上述代码将打印一个TensorFlow对象，`Tensor("Mul:0", shape=(), dtype=int32)`。
- en: As we discussed earlier, whenever we import TensorFlow, a default computational
    graph will automatically be created and all nodes will get attached to the graph.
    Hence, when we print `a`, it just returns the TensorFlow object because the value
    for `a` is not computed yet, as the computation graph has not been executed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，每当导入 TensorFlow 时，将自动创建一个默认计算图，并且所有节点都将附加到该图中。因此，当我们打印 `a` 时，它只返回
    TensorFlow 对象，因为尚未计算 `a` 的值，因为尚未执行计算图。
- en: 'In order to execute the graph, we need to initialize and run the TensorFlow
    session, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行图，我们需要初始化并运行 TensorFlow 会话，如下所示：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code will print `9`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印 `9`。
- en: Variables, constants, and placeholders
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: Variables, constants, and placeholders are fundamental elements of TensorFlow.
    However, there is always confusion between these three. Let's look at each element,
    one by one, and learn the difference between them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 变量、常量和占位符是 TensorFlow 的基本元素。但是，总会有人对这三者之间感到困惑。让我们逐个看看每个元素，并学习它们之间的区别。
- en: Variables
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量
- en: 'Variables are containers used to store values. Variables are used as input
    to several other operations in a computational graph. A variable can be created
    using the `tf.Variable()` function, as shown in the following code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是用来存储值的容器。变量作为计算图中几个其他操作的输入。可以使用 `tf.Variable()` 函数创建变量，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s create a variable called `W`, using `tf.Variable()`, as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `tf.Variable()` 创建一个名为 `W` 的变量，如下所示：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see in the preceding code, we create a variable, `W`, by randomly
    drawing values from a normal distribution with a standard deviation of `0.35`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们通过从标准差为 `0.35` 的正态分布中随机抽取值来创建变量 `W`。
- en: What is that parameter called `name` in `tf.Variable()`?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable()` 中的参数 `name` 被称为什么？'
- en: It is used to set the name of the variable in the computational graph. So, in
    the preceding code, Python saves the variable as `W` but in the TensorFlow graph,
    it will be saved as `weights`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它用于在计算图中设置变量的名称。因此，在上述代码中，Python 将变量保存为 `W`，但在 TensorFlow 图中，它将保存为 `weights`。
- en: 'We can also initialize a new variable with a value from another variable using
    `initialized_value()`. For instance, if we want to create a new variable called
    `weights_2`, using a value from the previously defined `weights` variable, it
    can be done as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `initialized_value()` 从另一个变量中初始化新变量的值。例如，如果我们想要创建一个名为 `weights_2` 的新变量，并使用先前定义的
    `weights` 变量的值，可以按以下方式完成：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: However, after defining a variable, we need to initialize all of the variables
    in the computational graph. That can be done using `tf.global_variables_initializer()`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在定义变量之后，我们需要初始化计算图中的所有变量。可以使用 `tf.global_variables_initializer()` 完成此操作。
- en: 'Once we create a session, first, we run the initialization operation, which
    will initialize all of the defined variables, and only then can we run the other
    operations, as shown in the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了会话，首先运行初始化操作，这将初始化所有已定义的变量，然后才能运行其他操作，如下所示：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can also create a TensorFlow variable using `tf.get_variable()`. It takes
    the three important parameters, which are `name`, `shape`, and `initializer`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `tf.get_variable()` 创建 TensorFlow 变量。它需要三个重要参数，即 `name`、`shape` 和 `initializer`。
- en: Unlike `tf.Variable()`, we cannot pass the value directly to `tf.get_variable()`;
    instead, we use `initializer`. There are several initializers available for initializing
    values. For example, `tf.constant_initializer(value)` initializes the variable
    with a constant value, and `tf.random_normal_initializer(mean, stddev)` initializes
    the variable by drawing values from random normal distribution with a specified
    mean and standard deviation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `tf.Variable()` 不同，我们不能直接将值传递给 `tf.get_variable()`；相反，我们使用 `initializer`。有几种初始化器可用于初始化值。例如，`tf.constant_initializer(value)`
    使用常量值初始化变量，`tf.random_normal_initializer(mean, stddev)` 使用指定均值和标准差的随机正态分布初始化变量。
- en: 'Variables created using `tf.Variable()` cannot be shared, and every time we
    call `tf.Variable()`, it will create a new variable. But `tf.get_variable()` checks
    the computational graph for an existing variable with the specified parameter.
    If the variable already exists, then it will be reused; otherwise, a new variable
    will be created:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.Variable()` 创建的变量不能共享，每次调用 `tf.Variable()` 时都会创建一个新变量。但是 `tf.get_variable()`
    会检查计算图中是否存在指定参数的现有变量。如果变量已存在，则将重用它；否则将创建一个新变量：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So, the preceding code checks whether there is any variable already existing
    with the given parameters. If yes, then it will reuse it; otherwise, it will create
    a new variable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述代码检查是否存在与给定参数匹配的任何变量。如果是，则将重用它；否则，将创建一个新变量。
- en: 'Since we are reusing variables using `tf.get_variable()`, in order to avoid
    name conflicts, we use `tf.variable_scope`, as shown in the following code. A
    variable scope is basically a name-scoping technique that just adds a prefix to
    the variable within the scope to avoid the naming clash:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用`tf.get_variable()`重用变量，为了避免名称冲突，我们使用`tf.variable_scope`，如下面的代码所示。变量作用域基本上是一种命名技术，在作用域内为变量添加前缀以避免命名冲突：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you print `a.name` and `b.name`, then it will return the same name, which
    is `scope/x:0`. As you can see, we specified the `reuse=True` parameter in the
    variable scope named `scope`, which implies that the variables can be shared.
    If we don't set `reuse = True`, then it will give an error saying that the variable
    already exists.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打印`a.name`和`b.name`，则它将返回相同的名称`scope/x:0`。正如您所见，我们在名为`scope`的变量作用域中指定了`reuse=True`参数，这意味着变量可以被共享。如果我们不设置`reuse=True`，则会出现错误，提示变量已经存在。
- en: It is recommended to use `tf.get_variable()` rather than `tf.Variable()`, because
    `tf.get_variable`, allows you to share variables, and it will make the code refactoring
    easier.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用`tf.get_variable()`而不是`tf.Variable()`，因为`tf.get_variable`允许您共享变量，并且可以使代码重构更容易。
- en: Constants
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常量
- en: 'Constants, unlike variables, cannot have their values changed. That is, constants
    are immutable. Once they are assigned values, they cannot be changed throughout.
    We can create constants using the `tf.constant()`, as shown in the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 常量与变量不同，不能改变其值。也就是说，常量是不可变的。一旦为它们分配了值，就不能在整个过程中更改它们。我们可以使用`tf.constant()`创建常量，如下面的代码所示：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Placeholders and feed dictionaries
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 占位符和feed字典
- en: We can think of placeholders as variables, where we only define the type and
    dimension, but do not assign the value. Values for the placeholders will be fed
    at runtime. We feed the data to the computational graphs using placeholders. Placeholders
    are defined with no values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将占位符视为变量，其中仅定义类型和维度，但不分配值。占位符的值将在运行时通过数据填充到计算图中。占位符是没有值的定义。
- en: 'A placeholder can be defined using `tf.placeholder()`. It takes an optional
    argument called `shape`, which denotes the dimensions of the data. If `shape`
    is set to `None`, then we can feed data of any size at runtime. A placeholder
    can be defined as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tf.placeholder()`来定义占位符。它接受一个可选参数`shape`，表示数据的维度。如果`shape`设置为`None`，则可以在运行时提供任意大小的数据。占位符可以定义如下：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To put it in simple terms, we use `tf.Variable` to store the data and `tf.placeholder`
    for feeding the external data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们使用`tf.Variable`存储数据，使用`tf.placeholder`来提供外部数据。
- en: 'Let''s consider a simple example to better understand placeholders:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来更好地理解占位符：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If we run the preceding code, then it will return an error because we are trying
    to compute `y`, where `y= x+3` and `x` is a placeholder whose value is not assigned.
    As we have learned, values for the placeholders will be assigned at runtime. We
    assign the values of the placeholder using the `feed_dict` parameter. The `feed_dict`
    parameter is basically the dictionary where the key represents the name of the
    placeholder, and the value represents the value of the placeholder.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行上述代码，则会返回错误，因为我们试图计算`y`，其中`y = x + 3`，而`x`是一个占位符，其值尚未分配。正如我们所学的，占位符的值将在运行时分配。我们使用`feed_dict`参数来分配占位符的值。`feed_dict`参数基本上是一个字典，其中键表示占位符的名称，值表示占位符的值。
- en: 'As you can see in the following code, we set `feed_dict = {x:5}`, which implies
    that the value for the `x` placeholder is `5`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在下面的代码中所见，我们设置`feed_dict = {x:5}`，这意味着`x`占位符的值为`5`：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding code returns `8.0`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回`8.0`。
- en: 'What if we want to use multiple values for `x`? As we have not defined any
    shapes for the placeholders, it takes any number of values, as shown in the following
    code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想为`x`使用多个值会怎样？由于我们没有为占位符定义任何形状，它可以接受任意数量的值，如下面的代码所示：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It will return the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回以下内容：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s say we define the shape of `x` as `[None,2]`, as shown in the following
    code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将`x`的形状定义为`[None,2]`，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This means that `x` can take a matrix of any rows but with `2` columns, as
    shown in the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 `x` 可以取任意行但列数为 `2` 的矩阵，如以下代码所示：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code returns the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回以下内容：
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Introducing TensorBoard
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 TensorBoard
- en: TensorBoard is TensorFlow's visualization tool, which can be used to visualize
    a computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it becomes confusing when we have to debug the network. So,
    if we can visualize the computational graph in TensorBoard, we can easily understand
    such complex models, debug them, and optimize them. TensorBoard also supports
    sharing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是 TensorFlow 的可视化工具，可用于显示计算图。它还可以用来绘制各种定量指标和几个中间计算的结果。当我们训练一个非常深的神经网络时，如果需要调试网络，会变得很困惑。因此，如果我们能在
    TensorBoard 中可视化计算图，就能轻松理解这些复杂模型，进行调试和优化。TensorBoard 还支持共享。
- en: 'As shown in the following screenshot, the TensorBoard panel consists of several
    tabs—SCALARS, IMAGES, AUDIO, GRAPHS, DISTRIBUTIONS, HISTOGRAMS, and EMBEDDINGS:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下截图所示，TensorBoard 面板由几个选项卡组成： SCALARS，IMAGES，AUDIO，GRAPHS，DISTRIBUTIONS，HISTOGRAMS
    和 EMBEDDINGS：
- en: '![](img/fdbe9a38-d2d4-464f-ba46-67bf44e48477.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdbe9a38-d2d4-464f-ba46-67bf44e48477.png)'
- en: The tabs are pretty self-explanatory. The SCALARS tab shows useful information
    about the scalar variables we use in our program. For example, it shows how the
    value of a scalar variable called loss changes over several iterations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 选项卡的含义很明显。 SCALARS 选项卡显示有关程序中使用的标量变量的有用信息。例如，它显示标量变量名为损失的值如何随着多次迭代而变化。
- en: The GRAPHS tab shows the computational graph. The DISTRIBUTIONS and HISTOGRAMS
    tabs show the distribution of a variable. For example, our model's weight distribution
    and histogram can be seen under these tabs. The **EMBEDDINGS** tab is used for
    visualizing high-dimensional vectors, such as word embeddings (we will learn about
    this in detail in [Chapter 7](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml), *Learning
    Text Representations*).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GRAPHS 选项卡显示计算图。 DISTRIBUTIONS 和 HISTOGRAMS 选项卡显示变量的分布。例如，我们模型的权重分布和直方图可以在这些选项卡下看到。
    EMBEDDINGS 选项卡用于可视化高维向量，如词嵌入（我们将在[第7章](d184e022-0b11-492a-8303-37a6021c4bf6.xhtml)，*学习文本表示*中详细学习）。
- en: 'Let''s build a basic computational graph and visualize it in TensorBoard. Let''s
    say we have four variables, shown as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个基本的计算图，并在 TensorBoard 中进行可视化。假设我们有四个变量，如下所示：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s multiply `x` and `y` and `a` and `b` and save them as `prod1` and `prod2`,
    as shown in the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `x` 和 `y` 以及 `a` 和 `b` 相乘，并将它们保存为 `prod1` 和 `prod2`，如以下代码所示：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Add `prod1` and `prod2` and store them in `sum`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `prod1` 和 `prod2` 相加并存储在 `sum` 中：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, we can visualize all of these operations in TensorBoard. In order to visualize
    in TensorBoard, we first need to save our event files. It can be done using `tf.summary.FileWriter()`.
    It takes two important parameters, `logdir` and `graph`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在 TensorBoard 中可视化所有这些操作。为了在 TensorBoard 中进行可视化，我们首先需要保存我们的事件文件。可以使用
    `tf.summary.FileWriter()` 来完成。它需要两个重要参数，`logdir` 和 `graph`。
- en: 'As the name suggests, `logdir` specifies the directory where we want to store
    the graph, and `graph` specifies which graph we want to store:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，`logdir` 指定我们希望存储图形的目录，`graph` 指定我们希望存储的图形：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding code, `graphs` is the directory where we are storing our event
    file, and `sess.graph` specifies the current graph in our TensorFlow session.
    So, we are storing the current graph in the TensorFlow session in the `graphs`
    directory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`graphs` 是我们存储事件文件的目录，`sess.graph` 指定了我们 TensorFlow 会话中的当前图。因此，我们正在将
    TensorFlow 会话中的当前图存储在 `graphs` 目录中。
- en: 'To start TensorBoard, go to your Terminal, locate the working directory, and
    type the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 TensorBoard，请转到您的终端，找到工作目录，并键入以下内容：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `logdir` parameter indicates the directory where the event file is stored
    and `port` is the port number. Once you run the preceding command, open your browser
    and type `http://localhost:8000/`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`logdir` 参数指示事件文件存储的目录，`port` 是端口号。运行上述命令后，打开浏览器并输入 `http://localhost:8000/`。'
- en: 'In the TensorBoard panel, under the GRAPHS tab, you can see the computational
    graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorBoard 面板中，GRAPHS 选项卡下，您可以看到计算图：
- en: '![](img/b63f2545-f298-46d0-a6fd-9cc136644f54.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b63f2545-f298-46d0-a6fd-9cc136644f54.png)'
- en: As you may notice, all of the operations we have defined are clearly shown in
    the graph.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能注意到的，我们定义的所有操作都清楚地显示在图中。
- en: Creating a name scope
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个命名作用域
- en: Scoping is used to reduce complexity and helps us to better understand a model
    by grouping related nodes together. Having a name scope helps us to group similar
    operations in a graph. It comes in handy when we are building a complex architecture.
    Scoping can be created using `tf.name_scope()`. In the previous example, we performed
    two operations, `Product` and `sum`. We can simply group them into two different
    name scopes as `Product` and `sum`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作用域用于降低复杂性，并帮助我们通过将相关节点分组在一起来更好地理解模型。使用命名作用域有助于在图中分组相似的操作。当我们构建复杂的架构时，它非常方便。可以使用
    `tf.name_scope()` 创建作用域。在前面的示例中，我们执行了两个操作，`Product` 和 `sum`。我们可以简单地将它们分组为两个不同的命名作用域，如
    `Product` 和 `sum`。
- en: 'In the previous section, we saw how `prod1` and `prod2` perform multiplication
    and compute the result. We''ll define a name scope called `Product`, and group
    the `prod1` and `prod2` operations, as shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们看到了 `prod1` 和 `prod2` 如何执行乘法并计算结果。我们将定义一个名为 `Product` 的命名作用域，并将 `prod1`
    和 `prod2` 操作分组，如以下代码所示：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, define the name scope for `sum`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为 `sum` 定义命名作用域：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Store the file in the `graphs` directory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件存储在 `graphs` 目录中：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Visualize the graph in TensorBoard:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard中可视化图形：
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you may notice, now, we have only two nodes, **sum** and **Product**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，现在我们只有两个节点，**sum** 和 **Product**：
- en: '![](img/fa3246f8-34bc-4496-a021-d8e191e5a085.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa3246f8-34bc-4496-a021-d8e191e5a085.png)'
- en: 'Once we double-click on the nodes, we can see how the computation is happening.
    As you can see, the **prod1** and **prod2** nodes are grouped under the **Product**
    scope, and their results are sent to the **sum** node, where they will be added.
    You can see how the **prod1** and **prod2** nodes compute their value:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们双击节点，我们就能看到计算是如何进行的。正如你所见，**prod1** 和 **prod2** 节点被分组在 **Product** 作用域下，它们的结果被发送到
    **sum** 节点，在那里它们将被相加。你可以看到 **prod1** 和 **prod2** 节点如何计算它们的值：
- en: '![](img/d11d7a2b-389d-497b-bc80-9bd9c9e9bdb4.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d11d7a2b-389d-497b-bc80-9bd9c9e9bdb4.png)'
- en: The preceding example is just a simple example. When we are working on a complex
    project with a lot of operations, name scoping helps us to group similar operations
    together and enables us to understand the computational graph better.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子只是一个简单的示例。当我们在处理包含大量操作的复杂项目时，命名作用域帮助我们将相似的操作分组在一起，并且能够更好地理解计算图。
- en: Handwritten digit classification using TensorFlow
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行手写数字分类
- en: Putting together all the concepts we have learned so far, we will see how we
    can use TensorFlow to build a neural network to recognize handwritten digits.
    If you have been playing around with deep learning of late, then you must have
    come across the MNIST dataset. It has been called the *hello world* of deep learning.
    It consists of 55,000 data points of handwritten digits (0 to 9).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们迄今学到的所有概念整合在一起，我们将看到如何使用TensorFlow构建一个神经网络来识别手写数字。如果你最近一直在玩深度学习，那么你一定听说过MNIST数据集。它被称为深度学习的*hello
    world*。它包含了55000个手写数字数据点（0到9）。
- en: In this section, we will see how we can use our neural network to recognize
    these handwritten digits, and we will get the hang of TensorFlow and TensorBoard.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将看到如何使用我们的神经网络来识别这些手写数字，并且我们会掌握TensorFlow和TensorBoard。
- en: Importing the required libraries
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所需的库
- en: 'As a first step, let''s import all of the required libraries:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们导入所有所需的库：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Loading the dataset
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'Load the dataset, using the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集，使用以下代码：
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code, `data/mnist` implies the location where we store the
    MNIST dataset, and `one_hot=True` implies that we are one-hot encoding the labels
    (0 to 9).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`data/mnist` 表示我们存储MNIST数据集的位置，而 `one_hot=True` 表示我们正在对标签（0到9）进行one-hot编码。
- en: 'We will see what we have in our data by executing the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下代码，我们将看到我们的数据中包含什么：
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have `55000` images in the training set, each image is of size `784`, and
    we have `10` labels, which are actually 0 to 9\. Similarly, we have `10000` images
    in the test set.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集中有 `55000` 张图像，每个图像的大小是 `784`，我们有 `10` 个标签，实际上是从0到9。类似地，我们在测试集中有 `10000`
    张图像。
- en: 'Now, we''ll plot an input image to see what it looks like:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将绘制一个输入图像，看看它的样子：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Thus, our input image looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的输入图像看起来如下：
- en: '![](img/52193d70-1696-4ae6-8bfa-ed05915c159c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52193d70-1696-4ae6-8bfa-ed05915c159c.png)'
- en: Defining the number of neurons in each layer
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义每个层中神经元的数量
- en: 'We''ll build a four-layer neural network with three hidden layers and one output
    layer. As the size of the input image is `784`, we set `num_input` to `784`, and
    since we have 10 handwritten digits (0 to 9), we set `10` neurons in the output
    layer. We define the number of neurons in each layer as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个具有三个隐藏层和一个输出层的四层神经网络。由于输入图像的大小为 `784`，我们将 `num_input` 设置为 `784`，并且由于有
    10 个手写数字（0 到 9），我们在输出层设置了 `10` 个神经元。我们如下定义每一层的神经元数量：
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Defining placeholders
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义占位符
- en: 'As we have learned, we first need to define the placeholders for `input` and
    `output`. Values for the placeholders will be fed in at runtime through `feed_dict`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学，我们首先需要为 `input` 和 `output` 定义占位符。占位符的值将通过 `feed_dict` 在运行时传入：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Since we have a four-layer network, we have four weights and four biases. We
    initialize our weights by drawing values from the truncated normal distribution
    with a standard deviation of `0.1`. Remember, the dimensions of the weights matrix
    should be a *numb**er of neurons in the previous layer* x *a number of neurons
    in the current layer*. For instance, the dimension of weight matrix `w3` should
    be the *number of neurons in hidden layer 2* x *the number of neurons in hidden
    layer 3*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个四层网络，我们有四个权重和四个偏置。我们通过从标准偏差为 `0.1` 的截断正态分布中抽取值来初始化我们的权重。记住，权重矩阵的维度应该是*前一层神经元的数量*
    x *当前层神经元的数量*。例如，权重矩阵 `w3` 的维度应该是*隐藏层2中的神经元数* x *隐藏层3中的神经元数*。
- en: 'We often define all of the weights in a dictionary, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将所有权重定义在一个字典中，如下所示：
- en: '[PRE39]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The shape of the bias should be the number of neurons in the current layer.
    For instance, the dimension of the `b2` bias is the number of neurons in hidden
    layer 2\. We set the bias value as a constant; `0.1` in all of the layers:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置的形状应该是当前层中的神经元数。例如，`b2` 偏置的维度是隐藏层2中的神经元数。我们在所有层中将偏置值设置为常数 `0.1`：
- en: '[PRE40]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Forward propagation
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'Now we''ll define the forward propagation operation. We''ll use ReLU activations
    in all layers. In the last layers, we''ll apply `sigmoid` activation, as shown
    in the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义前向传播操作。我们在所有层中使用 ReLU 激活函数。在最后一层中，我们将应用 `sigmoid` 激活函数，如下所示的代码：
- en: '[PRE41]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Computing loss and backpropagation
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失和反向传播
- en: 'Next, we''ll define our loss function. We''ll use softmax cross-entropy as
    our loss function. TensorFlow provides the `tf.nn.softmax_cross_entropy_with_logits()`
    function for computing softmax cross-entropy loss. It takes two parameters as
    inputs, `logits` and `labels`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的损失函数。我们将使用 softmax 交叉熵作为我们的损失函数。TensorFlow 提供了 `tf.nn.softmax_cross_entropy_with_logits()`
    函数来计算 softmax 交叉熵损失。它接受两个参数作为输入，`logits` 和 `labels`：
- en: The `logits` parameter specifies the `logits` predicted by our network; for
    example, `y_hat`
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` 参数指定了网络预测的 `logits`；例如，`y_hat`'
- en: The `labels` parameter specifies the actual labels; for example, true labels,
    `Y`
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` 参数指定了实际的标签；例如，真实标签 `Y`'
- en: 'We take the mean of the `loss` function using `tf.reduce_mean()`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.reduce_mean()` 取 `loss` 函数的均值：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we need to minimize the loss using backpropagation. Don''t worry! We don''t
    have to calculate the derivatives of all the weights manually. Instead, we can
    use TensorFlow''s optimizer. In this section, we the use the Adam optimizer. It
    is a variant of the gradient descent optimization technique we learned about in
    [Chapter 1](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml), *Introduction to Deep
    Learning*. In [Chapter 3](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml), *Gradient
    Descent and Its Variants*, we will dive into the details and see how exactly the
    Adam optimizer and several other optimizers work. For now, let''s say we use the
    Adam optimizer as our backpropagation algorithm:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要使用反向传播来最小化损失。别担心！我们不必手动计算所有权重的导数。相反，我们可以使用 TensorFlow 的优化器。在本节中，我们使用
    Adam 优化器。它是我们在《[第一章](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml)》*深度学习导论*中学到的梯度下降优化技术的一个变体。在《[第三章](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml)》*梯度下降及其变体*中，我们将深入探讨细节，看看
    Adam 优化器和其他几种优化器的工作原理。现在，让我们说我们使用 Adam 优化器作为我们的反向传播算法：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Computing accuracy
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算精度
- en: 'We calculate the accuracy of our model as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算模型的精度如下：
- en: The `y_hat` parameter denotes the predicted probability for each class of our
    model. Since we have `10` classes, we will have `10` probabilities. If the probability
    is high at position `7`, then it means that our network predicts the input image
    as digit `7` with high probability. The `tf.argmax()` function returns the index
    of the largest value. Thus, `tf.argmax(y_hat,1)` gives the index where the probability
    is high. Thus, if the probability is high at index `7`, then it returns `7`.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数 `y_hat` 表示我们模型每个类别的预测概率。由于我们有 `10` 类别，我们将有 `10` 个概率。如果在位置 `7` 处的概率很高，则表示我们的网络以高概率预测输入图像为数字
    `7`。函数 `tf.argmax()` 返回最大值的索引。因此，`tf.argmax(y_hat,1)` 给出概率高的索引位置。因此，如果索引 `7` 处的概率很高，则返回
    `7`。
- en: The `Y` parameter denotes the actual labels, and they are the one-hot encoded
    values. That is, it consists of zeros everywhere except at the position of the
    actual image, where it consists of `1`. For instance, if the input image is `7`,
    then `Y` has 0 at all indices except at index `7`, where it has `1`. Thus, `tf.argmax(Y,1)`
    returns `7` because that is where we have a high value, `1`.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数 `Y` 表示实际标签，它们是独热编码的值。也就是说，除了实际图像的位置处为 `1` 外，在所有位置处都是 `0`。例如，如果输入图像是 `7`，则
    `Y` 在索引 `7` 处为 `1`，其他位置为 `0`。因此，`tf.argmax(Y,1)` 返回 `7`，因为这是我们有高值 `1` 的位置。
- en: Thus, `tf.argmax(y_hat,1)` gives the predicted digit, and `tf.argmax(Y,1)` gives
    us the actual digit.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`tf.argmax(y_hat,1)` 给出了预测的数字，而 `tf.argmax(Y,1)` 给出了实际的数字。
- en: The `tf.equal(x, y)` function takes `x` and `y` as inputs and returns the truth
    value of *(x == y)* element-wise. Thus, `correct_pred = tf.equal(predicted_digit,actual_digit)`
    consists of `True` where the actual and predicted digits are the same, and `False`
    where the actual and predicted digits are not the same. We convert the Boolean
    values in `correct_pred` into float values using TensorFlow's cast operation,
    `tf.cast(correct_pred, tf.float32)`. After converting them into float values,
    we take the average using `tf.reduce_mean()`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `tf.equal(x, y)` 接受 `x` 和 `y` 作为输入，并返回 *(x == y)* 的逻辑值。因此，`correct_pred =
    tf.equal(predicted_digit,actual_digit)` 包含了当实际和预测的数字相同时为 `True`，不同时为 `False`。我们使用
    TensorFlow 的 `cast` 操作将 `correct_pred` 中的布尔值转换为浮点值，即 `tf.cast(correct_pred, tf.float32)`。将它们转换为浮点值后，我们使用
    `tf.reduce_mean()` 取平均值。
- en: 'Thus, `tf.reduce_mean(tf.cast(correct_pred, tf.float32))` gives us the average
    correct predictions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`tf.reduce_mean(tf.cast(correct_pred, tf.float32))` 给出了平均正确预测值：
- en: '[PRE44]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Creating summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建摘要
- en: 'We can also visualize how the loss and accuracy of our model changes during
    several iterations in TensorBoard. So, we use `tf.summary()` to get the summary
    of the variable. Since the loss and accuracy are scalar variables, we use `tf.summary.scalar()`,
    as shown in the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化模型在多次迭代过程中损失和准确率的变化。因此，我们使用 `tf.summary()` 来获取变量的摘要。由于损失和准确率是标量变量，我们使用
    `tf.summary.scalar()`，如下所示：
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we merge all of the summaries we use in our graph, using `tf.summary.merge_all()`.
    We do this because when we have many summaries, running and storing them would
    become inefficient, so we run them once in our session instead of running multiple
    times:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们合并图中使用的所有摘要，使用 `tf.summary.merge_all()`。我们这样做是因为当我们有许多摘要时，运行和存储它们会变得低效，所以我们在会话中一次性运行它们，而不是多次运行：
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Training the model
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now, it is time to train our model. As we have learned, first, we need to initialize
    all of the variables:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候训练我们的模型了。正如我们所学的，首先，我们需要初始化所有变量：
- en: '[PRE47]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define the batch size, number of iterations, and learning rate, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批大小、迭代次数和学习率，如下所示：
- en: '[PRE48]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Start the TensorFlow session:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话：
- en: '[PRE49]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Initialize all the variables:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE50]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Save the event files:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 保存事件文件：
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Train the model for a number of iterations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型一定数量的迭代次数：
- en: '[PRE52]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Get a batch of data according to the batch size:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 根据批大小获取一批数据：
- en: '[PRE53]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Train the network:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE54]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print `loss` and `accuracy` for every 100^(th) iteration:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每100次迭代打印一次 `loss` 和 `accuracy`：
- en: '[PRE55]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As you may notice from the following output, the loss decreases and the accuracy
    increases over various training iterations:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从以下输出中注意到的那样，损失减少，准确率在各种训练迭代中增加：
- en: '[PRE56]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Visualizing graphs in TensorBoard
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorBoard中可视化图表
- en: 'After training, we can visualize our computational graph in TensorBoard, as
    shown in the following diagram. As you can see, our **Model** takes input, weights,
    and biases as input and returns the output. We compute Loss and Accuracy based
    on the output of the model. We minimize the loss by calculating gradients and
    updating weights. We can observe all of this in the following diagram:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以在 TensorBoard 中可视化我们的计算图，如下图所示。正如您所见，我们的**模型**接受输入、权重和偏差作为输入，并返回输出。我们根据模型的输出计算损失和准确性。通过计算梯度和更新权重来最小化损失。我们可以在下图中观察到所有这些：
- en: '![](img/d073d2d4-a592-4e2a-871a-c3f4956612dd.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d073d2d4-a592-4e2a-871a-c3f4956612dd.png)'
- en: 'If we double-click and expand the **Model**, we can see that we have three
    hidden layers and one output layer:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们双击并展开**模型**，我们可以看到我们有三个隐藏层和一个输出层：
- en: '![](img/df7d8d3c-94b9-40f4-a951-3edb199393fc.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df7d8d3c-94b9-40f4-a951-3edb199393fc.png)'
- en: 'Similarly, we can double-click and see every node. For instance, if we open
    **weights**, we can see how the four weights are initialized using truncated normal
    distribution, and how it is updated using the Adam optimizer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以双击并查看每个节点。例如，如果我们打开**权重**，我们可以看到四个权重如何使用截断正态分布进行初始化，并且如何使用 Adam 优化器进行更新：
- en: '![](img/662ac908-27ec-4570-88c8-0cad3354fb23.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/662ac908-27ec-4570-88c8-0cad3354fb23.png)'
- en: 'As we have learned, the computational graph helps us to understand what is
    happening on each node. We can see how the accuracy is being calculated by double-clicking
    on the **Accuracy** node:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学到的，计算图帮助我们理解每个节点发生的情况。我们可以通过双击**准确性**节点来查看如何计算准确性：
- en: '![](img/d48fcb84-8a10-4716-a8a3-69006100ce30.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d48fcb84-8a10-4716-a8a3-69006100ce30.png)'
- en: 'Remember that we also stored the summary of our `loss` and `accuracy` variables.
    We can find them under the SCALARS tab in TensorBoard, as shown in the following
    screenshot. We can see how that loss decreases over iterations, as shown in the
    following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们还存储了`loss`和`accuracy`变量的摘要。我们可以在 TensorBoard 的 SCALARS 选项卡下找到它们，如下面的截图所示。我们可以看到损失如何随迭代而减少，如下图所示：
- en: '![](img/9fa0158f-52d0-433f-9aa1-451efad738bc.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fa0158f-52d0-433f-9aa1-451efad738bc.png)'
- en: 'The following screenshot shows how accuracy increases over iterations:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示准确性随迭代次数增加的情况：
- en: '![](img/b81c6aa3-29d8-4988-a7ec-a916fe0727a7.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b81c6aa3-29d8-4988-a7ec-a916fe0727a7.png)'
- en: Introducing eager execution
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入急切执行
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operations, eager execution follows the imperative programming paradigm, where
    any operations can be performed immediately, without having to create a graph,
    just like we do in Python. Hence, with eager execution, we can say goodbye to
    sessions and placeholders. It also makes the debugging process easier with an
    immediate runtime error, unlike the graph mode.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的急切执行更符合 Python 风格，允许快速原型设计。与图模式不同，在图模式中，我们每次执行操作都需要构建一个图，而急切执行遵循命令式编程范式，可以立即执行任何操作，无需创建图，就像在
    Python 中一样。因此，使用急切执行，我们可以告别会话和占位符。与图模式不同，它还通过立即运行时错误使得调试过程更加简单。
- en: 'For instance, in the graph mode, to compute anything, we run the session. As
    shown in the following code, to evaluate the value of `z`, we have to run the
    TensorFlow session:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图模式中，要计算任何内容，我们需要运行会话。如下面的代码所示，要评估`z`的值，我们必须运行 TensorFlow 会话：
- en: '[PRE57]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With eager execution, we don''t need to create a session; we can simply compute
    `z`, just like we do in Python. In order to enable eager execution, just call
    the `tf.enable_eager_execution()` function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用急切执行，我们无需创建会话；我们可以像在 Python 中一样简单地计算`z`。为了启用急切执行，只需调用`tf.enable_eager_execution()`函数：
- en: '[PRE58]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'It will return the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回以下内容：
- en: '[PRE59]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In order to get the output value, we can print the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取输出值，我们可以打印以下内容：
- en: '[PRE60]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Math operations in TensorFlow
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的数学操作
- en: 'Now, we will explore some of the operations in TensorFlow using the eager execution
    mode:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用急切执行模式探索 TensorFlow 中的一些操作：
- en: '[PRE61]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Let's start with some basic arithmetic operations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些基本算术操作开始。
- en: 'Use `tf.add` to add two numbers:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.add`来添加两个数：
- en: '[PRE62]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The `tf.subtract` function is used for finding the difference between two numbers:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.subtract` 函数用于找出两个数之间的差异：'
- en: '[PRE63]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The `tf.multiply` function is used for multiplying two numbers:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.multiply` 函数用于两个数的乘法：'
- en: '[PRE64]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Divide two numbers using `tf.divide`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.divide`除两个数：
- en: '[PRE65]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The dot product can be computed as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 点积可以计算如下：
- en: '[PRE66]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, let''s find the index of the minimum and maximum elements:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，让我们找到最小和最大元素的索引：
- en: '[PRE67]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The index of the minimum value is computed using `tf.argmin()`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值的索引是使用`tf.argmin()`计算的：
- en: '[PRE68]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The index of the maximum value is computed using `tf.argmax()`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值的索引是使用`tf.argmax()`计算的：
- en: '[PRE69]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Run the following code to find the squared difference between `x` and `y`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码以找到`x`和`y`之间的平方差：
- en: '[PRE70]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Let's try typecasting; that is, converting from one data type into another.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试类型转换；即，从一种数据类型转换为另一种。
- en: 'Print the type of `x`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 打印`x`的类型：
- en: '[PRE71]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can convert the type of `x`, which is `tf.int32`, into `tf.float32` using
    `tf.cast`, as shown in the following code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tf.cast`将`x`的类型（`tf.int32`）转换为`tf.float32`，如下所示：
- en: '[PRE72]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, check the `x` type. It will be `tf.float32`, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查`x`的类型。它将是`tf.float32`，如下所示：
- en: '[PRE73]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Concatenate the two matrices:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 按列连接两个矩阵：
- en: '[PRE74]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Concatenate the matrices row-wise:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 按行连接矩阵：
- en: '[PRE75]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Use the following code to concatenate the matrices column-wise:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码按列连接矩阵：
- en: '[PRE76]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Stack the `x` matrix using the `stack` function:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`stack`函数堆叠`x`矩阵：
- en: '[PRE77]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, let'' see how to perform the `reduce_mean` operation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何执行`reduce_mean`操作：
- en: '[PRE78]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Compute the mean value of `x`; that is, *(1.0 + 5.0 + 2.0 + 3.0) / 4*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`x`的均值；即，*(1.0 + 5.0 + 2.0 + 3.0) / 4*：
- en: '[PRE79]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Compute the mean across the row; that is, *(1.0+5.0)/2, (2.0+3.0)/2*:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 计算行的均值；即，*(1.0+5.0)/2, (2.0+3.0)/2*：
- en: '[PRE80]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Compute the mean across the column; that is, *(1.0+5.0)/2.0, (2.0+3.0)/2.0*:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 计算列的均值；即，*(1.0+5.0)/2.0, (2.0+3.0)/2.0*：
- en: '[PRE81]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Draw random values from the probability distributions:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率分布中绘制随机值：
- en: '[PRE82]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Compute the softmax probabilities:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 计算softmax概率：
- en: '[PRE83]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Now, we'll look at how to compute the gradients.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何计算梯度。
- en: 'Define the `square` function:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`square`函数：
- en: '[PRE84]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The gradients can be computed for the preceding `square` function using `tf.GradientTape`,
    as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tf.GradientTape`计算前述`square`函数的梯度，如下所示：
- en: '[PRE85]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: More TensorFlow operations are available in the Notebook on GitHub at [http://bit.ly/2YSYbYu](http://bit.ly/2YSYbYu).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 更多TensorFlow操作可在GitHub上的Notebook中查看，网址为[http://bit.ly/2YSYbYu](http://bit.ly/2YSYbYu)。
- en: TensorFlow is a lot more than this. We will learn about various important functionalities
    of TensorFlow as we move on through this book.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow远不止如此。随着我们在本书中的学习，我们将了解TensorFlow的各种重要功能。
- en: TensorFlow 2.0 and Keras
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2.0和Keras
- en: TensorFlow 2.0 has got some really cool features. It sets the eager execution
    mode by default. It provides a simplified workflow and uses Keras as the main
    API for building deep learning models. It is also backward compatible with TensorFlow
    1.x versions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0具有一些非常酷的功能。它默认设置为即时执行模式。它提供了简化的工作流程，并使用Keras作为构建深度学习模型的主要API。它还与TensorFlow
    1.x版本向后兼容。
- en: 'To install TensorFlow 2.0, open your Terminal and type the following command:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装TensorFlow 2.0，请打开您的终端并输入以下命令：
- en: '[PRE86]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras
    works in the next section.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow 2.0使用Keras作为高级API，我们将在下一节中看看Keras的工作原理。
- en: Bonjour Keras
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bonjour Keras
- en: Keras is another popularly used deep learning library. It was developed by François
    Chollet at Google. It is well known for its fast prototyping, and it makes model
    building simple. It is a high-level library, meaning that it does not perform
    any low-level operations on its own, such as convolution. It uses a backend engine
    for doing that, such as TensorFlow. The Keras API is available in `tf.keras`,
    and TensorFlow 2.0 uses it as the primary API.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是另一个广泛使用的深度学习库。它由谷歌的François Chollet开发。它以快速原型设计而闻名，使模型构建简单。它是一个高级库，意味着它本身不执行任何低级操作，如卷积。它使用后端引擎来执行这些操作，比如TensorFlow。Keras
    API在`tf.keras`中可用，TensorFlow 2.0将其作为主要API。
- en: 'Building a model in Keras involves four important steps:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中构建模型涉及四个重要步骤：
- en: Defining the model
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型
- en: Compiling the model
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型
- en: Fitting the model
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Evaluating the model
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Defining the model
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'The first step is defining the model. Keras provides two different APIs to
    define the model:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义模型。Keras提供了两种不同的API来定义模型：
- en: The sequential API
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序API
- en: The functional API
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数式API
- en: Defining a sequential model
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个序列模型
- en: 'In a sequential model, we stack each layer, one above another:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列模型中，我们将每个层堆叠在一起：
- en: '[PRE87]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'First, let''s define our model as a `Sequential()` model, as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将我们的模型定义为`Sequential()`模型，如下所示：
- en: '[PRE88]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now, define the first layer, as shown in the following code:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，定义第一层，如下所示：
- en: '[PRE89]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: In the preceding code, `Dense` implies a fully connected layer, `input_dim`
    implies the dimension of our input, and `activation` specifies the activation
    function that we use. We can stack up as many layers as we want, one above another.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`Dense` 表示全连接层，`input_dim` 表示输入的维度，`activation` 指定我们使用的激活函数。我们可以堆叠任意多层，一层叠在另一层之上。
- en: 'Define the next layer with the `relu` activation, as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 定义带有 `relu` 激活的下一层，如下所示：
- en: '[PRE90]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Define the output layer with the `sigmoid` activations:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 定义带有 `sigmoid` 激活函数的输出层：
- en: '[PRE91]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The final code block of the sequential model is shown as follows. As you can
    see, the Keras code is much simpler than the TensorFlow code:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型的最终代码块如下所示。正如您所见，Keras 代码比 TensorFlow 代码简单得多：
- en: '[PRE92]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Defining a functional model
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义函数式模型
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack of one above another. A functional
    model comes in handy when creating complex models, such as directed acyclic graphs,
    models with multiple input values, multiple output values, and shared layers.
    Now, we will see how to define a functional model in Keras.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式模型比顺序模型更灵活。例如，在函数式模型中，我们可以轻松连接任意一层到另一层，而在顺序模型中，每一层都是堆叠在另一层之上的。当创建复杂模型时，如有向无环图、具有多个输入值、多个输出值和共享层的模型时，函数式模型非常实用。现在，我们将看看如何在
    Keras 中定义函数式模型。
- en: 'The first step is to define the input dimensions:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义输入维度：
- en: '[PRE93]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Now, we''ll define our first fully connected layer with `10` neurons and `relu`
    activations, using the `Dense` class, as shown:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在测试集上评估我们的模型，首先定义第一个具有 `10` 个神经元和 `relu` 激活函数的全连接层，使用 `Dense` 类，如下所示：
- en: '[PRE94]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'We defined `layer1`, but where is the input to `layer1` coming from? We need
    to specify the input to `layer1` in a bracket notation at the end, as shown:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了 `layer1`，但是 `layer1` 的输入从哪里来？我们需要在末尾的括号符号中指定 `layer1` 的输入，如下所示：
- en: '[PRE95]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'We define the next layer, `layer2`, with `13` neurons and `relu` activation.
    The input to `layer2` comes from `layer1`, so that is added in the bracket at
    the end, as shown in the following code:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `13` 个神经元和 `relu` 激活函数定义下一层 `layer2`。`layer2` 的输入来自 `layer1`，因此在代码末尾加上括号，如下所示：
- en: '[PRE96]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Now, we can define the output layer with the `sigmoid` activation function.
    Input to the output layer comes from `layer2`, so that is added in parentheses
    at the end:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义具有 `sigmoid` 激活函数的输出层。输出层的输入来自 `layer2`，因此在括号中添加了这一部分：
- en: '[PRE97]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'After defining all of the layers, we define the model using a `Model` class,
    where we need to specify `inputs` and `outputs`, as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义完所有层之后，我们使用 `Model` 类定义模型，需要指定 `inputs` 和 `outputs`，如下所示：
- en: '[PRE98]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The complete code for the functional model is shown here:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式模型的完整代码如下所示：
- en: '[PRE99]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Compiling the model
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'Now that we have defined the model, the next step is to compile it. In this
    phase, we set up how the model should learn. We define three parameters when compiling
    the model:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，下一步是编译它。在这个阶段，我们设置模型学习的方式。在编译模型时，我们定义了三个参数：
- en: 'The `optimizer` parameter: This defines the optimization algorithm we want
    to use; for example, the gradient descent, in this case.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` 参数：这定义了我们想要使用的优化算法，例如在这种情况下的梯度下降。'
- en: 'The `loss` parameter: This is the objective function that we are trying to
    minimize; for example, the mean squared error or cross-entropy loss.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` 参数：这是我们试图最小化的目标函数，例如均方误差或交叉熵损失。'
- en: 'The `metrics` parameter: This is the metric through which we want to assess
    the model''s performance; for example, `accuracy`. We can also specify more than
    one metric.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` 参数：这是我们想通过哪种度量来评估模型性能，例如 `accuracy`。我们也可以指定多个度量。'
- en: 'Run the following code to compile the model:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码来编译模型：
- en: '[PRE100]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Training the model
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We defined and also compiled the model. Now, we will train the model. Training
    the model can be done using the `fit` function. We specify our features, `x`;
    labels, `y`; the number of `epochs` we want to train; and the `batch_size`, as
    follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义并编译了模型。现在，我们将训练模型。使用 `fit` 函数可以完成模型的训练。我们指定特征 `x`、标签 `y`、训练的 `epochs`
    数量和 `batch_size`，如下所示：
- en: '[PRE101]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Evaluating the model
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'After training the model, we will evaluate the model on the test set:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完模型后，我们将在测试集上评估模型：
- en: '[PRE102]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We can also evaluate the model on the same train set, and that will help us
    to understand the training accuracy:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在相同的训练集上评估模型，这将帮助我们了解训练准确性：
- en: '[PRE103]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: MNIST digit classification using TensorFlow 2.0
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2.0进行MNIST数字分类
- en: Now, we will see how we can perform MNIST handwritten digit classification,
    using TensorFlow 2.0\. It requires only a few lines of code compared to TensorFlow
    1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just
    need to add `tf.keras` to the Keras code.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何使用TensorFlow 2.0执行MNIST手写数字分类。与TensorFlow 1.x相比，它只需要少量代码。正如我们所学的，TensorFlow
    2.0使用Keras作为其高级API；我们只需在Keras代码中添加`tf.keras`。
- en: 'Let''s start by loading the dataset:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始：
- en: '[PRE104]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Create a train and test set with the following code:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建训练集和测试集：
- en: '[PRE105]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Normalize the train and test sets by dividing the values of `x` by the maximum
    value of `x`; that is, `255.0`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练集和测试集标准化，通过将`x`的值除以最大值`255.0`来完成：
- en: '[PRE106]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Define the sequential model as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如下定义序列模型：
- en: '[PRE107]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Now, let''s add layers to the model. We use a three-layer network with the
    `relu` function and `softmax` in the final layer:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为模型添加层次。我们使用一个三层网络，其中最后一层采用`relu`函数和`softmax`：
- en: '[PRE108]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Compile the model by running the following line of code:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下代码行来编译模型：
- en: '[PRE109]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Train the model:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE110]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Evaluate the model:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型：
- en: '[PRE111]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: That's it! Writing code with the Keras API is that simple.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！使用Keras API编写代码就是这么简单。
- en: Should we use Keras or TensorFlow?
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该使用Keras还是TensorFlow？
- en: We have learned that TensorFlow 2.0 uses Keras as a high-level API. Using a
    high-level API enables rapid prototyping. But we can't use high-level APIs when
    we want to build a model on a low level, or if we want to build something that
    a high-level API doesn't provide.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到TensorFlow 2.0使用Keras作为其高级API。使用高级API可以进行快速原型设计。但是当我们想要在低级别上构建模型时，或者当高级API无法提供所需功能时，我们就不能使用高级API。
- en: In addition to this, writing code from scratch strengthens our knowledge of
    algorithms and helps us to understand and learn concepts better than directly
    diving into high-level APIs. That's why, in this book, we will code most of the
    algorithms from scratch using TensorFlow, without using high-level APIs such as
    Keras. We will be using TensorFlow version 1.13.1.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，从头开始编写代码可以加深我们对算法的理解，并帮助我们更好地理解和学习概念，远胜于直接使用高级API。这就是为什么在这本书中，我们将使用TensorFlow编写大部分算法，而不使用Keras等高级API。我们将使用TensorFlow版本1.13.1。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off this chapter by learning about TensorFlow and how it uses computational
    graphs. We learned that every computation in TensorFlow is represented by a computational
    graph, which consists of several nodes and edges, where nodes are mathematical
    operations, such as addition and multiplication, and edges are tensors.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本章开始学习TensorFlow及其如何使用计算图。我们了解到，在TensorFlow中，每个计算都表示为一个计算图，该图由多个节点和边组成，其中节点是数学运算，如加法和乘法，边是张量。
- en: We learned that variables are containers used to store values and they are used
    as input to several other operations in a computational graph. Later, we learned
    that placeholders are like variables, where we only define the type and dimension
    but will not assign the values, and values for the placeholders will be fed at
    runtime.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了变量是用来存储值的容器，并且它们作为计算图中多个其他操作的输入。稍后，我们了解到占位符类似于变量，其中我们只定义类型和维度，但不会分配值，占位符的值将在运行时提供。
- en: Going forward, we learned about TensorBoard, which is TensorFlow's visualization
    tool and can be used to visualize a computational graph. It can also be used to
    plot various quantitative metrics and the results of several intermediate calculations.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们学习了TensorBoard，这是TensorFlow的可视化工具，可用于可视化计算图。它还可以用来绘制各种定量指标和多个中间计算结果的结果。
- en: We also learned about eager execution, which is more Pythonic, and allows for
    rapid prototyping. We understood that, unlike the graph mode, where we need to
    construct a graph every time to perform any operations, eager execution follows
    the imperative programming paradigm, where any operations can be performed immediately,
    without having to create a graph, just like we do in Python.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了急切执行，它更符合Python风格，允许快速原型设计。我们了解到，与图模式不同，我们无需每次执行操作时都构建一个图来执行任何操作，急切执行遵循命令式编程范式，可以立即执行任何操作，就像我们在Python中所做的那样，而无需创建图形。
- en: In the next chapter, we will learn about gradient descent and the variants of
    gradient descent algorithms.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习梯度下降及其变种算法。
- en: Questions
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Assess your knowledge about TensorFlow by answering the following questions:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题来评估你对TensorFlow的了解：
- en: Define a computational graph.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个计算图。
- en: What are sessions?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 会话是什么？
- en: How do we create a session in TensorFlow?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何在TensorFlow中创建一个会话？
- en: What is the difference between variables and placeholders?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量和占位符之间有什么区别？
- en: Why do we need TensorBoard?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么需要TensorBoard？
- en: What is the name scope and how is it created?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 名称作用域是什么，它是如何创建的？
- en: What is eager execution?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是即时执行？
- en: Further reading
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: You can learn more about TensorFlow by checking out the official documentation
    at [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看官方文档[https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)来了解更多关于TensorFlow的信息。
