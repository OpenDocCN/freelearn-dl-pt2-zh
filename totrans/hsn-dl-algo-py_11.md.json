["```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntf.reset_default_graph()\n```", "```py\ndata = input_data.read_data_sets(\"data/mnist\",one_hot=True)\n```", "```py\nplt.imshow(data.train.images[13].reshape(28,28),cmap=\"gray\")\n```", "```py\ndef generator(z,reuse=None):\n\n    with tf.variable_scope('generator',reuse=reuse):\n\n        hidden1 = tf.layers.dense(inputs=z,units=128,activation=tf.nn.leaky_relu)\n        hidden2 = tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n        output = tf.layers.dense(inputs=hidden2,units=784,activation=tf.nn.tanh)\n\n        return output\n```", "```py\ndef discriminator(X,reuse=None):\n\n    with tf.variable_scope('discriminator',reuse=reuse):\n\n        hidden1 = tf.layers.dense(inputs=X,units=128,activation=tf.nn.leaky_relu)\n        hidden2 = tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n        logits = tf.layers.dense(inputs=hidden2,units=1)\n        output = tf.sigmoid(logits)\n\n        return logits \n```", "```py\nx = tf.placeholder(tf.float32,shape=[None,784])\nz = tf.placeholder(tf.float32,shape=[None,100])\n```", "```py\nfake_x = generator(z)\n```", "```py\nD_logits_real = discriminator(x)\n```", "```py\nD_logits_fake = discriminator(fake_x,reuse=True)\n```", "```py\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real,\n labels=tf.ones_like(D_logits_real)))\n```", "```py\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,\n labels=tf.zeros_like(D_logits_fake)))\n```", "```py\nD_loss = D_loss_real + D_loss_fake\n```", "```py\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, labels=tf.ones_like(D_logits_fake)))\n```", "```py\ntraining_vars = tf.trainable_variables()\ntheta_D = [var for var in training_vars if 'dis' in var.name]\ntheta_G = [var for var in training_vars if 'gen' in var.name]\n```", "```py\nlearning_rate = 0.001\n\nD_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss,var_list = theta_D)\nG_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list = theta_G)\n```", "```py\nbatch_size = 100\nnum_epochs = 1000\n```", "```py\ninit = tf.global_variables_initializer()\n```", "```py\nwith tf.Session() as session:\n```", "```py\n    session.run(init)\n```", "```py\n    for epoch in range(num_epochs):\n```", "```py\n        num_batches = data.train.num_examples // batch_size\n```", "```py\n        for i in range(num_batches):\n```", "```py\n            batch = data.train.next_batch(batch_size)\n```", "```py\n            batch_images = batch[0].reshape((batch_size,784))\n            batch_images = batch_images * 2 - 1\n```", "```py\n\n            batch_noise = np.random.uniform(-1,1,size=(batch_size,100))\n```", "```py\n            feed_dict = {x: batch_images, z : batch_noise}\n```", "```py\n            _ = session.run(D_optimizer,feed_dict = feed_dict)\n            _ = session.run(G_optimizer,feed_dict = feed_dict)\n```", "```py\n            discriminator_loss = D_loss.eval(feed_dict)\n            generator_loss = G_loss.eval(feed_dict)\n\n```", "```py\n        if epoch%100==0:\n            print(\"Epoch: {}, iteration: {}, Discriminator Loss:{}, Generator Loss: {}\".format(epoch,i,discriminator_loss,generator_loss))\n\n            _fake_x = fake_x.eval(feed_dict)\n\n            plt.imshow(_fake_x[0].reshape(28,28))\n            plt.show()\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nfrom keras.datasets import cifar10\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display\n\nfrom scipy.misc import toimage\n```", "```py\n(x_train, y_train), _ = cifar10.load_data()\n\nx_train = x_train.astype('float32')/255.0\n```", "```py\ndef plot_images(X):\n    plt.figure(1)\n    z = 0\n    for i in range(0,4):\n        for j in range(0,4):\n            plt.subplot2grid((4,4),(i,j))\n            plt.imshow(toimage(X[z]))\n            z = z + 1\n\n    plt.show()\n```", "```py\nplot_images(x_train[:17])\n```", "```py\ndef discriminator(input_images, reuse=False, is_training=False, alpha=0.1):\n\n    with tf.variable_scope('discriminator', reuse= reuse):\n```", "```py\n        layer1 = tf.layers.conv2d(input_images, \n                                  filters=64, \n                                  kernel_size=5, \n                                  strides=2, \n                                  padding='same', \n                                  kernel_initializer=kernel_init, \n                                  name='conv1')\n\n        layer1 = tf.nn.leaky_relu(layer1, alpha=0.2, name='leaky_relu1')\n```", "```py\n        layer2 = tf.layers.conv2d(layer1, \n                                  filters=128, \n                                  kernel_size=5, \n                                  strides=2, \n                                  padding='same', \n                                  kernel_initializer=kernel_init, \n                                  name='conv2')\n        layer2 = tf.layers.batch_normalization(layer2, training=is_training, name='batch_normalization2')\n\n        layer2 = tf.nn.leaky_relu(layer2, alpha=0.2, name='leaky_relu2')\n```", "```py\n        layer3 = tf.layers.conv2d(layer2, \n                                 filters=256, \n                                 kernel_size=5, \n                                 strides=1,\n                                 padding='same',\n                                 name='conv3')\n        layer3 = tf.layers.batch_normalization(layer3, training=is_training, name='batch_normalization3')\n        layer3 = tf.nn.leaky_relu(layer3, alpha=0.1, name='leaky_relu3')\n```", "```py\n        layer3 = tf.reshape(layer3, (-1, layer3.shape[1]*layer3.shape[2]*layer3.shape[3]))\n\n```", "```py\n        logits = tf.layers.dense(layer3, 1)\n\n        output = tf.sigmoid(logits)\n\n        return logits  \n```", "```py\ndef generator(z, z_dim, batch_size, is_training=False, reuse=False):\n    with tf.variable_scope('generator', reuse=reuse):\n```", "```py\n        input_to_conv = tf.layers.dense(z, 8*8*128)\n```", "```py\n        layer1 = tf.reshape(input_to_conv, (-1, 8, 8, 128))\n        layer1 = tf.layers.batch_normalization(layer1, training=is_training, name='batch_normalization1')\n        layer1 = tf.nn.relu(layer1, name='relu1')\n```", "```py\n        layer2 = tf.layers.conv2d_transpose(layer1, filters=256, kernel_size=5, strides= 2, padding='same', \n                                            kernel_initializer=kernel_init, name='deconvolution2')\n        layer2 = tf.layers.batch_normalization(layer2, training=is_training, name='batch_normalization2')\n        layer2 = tf.nn.relu(layer2, name='relu2')\n\n```", "```py\n        layer3 = tf.layers.conv2d_transpose(layer2, filters=256, kernel_size=5, strides= 2, padding='same', \n                                            kernel_initializer=kernel_init, name='deconvolution3')\n        layer3 = tf.layers.batch_normalization(layer3,training=is_training, name='batch_normalization3')\n        layer3 = tf.nn.relu(layer3, name='relu3')\n```", "```py\n        layer4 = tf.layers.conv2d_transpose(layer3, filters=256, kernel_size=5, strides= 1, padding='same', \n                                            kernel_initializer=kernel_init, name='deconvolution4')\n        layer4 = tf.layers.batch_normalization(layer4,training=is_training, name='batch_normalization4')\n        layer4 = tf.nn.relu(layer4, name='relu4')\n```", "```py\n        layer5 = tf.layers.conv2d_transpose(layer4, filters=3, kernel_size=7, strides=1, padding='same', \n                                            kernel_initializer=kernel_init, name='deconvolution5')\n\n        logits = tf.tanh(layer5, name='tanh')\n\n        return logits\n```", "```py\nimage_width = x_train.shape[1]\nimage_height = x_train.shape[2]\nimage_channels = x_train.shape[3]\n\nx = tf.placeholder(tf.float32, shape= (None, image_width, image_height, image_channels), name=\"d_input\")\n```", "```py\nlearning_rate = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\nis_training = tf.placeholder(tf.bool, [], name='is_training')\n```", "```py\nbatch_size = 100\nz_dim = 100\n```", "```py\nz = tf.random_normal([batch_size, z_dim], mean=0.0, stddev=1.0, name='z')\n```", "```py\nfake_x = generator(z, z_dim, batch_size, is_training=is_training)\n```", "```py\nD_logit_real = discriminator(x, reuse=False, is_training=is_training)\n```", "```py\nD_logit_fake = discriminator(fake_x, reuse=True,  is_training=is_training)\n```", "```py\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real,\n labels=tf.ones_like(D_logits_real)))\n\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,\n labels=tf.zeros_like(D_logits_fake)))\n\nD_loss = D_loss_real + D_loss_fake\n```", "```py\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, labels=tf.ones_like(D_logits_fake)))\n```", "```py\ntraining_vars = tf.trainable_variables()\ntheta_D = [var for var in training_vars if 'dis' in var.name]\ntheta_G = [var for var in training_vars if 'gen' in var.name]\n```", "```py\nd_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=theta_D)\ng_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=theta_G)\n```", "```py\nnum_batches = int(x_train.shape[0] / batch_size)\nsteps = 0\nnum_epcohs = 500\nlr = 0.00002\n```", "```py\ndef generate_new_samples(session, n_images, z_dim):\n\n    z = tf.random_normal([1, z_dim], mean=0.0, stddev=1.0)\n\n    is_training = tf.placeholder(tf.bool, [], name='training_bool') \n\n    samples = session.run(generator(z, z_dim, batch_size, is_training, reuse=True),feed_dict={is_training: True})\n\n    img = (samples[0] * 255).astype(np.uint8)\n    plt.imshow(img)\n    plt.show()\n```", "```py\nwith tf.Session() as session:\n```", "```py\n    session.run(tf.global_variables_initializer())\n```", "```py\n    for epoch in range(num_epcohs):\n\n        #for each batch\n        for i in range(num_batches):\n```", "```py\n            start = i * batch_size\n            end = (i + 1) * batch_size\n```", "```py\n            batch_images = x_train[start:end]\n```", "```py\n            if(steps % 2 == 0):\n\n                _, discriminator_loss = session.run([d_optimizer,D_loss], feed_dict={x: batch_images, is_training:True, learning_rate:lr})            \n```", "```py\n            _, generator_loss = session.run([g_optimizer,G_loss], feed_dict={x: batch_images, is_training:True, learning_rate:lr})\n            _, discriminator_loss = session.run([d_optimizer,D_loss], feed_dict={x: batch_images, is_training:True, learning_rate:lr})\n```", "```py\n           display.clear_output(wait=True) \n            generate_new_samples(session, 1, z_dim)\n            print(\"Epoch: {}, iteration: {}, Discriminator Loss:{}, Generator Loss: {}\".format(epoch,i,discriminator_loss,generator_loss))   \n\n            steps += 1\n```", "```py\nD_loss_real = 0.5*tf.reduce_mean(tf.square(D_logits_real-1))\n```", "```py\nD_loss_fake = 0.5*tf.reduce_mean(tf.square(D_logits_fake))\n```", "```py\nD_loss = D_loss_real + D_loss_fake\n```", "```py\nG_loss = 0.5*tf.reduce_mean(tf.square(D_logits_fake-1))\n```", "```py\nD_loss = - tf.reduce_mean(D_real) + tf.reduce_mean(D_fake)\n```", "```py\nG_loss = -tf.reduce_mean(D_fake)\n```", "```py\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n```"]