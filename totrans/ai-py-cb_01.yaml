- en: Getting Started with Artificial Intelligence in Python
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll start by setting up a Jupyter environment to run our
    experiments and algorithms in, we'll get into different nifty Python and Jupyter
    hacks for **artificial intelligence** (**AI**), we'll do a toy example in scikit-learn,
    Keras, and PyTorch, and then a slightly more elaborate example in Keras to round
    things off. This chapter is largely introductory, and a lot of what see in this
    chapter will be built on in subsequent chapters as we get into more advanced applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Jupyter environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting proficient in Python for AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying in scikit-learn, Keras, and PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling with Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You really should have a GPU available in order to run some of the recipes in
    this book, or you would better off using Google Colab. There are some extra steps
    required to make sure you have the correct NVIDIA graphics drivers installed,
    along with some additional libraries. Google provides up-to-date instructions
    on the TensorFlow website at [https:/​/​www. tensorflow.​org/​install/​gpu](https:/%E2%80%8B/%E2%80%8Bwww.%20tensorflow.%E2%80%8Borg/%E2%80%8Binstall/%E2%80%8Bgpu).
    Similarly, PyTorch versions have minimum requirements for NVIDIA driver versions
    (which you'd have to check manually for each PyTorch version). Let's see how to
    use dockerized environments to help set this up.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the recipes in this chapter in the GitHub repository of this book
    at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Jupyter environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you are aware, since you've acquired this book, Python is the dominant programming
    language in AI. It has the richest ecosystem of all programming languages, including
    many implementations of state-of-the-art algorithms that make using them often
    a matter of simply importing and setting a few selected parameters. It should
    go without saying that we will go beyond the basic usage in many cases and we
    will talk about a lot of the underlying ideas and technologies as we go through
    the recipes.
  prefs: []
  type: TYPE_NORMAL
- en: We can't emphasize enough the importance of being able to quickly prototype
    ideas and see how well they work as part of a solution. This is often the main
    part of AI or data science work. A **read-eval-print loop** (**REPL**) is essential
    for quick iteration when turning an idea into a prototype, and you want functionality
    such as edit history, graphing, and more. This explains why Jupyter Notebook (where
    **Jupyter** is short for **Julia, Python, R**) is so central to working in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, although we'll be focusing on Jupyter Notebook, or Google
    Colab, which runs Jupyter notebooks in the cloud, there are a few functionally
    similar alternatives around such as JupyterLab or even PyCharm running with a
    remote interpreter. Jupyter Notebook is still, however, the most popular (and
    probably the best supported) choice.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will make sure we have a working Python environment with
    the software libraries that we need throughout this book. We'll be dealing with
    installing relevant Python libraries for working with AI, and we'll set up a Jupyter
    Notebook server.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Firstly, ensure you have Python installed, as well as a method of installing
    libraries. There are different ways of using and installing libraries, depending
    on the following two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: You use one of the services that host interactive notebooks, such as Google
    Colab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You install Python libraries on your own machine(s).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Python, a **module** is a Python file that contains functions, variables,
    or classes. A **package** is a collection of modules within the same path. A **library**
    is a collection of related functionality, often in the form of different packages
    or modules. Informally, it's quite common to refer to a Python library as a package,
    and we'll sometimes do this here as well.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's set up our Python environment(s)!
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve mentioned, we''ll be looking at two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Google Colab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a computer ourselves to host a Jupyter Notebook instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first case, we won't need to set up anything on our server as we'll only
    be installing a few additional libraries. In the second case, we'll be installing
    an environment with the Anaconda distribution, and we'll be looking at setup options
    for Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we'll have an interactive Python notebook available through which
    we'll be running most of our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Installing libraries with Google Colab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Colab is a modified version of Jupyter Notebook that runs on Google hardware and provides
    access to runtimes  with hardware acceleration such as TPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The downside of using Colab is that there is a maximum timeout of 12 hours;
    that is, jobs that run longer than 12 hours will stop. If you want to get around that,
    you can do either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Run Colab with local kernels. This means you use the Colab interface but the models compute
    on your own  computer ([https://research.google.com/Colaboratory/local-runtimes.html](https://research.google.com/colaboratory/local-runtimes.html)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Jupyter Notebook yourself and don't use Google Colab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Google Colab, just go to [https://colab.research.google.com/](https://colab.research.google.com/), and
    sign in with your Google credentials. In the following section, we'll deal with
    hosting notebooks on your own machine(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Google Colab, you can save and re-load your models to and from the remote
    disk on Google servers. From there you can either download the models to your
    own computer or synchronize with Google Drive. The Colab GUI provides many useful
    code snippets for these use cases. Here''s how to download files from Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from joblib import dump`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dump(`'
  prefs: []
  type: TYPE_NORMAL
- en: '`     my_model,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`     ''my_model_auc0.84.joblib''`'
  prefs: []
  type: TYPE_NORMAL
- en: '`)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`files.download(''my_model_auc0.84.joblib'')`'
  prefs: []
  type: TYPE_NORMAL
- en: Self-hosting a Jupyter Notebook environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are different ways to maintain your Python libraries (see [https://packaging.Python.org/tutorials/installing-packages/](https://packaging.python.org/tutorials/installing-packages/)
    for more details). For installations of Jupyter Notebook and all libraries, we recommend the
    Anaconda Python distribution, which works with the `conda` environment manager.
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda is a Python distribution that comes with its own package installer
    and environment manager, called `conda`. This makes it easier to keep your libraries
    up to date and it handles system dependency management as well as Python dependency
    management. We'll mention a few alternatives to Anaconda/conda later; for now,
    we will quickly go through instructions for a local install. In the online material,
    you'll find instructions that will show how to serve similar installations to
    other people across a team, for example, in a company using a dockerized setup,
    which helps manage the setup of a machine or a set of machines across a network
    with a Python environment for AI.
  prefs: []
  type: TYPE_NORMAL
- en: If you have your computer already set up, and you are familiar with `conda`
    and `pip`, please feel free to skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Anaconda installation, we will need to download an installer and then
    choose a few settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Anaconda distribution page at [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual) and download the
    appropriate installer for Python 3.7 for your system, such as 64-Bit (x86) Installer (506 MB).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anaconda supports Linux, macOS, and Windows installers.
  prefs: []
  type: TYPE_NORMAL
- en: For macOS and Windows, you also have the choice of a graphical installer. This
    is all well explained in the Anaconda documentation; however, we'll just quickly
    go through the terminal installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the downloaded shell script from your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You need to read and confirm the license agreement. You can do this by pressing
    the spacebar until you see the question asking  you to agree. You need to press
    *Y* and then *Enter*.
  prefs: []
  type: TYPE_NORMAL
- en: You can go with the suggested download location or choose a directory that's shared between users
    on your computer.  Once you've done that, you can get yourself a cup of tasty
    coffee or stay to watch the installation of Python and lots of Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, you can decide if you want to run the `conda init` routine. This will set up the PATH variables on your
    terminal, so when you type `python`, `pip`, `conda`, or `jupyter`, the conda versions
    will take precedence before any other installed version on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that on Unix/Linux based systems, including macOS, you can always check
    the location of the Python binary you are using as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: On Windows, you can use the `where.exe` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see something like the following, then you know you are using the right
    Python runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don''t see the correct path, you  might have to run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will set up your environment variables, including `PATH`. On Windows, you'd
    have to check your PATH variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s also possible to set up and switch between different environments on
    the same machine. Anaconda comes with Jupyter/iPython by default, so you can start
    your Jupyter notebook from the terminal as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You should see the Jupyter Notebook server starting up. As a part of this information,
    a URL for login is printed to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: If you run this from a server that you access over the network, make sure you
    use a screen multiplexer such as GNU screen or tmux to make sure your Jupyter
    Notebook client doesn't stop once your terminal gets disconnected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use many libraries in this book such as pandas, NumPy, scikit-learn,
    TensorFlow, Keras, PyTorch, Dash, Matplotlib, and others, so we''ll be installing lots as we go through the recipes. This will often look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, sometimes, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we use conda's `pip`, or conda directly, this means the libraries will all
    be managed by Anaconda's Python installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install the aforementioned libraries like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that for the `tensorflow-gpu` library, you need to have a GPU available
    and ready to use. If not, change this to `tensorflow` (that is, without `-gpu`).
  prefs: []
  type: TYPE_NORMAL
- en: This should use the `pip` binary that comes with Anaconda and run it to install
    the preceding libraries. Please note that Keras is part of the TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can run the `conda` package installer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Well done! You've successfully set up your computer for working with the many
    exciting recipes to come.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conda is an environment and package manager. Like many other libraries that
    we will use throughout this book, and like the Python language itself, conda is
    open source, so we can always find out exactly what an algorithm does and easily
    modify it. Conda is also cross-platform and not only supports Python but also
    R and other languages.
  prefs: []
  type: TYPE_NORMAL
- en: Package management can present many vexing challenges and, if you've been around
    for some time, you will probably remember spending many hours on issues such as
    conflicting dependencies or re-compiling packages and fixing paths – and you might
    be lucky if it's only that.
  prefs: []
  type: TYPE_NORMAL
- en: Conda goes beyond the earlier `pip` package manager (see [https://pip.pypa.io/en/stable/](https://pip.pypa.io/en/stable/)) in
    that it checks dependencies of all packages installed within the environment and
    tries to come up with a way to resolve all the requirements. It also not only
    installs packages, but also allows us to set up environments that have separate
    installations of Python and binaries from different software repositories such
    as Bioconda ([https://bioconda.github.io/](https://bioconda.github.io/)), which
    specializes in bioinformatics, or the Anaconda repositories ([https://anaconda.org/anaconda/repo](https://anaconda.org/anaconda/repo)).
  prefs: []
  type: TYPE_NORMAL
- en: There are hundreds of dedicated channels that you can use with conda. These
    are sub-repositories that can contain hundreds or thousands of different packages.
    Some of them are maintained by companies that develop specific libraries or software.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can install the `pytorch` package from the PyTorch channel
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It's tempting to enable many channels in order to get the bleeding edge technology
    for everything. There's one catch, however, with this. If you enable many channels,
    or channels that are very big, conda's dependency resolution can become very slow.
    So be careful with using many additional channels, especially if they contain
    a lot of libraries.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of Jupyter options you should probably be familiar with. These are
    in the file at `$HOME/.jupyter/jupyter_notebook_config.py`. If you don''t have
    the file yet, you can create it using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example configuration for `/home/ben/.jupyter/jupyter_notebook_config.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you install your Python environment on a server that you want to access from
    your laptop (I have my local compute server in the attic), you'd first want make sure you
    can access the compute server remotely from another computer such as a laptop
    (`c.NotebookApp.ip = '*'`).
  prefs: []
  type: TYPE_NORMAL
- en: Then we create a random password and configure it.  We disable the option to
    have the browser open when we run Jupyter Notebook, and we then set the default port to `8888`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So Jupyter Notebook will be available when you open `localhost:8888` in a browser
    on the same computer. If you are in a team as part of a larger organization, you''d be mostly working on remote number-crunching
    machines, and as a convenience, you – or your sysadmins – can set up a hosted
    Jupyter Notebook environment. This has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the resources of a powerful server while simply accessing it through your browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can manage your packages in a contained environment on that server, while not affecting the server itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll find yourself interacting with Jupyter Notebook's familiar REPL, which allows you to quickly test ideas and prototype projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are a single person, you don't need this; however, if you work in a team,
    you can put each person into a contained environment using either Docker or JupyterHub.
    Online, you'll find setup instructions for setting up a Jupyter environment with
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read up more on conda, Docker, JupyterHub, and other related tools
    on their respective documentation sites, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The conda documentation: [https://docs.conda.io/en/latest/](https://docs.conda.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker documentation: [https://docs.docker.com/](https://docs.docker.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JupyterHub: [https://jupyterhub.readthedocs.io/en/stable/](https://jupyterhub.readthedocs.io/en/stable/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter: [https://jupyter.org/](https://jupyter.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JupyterLab: [https://jupyterlab.readthedocs.io/en/stable/](https://jupyterlab.readthedocs.io/en/stable/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyCharm: [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Colab: [https://Colab.research.google.com](https://colab.research.google.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipenv: [https://pipenv-fork.readthedocs.io/en/latest/](https://pipenv-fork.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pip: [https://pip.pypa.io/en/stable/](https://pip.pypa.io/en/stable/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting proficient in Python for AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this set of quick-fire recipes, we'll look at ways to become more productive
    in Jupyter and to write more efficient Python code. If you are not familiar with
    Jupyter Notebook, please read a tutorial and then come back here. You can find
    a well-written tutorial at [https://realPython.com/jupyter-notebook-introduction/](https://realpython.com/jupyter-notebook-introduction/). In
    the following recipe, we'll assume you have some familiarity with Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some simple but very handy tricks to make working in notebooks
    more comfortable and efficient. These are applicable whether you are relying on a local or
    hosted Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll look at a lot of different things that can help you become
    more productive when you are working in your notebook and writing Python code
    for AI solutions. Some of the built-in or externally available magic commands or extensions can also come in handy
    (see [https://iPython.readthedocs.io/en/stable/interactive/magics.html](https://ipython.readthedocs.io/en/stable/interactive/magics.html)
    for more details).
  prefs: []
  type: TYPE_NORMAL
- en: It's important to be aware of some of the Python efficiency hacks when it comes
    to machine learning, especially when working with some of the bigger datasets
    or more complex algorithms. Sometimes, your jobs can take very long to run, but
    often there are ways around it. For example, one, often relatively easy, way of
    finishing a job faster is to use parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following short recipes cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the Jupyter command history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-reloading packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timing code execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up pandas DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying progress bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are using your own installation, whether directly on your system or inside
    a Docker environment, make sure that it's running. Then put the address of your
    Colab or Jupyter Notebook instance into your browser and press *Enter*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tqdm` for progress bars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swifter` for quicker pandas processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ray` and `joblib` for multiprocessing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numba` for **just-in-time** (**JIT**) compilation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jax` (later on in this section) for array processing with autograd'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cython` for compiling Cython extensions in the notebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can install them with `pip` as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With that done, let's get to some efficiency hacks that make working in Jupyter
    faster and more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sub-recipes here are short and sweet, and all provide ways to be more productive
    in Jupyter and Python.
  prefs: []
  type: TYPE_NORMAL
- en: If not indicated otherwise, all of the code needs to be run in a notebook, or,
    more precisely, in a notebook cell.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get to these little recipes!
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the history of Jupyter commands and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are lots of different ways to obtain the code in Jupyter cells programmatically.
    Apart from these inputs, you can also look at the generated outputs. We'll get
    to both, and we can use global variables for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Execution history
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to get the execution history of your cells, the `_ih` list holds the code of executed cells. In
    order to get the complete execution history and write it to a file, you can do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If up to this point, we only ran a single cell consisting of `print(''hello,
    world!'')`, that''s exactly what we should see in our newly created file, `command_history.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: On Windows, to print the content of a file, you can use the `type` command.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `_ih`, we can use a shorthand for the content of the last three cells.
    `_i` gives you the code of the cell that just executed, `_ii` is used for the
    code of the cell executed before that, and `_iii` for the one before that.
  prefs: []
  type: TYPE_NORMAL
- en: Outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to get recent outputs, you can use `_` (single underscore), `__` (double
    underscore), and `___` (triple underscore), respectively, for the most recent,
    second, and third most recent outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-reloading packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`autoreload` is a built-in extension that reloads the module when you make
    changes to a module on disk. It will automagically reload the module once you''ve saved it.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of manually reloading your package or restarting the notebook, with
    `autoreload`, the only thing you have to do is to load and enable the extension,
    and it will do its magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the extension as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we enable it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This can save a lot of time when you are developing (and testing) a library
    or module.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you cannot spot an error and the traceback of the error is not enough to find the problem, debugging can speed up
    the error-searching process a lot. Let''s have a quick look at the debug magic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Put the following code into a cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You should see `5.0` as the cell output.
  prefs: []
  type: TYPE_NORMAL
- en: However, there's an error in the function, and I am sure the attentive reader
    will already have spotted it. Let's debug!
  prefs: []
  type: TYPE_NORMAL
- en: 'Put this into a new cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the cell by pressing *Ctrl* + *Enter* or *Alt* + *Enter*. You will
    get a debug prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We've used the argument command to print out the arguments of the executed function,
    and then we quit the debugger with the `quit` command. You can find more commands
    on **The Python Debugger** (**pdb**) documentation page at [https://docs.Python.org/3/library/pdb.html](https://docs.python.org/3/library/pdb.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a few more useful magic commands.
  prefs: []
  type: TYPE_NORMAL
- en: Timing code execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once your code does what it's supposed to, you often get into squeezing every
    bit of performance out of your models or algorithms. For this, you'll check execution
    times and create benchmarks using them. Let's see how to time executions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a built-in magic command for timing cell execution – `timeit`. The
    `timeit` functionality is part of the Python standard library ([https://docs.Python.org/3/library/timeit.html](https://docs.python.org/3/library/timeit.html)).
    It runs a command 10,000 times (by default) in a period of 5 times inside a loop
    (by default) and shows an average execution time as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `iPython-autotime` library ([https://github.com/cpcloud/iPython-autotime](https://github.com/cpcloud/ipython-autotime)) is an external extension
    that provides you the  timings for all the cells that execute, rather than having
    to use `%%timeit` every time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `autotime` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that this syntax works for Colab, but not in standard Jupyter Notebook.
    What always works to install libraries is using the `pip` or `conda` magic commands, `%pip` and `%conda`, respectively.
    Also, you can  execute any  shell command from the notebook if you start your line with an exclamation mark, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Test how long a simple list comprehension takes with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see this output: `time: 5.62 ms`.'
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you can see how this can come in handy for comparing different implementations.
    Especially in situations where you have a lot of data, or complex processing,
    this can be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying progress bars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Even if your code is optimized, it''s good to know if it''s going to finish
    in minutes, hours, or days. `tqdm` provides progress bars with time estimates.
    If you aren''t sure how long your job will run, it''s just one letter away – in
    many cases, it''s just a matter of changing `range` for `trange`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `tqdm` pandas integration (optional) means that you can see progress bars
    for pandas `apply` operations. Just swap `apply` for `progress_apply`.
  prefs: []
  type: TYPE_NORMAL
- en: For Python loops just wrap your loop with a `tqdm` function and voila, there'll
    be a progress bar and time estimates for your loop completion!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Tqdm provides different ways to do this, and they all require minimal code
    changes - sometimes as little as one letter, as you can see in the previous example.
    The more general syntax is wrapping your loop iterator with `tqdm` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a progress bar like in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2299dce8-af59-48c8-8528-ab2a4c777965.png)'
  prefs: []
  type: TYPE_IMG
- en: So, next time you are just about to set off long-running loop, and you are not
    just sure how long it will take, just remember this sub-recipe, and use `tqdm`.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling your code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python is an interpreted language, which is a great advantage for experimenting,
    but it can be detrimental to speed. There are different ways to compile your Python
    code, or to use compiled code from Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first look at Cython. Cython is an optimizing static compiler for Python,
    and the programming language compiled by the Cython compiler. The main idea is
    to write code in a language very similar to Python, and generate C code. This
    C code can then be compiled as a binary Python extension. SciPy (and NumPy), scikit-learn,
    and many other libraries have significant parts written in Cython for speed up.
    You can find out more about Cython on its website at [https://cython.org/](https://cython.org/):'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `Cython` extension for building cython functions in your notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the extension, annotate your cell as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call this function just like any Python function – with the added benefit
    that it''s already compiled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This is perhaps not the most useful example of compiling code. For such a small
    function, the overhead of compilation is too big. You would probably want to compile
    something that's a bit more complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba is a JIT compiler for Python ([https://numba.pydata.org/](https://numba.pydata.org/)).
    You can often get a speed-up similar to C or Cython using `numba` and writing idiomatic Python code
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With `autotime` activated, you should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'So again, the overhead of the compilation is too big to make a meaningful impact. Of course, we''d only see the
    benefit if it''s offset against the compilation. However, if we use this function
    again, we should see a speedup. Try it out yourself! Once the code is already compiled, the time significantly improves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There are other libraries that provide JIT compilation including TensorFlow,
    PyTorch, and JAX, that can help you get similar benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example comes directly from the JAX documentation, at [https://jax.readthedocs.io/en/latest/index.html](https://jax.readthedocs.io/en/latest/index.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So there are different ways to get speed benefits from using JIT or ahead-of-time
    compilation. We'll see some other ways of speeding up your code in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most important libraries throughout this book will be `pandas`, a
    library for tabular data that's useful for **Extract**, **Transform**, **Load**
    (**ETL**) jobs. Pandas is a wonderful library, however; once you get to more demanding
    tasks, you'll hit some of its limitations. Pandas is the go-to library for loading
    and transforming data. One problem with data processing is that it can be slow,
    even if you vectorize the function or if you use `df.apply()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can move further by parallelizing `apply`. Some libraries, such as `swifter`,
    can help you by choosing backends for computations for you, or you can make the
    choice yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use Dask DataFrames instead of pandas if you want to run on multiple
    cores of the same or several machines over a network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use CuPy or cuDF if you want to run computations on the GPU instead
    of the CPU. These have stable integrations with Dask, so you can run both on multiple
    cores and multiple GPUs, and you can still rely on a pandas-like syntax (see [https://docs.dask.org/en/latest/gpu.html](https://docs.dask.org/en/latest/gpu.html)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we''ve mentioned, `swifter` can choose a backend for you with no change
    of syntax. Here is a quick setup for using `pandas` with `swifter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Generally, `apply()` is much faster than looping over DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can further improve the speed of execution by using the underlying NumPy arrays
    directly and accessing NumPy functions, for example, using `df.values.apply()`.
    NumPy vectorization can be a breeze, really. See the following example of applying
    a NumPy vectorization on a pandas DataFrame column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: These are just two ways, but if you look at the next sub-recipe, you should
    be able to write a parallel map function as yet another alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing your code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way to get something done more quickly is to do multiple things at once.
    There are different ways to implement your routines or algorithms with parallelism.
    Python has a lot of libraries that support this functionality. Let's see a few
    examples with multiprocessing, Ray, joblib, and how to make use of scikit-learn's
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multiprocessing library comes as part of Python''s standard library. Let''s
    look at it first. We don''t provide a dataset of millions of points here – the
    point is to show a usage pattern – however, please imagine a large dataset. Here''s
    a code snippet of using our pseudo-dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Using Ray, you can parallelize over multiple machines in addition to multiple
    cores, leaving your code virtually unchanged. Ray efficiently handles data through
    shared memory (and zero-copy serialization) and uses a distributed task scheduler
    with fault tolerance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Scikit-learn, the machine learning library we installed earlier, internally
    uses `joblib` for parallelization. The following is an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This would give you `[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]`. We
    took this example from the `joblib` examples about parallel for loops, available
    at [https://joblib.readthedocs.io/en/latest/parallel.html](https://joblib.readthedocs.io/en/latest/parallel.html).
  prefs: []
  type: TYPE_NORMAL
- en: When using scikit-learn, watch out for functions that have an `n_jobs` parameter.
    This parameter is directly handed over to `joblib.Parallel` ([https://github.com/joblib/joblib/blob/master/joblib/parallel.py](https://github.com/joblib/joblib/blob/master/joblib/parallel.py)).
    `none` (the default setting) means sequential execution, in other words, no parallelism.
    So if you want to execute code in parallel, make sure to set this `n_jobs` parameter,
    for example, to `-1` in order to make full use of all your CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch and Keras both support multi-GPU and multi-CPU execution. Multi-core
    parallelization is done by default. Multi-machine execution in Keras is getting
    easier from release to release with TensorFlow as the default backend.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While notebooks are convenient, they are often messy, not conducive to good coding habits, and they cannot be versioned
    cleanly. Fastai has developed an extension for literate code development in notebooks called nbdev
    ([https://github.com/fastai/nbdev](https://github.com/fastai/nbdev)), which provides tools for exporting and documenting code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot more useful extensions that you can find in different places:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The extension index: [https://github.com/iPython/iPython/wiki/Extensions-Index](https://github.com/ipython/ipython/wiki/Extensions-Index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter contrib extensions: [https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions.html](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The awesome-jupyter list: [https://github.com/markusschanta/awesome-jupyter](https://github.com/markusschanta/awesome-jupyter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We would also like to highlight the following extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: SQL Magic, which performs SQL queries: [https://github.com/catherinedevlin/iPython-sql](https://github.com/catherinedevlin/ipython-sql)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watermark, which extracts version information for used packages: [https://github.com/rasbt/watermark](https://github.com/rasbt/watermark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pyheatmagic, for profiling with heat maps: [https://github.com/csurfer/pyheatmagic](https://github.com/csurfer/pyheatmagic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nose testing, for testing using nose: [https://github.com/taavi/iPython_nose](https://github.com/taavi/ipython_nose)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pytest magic, for testing using pytest: [https://github.com/cjdrake/iPython-magic](https://github.com/cjdrake/ipython-magic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dot and others, used for drawing diagrams using graphviz: [https://github.com/cjdrake/iPython-magic](https://github.com/cjdrake/ipython-magic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalene, a CPU and memory profiler: [https://github.com/emeryberger/scalene](https://github.com/emeryberger/scalene)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some other libraries used or mentioned in this recipe include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Swifter`: [https://github.com/jmcarpenter2/swifter](https://github.com/jmcarpenter2/swifter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Autoreload`: [https://iPython.org/iPython-doc/3/config/extensions/autoreload.html](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdb`: [https://docs.Python.org/3/library/pdb.html](https://docs.python.org/3/library/pdb.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tqdm`: [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JAX`: [https://jax.readthedocs.io/](https://jax.readthedocs.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Seaborn`: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Numba`: [https://numba.pydata.org/numba-doc/latest/index.html](https://numba.pydata.org/numba-doc/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dask`: [https://ml.dask.org/](https://ml.dask.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CuPy`: [https://cupy.chainer.org](https://cupy.chainer.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cuDF`: [https://github.com/rapidsai/cudf](https://github.com/rapidsai/cudf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ray`: [http://ray.readthedocs.io/en/latest/rllib.html](http://ray.readthedocs.io/en/latest/rllib.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`joblib`: [https://joblib.readthedocs.io/en/latest/](https://joblib.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying in scikit-learn, Keras, and PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll be looking at data exploration, and modeling in three
    of the most important libraries. Therefore, we''ll break things down into the
    following sub-recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data in Seaborn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling in scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout these recipes and several subsequent ones, we''ll focus on covering
    first the basics of the three most important libraries for AI in Python: scikit-learn,
    Keras, and PyTorch. Through this, we will introduce basic and intermediate techniques
    in supervised machine learning with deep neural networks and other algorithms. This
    recipe will cover the basics of these three main libraries in machine learning
    and deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through a simple classification task using scikit-learn, Keras, and
    PyTorch in turn. We'll run both of the deep learning frameworks in offline mode.
  prefs: []
  type: TYPE_NORMAL
- en: These recipes are for introducing the basics of the three libraries. However,
    even if you've already worked with all of them, you might still find something
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Iris Flower dataset is one of the oldest machine learning datasets still
    in use. It was published by Ronald Fisher in 1936 to illustrate linear discriminant
    analysis. The problem is to classify one of three iris flower species based on
    measurements of sepal and petal width and length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this is a very simple problem, the basic workflow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess and transform the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a model to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret and understand the model (this stage is often optional).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a standard process template that we will have to apply to most of the
    problems shown throughout this book. Typically, with industrial-scale problems,
    *Steps 1* and *2* can take much longer (sometimes estimated to take about 95 percent
    of the time) than for one of the already preprocessed datasets that you will get
    for a Kaggle competition or at the UCI machine learning repository. We will go
    into the complexities of each of these steps in later recipes and chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll assume you''ve installed the three libraries earlier on and that you
    have your Jupyter Notebook or Colab instance running. Additionally, we will use
    the seaborn and scikit-plot libraries for visualization, so we''ll install them
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The convenience of using a dataset so well known is that we can easily load
    it from many packages, for example, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Let's jump right in, starting with data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first have a look at the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data in seaborn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll go through the basic steps of data exploration. This
    is often important to understand the complexity of the problem and any underlying
    issues with the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a pair-plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here it comes (rendered in seaborn''s pleasant spacing and coloring):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfd7ec2a-4641-48a0-88bf-bec5dcfe675c.png)'
  prefs: []
  type: TYPE_IMG
- en: A pair-plot in seaborn visualizes pair-wise relationships in a dataset. Each
    subplot shows one variable plotted against another in a scatterplot. The subplots
    on the diagonal show the distribution of the variables. The colors correspond
    to the three classes.
  prefs: []
  type: TYPE_NORMAL
- en: From this plot, especially if you look along the diagonal, we can see that the
    virginica and versicolor species are not (linearly) separable. This is something
    we are going to struggle with, and that we'll have to overcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick look at the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We only see setosa, since the flower species are ordered and listed one after
    another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ae80d8e-725a-4cf3-8933-e84757f96b0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Separate the features and target in preparation for training as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The last line converted the three strings corresponding to the three classes
    into numbers – this is called an ordinal coding. A multiclass machine learning
    algorithm can deal with this. For neural networks, we'll use another encoding,
    as you'll see later.
  prefs: []
  type: TYPE_NORMAL
- en: After these basic steps, we are ready to start developing predictive models.
    These are models that predict the flower class from the features. We'll see this
    in turn for each of the three most important machine learning libraries in Python.
    Let's start with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we'll create a classifier in scikit-learn, and check its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn (also known as sklearn) is a Python machine learning framework
    developed since 2007\. It is also one of the most comprehensive frameworks available,
    and it is interoperable with the pandas, NumPy, SciPy, and Matplotlib libraries.
    Much of scikit-learn has been optimized for speed and efficiency in Cython, C,
    and C++.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that not all scikit-learn classifiers can do multiclass problems.
    All classifiers can do binary classification, but not all can do more than two
    classes. The random forest model can, fortunately. The **random forest** model
    (sometimes referred to as **random decision forest**) is an algorithm that can
    be applied to classification and regression tasks, and is an ensemble of decision
    trees. The main idea is that we can increase precision by creating decision trees
    on bootstrapped samples of the dataset, and average over these trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the following lines of code should appear to you as boilerplate, and
    we''ll use them over and over:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate training and validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a matter of good practice, we should always test the performance of our
    models on a sample of our data that wasn''t used in training (referred to as a **hold-out
    set** or **validation set**). We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Define a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here we define our model **hyperparameters**, and create the model instance
    with these hyperparameters. This goes as follows in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are parameters that are not part of the learning process, but
    control the learning. In the case of neural networks, this includes the learning
    rate, model architecture, and activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we pass the training dataset to our model. During training, the parameters
    of the model are being fit so that we obtain better results (where *better* is
    defined by a function, called the **cost function** or **loss function**).
  prefs: []
  type: TYPE_NORMAL
- en: 'For training we use the `fit` method, which is available for all sklearn-compatible
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Check the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While there''s a measure internal to the model (the cost function), we might
    want to look at additional measures. In the context of modeling, these are referred
    to as metrics. In scikit-learn, we have a lot of metrics at our fingertips. For
    classification, we would usually look at the confusion matrix, and often we''d
    want to plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix is relatively intuitive, especially when the presentation
    is as clear as with sklearn's `plot_confusion_matrix()`. Basically, we see how
    well our class predictions line up with the actual classes. We can see the predictions
    against actual labels, grouped by class, so that each entry corresponds to how
    many times class A was predicted given the actual class B. In this case, we've
    normalized the matrix, so that each row (actual labels) sums to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/325f66c1-c69a-48e8-8691-29c58bf857e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Since this is a normalized martix, the numbers on the diagonal are also called
    the **hit rate **or **true positive rate**. We can see that setosa was predicted
    as setosa 100% (1) of the time. By contrast, versicolor was predicted as versicolor
    95% of the time (0.95), while 5% of the time (0.053) it was predicted as virginica.
  prefs: []
  type: TYPE_NORMAL
- en: The performance is very good in terms of hit rate, however; as expected, we
    are having a small problem distinguishing between versicolor and virginica.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we'll be predicting the flower species in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Keras is a high-level interface for (deep) neural network models that can use
    TensorFlow as a backend, but also **Microsoft Cognitive Toolkit** (**CNTK**),
    Theano, or PlaidML. Keras is an interface for developing AI models, rather than
    a standalone framework itself. Keras has been integrated as part of TensorFlow,
    so we import Keras from TensorFlow. Both TensorFlow and Keras are open source
    and developed by Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Keras is tightly integrated with TensorFlow, Keras models can be saved
    as TensorFlow models and then deployed in Google''s deployment system, TensorFlow
    Serving (see [https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)),
    or used from any of the programming languages such as, C++ or Java. Let''s get
    into it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code. If you are familiar with Keras, you''ll recognize it
    as boilerplate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following model construction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2ed7f28-0410-4d45-b145-f52df00637d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can visualize this model in different ways. We can use the built-in Keras
    functionality as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This writes a visualization of the network to a file called `iris_model_keras.png`.
    The image produced looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5556bf37-dfcc-4ab1-80e6-92e6b4d7f02f.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows that we have 4 input neurons, 10 hidden neurons, and 3 output neurons,
    fully connected in a feed-forward fashion. This means that all neurons in the
    input feed input to all neurons in the hidden layer, which in turn feed to all
    neurons in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the sequential model construction (as opposed to the graph). The
    sequential model type is more straightforward to build than the graph type. The
    layers are constructed the same way; however, for the sequential model, you have
    to define the input dimensionality, `input_dim`.
  prefs: []
  type: TYPE_NORMAL
- en: We use two dense layers, the intermediate layer with SELU activation function,
    and the final layer with the softmax activation function. We'll explain both of
    these in the *How it works...* section. As for the **SELU activation function**,
    suffice it to say for now that it provides a necessary nonlinearity so that the
    neural network can deal with more variables that are not linearly separable, as
    in our case. In practice, it is rare to use a linear (**identity function**) activation
    in the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Each unit (or neuron) in the final layer corresponds to one of the three classes.
    The **softmax function** normalizes the output layer so that its neural activations
    add up to 1\. We train with categorical cross-entropy as our loss function. Cross-entropy
    is typically used for classification problems with neural networks. The binary
    cross-entropy loss is for two classes, and categorical cross-entropy is for two
    or more classes (cross-entropy will be explained in more detail in the *How it
    works...* section).
  prefs: []
  type: TYPE_NORMAL
- en: Next, one-hot encode the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This means we have three columns that each stand for one of the species, and
    one of them will be set to `1` for the corresponding class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Our `y_categorical` therefore has the shape (150, 3). This means that to indicate
    class 0 as the label, instead of having a `0` (this would be sometimes called
    **label encoding** or **integer encoding**), we have a vector of `[1.0, 0.0, 0.0]`.
    This is called **one-hot encoding**. The sum of each row is equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For neural networks, our features should be normalized in a way that the activation
    functions can deal with the whole range of inputs – often this normalization is
    to the standard distribution, which has a mean of 0.0 and standard deviation of
    1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this cell is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the mean values for each column are very close to zero. We can
    also see the standard deviations with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The standard deviation is exactly `1`, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Display our training progress in TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TensorBoard is a visualization tool for neural network learning, such as tracking
    and visualizing metrics, model graphs, feature histograms, projecting embeddings,
    and much more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, a TensorBoard widget should pop up in your notebook. We just
    have to make sure it gets the information it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plug the TensorBoard details into the Keras training function as a callback
    so TensorBoard gets the training information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This runs our training. An epoch is an entire pass of the dataset through the
    neural network. We use `150` here, which is a bit arbitrary. We could have used
    a stopping criterion to stop training automatically when validation and training
    errors start to diverge, or in other words, when overfitting occurs.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use `plot_confusion_matrix()` as before, for comparison, we'd have
    to wrap the model in a class that implements the `predict()` method, and has a
    list of `classes_` and an attribute of `_estimator_type` that is equal to the
    classifier. We will show that in the online material.
  prefs: []
  type: TYPE_NORMAL
- en: Plot the confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, it''s easier to use a `scikitplot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, as before, we normalize the matrix, so we get fractions. The output
    should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21ef449d-bb15-482e-be0d-bdfe51162cd1.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a bit worse than our previous attempt in scikit-learn, but with some
    tweaking we can get to a comparable level, or maybe even better performance. Examples
    of tweaking would be changing any of the model's hyperparameters such as the number
    of neurons in the hidden layer, any changes to the network architecture (adding
    a new layer), or changing the activation function of the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the charts from TensorBoard: the training progress and the model graph.
    Here they are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4c4ed8a1-0309-42c6-9043-cb4e2e5232d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These plots show the accuracy and loss, respectively, over the entire training.
    We also get another visualization of the network in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7234bd4-41b4-41cf-a8d7-1bec34ba6bf3.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows all the network layers, the loss and metrics, the optimizer (RMSprop),
    and the training routine, and how they are related. As for the network architecture,
    we can see four dense layers (the presented input and targets are not considered
    proper parts of the network, and are therefore colored in white). The network
    consists of a dense hidden layer (being fed by the input), and an dense output
    layer (being fed by the hidden layer). The loss function is calculated between
    the output layer activation and the targets. The optimizer works with all layers
    based on the loss. You can find a tutorial on TensorBoard at [https://www.tensorflow.org/tensorboard/get_started](https://www.tensorflow.org/tensorboard/get_started).
    The TensorBoard documentation explains more about configuration and options.
  prefs: []
  type: TYPE_NORMAL
- en: So the classification accuracy is improving and the loss is decreasing over
    the course of the training epochs. The final graph shows the network and training
    architecture, including the two dense layers, the loss and metrics, and the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we will describe a network equivalent to the previous one shown
    in Keras, train it, and plot the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch is a deep learning framework that is based on the Torch library primarily
    developed by Facebook. For some time, Facebook was developing another deep learning
    framework, called Caffe2; however, it was merged into PyTorch in March 2018\.
    Some of the strengths of PyTorch are in image and language processing applications.
    Apart from Python, Torch provides a C++ interface, both for learning and model
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the model architecture first. This looks very similar to Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same architecture that we defined before in Keras: this is a feed-forward,
    two-layer neural network with a SELU activation on the hidden layer, and 10 and
    3 neurons in the 2 layers.'
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer an output similar to the `summary()` function in Keras, you can
    use the `torchsummary` package ([https://github.com/sksq96/pytorch-summary](https://github.com/sksq96/pytorch-summary)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to convert our NumPy arrays to Torch tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`y_train` is the one-hot encoded target matrix that we created earlier. We
    are converting it back to integer encoding since the PyTorch cross-entropy loss
    expects this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can train, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we''ll use `scikitplot` to visualize our results, similar to before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the plot we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51e3ae5f-09dc-4f19-aed3-6f27cf259b29.png)'
  prefs: []
  type: TYPE_IMG
- en: Your plot might differ. Neural network learning is not deterministic, so you
    could get better or worse numbers, or just different ones.
  prefs: []
  type: TYPE_NORMAL
- en: We can get better performance if we let this run longer. This is left as an
    exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll first look at the intuitions behind neural network training, then we'll
    look a bit more at some of the technical details that we will use in the PyTorch
    and Keras recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic idea in machine learning is that we try to minimize an error by changing
    the parameters of a model. This adaption of the parameter is called learning.
    In supervised learning, the error is defined by a loss function calculated between
    the prediction of the model and the target. This error is calculated at every
    step and the model parameters are adjusted accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are composable function approximators consisting of tunable
    affine transformations (*f*) with an activation function (sigma):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6425a5ac-bd5c-4e81-9ab6-a6bb964f35b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the simplest terms, in a feed-forward neural network of one layer with linear
    activations, the model predictions are given by the sum of the product of the
    coefficients with the input in all of its dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd55b3c9-45ed-44de-adec-ec3c4333e188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is called a **perceptron**, and it is a linear binary classifier. A simple
    illustration with four inputs is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e27d0f2-78c4-4412-b4bc-bd5b03621320.png)'
  prefs: []
  type: TYPE_IMG
- en: The predictor for a one-dimensional input breaks down to the slope-intercept
    form of a line in two dimensions, ![](img/a4f3202a-015a-495a-81b3-4961062c7b23.png). Here,
    *m* is the slope and *b* the y-intercept. For higher-dimensional inputs, we can
    write (changing notation and vectorizing) ![](img/17e269e4-d56c-43fe-86ed-87f0bf939d21.png) with
    bias term ![](img/8689ae91-7972-4bdd-bdc7-26012b51bcdf.png) and weights ![](img/3a360d26-1c7e-4e50-94bf-429445299518.png). This
    is still a line, just in a space of the same dimensionality as the input. Please
    note that ![](img/6041b6ec-241c-4596-b926-39a613d89aa4.png) denotes our model
    prediction for ![](img/a696c282-1619-4d11-bafe-1b295f4e8775.png), and for the
    examples where we know ![](img/04cca892-4e5e-4509-a9b4-bfac3eee0493.png), we can
    calculate the difference between the two as our prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the same very simple linear algebra to define the binary classifier
    by thresholding as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fbde85b-3a06-4d69-8145-130055ff600c.png)'
  prefs: []
  type: TYPE_IMG
- en: This is still very simple linear algebra. This linear model with just one layer,
    called a perceptron, has difficulty predicting any more complex relationships.
    This lead to deep concern about the limitations of neural networks following an
    influential paper by Minsky and Papert in 1969\. However, since the 1990s, neural
    networks have been experiencing a resurgence in the shape of **support vector
    machines** (**SVMs**) and the **multilayer perceptron** (**MLP**). The MLP is
    a feed-forward neural network with at least one layer between the input and output
    (**hidden layer**). Since a multilayer perceptron with many layers of linear activations
    can be reduced to just one layer, non-trivially, we'll be referring to neural
    networks with hidden layers and nonlinear activation functions. These types of
    models can approximate arbitrary functions and perform nonlinear classification
    (according to the Universal Approximation Theorem). The activation function on
    any layer can be any differentiable nonlinearity; traditionally, the sigmoid, ![](img/dc02480a-fdad-4123-878d-6dfe65543152.png), has
    been used a lot for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'For illustration, let''s write this down with `jax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: If you look at this code, you'll see that we could have equally written this
    up with operations in NumPy, TensorFlow, or PyTorch. You'll also note that the
    `construct_network()` function takes a `layer_sizes` argument. This is one of
    the hyperparameters of the network, something to decide on before learning. We
    can choose just an output of [1] to get the perceptron, or [10, 1] to get a two-layer
    perceptron. So this shows how to get a network as a set of parameters and how
    to get a prediction from this network. We still haven't discussed how we learn
    the parameters, and this brings us to errors.
  prefs: []
  type: TYPE_NORMAL
- en: There's an adage that says, *"all models are wrong, but some are useful."* We
    can measure the error of our model, and this can help us to calculate the magnitude
    and direction of changes that we can make to our parameters in order to reduce
    the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a (differentiable) loss function (also called the cost function), ![](img/964429e0-1680-46d8-9984-1bea6fdb7def.png),
    such as the **mean squared error** (**MSE**), we can calculate our error. In the
    case of the MSE, the loss function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6a8cd3-5213-4436-9780-717e176e8ddf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then in order to get the change to our weights, we''ll use the derivative of
    the loss over the points in training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/663fd8c7-b8df-4971-9b56-6a230558dc5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means we are applying a gradient descent, which means that over time,
    our error will be reduced proportionally to the gradient (scaled by learning rate
    ![](img/1e7e2a35-9229-426f-af13-21aa5dd25e61.png)). Let''s continue with our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Both PyTorch and JAX have `autograd` functionality, which means that we can
    automatically get derivatives (gradients) of a wide range of functions.
  prefs: []
  type: TYPE_NORMAL
- en: We'll encounter a lot of different activation and loss functions throughout
    this book. In this chapter, we used the SELU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The SELU activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **scaled exponential linear unit** (**SELU**) activation function was published
    quite recently by Klambauer et al in 2017 ([http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf](http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e764eba-0ced-43e8-b1f6-e8646d4e3a6e.png)'
  prefs: []
  type: TYPE_IMG
- en: The SELU function is linear for positive values of *x,* a scaled exponential
    for negative values, and 0 when *x* is 0. ![](img/3c854e13-516d-455d-88f0-2eb1c4132e4f.png) is
    a value greater than 1.  You can find the details in the original paper. The SELU
    function has been shown to have better convergence properties than other functions.
    You can find a comparison of activation functions in Padamonti (2018) at [https://arxiv.org/pdf/1804.02763.pdf](https://arxiv.org/pdf/1804.02763.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Softmax activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As our activation function for the output layer in the neural networks, we
    use a softmax function. This works as a normalization to sum 1.0 of the neural
    activations of the output layer. The output can be therefore interpreted as the
    class probabilities. The softmax activation function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f36d5b2e-1c77-47fb-98e4-c3b7839e1275.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the multiclass training with neural networks, it''s common to train for
    cross-entropy. The binary cross-entropy for multiclass cases looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73a3d1d0-507a-40e8-8035-4b1075d149b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *M* is the number of classes (setosa, versicolor, and virginica), *y*
    is 0 or 1 if the class label *c* is correct, and *p* is the predicted probability
    that the observation *o* is of class *c*. You can read up more on different loss
    functions and metrics on the ml-cheatsheet site, at [https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find out more details on the website of each of the libraries used
    in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Seaborn`: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Scikit-plot`: [https://scikit-plot.readthedocs.io/](https://scikit-plot.readthedocs.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Scikit-learn`: [https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Keras`: [https://github.com/keras-team/keras](https://github.com/keras-team/keras)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorFlow`: [http://tensorflow.org/](http://tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorBoard`: [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PyTorch`: [https://pytorch.org/](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorboardX is a TensorBoard interface for other deep learning frameworks apart
    from TensorFlow (PyTorch, Chainer, MXNet, and others), available at [https://github.com/lanpa/tensorboardX](https://github.com/lanpa/tensorboardX).
  prefs: []
  type: TYPE_NORMAL
- en: It should probably be noted that scikit-plot is not maintained anymore. For
    the plotting of machine learning metrics and charts, mlxtend is a good option,
    at [http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other libraries we used here and that we will encounter throughout this
    book include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Matplotlib`: [https://matplotlib.org/](https://matplotlib.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NumPy`: [https://docs.scipy.org/doc/numpy](https://docs.scipy.org/doc/numpy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SciPy`: [https://docs.scipy.org/doc/scipy/reference](https://docs.scipy.org/doc/scipy/reference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`: [https://pandas.pydata.org/pandas-docs/stable](https://pandas.pydata.org/pandas-docs/stable)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following recipe, we'll get to grips with a more realistic example in
    Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will load a dataset and then we will conduct **exploratory data analysis** (**EDA**),
    such as visualizing the distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do typical preprocessing tasks such as encoding categorical variables,
    and normalizing and rescaling for neural network training. We will then create a simple neural network model in
    Keras, train the model plotting using a generator, and plot the training and validation performance. We will
    look at a still quite simple dataset: the dult dataset from the UCI machine learning repository.
    With this dataset (also known as the Census Income dataset), the goal is to predict from census data
    whether someone earns more than US$50,000 per year.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have a few categorical variables, we'll also deal with the encoding
    of categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is still an introductory recipe, we''ll go through this problem
    with a lot of detail for illustration. We''ll have the following parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data loading and preprocessing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the datasets
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspecting the data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorical encoding
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plotting variables and distributions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plotting correlations
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Label encoding
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalizing and scaling
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving the preprocessed data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing a data generator
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plotting the performance
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting performance metrics
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating feature importances
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll need a few libraries for this recipe in addition to the libraries we
    installed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '`category_encoders` for the encoding of categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minepy` for information-based correlation measures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eli5` for the inspection of black-box models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've used Seaborn before for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can install these libraries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'As a note to you, the reader: if you use `pip` and `conda` together, there
    is a danger that some of the libraries might become incompatible, creating a broken
    environment. We''d recommend using `conda` when a version of `conda` is available,
    although it is usually faster to use `pip`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is already split into training and test. Let''s download the dataset
    from UCI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '`wget` doesn''t ship with macOS by default; we suggest installing `wget` using
    `brew` ([https://formulae.brew.sh/formula/wget](https://formulae.brew.sh/formula/wget)). On
    Windows, you can visit the two preceding URLs and download both via the File menu.
    Make sure you remember the directory where you save the files, so you can find
    them later. There are a few alternatives, however:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the download script we provide in [Chapter 2](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml),
    *Advanced Topics in Supervised Machine Learning*, in the *Predicting house prices
    in PyTorch* recipe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can install the `wget` library and run `import wget; wget.download(URL,
    filepath)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have the following information from the UCI dataset description page:'
  prefs: []
  type: TYPE_NORMAL
- en: '- age: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,
    State-gov, Without-pay, Never-worked.'
  prefs: []
  type: TYPE_NORMAL
- en: '- fnlwgt: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm,
    Assoc-voc, 9th, 7th-th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.'
  prefs: []
  type: TYPE_NORMAL
- en: '- education-num: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed,
    Married-spouse-absent, Married-AF-spouse.'
  prefs: []
  type: TYPE_NORMAL
- en: '- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial,
    Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing,
    Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.'
  prefs: []
  type: TYPE_NORMAL
- en: '- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.'
  prefs: []
  type: TYPE_NORMAL
- en: '- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.'
  prefs: []
  type: TYPE_NORMAL
- en: '- sex: Female, Male.'
  prefs: []
  type: TYPE_NORMAL
- en: '- capital-gain: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- capital-loss: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- hours-per-week: continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '- native-country: United-States, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fnlwgt` actually stands for the final weight; in other words, the total number
    of people constituting the entry.'
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that this dataset is a well-known dataset that has been
    used many times in scientific publications and in machine learning tutorials.
    We are using it here to go over some basics in Keras without having to focus on
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we've mentioned before, we'll first load the dataset, do some EDA, then create
    a model in Keras, train it, and look at the performance.
  prefs: []
  type: TYPE_NORMAL
- en: We've split this recipe up into data loading and preprocessing, and secondly,
    model training.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will start by loading the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading the dataset**: In order to load the dataset, we''ll use pandas again.
    We use pandas'' `read_csv()` command as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Now let's look at the data!
  prefs: []
  type: TYPE_NORMAL
- en: '**Inspecting the data**: The beginning of the DataFrame we can see with the
    `head()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aac1b92d-ed03-42d6-a37c-0640dc0561b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we''ll look at the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97723596-e151-4600-844b-e97d23b32359.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first row has 14 nulls and 1 unusable column out of 15 columns. We will
    discard this row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: And it's gone.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical encoding**: Let''s start with category encoding. For EDA, it''s
    good to use ordinal encoding. This means that for a categorical feature, we map
    each value to a distinct number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We are separating *X*, the features, and *y*, the targets, here. The features
    don't contain the labels; that's the purpose of the `drop()` method – we could
    have equally used `del train['50k']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73a4a855-f798-4be9-b4cd-90ed8471b4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: When starting with a new task, it's best to do EDA. Let's plot some of these
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot variables and distributions, use the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3b9e6d9-0e56-41bc-a33b-dfbbb277730e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we''ll look at a pair-plot again. We''ll plot all numerical variables
    against each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed in the previous recipe, the diagonal in the pair-plot shows us
    histograms of single variables – that is, the distribution of the variable – with
    the hue defined by the classes. Here we have orange versus blue (see the legend
    on the right of the following plot). The following subplots on the diagonal show
    scatter plots between the two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5c8972a-b5bb-49cb-b002-1f497047c2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: If we look at the age variable on the diagonal (second row), we see that the
    two classes have a different distribution, although they are still overlapping. Therefore,
    age seems to be discriminative with respect to our target class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that in a categorical plot as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the resulting plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e2aac99-455b-44dd-bd28-def2c5a841c1.png)'
  prefs: []
  type: TYPE_IMG
- en: After this, let's move on to a correlation plot.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plotting correlations**: In order to get an idea of the redundancy between
    variables, we''ll plot a correlation matrix based on the **Maximal Information
    Coefficient** (**MIC**), a correlation metric based on information entropy. We''ll
    explain the MIC at the end of this recipe.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since the MIC can take a while to compute, we''ll take the parallelization
    pattern we introduced   earlier. Please note the creation of the thread pool and
    the `map` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This can still take a while, but should be much faster than doing the computations
    in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize the correlation matrix as a heatmap: since the matrix is symmetric,
    here, we''ll only show the lower triangle and apply some nice styling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0db7638-366d-46f9-8058-cd18f98ea38a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see in the correlation matrix heatmap that most pair correlations are
    pretty low (most correlations are below 0.4), meaning that most features are relatively
    uncorrelated; however, there is one pair of variables that stands out, those of
    `education-num` and `education`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The output is `0.9995095286140694`.
  prefs: []
  type: TYPE_NORMAL
- en: This is about as close to a perfect correlation as it can get. These two variables
    do in fact refer to the same information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the variance in `education-num` for each value in `education`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We only see zeros. There's no variance. In other words, each value in `education`
    corresponds to exactly one value in `education-num`. The variables are exactly
    the same! We should be able to remove one of them, for example with `del train['education']`,
    or just ignore one of them during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UCI description page mentions missing variables. Let''s look for missing
    variables now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: We only see `False` for each variable, so we cannot see any missing values here.
  prefs: []
  type: TYPE_NORMAL
- en: In neural network training, for categorical variables, we have the choice of either using embeddings
    (we'll get to these in [Chapter 10](955f3e81-9e58-483b-bc9d-b23981325b62.xhtml),
    *Natural Language Processing*) or feeding them as one-hot encodings; this means
    that each factor, each possible value, is encoded in a binary variable that indicates
    whether it is given or not. Let's try one-hot encodings for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, first, let''s re-encode the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `x_cleaned_cols` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5289bf87-d1f6-4a83-9abc-dd74dc3ead59.png)'
  prefs: []
  type: TYPE_IMG
- en: After this, it's time to encode our labels.
  prefs: []
  type: TYPE_NORMAL
- en: '**Label encoding**: We are going to encode target values in two columns as
    1 if present and 0 if not present. It is good to remember that the Python truth
    values correspond to 0 and 1, respectively, for false and true. Since we have
    a binary classification task (that is, we only have two classes), we can use 0
    and 1 in a single output. If we have more than two classes, we''d have to use
    categorical encoding for the output, which typically means we use as many output
    neurons as we do classes. Often, we have to try different solutions in order to
    see what works best.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code block, we just made a choice and stuck with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '**Normalizing and scaling**: We have to convert all values to z-values. This
    is when we subtract the mean and divide by the standard deviation, in order to
    get a normal distribution with a mean of 0.0 and a standard deviation of 1.0\.
    It''s not necessary to have normal distributions for neural network input. However,
    it''s important that numerical values are scaled to the sensitive part of the
    neural network activation functions. Converting to z-scores is a standard way
    to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '**Saving our preprocessing**: For good practice, we save our datasets and the
    transformers so we have an audit trail. This can be useful for bigger projects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to train now.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll create the model, train it, plot performance, and then calculate the feature
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the model, we use the** `Sequential`** model type again. Here''s
    our network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the Keras model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eff395f3-5357-4681-8c71-d02b6e9e02c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's write a data generator. To make this a bit more interesting, we will
    use a generator this time to feed in our data in batches. This means that we stream
    in our data instead of putting all of our training data into the `fit()` function
    at once. This can be useful for very big datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll use the `fit_generator()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: If we had not done our preprocessing already, we could put it into this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our data generator, we can train our model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: This should be relatively quick since this is a small dataset; however, if you
    find that this takes too long, you can always reduce the dataset size or the number
    of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: We have the output from the training, such as loss and metrics, in our `history`
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time we will plot the training progress over epochs from the Keras training
    history instead of using TensorBoard. We didn''t do validation, so we will only
    plot the training loss and training accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Please note that in some versions of Keras, accuracy is stored as `accuracy`
    rather than `acc` in the history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the resulting graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a08f66b-0b15-49e0-8580-50a05b7c8e4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Over the training epochs, the accuracy is increasing while the loss is decreasing,
    so that's good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''ve already one-hot encoded and scaled our test data, we can directly
    predict and calculate our performance. We will calculate the **AUC** (**area-under-the-curve**) score
    using sklearn''s built-in functions. The AUC score comes from the receiver operating
    characteristics, which is a visualization of the false positive rate (also called
    the false alarm rate) on the *x* axis, against the true positive rate (also called
    the hit rate) on the *y* axis. The integral under this curve, the AUC score, is
    a popular measure of classification performance and is useful for understanding
    the trade-off between a high hit rate and any false alarms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: We get `0.7579310072282265` as the AUC score. An AUC score of 76% can be a good
    or bad score depending on the difficulty of the task. It's not bad for this dataset,
    but we could probably improve the performance by tweaking the model more. However,
    for now, we'll leave it as it is here.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are going to check the feature importances. For this, we are going
    to use the `eli5` library for black-box permutation importance. Black-box permutation
    importance encompasses a range of techniques that are model-agnostic, and, roughly
    speaking, permute features in order to establish their importance. You can read
    more about permuation importance in the *How it works...* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this to work, we need a scoring function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can print the feature importances in sorted order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain something like the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb2a0851-b9a9-4b7e-881c-c89f9b3c86b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Your final list might differ from the list here. The neural network training
    is not deterministic, although we could have tried to fix the random generator
    seed. Here, as we've expected, age is a significant factor; however, some categories
    in relationship status and marital status come up before age.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We went through a typical process in machine learning: we loaded a dataset,
    plotted and explored it, and did preprocessing with the encoding of categorical
    variables and normalization. We then created and trained a neural network model
    in Keras, and plotted the training and validation performance. Let''s talk about
    what we did in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Maximal information coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many ways to calculate and plot correlation matrices, and we'll see
    some more possibilities in the recipes to come. Here we've calculated correlations
    based on the **maximal information coefficient** (**MIC**). The MIC comes from
    the framework of *maximal information-based nonparametric exploration*. This was
    published in *Science* *Magazine* in 2011, where it was hailed as the correlation
    metric of the 21st century (the article can be found at [https://science.sciencemag.org/content/334/6062/1518.full](https://science.sciencemag.org/content/334/6062/1518.full)).
  prefs: []
  type: TYPE_NORMAL
- en: Applied to two variables, *X* and *Y*, it heuristically searches for bins in
    both variables, so that the mutual information between *X* and *Y* given the bins
    is maximal. The coefficient ranges between 0 (no correlation) and 1 (perfect correlation).
    It has an advantage with respect to the Pearson correlation coefficient, firstly
    in that it finds correlations that are non-linear, and secondly that it works
    with categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Data generators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with Python generators, you won't need an explanation for
    what this is, but maybe a few clarifying words are in order. Using a generator
    gives the possibility of loading data **on-demand** or **on-line**, rather than
    at once. This means that you can work with datasets much larger than your available
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Some important terminology for generators in neural networks and Keras is as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: '*Iterations* (`steps_per_epoch`) are the number of batches needed to complete
    one epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *batch size* is the number of training examples in a single batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are different ways to implement generators with Keras, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using any Python generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing `tensorflow.keras.utils.Sequence`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the first option, we can use any generator really, but this uses a function
    with yield. This means we're providing the `steps_per_epoch` parameter for the
    Keras `fit_generator()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the second, we write a class that inherits from `tensorflow.keras.utils.Sequence`,
    which implements the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`len()`, in order for the `fit_generator()` function to know how much more
    data is to come. This corresponds to `steps_per_epoch` and is ![](img/0a609a51-8332-480d-a92d-78bc4b0077ac.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__()`, for the `fit_generator` to ask for the next batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_epoch_end()` to do some shuffling or other things at the end of an epoch
    – this is optional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For simplicity, we've taken the former approach.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see later that batch data loading using generators is often a part of
    online learning, that is, the type of learning where we incrementally train a
    model on more and more data as it comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Permutation importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `eli5` library can calculate permutation importance, which measures the
    increase in the prediction error when features are not present. It's also called
    the **mean decrease accuracy** (**MDA**). Instead of re-training the model in
    a leave-one-feature-out fashion, the feature can be replaced by random noise.
    This noise is drawn from the same distribution as the feature so as to avoid distortions.
    Practically, the easiest way to do this is to randomly shuffle the feature values
    between rows. You can find more details about permutation importance in Breiman's *Random
    Forests* (2001), at [https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf](https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll cover a lot more about Keras, the underlying TensorFlow library, online
    learning, and generators in the recipes to come. I'd recommend you get familiar
    with layer types, data loaders and preprocessors, losses, metrics, and training
    options. All this is transferable to other frameworks such as PyTorch, where the
    **application programming interface** (**API**) differs; however, the essential
    principles are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are links to the documentation for TensorFlow/Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: Layer types: [https://www.tensorflow.org/api_docs/Python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data loading: [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Losses: [https://www.tensorflow.org/api_docs/Python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics: [https://www.tensorflow.org/api_docs/Python/tf/keras/metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training: [https://www.tensorflow.org/guide/keras/train_and_evaluate](https://www.tensorflow.org/guide/keras/train_and_evaluate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the Keras/TensorFlow combination and PyTorch provide a lot of interesting
    functionality that's beyond the scope of this recipe – or even this book. To name
    just a few, PyTorch has automatic differentiation functionality (in the form of
    autograd, with more info at [https://pytorch.org/docs/stable/autograd.html](https://pytorch.org/docs/stable/autograd.html)),
    and TensorFlow has an estimator API, which is an abstraction similar to Keras
    (for more detail on this, see [https://www.tensorflow.org/guide/estimator](https://www.tensorflow.org/guide/estimator)).
  prefs: []
  type: TYPE_NORMAL
- en: For information on `eli5`, please visit its website at [https://eli5.readthedocs.io/.](https://eli5.readthedocs.io/)
  prefs: []
  type: TYPE_NORMAL
- en: 'For more datasets, the following three websites are your friends:'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI machine learning datasets: [http://archive.ics.uci.edu/ml/datasets](http://archive.ics.uci.edu/ml/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaggle datasets: [https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Dataset Search: [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
