- en: 3 Getting Started with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 使用LangChain入门
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区Discord
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file19.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![二维码描述自动生成](img/file19.png)'
- en: 'In this chapter, we''ll first set up **LangChain** and the libraries needed
    for this book giving instructions for common dependency management tools such
    as **Docker**, **Conda**, **Pip**, and **Poetry**. Then we''ll go through model
    integrations that we can use such as **OpenAI''s** **Chatgpt**, models on Huggingface
    and Jina AI, and others. We''ll introduce, set up, and work with a few providers
    in turn. We''ll get an API key tokens and then do a short practical example. This
    will give us a bit more context at using **LangChain**, and introduce tips and
    tricks for using it effectively. As the final part, we''ll develop a **LangChain**
    application, a practical example that illustrate a way that **LangChain** can
    be applied in a real-world business use case in customer service.The main sections
    are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先设置**LangChain**和本书所需的库，提供对通用依赖管理工具的说明，例如**Docker**，**Conda**，**Pip**和**Poetry**。然后，我们将逐个介绍、设置和使用我们可以使用的模型集成，例如**OpenAI**的**Chatgpt**，Huggingface上的模型和Jina
    AI，等等。我们将获取API密钥令牌，然后进行一个简短的实际示例。这将为我们提供更多关于有效使用**LangChain**的上下文，并介绍使用它的技巧和窍门。作为最后部分，我们将开发一个**LangChain**应用程序，这是一个实际示例，它说明了**LangChain**如何在客户服务的实际业务用例中应用。主要章节包括：
- en: How to Set Up **LangChain**?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设置**LangChain**？
- en: Model Integrations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成
- en: Customer Service Helper
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户服务助手
- en: We'll start off the chapter by setting up **LangChain** on your computer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从在您的计算机上设置**LangChain**开始本章。
- en: How to Set Up LangChain?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何设置LangChain？
- en: 'In this book, we are talking about LangChain. We can install LangChain by simply
    typing `pip install langchain` from a terminal however, in this book, we''ll also
    be using a variety of other tools and integrations in a few different use cases.
    In order to make sure, all the examples and code snippets work as intended and
    they don''t just work on my machine, but for anyone installing this, I am providing
    different ways to set up an environment.There are various approaches to setting
    up a Python environment. Here, we describe four popular methods for installing
    related dependencies: Docker, Conda, Pip, and Poetry. In case you encounter issues
    during the installation process, consult the respective documentation or raise
    an issue on the Github repository of this book. The different installations have
    been tested at the time of the release of this book, however, things can change,
    and we will update the Github readme online to include workarounds for possible
    problems that could arise.Please find a `Dockerfile` for docker, a `requirements.txt`
    for pip a `pyproject.toml` for poetry and a `langchain_ai.yml` file for **Conda**
    in the book''s repository at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)Let''s
    set up our environment starting with Python.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们谈论的是LangChain。我们可以通过在终端中简单地输入`pip install langchain`来安装LangChain，但是在这本书中，我们也将在几种不同的用例中使用各种其他工具和集成。为了确保所有的示例和代码片段都能按预期工作，并且不只是在我的机器上能够工作，而是对于任何安装这个的人都能够工作，我提供了设置环境的不同方法。设置Python环境有各种不同的方法。在这里，我们描述了四种安装相关依赖的流行方法：Docker，Conda，Pip和Poetry。如果您在安装过程中遇到问题，请查阅各自的文档或在本书的Github仓库上提出问题。在发布本书时，已经测试了不同的安装方法，然而，事情是会变的，我们将更新Github上的readme，包括可能出现问题的解决方法。请在本书的仓库[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)中找到`Dockerfile`用于docker，`requirements.txt`用于pip，`pyproject.toml`用于poetry，以及`langchain_ai.yml`文件用于**Conda**。让我们从Python开始设置我们的环境。
- en: Python installation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python安装
- en: 'Before setting up a Python environment and installing related dependencies,
    you should usually have Python itself installed. I assume, most people who have
    bought this book will have Python installed, however, just in cases, let''s go
    through it. You may download the latest version from python.org for your operating
    system or use your platform''s package manager. Let''s see this with Homebrew
    for MacOS and apt-get for Ubuntu.On MacOS, with Homebrew, we can do:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置Python环境和安装相关依赖之前，您通常应该安装Python本身。我假设购买本书的大多数人都已经安装了Python，但是以防万一，让我们来看看。您可以从python.org下载适合您操作系统的最新版本，或者使用您平台的包管理器。让我们来看看MacOS上，使用Homebrew可以这样做：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For Ubuntu, with apt-get we can do:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Ubuntu，我们可以使用 apt-get：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Tip**: If you are new to programming or Python, it is advised to follow some
    beginner-level tutorials before proceeding with LangChain and the applications
    in this book.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示**：如果您是编程或 Python 新手，请在继续使用 LangChain 和本书中的应用程序之前，建议您先阅读一些初学者级别的教程。'
- en: An important tool for interactively trying out data processing and models is
    the Jupyter notebook and the lab. Let's have a look at this now.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用于交互式尝试数据处理和模型的重要工具是 Jupyter 笔记本和 lab。让我们现在来看一下这个。
- en: Jupyter Notebook and JupyterLab
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jupyter Notebook 和 JupyterLab
- en: Jupyter Notebook and JupyterLab are open-source web-based interactive environments
    for creating, sharing, and collaborating on computational documents. They enable
    users to write code, display visualizations, and include explanatory text in a
    single document called a notebook. The primary difference between the two lies
    in their interface and functionality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 和 JupyterLab 是用于创建、共享和协作计算文档的开源基于 Web 的交互式环境。它们使用户能够在单个文档中编写代码、显示可视化效果并包含解释性文本。两者之间的主要区别在于它们的界面和功能。
- en: '**Jupyter Notebook** aims to support various programming languages like Julia,
    Python, and R - in fact, the project name is a reference to these three languages.
    Jupyter Notebook offers a simple user interface that allows users to create, edit,
    and run notebooks with a linear layout. It also supports extensions for additional
    features and customization.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook** 的目标是支持 Julia、Python 和 R 等各种编程语言 - 实际上，项目名称是对这三种语言的参考。Jupyter
    Notebook 提供了一个简单的用户界面，允许用户以线性布局创建、编辑和运行笔记本。它还支持用于额外功能和定制的扩展。'
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**JupyterLab**, on the other hand, is an enhanced version of Jupyter Notebook.
    Introduced in 2018, JupyterLab offers a more powerful and flexible environment
    for working with notebooks and other file types. It provides a modular, extensible,
    and customizable interface where users can arrange multiple windows (for example,
    notebooks, text editors, terminals) side-by-side, facilitating more efficient
    workflows.'
  id: totrans-22
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一方面，**JupyterLab** 是 Jupyter Notebook 的增强版本。JupyterLab 于 2018 年推出，提供了一个更强大、更灵活的环境，用于处理笔记本和其他文件类型。它提供了一个模块化、可扩展和可定制的界面，用户可以在其中并排排列多个窗口（例如笔记本、文本编辑器、终端），从而促进更高效的工作流程。
- en: 'You can start up a notebook server on your computer from the terminal like
    this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像这样从终端在您的计算机上启动一个笔记本服务器：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see your browser opening a new tab with the Jupyter notebook like
    this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到您的浏览器打开一个新选项卡，并显示 Jupyter 笔记本，类似于这样：
- en: '![Figure 3.1: Jupyter Notebook with a LangChain Agent.](img/file20.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：带有 LangChain 代理的 Jupyter Notebook。](img/file20.png)'
- en: 'Figure 3.1: Jupyter Notebook with a LangChain Agent.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：带有 LangChain 代理的 Jupyter Notebook。
- en: 'Alternatively, we can also use JupyterLab, the next-generation notebook server
    that brings significant improvements in usability. You can start up a JupyterLab
    notebook server from the terminal like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用 JupyterLab，这是下一代笔记本服务器，它在可用性方面带来了显著的改进。您可以像这样从终端启动 JupyterLab 笔记本服务器：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should see something like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似于这样的东西：
- en: '![Figure 3.2: Jupyter Lab with a LangChain Agent.](img/file21.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：带有 LangChain 代理的 Jupyter Lab。](img/file21.png)'
- en: 'Figure 3.2: Jupyter Lab with a LangChain Agent.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：带有 LangChain 代理的 Jupyter Lab。
- en: Either one of these two, the `Jupyter notebook` or `JupyterLab`, will give you
    an **integrated development environment** (**IDE**) to work on some of the code
    that we'll be introducing in this book. After installing Python and the notebook
    or lab, let's quickly explore the differences between dependency management tools
    (**Docker**, **Conda**, **Pip**, and **Poetry**) and use them to fully set up
    our environment for our projects with LangChain!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个之一，`Jupyter notebook` 或 `JupyterLab`，都将为您提供一个集成开发环境（**IDE**），用于处理本书中将介绍的一些代码。在安装
    Python 和笔记本或 lab 之后，让我们快速探索依赖管理工具（**Docker**、**Conda**、**Pip** 和 **Poetry**）之间的差异，并使用它们完全设置我们的环境，用于
    LangChain 项目！
- en: Environment management
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境管理
- en: 'Before we explore various methods to set up a Python environment for working
    with generative models in **LangChain**, it''s essential to understand the differences
    between primary dependency management tools: **Docker**, **Conda**, **Pip**, and
    **Poetry**. All four are tools widely used in the realm of software development
    and deployment.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索各种设置 Python 环境以与 **LangChain** 中的生成模型一起工作的方法之前，了解主要依赖管理工具之间的差异是至关重要的：**Docker**、**Conda**、**Pip**
    和 **Poetry**。这四个工具在软件开发和部署领域被广泛使用。
- en: '**Docker** is an open-source platform that provides OS-level virtualization
    through containerization. It automates the deployment of applications inside lightweight,
    portable containers, which run consistently on any system with Docker installed.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Docker**是一个开源平台，通过容器化提供操作系统级别的虚拟化。它自动化了轻量级、便携式容器内应用程序的部署，在任何安装了Docker的系统上都可以一致地运行。'
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conda** is a cross-platform package manager and excels at installing and
    managing packages from multiple channels, not limited to Python. Geared predominantly
    toward data science and machine learning projects, it can robustly handle intricate
    dependency trees, catering to complex projects with numerous dependencies.'
  id: totrans-38
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Conda**是一个跨平台的包管理器，擅长于从多个通道安装和管理软件包，不局限于Python。它主要面向数据科学和机器学习项目，并且可以强大地处理复杂依赖树，适用于具有大量依赖项的复杂项目。'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Pip** is the most commonly used package manager for Python, allowing users
    to install and manage third-party libraries easily. However, Pip has limitations
    when handling complex dependencies, increasing the risk of dependency conflicts
    arising during package installation.'
  id: totrans-40
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Pip** 是Python中最常用的包管理器，可以轻松安装和管理第三方库。但是，在处理复杂依赖关系时，Pip存在一定的限制，增加了依赖冲突在包安装过程中出现的风险。'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Poetry** is a newer package manager that combines the best features of both
    Pip and Conda. Boasting a modern and intuitive interface, robust dependency resolution
    system, and support for virtual environments creation, Poetry offers additional
    functionalities such as dependency isolation, lock files, and version control.'
  id: totrans-42
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Poetry**是一个结合了Pip和Conda两种最佳特点的新型包管理器。Poetry具有现代化和直观的界面、强大的依赖关系解决系统以及虚拟环境创建的支持，同时提供了额外的功能，如依赖隔离、锁定文件和版本控制。'
- en: 'Poetry and Conda both streamline virtual environment management, whereas working
    with Pip typically involves utilizing a separate tool like virtualenv. Conda is
    the installation method recommended here. We''ll provide a requirements file for
    pip as well and instructions for poetry, however, some tweaking might be required
    in a few cases.We''ll go through installation with these different tools in turn.
    For all instructions, please make sure you have the book''s repository downloaded
    (using the Github user interface) or cloned on your computer, and you''ve changed
    into the project''s root directory.Here''s how you can find the download option
    on Github:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry和Conda都简化了虚拟环境管理，而使用Pip通常需要利用类似于virtualenv的单独工具。在这里推荐使用Conda进行安装。我们还提供了一个pip的requirements文件和使用poetry的说明，但在一些情况下可能需要进行一些调整。我们将依次介绍使用这些不同工具的安装过程。对于所有说明，请确保您已经下载了本书的存储库（使用Github用户界面）或在计算机上克隆了存储库，并且已切换到项目的根目录。下面是如何在Github上找到下载选项：
- en: '![Figure 3.3: Download options in the Github User Interface (UI).](img/file22.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3：Github用户界面（UI）中的下载选项。](img/file22.png)'
- en: 'Figure 3.3: Download options in the Github User Interface (UI).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：Github用户界面（UI）中的下载选项。
- en: 'If you are new to git, you can press **Download ZIP**, and then unzip the archive
    using your favorite tool.Alternatively, to clone the repository using git and
    change to the project directory, you can type the following commands:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是git的新手，可以按“Download ZIP”下载并使用您最喜欢的工具解压缩存档。或者，使用git克隆该存储库并进入项目目录，可以输入以下命令：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have the repository on our machine, let's start with Docker!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在计算机上拥有了该存储库，让我们从Docker开始！
- en: Docker
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Docker
- en: 'Docker is a platform that enables developers to automate deployment, packaging,
    and management of applications. Docker uses containerization technology, which
    helps standardize and isolate environments. The advantage of using a container
    is that it protects your local environment from any - potentially unsafe - code
    that you run within the container. The downside is that the image might require
    time to build and might require around 10 Gigabytes in storage capacity.Similar
    to the other tools for environment management, Docker is useful because you can
    create a reproducible environment for your project. You can use Docker to create
    an environment with all the libraries and tools you need for your project, and
    share that environment with others.To start with Docker, follow these steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个平台，使开发人员能够自动化应用程序的部署、打包和管理。Docker 使用容器化技术，帮助标准化和隔离环境。使用容器的好处是可以保护你的本地环境免受容器内运行的任何可能不安全的代码的影响。缺点是镜像可能需要时间来构建，并且可能需要大约
    10 GB 的存储容量。与其他环境管理工具类似，Docker 很有用，因为你可以为项目创建可重复的环境。你可以使用 Docker 创建包含你项目所需的所有库和工具的环境，并与其他人共享该环境。要开始使用
    Docker，请按照以下步骤操作：
- en: 'Install Docker on your machine. You can go to the Docker website.in your web
    browser and follow the installation instructions here: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的机器上安装 Docker。你可以在网页浏览器中访问 Docker 网站，并按照这里的安装说明进行安装：[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)
- en: 'In the terminal, run the following command to build the Docker image (please
    note: you need to be in the project root for this to work).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行以下命令来构建 Docker 镜像（请注意：你需要在项目根目录下才能正常工作）。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will pull, the continuumio/miniconda3 image from Docker Hub, and build
    the image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 Docker Hub 拉取 continuumio/miniconda3 镜像，并构建镜像。
- en: 'Start the Docker container interactively using the image created:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以交互方式启动创建的 Docker 容器：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This should start our notebook within the container. We should be able to navigate
    to the `Jupyter Notebook` from your browser. We can find it at this address: `http://localhost:8080/`Let''s
    look at conda next.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在容器内启动我们的笔记本。我们应该能够从浏览器导航到`Jupyter Notebook`。我们可以在这个地址找到它：`http://localhost:8080/`接下来让我们看一下
    conda。
- en: Conda
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Conda
- en: '`Conda` allows users to manage multiple environments for different projects.
    It works with Python, R, and other languages, and helps with the installation
    of system libraries as well by maintaining lists of libraries associated with
    Python libraries.The best way to get started with conda is to install anaconda
    or miniconda by following the instructions from this link: [https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)While
    the `conda` environment takes up less disk space than Docker, starting from anaconda,
    the full environment should still take up about 2.5 Gigabytes. The miniconda setup
    might save you a bit of disk space.There''s also a graphical interface to `conda`,
    Anaconda Navigator, which can be installed on macOS and Windows, and which can
    install any dependencies as well as the `conda` tool from the terminal.Let''s
    continue with the `conda` tool and install the dependencies of this book.To create
    a new environment, execute the following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conda` 允许用户为不同的项目管理多个环境。它与 Python、R 和其他语言一起工作，并通过维护与 Python 库关联的库列表来帮助安装系统库。开始使用
    conda 的最佳方法是按照此链接的说明安装 anaconda 或 miniconda：[https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/)尽管从
    anaconda 开始，`conda` 环境占用的磁盘空间比 Docker 少，但完整环境仍然需要大约 2.5 GB。miniconda 设置可能会节省一些磁盘空间。还有一个图形界面工具
    `conda`，Anaconda Navigator，可以安装在 macOS 和 Windows 上，可以从终端安装任何依赖项以及 `conda` 工具。让我们继续使用
    `conda` 工具并安装本书的依赖项。要创建一个新环境，请执行以下命令：'
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Conda` lets us create environments with lots of different libraries, but also
    different versions of Python. We are using Python 3.10 throughout this book. Activate
    the environment by running:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conda` 让我们可以创建具有许多不同库的环境，还可以使用不同版本的 Python。我们在本书中始终使用 Python 3.10。通过运行以下命令激活环境：'
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is all, we are done. We can see this should be painless and straightforward.
    You can now spin up a `jupyter notebook` or `jupyter lab` within the environment,
    for example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，我们完成了。我们可以看到这应该是简单和直接的。你现在可以在环境中启动一个 `jupyter notebook` 或 `jupyter lab`，例如：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's have a look at pip, an alternative to `conda`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 pip，作为`conda`的一个替代方案。
- en: Pip
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pip
- en: '`Pip` is the default package manager for Python. It allows you to easily install
    and manage third-party libraries. We can install individual libraries, but also
    maintain a full list of Python libraries.If it''s not already included in your
    Python distribution, install pip following instructions on [https://pip.pypa.io/](https://pip.pypa.io/)To
    install a library with pip, use the following command. For example, to install
    the NumPy library, you would use the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pip`是Python的默认包管理器。它允许你轻松安装和管理第三方库。我们可以安装单个库，也可以维护完整的Python库列表。如果它尚未包含在你的Python发行版中，请按照[https://pip.pypa.io/](https://pip.pypa.io/)上的说明安装pip。要使用pip安装库，可以使用以下命令。例如，要安装NumPy库，可以使用以下命令：'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also use `pip` to install a specific version of a library. For example,
    to install version 1.0 of the NumPy library, you would use the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`pip`安装库的特定版本。例如，要安装NumPy库的1.0版本，你可以使用以下命令：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In order to set up a full environment, we can start with a list of requirements
    - by convention, this list is in a file called `requirements.txt`. I''ve included
    this file in the project''s root directory, which lists all essential libraries.You
    can install all the libraries using this command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置一个完整的环境，我们可以从一个要求列表开始——按照惯例，该列表在一个名为`requirements.txt`的文件中。我已经在项目的根目录中包含了这个文件，列出了所有必要的库。你可以使用以下命令安装所有这些库：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Please note, however, as mentioned, that Pip doesn''t take care of the environments.
    Virtualenv is a tool that can help to maintain environments, for example different
    versions of libraries. Let''s see this quickly:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如前面提到的，Pip并不处理环境问题。Virtualenv是一个可以帮助维护环境的工具，比如不同版本的库。我们来快速看一下这个：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's do Poetry next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看Poetry。
- en: Poetry
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Poetry
- en: 'Poetry is a dependency management tool for Python that streamlines library
    installation and version control. Installation and usage is straightforward as
    we''ll see. Here''s a quick run-through of poetry:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry是Python的一个依赖管理工具，可以简化库的安装和版本控制。安装和使用都很简单，我们来看一下。下面是快速的Poetry概述：
- en: Install poetry following instructions on [https://python-poetry.org/](https://python-poetry.org/)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[https://python-poetry.org/](https://python-poetry.org/)上的说明安装poetry。
- en: Run `poetry install` in the terminal (from the project root as mentioned before)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行`poetry install`（来自项目根目录，如前所述）
- en: The command will automatically create a new environment (if you haven't created
    one already) and install all dependencies. This concludes the setup for Poetry.
    We'll get to model providers now.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将自动创建一个新的环境（如果你尚未创建），并安装所有依赖项。这完成了Poetry的设置。现在我们来看模型提供者。
- en: Model Integrations
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型整合
- en: 'Before properly starting with generative AI, we need to set up access to models
    such as **large language models** (**LLMs**) or text to image models so we can
    integrate them into our applications. As discussed in *Chapter 1*, *What are Generative
    Models*, there are various **LLMs** by tech giants, like **GPT-4** by **OpenAI**,
    **BERT** and **PaLM-2** by **Google**, **LLaMA** by **Meta AI**, and many more.With
    the help of **LangChain**, we can interact with all of these, for example through
    **Application Programming Interface** (**APIs**), or we can call open-source models
    that we have downloaded on our computer. Several of these integrations support
    text generation and embeddings. We''ll focus on text generation in this chapter,
    and discuss embeddings, vector databases, and neural search in *Chapter 5*, *Building
    a Chatbot like ChatGPT*.There are many providers for model hosting. For **LLMs**,
    currently, **OpenAI**, **Hugging Face**, **Cohere**, **Anthropic**, **Azure**,
    **Google Cloud Platform Vertex AI** (**PaLM-2**), and **Jina AI** are among the
    many providers supported in **LangChain**, however this list is growing all the
    time. You can all the supported integrations for **LLMs** at [https://integrations.langchain.com/llms](https://integrations.langchain.com/llms)As
    for image models, the big developers include **OpenAI** (**DALL-E**), **Midjourney**,
    Inc. (Midjourney), and Stability AI (**Stable Diffusion**). **LangChain** currently
    doesn''t have out-of-the-box handling of models that are not for text, however,
    its docs describe how to work with Replicate, which also provides an interface
    to Stable Diffusion models.For each of these providers, to make calls against
    their Application Programming Interface (API), you''ll first need to create an
    account and obtain an API key. This is free for all of them. With some of them
    you don''t even have to give them your credit card details.In order to set an
    API key in an environment, in Python we can do:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在正式开始生成式人工智能之前，我们需要设置访问诸如**大型语言模型**（**LLMs**）或文本到图像模型等模型的权限，以便将它们集成到我们的应用程序中。正如*第1章*，*生成模型是什么*中所讨论的那样，有各种各样的**LLMs**由科技巨头提供，比如**OpenAI**的**GPT-4**，**Google**的**BERT**和**PaLM-2**，**Meta
    AI**的**LLaMA**等等。借助**LangChain**的帮助，我们可以与所有这些模型进行交互，例如通过**应用程序编程接口**（**APIs**），或者我们可以调用已在计算机上下载的开源模型。其中一些集成支持文本生成和嵌入。在本章中，我们将专注于文本生成，并在*第5章*，*构建像ChatGPT这样的聊天机器人*中讨论嵌入，向量数据库和神经搜索。模型托管有许多提供商。目前，**OpenAI**，**Hugging
    Face**，**Cohere**，**Anthropic**，**Azure**，**Google Cloud Platform Vertex AI**（**PaLM-2**）和**Jina
    AI**是**LangChain**支持的众多提供商之一，但这个列表一直在不断增加。您可以在[https://integrations.langchain.com/llms](https://integrations.langchain.com/llms)上找到所有支持的**LLMs**的集成。至于图像模型，主要的开发者包括**OpenAI**（**DALL-E**），**Midjourney**，Inc.（Midjourney），和**Stability
    AI**（**Stable Diffusion**）。**LangChain**目前没有针对非文本模型的开箱即用处理，但其文档描述了如何使用Replicate与Stable
    Diffusion模型进行交互。对于这些提供商的每一个，要调用其应用程序编程接口（API），您首先需要创建一个帐户并获取一个API密钥。这对于所有人都是免费的。其中一些甚至无需提供信用卡信息。为了在环境中设置API密钥，在Python中我们可以这样做：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here `OPENAI_API_KEY` is the environment key appropriate for OpenAI. Setting
    the keys in your environment has the advantage that we don''t include them in
    our code.You can also expose these variables from your terminal like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `OPENAI_API_KEY` 是适用于OpenAI的环境密钥。将密钥设置在您的环境中的优点是我们不会将它们包含在我们的代码中。您也可以像这样从终端公开这些变量：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let's go through a few prominent model providers in turn. We'll give an example
    usage for each of them.Let's' start with a Fake LLM that's used for testing so
    we can show the basic idea!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次了解一些著名的模型提供商。我们将为每个模型提供一个示例用法。我们从一个用于测试的Fake LLM开始，这样我们就可以展示基本的概念！
- en: Fake LLM
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fake LLM
- en: The Fake LM is for testing. The LangChain documentation has an example for the
    tool use with LLMs. You can execute this example in either Python directly or
    in a notebook.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 用于测试的Fake LM。LangChain文档中有一个示例，用于与LLMs的工具使用。您可以直接在Python中或笔记本中执行此示例。
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We connect a tool, a Python **Read-Eval-Print Loop** (**REPL**) that will be
    called depending on the output of the **LLM**. The Fake List **LLM** will give
    two responses, `responses`, that won''t change based on the input. We set up an
    agent that makes decisions based on the ReAct strategy that we explained in chapter
    2, Introduction to LangChain (`ZERO_SHOT_REACT_DESCRIPTION`). We run the agent
    with a text, the question "what''s 2 + 2".We can observe how the Fake LLM output,
    leads to a call to the Python Interpreter, which returns 4\. Please note that
    the action has to match the `name` attribute of the tool, the `PythonREPLTool`,
    which is starts like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们连接了一个工具，一个 Python **Read-Eval-Print Loop**（**REPL**），根据**LLM**的输出调用它。Fake
    List **LLM** 将给出两个响应，`responses`，它们不会根据输入而改变。我们设置了一个代理，它基于我们在第二章介绍的 ReAct 策略做出决策，介绍了
    LangChain (`ZERO_SHOT_REACT_DESCRIPTION`)。我们用一个文本运行代理，问题是“2 + 2 等于多少”。我们可以观察到假
    LLM 输出导致对 Python 解释器的调用，它返回 4\. 请注意，操作必须与工具的`name`属性匹配，即`PythonREPLTool`，它的开头是这样的：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The names and descriptions of the tools are passed to the **LLM**, which then
    decides based on the provided information.The output of the Python interpreter
    is passed to the Fake **LLM**, which ignores the observation and returns 4\. Obviously,
    if we change the second response to "`Final Answer: 5`", the output of the agent
    wouldn''t correspond to the question.In the next sections, we''ll make this more
    meaningful by using an actual **LLM** rather than a fake one. One of the first
    providers that anyone will think of at the moment is OpenAI.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的名称和描述被传递给**LLM**，然后根据提供的信息做出决定。Python 解释器的输出被传递给 Fake **LLM**，它忽略观察并返回 4\.
    很明显，如果我们将第二个响应改为“`最终答案：5`”，那么代理的输出就不会对应问题。在接下来的章节中，我们将通过使用真正的**LLM**而不是假的来使其更有意义。目前，任何人首先想到的提供者之一是
    OpenAI。
- en: OpenAI
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI
- en: 'As explained in *Chapter 1*, *What are Generative Models?*, OpenAI is an American
    AI research laboratory that is the current market leader in generative AI models,
    especially LLMs. They offer a spectrum of models with different levels of power
    suitable for different tasks. We''ll see in this chapter how to interact with
    OpenAI models with **LangChain''s** and the OpenAI Python client library. OpenAI
    also offers an Embedding class for text embedding models.We will mostly use OpenAI
    for our applications. There are several models to choose from - each model has
    its own pros, token usage counts, and use cases. The main LLM models are GPT-3.5
    and GPT-4 with different token length. You can see the pricing of different models
    at [https://openai.com/pricing](https://openai.com/pricing)We need to obtain an
    OpenAI API key first. In order to create an API key, follow these steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *第一章*，*生成模型是什么？* 中所解释的，OpenAI 是一家美国人工智能研究实验室，是当前生成式人工智能模型领域的市场领导者，特别是 LLM。他们提供一系列具有不同功率水平的模型，适用于不同的任务。我们将在本章中看到如何通过
    **LangChain** 和 OpenAI Python 客户端库与 OpenAI 模型交互。OpenAI 还为文本嵌入模型提供了 Embedding 类。我们将主要用
    OpenAI 来进行我们的应用程序。有几个模型可供选择 - 每个模型都有其自己的优点、令牌使用计数和用例。主要的 LLM 模型是 GPT-3.5 和 GPT-4，具有不同的令牌长度。您可以在[https://openai.com/pricing](https://openai.com/pricing)看到不同模型的定价我们首先需要获得一个
    OpenAI API 密钥。为了创建一个 API 密钥，请按照以下步骤操作：
- en: You need to create a login at [https://platform.openai.com/](https://platform.openai.com/)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要在[https://platform.openai.com/](https://platform.openai.com/)创建一个登录账号。
- en: Set up your billing information.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置您的计费信息。
- en: You can see the **API keys** under *Personal -> View API Keys*.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在*个人 -> 查看 API 秘钥*下看到**API 秘钥**。
- en: Click **Create new secret key** and give it a **Name**.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建新的秘钥**并给它一个**名称**。
- en: 'Here''s how this should look like on the OpenAI platform:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是在 OpenAI 平台上应该是什么样子：
- en: '![Figure 3.4: OpenAI API platform - Create new secret key.](img/file23.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4: OpenAI API 平台 - 创建新的秘钥。](img/file23.png)'
- en: 'Figure 3.4: OpenAI API platform - Create new secret key.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.4: OpenAI API 平台 - 创建新的秘钥。'
- en: 'After pressing "**Create secret key**", you should see the message "API key
    generated." You need to copy the key to your clipboard and keep it. We can set
    the key as an environment variable (`OPENAI_API_KEY`) or pass it as a parameter
    every time you construct a class for OpenAI calls.We can use the `OpenAI` language
    model class to set up an **LLM** to interact with. Let''s create an agent that
    calculates using this model - I am omitting the imports from the previous example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按下“**创建秘钥**”后，您应该看到消息“API 秘钥已生成”。您需要将秘钥复制到剪贴板上并保存。我们可以将该秘钥设置为环境变量（`OPENAI_API_KEY`），或者每次构造
    OpenAI 调用类时都将其作为参数传递。我们可以使用 `OpenAI` 语言模型类来设置一个**LLM**以进行交互。让我们创建一个使用这个模型进行计算的代理
    - 我省略了前面示例中的导入部分：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We should be seeing this output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到这个输出：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This looks quite promising, I think. Let's move on to the next provider and
    more examples!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当有前途，我认为。让我们继续下一个提供商和更多示例！
- en: Hugging Face
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face
- en: 'Hugging Face is a very prominent player in the NLP space, and has considerable
    traction in open-source and hosting solutions. The company is an American company
    that develops tools for building machine learning applications. Its employees
    develop and maintain the Transformers Python library, which is used for natural
    language processing tasks, includes implementations of state-of-the-art and popular
    models like BERT and GPT-2, and is compatible with **PyTorch**, **TensorFlow**,
    and **JAX**.Hugging Face also provides the Hugging Face Hub, a platform for hosting
    Git-based code repositories, machine learning models, datasets, and web applications,
    which provides over 120k models, 20k datasets, and 50k demo apps (Spaces) for
    machine learning. It is an online platform where people can collaborate and build
    ML together.These tools allow users to load and use models, embeddings, and datasets
    from Hugging Face. The `HuggingFaceHub` integration, for example, provides access
    to different models for tasks like text generation and text classification. The
    `HuggingFaceEmbeddings` integration allows users to work with sentence-transformers
    models.They offer various other libraries within their ecosystem, including Datasets
    for dataset processing, *Evaluate* for model evaluation, *Simulate* for simulation,
    and *Gradio* for machine learning demos.In addition to their products, Hugging
    Face has been involved in initiatives such as the BigScience Research Workshop,
    where they released an open large language model called BLOOM with 176 billion
    parameters. They have received significant funding, including a $40 million Series
    B round and a recent Series C funding round led by Coatue and Sequoia at a $2
    billion valuation. Hugging Face has also formed partnerships with companies like
    Graphcore and Amazon Web Services to optimize their offerings and make them available
    to a broader customer base.In order to use Hugging Face as a provider for your
    models, you can create an account and API keys at [https://huggingface.co/settings/profile](https://huggingface.co/settings/profile)You
    can make the token available in your environment as `HUGGINGFACEHUB_API_TOKEN`.Let''s
    see an example, where we use an open-source model developed by Google, the Flan-T5-XXL
    model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是 NLP 领域中非常突出的参与者，在开源和托管解决方案中具有相当大的影响力。该公司是一家美国公司，开发用于构建机器学习应用程序的工具。其员工开发和维护
    Transformers Python 库，该库用于自然语言处理任务，包括最先进和流行的模型实现，如 BERT 和 GPT-2，并兼容 **PyTorch**、**TensorFlow**
    和 **JAX**。Hugging Face 还提供了 Hugging Face Hub，这是一个托管 Git 的代码库、机器学习模型、数据集和 Web 应用程序的平台，提供超过
    120k 个模型、20k 个数据集和 50k 个演示应用程序（Spaces）用于机器学习。这是一个在线平台，人们可以在其中合作和共同构建 ML。这些工具允许用户加载和使用
    Hugging Face 的模型、嵌入和数据集。例如，`HuggingFaceHub` 集成提供了访问不同模型的功能，例如文本生成和文本分类。`HuggingFaceEmbeddings`
    集成允许用户使用句子转换模型。他们在其生态系统中还提供了各种其他库，包括用于数据集处理的 Datasets，用于模型评估的 *Evaluate*，用于模拟的
    *Simulate* 和用于机器学习演示的 *Gradio*。除了他们的产品之外，Hugging Face 还参与了诸如 BigScience 研究研讨会之类的倡议，他们在其中发布了一个名为
    BLOOM 的开放大型语言模型，具有 1760 亿个参数。他们获得了大量资金，包括 4 亿美元的 B 轮融资和最近由 Coatue 和 Sequoia 领投的
    C 轮融资，估值为 20 亿美元。Hugging Face 还与 Graphcore 和 Amazon Web Services 等公司建立了合作关系，优化了他们的产品，并使其能够面向更广泛的客户群体。为了将
    Hugging Face 用作您的模型提供商，您可以在 [https://huggingface.co/settings/profile](https://huggingface.co/settings/profile)
    创建帐户和 API 秘钥。您可以在环境中将令牌设置为 `HUGGINGFACEHUB_API_TOKEN`。让我们看一个例子，我们使用了由谷歌开发的开源模型，即
    Flan-T5-XXL 模型：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the response "`japan`."The **LLM** takes a text input, a question in
    this case, and returns a completion. The model has a lot of knowledge and can
    come up with answers to knowledge questions. We can also get simple recommendations:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了响应“`japan`”。LLM接受文本输入，这里是一个问题，并返回完成。该模型具有许多知识，并可以回答知识问题。我们还可以获得简单的建议：
- en: Azure
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure
- en: Azure, the cloud computing platform run by Microsoft, integrates with OpenAI
    to provide powerful language models like GPT-3, Codex, and Embeddings. It offers
    access, management, and development of applications and services through their
    global data centers for use cases such as writing assistance, summarization, code
    generation, and semantic search. It provides capabilities such as **software as
    a service** (**SaaS**), **platform as a service** (**PaaS**), and **infrastructure
    as a service** (**IaaS**).Authenticating either through Github or Microsoft credentials,
    we can create an account on Azure under [https://azure.microsoft.com/](https://azure.microsoft.com/)You
    can then create new API keys under *Cognitive Services -> Azure OpenAI*. There
    are a few more steps involved, and personally, I found this process annoying and
    frustrating, and I gave up. After set up, the models should be accessible through
    the `AzureOpenAI()` llm class in **LangChain**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由Microsoft运营的云计算平台Azure与OpenAI集成，提供强大的语言模型，如GPT-3、Codex和Embeddings。它通过全球数据中心提供访问、管理和开发应用程序和服务，用于编写辅助、摘要、代码生成和语义搜索等用例。它提供的功能包括软件即服务(SaaS)、平台即服务(PaaS)和基础设施即服务(IaaS)。通过Github或Microsoft凭据进行身份验证，我们可以在[https://azure.microsoft.com/](https://azure.microsoft.com/)下创建Azure帐户。然后，您可以在*Cognitive
    Services -> Azure OpenAI*下创建新的API密钥。需要进行一些步骤，个人而言，我觉得这个过程很烦人和沮丧，所以我放弃了。设置完成后，模型应通过LangChain中的`AzureOpenAI()`
    LLM类访问。
- en: Google Cloud
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud
- en: 'There are many models and functions available through **Google Cloud Platform**
    (**GCP**) and Vertex its machine learning platform. Google Cloud provide access
    to **LLMs** like **LaMDA**, **T5**, and **PaLM**. Google has also updated the
    Google Cloud **Natural Language** (**NL**) API with a new LLM-based model for
    Content Classification. This updated version offers an expansive pre-trained classification
    taxonomy to help with ad targeting, and content-based filtering. The **NL** API''s
    improved v2 classification model is enhanced with over 1,000 labels and support
    for 11 languages with improved accuracy. For models with GCP, you need to have
    gcloud **command line interface** (**CLI**) installed. You can find the instructions
    here: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)You
    can then authenticate and print a key token with this command from the terminal:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Google Cloud Platform（GCP）和Vertex的机器学习平台，有许多模型和功能可用。 Google Cloud提供访问LLM，如LaMDA、T5和PaLM。Google还通过基于LLM的模型更新了Google
    Cloud自然语言（NL）API，以进行内容分类。这个更新的版本提供了一个广泛的预先训练的分类术语表，以帮助广告定向和基于内容的过滤。NL API的改进版v2分类模型配备有超过1000个标签，并支持11种语言，具有更高的准确性。对于使用GCP的模型，您需要安装gcloud命令行界面（CLI）。您可以在这里找到指令：[https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)然后，您可以使用终端中的此命令进行身份验证并打印密钥令牌：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You also need to enable Vertex for your project. If you haven't enabled it,
    you should get a helpful error message pointing you to the right website, where
    you have to click on "Enable".Let's run a model!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要为您的项目启用Vertex。如果尚未启用，则应收到一条有用的错误消息，指向正确的网站，在该网站上，您必须单击“启用”。让我们运行一个模型！
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We should see this response:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下的响应：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I''ve set verbose to True in order to see the reasoning process of the model.
    It''s quite impressive it comes up with the right response even given a misspelling
    of the name. The step by step prompt instruction is key to the correct answer.There
    are various models available through Vertex such as these:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将verbose设置为True，以便查看模型的推理过程。即使名称拼写错误，它也能给出正确的响应，这相当令人印象深刻。一步一步的提示指令是得出正确答案的关键。Vertex提供了各种模型，例如：
- en: '| **Model** | **Description** | **Properties** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **描述** | **属性** |'
- en: '| text-bison | Fine-tuned to follow natural language instructions | Max input
    token: 8,192Max output tokens: 1,024Training data: Up to Feb 2023 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| text-bison | 被细化以遵循自然语言指令 | 最大输入令牌：8,192最大输出令牌：1,024训练数据：截至2023年2月 |'
- en: '| chat-bison | Fine-tuned for multi-turn conversation | Max input token: 4,096
    Max output tokens: 1,024 Training data: Up to Feb 2023 Max turns : 2,500 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| chat-bison | 经过精细调整以用于多轮对话 | 最大输入标记数：4,096 最大输出标记数：1,024 训练数据： 至2023年2月 最大轮数：2,500
    |'
- en: '| code-bison | Fine-tuned to generate code based on a natural language description
    | Max input token: 4,096 Max output tokens: 2,048 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| code-bison | 经过精细调整，基于自然语言描述生成代码 | 最大输入标记数：4,096 最大输出标记数：2,048 |'
- en: '| codechat-bison | Fine-tuned for chatbot conversations that help with code-related
    questions | Max input token: 4,096 Max output tokens: 2,048 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| codechat-bison | 经过精细调整，用于帮助解决与代码相关的问题的聊天机器人对话 | 最大输入标记数：4,096 最大输出标记数：2,048
    |'
- en: '| code-gecko | Fine-tuned to suggest code completion | Max input tokens: 2,048
    Max output tokens: 64 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| code-gecko | 经过精细调整以建议代码完成  | 最大输入标记数：2,048 最大输出标记数：64 |'
- en: 'Table 3.1: Models available in Vertex Generative AI. You can check out the
    documentation at [https://cloud.google.com/vertex-ai/docs/generative-ai](https://cloud.google.com/vertex-ai/docs/generative-ai)We
    can also generate code. Let''s see if **Code-Bison** model can solve FizzBuzz,
    a common interview question for entry and mid-level software developer positions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：Vertex Generative AI提供的模型。你可以在[https://cloud.google.com/vertex-ai/docs/generative-ai](https://cloud.google.com/vertex-ai/docs/generative-ai)查看文档。我们还可以生成代码。让我们看看**Code-Bison**模型是否能够解决FizzBuzz，这是入门和中级软件开发人员职位的常见面试问题：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We are getting this response:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的响应是：
- en: '[PRE25]python'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE25]python（注：不需要翻译，这是内联代码）'
- en: answer = []
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: answer = []
- en: 'for i in range(1, n + 1):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(1, n + 1):'
- en: 'if i % 3 == 0 and i % 5 == 0:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果i % 3 == 0 and i % 5 == 0：
- en: answer.append("FizzBuzz")
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: answer.append("FizzBuzz")（注：不需要翻译，这是内联代码）
- en: 'elif i % 3 == 0:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则如果i % 3 == 0：
- en: answer.append("Fizz")
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: answer.append("Fizz")（注：不需要翻译，这是内联代码）
- en: 'elif i % 5 == 0:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则如果i % 5 == 0：
- en: answer.append("Buzz")
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: answer.append("Buzz")（注：不需要翻译，这是内联代码）
- en: 'else:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则：
- en: answer.append(str(i))
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: answer.append(str(i))
- en: return answer
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: return answer
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Would you hire code-bison into your team?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你会雇佣Code-Bison加入你的团队吗？
- en: Anthropic
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Anthropic
- en: Anthropic is an AI startup and public-benefit corporation based in the United
    States. It was founded in 2021 by former members of OpenAI, including siblings
    Daniela Amodei and Dario Amodei. The company specializes in developing general
    AI systems and language models with a focus on responsible AI usage. As of July
    2023, Anthropic has raised $1.5 billion in funding. They have also worked on projects
    such as Claude, an AI chatbot similar to OpenAI's ChatGPT, and have conducted
    research on the interpretability of machine learning systems, specifically the
    transformer architecture.Unfortunately, Claude is not available to the general
    public (yet). You need to apply for access to use Claude and set the `ANTHROPIC_API_KEY`
    environment variable.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic是一家总部位于美国的人工智能初创公司和公共利益公司。它由OpenAI的前成员，包括兄弟姐妹Daniela Amodei和Dario Amodei于2021年创立。该公司专注于开发通用人工智能系统和语言模型，并关注负责任的人工智能使用。截至2023年7月，Anthropic已经获得了15亿美元的资金。他们还参与了类似于OpenAI's
    ChatGPT的AI聊天机器人Claude的项目，并对机器学习系统的可解释性进行了研究，特别是变形器架构。不幸的是，Claude目前（尚）不对普通大众开放。你需要申请访问权限来使用Claude，并设置`ANTHROPIC_API_KEY`环境变量。
- en: Jina AI
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jina AI
- en: 'Jina AI, founded in February 2020 by Han Xiao and Xuanbin He, is a German AI
    company based in Berlin that specializes in providing cloud-native neural search
    solutions with models for text, image, audio, and video. Their open-source neural
    search ecosystem enables businesses and developers to easily build scalable and
    highly available neural search solutions, allowing for efficient information retrieval.
    Recently, **Jina AI** launched *Finetuner*, a tool that enables fine-tuning of
    any deep neural network to specific use cases and requirements.The company has
    raised a total of $37.5 million in funding through three rounds, with their most
    recent funding coming from a Series A round in November 2021\. Notable investors
    in **Jina AI** include **GGV Capital** and **Canaan Partners**.You can set up
    a login under [https://cloud.jina.ai/](https://cloud.jina.ai/)On the platform,
    we can set up APIs for different use cases such as image caption, text embedding,
    image embedding, visual question answering, visual reasoning, image upscale, or
    Chinese text embedding.Here, we are setting up a visual question answering API
    with the recommended model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Visual Question Answering API in Jina AI.](img/file24.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Visual Question Answering API in Jina AI.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'We get examples for client calls in Python and cURL, and a demo, where we can
    ask a question. This is cool, unfortunately, these APIs are not available yet
    through **LangChain**. We can implement such calls ourselves by subclassing the
    `LLM` class in **LangChain** as a custom **LLM** interface.Let''s set up another
    chatbot, this time powered by Jina AI. We can generate the API token, which we
    can set as `JINA_AUTH_TOKEN`, at [https://chat.jina.ai/api](https://chat.jina.ai/api)Let''s
    translate from English to French here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can set different temperatures, where a low temperature makes the responses
    more predictable. In this case it makes very little difference. We are starting
    the conversation with a system message clarifying the purpose of the chatbot.Let''s
    ask for some food recommendations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'I am seeing this response in Jupyter:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It ignored the one-word instruction, but I actually liked reading the ideas.
    I think I could try this for my son. With other chatbots, I am getting the suggestion
    of Ratatouille.It's important to understand the difference in LangChain between
    LLMs and Chat Models. LLMs are text completion models that take a string prompt
    as input and output a string completion. Chat models are similar to LLMs but are
    specifically designed for conversations. They take a list of chat messages as
    input, labeled with the speaker, and return a chat message as output. Both LLMs
    and Chat Models implement the Base Language Model interface, which includes methods
    such as `predict()` and `predict_messages()`. This shared interface allows for
    interchangeability between different types of models in applications as well as
    between Chat and LLM models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Replicate
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Established in 2019, Replicate Inc. is a San Francisco-based startup that presents
    a streamlined process to AI developers, where they can implement and publish AI
    models with minimal code input through the utilization of cloud technology. The
    platform works with private as well as public models and enables model inference
    and fine-tuning. The firm, deriving its most recent funding from a Series A funding
    round of which the invested total was $12.5 million, was spearheaded by Andreessen
    Horowitz, and involved the participation of Y Combinator, Sequoia, and various
    independent investors. Ben Firshman, who drove open-source product efforts at
    Docker, and Andreas Jansson, a former machine learning engineer at Spotify, co-founded
    Replicate Inc. with the mutual aspiration to eliminate the technical barriers
    that were hindering the mass acceptance of AI. Consequently, they created Cog,
    an open-source tool that packs machine learning models into a standard production-ready
    container that can run on any current operating system and automatically generates
    an API. These containers can also be deployed on clusters of GPUs through the
    replicate platform. As a result, developers can concentrate on other essential
    tasks, thereby enhancing their productivity.You can authenticate with your Github
    credentials on [https://replicate.com/](https://replicate.com/)If you then click
    on your user icon on the top left, you''ll find the API tokens - just copy the
    API key and make it available in your environment as `REPLICATE_API_TOKEN`. In
    order to run bigger jobs, you need to set up your credit card (under billing).You
    can find a lot of models available at [https://replicate.com/explore](https://replicate.com/explore)Here''s
    a simple example for creating an image:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 成立于2019年，Replicate Inc.是一家总部位于旧金山的初创公司，为AI开发人员提供简化流程，他们可以通过利用云技术以最少的代码输入实现和发布AI模型。该平台可与私人模型和公共模型一起工作，并实现模型推理和微调。该公司最新的资金来自一轮A轮融资，总投资额为1250万美元，由安德森·霍洛维茨（Andreessen
    Horowitz）领投，并参与的还有Y Combinator、Sequoia和各种独立投资者。本·菲舍曼（Ben Firshman）是Docker开源产品负责人，安德烈亚斯·杨松（Andreas
    Jansson）是Spotify的前机器学习工程师，他们共同创立了Replicate Inc.，共同期望消除AI大规模接受的技术障碍。因此，他们创建了Cog，这是一个开源工具，可以将机器学习模型打包成标准的生产就绪容器，可以在任何当前操作系统上运行，并自动生成API。这些容器还可以通过复制平台部署在GPU集群上。结果，开发人员可以集中精力进行其他重要任务，从而提高其生产力。您可以使用Github凭据在[https://replicate.com/](https://replicate.com/)上进行身份验证，然后点击左上角的用户图标，您将找到API令牌
    - 只需复制API密钥并在您的环境中提供为`REPLICATE_API_TOKEN`。为了运行更大的作业，您需要设置您的信用卡（在账单下）。您可以在[https://replicate.com/explore](https://replicate.com/explore)上找到很多可用的模型。这是一个创建图像的简单示例：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'I got this image:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到了这张图片：
- en: '![Figure 3.7: A Book Cover for a book about generative AI with Python - Stable
    Diffusion.](img/file25.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7：一本关于使用Python进行生成性AI的书籍封面 - 稳定扩散。](img/file25.png)'
- en: 'Figure 3.7: A Book Cover for a book about generative AI with Python - Stable
    Diffusion.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：一本关于使用Python进行生成性AI的书籍封面 - 稳定扩散。
- en: I think it's a nice image - is that an AI chip that creates art?Let's see quickly
    how to run a model locally in Huggingface transformers or Llama.cpp!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一张不错的图片 - 那是一个创作艺术的AI芯片吗？让我们快速看看如何在Huggingface transformers或Llama.cpp中在本地运行模型！
- en: Local Models
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地模型
- en: 'We can also run local models from LangChain. Let''s preface this with a note
    of caution: an LLM is big, which means that it''ll take up a lot of space. If
    you have an old computer, you can try hosted services such as google colabs. These
    will let you run on machines with a lot of memory and different hardware including
    Tensor Processing Units (TPUs) or GPUs.Since both these use cases can take very
    long to run or crash the Jupyter notebook, I haven''t included this code in the
    notebook or the dependencies in the setup instructions. I think it''s still worth
    discussing it here. The advantages of running models locally are complete control
    over the model and not sharing any data over the internet.Let''s see this first
    with the transformers library by Hugging Face.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从LangChain运行本地模型。让我们先说明一点注意事项：LLM很大，这意味着它会占用大量空间。如果您有一台老旧的计算机，可以尝试托管服务，例如谷歌colab。这些将让您在具有大量内存和不同硬件（包括张量处理单元（TPU）或GPU）的计算机上运行。由于这两种用例运行时间可能会很长或导致Jupyter笔记本崩溃，我没有在笔记本中包含此代码或在安装说明中包含相关依赖项。我认为在这里讨论这点仍然值得。在Hugging
    Face的transformers库中首先看看如何在本地运行模型，然后再看看这个。
- en: Hugging Face transformers
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hugging Face transformers
- en: 'I''ll quickly show the general recipe of setting up and running a pipeline:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This model is quite small (355 million parameters), but relative performant,
    and instruction tuned for conversations.Please note that we don''t need an API
    token for local models!This will download everything that''s needed for the model
    such as the tokenizer and model weights. We can then run a text completion to
    give us some content for this chapter.In order to plug in this pipeline into a
    LangChain agent or chain, we can use it the same way that we''ve seen before:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this example, we also see the use of a `PromptTemplate` that gives specific
    instructions for the task.Let's do Llama.cpp next.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Llama.cpp
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Llama.cpp is a C++ program that executes models based on architectures based
    on Llama, one of the first large open-source models released by Meta AI, which
    spawned the development of many other models in turn. Please note that you need
    to have an md5 checksum tool installed. This is included by default in several
    Linux distributions such as Ubuntu. On MacOs, you can install it with brew like
    this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We need to download the llama.cpp repository from Github. You can do this online
    choosing one of the download options on Github, or you can use a git command from
    the terminal like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then we need to install the python requirements, which we can do with the pip
    package installer - let''s also switch to the llama.cpp project root directory
    for convenience:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You might want to create a Python environment before you install requirements
    - but this is up to you. Now we need to compile llama.cpp:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can parallelize the build with 4 processes. In order to get the Llama model
    weights, you need to sign up with the T&Cs and wait for a registration email from
    Meta. There are tools such as the llama model downloader in the pyllama project,
    but please be advised that they might not conform with the license stipulations
    by Meta. You can download models from Hugging Face - these models should be compatible
    with llama.cpp, such as Vicuna or Alpaca. Let''s assume you have downloaded the
    model weights for the 7B Llama model into the models/7B directory.You can download
    models in much bigger sizes such as 13B, 30B, 65B, however, a note of caution
    is in order here: these models are fairly big both in terms of memory and disk
    space.We have to convert the model to llama.cpp format, which is called **ggml**,
    using the convert script. Then we can optionally quantize the models to save memory
    when doing inference. Quantization refers to reducing the number of bits that
    are used to store weights.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This last file is much smaller than the previous files and will take up much
    less space in memory as well, which means that you can run it on smaller machines.Once
    we have chosen a model that we want to run, we can integrate it into an agent
    or a chain for example as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: )
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the introduction to model providers. Let's build an application!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Customer Service Helper
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll build a text classification app in LangChain for customer
    service agents. Given a document such as an email, we want to classify it into
    different categories related to intent, extract the sentiment, and provide a summary.Customer
    service agents are responsible for answering customer inquiries, resolving issues
    and addressing complaints. Their work is crucial for maintaining customer satisfaction
    and loyalty, which directly affects a company''s reputation and financial success.
    Generative AI can assist customer service agents in several ways:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为客服代理构建一个文本分类应用程序。给定一个文档，比如一封电子邮件，我们希望将其分类到与意图相关的不同类别中，并提取情绪，并提供摘要。客服代理负责回答客户查询，解决问题和处理投诉。他们的工作对于维护客户满意度和忠诚度至关重要，这直接影响公司的声誉和财务成功。生成式人工智能可以在几个方面帮助客服代理：
- en: 'Sentiment classification: this helps identify customer emotions and allows
    agents to personalize their response.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分类：这有助于识别客户情绪，并允许代理进行个性化回应。
- en: 'Summarization: this enables agents to understand the key points of lengthy
    customer messages and save time.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要：这使代理人能够理解冗长的客户消息的要点，并节省时间。
- en: 'Intent classification: similar to summarization, this helps predict the customer''s
    purpose and allows for faster problem-solving.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图分类：类似于摘要，这有助于预测客户目的，并可以更快地解决问题。
- en: 'Answer suggestions: this provides agents with suggested responses to common
    inquiries, ensuring that accurate and consistent messaging is provided.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案建议：这为代理人提供了针对常见查询的建议回复，确保提供准确而一致的消息。
- en: 'These approaches combined can help customer service agents respond more accurately
    and in a timely manner, ultimately improving customer satisfaction.Here, we will
    concentrate on the first three points. We''ll document lookups, which we can use
    for answer suggestions in *Chapter 5*, *Building a Chatbot like ChatGPT*.**LangChain**
    is a very flexible library with many integrations that can enable us to tackle
    a wide range of text problems. We have a choice between many different integrations
    to perform these tasks. We could ask any LLM to give us an open-domain (any category)
    classification or choose between multiple categories. In particular, because of
    their large training size, LLMs are very powerful models, especially when given
    few-shot prompts, for sentiment analysis that don''t need any additional training.
    This was analyzed by Zengzhi Wang and others in their April 2023 study "Is ChatGPT
    a Good Sentiment Analyzer? A Preliminary Study". A prompt, for an LLM for sentiment
    analysis could be something like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结合方法可以帮助客服代理以更准确、及时地回应，最终提高客户满意度。在这里，我们将集中讨论前三点。我们将记录查找，这些可用于*第五章*中的答案建议，*构建类似ChatGPT的Chatbot*。**LangChain**是一个非常灵活的库，拥有许多整合，使我们能够解决各种文本问题。我们可以选择许多不同的整合来执行这些任务。我们可以要求任何LLM为我们提供开放领域（任何类别）分类或在多个类别之间选择。特别是由于它们庞大的训练量，LLM是非常强大的模型，尤其是在给定少量提示时进行情感分析，而不需要任何额外的训练。这是由Zengzhi
    Wang等人在2023年4月的研究“ChatGPT是一个好的情感分析器吗？初步研究”中进行的分析。LLM进行情感分析的提示可能是这样的：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'LLMs can be also very effective at summarization, much better than any previous
    models. The downside can be that these model calls are slower than more traditional
    ML models and more expensive.If we want to try out more traditional or smaller
    models. Cohere and other providers have text classification and sentiment analysis
    as part of their capabilities. For example, NLP Cloud''s model list includes spacy
    and many others: [https://docs.nlpcloud.com/#models-list](https://docs.nlpcloud.com/#models-list)Many
    Hugging Face models are supported for these tasks including:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在摘要方面也非常有效，比以前的模型好得多。不利的一面是这些模型调用比传统的机器学习模型更慢，而且成本更高。如果我们想尝试传统或较小的模型。Cohere和其他提供商将文本分类和情感分析作为其能力的一部分。例如，NLP
    Cloud的模型列表包括spacy和许多其他模型：[https://docs.nlpcloud.com/#models-list](https://docs.nlpcloud.com/#models-list)许多Hugging
    Face模型都支持这些任务，包括：
- en: document-question-answering
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档问题回答
- en: summarization
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: text-classification
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: text-question-answering
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本-问题-回答
- en: translation
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: 'We can execute these models either locally, by running a `pipeline` in transformer,
    remotely on the Hugging Face Hub server (`HuggingFaceHub`), or as tool through
    the `load_huggingface_tool()` loader.Hugging Face contains thousands of models,
    many fine-tuned for particular domains. For example, `ProsusAI/finbert` is a BERT
    model that was trained on a dataset called Financial PhraseBank, and can analyze
    sentiment of financial text. We could also use any local model. For text classification,
    the models tend to be much smaller, so this would be less of a drag on resources.
    Finally, text classification could also be a case for embeddings, which we''ll
    discuss in *Chapter 5*, *Building a Chatbot like ChatGPT*.I''ve decided to try
    make do as much as I can with smaller models that I can find on Hugging Face for
    this exercise.We can list the 5 most downloaded models on Hugging Face Hub for
    text classification through the huggingface API:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s see the list:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Downloads** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| nlptown/bert-base-multilingual-uncased-sentiment | 5805259 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| SamLowe/roberta-base-go_emotions | 5292490 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| cardiffnlp/twitter-roberta-base-irony | 4427067 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| salesken/query_wellformedness_score | 4380505 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| marieke93/MiniLM-evidence-types | 4370524 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: 'Tablee 3.2: The most popular text classification models on Hugging Face Hub.We
    can see that these models are about small ranges of categories such as sentiment,
    emotions, irony, or well-formedness. Let''s use the sentiment model.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'I am getting this result:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Not a happy camper. Let''s move on!Let''s see the 5 most popular models for
    summarization as well:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Downloads** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| t5-base | 2710309 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| t5-small | 1566141 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| facebook/bart-large-cnn | 1150085 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| sshleifer/distilbart-cnn-12-6 | 709344 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| philschmid/bart-large-cnn-samsum | 477902 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: 'Table 3.3: The most popular summarization models on Hugging Face Hub.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'All these models have a relatively small footprint compared to large models.
    Let''s execute the summarization model remotely on a server:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Please note that you need to have your `HUGGINGFACEHUB_API_TOKEN` set for this
    to work.I am seeing this summary:A customer''s coffee machine arrived ominously
    broken, evoking a profound sense of disbelief and despair. "This heartbreaking
    display of negligence shattered my dreams of indulging in daily coffee perfection,
    leaving me emotionally distraught and inconsolable," the customer writes. "I hope
    this email finds you amidst an aura of understanding, despite the tangled mess
    of emotions swirling within me as I write to you," he adds.This summary is just
    passable, but not very convincing. There''s still a lot of rambling in the summary.
    We could try other models or just go for an LLM with a prompt asking to summarize.
    Let''s move on.It could be quite useful to know what kind of issue the customer
    is writing about. Let''s ask **VertexAI**:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We get "product issues" back, which is correct for the long email example that
    I am using here.I hope it was exciting to see how quickly we can throw a few models
    and tools together in LangChain to get something that looks actually useful. We
    could easily expose this in an interface for customer service agents to see.Let's
    summarize.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we've walked through four distinct ways of installing LangChain
    and other libraries needed in this book as an environment. Then, we've introduced
    several providers of models for text and images. For each of them, we explained
    where to get the API token, and demonstrated how to call a model. Finally, we've
    developed an LLM app for text classification in a use case for customer service.
    By chaining together various functionalities in LangChain, we can help reduce
    response times in customer service and make sure answers are accurate and to the
    point.In the chapters 3 and 4, we'll dive more into use cases such as question
    answering through augmented retrieval using tools such as web searches and chatbots
    relying on document search through indexing. Let's see if you remember some of
    the key takeaways from this chapter!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please have a look to see if you can come up with the answers to these questions.
    I''d recommend you go back to the corresponding sections of this chapter, if you
    are unsure about any of them:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: How do you install LangChain?
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List at least 4 cloud providers of LLMs apart from OpenAI!
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Jina AI and Hugging Face?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you generate images with LangChain?
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you run a model locally on your own machine rather than through a service?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you perform text classification in LangChain?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we help customer service agents in their work through generative AI?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
