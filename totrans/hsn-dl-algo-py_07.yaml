- en: Improvements to the RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The drawback of a **recurrent neural network** (**RNN**) is that it will not
    retain information for a long time in memory. We know that an RNN stores sequences
    of information in its hidden state but when the input sequence is too long, it
    cannot retain all the information in its memory due to the vanishing gradient
    problem, which we discussed in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To combat this, we introduce a variant of RNN called a **long short-term memory**
    (**LSTM**) cell, which resolves the vanishing gradient problem by using a special
    structure called a **gate**. Gates keep the information in memory as long as it
    is required. They learn what information to keep and what information to discard
    from the memory.
  prefs: []
  type: TYPE_NORMAL
- en: We will start the chapter by exploring LSTM and exactly how LSTM overcomes the
    shortcomings of RNN. Later, we will learn how to perform forward and backward
    propagation with an LSTM cell. Going ahead, we will explore how to implement LSTM
    cells in TensorFlow and how to use them to predict a Bitcoin's price.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will get the hang of **gated recurrent unit** (**GRU**) cells,
    which act as a simplified version of the LSTM cell. We will learn how forward
    and backward propagation works in a GRU cell. Next, we will get a basic understanding
    of bidirectional RNNs and how they make use of past and future information to
    make predictions; we will also understand how deep RNNs work.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will learn about the sequence-to-sequence model,
    which maps the input of varying length to the output of varying length. We will
    dive into the architecture of the sequence-to-sequence model and the attention
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure a clear understanding of these topics, the chapter has been organized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM to the rescue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward and backward propagation in LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting Bitcoin price using LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward and backward propagation in GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While backpropagating an RNN, we discovered a problem called **vanishing gradients**.
    Due to the vanishing gradient problem, we cannot train the network properly, and
    this causes the RNN to not retain long sequences in the memory. To understand
    what we mean by this, let''s consider a small sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The sky is __*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN can easily predict the blank as *blue* based on the information it has
    seen, but it cannot cover the long-term dependencies. What does that mean? Let''s
    consider the following sentence to understand the problem better:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Archie lived in China for 13 years. He loves listening to good music. He is
    a fan of comics. He is fluent in ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we were asked to predict the missing word in the preceding sentence,
    we would predict it as *Chinese*, but how did we predict that? We simply remembered
    the previous sentences and understood that Archie lived for 13 years in China.
    This led us to the conclusion that Archie might be fluent in Chinese. An RNN,
    on the other hand, cannot retain all of this information in its memory to say
    that Archie is fluent in Chinese. Due to the vanishing gradient problem, it cannot
    recollect/remember information for a long time in its memory. That is, when the
    input sequence is long, the RNN memory (hidden state) cannot hold all the information.
    To alleviate this, we use an LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM is a variant of an RNN that resolves the vanishing gradient problem and
    retains information in the memory as long as it is required. Basically, RNN cells
    are replaced with LSTM cells in the hidden units, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83fc7036-f6b4-47d1-bc44-02569007c4fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the LSTM cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What makes LSTM cells so special? How do LSTM cells achieve long-term dependency?
    How does it know what information to keep and what information to discard from
    the memory?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all achieved by special structures called **gates**. As shown in the
    following diagram, a typical LSTM cell consists of three special gates called
    the input gate, output gate, and forget gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bed01df-ee73-4281-9a32-73366390c448.png)'
  prefs: []
  type: TYPE_IMG
- en: These three gates are responsible for deciding what information to add, output,
    and forget from the memory. With these gates, an LSTM cell effectively keeps information
    in the memory only as long as required.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an RNN cell, we used the hidden state, ![](img/a670126d-efa7-49a6-9b47-72b41e459bc0.png),
    for two purposes: one for storing the information and the other for making predictions.
    Unlike RNN, in the LSTM cell we break the hidden states into two states, called
    the **cell state** and the **hidden state**:'
  prefs: []
  type: TYPE_NORMAL
- en: The cell state is also called internal memory and is where all the information
    will be stored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden state is used for computing the output, that is, for making predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the cell state and hidden state are shared across every time step. Now,
    we will deep dive into the LSTM cell and see exactly how these gates are used
    and how the hidden state predicts the output.
  prefs: []
  type: TYPE_NORMAL
- en: Forget gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The forget gate, ![](img/e6ab9a8d-a968-4f98-bf96-8996e768b599.png), is responsible
    for deciding what information should be removed from the cell state (memory).
    Consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry* *is a good singer*. *He lives in New York. Zayn is also a good singer*.'
  prefs: []
  type: TYPE_NORMAL
- en: As soon as we start talking about Zayn, the network will understand that the
    subject has been changed from Harry to Zayn, and the information about Harry is
    no longer required. Now, the forget gate will remove/forget information about
    Harry from the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forget gate is controlled by a sigmoid function. At time step ![](img/5eb5937b-5a0a-4fa5-a9f4-a54ee188382a.png),
    we pass input ![](img/4b86f429-7089-4dfb-a350-f87a071613d4.png), and the previous
    hidden state, ![](img/518639c9-dc9e-4590-8132-afb793ff6307.png), to the forget
    gate. It return `0` if the particular information from the cell state should be
    removed and returns 1 if the information should not be removed. The forget gate,
    ![](img/b008e595-73f5-4779-9bb9-f0c13f921f38.png), at a time step, ![](img/b0d40b0d-15e7-41ea-bef2-59f3ca46dcec.png),
    is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e666c2a3-5145-44aa-9f12-9cc4e579e2f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b50ec728-ddb0-4569-bc15-862460d757d3.png)is the input-to-hidden layer
    weights of the forget gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f2d2a796-703e-4706-92b1-4e997698d77c.png)is the hidden-to-hidden layer
    weights of the forget gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5ced3ee9-3893-49af-8838-d6cb212cae67.png)is the bias of the forget
    gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the forget gate. As you can see, input ![](img/c3f4ca14-e495-43c9-b9ca-27920eabfac7.png)
    is multiplied with ![](img/777883d9-25a8-472b-81d7-453ad559c9a3.png) and the previous
    hidden state, ![](img/d9a8a349-d2d9-4691-9599-d96500320a27.png), is multiplied
    with ![](img/c1624be1-c68a-4969-b7a0-f8bc9dc24888.png), then both will be added
    together and sent to the sigmoid function, which returns ![](img/0685de1f-7e90-471e-a062-9d42c26055bb.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21c2e1af-c75a-466b-8f67-e4226799484f.png)'
  prefs: []
  type: TYPE_IMG
- en: Input gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The input gate is responsible for deciding what information should be stored
    in the cell state. Let''s consider the same example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry is a good singer. He lives in New York. Zayn is also a good singer.*'
  prefs: []
  type: TYPE_NORMAL
- en: After the forget gate removes information from the cell state, the input gate
    decides what information it has to keep in the memory. Here, since the information
    about Harry is removed from the cell state by the forget gate, the input gate
    decides to update the cell state with information about *Zayn*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the forget gate, the input gate is controlled by a sigmoid function
    that returns output in the range of 0 to 1\. If it returns `1`, then the particular
    information will be stored/updated to the cell state, and if it returns `0`, we
    will not store the information to the cell state. The input gate ![](img/5dc8f765-46d8-4432-93c9-db97e4a66144.png)
    at time step ![](img/b94192a5-ca93-4dbf-a997-9554df7b2123.png) is expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33badc8d-062a-47a8-acce-0efff9475568.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e52f1a4-475a-4814-8d88-10d81da859ab.png)is the input-to-hidden weights
    of the input gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/68924da4-5fb6-4342-ac3f-4a309454abc4.png)is the hidden-to-hidden weights
    of the input gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8d4321fe-6354-4f24-90dd-92cd3a1e5211.png)is the bias of the input gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the input gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2eaad551-d7b1-4fdd-a0bc-954e90830f75.png)'
  prefs: []
  type: TYPE_IMG
- en: Output gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will have a lot of information in the cell state (memory). The output gate
    is responsible for deciding what information should be taken from the cell state
    to give as an output. Consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Zayn''s debut album was a huge success. Congrats ____*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output gate will look up all the information in the cell state and select
    the correct information to fill the blank. Here, `congrats` is an adjective that
    is used to describe a noun. So, the output gate will predict *Zayn* (noun) to
    fill the blank. Similar to other gates, it is also controlled by a sigmoid function.
    The output gate ![](img/0655737c-c309-48d8-ba3c-32ad669ea0bf.png) at time step
    ![](img/187722bf-21fb-4d17-8dde-39169fa5ac77.png) is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdc58381-1789-40f8-90e4-d975b57d2cd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b76a5820-fcd7-4782-83ee-1dfdd24a3ca0.png)is the input-to-hidden weights
    of the output gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/50f5fe09-5490-45c5-9c4d-9d939cad28ad.png) is the hidden-to-hidden weights
    of the output gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d6cf8ce6-6d74-442f-b5e9-f8ec08c0dbb8.png)is the bias of the output
    gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output gate is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3abb122-fc53-43d3-9e09-6f53d8e6e370.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the cell state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how all three gates work in an LSTM network, but the question
    is, how can we actually update the cell state by adding relevant new information
    and deleting information that is not required from the cell state with the help
    of the gates?
  prefs: []
  type: TYPE_NORMAL
- en: First, we will see how to add new relevant information to the cell state. To
    hold all the new information that can be added to the cell state (memory), we
    create a new vector called ![](img/7010184a-9902-4f50-b3dc-64ff49f06e92.png).
    It is called a **candidate state** or **internal state vector**. Unlike gates
    that are regulated by the sigmoid function, the candidate state is regulated by
    the tanh function, but why? The sigmoid function returns values in the range of
    `o` to `1`, that is, it is always positive. We need to allow the values of ![](img/0693ff3d-4f5e-4004-8865-614ee2553802.png)
    to be either positive or negative. So, we use the tanh function, which returns
    values in the range of `-1` to `+1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The candidate state, ![](img/c3f66b9a-6c7e-4fbb-9a00-6e6e9643b218.png), at
    time ![](img/f553621f-c7c9-4f3d-bd57-674f16d5b2e5.png) is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd5e300f-79f4-480c-92b5-7a53392442e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6aed5e2-ec9f-4077-9175-341c5adf258b.png)is the input-to-hidden weights
    of the candidate state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d96cb484-7d53-4b0b-a81a-c3d05f1ba252.png) is the hidden-to-hidden weights
    of the candidate state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e39f7759-421a-4c62-af06-c6ba16fc8c7f.png)is the bias of the candidate
    state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, the candidate state holds all the new information that can be added to
    the cell state (memory). The following diagram shows the candidate state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31a44855-b4bb-4697-9d1b-43e46f81c937.png)'
  prefs: []
  type: TYPE_IMG
- en: How do we decide whether the information in the candidate state is relevant?
    How do we decide whether to add new information or not from the candidate state
    to the cell state? We learned that the input gate is responsible for deciding
    whether to add new information or not, so if we multiply ![](img/69f0d59a-7e8f-47bb-a52b-c954f49fd889.png)
    and ![](img/31980fd0-d617-4a69-9dd7-3c72700ba834.png), we only get relevant information
    that should be added to the memory.
  prefs: []
  type: TYPE_NORMAL
- en: That is, we know the input gate returns 0 if the information is not required
    and 1 if the information is required. Say ![](img/f1b28985-aaf5-4103-bf2b-582c9929da87.png),
    then multiplying ![](img/50c4bd55-f5c8-49ab-bd1f-a2ee36695ba9.png) and ![](img/83b75b28-d035-496c-8ff6-f30d77937811.png)
    gives us 0, which means the information in ![](img/824cdab3-ee4c-40e6-b090-ce2faabd730e.png)
    is not required and we don't want to update the cell state with ![](img/6a312070-52f1-46dc-9a3d-05455a03adc0.png).
    When ![](img/8dc8a573-bc68-47a1-aa12-294c68c36324.png), then multiplying ![](img/bec8dc05-a2ad-403c-a9c6-69d783388261.png)
    and ![](img/fe5e1426-2690-4cb9-9f00-954ed0822831.png) gives us ![](img/a3f46a0c-153c-474b-a988-eb76ebb2a079.png),
    which implies we can update the information in ![](img/1a5bd9a1-15bc-4e57-a25f-7a72def34a41.png)
    to the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the new information to the cell state with input gate ![](img/6e2dd534-874f-40da-b9e3-88440ae9c253.png)
    and candidate state ![](img/5904f3a8-5a43-49d1-9ce0-71ebee730515.png) is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d8b922a-ead9-4d15-927e-6dcc229f8532.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will see how to remove information from the previous cell state that
    is no longer required.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the forget gate is used for removing information that is not
    required in the cell state. So, if we multiply the previous cell state, ![](img/41f210c4-6880-4e4e-90aa-dee67fd723d6.png),
    and forget gate, ![](img/f52ba505-fe3d-49f2-99a6-0df0238b26a4.png), then we retain
    only relevant information in the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: Say ![](img/08a5d23e-261b-4cb2-a99f-db63239c239f.png), then multiplying ![](img/a9107933-985a-4348-a5d5-bd272786ffe3.png)
    and ![](img/bd8a9753-06d7-4593-aba6-b51903567a26.png) gives us 0, which means
    the information in cell state, ![](img/0df3abae-8597-478a-875f-89dd39594f57.png),
    is not required and should be removed (forgotten). When ![](img/50fa7006-ccca-4b1a-b53b-c3b259b3549f.png),
    then multiplying ![](img/0babfbf6-486a-4e70-b752-5e7009839fc6.png) and ![](img/541215b2-01bf-47eb-bc99-0e1c4226b0d1.png)
    gives ![](img/4d95c71f-bcb4-455a-8245-aa6ba10e7237.png), which implies that information
    in the previous cell state is required and should not be removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing information from the previous cell state, ![](img/ed140ab2-3961-4752-a6b3-c6a0b061939d.png),
    with the forget gate, ![](img/41c44951-0a3a-44e8-9671-34acc507a4ad.png), is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be393d9d-33f7-429d-aeca-a2db2e89517e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, in a nutshell, we update our cell state by multiplying ![](img/8aacb5a6-fc50-4376-bfa3-d7bd6a6be88f.png)
    and ![](img/108eb432-d875-4f0d-91d8-bfbbb53606c2.png) to add new information,
    and multiplying **![](img/7d32cace-bc58-4661-aa73-3fa46a6edaa5.png)** ,and ![](img/e7680c9e-a545-4bc4-9fd5-44215a5ed1eb.png)
    to remove information. We can express the cell state equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df6266fe-ac8b-4679-90fa-3c23aee2a14d.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating hidden state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how the information in the cell state will be updated. Now,
    we will see how the information in the hidden state will be updated. We learned
    that the hidden state, ![](img/09be6d36-046f-4f1c-aac7-be503fe678fb.png), is used
    for computing the output, but how can we compute the output?
  prefs: []
  type: TYPE_NORMAL
- en: We know that the output gate is responsible for deciding what information should
    be taken from the cell state to give as output. Thus, multiplying ![](img/7c5e428a-dd2c-44ec-9839-6f95408c2396.png)
    and tanh (to squash between -1 and +1) of cell state, ![](img/0a08740e-e7f3-49de-b21f-3d13898c2c65.png),
    gives us the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the hidden state, ![](img/b9849aba-ed4b-4a83-97d7-544790335046.png),
    is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/074a4b48-cf46-4633-996d-68c95dbf1c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows how the hidden state, ![](img/fecd1eda-5aa0-43c3-b37b-a46c9a50d939.png),
    is computed by multiplying ![](img/0753373c-df5f-4009-817a-6742f853390a.png) and
    ![](img/0c969a7e-719c-4526-8979-865b3bec2bdf.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/045de488-09e7-4920-9b96-2a2aca3107f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, once we have the hidden state value, we can apply the softmax function
    and compute ![](img/95012735-9e45-42d4-93a2-5d1732452c9b.png) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab5ba5f9-a9a3-42aa-abff-75f6600f1739.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/fa68cd80-0812-475e-82b0-c8313bf3da60.png) is the hidden-to-output
    layer weights.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation in LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting it all together, the final LSTM cell with all the operations is shown
    in the following diagram. Cell state and hidden states are shared across time
    steps, meaning that the LSTM computes the cell state, ![](img/1fbe13e7-ac1c-4577-aba8-10213738f557.png),
    and hidden state, ![](img/a41448bd-7eaa-4c55-af63-23f53b072f96.png), at time step
    ![](img/5e3b3d43-c481-4df0-a75e-8367d659a265.png), and sends it to the next time
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48f7218f-01e9-4109-beb2-3f2449f08570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The complete forward propagation steps in the LSTM cell can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate**: ![](img/cea4e9e7-fb6b-4e0d-9ae7-34e059e6e018.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forget gate**: ![](img/ae23d56d-86db-4dfc-9ffd-a769bfef5a83.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output gate**: ![](img/24d931a3-a0d3-4bfd-9c7f-4f1cae0c3e7f.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Candidate state**: ![](img/371519ee-9035-417e-b657-08ee8e7fc7f1.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cell state**: ![](img/5a4c735f-b059-4e30-bfd5-0a3436436082.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hidden state**: ![](img/0aea9e04-47b6-4d68-a0e5-7ea94594c929.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output**: ![](img/9c7f33b3-1b3b-470c-af11-451d8d885391.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagation in LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We compute the loss at each time step to determine how well our LSTM model
    is predicting the output. Say we use cross-entropy as a loss function, then the
    loss, ![](img/96cbb175-2daf-4cc9-bfab-ff5fcdf1b041.png), at time step ![](img/877e5abb-f731-421a-a531-fee33027608d.png)
    is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1e29a10-9192-4780-a6b7-0521d3c4974c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/8df65662-e615-422c-ae9d-593b19d4e085.png) is the actual output
    and ![](img/d8c35c2e-3ac8-4ed5-b837-88c02616c79d.png) is the predicted output
    at time step ![](img/e5c01070-a571-4262-b25c-329c0a2ea24a.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final loss is the sum of loss at all time steps, and can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e12ed7eb-f381-4809-a198-5989df1f6bc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We minimize the loss using gradient descent. We find the derivative of loss
    with respect to all of the weights used in the network and find the optimal weights
    to minimize the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: We have four inputs-to-hidden layer weights, ![](img/57a0c8ec-248e-4ae1-b055-3781a8e24024.png),
    which are the input-to-hidden layer weights of the input gate, forget gate, output
    gate, and candidate state, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have four hidden-to-hidden layer weights, ![](img/2c7f7639-7eed-4913-89be-650cf0771aba.png)
    , which implies hidden-to-hidden layer weights of input gate, forget gate, output
    gate, and candidate state, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have one hidden-to-output layer weight, ![](img/594a46af-c525-4fc2-bbda-963f90fe849b.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We find the optimal values for all these weights through gradient descent and
    update the weights according to the weight update rule. The weight update rule
    is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3afc2bbe-65d5-463e-a631-aa647fd0892f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll look at how to compute gradients of loss with respect
    to all of the weights used in the LSTM cell step by step.
  prefs: []
  type: TYPE_NORMAL
- en: You can skip the upcoming section if you are not interested in deriving gradients
    for all of the weights. However, it will strengthen your understanding of the
    LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to gates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating gradients of loss with respect to all the weights used in the LSTM
    cell requires the gradients of all the gates and the candidate state. So, in this
    section, we will learn how to compute the gradient of the loss function with respect
    to all of the gates and the candidate state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, let''s recollect the following two things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of a sigmoid function is expressed as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/86b0543e-e6c3-4ad4-b3a2-eef76bbb11e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative of a tanh function is expressed as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/03c9db62-92a2-4bd3-ba0a-e15f68d3269c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the upcoming calculations, we will be using gradients of loss with respect
    to hidden state ![](img/04b1bbe0-9a98-4fd2-bf43-2326ca721944.png) and cell state
    **![](img/79947213-05ea-4838-ab92-d50b5c63ffa1.png)** at multiple places. Thus,
    first, we will see how to compute gradients of loss with respect to hidden state
    ![](img/5a7453ed-c6b2-4601-9da4-b754deab49f9.png) and cell state **![](img/cb33785c-773a-484f-a66e-a49ce32caecb.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's see how to calculate the **gradients of loss with respect to a
    hidden state,** ![](img/aff235e2-6d45-4099-b535-b73f298c37aa.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that output ![](img/0c5c515b-482d-44a0-bc55-7cc367e382c5.png) is computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c505c7c0-8f83-463a-a971-555a168a4ea6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say ![](img/3e49322e-fb07-4005-b193-3dbf16d74314.png). We have the ![](img/680eac77-b21b-4b96-8f6a-f349834419eb.png)
    term in ![](img/5b3182c0-4ae1-4624-870a-62733e90f554.png), so by the chain rule,
    we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2808e7a-0d8e-42c3-a9be-2b70221debf8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/88e76828-1e21-44e2-8f47-03e0dd1d8128.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have already seen how to compute ![](img/849bb929-fbf6-4063-80d4-e45bd1383e47.png)
    in [Chapter 4](db170202-053f-4f45-b0d3-c2f15498ea34.xhtml), *Generating Song Lyrics
    Using RNN*, so directly from the equation *(9)* of [Chapter 4](db170202-053f-4f45-b0d3-c2f15498ea34.xhtml),
    *Generating Song Lyrics Using RNN*, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd0df0a9-66e1-4f63-b928-38eeacd4d190.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's see how to calculate the **gradient of the loss with respect to cell
    state**, ![](img/45208875-14c6-4221-b079-f1bca82c3913.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the gradients of loss with respect to the cell state, look at
    the equations of forward propagation and find out which equation has the ![](img/1812c1f1-9e31-45ac-8c67-4c1958b3f80e.png)
    term. On the equation of the hidden state, we have the ![](img/87e235e5-bc30-41c6-a638-0271e6d2094e.png)
    term as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d5f92d3-a3b1-4bc9-96a9-65bfe468d4a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11f7f1d7-0339-44d8-a188-a9e19b3f9899.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that the derivative of tanh is ![](img/85b44914-9957-44df-9cca-f1d59de773d6.png),
    thus we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbfceaaf-2494-47d5-af80-24451621f59e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have calculated the gradients of loss with respect to the hidden
    state and the cell state, let's see how to calculate the gradient of loss with
    respect to all the gates one by one.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will see how to calculate the **gradient of loss with respect to the
    output gate**, ![](img/829eda1c-c23c-4aa1-b061-324a07987dab.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the gradients of loss with respect to the output gate, look at
    the equations of forward propagation and find out in which equation we have the
    ![](img/3868b0a2-4822-49e2-b864-02ffca131759.png) term. In the equation of the
    hidden state we have the ![](img/2bfbcba3-17e5-4089-84a9-1d8b8efb8687.png) term
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd002aac-4b10-42a1-be7d-e82b88769eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39d60837-5619-4502-990c-f3e63598109c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/144189a8-3f1f-4ab2-bec2-bcf4ed72cc47.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will see how to calculate the **gradient of loss with respect to the
    input gate**, ![](img/221da6b9-eecb-44c9-8b01-48419b895c14.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the ![](img/426327c3-f651-46a0-9687-1de1676aabe9.png) term in the cell
    state equation for ![](img/29481156-1f9e-48fe-93a8-e6378d53aad7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2180062d-8ea8-4049-a8bd-d59742ef4f45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bc7fdbf-45c6-4f4a-a9e0-7be079d6cfa2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/2b7f7ac9-0f08-48d3-9ffe-9aee4389eb2e.png)'
  prefs: []
  type: TYPE_IMG
- en: And now we learn how to calculate the **gradient of loss with respect to the
    forget gate**, ![](img/5a957198-b737-4612-aaf0-dda6c302fae7.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have the ![](img/2407ceae-de39-4719-9d81-aefdae7865cf.png) term in
    the cell state equation for ![](img/d6f6908f-c0a5-47fc-8ee5-ff26b6648737.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cf851c6-598f-404b-bfc2-5a5c5b719791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa0defe6-30a7-4a37-9e6c-3e7fb8bc18e6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3a9a642e-f43e-4d53-9c52-813404648bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we learn how to calculate the **gradient of loss with respect to the
    candidate state**, ![](img/72970edd-bb07-4b02-b1e8-34d6f1a99f5f.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have the ![](img/f39da3ff-6660-4c2f-b180-dc541d98a818.png) term in
    the cell state equation for ![](img/51ae1b17-9ae6-494f-82bb-00766ee993e5.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62b91a3a-5975-4d77-9fdc-25e35e59dd63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/837534e6-29a8-49fc-b25e-eeef4ba43fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0ed71810-1be0-4191-a7a2-5b00efb2d45b.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have calculated the gradients of loss with respect to all the gates
    and the candidate state. In the next section, we'll see how to calculate the gradients
    of loss with respect to all of the weights used in the LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's see how to calculate gradients with respect to all the weights used
    in the LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to V
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After predicting the output, ![](img/7edada70-d300-4872-811f-57e83ea23e73.png),
    we are in the final layer of the network. Since we are backpropagating; that is,
    going from the output layer to the input layer, our first weight will be ![](img/28e1042c-b857-4de6-a4e4-ebe2576eaad0.png),
    which is hidden-to-output layer weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have learned throughout that the final loss is the sum of the loss over
    all the time steps. In a similar manner, our final gradient is the sum of gradients
    at all time steps as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f644de61-9b4b-4dd2-8260-d7900dd7387d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we have ![](img/d2193d10-a298-4be5-a7de-5de99b7e6253.png) layers, then we
    can write the gradient of loss with respect to ![](img/fd9b1b5c-6a97-4fbd-be40-6046235b7b27.png)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4706bc36-2544-43fa-b56a-9ce238a09a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the final equation of LSTM, that is, ![](img/38547eb6-31a6-4683-8d02-dbfd3415c3a7.png),
    is the same as RNN, calculating gradients of loss with respect to ![](img/941c7ac5-91ab-420b-b793-8ff4e11f5d2d.png)
    is exactly the same as what we computed in the RNN. Thus, we can directly write
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b2f7164-c2b8-4f1a-8d17-26eb7117039e.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradients with respect to W
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to calculate the gradients of loss with respect to hidden-to-hidden
    layer weights, ![](img/4dde845e-01a8-4703-9bce-6c15a8f049bb.png), for all the
    gates and the candidate state.
  prefs: []
  type: TYPE_NORMAL
- en: Let's calculate **gradients of loss with respect to** ![](img/df71f656-1289-49e5-a2e9-8ca49ea63d3d.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of the input gate, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e28e8482-bf06-4595-8ef5-eab0ace4adb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32367a92-eb50-421a-9b6a-86fc3a43c61e.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's calculate each of the terms in the preceding equation.
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen how to compute the first term, the gradient of loss with
    respect to the input gate,![](img/6640ea99-0003-480a-8009-9061d15a2c17.png), in
    the *Gradients with respect to gates* section. Refer to equation (*2)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s look at the second term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cbcee6f-9c1e-4f01-a863-8b173f8b209b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know the derivative of the sigmoid function, ![](img/8ecc5286-7a2f-41f3-9937-5aca92701106.png),
    we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bb15bec-f3c9-4df2-b5be-03b87e066c27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But ![](img/3688d455-05da-4c1c-ace6-2f0fd93bcd95.png) is already a result of
    sigmoid, that is, ![](img/e041ff53-d1f5-4b8a-b2b0-dd882b417807.png), so we can
    just write ![](img/98e86e32-88ed-41c3-ab2e-e61464eed7d5.png), Thus, our equation
    becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cc5def6-ee4e-477e-9caa-cde7c207c31c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/c26c4547-7431-4072-acf1-906ab26665f0.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ff4b581-a640-45a5-a60d-eef08c056f74.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's find out the **gradients of loss with respect to** ![](img/4ca82c19-eca4-49d7-8179-e107c10b80b8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of the forget gate, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd4f5c88-16ad-4dfb-a8f2-25f851999326.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8455693-3b32-4af2-9b88-3a35d2e33b9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have already seen how to compute ![](img/079e755e-0b0f-4a20-bb9b-5f93b62f8679.png)
    in the gradients with respect to the gates section. Refer to equation *(3)*. So,
    let''s look at computing the second term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b51c294-be13-4f6c-bffc-994a3f9b0b75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/f83344db-5707-4cc7-a86a-ea1576fdffaa.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51f8ec69-c2b4-4bfe-8d74-da0c49e333bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's calculate the **gradients of loss with respect to** ![](img/76bccc57-edc1-4e61-9580-a535b74fb0f8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of the output gate, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b43d3c0c-cfce-4de7-b3c3-cac620dcc4d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a839211-1e76-4573-8f73-c8746085b65b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check equation *(1)* for the first term. The second term can be computed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/487f7a94-71e3-48f0-9aae-bf081c5029b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/36529b85-5e3a-4c49-9a09-8b39799f1406.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90cf0690-88f4-4baf-91ef-6a650ed3306b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's move on to calculating the **gradient with respect to** ![](img/645a2ad4-94f6-4fa9-a97c-dc7086d89783.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the candidate state equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/788211a7-312f-4f5c-a35d-ae8fbf5019b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fadea06d-8414-447f-b135-3ad8bd773de8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Refer to equation *(4)* for the first term. The second term can be computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a723bc0-48b1-4c5b-ba17-788449f6e3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that derivative of tanh is ![](img/45d023b1-640a-4f3f-83b0-c88c7e00a6f9.png),
    so we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c747d9d-b05c-4db0-8065-c11c54a645fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to![](img/2a8030d8-33f7-445b-83e4-5015eccd81fc.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389d4034-18d8-4a33-8b45-54addca5326b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradients with respect to U
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's calculate the gradients of loss with respect to hidden-to-input layer
    weights ![](img/49138555-3ac0-4529-9db6-d52f3739c051.png) for all the gates and
    the candidate state. Computing gradients of loss with respect to ![](img/49138555-3ac0-4529-9db6-d52f3739c051.png)
    is exactly the same as the gradients we computed with respect to ![](img/d46ef0c9-23cb-49cf-820a-0bfa688f8c12.png),
    except that the last term will be ![](img/aceb813e-47ff-40b6-8da5-70db589b339e.png)
    instead of ![](img/326b737b-9984-4c3c-88f2-13f19d801669.png). Let's examine what
    we mean by that.
  prefs: []
  type: TYPE_NORMAL
- en: Let's find out the **gradients of loss with respect to** ![](img/b463fa17-3ede-4bf3-b562-4c19ef4e11ad.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The input gate equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/592ff201-63c2-44bd-b9d3-a42e4ee5e0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, using the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c88adacc-0b75-45fc-9bec-534c9af7073b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate each of the terms in the preceding equation. We already know
    the first term from equation *(2)*. So, the second term can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6824b2e1-a042-499c-91d8-8fb31bd88179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/42d1129a-5f98-42a9-a85f-17fd6f8c092a.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8030e7d5-2f29-4b81-8914-362627364f7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the preceding equation is exactly the same as ![](img/a1abf315-812d-49bf-ac5f-5306b5ad711f.png),
    except that the last term is ![](img/f52d64fe-0324-4fb3-9ef8-79270de4df65.png)
    instead of ![](img/954523a4-ac71-4553-ab87-971814fddc4b.png). This applies for
    all other weights, so we can directly write the equations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradients of loss with respect to ![](img/7f403a62-a89d-432d-9e57-c100f6450e58.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/531be703-bc6f-462d-8173-20542d359a9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradients of loss with respect to ![](img/ddf83867-c5d5-4321-98c2-ea00df20d9b1.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/62ee2e00-65f1-4596-acb5-26ef6bff42e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradients of loss with respect to ![](img/54698652-aa77-456c-aa2f-cf02bdcd93c7.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e42a96e1-641a-4e70-9c22-7aaf41893047.png)'
  prefs: []
  type: TYPE_IMG
- en: After the computing gradients, with respect to all of these weights, we update
    them using the weight update rule and minimize the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Bitcoin prices using LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that LSTM models are widely used for sequential datasets, that
    is, datasets in which order matters. In this section, we will learn how we can
    use LSTM networks for time series analysis. We will learn how to predict Bitcoin
    prices using an LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required libraries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will see how we can prepare our dataset in a way that our LSTM network
    needs. First, we read the input dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we display a few rows of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/482c8f90-ae85-4b04-987d-7a0832ef3cd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding data frame, the `Close` column represents the closing
    price of Bitcoin. We need only the `Close` column to make predictions, so we take
    that particular column alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we standardize the data and bring it to the same scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then plot and observe the trend of how the Bitcoin price changes. Since
    we scaled the price, it is not a bigger number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb8b7244-692d-4386-8014-3db1d5a5a4af.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we define a function called the `get_data` function, which generates the
    input and output. It takes the data and `window_size` as an input and generates
    the input and target column.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the window size here? We move the *x* values `window_size` times ahead
    and get the *y* values. For instance, as shown in the following table with `window_size`
    equal to 1, the *y* values are just one time step ahead of the *x* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x** | **y** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.13 | 0.56 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.56 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.11 | 0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.40 | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: 'The `get_data()` function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We choose `window_size` as `7` and generate the input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider the first `1000` points as the train set and the rest of the points
    in the dataset as the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of `X_train` is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'What does the preceding shape mean? It implies that the `sample_size`, `time_steps`,
    and `features` functions and the LSTM network require input exactly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1000` sets the number of data points (`sample_size`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`7` specifies the window size (`time_steps`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` specifies the dimension of our dataset (`features`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the network parameters as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define `placeholders` for our input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's now define all the weights we will use in our LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights of the input gate are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights of the forget gate are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights of the output gate are defined as given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights of the candidate state are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output layer weight is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Define the LSTM cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we define the function called `LSTM_cell`, which returns the cell state
    and hidden state as an output. Recall the steps we saw in the forward propagation
    of LSTM, which is implemented as shown in the following code. `LSTM_cell` takes
    the input, previous hidden state, and previous cell state as inputs, and returns
    the current cell state and current hidden state as outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Defining forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will perform forward propagation and predict the output, ![](img/1e63eb13-17e9-4d49-b4da-5f050305776d.png),
    and initialize a list called `y_hat` for storing the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For each iteration, we compute the output and store it in the `y_hat` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize the hidden state and cell state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform the forward propagation and compute the hidden state and cell state
    of the LSTM cell for each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that output ![](img/8cdf455d-e02b-4e47-b962-438e508ee1ff.png) can be
    computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/214a7de4-6b0e-4c5c-8fd2-c9c8ec5ad246.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute `y_hat`, and append it to the `y_hat` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Defining backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After performing forward propagation and predicting the output, we compute
    the loss. We use mean squared error as our loss function, and the total loss is
    the sum of losses across all of the time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid the exploding gradient problem, we perform gradient clipping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the Adam optimizer and minimize our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Training the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start the TensorFlow session and initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of `epochs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each iteration, perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then sample a batch of data and train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the loss on every `10` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may see in the following output, the loss decreases over the epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Making predictions using the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will start making predictions on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the predicted output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the result as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the values of test predictions are in a nested list, so we
    will flatten them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we print the predicted values, they are no longer in a nested list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As we took the first `1000` points as a training set, we make predictions for
    time steps greater than `1000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We plot and see how well the predicted value matches the actual value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following plot, the actual value is shown in red and
    the predicted value is shown in blue. As we are making predictions for time steps
    greater than `1000`, you can see after time step **1000**, the red and blue lines
    into each other, which implies that our model has correctly predicted the actual
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2232e1fb-4b94-4d85-9ce8-fc94362edfc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Gated recurrent units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about how the LSTM cell uses different gates and how
    it solves the vanishing gradient problem of the RNN. But, as you may have noticed,
    the LSTM cell has too many parameters due to the presence of many gates and states.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while backpropagating the LSTM network, we need to update a lot of parameters
    in every iteration. This increases our training time. So, we introduce the **Gated
    Recurrent Units** (**GRU**) cell, which acts as a simplified version of the LSTM
    cell. Unlike the LSTM cell, the GRU cell has only two gates and one hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN with a GRU cell is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d48cdd-f7d0-48f1-99b2-f8820ea6616c.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the GRU cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, a GRU cell has only two gates, called the
    update gate and the reset gate, and one hidden state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6de6de0-f01c-4f8a-a4cc-69467795b056.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's delve deeper and see how these gates are used and how the hidden state
    is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Update gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The update gate helps to decide what information from the previous time step,
    ![](img/ca543331-1f78-4714-8082-23c2df67e94a.png), can be taken forward to the
    next time step, ![](img/40dc7154-c4d2-4617-9724-422b72a8ba16.png). It is basically
    a combination of an input gate and a forget gate, which we learned about in LSTM
    cells. Similar to the gates about the LSTM cell, the update gate is also regulated
    by the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The update gate, ![](img/f4a8957d-4e8a-4985-87d7-fc3a51192777.png), at time
    step ![](img/dbab5c15-c0fd-40dc-84d2-ed47b76524ac.png) is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50ded757-84e8-4a7d-9c98-d3714c058f7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/685a5db9-7362-42dd-9150-e87553a756af.png)is the input-to-hidden weights
    of the update gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/860a056f-7024-43a2-85e2-d723096fed6e.png)is the hidden-to-hidden weights
    of the update gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fc870073-2339-4aff-92de-88731221b785.png)is the bias of the update
    gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the update gate. As you can see, input ![](img/2148ff3c-2102-43cf-93be-d01300bb9d15.png)
    is multiplied with ![](img/4b197a14-642f-402d-ba45-ae45c3f3f7d2.png), and the
    previous hidden state, ![](img/23255254-0486-46a9-a27c-2d9289ffc00f.png), 0 and
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea43579f-2493-4912-9181-43229cbbb46e.png)'
  prefs: []
  type: TYPE_IMG
- en: Reset gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reset gate helps to decide how to add the new information to the memory,
    that is, how much of the past information it can forget. The reset gate, ![](img/dc243755-ac57-4e04-aaec-33f63ae1f369.png)
    ,at time step ![](img/aff87a8a-21a9-4b42-8623-8fd73549c61f.png) is expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1059b3f7-1f07-41c3-8607-2cd07a280781.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/663d529b-93df-49c5-9038-848d4758b1a9.png) is the input-to-hidden weights
    of the reset gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5efc1bb2-5707-4aa5-93fb-ca29d9cbab72.png) is the hidden-to-hidden weights
    of the reset gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5b8a4123-9c37-46a3-8371-b8e113c8aecc.png) is the bias of the reset
    gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reset gate is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f58f7a88-7e0f-4107-975f-f7afe26e9a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating hidden state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how the update and reset gates work, but how do these gates
    help in updating the hidden state? That is, how do you add new information to
    the hidden state and how do you remove unwanted information from the hidden state
    with the help of the reset and update gates?
  prefs: []
  type: TYPE_NORMAL
- en: First, we will see how to add new information to the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: We create a new state called the **content state**, ![](img/1cf0e78d-66a7-4773-81be-1397536c6f58.png),
    for holding the information. We know that the reset gate is used to remove information
    that is not required. So, using the reset gate, we create a content state, ![](img/1cf0e78d-66a7-4773-81be-1397536c6f58.png),
    that holds only the required information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content state, ![](img/a7eca8be-17cf-401e-a3b2-95d2477d60df.png), at time
    step ![](img/fc68ff5a-bba6-4753-ba76-9515b7ece4e9.png) is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e4b03f-9d02-4703-a00b-9675c1e0bcd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows how the content state is created with the reset
    gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebea80bb-d016-4b93-9a31-74aac916781e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will see how to remove information from the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the update gate, ![](img/e09c8a23-2a52-4a5e-86d1-106944f8980b.png),
    helps to decide what information from the previous time step, ![](img/a6a01139-221f-46f7-88be-ca70346c67cd.png),
    can be taken forward to the next time step, ![](img/deb19d44-54c9-444c-b4f8-50a6991a1812.png).
    Multiplying ![](img/f8dc0900-cf2a-4754-9c4f-1d24724daece.png) and ![](img/85a8fa1c-a7d8-4def-93ba-15c9acbe7a47.png)
    gives us only the relevant information from the previous step. Instead of having
    a new gate, we just a take a complement of ![](img/d5b1a3a7-ce26-4e57-b714-896e9e27e8c2.png),
    that is ![](img/eb7ba754-ed5d-4b47-a9ad-e15e197602c3.png), and multiply them with
    ![](img/b3a41778-5ef9-46f7-89f1-2883a5988774.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state is then updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/267cbcdf-84e8-4996-a3db-24b225663e6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the hidden state is computed, we can apply the softmax function and compute
    the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96c12b33-2dc6-4377-9cce-b2d173c0d139.png)'
  prefs: []
  type: TYPE_IMG
- en: Forward propagation in a GRU cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting it all together, we learned in the previous section that the complete
    forward propagation steps in the GRU cell can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Update Gate**: ![](img/847e9d1e-f3ef-4209-b5be-00f938f5ee8c.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reset Gate**: ![](img/8a3a5b9f-54f4-4904-9ac0-2c2fb0290d37.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content State**: ![](img/4ee3853c-c054-46d0-86d9-d958683b5d8b.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden State**: ![](img/13ad384e-3b6a-431d-a00e-9a3694dee3c6.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: ![](img/aeab6138-f758-453e-8630-e45f9af2952c.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation in a GRU cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The total loss, ![](img/63ab3607-3992-42fe-b860-6c3304219d4b.png), is the sum
    of losses at all time steps, and can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0702a3f5-c713-485e-8e8b-7e2e2af1ef7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To minimize the loss using gradient descent, we find the derivative of loss
    with respect to all of the weights used in the GRU cell as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We have three input-to-hidden layer weights, ![](img/1a9494ac-cb8d-4d6d-846d-8f456bbf67f6.png),
    which are the input-to-hidden layer weights of the update gate, reset gate, and
    content state, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have three hidden-to-hidden layer weights, ![](img/3dd8d867-6db0-4ad7-bb26-788ae1a0cd90.png),
    which are the hidden-to-hidden layer weights of the update gate, reset gate, and
    content state respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have one hidden-to-output layer weight, ![](img/a9ce24d1-a366-4c22-bda6-400cd788eda4.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We find the optimal values for all these weights through gradient descent and
    update the weights according to the weight update rule.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient with respect to gates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw when we discussed LSTM cells, calculating gradients of loss with respect
    to all the weights requires the gradients of all the gates and content state.
    So, first, we will see how to calculate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming calculations, we will be using the gradients of loss with respect
    to the hidden state, ![](img/923169e7-19ba-4dc0-8a13-4214bbb178ce.png), which
    is ![](img/ddef7679-ba54-482d-ab14-333662ddc788.png) at multiple places, so we
    will see how to calculate that. Computing the gradients of loss with respect to
    the hidden state, ![](img/4148809c-68e3-4c3e-9706-980b7bd27bfc.png), is exactly
    the same as we saw in the LSTM cell, and can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f598ff53-6808-4463-804a-f531587fe763.png)'
  prefs: []
  type: TYPE_IMG
- en: First, let's see how to calculate the **gradients of loss with respect to the
    content state**, **![](img/4e5f3d3c-8df9-4ac7-89c3-0d4e8d96b620.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the gradients of loss with respect to content state, look at the
    equations of forward propagation and find out which equation has the ![](img/1928409a-0882-4021-bbe0-f29c523ec6fe.png)
    term. In the hidden state equation, that is, equation *(8)*, we have the ![](img/8bac39e4-c1e7-476c-acef-5cf6db18ec0d.png)
    term, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ed2980b-f1d6-4508-91a1-f61928ff475a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34b11e23-4329-4076-9916-51b6e40fc15a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9f527edc-d8dc-4f7f-8a97-4a5e4f1764a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's see how to calculate the **gradient of loss with respect to the reset
    gate ![](img/445168d8-588f-4862-b9f5-ce99849bfa1c.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the **![](img/445168d8-588f-4862-b9f5-ce99849bfa1c.png)** term in the
    content state equation and it can be given as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e80c58f3-9cfb-4074-a4a7-008fe5411bff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab951795-aa0c-4aab-8e2c-b97201cecbb5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e628d532-0d57-4b99-81c5-03ed86cb702c.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we see the **gradient of loss with respect to the update gate**, **![](img/0c368ec1-8c86-411c-bb37-2aec3661376e.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the ![](img/ef8eadfa-f6cf-4863-8bb6-9725aa8c6486.png) term in the hidden
    state, ![](img/f121ba04-5390-40f5-973c-563a17ad4305.png), equation, which can
    be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbc9da3a-b8eb-4856-b11b-9b0cbc696e0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, by the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5402022-2f76-4529-bcb1-0bcc89805103.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/fbcf38cd-f3cb-4b0e-9804-c0a252c96bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have calculated the gradients of loss with respect to all the gates
    and the content state, we will see how to calculate gradients of loss with respect
    to all the weights in our GRU cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how to calculate gradients with respect to all the weights
    used in the GRU cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to V
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the final equation of GRU, that is, ![](img/aa716d72-cf77-4c3a-b868-177796228d2f.png),
    is the same as with the RNN, calculating the gradients of loss with respect to
    hidden-to-output layer weight ![](img/fd2c30d3-d234-4723-a34f-3ed01e446a93.png)
    is exactly the same as what we computed in the RNN. Thus, we can directly write
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d73e3eea-1053-42f3-a11d-2019c45a736b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradients with respect to W
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how to calculate the gradients of loss with respect to hidden-to-hidden
    layer weights, ![](img/788453d8-602f-4fc5-a548-41f56f1a4211.png), for all the
    gates and the content state.
  prefs: []
  type: TYPE_NORMAL
- en: Let's calculate the **gradients of loss with respect to** ![](img/32102421-1ca5-405c-a7e4-b309392b8f93.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of the reset gate, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97cf5199-4f32-49d8-83f3-ed7693ade454.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/487945b5-2b81-409e-94c0-bddd6b37bbad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate each of the terms in the preceding equation. The first term,![](img/fe0f7034-db54-4457-b4ab-f432c716e32e.png),
    we already calculated in equation *(11)*. The second term is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ffde3db-4f9b-468b-a843-bd464ac69bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/f5c2f6ff-bcba-4453-b2ad-efafd43a4c6f.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/305bc7f0-e404-4f2d-bee8-3127457c22da.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's move on to finding the **gradients of loss with respect to** ![](img/77d0b331-8cf2-4007-a59c-564ee2ca0720.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of the update gate, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/141ee87d-fdcf-49a7-9e5c-cc3566e62fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6eaff2f8-a5b1-4c20-993d-f8c5876bb000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have already computed the first term in equation *(12)*. The second term
    is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/203434df-078f-44be-9968-3cf74849a178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/bd20ff27-1633-40d0-a72b-7ca273217dcf.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe1aa551-62a2-4e10-bf5f-7dc68b146491.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will find the **gradients of loss with respect to** ![](img/01668886-4be0-44a5-9f6e-15a4a909b677.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the content state equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/195a4d7d-2353-4c6c-8c1b-35d8db0a03fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the chain rule, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c34b6db-257e-4dfa-8563-98d508bb5330.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Refer to equation *(10)* for the first term. The second term is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/505f096b-8390-4702-b18a-3e491886e12f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our final equation for calculating the gradient of loss with respect
    to ![](img/043b0ae1-3783-45ba-91a8-c690cee78bc9.png) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d54ee1-6c3d-4dc6-bb26-4ee28c5d6240.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradients with respect to U
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to calculate the gradients of loss with respect to the input
    to the hidden weights, ![](img/0b8de361-8ecb-4102-9c63-75dbdf526d46.png), for
    all the gates and the content state. Computing gradients with respect to ![](img/0b8de361-8ecb-4102-9c63-75dbdf526d46.png)
    is exactly the same as for those we computed with respect to ![](img/b16fc05c-9ed5-4023-b394-2c14fe1045fd.png),
    except that the last term will be ![](img/01ccd390-2098-4826-bad5-9c2cddc0e47a.png)
    instead of ![](img/982aaf04-59cd-4cd9-a220-6c2d024224b1.png), similar to what
    we learned when we covered the LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the **gradients of loss with respect to ![](img/31605843-c79f-4a17-9b7c-c53413e19212.png)**
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adad0428-f7e0-428c-a107-b09bdef4e19c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The gradients of loss with respect to** ![](img/8c284527-0e48-4887-b8f2-918f353a812e.png)
    are represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85f1ffc8-bf99-4503-bf02-210c5a205d83.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The gradients of loss with respect to** ![](img/ec18e7d7-2132-4be0-95c9-9de487839d73.png)
    are represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1516e03a-d4c4-427c-9c78-aa6e160553b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing a GRU cell in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how to implement a GRU cell in TensorFlow. Instead of looking
    at the code, we will only see how to implement forward propagation of GRUs in
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s define all the weights. The weights of the update gate are defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights of the reset gate are defined as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights of the content state are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Weights of the output layer are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Defining forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the update gate as given in equation *(5)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the reset gate as given in equation *(6)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the content state as given in equation *(7)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the hidden state as given in equation *(8)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the output as given in equation *(9)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Bidirectional RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a bidirectional RNN, we have two different layers of hidden units. Both of
    these layers connect from the input layer to the output layer. In one layer, the
    hidden states are shared from left to right, and in the other layer, they are
    shared from right to left.
  prefs: []
  type: TYPE_NORMAL
- en: But what does this mean? To put it simply, one hidden layer moves forward through
    time from the start of the sequence, while the other hidden layer moves backward
    through time from the end of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, we have two hidden layers: a forward hidden
    layer and a backward hidden layer, which are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the forward hidden layer, hidden state values are shared from past time steps,
    that is, ![](img/b085442d-8fcf-432e-b043-965a97c0568a.png) is shared to ![](img/a81ec639-6e9f-42e4-bee2-3f2254f8af4c.png),
    ![](img/61ae5015-89e5-48c7-a39f-24721e059c4a.png) is shared to ![](img/b574afd9-1cd4-4654-b3b5-6564485798e0.png),
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the backward hidden layer, hidden start values are shared from future time
    steps, that is, ![](img/992a0d8a-6e5a-471b-99e6-c6a7e56d65a3.png)to ![](img/d02c4d9a-11d6-4231-adb9-12c324054f7e.png),
    ![](img/6e62c00e-9b75-4e01-b4c8-369bb83bcd8e.png)to ![](img/48cdf1b9-46a3-4eb6-99e9-48200baa02d7.png),
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The forward hidden layer and the backward hidden layer are represented as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2496305a-f7ff-42dd-9e6d-ff080eaff0b7.png)'
  prefs: []
  type: TYPE_IMG
- en: What is the use of bidirectional RNNs? In certain cases, reading the input sequence
    from both sides is very useful. So, a bidirectional RNN consists of two RNNs,
    one reading the sentence forward and the other reading the sentence backward.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Archie lived for 13 years in _____. So he is good at speaking Chinese.*'
  prefs: []
  type: TYPE_NORMAL
- en: If we use an RNN to predict the blank in the preceding sentence, it would be
    ambiguous. As we know, an RNN can make predictions based on only the set of words
    it has seen so far. In the preceding sentence, to predict the blank, the RNN has
    seen only the words *Archie*, *lived*, *for*, *13*, *years*, and *in, b*ut these
    words alone do not provide much context and do not give any clarity to predict
    the correct word. It just says *Archie lived for 13 years in.* With this information
    alone, we cannot predict the next word correctly.
  prefs: []
  type: TYPE_NORMAL
- en: But if we read the words following the blank as well, which are *So, he, is,
    good, at, speaking*, and *Chinese,* then we can say that Archie lived for 13 years
    in *China*, since it is given that he is good at speaking Chinese. So, in this
    circumstance, if we use a bidirectional RNN to predict the blank, it will predict
    correctly, since it reads the sentence in both forward and backward directions
    before making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs have been used in various applications, such as **part-of-speech**
    (**POS**) tagging, in which it is vital to know the word before and after the
    target word, language translation, predicting protein structure, dependency parsing,
    and more. However, a bidirectional RNN is not suitable for online settings where
    we don't know the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward propagation steps in bidirectional RNNs are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forward hidden layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0d43e50d-cad2-41ee-b068-45c881e13b49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Backward hidden layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/050f71ab-72fc-408e-88b2-e10df4889c4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4bb182a9-6c94-4ad0-b041-54002149ad6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Implementing a bidirectional RNN is simple with TensorFlow. Assuming we use
    the LSTM cell in the bidirectional RNN, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `rnn` from TensorFlow `contrib` as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define forward and backward hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the bidirectional RNN with `rnn.static_bidirectional_rnn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Going deep with deep RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that a deep neural network is a network that has many hidden layers.
    Similarly, a deep RNN has more than one hidden layer, but how are the hidden states
    computed when we have more than one hidden layer? We know that an RNN computes
    the hidden state by taking inputs and the previous hidden state, but how are the
    hidden states in the later layers computed?
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's see how ![](img/32e70998-4576-4af1-aa2b-6b05010aafdd.png)
    in hidden layer 2 is computed. It takes the previous hidden state, ![](img/1eb977fd-947d-4b96-85aa-df56ef1a212e.png),
    and the previous layer's output, ![](img/6da43a1a-a146-483a-a5a7-6ed2e7af391a.png),
    as inputs to compute ![](img/b125c9ce-dca1-4210-b87b-a64dc4cf9626.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, when we have an RNN with more than one hidden layer, hidden layers at
    the later layers will be computed by taking the previous hidden state and the
    previous layer''s output as input, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7a7594f-1f84-45c1-9691-c781cad36333.png)'
  prefs: []
  type: TYPE_IMG
- en: Language translation using the seq2seq model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **sequence-to-sequence model** (**seq2seq**) is basically the many-to-many
    architecture of an RNN. It has been used for various applications because it can
    map an arbitrary-length input sequence to an arbitrary-length output sequence.
    Some of the applications of the seq2seq model include language translation, music
    generation, speech generation, and chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: In most real-world scenarios, input and output sequences vary in length. For
    instance, let's take the language translation task, during which we need to convert
    a sentence from a source language to a target language. Let's assume we are converting
    from English (source) to French (target).
  prefs: []
  type: TYPE_NORMAL
- en: Consider our input sentence is *what are you doing?* Then, it would be mapped
    to *que faites vous?* As we can observe, the input sequence consists of four words,
    whereas the output sequence consists of three words. The seq2seq model handles
    this varying length of input and output sequence and maps the source to the target.
    Thus, they are widely used in applications for which the input and output sequences
    vary in length.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the seq2seq model is very simple. It comprises two vital
    components, namely an encoder and a decoder. Let's consider the same language
    translation task. First, we feed the input sentence to an encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder learns the representation (embeddings) of the input sentence, but
    what is a representation? Representation, or embedding, is basically a vector
    comprising the meaning of the sentence. It is also called the **thought vector**
    or **context vector**. Once the encoder learns the embedding, it sends the embedding
    to the decoder. The decoder takes this embedding (thought vector) as input and
    tries to construct a target sentence. So, the decoder tries to generate the French
    translation for the English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following diagram, the encoder takes the input English
    sentence, learns the embeddings, and feeds the embeddings to the decoder, and
    then the decoder generates the translated French sentence using those embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb406d2e-a21b-4815-894b-6eb66da3ad8a.png)'
  prefs: []
  type: TYPE_IMG
- en: But how does this really work? How does the encoder understand the sentence?
    How does the decoder translate the sentence using the encoder's embeddings? Let's
    delve deeper and see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An encoder is basically an RNN with LSTM or GRU cells. It can also be a bidirectional
    RNN. We feed the input sentence to an encoder and, instead of taking the output,
    we take the hidden state from the final time step as the embeddings. Let's better
    understand encoders with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider we are using an RNN with a GRU cell and the input sentence is *what
    are you doing.* Let''s represent the hidden state of the encoder with *e*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c41e25d-29b2-4797-bb1b-c55daf55f12c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows how the encoder computes the thought vectors; this
    is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first time step, ![](img/e34b07fb-7a00-433c-8db9-7dc532d394fd.png).
    To a GRU cell, we pass the input, ![](img/924c18cd-c3a2-45c0-9709-61cf6bc7fc0a.png),
    which is the first word in the input sentence, *what*, and also the initial hidden
    state, ![](img/5e6d805a-5f4c-4be0-9c1d-aea4cb747b6d.png), which is randomly initialized.
    With these inputs, the GRU cell computes the first hidden state, ![](img/17df6ef9-4256-4fd0-8adf-31e46c59cae5.png),
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e418bc62-3d67-40a4-9bdb-f1ca15a275e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next time step, ![](img/3fbeb7b9-f455-4f83-a654-3d77aa45f0cf.png), we
    pass the input, ![](img/594beef5-f654-4a79-83fe-3e4ebf1251f8.png), which is the
    next word in the input sentence, *are*, to the encoder. Along with this, we also
    pass the previous hidden state, ![](img/b1ceff9f-3617-4ec6-9885-c96bfe885a10.png),
    and compute the hidden state, ![](img/d8dafef5-2c21-4bba-a469-64762a296725.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ffaad91e-ab69-429d-bdbc-cdbd3b7b4eba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next time step, ![](img/6de00d57-0eb1-454c-a7f4-6129e6e8a89f.png), we
    pass the input, ![](img/9b0c5b4d-c8c9-4a38-b6ca-76ae22d27a7f.png), which is the
    next word, *you*, to the encoder. Along with this, we also pass the previous hidden
    state, ![](img/316bbbe8-dfed-4391-9a1d-f2ff08a6e10a.png), and compute the hidden
    state, ![](img/9c195ab0-4ec6-4b54-b884-c0e94c56ad32.png), as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fed1de64-bb41-4794-90ea-487785fb7cd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the final time step, ![](img/1fb887d2-d3c9-41ed-b750-0a9891e4f39b.png),
    we feed the input, ![](img/503e5936-add0-4b71-b5b4-b6faf9501bd0.png), which is
    the last word in the input sentence, *doing.* Along with this, we also pass the
    previous hidden state, ![](img/6b36f190-3001-4333-b5ff-92b7aee41ce9.png), and
    compute the hidden state, ![](img/563552de-06a9-4f23-bd52-5f6aa84daccf.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a739f402-78c5-4264-b9aa-3ad155084ec0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, ![](img/ed565eaf-65a4-4944-9907-c5cc0e9003d9.png) is our final hidden
    state. We learned that the RNN captures the context of all the words it has seen
    so far in its hidden state. Since ![](img/ed565eaf-65a4-4944-9907-c5cc0e9003d9.png)
    is the final hidden state, it holds the context of all the words that the network
    has seen, which will be all the words in our input sentence, that is, *what, are,
    you*, and *doing.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the final hidden state, ![](img/ed565eaf-65a4-4944-9907-c5cc0e9003d9.png),
    holds the context of all the words in our input sentence, it holds the context
    of the input sentence, and this essentially forms our embedding, ![](img/f16c2c9b-4d5a-4aff-a250-081783a4b22f.png),
    which is otherwise called a thought or context vector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64336f60-c1ee-43f3-b18a-e53d79fc13a4.png)'
  prefs: []
  type: TYPE_IMG
- en: We feed the context vector, ![](img/873ee16b-63e7-4b6e-84f5-ab8d4da349ef.png),
    to the decoder to convert it to the target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in the encoder, on every time step, ![](img/cb86a3c5-8094-481c-b6a3-aeba3898ba33.png),
    we feed an input word and along with it we also feed the previous hidden state,
    ![](img/2e811f4c-cafc-481a-bb02-e4245d8ce3f6.png), and compute the current hidden
    state, ![](img/f87b5d21-8d53-45c5-b560-f9d0e5a935c7.png). The hidden state at
    the final step, ![](img/21ba6237-3c97-44c7-aed4-5c311a216198.png), holds the context
    of the input sentence and it will become the embedding, ![](img/86d73894-d18d-4d2c-947a-dfa489832953.png),
    which will be sent to the decoder to convert it to the target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will learn how the decoder generates the target sentence by using the
    thought vector, ![](img/5611c453-0f79-42da-badf-e2f0d9457e5c.png), generated by
    the encoder. A decoder is an RNN with LSTM or GRU cells. The goal of our decoder
    is to generate the target sentence for the given input (source) sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that we start off an RNN by initializing its initial hidden state with
    random values, but for the decoder''s RNN, we initialize the hidden state with
    the thought vector, ![](img/5611c453-0f79-42da-badf-e2f0d9457e5c.png), generated
    by the encoder, instead of initializing them with random values. The decoder network
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16201a52-10b3-4e44-99bb-2360b2e765fd.png)'
  prefs: []
  type: TYPE_IMG
- en: But, what should be the input to the decoder? We simply pass **<sos>** as an
    input to the decoder, which indicates the start of the sentence. So, once the
    decode receives **<sos>**, it tries to predict the actual starting word of the
    target sentence. Let's represent the decoder hidden state by ![](img/f55e47fa-02bb-4d56-9cee-6ae073459bc9.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the first time step, ![](img/bb6f3248-d10c-430b-904b-3fd6b8dcfbf3.png),
    we feed the first input, which is **<sos>**, to the decoder, and along with it,
    we pass the thought vector as the initial hidden state as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3af7b26-63b2-4d53-baf1-fb4af71207ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay. What are we really doing here? We need to predict the output sequence,
    which is a French equivalent for our input English sentence. There are a lot of
    French words in our vocabulary. How does the decoder decide which word to output?
    That is, how does it decide the first word in our output sequence?
  prefs: []
  type: TYPE_NORMAL
- en: 'We feed the decoder hidden state, ![](img/17b762eb-33fd-4586-ac58-8f924f094aaa.png),
    to ![](img/aee8660c-9dda-48a3-be0a-0a61af8bc3e7.png), which returns the score
    for all the words in our vocabulary to be the first output word. That is, the
    output word at a time step, ![](img/a500d9e6-c50f-458d-addb-e9763adb785c.png)
    is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54b3e377-4c58-46c9-b529-e08a64d678b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of having raw scores, we convert them into probabilities. Since we
    learned that the softmax function squashes values between 0 to 1, we use the softmax
    function for converting the score, ![](img/25c8b470-88c7-4d5e-9272-f6eee9c2046e.png),
    into a probability, ![](img/99b1c5e2-d4a4-41e0-aee5-97e51f839191.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f232af41-0339-40c2-b870-a0a68db6d0f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we have probabilities for all the French words in our vocabulary to be
    the first output word. We select the word that has the highest probability as
    the first output word using the argmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e427382-3160-4167-928a-c4994658f405.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we have predicted that the first output word, ![](img/f0186d5c-499b-4870-b087-725c99e997d5.png),
    is *Que*, as shown in the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next time step, ![](img/d37e3c9e-9a88-4c20-a355-9addddfcb9e4.png), we
    feed the output word predicted at the previous time step, ![](img/af587c78-f7e6-4aab-b8e6-0050d6daf143.png),
    as input to the decoder. Along with it, we also pass the previous hidden state,
    ![](img/fc6b17ac-7414-4abb-8bed-bb1765234945.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d8355c8-f013-42a2-8805-3fcd8d358ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we compute the score for all the words in our vocabulary to be the next
    output word, that is, the output word at time step ![](img/3629c1fb-1a2f-44fb-93b1-41e845b698e0.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8831d28-a8bc-467c-a577-dbe515b496da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we convert the scores to probabilities using the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca4ae24d-bb32-4d87-959f-76299a956420.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we select the word that has the highest probability as the output word,
    ![](img/5d0f3231-b41f-44c0-b2a0-7a814bf59961.png), at a time step, ![](img/c79af8e1-8071-494a-8035-5dbd83febde7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b2cb5ad-7c33-43b3-a6c7-c186a330cb0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we initialize the decoder's initial hidden state with ![](img/d4ee253b-da1a-4e03-a597-fd7962fbef9d.png),
    and, on every time step, ![](img/a6175e48-f082-41d3-b5a0-8d88cd81acdd.png), we
    feed the predicted output word from the previous time step, ![](img/0d3ea9bc-9fd3-4388-bc27-0dd6ceb7da97.png),
    and the previous hidden state, ![](img/46f55371-9ab0-4dff-aacb-8afb1de0a904.png),
    as an input to the decoder, ![](img/6abe5d13-c61e-4ae8-9ead-a97dbaf01f25.png),
    at the current time step, and predict the current output, ![](img/9460279a-8946-4b1b-a9dd-94c2a6618c1f.png).
  prefs: []
  type: TYPE_NORMAL
- en: But when does the decoder stop? Because our output sequence has to stop somewhere,
    we cannot keep on feeding the predicted output word from the previous time step
    as an input to the next time step. When the decoder predicts the output word as
    **<sos>**, this implies the end of the sentence. Then, the decoder learns that
    an input source sentence is converted to a meaningful target sentence and stops
    predicting the next word.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, this is how the seq2seq model converts the source sentence to the target
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Attention is all we need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how the seq2seq model works and how it translates a sentence
    from the source language to the target language. We learned that a context vector
    is basically a hidden state vector from the final time step of an encoder, which
    captures the meaning of the input sentence, and it is used by the decoder to generate
    the target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: But when the input sentence is long, the context vector does not capture the
    meaning of the whole sentence, since it is just the hidden state from the final
    time step. So, instead of taking the last hidden state as a context vector and
    using it for the decoder, we take the sum of all the hidden states from the encoder
    and use it as a context vector.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say the input sentence has 10 words; then we would have 10 hidden states.
    We take a sum of all these 10 hidden states and use it for the decoder to generate
    the target sentence. However, not all of these hidden states might be helpful
    in generating a target word at time step ![](img/6936ee89-bfce-44d9-b004-e6c5c05492d4.png).
    Some hidden states will be more useful than other hidden states. So, we need to
    know which hidden state is more important than another at time step ![](img/6936ee89-bfce-44d9-b004-e6c5c05492d4.png)
    to predict the target word. To get this importance, we use the attention mechanism,
    which tells us which hidden state is more important to generate the target word
    at the time step ![](img/6936ee89-bfce-44d9-b004-e6c5c05492d4.png). Thus, attention
    mechanisms basically give the importance for each of the hidden states of the
    encoder to generate the target word at time step ![](img/6936ee89-bfce-44d9-b004-e6c5c05492d4.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'How does an attention mechanism work? Let''s say we have three hidden states
    of an encoder, ![](img/ab2fa4ea-aaff-4377-bc9e-a0b28296284c.png), ![](img/5860915c-794c-4fdd-ad57-8e642566f4f6.png),
    and ![](img/30024c5d-abdb-4ce5-9cdc-71871912ce1e.png), and a decoder hidden state,
    ![](img/9368e002-9c63-44b1-b2d0-99f41fb3dfbc.png), as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50f65fd0-d5ba-4da2-a699-b23638e790e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we need to know the importance of all the hidden states of an encoder to
    generate a target word at time step ![](img/6936ee89-bfce-44d9-b004-e6c5c05492d4.png),
    So, we take each encoder hidden state, ![](img/e6f7c741-6fb6-4bcd-b238-37a217a333bf.png),
    and decoder hidden state, ![](img/88105601-2aa2-47b8-8746-306e23c3ee0c.png), and
    feed them to a function, ![](img/a835122d-5352-40d5-93ef-213096cfc65c.png), which
    is called a **score function** or **alignment function**, and it returns the score
    for each of the encoder hidden states indicating their importance. But what is
    this score function? There are a number of choices for the score function, such
    as dot product, scaled dot product, cosine similarity, and more.
  prefs: []
  type: TYPE_NORMAL
- en: We use a simple dot product as the score function; that is, the dot product
    between the encoder hidden states and the decoder hidden states. For instance,
    to know the importance of ![](img/7c724e95-9b8e-4f08-bf46-c5548b025209.png) in
    generating the target word, we simply compute the dot product between ![](img/7c724e95-9b8e-4f08-bf46-c5548b025209.png)
    and ![](img/7b552100-35cf-4ec3-bd70-d4c26226c57c.png), which gives us a score
    indicating how similar ![](img/7c724e95-9b8e-4f08-bf46-c5548b025209.png) and ![](img/7b552100-35cf-4ec3-bd70-d4c26226c57c.png)
    are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the score, we convert them into probabilities using the softmax
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/444b78ec-2865-4896-ada8-4d8d2ff24499.png)'
  prefs: []
  type: TYPE_IMG
- en: These probabilities, ![](img/981305de-f7b0-4f08-a314-5ddf7dcb8cec.png), are
    called **attention weights**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following diagram, we compute the similarity score between
    each of the encoder''s hidden states with the decoder''s hidden state using a
    function, ![](img/b584b42e-3bde-4b0a-9fc0-92a6b9044445.png). Then, the similarity
    score is converted into probabilities using the softmax function, which are called
    attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a795cfb5-5829-4874-b642-9b8c64624ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we have attention weights (probabilities) for each of the encoder''s
    hidden states. Now, we multiply the attention weights with their corresponding
    encoder''s hidden state, that is, ![](img/a24e9bf0-819d-41ef-9958-0556abd202fa.png).
    As shown in the following diagram, the encoder''s hidden state, ![](img/842aced5-251e-47ac-84fe-fe4ba8dacc32.png),
    is multiplied by **0.106**, ![](img/b225d1d3-8668-4509-b83f-890f57a03688.png)
    is multiplied by **0.106**, and ![](img/124f9743-e3df-43c9-ab5b-5d6780b61fa4.png)
    is multiplied by **0.786**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4b71126-368c-47cb-bbb4-d8c3b525a8ee.png)'
  prefs: []
  type: TYPE_IMG
- en: But, why do we have to multiply attention weights by the encoder's hidden state?
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying the encoder's hidden states by their attention weights indicates
    that we are giving more importance to the hidden states that have more attention
    weights, and less importance to hidden states that have fewer attention weights.
    As shown in the preceding diagram, multiplying **0.786** with hidden state ![](img/124f9743-e3df-43c9-ab5b-5d6780b61fa4.png)
    implies we are giving more importance to ![](img/124f9743-e3df-43c9-ab5b-5d6780b61fa4.png)
    than the other two hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, this is how the attention mechanism decides which hidden state is more
    important to generate the target word at time step ![](img/99f9244a-a159-4c7a-8a0e-2bb3bded1610.png).
    After multiplying the encoder''s hidden state by their attention weights, we simply
    sum them up, and this now forms our context/thought vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b35b4ea-9e72-442b-85a3-1d67a98b7f62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following diagram, the context vector is obtained by the sum
    of the encoder''s hidden state multiplied by its respective attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff2ac115-5a3d-4f35-927b-ded90f66b89f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, to generate a target word at time step ![](img/99f9244a-a159-4c7a-8a0e-2bb3bded1610.png),
    the decoder uses context vector ![](img/a3f119f5-ae5c-485d-af13-98f8387e724c.png)
    at time step ![](img/df8ae413-f3a8-4d68-b6c6-6495a04084f3.png). With the attention
    mechanism, instead of taking the last hidden state as a context vector and using
    it for the decoder, we take the sum of all the hidden states from the encoder
    and use it as a context vector.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how the LSTM cell uses several gates to combat the
    vanishing gradient problem. Then, we saw how to use the LSTM cell to predict a
    Bitcoin's price in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: After looking at LSTM cells, we learned about the GRU cell, which is a simplified
    version of LSTM. We also learned about bidirectional RNNs, where we had two layers
    of hidden states with one layer moving forward through time from the start of
    the sequence, while another layer moved backward through time from the end of
    the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about the seq2seq model, which maps an
    input sequence of varying length to an output sequence of varying length. We also
    understood how the attention mechanism is used in the seq2seq model and how it
    focuses on important information.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about convolutional neural networks and how
    they are used for recognizing images.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our newly gained knowledge to the test. Answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How does LSTM solve the vanishing gradient problem of RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are all the different gates and their functions in an LSTM cell?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the use of the cell state?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a GRU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do bidirectional RNNs work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do deep RNNs compute the hidden state?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are encoders and decoders in the seq2seq architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the use of the attention mechanism?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out some of these cool projects on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human activity recognition using LSTM: [https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building a chatbot using seq2seq: [https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text summarization using a bidirectional GRU: [https://github.com/harpribot/deep-summarization](https://github.com/harpribot/deep-summarization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
