- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Continuous Target Variables with Regression Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout the previous chapters, you learned a lot about the main concepts
    behind **supervised learning** and trained many different models for classification
    tasks to predict group memberships or categorical variables. In this chapter,
    we will dive into another subcategory of supervised learning: **regression analysis**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Regression models are used to predict target variables on a continuous scale,
    which makes them attractive for addressing many questions in science. They also
    have applications in industry, such as understanding relationships between variables,
    evaluating trends, or making forecasts. One example is predicting the sales of
    a company in future months.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the main concepts of regression models and
    cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at different approaches to implementing linear regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training regression models that are robust to outliers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models and diagnosing common problems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting regression models to nonlinear data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing linear regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of linear regression is to model the relationship between one or multiple
    features and a continuous target variable. In contrast to classification—a different
    subcategory of supervised learning—regression analysis aims to predict outputs
    on a continuous scale rather than categorical class labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, you will be introduced to the most basic type
    of linear regression, **simple linear regression**, and understand how to relate
    it to the more general, multivariate case (linear regression with multiple features).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of simple (**univariate**) linear regression is to model the relationship
    between a single feature (**explanatory variable**, *x*) and a continuous-valued
    **target** (**response variable**, *y*). The equation of a linear model with one
    explanatory variable is defined as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Here, the parameter (bias unit), *b*, represents the *y* axis intercept and
    *w*[1] is the weight coefficient of the explanatory variable. Our goal is to learn
    the weights of the linear equation to describe the relationship between the explanatory
    variable and the target variable, which can then be used to predict the responses
    of new explanatory variables that were not part of the training dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the linear equation that we defined previously, linear regression
    can be understood as finding the best-fitting straight line through the training
    examples, as shown in *Figure 9.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17582_09_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: A simple one-feature linear regression example'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This best-fitting line is also called the **regression line**, and the vertical
    lines from the regression line to the training examples are the so-called **offsets**
    or **residuals**—the errors of our prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section introduced simple linear regression, a special case of
    linear regression with one explanatory variable. Of course, we can also generalize
    the linear regression model to multiple explanatory variables; this process is
    called **multiple linear regression**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9.2* shows how the two-dimensional, fitted hyperplane of a multiple
    linear regression model with two features could look:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_09_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: A two-feature linear regression model'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, visualizations of multiple linear regression hyperplanes in
    a three-dimensional scatterplot are already challenging to interpret when looking
    at static figures. Since we have no good means of visualizing hyperplanes with
    two dimensions in a scatterplot (multiple linear regression models fit to datasets
    with three or more features), the examples and visualizations in this chapter
    will mainly focus on the univariate case, using simple linear regression. However,
    simple and multiple linear regression are based on the same concepts and the same
    evaluation techniques; the code implementations that we will discuss in this chapter
    are also compatible with both types of regression model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Ames Housing dataset
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we implement the first linear regression model, we will discuss a new
    dataset, the Ames Housing dataset, which contains information about individual
    residential property in Ames, Iowa, from 2006 to 2010\. The dataset was collected
    by Dean De Cock in 2011, and additional information is available via the following
    links:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'A report describing the dataset: [http://jse.amstat.org/v19n3/decock.pdf](http://jse.amstat.org/v19n3/decock.pdf)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Detailed documentation regarding the dataset’s features: [http://jse.amstat.org/v19n3/decock/DataDocumentation.txt](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset in a tab-separated format: [http://jse.amstat.org/v19n3/decock/AmesHousing.txt](http://jse.amstat.org/v19n3/decock/AmesHousing.txt)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with each new dataset, it is always helpful to explore the data through a
    simple visualization, to get a better feeling of what we are working with, which
    is what we will do in the following subsections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Ames Housing dataset into a DataFrame
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will load the Ames Housing dataset using the pandas `read_csv`
    function, which is fast and versatile and a recommended tool for working with
    tabular data stored in a plaintext format.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The Ames Housing dataset consists of 2,930 examples and 80 features. For simplicity,
    we will only work with a subset of the features, shown in the following list.
    However, if you are curious, follow the link to the full dataset description provided
    at the beginning of this section, and you are encouraged to explore other variables
    in this dataset after reading this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'The features we will be working with, including the target variable, are as
    follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '`Overall Qual`: Rating for the overall material and finish of the house on
    a scale from 1 (very poor) to 10 (excellent)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Overall Qual`: 房屋整体材料和装饰的评分，范围从1（非常差）到10（优秀）'
- en: '`Overall Cond`: Rating for the overall condition of the house on a scale from
    1 (very poor) to 10 (excellent)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Overall Cond`: 房屋整体条件的评分，范围从1（非常差）到10（优秀）'
- en: '`Gr Liv Area`: Above grade (ground) living area in square feet'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gr Liv Area`: 地面以上的居住面积，以平方英尺为单位'
- en: '`Central Air`: Central air conditioning (N=no, Y=yes)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Central Air`: 中央空调（N=否，Y=是）'
- en: '`Total Bsmt SF`: Total square feet of the basement area'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Total Bsmt SF`: 地下室总面积，以平方英尺为单位'
- en: '`SalePrice`: Sale price in U.S. dollars ($)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SalePrice`: 销售价格（美元）'
- en: 'For the rest of this chapter, we will regard the sale price (`SalePrice`) as
    our target variable—the variable that we want to predict using one or more of
    the five explanatory variables. Before we explore this dataset further, let’s
    load it into a pandas `DataFrame`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将把销售价格 (`SalePrice`) 视为我们的目标变量 —— 我们希望使用五个或更多的解释变量来预测的变量。在进一步探索这个数据集之前，让我们将其加载到一个
    pandas `DataFrame` 中：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To confirm that the dataset was loaded successfully, we can display the first
    five lines of the dataset, as shown in *Figure 9.3*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据集已成功加载，我们可以显示数据集的前五行，如 *Figure 9.3* 所示：
- en: '![Table  Description automatically generated](img/B17582_09_03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B17582_09_03.png)'
- en: 'Figure 9.3: The first five rows of the housing dataset'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 9.3: 房屋数据集的前五行'
- en: 'After loading the dataset, let’s also check the dimensions of the `DataFrame`
    to make sure that it contains the expected number of rows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，让我们还检查一下 `DataFrame` 的维度，以确保其包含预期数量的行：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, the `DataFrame` contains 2,930 rows, as expected.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`DataFrame` 包含了预期的 2,930 行。
- en: 'Another aspect we have to take care of is the `''Central Air''` variable, which
    is encoded as type `string`, as we can see in *Figure 9.3*. As we learned in *Chapter
    4*, *Building Good Training Datasets – Data Preprocessing*, we can use the `.map`
    method to convert `DataFrame` columns. The following code will convert the string
    `''``Y''` to the integer 1, and the string `''N''` to the integer 0:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要注意 `'Central Air'` 变量，它被编码为 `string` 类型，正如我们在 *Figure 9.3* 中看到的。正如我们在 *Chapter
    4* 中学到的，在转换 `DataFrame` 列时，我们可以使用 `.map` 方法。以下代码将字符串 `'Y'` 转换为整数 1，字符串 `'N'` 转换为整数
    0：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Lastly, let’s check whether any of the data frame columns contain missing values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查数据框中是否有任何缺失值的列：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the `Total Bsmt SF` feature variable contains one missing value.
    Since we have a relatively large dataset, the easiest way to deal with this missing
    feature value is to remove the corresponding example from the dataset (for alternative
    methods, please see *Chapter 4*):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`Total Bsmt SF` 特征变量包含一个缺失值。由于我们有一个相对较大的数据集，处理这个缺失的特征值的最简单方法是从数据集中删除相应的示例（有关替代方法，请参见
    *Chapter 4*）：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Visualizing the important characteristics of a dataset
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化数据集的重要特征
- en: '**Exploratory data analysis** (**EDA**) is an important and recommended first
    step prior to the training of a machine learning model. In the rest of this section,
    we will use some simple yet useful techniques from the graphical EDA toolbox that
    may help us to visually detect the presence of outliers, the distribution of the
    data, and the relationships between features.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析** (**EDA**) 是在训练机器学习模型之前的一个重要且推荐的第一步。在本节的其余部分，我们将使用一些简单但有用的技术来自可视化
    EDA 工具箱，这些技术有助于我们在视觉上检测异常值的存在、数据的分布以及特征之间的关系。'
- en: First, we will create a **scatterplot matrix** that allows us to visualize the
    pair-wise correlations between the different features in this dataset in one place.
    To plot the scatterplot matrix, we will use the `scatterplotmatrix` function from
    the mlxtend library ([http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)),
    which is a Python library that contains various convenience functions for machine
    learning and data science applications in Python.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个 **散点图矩阵**，它允许我们在一个地方可视化数据集中不同特征之间的两两相关性。为了绘制散点图矩阵，我们将使用 mlxtend 库中的
    `scatterplotmatrix` 函数（[http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)），这是一个包含各种方便函数的
    Python 库，用于机器学习和数据科学应用。
- en: You can install the `mlxtend` package via `conda install mlxtend` or `pip install
    mlxtend`. For this chapter, we used mlxtend version 0.19.0.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `conda install mlxtend` 或 `pip install mlxtend` 安装 `mlxtend` 包。本章中，我们使用的是
    mlxtend 版本 0.19.0。
- en: 'Once the installation is complete, you can import the package and create the
    scatterplot matrix as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以导入包并按如下方式创建散点图矩阵：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see in *Figure 9.4*, the scatterplot matrix provides us with a useful
    graphical summary of the relationships in a dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图9.4*中所看到的，散点图矩阵为我们提供了数据关系的有用图形总结：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_09_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图形用户界面描述的图片](img/B17582_09_04.png)'
- en: 'Figure 9.4: A scatterplot matrix of our data'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：我们数据的散点图矩阵
- en: Using this scatterplot matrix, we can now quickly see how the data is distributed
    and whether it contains outliers. For example, we can see (fifth column from the
    left of the bottom row) that there is a somewhat linear relationship between the
    size of the living area above ground (`Gr Liv Area`) and the sale price (`SalePrice`).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个散点图矩阵，我们现在可以快速查看数据的分布情况及其是否包含异常值。例如，我们可以看到（底部行的第五列）地面以上生活区的大小(`Gr Liv Area`)与销售价格(`SalePrice`)之间存在某种线性关系。
- en: Furthermore, we can see in the histogram—the lower-right subplot in the scatterplot
    matrix—that the `SalePrice` variable seems to be skewed by several outliers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在直方图中（散点图矩阵的右下子图），我们可以看到`SalePrice`变量似乎受到几个异常值的影响。
- en: '**The normality assumption of linear regression**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归的正态性假设**'
- en: 'Note that in contrast to common belief, training a linear regression model
    does not require that the explanatory or target variables are normally distributed.
    The normality assumption is only a requirement for certain statistics and hypothesis
    tests that are beyond the scope of this book (for more information on this topic,
    please refer to *Introduction to Linear Regression Analysis* by *Douglas C. Montgomery*,
    *Elizabeth A. Peck*, and *G. Geoffrey Vining*, *Wiley*, pages: 318-319, 2012).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与普遍观念相反，训练线性回归模型并不要求解释变量或目标变量服从正态分布。正态性假设仅适用于某些超出本书范围的统计和假设检验（有关更多信息，请参阅*道格拉斯C.
    蒙哥马利*、*伊丽莎白A. 佩克*和*G. 杰弗里·文宁*的*《线性回归分析导论》*，*Wiley*，2012年，第318-319页）。
- en: Looking at relationships using a correlation matrix
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看相关矩阵以探索关系
- en: In the previous section, we visualized the data distributions of the Ames Housing
    dataset variables in the form of histograms and scatterplots. Next, we will create
    a correlation matrix to quantify and summarize linear relationships between variables.
    A correlation matrix is closely related to the covariance matrix that we covered
    in the section *Unsupervised dimensionality reduction via principal component
    analysis* in *Chapter 5*, *Compressing Data via Dimensionality Reduction*. We
    can interpret the correlation matrix as being a rescaled version of the covariance
    matrix. In fact, the correlation matrix is identical to a covariance matrix computed
    from standardized features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们通过直方图和散点图的形式可视化了艾姆斯房屋数据集变量的数据分布情况。接下来，我们将创建一个相关矩阵，以量化和总结变量之间的线性关系。相关矩阵与我们在*第5章*
    *通过主成分分析进行无监督降维*中讨论的协方差矩阵密切相关。我们可以将相关矩阵解释为从标准化特征计算的协方差矩阵的重新缩放版本。实际上，相关矩阵与从标准化特征计算的协方差矩阵相同。
- en: 'The correlation matrix is a square matrix that contains the **Pearson product-moment
    correlation coefficient** (often abbreviated as **Pearson’s r**), which measures
    the linear dependence between pairs of features. The correlation coefficients
    are in the range –1 to 1\. Two features have a perfect positive correlation if
    *r* = 1, no correlation if *r* = 0, and a perfect negative correlation if *r*
    = –1\. As mentioned previously, Pearson’s correlation coefficient can simply be
    calculated as the covariance between two features, *x* and *y* (numerator), divided
    by the product of their standard deviations (denominator):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相关矩阵是一个方阵，包含**皮尔逊积矩相关系数**（通常缩写为**皮尔逊r**），用于衡量特征对之间的线性依赖关系。相关系数的取值范围是–1到1。如果*r*
    = 1，则两个特征具有完全正相关性；如果*r* = 0，则没有相关性；如果*r* = –1，则具有完全负相关性。如前所述，皮尔逊相关系数可以简单地计算为两个特征*x*和*y*的协方差（分子）除以它们标准差的乘积（分母）：
- en: '![](img/B17852_09_003.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![我们的数据的散点图矩阵](img/B17852_09_003.png)'
- en: Here, ![](img/B17852_09_004.png) denotes the mean of the corresponding feature,
    ![](img/B17852_09_005.png) is the covariance between the features *x* and *y*,
    and ![](img/B17852_09_006.png) and ![](img/B17852_09_007.png) are the features’
    standard deviations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance versus correlation for standardized features**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'We can show that the covariance between a pair of standardized features is,
    in fact, equal to their linear correlation coefficient. To show this, let’s first
    standardize the features *x* and *y* to obtain their z-scores, which we will denote
    as *x’* and *y’*, respectively:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_008.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Remember that we compute the (population) covariance between two features as
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_009.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Since standardization centers a feature variable at mean zero, we can now calculate
    the covariance between the scaled features as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_010.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'Through resubstitution, we then get the following result:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_011.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can simplify this equation as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_012.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'In the following code example, we will use NumPy’s `corrcoef` function on the
    five feature columns that we previously visualized in the scatterplot matrix,
    and we will use mlxtend’s `heatmap` function to plot the correlation matrix array
    as a heat map:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see in *Figure 9.5*, the correlation matrix provides us with another
    useful summary graphic that can help us to select features based on their respective
    linear correlations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17582_09_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: A correlation matrix of the selected variables'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: To fit a linear regression model, we are interested in those features that have
    a high correlation with our target variable, `SalePrice`. Looking at the previous
    correlation matrix, we can see that `SalePrice` shows the largest correlation
    with the `Gr Liv Area` variable (`0.71`), which seems to be a good choice for
    an exploratory variable to introduce the concepts of a simple linear regression
    model in the following section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an ordinary least squares linear regression model
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we mentioned that linear regression can be
    understood as obtaining the best-fitting straight line through the examples of
    our training data. However, we have neither defined the term *best-fitting* nor
    have we discussed the different techniques of fitting such a model. In the following
    subsections, we will fill in the missing pieces of this puzzle using the **ordinary
    least squares** (**OLS**) method (sometimes also called **linear least squares**)
    to estimate the parameters of the linear regression line that minimizes the sum
    of the squared vertical distances (residuals or errors) to the training examples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Solving regression for regression parameters with gradient descent
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider our implementation of the **Adaptive Linear Neuron** (**Adaline**)
    from *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*.
    You will remember that the artificial neuron uses a linear activation function.
    Also, we defined a loss function, *L*(**w**), which we minimized to learn the
    weights via optimization algorithms, such as **gradient descent** (**GD**) and
    **stochastic gradient descent** (**SGD**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'This loss function in Adaline is the **mean squared error** (**MSE**), which
    is identical to the loss function that we use for OLS:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_013.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_04_008.png) is the predicted value ![](img/B17852_09_015.png)
    (note that the term ![](img/B17852_09_016.png) is just used for convenience to
    derive the update rule of GD). Essentially, OLS regression can be understood as
    Adaline without the threshold function so that we obtain continuous target values
    instead of the class labels `0` and `1`. To demonstrate this, let’s take the GD
    implementation of Adaline from *Chapter 2* and remove the threshold function to
    implement our first linear regression model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Weight updates with gradient descent**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: If you need a refresher about how the weights are updated—taking a step in the
    opposite direction of the gradient—please revisit the *Adaptive linear neurons
    and the convergence of learning* section in *Chapter 2*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'To see our `LinearRegressionGD` regressor in action, let’s use the `Gr Living
    Area` (size of the living area above ground in square feet) feature from the Ames
    Housing dataset as the explanatory variable and train a model that can predict
    `SalePrice`. Furthermore, we will standardize the variables for better convergence
    of the GD algorithm. The code is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice the workaround regarding `y_std`, using `np.newaxis` and `flatten`. Most
    data preprocessing classes in scikit-learn expect data to be stored in two-dimensional
    arrays. In the previous code example, the use of `np.newaxis` in `y[:, np.newaxis]`
    added a new dimension to the array. Then, after `StandardScaler` returned the
    scaled variable, we converted it back to the original one-dimensional array representation
    using the `flatten()` method for our convenience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed in *Chapter 2* that it is always a good idea to plot the loss
    as a function of the number of epochs (complete iterations) over the training
    dataset when we are using optimization algorithms, such as GD, to check that the
    algorithm converged to a loss minimum (here, a *global* loss minimum):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see in *Figure 9.6*, the GD algorithm converged approximately after
    the tenth epoch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape  Description automatically generated](img/B17582_09_06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: The loss function versus the number of epochs'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s visualize how well the linear regression line fits the training
    data. To do so, we will define a simple helper function that will plot a scatterplot
    of the training examples and add the regression line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we will use this `lin_regplot` function to plot the living area against
    the sale price:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `lin_regplot` 函数绘制居住面积与销售价格：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see in *Figure 9.7*, the linear regression line reflects the general
    trend that house prices tend to increase with the size of the living area:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图 9.7*中所看到的，线性回归线反映了房屋价格倾向于随着居住面积的增加而增加的一般趋势：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_07.png)'
- en: 'Figure 9.7: A linear regression plot of sale prices versus living area size'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：销售价格与居住面积大小的线性回归图
- en: Although this observation makes sense, the data also tells us that the living
    area size does not explain house prices very well in many cases. Later in this
    chapter, we will discuss how to quantify the performance of a regression model.
    Interestingly, we can also observe several outliers, for example, the three data
    points corresponding to a standardized living area greater than 6\. We will discuss
    how we can deal with outliers later in this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种观察是有道理的，但数据还告诉我们，在许多情况下，居住面积大小并不能很好地解释房价。本章后面我们将讨论如何量化回归模型的性能。有趣的是，我们还可以观察到几个异常值，例如，对应于标准化后大于
    6 的生活区的三个数据点。我们将在本章后面讨论如何处理异常值。
- en: 'In certain applications, it may also be important to report the predicted outcome
    variables on their original scale. To scale the predicted price back onto the
    original *price in U.S. dollars* scale, we can simply apply the `inverse_transform`
    method of `StandardScaler`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，报告预测结果变量在其原始比例上也可能很重要。要将预测的价格重新缩放到原始的美元价格尺度上，我们可以简单地应用 `StandardScaler`
    的 `inverse_transform` 方法：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this code example, we used the previously trained linear regression model
    to predict the price of a house with an aboveground living area of 2,500 square
    feet. According to our model, such a house will be worth $292,507.07.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们使用之前训练过的线性回归模型预测了一个地面以上居住面积为 2,500 平方英尺的房屋的价格。根据我们的模型，这样一栋房子价值 $292,507.07。
- en: 'As a side note, it is also worth mentioning that we technically don’t have
    to update the intercept parameter (for instance, the bias unit, *b*) if we are
    working with standardized variables, since the *y* axis intercept is always 0
    in those cases. We can quickly confirm this by printing the model parameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一句，值得一提的是，如果我们使用标准化的变量，我们在技术上不必更新截距参数（例如，偏置单元，*b*），因为在这些情况下 *y* 轴截距始终为 0。我们可以通过打印模型参数来快速确认这一点：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Estimating the coefficient of a regression model via scikit-learn
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 scikit-learn 估计回归模型的系数
- en: 'In the previous section, we implemented a working model for regression analysis;
    however, in a real-world application, we may be interested in more efficient implementations.
    For example, many of scikit-learn’s estimators for regression make use of the
    least squares implementation in SciPy (`scipy.linalg.lstsq`), which, in turn,
    uses highly optimized code optimizations based on the **Linear Algebra Package**
    (**LAPACK**). The linear regression implementation in scikit-learn also works
    (better) with unstandardized variables, since it does not use (S)GD-based optimization,
    so we can skip the standardization step:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们实现了一个用于回归分析的工作模型；然而，在实际应用中，我们可能对更高效的实现感兴趣。例如，许多 scikit-learn 中用于回归的估计器使用了
    SciPy 中的最小二乘实现 (`scipy.linalg.lstsq`)，而 SciPy 又使用了基于**线性代数包**（**LAPACK**）的高度优化代码。scikit-learn
    中的线性回归实现也可以（更好地）处理非标准化的变量，因为它不使用（S）GD-based 优化，所以我们可以跳过标准化步骤：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see from executing this code, scikit-learn’s `LinearRegression`
    model, fitted with the unstandardized `Gr Liv Area` and `SalePrice` variables,
    yielded different model coefficients, since the features have not been standardized.
    However, when we compare it to our GD implementation by plotting `SalePrice` against
    `Gr Liv Area`, we can qualitatively see that it fits the data similarly well:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您通过执行此代码所看到的，用未标准化的 `Gr Liv Area` 和 `SalePrice` 变量拟合的 scikit-learn 的 `LinearRegression`
    模型产生了不同的模型系数，因为这些特征没有被标准化。然而，当我们将其与通过绘制 `SalePrice` 对 `Gr Liv Area` 进行的 GD 实现进行比较时，我们可以从质量上看到它与数据的拟合程度相似：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For instance, we can see that the overall result looks identical to our GD
    implementation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到整体结果与我们的 GD 实现看起来是相同的：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_08.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_08.png)'
- en: 'Figure 9.8: A linear regression plot using scikit-learn'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：使用 scikit-learn 的线性回归绘制的线性回归图
- en: '**Analytical solutions of linear regression**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to using machine learning libraries, there is also a closed-form
    solution for solving OLS involving a system of linear equations that can be found
    in most introductory statistics textbooks:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_017.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'We can implement it in Python as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The advantage of this method is that it is guaranteed to find the optimal solution
    analytically. However, if we are working with very large datasets, it can be computationally
    too expensive to invert the matrix in this formula (sometimes also called the
    normal equation), or the matrix containing the training examples may be singular
    (non-invertible), which is why we may prefer iterative methods in certain cases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in more information on how to obtain normal equations,
    take a look at Dr. Stephen Pollock’s chapter *The Classical Linear Regression
    Model*, from his lectures at the University of Leicester, which is available for
    free at [http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf](http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you want to compare linear regression solutions obtained via GD, SGD,
    the closed-form solution, QR factorization, and singular vector decomposition,
    you can use the `LinearRegression` class implemented in mlxtend ([http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)),
    which lets users toggle between these options. Another great library to recommend
    for regression modeling in Python is statsmodels, which implements more advanced
    linear regression models, as illustrated at [https://www.statsmodels.org/stable/examples/index.html#regression](https://www.statsmodels.org/stable/examples/index.html#regression).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a robust regression model using RANSAC
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression models can be heavily impacted by the presence of outliers.
    In certain situations, a very small subset of our data can have a big effect on
    the estimated model coefficients. Many statistical tests can be used to detect
    outliers, but these are beyond the scope of the book. However, removing outliers
    always requires our own judgment as data scientists as well as our domain knowledge.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to throwing out outliers, we will look at a robust method
    of regression using the **RANdom SAmple Consensus** (**RANSAC**) algorithm, which
    fits a regression model to a subset of the data, the so-called **inliers**.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize the iterative RANSAC algorithm as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Select a random number of examples to be inliers and fit the model.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test all other data points against the fitted model and add those points that
    fall within a user-given tolerance to the inliers.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refit the model using all inliers.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the error of the fitted model versus the inliers.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Terminate the algorithm if the performance meets a certain user-defined threshold
    or if a fixed number of iterations was reached; go back to *step 1* otherwise.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能达到某个用户定义的阈值或达到固定迭代次数，则终止算法；否则返回到 *步骤 1*。
- en: 'Let’s now use a linear model in combination with the RANSAC algorithm as implemented
    in scikit-learn’s `RANSACRegressor` class:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 scikit-learn 的 `RANSACRegressor` 类中实现的线性模型与 RANSAC 算法结合使用：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We set the maximum number of iterations of the `RANSACRegressor` to 100, and
    using `min_samples=0.95`, we set the minimum number of the randomly chosen training
    examples to be at least 95 percent of the dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `RANSACRegressor` 的最大迭代次数设置为 100，并且使用 `min_samples=0.95`，将随机选择的训练样本的最小数量设置为数据集的至少
    95%。
- en: By default (via `residual_threshold=None`), scikit-learn uses the **MAD** estimate
    to select the inlier threshold, where MAD stands for the **median absolute deviation**
    of the target values, `y`. However, the choice of an appropriate value for the
    inlier threshold is problem-specific, which is one disadvantage of RANSAC.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下（通过 `residual_threshold=None`），scikit-learn 使用 **MAD** 估计来选择内点阈值，其中 MAD
    表示目标值 `y` 的 **中位数绝对偏差**。然而，选择适当的内点阈值的适用性问题特定，这是 RANSAC 的一个缺点。
- en: 'Many different approaches have been developed in recent years to select a good
    inlier threshold automatically. You can find a detailed discussion in *Automatic
    Estimation of the Inlier Threshold in Robust Multiple Structures Fitting* by *R.
    Toldo* and *A. Fusiello*, *Springer*, 2009 (in *Image Analysis and Processing–ICIAP
    2009*, pages: 123-131).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来已开发了许多不同的方法来自动选择良好的内点阈值。您可以在 *R. Toldo* 和 *A. Fusiello* 的 *Springer*, 2009
    年的 *Image Analysis and Processing–ICIAP 2009* （页面：123-131）中找到详细讨论。
- en: 'Once we have fitted the RANSAC model, let’s obtain the inliers and outliers
    from the fitted RANSAC linear regression model and plot them together with the
    linear fit:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了 RANSAC 模型，让我们从拟合的 RANSAC 线性回归模型中获取内点和外点，并将它们与线性拟合一起绘制：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see in *Figure 9.9*, the linear regression model was fitted on the
    detected set of inliers, which are shown as circles:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 *图 9.9* 中所看到的那样，线性回归模型是在检测到的内点集上拟合的，这些内点显示为圆圈：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_09.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_09.png)'
- en: 'Figure 9.9: Inliers and outliers identified via a RANSAC linear regression
    model'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：通过 RANSAC 线性回归模型识别的内点和外点
- en: 'When we print the slope and intercept of the model by executing the following
    code, the linear regression line will be slightly different from the fit that
    we obtained in the previous section without using RANSAC:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行以下代码打印模型的斜率和截距时，线性回归线将与我们在之前未使用 RANSAC 时得到的拟合略有不同：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Remember that we set the `residual_threshold` parameter to `None`, so RANSAC
    was using the MAD to compute the threshold for flagging inliers and outliers.
    The MAD, for this dataset, can be computed as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们将 `residual_threshold` 参数设置为 `None`，因此 RANSAC 使用 MAD 来计算标记内点和外点的阈值。对于此数据集，MAD
    可以计算如下：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So, if we want to identify fewer data points as outliers, we can choose a `residual_threshold`
    value greater than the preceding MAD. For example, *Figure 9.10* shows the inliers
    and outliers of a RANSAC linear regression model with a residual threshold of
    65,000:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们希望将较少的数据点识别为离群值，我们可以选择一个比前面的 MAD 更大的 `residual_threshold` 值。例如，*图 9.10*
    展示了具有 65,000 的残差阈值的 RANSAC 线性回归模型的内点和外点：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_10.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_10.png)'
- en: 'Figure 9.10: Inliers and outliers determined by a RANSAC linear regression
    model with a larger residual threshold'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：由具有较大残差阈值的 RANSAC 线性回归模型确定的内点和外点
- en: Using RANSAC, we reduced the potential effect of the outliers in this dataset,
    but we don’t know whether this approach will have a positive effect on the predictive
    performance for unseen data or not. Thus, in the next section, we will look at
    different approaches for evaluating a regression model, which is a crucial part
    of building systems for predictive modeling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RANSAC，我们减少了数据集中离群值的潜在影响，但我们不知道这种方法是否会对未见数据的预测性能产生积极影响。因此，在接下来的章节中，我们将探讨不同的方法来评估回归模型，这是构建预测建模系统的关键部分。
- en: Evaluating the performance of linear regression models
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估线性回归模型的性能
- en: In the previous section, you learned how to fit a regression model on training
    data. However, you discovered in previous chapters that it is crucial to test
    the model on data that it hasn’t seen during training to obtain a more unbiased
    estimate of its generalization performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学会了如何在训练数据上拟合回归模型。然而，你在之前的章节中发现，将模型在训练过程中未见过的数据上进行测试是至关重要的，以获得其泛化性能的更加无偏估计。
- en: 'As you may remember from *Chapter 6*, *Learning Best Practices for Model Evaluation
    and Hyperparameter Tuning*, we want to split our dataset into separate training
    and test datasets, where we will use the former to fit the model and the latter
    to evaluate its performance on unseen data to estimate the generalization performance.
    Instead of proceeding with the simple regression model, we will now use all five
    features in the dataset and train a multiple regression model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的那样，来自第6章《学习模型评估和超参数调整的最佳实践》的内容，我们希望将数据集分成单独的训练和测试数据集，其中我们将使用前者来拟合模型，并使用后者来评估其在未见数据上的性能，以估计泛化性能。现在，我们不再使用简单的回归模型，而是使用数据集中的所有五个特征并训练多元回归模型：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since our model uses multiple explanatory variables, we can’t visualize the
    linear regression line (or hyperplane, to be precise) in a two-dimensional plot,
    but we can plot the residuals (the differences or vertical distances between the
    actual and predicted values) versus the predicted values to diagnose our regression
    model. **Residual plots** are a commonly used graphical tool for diagnosing regression
    models. They can help to detect nonlinearity and outliers and check whether the
    errors are randomly distributed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型使用多个解释变量，我们无法在二维图中可视化线性回归线（或者更精确地说是超平面），但我们可以绘制残差（实际值与预测值之间的差异或垂直距离）与预测值的图，来诊断我们的回归模型。**残差图**是诊断回归模型常用的图形工具，它们有助于检测非线性和异常值，并检查错误是否随机分布。
- en: 'Using the following code, we will now plot a residual plot where we simply
    subtract the true target variables from our predicted responses:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们将绘制一个残差图，其中我们简单地从预测响应中减去真实目标变量：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After executing the code, we should see residual plots for the test and training
    datasets with a line passing through the *x* axis origin, as shown in *Figure
    9.11*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们应该能看到测试和训练数据集的残差图，其中有一条通过*x*轴原点的线，如*图 9.11*所示：
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B17582_09_11.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  低置信度自动生成描述](https://img.example.org/B17582_09_11.png)'
- en: 'Figure 9.11: Residual plots of our data'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：我们数据的残差图
- en: In the case of a perfect prediction, the residuals would be exactly zero, which
    we will probably never encounter in realistic and practical applications. However,
    for a good regression model, we would expect the errors to be randomly distributed
    and the residuals to be randomly scattered around the centerline. If we see patterns
    in a residual plot, it means that our model is unable to capture some explanatory
    information, which has leaked into the residuals, as you can see to a degree in
    our previous residual plot. Furthermore, we can also use residual plots to detect
    outliers, which are represented by the points with a large deviation from the
    centerline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在完美预测的情况下，残差将恰好为零，在现实和实际应用中，我们可能永远不会遇到这种情况。然而，对于一个良好的回归模型，我们期望错误是随机分布的，残差在中心线周围随机分散。如果在残差图中看到模式，这意味着我们的模型无法捕获某些解释信息，这些信息已泄漏到残差中，正如我们之前的残差图中可能看到的那样。此外，我们还可以使用残差图检测异常值，这些异常值由偏离中心线较大的点表示。
- en: 'Another useful quantitative measure of a model’s performance is the **mean
    squared error** (**MSE**) that we discussed earlier as our loss function that
    we minimized to fit the linear regression model. The following is a version of
    the MSE without the ![](img/B17852_09_016.png) scaling factor that is often used
    to simplify the loss derivative in gradient descent:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个衡量模型性能的有用量化指标是我们之前讨论过的**均方误差**（**MSE**），它是我们用来最小化以拟合线性回归模型的损失函数。以下是不带缩放因子的MSE版本，通常用于简化梯度下降中的损失导数：
- en: '![](img/B17852_09_019.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![https://img.example.org/B17852_09_019.png](https://img.example.org/B17852_09_019.png)'
- en: Similar to prediction accuracy in classification contexts, we can use the MSE
    for cross-validation and model selection as discussed in *Chapter 6*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于分类环境中的预测准确性，我们可以使用MSE进行交叉验证和模型选择，如第6章讨论的那样。
- en: Like classification accuracy, MSE also normalizes according to the sample size,
    *n*. This makes it possible to compare across different sample sizes (for example,
    in the context of learning curves) as well.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now compute the MSE of our training and test predictions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see that the MSE on the training dataset is larger than on the test
    set, which is an indicator that our model is slightly overfitting the training
    data in this case. Note that it can be more intuitive to show the error on the
    original unit scale (here, dollar instead of dollar-squared), which is why we
    may choose to compute the square root of the MSE, called *root mean squared error*,
    or the **mean absolute error** (**MAE**), which emphasizes incorrect prediction
    slightly less:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_020.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the MAE similar to the MSE:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Based on the test set MAE, we can say that the model makes an error of approximately
    $25,000 on average.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: When we use the MAE or MSE for comparing models, we need to be aware that these
    are unbounded in contrast to the classification accuracy, for example. In other
    words, the interpretations of the MAE and MSE depend on the dataset and feature
    scaling. For example, if the sale prices were presented as multiples of 1,000
    (with the K suffix), the same model would yield a lower MAE compared to a model
    that worked with unscaled features. To further illustrate this point,
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_021.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Thus, it may sometimes be more useful to report the **coefficient of determination**
    (*R*²), which can be understood as a standardized version of the MSE, for better
    interpretability of the model’s performance. Or, in other words, *R*² is the fraction
    of response variance that is captured by the model. The *R*² value is defined
    as:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_022.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Here, SSE is the sum of squared errors, which is similar to the MSE but does
    not include the normalization by sample size *n*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_023.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'And SST is the total sum of squares:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_024.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: In other words, SST is simply the variance of the response.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s briefly show that *R*² is indeed just a rescaled version of the
    MSE:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_025.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: For the training dataset, *R*² is bounded between 0 and 1, but it can become
    negative for the test dataset. A negative *R*² means that the regression model
    fits the data worse than a horizontal line representing the sample mean. (In practice,
    this often happens in the case of extreme overfitting, or if we forget to scale
    the test set in the same manner we scaled the training set.) If *R*² = 1, the
    model fits the data perfectly with a corresponding *MSE* = 0.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluated on the training data, the *R*² of our model is 0.77, which isn’t
    great but also not too bad given that we only work with a small set of features.
    However, the *R*² on the test dataset is only slightly smaller, at 0.75, which
    indicates that the model is only overfitting slightly:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using regularized methods for regression
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    Scikit-Learn*, regularization is one approach to tackling the problem of overfitting
    by adding additional information and thereby shrinking the parameter values of
    the model to induce a penalty against complexity. The most popular approaches
    to regularized linear regression are the so-called **ridge regression**, **least
    absolute shrinkage and selection operator** (**LASSO**), and **elastic net**.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第3章*，*使用Scikit-Learn进行机器学习分类器的一次旅行*中讨论的那样，正则化是通过添加额外信息来解决过拟合问题的一种方法，从而缩小模型参数值以对复杂性施加惩罚。正则化线性回归的最流行方法是所谓的**Ridge回归**，**最小绝对收缩和选择算子**（**LASSO**），以及**弹性网络**。
- en: 'Ridge regression is an L2 penalized model where we simply add the squared sum
    of the weights to the MSE loss function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归是一种L2惩罚模型，我们只需将权重的平方和添加到MSE损失函数中：
- en: '![](img/B17852_09_026.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_026.png)'
- en: 'Here, the L2 term is defined as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，L2项被定义如下：
- en: '![](img/B17852_09_027.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_027.png)'
- en: By increasing the value of hyperparameter ![](img/B17852_09_028.png), we increase
    the regularization strength and thereby shrink the weights of our model. Please
    note that, as mentioned in *Chapter 3*, the bias unit *b* is not regularized.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加超参数 ![](img/B17852_09_028.png) 的值，我们增加正则化强度，从而缩小模型的权重。请注意，正如*第3章*中提到的，偏置单元
    *b* 没有经过正则化。
- en: 'An alternative approach that can lead to sparse models is LASSO. Depending
    on the regularization strength, certain weights can become zero, which also makes
    LASSO useful as a supervised feature selection technique:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以导致稀疏模型的替代方法是LASSO。根据正则化强度，某些权重可以变为零，这也使得LASSO作为一种监督特征选择技术非常有用：
- en: '![](img/B17852_09_029.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_029.png)'
- en: 'Here, the L1 penalty for LASSO is defined as the sum of the absolute magnitudes
    of the model weights, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，LASSO的L1惩罚被定义为模型权重的绝对值的总和，如下所示：
- en: '![](img/B17852_09_030.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_030.png)'
- en: However, a limitation of LASSO is that it selects at most *n* features if *m*
    > *n*, where *n* is the number of training examples. This may be undesirable in
    certain applications of feature selection. In practice, however, this property
    of LASSO is often an advantage because it avoids saturated models. The saturation
    of a model occurs if the number of training examples is equal to the number of
    features, which is a form of overparameterization. As a consequence, a saturated
    model can always fit the training data perfectly but is merely a form of interpolation
    and thus is not expected to generalize well.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LASSO的一个限制是，如果 *m* > *n*，它最多选择 *n* 个特征，其中 *n* 是训练示例的数量。在某些特征选择的应用中，这可能是不希望的。然而，在实践中，LASSO的这种性质通常是优点，因为它避免了饱和模型。模型的饱和发生在训练示例数等于特征数时，这是一种过度参数化的形式。因此，饱和模型可以完美拟合训练数据，但仅仅是一种插值形式，因此不太可能很好地推广。
- en: 'A compromise between ridge regression and LASSO is elastic net, which has an
    L1 penalty to generate sparsity and an L2 penalty such that it can be used for
    selecting more than *n* features if *m* > *n*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归和LASSO之间的折衷方案是弹性网络，它具有L1惩罚以生成稀疏性，并且具有L2惩罚，因此可以用于选择超过 *n* 个特征，如果 *m* >
    *n*：
- en: '![](img/B17852_09_031.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_031.png)'
- en: Those regularized regression models are all available via scikit-learn, and
    their usage is similar to the regular regression model except that we have to
    specify the regularization strength via the parameter ![](img/B17852_09_028.png),
    for example, optimized via k-fold cross-validation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些正则化回归模型都可以通过scikit-learn获得，并且它们的使用与常规回归模型类似，只是我们必须通过参数 ![](img/B17852_09_028.png)
    指定正则化强度，例如通过k折交叉验证进行优化。
- en: 'A ridge regression model can be initialized via:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式初始化Ridge回归模型：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note that the regularization strength is regulated by the parameter `alpha`,
    which is similar to the parameter ![](img/B17852_09_033.png). Likewise, we can
    initialize a LASSO regressor from the `linear_model` submodule:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，正则化强度由参数 `alpha` 调节，这类似于参数 ![](img/B17852_09_033.png)。同样，我们可以从`linear_model`子模块初始化一个LASSO回归器：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Lastly, the `ElasticNet` implementation allows us to vary the L1 to L2 ratio:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`ElasticNet`实现允许我们改变L1到L2的比例：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For example, if we set `l1_ratio` to 1.0, the `ElasticNet` regressor would be
    equal to LASSO regression. For more detailed information about the different implementations
    of linear regression, please refer to the documentation at [http://scikit-learn.org/stable/modules/linear_model.html](http://scikit-learn.org/stable/modules/linear_model.html).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Turning a linear regression model into a curve – polynomial regression
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we assumed a linear relationship between explanatory
    and response variables. One way to account for the violation of linearity assumption
    is to use a polynomial regression model by adding polynomial terms:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_034.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Here, *d* denotes the degree of the polynomial. Although we can use polynomial
    regression to model a nonlinear relationship, it is still considered a multiple
    linear regression model because of the linear regression coefficients, **w**.
    In the following subsections, we will see how we can add such polynomial terms
    to an existing dataset conveniently and fit a polynomial regression model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Adding polynomial terms using scikit-learn
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now learn how to use the `PolynomialFeatures` transformer class from
    scikit-learn to add a quadratic term (*d* = 2) to a simple regression problem
    with one explanatory variable. Then, we will compare the polynomial to the linear
    fit by following these steps:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a second-degree polynomial term:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fit a simple linear regression model for comparison:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit a multiple regression model on the transformed features for polynomial
    regression:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Plot the results:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the resulting plot, you can see that the polynomial fit captures the relationship
    between the response and explanatory variables much better than the linear fit:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart, scatter chart  Description automatically generated](img/B17582_09_12.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: A comparison of a linear and quadratic model'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will compute the MSE and *R*² evaluation metrics:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see after executing the code, the MSE decreased from 570 (linear
    fit) to 61 (quadratic fit); also, the coefficient of determination reflects a
    closer fit of the quadratic model (*R*² = 0.982) as opposed to the linear fit
    (*R*² = 0.832) in this particular toy problem.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Modeling nonlinear relationships in the Ames Housing dataset
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding subsection, you learned how to construct polynomial features
    to fit nonlinear relationships in a toy problem; let’s now take a look at a more
    concrete example and apply those concepts to the data in the Ames Housing dataset.
    By executing the following code, we will model the relationship between sale prices
    and the living area above ground using second-degree (quadratic) and third-degree
    (cubic) polynomials and compare that to a linear fit.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by removing the three outliers with a living area greater than 4,000
    square feet, which we can see in previous figures, such as in *Figure 9.8*, so
    that these outliers don’t skew our regression fits:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we fit the regression models:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot is shown in *Figure 9.13*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_13.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: A comparison of different curves fitted to the sale price and
    living area data'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, using quadratic or cubic features does not really have an effect.
    That’s because the relationship between the two variables appears to be linear.
    So, let’s take a look at another feature, namely, `Overall Qual`. The `Overall
    Qual` variable rates the overall quality of the material and finish of the houses
    and is given on a scale from 1 to 10, where 10 is best:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After specifying the `X` and `y` variables, we can reuse the previous code
    and obtain the plot in *Figure 9.14*:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B17582_09_14.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: A linear, quadratic, and cubic fit on the sale price and house
    quality data'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the quadratic and cubic fits capture the relationship between
    sale prices and the overall quality of the house better than the linear fit. However,
    you should be aware that adding more and more polynomial features increases the
    complexity of a model and therefore increases the chance of overfitting. Thus,
    in practice, it is always recommended to evaluate the performance of the model
    on a separate test dataset to estimate the generalization performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with nonlinear relationships using random forests
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to look at **random forest** regression, which
    is conceptually different from the previous regression models in this chapter.
    A random forest, which is an ensemble of multiple **decision trees**, can be understood
    as the sum of piecewise linear functions, in contrast to the global linear and
    polynomial regression models that we discussed previously. In other words, via
    the decision tree algorithm, we subdivide the input space into smaller regions
    that become more manageable.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree regression
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An advantage of the decision tree algorithm is that it works with arbitrary
    features and does not require any transformation of the features if we are dealing
    with nonlinear data because decision trees analyze one feature at a time, rather
    than taking weighted combinations into account. (Likewise, normalizing or standardizing
    features is not required for decision trees.) As mentioned in *Chapter 3*, *A
    Tour of Machine Learning Classifiers Using Scikit-Learn*, we grow a decision tree
    by iteratively splitting its nodes until the leaves are pure or a stopping criterion
    is satisfied. When we used decision trees for classification, we defined entropy
    as a measure of impurity to determine which feature split maximizes the **information
    gain** (**IG**), which can be defined as follows for a binary split:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17852_09_035.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x*[i] is the feature to perform the split, *N*[p] is the number of training
    examples in the parent node, *I* is the impurity function, *D*[p] is the subset
    of training examples at the parent node, and *D*[left] and *D*[right] are the
    subsets of training examples at the left and right child nodes after the split.
    Remember that our goal is to find the feature split that maximizes the information
    gain; in other words, we want to find the feature split that reduces the impurities
    in the child nodes most. In *Chapter 3*, we discussed Gini impurity and entropy
    as measures of impurity, which are both useful criteria for classification. To
    use a decision tree for regression, however, we need an impurity metric that is
    suitable for continuous variables, so we define the impurity measure of a node,
    *t*, as the MSE instead:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[i]是进行分裂的特征，*N*[p]是父节点中的训练样本数，*I*是不纯度函数，*D*[p]是父节点中训练样本的子集，*D*[left]和*D*[right]是分裂后左右子节点中的训练样本子集。记住我们的目标是找到最大化信息增益的特征分裂；换句话说，我们希望找到能够最大程度降低子节点中不纯度的特征分裂。在第三章中，我们讨论了基尼不纯度和熵作为不纯度的度量标准，它们对于分类非常有用。然而，为了将决策树用于回归，我们需要一个适合连续变量的不纯度度量，因此我们将节点*t*的不纯度度量定义为均方误差（MSE）：
- en: '![](img/B17852_09_036.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_036.png)'
- en: 'Here, *N*[t] is the number of training examples at node *t*, *D*[t] is the
    training subset at node *t*, ![](img/B17852_09_037.png) is the true target value,
    and ![](img/B17852_09_038.png) is the predicted target value (sample mean):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N*[t]是节点*t*上的训练样本数量，*D*[t]是节点*t*上的训练子集，![](img/B17852_09_037.png)是真实的目标值，而![](img/B17852_09_038.png)是预测的目标值（样本均值）：
- en: '![](img/B17852_09_039.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_039.png)'
- en: In the context of decision tree regression, the MSE is often referred to as
    **within-node variance**, which is why the splitting criterion is also better
    known as **variance reduction**.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树回归的背景下，均方误差（MSE）通常称为**节点内方差**，这也是为什么分裂准则更为人熟知为**方差减少**。
- en: 'To see what the line fit of a decision tree looks like, let’s use the `DecisionTreeRegressor`
    implemented in scikit-learn to model the relationship between the `SalePrice`
    and `Gr Living Area` variables. Note that `SalePrice` and `Gr Living Area` do
    not necessarily represent a nonlinear relationship, but this feature combination
    still demonstrates the general aspects of a regression tree quite nicely:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解决策树回归的拟合线是什么样子，让我们使用scikit-learn中实现的`DecisionTreeRegressor`来建模`SalePrice`与`Gr
    Living Area`变量之间的关系。请注意，`SalePrice`和`Gr Living Area`不一定代表非线性关系，但这种特征组合仍然很好地展示了回归树的一般特性：
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see in the resulting plot, the decision tree captures the general
    trend in the data. And we can imagine that a regression tree could also capture
    trends in nonlinear data relatively well. However, a limitation of this model
    is that it does not capture the continuity and differentiability of the desired
    prediction. In addition, we need to be careful about choosing an appropriate value
    for the depth of the tree so as to not overfit or underfit the data; here, a depth
    of three seemed to be a good choice.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的图中可以看出，决策树捕捉了数据的一般趋势。我们可以想象，回归树也可以相对较好地捕捉非线性数据的趋势。然而，这种模型的局限性在于它不能捕捉所需预测的连续性和可微性。此外，我们需要注意选择树深度的适当值，以避免数据过拟合或欠拟合；在这里，深度为三似乎是一个不错的选择。
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_15.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B17582_09_15.png)'
- en: 'Figure 9.15: A decision tree regression plot'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：决策树回归图
- en: You are encouraged to experiment with deeper decision trees. Note that the relationship
    between `Gr Living Area` and `SalePrice` is rather linear, so you are also encouraged
    to apply the decision tree to the `Overall Qual` variable instead.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励你尝试更深的决策树。请注意，`Gr Living Area`与`SalePrice`之间的关系相当线性，因此也鼓励你将决策树应用于`Overall
    Qual`变量。
- en: 'In the next section, we will look at a more robust way of fitting regression
    trees: random forests.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨更稳健的回归树拟合方法：随机森林。
- en: Random forest regression
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: As you learned in *Chapter 3*, the random forest algorithm is an ensemble technique
    that combines multiple decision trees. A random forest usually has a better generalization
    performance than an individual decision tree due to randomness, which helps to
    decrease the model’s variance. Other advantages of random forests are that they
    are less sensitive to outliers in the dataset and don’t require much parameter
    tuning. The only parameter in random forests that we typically need to experiment
    with is the number of trees in the ensemble. The basic random forest algorithm
    for regression is almost identical to the random forest algorithm for classification
    that we discussed in *Chapter 3*. The only difference is that we use the MSE criterion
    to grow the individual decision trees, and the predicted target variable is calculated
    as the average prediction across all decision trees.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 *第三章* 中学到的，随机森林算法是一种集成技术，结合了多个决策树。随机森林通常比单个决策树具有更好的泛化性能，这归功于随机性，它有助于减少模型的方差。随机森林的其他优点是，在数据集中不太敏感于异常值，并且不需要太多的参数调整。在随机森林中，我们通常需要尝试不同的参数是集成中的树的数量。回归的基本随机森林算法几乎与我们在
    *第三章* 中讨论的分类随机森林算法相同。唯一的区别是我们使用 MSE 准则来生长单个决策树，并且预测的目标变量是跨所有决策树的平均预测。
- en: 'Now, let’s use all the features in the Ames Housing dataset to fit a random
    forest regression model on 70 percent of the examples and evaluate its performance
    on the remaining 30 percent, as we have done previously in the *Evaluating the
    performance of linear regression models* section. The code is as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用艾姆斯房屋数据集中的所有特征，在 70% 的示例上拟合一个随机森林回归模型，并在剩余的 30% 上评估其性能，就像我们在 *评估线性回归模型性能*
    部分中所做的那样。代码如下：
- en: '[PRE38]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Unfortunately, you can see that the random forest tends to overfit the training
    data. However, it’s still able to explain the relationship between the target
    and explanatory variables relatively well (![](img/B17852_09_040.png) on the test
    dataset). For comparison, the linear model from the previous section, *Evaluating
    the performance of linear regression models*, which was fit to the same dataset,
    was overfitting less but performed worse on the test set (![](img/B17852_09_041.png)).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你可以看到随机森林倾向于过度拟合训练数据。然而，它仍能相对良好地解释目标与解释变量之间的关系（在测试数据集上的 ![](img/B17852_09_040.png)）。作为比较，在前一节
    *评估线性回归模型性能* 中拟合到相同数据集的线性模型过度拟合较少，但在测试集上表现较差（![](img/B17852_09_041.png)）。
- en: 'Lastly, let’s also take a look at the residuals of the prediction:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们也来看看预测的残差：
- en: '[PRE39]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As it was already summarized by the *R*² coefficient, you can see that the model
    fits the training data better than the test data, as indicated by the outliers
    in the *y* axis direction. Also, the distribution of the residuals does not seem
    to be completely random around the zero center point, indicating that the model
    is not able to capture all the exploratory information. However, the residual
    plot indicates a large improvement over the residual plot of the linear model
    that we plotted earlier in this chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *R*² 系数已经总结的那样，你可以看到模型对训练数据的拟合比测试数据更好，这由 *y* 轴方向的异常值表示。此外，残差的分布似乎并不完全围绕零中心点随机，这表明模型无法捕捉所有的探索信息。然而，残差图表明相对于本章早期绘制的线性模型的残差图有了很大的改进。
- en: '![Scatter chart  Description automatically generated](img/B17582_09_16.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![散点图 描述自动生成](img/B17582_09_16.png)'
- en: 'Figure 9.16: The residuals of the random forest regression'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：随机森林回归的残差
- en: Ideally, our model error should be random or unpredictable. In other words,
    the error of the predictions should not be related to any of the information contained
    in the explanatory variables; rather, it should reflect the randomness of the
    real-world distributions or patterns. If we find patterns in the prediction errors,
    for example, by inspecting the residual plot, it means that the residual plots
    contain predictive information. A common reason for this could be that explanatory
    information is leaking into those residuals.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的模型误差应该是随机或不可预测的。换句话说，预测误差不应与解释变量中包含的任何信息相关；相反，它应反映真实世界分布或模式的随机性。如果我们在预测误差中发现模式，例如通过检查残差图，这意味着残差图包含预测信息。这种常见的原因可能是解释信息渗入到这些残差中。
- en: Unfortunately, there is not a universal approach for dealing with non-randomness
    in residual plots, and it requires experimentation. Depending on the data that
    is available to us, we may be able to improve the model by transforming variables,
    tuning the hyperparameters of the learning algorithm, choosing simpler or more
    complex models, removing outliers, or including additional variables.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, you learned about simple linear regression
    analysis to model the relationship between a single explanatory variable and a
    continuous response variable. We then discussed a useful explanatory data analysis
    technique to look at patterns and anomalies in data, which is an important first
    step in predictive modeling tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'We built our first model by implementing linear regression using a gradient-based
    optimization approach. You then saw how to utilize scikit-learn’s linear models
    for regression and also implement a robust regression technique (RANSAC) as an
    approach for dealing with outliers. To assess the predictive performance of regression
    models, we computed the mean sum of squared errors and the related *R*² metric.
    Furthermore, we also discussed a useful graphical approach for diagnosing the
    problems of regression models: the residual plot.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: After we explored how regularization can be applied to regression models to
    reduce the model complexity and avoid overfitting, we also covered several approaches
    for modeling nonlinear relationships, including polynomial feature transformation
    and random forest regressors.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: We discussed supervised learning, classification, and regression analysis in
    detail in the previous chapters. In the next chapter, we are going to learn about
    another interesting subfield of machine learning, unsupervised learning, and also
    how to use cluster analysis to find hidden structures in data in the absence of
    target variables.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
