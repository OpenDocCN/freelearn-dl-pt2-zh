- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: What are Transformers?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是变压器？
- en: Transformers are industrialized, homogenized post-deep learning models designed
    for parallel computing on supercomputers. Through homogenization, one transformer
    model can carry out a wide range of tasks with no fine-tuning. Transformers can
    perform self-supervised learning on billions of records of raw unlabeled data
    with billions of parameters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是工业化、同质化的后深度学习模型，设计用于超级计算机上的并行计算。通过同质化，一个变压器模型可以在不经过微调的情况下执行各种任务。变压器可以对数十亿条原始未标记数据进行自监督学习，其参数达到数十亿个。
- en: These particular architectures of post-deep learning are called **foundation
    models**. Foundation model transformers represent the epitome of the Fourth Industrial
    Revolution that began in 2015 with machine-to-machine automation that will connect
    everything to everything. Artificial intelligence in general and specifically
    **Natural Language Processing** (**NLP**) for **Industry 4.0** (**I4.0**) has
    gone far beyond the software practices of the past.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊的后深度学习架构被称为**基础模型**。基础模型变压器代表了 2015 年开始的第四次工业革命的顶峰，即机器对机器自动化将一切连接到一切。人工智能总体上，特别是**自然语言处理**（**NLP**）对**工业
    4.0**（**I4.0**）的发展已经远远超越了过去的软件实践。
- en: In less than five years, AI has become an effective cloud service with seamless
    APIs. The former paradigm of downloading libraries and developing is becoming
    an educational exercise in many cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到五年的时间里，人工智能已经成为一个具有无缝 API 的有效云服务。以前的下载库和开发范式在许多情况下已经成为一种教育练习。
- en: An Industry 4.0 project manager can go to OpenAI’s cloud platform, sign up,
    obtain an API key, and get to work in a few minutes. A user can then enter a text,
    specify the NLP task, and obtain a response sent by a GPT-3 transformer engine.
    Finally, a user can go to GPT-3 Codex and create applications with no knowledge
    of programming. Prompt engineering is a new skill that emerged from these models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 项目经理可以进入 OpenAI 的云平台，注册，获取 API 密钥，并在几分钟内开始工作。用户然后可以输入文本，指定 NLP 任务，并获得由
    GPT-3 变压器引擎发送的响应。最后，用户可以转到 GPT-3 Codex，并创建没有编程知识的应用程序。提示工程是从这些模型中出现的新技能。
- en: However, sometimes a GPT-3 model might not fit a specific task. For example,
    a project manager, consultant, or developer might want to use another system provided
    by Google AI, **Amazon Web Services** (**AWS**), the Allen Institute for AI, or
    Hugging Face.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时候 GPT-3 模型可能不适合特定的任务。例如，项目经理、顾问或开发人员可能希望使用由谷歌 AI、**亚马逊网络服务**（**AWS**）、艾伦人工智能研究所或
    Hugging Face 提供的另一个系统。
- en: Should a project manager choose to work locally? Or should the implementation
    be done directly on Google Cloud, Microsoft Azure, or AWS? Should a development
    team select Hugging Face, Google Trax, OpenAI, or AllenNLP? Should an artificial
    intelligence specialist or a data scientist use an API with practically no AI
    development?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 项目经理应该选择在本地工作吗？还是应该直接在谷歌云、微软 Azure 或 AWS 上实施？开发团队应该选择 Hugging Face、Google Trax、OpenAI
    还是 AllenNLP？人工智能专家或数据科学家是否应该使用几乎没有 AI 开发的 API？
- en: The answer is *all* the above. You do not know what a future employer, customer,
    or user may want or specify. Therefore, you must be ready to adapt to any need
    that comes up. This book does not describe all the offers that exist on the market.
    However, this book provides the reader with enough solutions to adapt to Industry
    4.0 AI-driven NLP challenges.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是*以上全部*。您不知道未来的雇主、客户或用户可能想要或指定什么。因此，您必须准备好适应任何可能出现的需求。本书并未描述市场上存在的所有提供。但是，本书为读者提供了足够的解决方案，以适应工业
    4.0 AI 驱动的 NLP 挑战。
- en: This chapter first explains what transformers are at a high level. Then the
    chapter explains the importance of acquiring a flexible understanding of all types
    of methods to implement transformers. The definition of platforms, frameworks,
    libraries, and languages is blurred by the number of APIs and automation available
    on the market.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先从高层次解释了变压器是什么。然后，本章解释了获取对实现变压器的所有类型方法的灵活理解的重要性。平台、框架、库和语言的定义在市场上提供的 API
    和自动化的数量下变得模糊。
- en: Finally, this chapter introduces the role of an Industry 4.0 AI specialist with
    advances in embedded transformers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章介绍了在嵌入式变压器方面的工业 4.0 人工智能专家的角色。
- en: We need to address these critical notions before starting our journey to explore
    the variety of transformer model implementations described in this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始我们探索本书中描述的各种变压器模型实现之前，我们需要解决这些关键概念。
- en: 'This chapter covers the following topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: The emergence of the Fourth Industrial Revolution, Industry 4.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四次工业革命，工业4.0的出现
- en: The paradigm change of foundation models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型的范式变革
- en: Introducing prompt engineering, a new skill
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入提示工程，一门新的技能
- en: The background of transformers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器的背景
- en: The challenges of implementing transformers
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施变压器的挑战
- en: The game-changing transformer model APIs
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颠覆性的变压器模型API
- en: The difficulty of choosing a transformer library
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择变压器库的难度
- en: The difficulty of choosing a transformer model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择变压器模型的困难
- en: The new role of an Industry 4.0 artificial intelligence specialist
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工业4.0人工智能专家的新角色
- en: Embedded transformers
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式变压器
- en: Our first step will be to explore the ecosystem of transformers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是探索变压器的生态系统。
- en: The ecosystem of transformers
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器的生态系统
- en: 'Transformer models represent such a paradigm change that they require a new
    name to describe them: **foundation models**. Accordingly, Stanford University
    created the **Center for Research on Foundation Models** (**CRFM**). In August
    2021, the CRFM published a two-hundred-page paper (see the *References* section)
    written by over one hundred scientists and professionals: *On the Opportunities
    and Risks of Foundation Models*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型代表了一种范式变革，需要一个新名称来描述它们：**基础模型**。因此，斯坦福大学创建了**基础模型研究中心**（**CRFM**）。2021年8月，CRFM发表了一份200页长的论文（见*参考文献*部分），由一百多位科学家和专业人士所写：《关于基础模型的机遇和风险》。
- en: Foundation models were not created by academia but by the big tech industry.
    For example, Google invented the transformer model, which led to Google BERT.
    Microsoft entered a partnership with OpenAI to produce GPT-3.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型并不是由学术界而是由大型科技行业所创建。例如，谷歌发明了变压器模型，从而导致了Google BERT。微软与OpenAI合作生产了GPT-3。
- en: Big tech had to find a better model to face the exponential increase of petabytes
    of data flowing into their data centers. Transformers were thus born out of necessity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大型科技公司不得不找到一个更好的模型来应对其数据中心中流动的百万吉字节数据的指数增长。因此，变压器由此诞生。
- en: Let’s first take Industry 4.0 into consideration to understand the need to have
    industrialized artificial intelligence models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑工业4.0，以了解需要具有工业化人工智能模型的需求。
- en: Industry 4.0
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工业4.0
- en: The Agricultural Revolution led to the First Industrial Revolution, which introduced
    machinery. The Second Industrial Revolution gave birth to electricity, the telephone,
    and airplanes. The Third Industrial Revolution was digital.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 农业革命引发了第一次工业革命，引入了机械。第二次工业革命诞生了电力、电话和飞机。第三次工业革命是数字化的。
- en: 'The Fourth Industrial Revolution, or Industry 4.0, has given birth to an unlimited
    number of machine to machine connections: bots, robots, connected devices, autonomous
    cars, smartphones, bots that collect data from social media storage, and more.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第四次工业革命，或工业4.0，产生了无限的机器之间的连接：机器人、机器人、连接设备、自动驾驶汽车、智能手机、从社交媒体存储库搜集数据的机器人等。
- en: 'In turn, these millions of machines and bots generate billions of data records
    every day: images, sound, words, and events, as shown in *Figure 1.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，这些数百万台机器和机器人每天生成数十亿的数据记录：图像、声音、文字和事件，如*图1.1*所示：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_01_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文本描述的图像](img/B17948_01_01.png)'
- en: 'Figure 1.1: The scope of Industry 4.0'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：工业4.0的范围
- en: Industry 4.0 requires intelligent algorithms that process data and make decisions
    without human intervention on a large scale to face this unseen amount of data
    in the history of humanity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 工业4.0需要进行大规模的数据处理和决策的智能算法，以应对人类历史上前所未有的数据量而无需人为干预。
- en: Big tech needed to find a single AI model that could perform a variety of tasks
    that required several separate algorithms in the past.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大型科技公司需要找到一个能够执行以往需要多个独立算法的各种任务的单一AI模型。
- en: Foundation models
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型
- en: 'Transformers have two distinct features: a high level of homogenization and
    mind-blowing emergence properties. *Homogenization* makes it possible to use one
    model to perform a wide variety of tasks. These abilities *emerge* through training
    billion-parameter models on supercomputers.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器具有两个显著特点：高度同质化和令人震惊的新兴特性。 *同质化* 使得可以使用一个模型执行各种任务。这些能力 *通过* 在超级计算机上训练千亿参数模型而得到。
- en: 'The paradigm change makes foundation models a post-deep learning ecosystem,
    as shown in *Figure 1.2*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式变化使得基础模型成为深度学习后生态系统，如*图1.2* 所示：
- en: '![](img/B17948_01_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_02.png)'
- en: 'Figure 1.2: The scope of an I4.0 AI specialist'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：I4.0人工智能专家的范围
- en: Foundation models, although designed with an innovative architecture, are built
    on top of the history of AI. As a result, an artificial intelligence specialist’s
    range of skills is stretching!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型，虽然设计了创新的架构，但是建立在人工智能的历史基础之上。因此，人工智能专家的技能范围正在拓展！
- en: 'The present ecosystem of transformer models is unlike any other evolution in
    artificial intelligence and can be summed up with four properties:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目前变压器模型的生态系统与人工智能领域的任何其他演进都不同，并且可以总结为四个特性：
- en: '**Model architecture**'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: The model is industrial. The layers of the model are identical, and they are
    specifically designed for parallel processing. We will go through the architecture
    of transformers in *Chapter 2*, *Getting Started with the Architecture of the
    Transformer Model*.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型是工业化的。模型的层是相同的，它们专门设计用于并行处理。我们将在*第2章* *变压器模型架构入门*中详细介绍变压器的架构。
- en: '**Data**'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**'
- en: Big tech possesses the hugest data source in the history of humanity, first
    generated by the Third Industrial Revolution (digital) and boosted to unfathomable
    sizes by Industry 4.0.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大型科技公司拥有人类历史上最庞大的数据来源，首先由第三次工业革命（数字化）产生，并由工业4.0推动至不可想象的规模。
- en: '**Computing power**'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算能力**'
- en: Big tech possesses computer power never seen before at that scale. For example,
    GPT-3 was trained at about 50 PetaFLOPS/second, and Google now has domain-specific
    supercomputers that exceed 80 PetaFLOPS/second.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大型科技公司拥有前所未见规模的计算能力。例如，GPT-3的训练速度约为50 PetaFLOPS/秒，谷歌现在拥有领域特定的超级计算机，超过了80 PetaFLOPS/秒。
- en: '**Prompt engineering**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**'
- en: Highly trained transformers can be triggered to do a task with a prompt. The
    prompt is entered in natural language. However, the words used require some structure,
    making prompts a metalanguage.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经过高度训练的变压器可以通过提示来执行任务。提示以自然语言输入。然而，所使用的单词需要一定的结构，这使得提示成为一种元语言。
- en: A foundation model is thus a transformer model that has been trained on supercomputers
    on billions of records of data and billions of parameters. The model can then
    perform a wide range of tasks with no further fine-tuning. Thus, the scale of
    foundation models is unique. These fully trained models are often called engines.
    Only GPT-3, Google BERT, and a handful of transformer engines can thus qualify
    as foundation models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基础模型是在亿万条数据和亿级参数的超级计算机上进行训练的变压器模型。该模型然后可以执行各种任务，而无需进一步的微调。因此，基础模型的规模是独一无二的。这些完全训练的模型通常被称为引擎。只有GPT-3、Google
    BERT和少数几个变压器引擎可以被视为基础模型。
- en: I will only refer to foundation models in this book when mentioning OpenAI’s
    GPT-3 or Google’s BERT model. This is because GPT-3 and Google BERT were fully
    trained on supercomputers. Though interesting and effective for limited use, other
    models do not reach the homogenization level of foundation models due to the lack
    of resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本书中只会在提及OpenAI的GPT-3或Google的BERT模型时提及基础模型。这是因为GPT-3和Google BERT是在超级计算机上进行完全训练的。尽管其他模型对于有限的用途来说很有趣且有效，但由于缺乏资源，其他模型并未达到基础模型的同质化水平。
- en: Let’s now explore an example of how foundation models work and have changed
    the way we develop programs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探究一个基础模型是如何工作的，并且如何改变了我们开发程序的方式。
- en: Is programming becoming a sub-domain of NLP?
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编程现在是否成为NLP的一个子领域？
- en: '*Chen* et al. (2021) published a bombshell paper in August 2021 on Codex, a
    GPT-3 model that can convert natural language into source code. Codex was trained
    on 54 million public GitHub software repositories. Codex can produce interesting
    natural language to source code, as we will see in *Chapter 16*, *The Emergence
    of Transformer-Driven Copilots*.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chen*等人（2021年）于2021年8月发表了一篇爆炸性的论文，介绍了Codex，这是一个能将自然语言转换为源代码的GPT-3模型。Codex是在5400万个公共GitHub软件仓库上进行训练的。Codex可以生成有趣的自然语言到源代码，我们将在*第16章*
    *变压器驱动共同合作者的出现* 中看到。'
- en: Is programming now a translation task from natural language to source code languages?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编程是一种从自然语言到源代码语言的翻译任务吗？
- en: Is programming becoming an NLP task for GPT-3 engines?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 编程是否正在成为GPT-3引擎的一项NLP任务？
- en: Let’s look into an example before answering that question.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答那个问题之前，让我们看一个例子。
- en: Bear in mind that Codex is a stochastic algorithm, so the metalanguage is tricky.
    You might not generate what you expect if you are not careful to engineer the
    prompt correctly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Codex是一种随机算法，所以元语言很棘手。如果您不小心正确地设计提示，可能不会生成您期望的结果。
- en: I created some prompts as I was experimenting with Codex. This example is just
    to give an idea of how Codex works and is purely for educational purposes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我试验Codex时，我创建了一些提示。这个例子只是为了让您了解Codex的工作原理，纯粹是为了教育目的。
- en: 'My prompts were:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我的提示是：
- en: “generate a random distribution of 200 integers between 1 and 100” in Python
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中“生成200个介于1到100之间的随机整数的随机分布”
- en: “plot the data using matplotlib”
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “使用matplotlib绘制数据”
- en: “create a k-means clustering model with 3 centroids and fit the model”
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “创建具有3个质心的k均值聚类模型并拟合模型”
- en: “print the cluster labels”
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “打印聚类标签”
- en: “plot the clusters”
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “绘制聚类”
- en: “plot the clusters with centroids”
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “绘制带有质心的聚类”
- en: Codex translated my natural metalanguage prompts into Python automatically!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Codex自动将我的自然元语言提示翻译成Python！
- en: Codex is a stochastic model, so it might not reproduce exactly the same code
    if you try again. You will have to learn the metalanguage through experimentation
    until you can drive it like a race car!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Codex是一种随机模型，因此如果您再次尝试，可能不会完全再现相同的代码。您将不得不通过实验来学习元语言，直到能像驾驶赛车一样驾驭它！
- en: 'The Python program is generated automatically and can be copied and tested:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Python程序将自动生成，并可复制和测试：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can copy and paste this program. It works. You can also try JavaScript,
    among other experiments.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以复制粘贴这个程序。它有效。你也可以尝试JavaScript，等等其他实验。
- en: GitHub Copilot is now available with some Microsoft developing tools, as we
    will see in *Chapter 16*, *The Emergence of Transformer-Driven Copilots*. If you
    learn the prompt engineering metalanguage, you will reduce your development time
    in years to come.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot现已与一些微软开发工具一起提供，我们将在*第16章*《基于Transformer的协作工具的出现》中看到。如果您学会了提示工程元语言，您将在未来几年内缩短开发时间。
- en: End users can create prototypes and or small tasks if they master the metalanguage.
    In the future, coding copilots will expand.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 终端用户如果掌握了元语言，就能创建原型或小任务。未来，编码协助工具将会扩展。
- en: We will see where Codex fits in the future of artificial intelligence in *Chapter
    16*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第16章*中看到Codex在人工智能未来中的地位。
- en: At this point, let’s take a glimpse into the bright future of artificial intelligence
    specialists.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，让我们一睹人工智能专家美好未来的一瞥。
- en: The future of artificial intelligence specialists
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能专家的未来
- en: The societal impact of foundation models should not be underestimated. Prompt
    engineering has become a skill required for artificial intelligence specialists.
    However, the future of AI specialists cannot be limited to transformers. AI and
    data science overlap in I4.0.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的社会影响不容小觑。提示工程已成为人工智能专家所需的一项技能。然而，人工智能专家的未来不能仅限于变压器。人工智能和数据科学在I4.0中重叠。
- en: An AI specialist will be involved in machine to machine algorithms using classical
    AI, IoT, edge computing, and more. An AI specialist will also design and develop
    fascinating connections between bots, robots, servers, and all types of connected
    devices using classical algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: AI专家将参与使用经典人工智能、物联网、边缘计算等机器之间算法。AI专家还将使用经典算法设计和开发有趣的机器人、服务器和各种连接设备之间的连接。
- en: This book is thus not limited to prompt engineering but to a wide range of design
    skills required to be an “Industry 4.0 artificial intelligence specialist” or
    “I4.0 AI specialist.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这本书不仅限于提示工程，还涵盖了成为“工业4.0人工智能专家”或“I4.0人工智能专家”所需的广泛设计技能。
- en: Prompt engineering is a subset of the design skills an AI specialist will have
    to develop. In this book, I will thus refer to the future AI specialist as an
    “Industry 4.0 artificial intelligence specialist.”
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是AI专家将必须开发的设计技能的子集。在这本书中，我将将未来的AI专家称为“工业4.0人工智能专家”。
- en: Let’s now get a general view of how transformers optimize NLP models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对变压器如何优化NLP模型有一个总体了解。
- en: Optimizing NLP models with transformers
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用变压器优化NLP模型
- en: '**Recurrent Neural Networks** (**RNNs**), including LSTMs, have applied neural
    networks to NLP sequence models for decades. However, using recurrent functionality
    reaches its limit when faced with long sequences and large numbers of parameters.
    Thus, state-of-the-art transformer models now prevail.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNNs**），包括LSTMs，几十年来一直将神经网络应用于自然语言处理的序列模型。然而，当面对长序列和大量参数时，使用递归功能达到了极限。因此，现在最先进的转换器模型占据了主导地位。'
- en: This section goes through a brief background of NLP that led to transformers,
    which we’ll describe in more detail in *Chapter 2*, *Getting Started with the
    Architecture of the Transformer Model*. First, however, let’s have an intuitive
    look at the attention head of a transformer that has replaced the RNN layers of
    an NLP neural network.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将简要介绍导致变压器的自然语言处理背景，我们将在*第2章*中更详细地描述这一点，*开始使用变压器模型的架构*。然而，首先，让我们直观地看一下变压器的注意头，它取代了自然语言处理神经网络的RNN层。
- en: 'The core concept of a transformer can be summed up loosely as “mixing tokens.”
    NLP models first convert word sequences into tokens. RNNs analyze tokens in recurrent
    functions. Transformers do not analyze tokens in sequences but relate every token
    to the other tokens in a sequence, as shown in *Figure 1.3*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的核心概念可以粗略地总结为“混合标记”。自然语言处理模型首先将单词序列转换为标记。递归神经网络在递归函数中分析标记。变压器不会按顺序分析标记，而是将每个标记与序列中的其他标记相关联，如*图1.3*所示：
- en: '![](img/B17948_01_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_03.png)'
- en: 'Figure 1.3: An attention head of a layer of a transformer'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：变压器层的一个注意头
- en: We will go through the details of an attention head in *Chapter 2*. For the
    moment, the takeaway of *Figure 1.3* is that *each word* (*token*) *of a sequence
    is related to all the other words of a sequence*. This model opens the door to
    Industry 4.0 NLP.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第2章*详细讨论一个注意力头的细节。目前，*图1.3*的要点是*序列的每个单词*（*标记*）*都与序列的所有其他单词相关*。这个模型为工业4.0的自然语言处理打开了大门。
- en: Let’s briefly go through the background of transformers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地了解一下变压器的背景。
- en: The background of transformers
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器的背景
- en: Over the past 100+ years, many great minds have worked on sequence patterns
    and language modeling. As a result, machines progressively learned how to predict
    probable sequences of words. It would take a whole book to cite all the giants
    that made this happen.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的100多年里，许多伟大的思想家都致力于序列模式和语言建模。因此，机器逐渐学会了预测可能的单词序列。要列出所有使这一切发生的巨人，需要一整本书。
- en: In this section, I will share some of my favorite researchers with you to lay
    the grounds for the arrival of the Transformer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我将与您分享一些我喜欢的研究人员，为变压器的到来奠定基础。 '
- en: In the early 20th century, Andrey Markov introduced the concept of random values
    and created a theory of stochastic processes. We know them in AI as the **Markov
    Decision Process** (**MDP**), **Markov Chains**, and **Markov Processes**. In
    the early 20^(th) century, Markov showed that we could predict the next element
    of a chain, a sequence, using only the last past elements of that chain. He applied
    his method to a dataset containing thousands of letters using past sequences to
    predict the following letters of a sentence. Bear in mind that he had no computer
    but proved a theory still in use today in artificial intelligence.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪初，安德烈·马尔可夫介绍了随机值的概念，并创建了随机过程的理论。我们在人工智能中称之为**马尔可夫决策过程**（**MDP**）、**马尔可夫链**和**马尔可夫过程**。在20世纪初，马尔可夫证明了我们可以仅使用链的最后几个元素来预测链、序列的下一个元素。他将自己的方法应用到一个包含数千封信的数据集中，使用过去的序列来预测句子的后续字母。请记住，他没有计算机，但证明了一种至今在人工智能中仍在使用的理论。
- en: In 1948, Claude Shannon’s *The Mathematical Theory of Communication* was published.
    Claude Shannon laid the grounds for a communication model based on a source encoder,
    transmitter, and a receiver or semantic decoder. He created information theory
    as we know it today.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1948年，克劳德·香农出版了*通信的数学理论*。克劳德·香农奠定了基于源编码器、传输器和接收器或语义解码器的通信模型的基础。他创造了今天我们所知的信息论。
- en: 'In 1950, Alan Turing published his seminal article: *Computing Machinery and
    Intelligence*. Alan Turing based this article on machine intelligence on the successful
    Turing machine, which decrypted German messages during World War II. The messages
    consisted of sequences of words and numbers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 1950年，艾伦·图灵发表了他的里程碑文章：*计算机与智能*。艾伦·图灵将这篇文章基于在二战期间解密德国消息的成功图灵机的机器智能。这些消息由一系列单词和数字组成。
- en: In 1954, the Georgetown-IBM experiment used computers to translate Russian sentences
    into English using a rule system. A rule system is a program that runs a list
    of rules that will analyze language structures. Rule systems still exist and are
    everywhere. However, in some cases, machine intelligence can replace rule lists
    for the billions of language combinations by automatically learning the patterns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1954 年，乔治城-IBM 实验使用计算机利用规则系统将俄语句子翻译成英语。 规则系统是一个运行规则列表的程序，该列表将分析语言结构。 规则系统仍然存在，并且无处不在。
    但是，在某些情况下，机器智能可以通过自动学习模式来替代数十亿种语言组合的规则列表。
- en: The expression “Artificial Intelligence” was first used by John McCarthy in
    1956 when it was established that machines could learn.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1956 年，约翰·麦卡锡首次使用了“人工智能”这个表达，当时确立了机器可以学习的事实。
- en: In 1982, John Hopfield introduced an **RNN**, known as Hopfield networks or
    “associative” neural networks. John Hopfield was inspired by W.A. Little, who
    wrote *The existence of persistent states in the brain* in 1974, which laid the
    theoretical grounds of learning processes for decades. RNNs evolved, and LSTMs
    emerged as we know them today.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1982 年，约翰·霍普菲尔德引入了一种称为霍普菲尔德网络或“联想”神经网络的**循环神经网络**（**RNN**）。 约翰·霍普菲尔德受到 W.A.
    Little 的启发，后者在 1974 年撰写了*大脑中持久状态的存在*，为学习过程奠定了几十年的理论基础。 RNN 不断发展，以及我们今天所知的 LSTM
    也随之出现。
- en: 'An RNN memorizes the persistent states of a sequence efficiently, as shown
    in *Figure 1.4*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 高效地记忆了序列的持久状态，如*图 1.4*所示：
- en: '![](img/B17948_01_04.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_04.png)'
- en: 'Figure 1.4: The RNN process'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：RNN 过程
- en: 'Each state *S*[n] captures the information of *S*[n-1]. When the network’s
    end is reached, a function *F* will perform an action: transduction, modeling,
    or any other type of sequence-based task.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态 *S*[n] 捕获 *S*[n-1] 的信息。 当网络的末端到达时，函数 *F* 将执行一个动作：转换、建模或任何其他类型的基于序列的任务。
- en: In the 1980s, Yann LeCun designed the multipurpose **Convolutional Neural Network**
    (**CNN**). He applied CNNs to text sequences, and they also apply to sequence
    transduction and modeling. They are also based on W.A. Little’s persistent states
    that process information layer by layer. In the 1990s, summing up several years
    of work, Yann LeCun produced LeNet-5, which led to the many CNN models we know
    today. However, a CNN’s otherwise efficient architecture faces problems when dealing
    with long-term dependencies in lengthy and complex sequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1980 年代，Yann LeCun 设计了多功能的**卷积神经网络**（**CNN**）。 他将 CNN 应用于文本序列，它们还适用于序列转换和建模。
    它们还基于 W.A. Little 的持久状态，该状态逐层处理信息。 在 1990 年代，总结了几年的工作后，Yann LeCun 制作了 LeNet-5，导致了我们今天所知道的许多
    CNN 模型。 但是，在处理冗长且复杂序列中的长期依赖关系时，CNN 的有效架构面临问题。
- en: 'We could mention many other great names, papers, and models that would humble
    any AI specialist. It seemed that everybody in AI was on the right track for all
    these years. Markov Fields, RNNs, and CNNs evolved into multiple other models.
    The notion of attention appeared: peeking at other tokens in a sequence, not just
    the last one. It was added to the RNN and CNN models.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提到许多其他的名字、论文和模型，这些都会让任何 AI 专家感到自愧不如。 AI 领域的每个人似乎都走在正确的轨道上。 马尔可夫场、RNN 和 CNN
    发展成了多种其他模型。 注意力的概念出现了：窥视序列中的其他令牌，而不仅仅是最后一个。 它被添加到了 RNN 和 CNN 模型中。
- en: After that, if AI models needed to analyze longer sequences requiring increasing
    computer power, AI developers used more powerful machines and found ways to optimize
    gradients.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，如果 AI 模型需要分析需要增加计算机功率的更长序列，AI 开发人员使用了更强大的机器，并找到了优化梯度的方法。
- en: Some research was done on sequence-to-sequence models, but they did not meet
    expectations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于序列到序列模型进行了一些研究，但并未达到预期。
- en: It seemed that nothing else could be done to make more progress. Thirty years
    passed this way. And then, starting late 2017, the industrialized state-of-the-art
    Transformer came with its attention head sublayers and more. RNNs did not appear
    as a pre-requisite for sequence modeling anymore.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来似乎没有其他方法可以取得更多进展。 这样过了三十年。 然后，从 2017 年末开始，工业化的最先进的 Transformer 带来了它的注意力头子层等等。
    RNN 不再被视为序列建模的先决条件。
- en: Before diving into the original Transformer’s architecture, which we will do
    in *Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    let’s start at a high level by examining the paradigm change in software resources
    we should use to learn and implement transformer models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究原始 Transformer 的架构之前，我们将在 *第二章*，*开始使用 Transformer 模型的架构* 中进行。让我们从高层次上审视一下学习和实现
    transformer 模型应该使用的软件资源的范式变化。
- en: What resources should we use?
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该使用什么资源？
- en: Industry 4.0 AI has blurred the lines between cloud platforms, frameworks, libraries,
    languages, and models. Transformers are new, and the range and number of ecosystems
    are mind-blowing. Google Cloud provides ready-to-use transformer models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 AI 已经模糊了云平台、框架、库、语言和模型之间的界限。Transformer 是新的，生态系统的范围和数量令人惊叹。Google Cloud
    提供了可直接使用的 transformer 模型。
- en: OpenAI has deployed a “Transformer” API that requires practically no programming.
    Hugging Face provides a cloud library service, and the list is endless.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 部署了一个几乎不需要编程的“Transformer” API。Hugging Face 提供了一个云库服务，列表是无穷无尽的。
- en: This chapter will go through a high-level analysis of some of the transformer
    ecosystems we will be implementing throughout this book.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将对我们将在本书中实施的一些 transformer 生态系统进行高层次的分析。
- en: Your choice of resources to implement transformers for NLP is critical. It is
    a question of survival in a project. Imagine a real-life interview or presentation.
    Imagine you are talking to your future employer, your employer, your team, or
    a customer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的资源来实现 NLP 的 transformer 是至关重要的。这是一个在项目中生存的问题。想象一下现实生活中的面试或演示。想象一下你正在与你的未来雇主、你的雇主、你的团队或一个客户交谈。
- en: You begin your presentation with an excellent PowerPoint with Hugging Face,
    for example. You might get an adverse reaction from a manager who may say, “*I’m
    sorry, but we use Google Trax here for this type of project, not Hugging Face.
    Can you implement Google Trax, please?*” If you don’t, it’s game over for you.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你以一个出色的 PowerPoint 开始你的演示，比如说使用 Hugging Face。例如。 你可能会从一个经理那里得到一个负面的反应，他可能会说：“*对不起，但是我们在这种类型的项目中使用的是
    Google Trax，而不是 Hugging Face。你能实现 Google Trax 吗？*” 如果你不这样做，那么游戏结束了。
- en: The same problem could have arisen by specializing in Google Trax. But, instead,
    you might get the reaction of a manager who wants to use OpenAI’s GPT-3 engines
    with an API and no development. If you specialize in OpenAI’s GPT-3 engines with
    APIs and no development, you might face a project manager or customer who prefers
    Hugging Face’s AutoML APIs. The worst thing that could happen to you is that a
    manager accepts your solution, but in the end, it does not work at all for the
    NLP tasks of that project.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 专门从事 Google Trax 可能会出现同样的问题。但是，相反，你可能会得到一个希望使用 OpenAI 的 GPT-3 引擎和 API 而不进行开发的经理的反应。如果你专门使用
    OpenAI 的 GPT-3 引擎和 API 而不进行开发，你可能会面临一个希望使用 Hugging Face 的 AutoML API 的项目经理或客户。最糟糕的事情是，一个经理接受了你的解决方案，但最终它根本不适用于该项目的
    NLP 任务。
- en: The key concept to keep in mind is that if you only focus on the solution that
    you like, you will most likely sink with the ship at some point.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要牢记的关键概念是，如果你只关注你喜欢的解决方案，你很可能最终会随着船沉没。
- en: Focus on the system you need, not the one you like.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关注你需要的系统，而不是你喜欢的那个。
- en: This book is not designed to explain every transformer solution that exists
    on the market. Instead, this book aims to explain enough transformer ecosystems
    for you to be flexible and adapt to any situation you face in an NLP project.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书并不是为了解释市场上存在的每一个 transformer 解决方案。相反，这本书旨在为您解释足够的 transformer 生态系统，以便您能够灵活地适应您在
    NLP 项目中面临的任何情况。
- en: In this section, we will go through some of the challenges that you’ll face.
    But first, let’s begin with APIs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍您将面临的一些挑战。但首先，让我们从 API 开始。
- en: The rise of Transformer 4.0 seamless APIs
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 4.0 无缝 API 的崛起
- en: We are now well into the industrialization era of artificial intelligence. Microsoft,
    Google, **Amazon Web Services** (**AWS**), and IBM, among others, offer AI services
    that no developer or team of developers could hope to outperform. Tech giants
    have million-dollar supercomputers with massive datasets to train transformer
    models and AI models in general.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经深入到人工智能的工业化时代。微软、谷歌、**亚马逊网络服务**（**AWS**）和 IBM 等公司提供的 AI 服务，没有任何开发人员或开发团队能够超越。科技巨头拥有百万美元的超级计算机，用于训练
    transformer 模型和 AI 模型的大数据集。
- en: Big tech giants have a wide range of corporate customers that already use their
    cloud services. As a result, adding a transformer API to an existing cloud architecture
    requires less effort than any other solution.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 大型科技巨头拥有广泛的企业客户群体，已经使用他们的云服务。因此，将变压器 API 添加到现有的云架构中所需的工作量比任何其他解决方案都要少。
- en: A small company or even an individual can access the most powerful transformer
    models through an API with practically no investment in development. An intern
    can implement the API in a few days. There is no need to be an engineer or have
    a Ph.D. for such a simple implementation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小公司甚至一个个人都可以通过 API 访问最强大的变压器模型，几乎不需要投资于开发。一名实习生可以在几天内实现 API。对于这样简单的实现，不需要成为工程师或拥有博士学位。
- en: For example, the OpenAI platform now has a **SaaS** (**Software as a Service**)
    API for some of the most effective transformer models on the market.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI 平台现在为市场上一些最有效的变压器模型提供 **SaaS** (**软件即服务**) API。
- en: OpenAI transformer models are so effective and humanlike that the present policy
    requires a potential user to fill out a request form. Once the request has been
    accepted, the user can access a universe of natural language processing!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 变压器模型如此有效且人性化，以至于目前的政策要求潜在用户填写请求表格。一旦请求被接受，用户就可以访问自然语言处理的宇宙！
- en: 'The simplicity of OpenAI’s API takes the user by surprise:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 API 的简单性令用户感到惊讶：
- en: Obtain an API key in one click
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一键获取 API 密钥
- en: Import OpenAI in a notebook in one lin
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中导入 OpenAI 只需一行
- en: Enter any NLP task you wish in a *prompt*
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *提示* 中输入您希望的任何 NLP 任务
- en: You will receive the response as a *completion* in a certain number of *tokens*
    (length)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将以一定数量的 *令牌*（长度）作为 *完成* 收到响应
- en: And that’s it! Welcome to the Fourth Industrial Revolution and AI 4.0!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！欢迎来到第四次工业革命和 AI 4.0！
- en: Industry 3.0 developers that focus on code-only solutions will evolve into Industry
    4.0 developers with cross-disciplinary mindsets.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于仅代码解决方案的工业 3.0 开发者将进化为具有跨学科思维的工业 4.0 开发者。
- en: The 4.0 developer will learn how to design ways to *show* a transformer model
    what is expected and not intuitively *tell* it what to do, like a 3.0 developer
    would do. We will explore this new approach through GPT-2 and GPT-3 models in
    *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 4.0 开发者将学习如何设计一种方式来 *展示* 变压器模型期望的内容，而不是像 3.0 开发者那样直观地 *告诉* 它要做什么。我们将通过 *第 7
    章*，*GPT-3 引擎崛起的超人类变压器* 来探讨这种新方法。
- en: AllenNLP offers the free use of an online educational interface for transformers.
    AllenNLP also provides a library that can be installed in a notebook. For example,
    suppose we are asked to implement coreference resolution. We can start by running
    an example online.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 提供免费使用在线教育界面的变压器。AllenNLP 还提供一个可以安装在笔记本中的库。例如，假设我们被要求实现共指消解。我们可以从在线运行示例开始。
- en: 'Coreference resolution tasks involve finding the entity to which a word refers,
    as in the sentence shown in *Figure 1.5*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 共指消解任务涉及找到一个词所指的实体，就像在 *图 1.5* 中所示的句子中那样。
- en: '![](img/B17948_01_05.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_05.png)'
- en: 'Figure 1.5: Running an NLP task online'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：在线运行 NLP 任务
- en: 'The word “it” could refer to the website or the transformer model. In this
    case, the BERT-like model decided to link “it” to the transformer model. AllenNLP
    provides a formatted output, as shown in *Figure 1.6*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: “它” 这个词可能指的是网站或变压器模型。在这种情况下，类似 BERT 的模型决定将 “it” 链接到变压器模型。AllenNLP 提供了格式化的输出，如
    *图 1.6* 所示：
- en: '![](img/B17948_01_06.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_06.png)'
- en: 'Figure 1.6: The output of an AllenNLP transformer model'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：AllenNLP 变压器模型的输出
- en: This example can be run at [https://demo.allennlp.org/coreference-resolution](https://demo.allennlp.org/coreference-resolution).
    Transformer models are continuously updated, so you might obtain a different result.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例可以在 [https://demo.allennlp.org/coreference-resolution](https://demo.allennlp.org/coreference-resolution)
    上运行。变压器模型不断更新，因此您可能会获得不同的结果。
- en: Though APIs may satisfy many needs, they also have limits. A multipurpose API
    might be reasonably good in all tasks but not good enough for a specific NLP task.
    Translating with transformers is no easy task. In that case, a 4.0 developer,
    consultant, or project manager will have to prove that an API alone cannot solve
    the specific NLP task required. We need to search for a solid library.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 API 可能满足许多需求，但它们也有限制。一个多用途的 API 可能在所有任务中都相当不错，但对于特定的 NLP 任务来说并不够好。使用变压器进行翻译并不是一件容易的事情。在这种情况下，4.0
    开发者、顾问或项目经理将不得不证明仅靠 API 无法解决所需的特定 NLP 任务。我们需要寻找一个可靠的库。
- en: Choosing ready-to-use API-driven libraries
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择现成的 API 驱动库
- en: In this book, we will explore several libraries. For example, Google has some
    of the most advanced AI labs in the world. Google Trax can be installed in a few
    lines in Google Colab. You can choose free or paid services. We can get our hands
    on source code, tweak the models, and even train them on our servers or Google
    Cloud. For example, it’s a step down from ready-to-use APIs to customize a transformer
    model for translation tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将探索几个库。例如，Google 在世界上拥有一些最先进的人工智能实验室。Google Trax 可以在 Google Colab 中只需几行代码进行安装。你可以选择免费或付费服务。我们可以获得源代码，调整模型，甚至在我们自己的服务器或谷歌云上进行训练。例如，从使用现成的
    API 到为翻译任务定制转换器模型是一个逐步进行的过程。
- en: However, it can prove to be both educational and effective in some cases. We
    will explore the recent evolution of Google in translations and implement Google
    Trax in *Chapter 6*, *Machine Translation with the Transformer*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在某些情况下，这可能既具有教育意义，又彰显效果。我们将探索 Google 在翻译方面的最新发展，并在第六章《使用 Transformer 进行机器翻译》中实施
    Google Trax。
- en: We have seen that APIs such as OpenAI require limited developer skills, and
    libraries such as Google Trax dig a bit deeper into code. Both approaches show
    that AI 4.0 APIs will require more development on the editor side of the API but
    much less effort when implementing transformers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到像 OpenAI 这样的 API 需要有限的开发者技能，而像 Google Trax 这样的库则更深入地涉及代码。这两种方法表明，AI 4.0
    API 将需要更多的开发，但在实施转换器时需要更少的工作。
- en: One of the most famous online applications that use transformers, among other
    algorithms, is Google Translate. Google Translate can be used online or through
    an API.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中应用了转换器的最著名的线上应用之一，除其他算法之外，就是 Google 翻译。Google 翻译可以在线使用，也可以通过 API 使用。
- en: 'Let’s try to translate a sentence requiring coreference resolution in an English
    to French translation using Google Translate:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 Google 翻译将一个需要指代消解的句子从英语翻译成法语：
- en: '![](img/B17948_01_07.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_01_07.png)'
- en: 'Figure 1.7: Coreference resolution in a translation using Google Translate'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：利用 Google 翻译进行指代消解的翻译
- en: Google Translate appears to have solved the coreference resolution, but the
    word *transformateur* in French means an electric device. The word *transformer*
    is a neologism (new word) in French. An artificial intelligence specialist might
    be required to have language and linguistic skills for a specific project. Significant
    development is not required in this case. However, the project might require clarifying
    the input before requesting a translation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Google 翻译似乎已经解决了指代消解的问题，但是法语中的 *transformateur* 这个词是指电器设备。*transformer* 这个词是法语中的一个新词。在某些项目中可能需要人工智能专家具有语言和语言学技能。这种情况下并不需要进行重大开发。但是，在请求翻译之前，项目可能需要澄清输入。
- en: This example shows that you might have to team up with a linguist or acquire
    linguistic skills to work on an input context. In addition, it might take a lot
    of development to enhance the input with an interface for contexts.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，你可能需要与语言学家合作或者获取语言学技能来处理输入上下文。此外，可能需要进行大量开发来为上下文增加一个界面。
- en: So, we still might have to get our hands dirty to add scripts to use Google
    Translate. Or we might have to find a transformer model for a specific translation
    need, such as BERT, T5, or other models we will explore in this book.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可能仍然需要费一些力气才能使用 Google 翻译添加脚本。或者我们可能需要为特定的翻译需求找到一个转换器模型，比如 BERT、T5 或我们在本书中将要探索的其他模型。
- en: Choosing a model is no easy task with the increasing range of solutions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 随着解决方案范围的不断增加，选择一个模型并不是一件容易的事情。
- en: Choosing a Transformer Model
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个转换器模型
- en: Big tech corporations dominate the NLP market. Google, Facebook, and Microsoft
    alone run billions of NLP routines per day, increasing their AI models’ unequaled
    power. The big giants now offer a wide range of transformer models and have top-ranking
    foundation models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 大型科技公司主导着自然语言处理（NLP）市场。仅有谷歌、Facebook 和微软每天运行数十亿次 NLP 例程，增强了它们的人工智能模型无与伦比的力量。这些巨头现在提供了各种转换器模型，并拥有顶尖的基础模型。
- en: However, smaller companies, spotting the vast NLP market, have entered the game.
    Hugging Face now has a free or paid service approach too. It will be challenging
    for Hugging Face to reach the level of efficiency acquired through the billions
    of dollars poured into Google’s research labs and Microsoft’s funding of OpenAI.
    The entry point of foundation models is fully trained transformers on supercomputers
    such as GPT-3 or Google BERT.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更小的公司看到庞大的自然语言处理市场后也加入了竞争。Hugging Face 现在也有免费或付费的服务方式。Hugging Face 要达到通过投入数十亿美元进入
    Google 研究实验室和微软资助 OpenAI 而获得的效率水平将是一个挑战。基础模型的入口是在超级计算机上完全训练的变压器，如 GPT-3 或 Google
    BERT。
- en: Hugging Face has a different approach and offers a wide range and number of
    transformer models for a task, which is an interesting philosophy. Hugging Face
    offers flexible models. In addition, Hugging Face offers high-level APIs and developer-controlled
    APIs. We will explore Hugging Face in several chapters of this book as an educational
    tool and a possible solution for specific tasks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 采取了不同的方法，为任务提供了广泛的变压器模型选择，这是一种有趣的哲学。Hugging Face 提供灵活的模型。此外，Hugging
    Face 还提供高级 API 和开发者可控的 API。我们将在本书的几个章节中探讨 Hugging Face 作为一个教育工具和可能的特定任务解决方案。
- en: Yet, OpenAI has focused on a handful of the most potent transformer engines
    globally and can perform many NLP tasks at human levels. We will show the power
    of OpenAI’s GPT-3 engines in *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OpenAI 已经专注于全球最强大的少数几个变压器引擎，并且可以以人类水平执行许多自然语言处理任务。我们将展示 OpenAI 的 GPT-3 引擎的威力在
    *第七章*，*GPT-3 引擎崛起* 中。
- en: These opposing and often conflicting strategies leave us with a wide range of
    possible implementations. We must thus define the role of Industry 4.0 artificial
    intelligence specialists.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相反而经常冲突的策略给我们留下了一系列可能的实现方案。因此，我们必须定义工业 4.0 人工智能专家的角色。
- en: The role of Industry 4.0 artificial intelligence specialists
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工业 4.0 人工智能专家的角色
- en: Industry 4.0 is connecting everything to everything, everywhere. Machines communicate
    directly with other machines. AI-driven IoT signals trigger automated decisions
    without human intervention. NLP algorithms send automated reports, summaries,
    emails, advertisements, and more.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 正将所有事物与所有事物连接起来，无处不在。机器直接与其他机器通信。由人工智能驱动的物联网信号触发自动决策，无需人类干预。自然语言处理算法发送自动报告、摘要、电子邮件、广告等。
- en: Artificial intelligence specialists will have to adapt to this new era of increasingly
    automated tasks, including transformer model implementations. Artificial intelligence
    specialists will have new functions. If we list transformer NLP tasks that an
    AI specialist will have to do, from top to bottom, it appears that some high-level
    tasks require little to no development on the part of an artificial intelligence
    specialist. An AI specialist can be an AI guru, providing design ideas, explanations,
    and implementations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能专家将不得不适应这个新时代日益自动化的任务，包括变压器模型的实现。人工智能专家将有新的功能。如果我们按照一个人工智能专家将要执行的变压器自然语言处理任务列出来，从头到尾，似乎一些高级任务不需要人工智能专家做任何开发。一个人工智能专家可以成为一个人工智能大师，提供设计思路、解释和实现。
- en: The pragmatic definition of what a transformer represents for an artificial
    intelligence specialist will vary with the ecosystem.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器对于人工智能专家的实际定义将随着生态系统的变化而变化。
- en: 'Let’s go through a few examples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些例子来说明：
- en: '**API**: The OpenAI API does not require an AI developer. A web designer can
    create a form, and a linguist or Subject Matter Expert (SME) can prepare the prompt
    input texts. The primary role of an AI specialist will require linguistic skills
    to show, not just tell, the GPT-3 engines how to accomplish a task. Showing, for
    example, involves working on the context of the input. This new task is named
    *prompt engineering*. A *prompt engineer* has quite a future in AI!'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API**：OpenAI API 不需要一个 AI 开发者。一个网页设计师可以创建一个表单，一个语言学家或者专业领域专家（SME）可以准备提示输入文本。一个
    AI 专家的主要角色将需要语言学技能来向 GPT-3 引擎展示如何完成任务，而不仅仅是告诉它们。例如，展示涉及处理输入的上下文。这个新任务被称为 *提示工程*。*提示工程师*
    在人工智能领域有着很好的发展前景！'
- en: '**Library**: The Google Trax library requires a limited amount of development
    to start with ready-to-use models. An AI specialist mastering linguistics and
    NLP tasks can work on the datasets and the outputs.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Library**：Google Trax 库需要一定量的开发工作才能开始使用预先训练好的模型。一个精通语言学和自然语言处理任务的 AI 专家可以处理数据集和输出。'
- en: '**Training and fine-tuning**: Some of the Hugging Face functionality requires
    a limited amount of development, providing both APIs and libraries. However, in
    some cases, we still have to get our hands dirty. In that case, training, fine-tuning
    the models, and finding the correct hyperparameters will require the expertise
    of an artificial intelligence specialist.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练和调优**：一些Hugging Face功能需要有限的开发量，提供API和库。然而，在某些情况下，我们仍然需要细致入微。在这种情况下，训练、调优模型和找到正确的超参数将需要人工智能专家的专业知识。'
- en: '**Development-level skills**: In some projects, the tokenizers and the datasets
    do not match, as explained in *Chapter 9*, *Matching Tokenizers and Datasets*.
    In this case, an artificial intelligence developer working with a linguist, for
    example, can play a crucial role. Therefore, computational linguistics training
    can come in very handy at this level.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发级技能**：在一些项目中，分词器和数据集不匹配，正如*第9章*中所解释的那样，*匹配分词器和数据集*。在这种情况下，与语言学家一起工作的人工智能开发人员可以发挥关键作用。因此，在这个层面上，计算语言学培训非常有用。'
- en: 'The recent evolution of NLP AI can be termed as “embedded transformers,” which
    is disrupting the AI development ecosystem:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最近NLP AI的演进可以称为“嵌入式转换器”，正在扰乱AI开发生态系统：
- en: GPT-3 transformers are currently embedded in several Microsoft Azure applications
    with GitHub Copilot, for example. As introduced in the *Foundation models* section
    of this chapter, Codex is another example we will look into in *Chapter 16*, *The
    Emergence of Transformer-Driven Copilots*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3转换器目前嵌入在几个Microsoft Azure应用程序中，例如GitHub Copilot。正如本章节*基础模型*部分中所介绍的那样，Codex是我们将在*第16章*中探讨的另一个例子，*基于转换器的合作者的出现*。
- en: The embedded transformers are not accessible directly but provide automatic
    development support such as automatic code generation.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式转换器不能直接访问，但提供自动开发支持，如自动代码生成。
- en: The usage of embedded transformers is seamless for the end user with assisted
    text completion.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式转换器的使用对于最终用户来说是无缝的，辅助文本自动完成。
- en: To access GPT-3 engines directly, you must first create an OpenAI account. Then
    you can use the API or directly run examples in the OpenAI user interface.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要直接访问GPT-3引擎，您必须首先创建一个OpenAI账户。然后您可以使用API或直接在OpenAI用户界面中运行示例。
- en: We will explore this fascinating new world of embedded transformers in *Chapter
    16*. But to get the best out of that chapter, you should first master the previous
    chapters’ concepts, examples, and programs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第16章*中探索这个迷人的嵌入式转换器的新世界。但要充分利用该章节，您首先应该掌握前几章的概念、示例和程序。
- en: The skillset of an Industry 4.0 AI specialist requires flexibility, cross-disciplinary
    knowledge, and above all, *flexibility*. This book will provide the artificial
    intelligence specialist with a variety of transformer ecosystems to adapt to the
    new paradigms of the market.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 工业4.0 AI专家的技能需要灵活性、跨学科知识，尤其是*灵活性*。本书将为人工智能专家提供各种转换器生态系统，以适应市场的新模式。
- en: It’s time to summarize the ideas of this chapter before diving into the fascinating
    architecture of the original Transformer in *Chapter 2*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜入原始Transformer的迷人架构之前，现在是总结本章思想的时候了。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The Fourth Industrial Revolution, or Industry 4.0, has forced artificial intelligence
    to make profound evolutions. The Third Industrial Revolution was digital. Industry
    4.0 is built on top of the digital revolution connecting everything to everything,
    everywhere. Automated processes are replacing human decisions in critical areas,
    including NLP.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第四次工业革命，或工业4.0，迫使人工智能发生深刻的演变。第三次工业革命是数字化的。工业4.0建立在数字革命的基础上，将一切与一切、无处不在的连接起来。自动化流程正在取代人类在重要领域的决策，包括NLP。
- en: RNNs had limitations that slowed the progression of automated NLP tasks required
    in a fast-moving world. Transformers filled the gap. A corporation needs summarization,
    translation, and a wide range of NLP tools to meet the challenges of Industry
    4.0.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: RNN存在限制，这些限制减缓了在快速移动世界中所需的自动化NLP任务的进展。转换器填补了这一空白。一个公司需要摘要、翻译和各种NLP工具来应对工业4.0的挑战。
- en: Industry 4.0 (I4.0) has thus spurred an age of artificial intelligence industrialization.
    The evolution of the concepts of platforms, frameworks, language, and models represents
    a challenge for an industry 4.0 developer. Foundation models bridge the gap between
    the Third Industrial Revolution and I4.0 by providing homogenous models that can
    carry out a wide range of tasks without further training or fine-tuning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，工业4.0（I4.0）推动了人工智能产业化的时代。平台、框架、语言和模型概念的演进对工业4.0开发者构成了挑战。基础模型通过提供可以在没有进一步训练或微调的情况下执行各种任务的同质模型，弥合了第三次工业革命和I4.0之间的差距。
- en: Websites such as AllenNLP, for example, provide educational NLP tasks with no
    installation, but it also provides resources to implement a transformer model
    in customized programs. OpenAI provides an API requiring only a few code lines
    to run one of the powerful GPT-3 engines. Google Trax provides an end-to-end library,
    and Hugging Face offers various transformer models and implementations. We will
    be exploring these ecosystems throughout this book.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，AllenNLP等网站提供了不需要安装的教育性NLP任务，但它也提供了在定制程序中实现转换器模型的资源。OpenAI提供了一个API，只需几行代码就可以运行强大的GPT-3引擎之一。Google
    Trax提供了一个端到端库，Hugging Face提供了各种转换器模型和实现。我们将在本书中探索这些生态系统。
- en: Industry 4.0 is a radical deviation from former AI with a broader skillset.
    For example, a project manager can decide to implement transformers by asking
    a web designer to create an interface for OpenAI’s API through prompt engineering.
    Or, when required, a project manager can ask an artificial intelligence specialist
    to download Google Trax or Hugging Face to develop a full-blown project with a
    customized transformer model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 工业4.0是与以前的人工智能的根本偏差，具有更广泛的技能集。例如，项目经理可以决定通过询问网页设计师创建一个接口来实现转换器的项目，通过提示工程来实现OpenAI的API。或者，在必要时，项目经理可以要求人工智能专家下载Google
    Trax或Hugging Face来开发一个完整的项目，并使用定制的转换器模型。
- en: Industry 4.0 is a game-changer for developers whose role will expand and require
    more designing than programming. In addition, embedded transformers will provide
    assisted code development and usage. These new skillsets are a challenge but open
    new exciting horizons.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 工业4.0对开发者来说是一个改变游戏规则的因素，他们的角色将会扩展，并且需要更多的设计而不是编程。此外，嵌入式转换器将提供辅助代码开发和使用。这些新的技能集是一个挑战，但也打开了新的激动人心的视野。
- en: In *Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    we will get started with the architecture of the original Transformer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*中，*开始使用Transformer模型的架构*，我们将开始使用原始Transformer的架构。
- en: Questions
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: We are still in the Third Industrial Revolution. (True/False)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们仍处于第三次工业革命中。（True/False）
- en: The Fourth Industrial Revolution is connecting everything to everything. (True/False)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四次工业革命正在将一切连接到一切。（True/False）
- en: Industry 4.0 developers will sometimes have no AI development to do. (True/False)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0开发者有时不需要进行AI开发。（True/False）
- en: Industry 4.0 developers might have to implement transformers from scratch. (True/False)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0开发者可能需要从零开始实现转换器。（True/False）
- en: It’s not necessary to learn more than one transformer ecosystem, such as Hugging
    Face, for example. (True/False)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有必要学习超过一个转换器生态系统，例如Hugging Face。（True/False）
- en: A ready-to-use transformer API can satisfy all needs. (True/False)
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个即用型的转换器API可以满足所有需求。（True/False）
- en: A company will accept the transformer ecosystem a developer knows best. (True/False)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 公司将接受开发者最擅长的转换器生态系统。（True/False）
- en: Cloud transformers have become mainstream. (True/False)
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 云转换器已经成为主流。（True/False）
- en: A transformer project can be run on a laptop. (True/False)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个转换器项目可以在笔记本电脑上运行。（True/False）
- en: Industry 4.0 artificial intelligence specialists will have to be more flexible
    (True/False)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0人工智能专家将不得不更加灵活（True/False）
- en: References
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*Bommansani* et al. 2021, *On the Opportunities and Risks of Foundation Models*,
    [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bommansani*等人，2021年，《基础模型的机遇和风险》，[https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)'
- en: '*Chen* et al.,2021, *Evaluating Large Language Models Trained on Code*, [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chen*等人，2021年，《评估基于代码的大型语言模型》，[https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)'
- en: 'Microsoft AI: [https://innovation.microsoft.com/en-us/ai-at-scale](https://innovation.microsoft.com/en-us/ai-at-scale)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软人工智能：[https://innovation.microsoft.com/en-us/ai-at-scale](https://innovation.microsoft.com/en-us/ai-at-scale)
- en: 'OpenAI: [https://openai.com/](https://openai.com/)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI：[https://openai.com/](https://openai.com/)
- en: 'Google AI: [https://ai.google/](https://ai.google/)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Google AI: [https://ai.google/](https://ai.google/)'
- en: 'Google Trax: [https://github.com/google/trax](https://github.com/google/trax)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Google Trax: [https://github.com/google/trax](https://github.com/google/trax)'
- en: 'AllenNLP: [https://allennlp.org/](https://allennlp.org/)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AllenNLP: [https://allennlp.org/](https://allennlp.org/)'
- en: 'Hugging Face: [https://huggingface.co/](https://huggingface.co/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugging Face: [https://huggingface.co/](https://huggingface.co/)'
- en: Join our book’s Discord space
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参加书籍的Discord工作区，与作者进行每月的*问我任何*（Ask me Anything）的交流活动：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
