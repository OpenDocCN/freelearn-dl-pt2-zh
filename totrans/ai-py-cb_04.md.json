["```py\npip install yfinance\n```", "```py\nimport yfinance as yf\n\nmsft = yf.Ticker('MSFT')\nhist = msft.history(period='max')\n```", "```py\nfrom typing import Tuple\nimport numpy as np\nimport pandas as pd\nimport scipy\n\ndef generate_data(\n    data: pd.DataFrame, window_size: int, shift: int\n) -> Tuple[np.array, np.array]:\n    y = data.shift(shift + window_size)\n    observation_window = []\n    for i in range(window_size):\n        observation_window.append(\n            data.shift(i)\n        )\n    X = pd.concat(observation_window, axis=1)\n    y = (y - X.values[:, -1]) / X.values[:, -1]\n    X = X.pct_change(axis=1).values[:, 1:]\n    inds = (~np. isnan(X).any(axis=1)) & (~np. isnan(y))\n    X, y = X[inds], y[inds]\n    return X, y\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX, y = generate_data(hist.Close, shift=1, window_size=30)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n```", "```py\ndef threshold_vector(x, threshold=0.02):\n    def threshold_scalar(f):\n        if f > threshold:\n            return 1\n        elif f < -threshold:\n            return -1\n        return 0\n    return np.vectorize(threshold_scalar)(x)\n\ny_train_classes, y_test_classes = threshold_vector(y_train), threshold_vector(y_test)\n```", "```py\nfrom sklearn import metrics\n\ndef to_one_hot(a):\n    \"\"\"convert from integer encoding to one-hot\"\"\"\n    b = np.zeros((\n       a.size, 3\n    ))\n    b[np.arange(a.size), a+1] = 1\n    return b\n\ndef measure_perf(model, y_test_classes):\n  y_pred = model.predict(X_test)\n  auc = metrics.roc_auc_score(\n    to_one_hot(y_test_classes),\n    to_one_hot(y_pred),\n    multi_class='ovo'\n  )\n  print('AUC: {:.3f}'.format(auc))\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrf = RandomForestClassifier(\n    n_estimators=500, n_jobs=-1\n).fit(X_train, y_train_classes)\nplatt = CalibratedClassifierCV(\n    rf, method='sigmoid'\n).fit(X_train, y_train_classes)\nisotonic = CalibratedClassifierCV(\n    rf, method='isotonic'\n).fit(X_train, y_train_classes)\nprint('Platt:')\nmeasure_perf(platt, y_test_classes)\nprint('Isotonic:')\nmeasure_perf(isotonic, y_test_classes)\n#Platt:\n#AUC: 0.504\n#Isotonic:\n#AUC: 0.505\n```", "```py\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import ComplementNB, CategoricalNB\n\ndef create_classifier(final_estimator):\n    estimators = [\n        ('rf', RandomForestClassifier(\n            n_estimators=100,\n            n_jobs=-1\n        ))\n    ] \n    return StackingClassifier(\n        estimators=estimators,\n        final_estimator=final_estimator,\n        stack_method='predict_proba'\n    ).fit(X_train, y_train_classes)\n\nmeasure_perf(create_classifier(CategoricalNB()), y_test_classes)\nmeasure_perf(create_classifier(ComplementNB()), y_test_classes)\n#CategoricalNB:\n#AUC: 0.500\n#ComplementNB: \n#AUC: 0.591\n```", "```py\npip install lifetimes\n```", "```py\nfrom lifetimes.datasets import load_cdnow_summary_data_with_monetary_value\nfrom lifetimes import BetaGeoFitter\n\nbgf = BetaGeoFitter(penalizer_coef=0.0)\nbgf.fit(\n   data['frequency'],\n    data['recency'],\n    data['T']\n)\n```", "```py\nfrom lifetimes import GammaGammaFitter\n\ndata_repeat = data[data.frequency>0]\nggf = GammaGammaFitter(penalizer_coef=0.0)\nggf.fit(\n    data_repeat.frequency,\n    data_repeat.monetary_value\n)\n```", "```py\nprint(ggf.customer_lifetime_value(\n    bgf,\n    data['frequency'],\n    data['recency'],\n    data['T'],\n    data['monetary_value'],\n    time=12,\n    discount_rate=0.01\n).head(5))\n```", "```py\ncustomer_id\n1      140.096218\n2       18.943466\n3       38.180575\n4       38.180575\n5       38.180575\n```", "```py\npip install tensorflow-probability\n```", "```py\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX, y = fetch_openml(data_id=1565, return_X_y=True, as_frame=True)\ntarget = (y.astype(int) > 1).astype(float)\nscaler = StandardScaler()\nX_t = scaler.fit_transform(X)\nXt_train, Xt_test, y_train, y_test = train_test_split(\n    X_t, target, test_size=0.33, random_state=42\n)\n```", "```py\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\nfrom tensorflow import keras\n\nnegloglik = lambda y, p_y: -p_y.log_prob(y)\n\nmodel = keras.Sequential([\n  keras.layers.Dense(12, activation='relu', name='hidden'),\n  keras.layers.Dense(1, name='output'),\n  tfp.layers.DistributionLambda(\n      lambda t: tfd.Bernoulli(logits=t)\n  ),\n])\n\nmodel.compile(optimizer=tf.optimizers.Adagrad(learning_rate=0.05), loss=negloglik)\n```", "```py\n%load_ext tensorboard\ncallbacks = [\n    keras.callbacks.EarlyStopping(patience=10, monitor='loss'),\n    keras.callbacks.TensorBoard(log_dir='./logs'),\n]\nhistory = model.fit(\n    Xt_train,\n    y_train.values,\n    epochs=2000,\n    verbose=False,\n    callbacks=callbacks\n)\n```", "```py\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ny_pred = model(Xt_test)\na = y_pred.mean().numpy()[10]\nb = y_pred.variance().numpy()[10]\nfig, ax = plt.subplots(1, 1)\nx = np.linspace(\n    norm.ppf(0.001, a, b),\n    norm.ppf(0.999, a, b),\n    100\n)\npdf = norm.pdf(x, a, b)\nax.plot(\n    x, \n    pdf / np.sum(pdf), \n    'r-', lw=5, alpha=0.6, \n    label='norm pdf'\n)\nplt.ylabel('probability density')\nplt.xlabel('predictions')\n```", "```py\ndef to_classprobs(y_pred):\n    N = y_pred.mean().numpy().shape[0]\n    class_probs = np.zeros(\n        shape=(N, 2)\n    )\n    for i, (a, b) in enumerate(\n        zip(\n            y_pred.mean().numpy(),\n            y_pred.variance().numpy()\n        )\n    ):\n        conf = norm.cdf(0.5, a, b)\n        class_probs[i, 0] = conf\n        class_probs[i, 1] = 1 - conf\n    return class_probs\n\nclass_probs = to_classprobs(y_pred)\n```", "```py\nimport sklearn\n\ndef to_one_hot(a):\n    \"\"\"convert from integer encoding to one-hot\"\"\"\n    b = np.zeros((a.size, 2))\n    b[np.arange(a.size), np.rint(a).astype(int)] = 1\n    return b\n\nsklearn.metrics.roc_auc_score(\n    to_one_hot(y_test),\n    class_probs\n)\nprint('{:.3f}'.format(sklearn.metrics.roc_auc_score(to_one_hot(y_test), class_probs)))\n0.859\n```", "```py\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import average_precision_score\n\nclass ModelWrapper(sklearn.base.ClassifierMixin):\n    _estimator_type = 'classifier'\n    classes_ = [0, 1]\n    def predict_proba(self, X):\n        pred = model(X)\n        return to_classprobs(pred)\n\nmodel_wrapper = ModelWrapper()\naverage_precision = average_precision_score(\n    to_one_hot(y_test),\n    class_probs\n)\nfig = plot_precision_recall_curve(\n    model_wrapper, Xt_test, y_test\n)\nfig.ax_.set_title(\n    '2-class Precision-Recall curve: '\n    'AP={0:0.2f}'.format(average_precision)\n)\n```", "```py\nnegloglik = lambda y, p_y: -p_y.log_prob(y)\n```", "```py\npip install tensorflow-probability\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\nopenml_frame = fetch_openml(data_id=42477, as_frame=True)\ndata = openml_frame['data']\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\nX = scaler.fit_transform(\n    data\n)\ntarget_dict = {val: num for num, val in enumerate(list(openml_frame['target'].unique()))}\ny = openml_frame['target'].apply(lambda x: target_dict[x]).astype('float').values\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n```", "```py\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\ntfd = tfp.distributions\n%matplotlib inline\n\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\ndef prior_trainable(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    return tf.keras.Sequential([\n    tfp.layers.VariableLayer(n, dtype=dtype),\n    tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n        tfd.Normal(loc=t, scale=1),\n        reinterpreted_batch_ndims=1)),\n    ])\ndef posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    c = np.log(np.expm1(1.))\n    return tf.keras.Sequential([\n    tfp.layers.VariableLayer(2 * n, dtype=dtype),\n    tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n        tfd.Normal(\n            loc=t[..., :n],\n            scale=1e-5 + tf.nn.softplus(c + t[..., n:])\n        ),\n    reinterpreted_batch_ndims=1)),\n    ])\n```", "```py\nmodel = tf.keras.Sequential([\n    tfp.layers.DenseVariational(2, posterior_mean_field, prior_trainable, kl_weight=1/X.shape[0]),\n    tfp.layers.DistributionLambda(\n        lambda t: tfd.Normal(\n            loc=t[..., :1],\n            scale=1e-3 + tf.math.softplus(0.01 * t[...,1:])\n        )\n    ),\n])\nmodel.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    loss=negloglik\n)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nmodel.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    epochs=1000,\n    verbose=False,\n    callbacks=[callback]\n)\n```", "```py\nfrom sklearn.metrics import roc_auc_score\npreds = model(X_test)\nroc_auc_score(y_test, preds.mean().numpy())\n```", "```py\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, preds.mean().numpy() >= 0.5)\ncm = pd.DataFrame(\n data=cm / cm.sum(axis=0),\n columns=['False', 'True'],\n index=['False', 'True']\n)\nsns.heatmap(\n cm,\n fmt='.2f',\n cmap='Blues',\n annot=True,\n annot_kws={'fontsize': 18}\n)\n```", "```py\nimport scipy\nscipy.stats.spearmanr(np.abs(y_test - preds.mean().numpy().squeeze()), preds.variance().numpy().squeeze())\n```"]