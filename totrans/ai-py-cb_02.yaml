- en: Advanced Topics in Supervised Machine Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习中的高级主题
- en: Following the tasters with scikit-learn, Keras, and PyTorch in the previous
    chapter, in this chapter, we will move on to more end-to-end examples. These examples
    are more advanced in the sense that they include more complex transformations
    and model types.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节中跟随 scikit-learn、Keras 和 PyTorch 的尝试后，我们将进入更多端到端示例。这些示例更加先进，因为它们包括更复杂的转换和模型类型。
- en: We'll be predicting partner choices with sklearn, where we'll implement a lot
    of custom transformer steps and more complicated machine learning pipelines. We'll
    then predict house prices in PyTorch and visualize feature and neuron importance.
    After that, we will perform active learning to decide customer values together
    with online learning in sklearn. In the well-known case of repeat offender prediction,
    we'll build a model without racial bias. Last, but not least, we'll forecast time
    series of CO[2] levels.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 sklearn 预测伴侣选择，在这里我们会实现许多自定义转换步骤和更复杂的机器学习流程。然后，我们将在 PyTorch 中预测房价并可视化特征和神经元的重要性。之后，我们将执行主动学习来共同决定客户价值，并进行
    sklearn 中的在线学习。在已知的多次违法者预测案例中，我们将建立一个没有种族偏见的模型。最后但同样重要的是，我们将预测 CO[2] 水平的时间序列。
- en: '**Online learning** in this context (as opposed to internet-based learning)
    refers to a model update strategy that incorporates training data that comes in
    sequentially. This can be useful in cases where the dataset is very big (often
    the case with images, videos, and texts) or where it''s important to keep the
    model up to date given the changing nature of the data.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线学习** 在这个上下文中（与基于互联网的学习相对），指的是一种包含顺序接收的训练数据的模型更新策略。这在数据集非常庞大（通常出现在图像、视频和文本中）或者由于数据变化的性质需要保持模型更新时非常有用。'
- en: In many of these recipes, we've shortened the description to the most salient
    details in order to highlight particular concepts. For the full details, please
    refer to the notebooks on GitHub.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多这些示例中，我们已经缩短了描述以突出特定概念的最显著细节。有关完整详情，请参阅 GitHub 上的笔记本。
- en: 'In this chapter, we''ll be covering the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下几个示例：
- en: Transforming data in scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中进行数据转换
- en: Predicting house prices in PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 预测房价
- en: Live decisioning customer values
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时决策客户价值
- en: Battling algorithmic bias
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗算法偏见
- en: Forecasting CO[2] time series
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测 CO[2] 时间序列
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PackPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter02](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter02).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和笔记本在 GitHub 上可供查阅：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter02](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter02)
- en: Transforming data in scikit-learn
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中进行数据转换
- en: In this recipe, we will be building more complex pipelines using mixed-type
    columnar data. We'll use a speed dating dataset that was published in 2006 by
    Fisman *et al.*: [https://doi.org/10.1162/qjec.2006.121.2.673](https://doi.org/10.1162/qjec.2006.121.2.673)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用混合类型的列式数据构建更复杂的管道。我们将使用由 Fisman 等人于 2006 年发布的速度约会数据集：[https://doi.org/10.1162/qjec.2006.121.2.673](https://doi.org/10.1162/qjec.2006.121.2.673)
- en: Perhaps this recipe will be informative in more ways than one, and we'll learn
    something useful about the mechanics of human mating choices.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这些示例将在多方面为我们提供信息，并且我们将了解到关于人类交配选择机制的一些有用机制。
- en: 'The dataset description on the OpenML website reads as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenML 网站上的数据集描述如下：
- en: 'This data was gathered from participants in experimental speed dating events
    from 2002-2004\. During the events, the attendees would have a four-minute **first
    date** with every other participant of the opposite sex. At the end of their 4
    minutes, participants were asked whether they would like to see their date again.
    They were also asked to rate their date on six attributes: attractiveness, sincerity,
    intelligence, fun, ambition, and shared interests. The dataset also includes questionnaire
    data gathered from participants at different points in the process. These fields
    include demographics, dating habits, self-perception across key attributes, beliefs
    in terms of what others find valuable in a mate, and lifestyle information.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据是从2002年至2004年实验性速配活动的参与者中收集的。在活动中，与异性的每位参与者进行四分钟的**第一次约会**。在他们的4分钟结束时，参与者被问及是否愿意再见对方。他们还被要求评价约会对象的六个属性：吸引力、诚实、智力、趣味、野心和共同兴趣。数据集还包括在过程不同阶段收集的参与者问卷数据。这些字段包括人口统计信息、约会习惯、跨关键属性的自我感知、对伴侣所认为有价值的东西的信念以及生活方式信息。
- en: 'The problem is to predict mate choices from what we know about participants
    and their matches. This dataset presents some challenges that can serve an illustrative
    purpose:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是根据我们对参与者及其配对的了解来预测伴侣选择。这个数据集呈现了一些可以作为说明用途的挑战：
- en: 'It contains 123 different features, of different types:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含123个不同类型的特征：
- en: Categorical
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Numerical
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值
- en: Range features
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围特征
- en: 'It also contains the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括以下内容：
- en: Some missing values
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些缺失值
- en: Target imbalance
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标不平衡
- en: On the way to solving this problem of predicting mate choices, we will build
    custom encoders in scikit-learn and a pipeline comprising all features and their
    preprocessing steps.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决预测伴侣选择问题的过程中，我们将在scikit-learn中构建自定义编码器和包含所有特征及其预处理步骤的管道。
- en: The primary focus in this recipe will be on pipelines and transformers. In particular,
    we will build a custom transformer for working with range features and another
    one for numerical features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的主要焦点将放在管道和转换器上。特别是，我们将为处理范围特征构建一个自定义转换器，以及为数值特征构建另一个转换器。
- en: Getting ready
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We''ll need the following libraries for this recipe. They are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将需要以下库：
- en: OpenML to download the dataset
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenML 用于下载数据集
- en: '`openml_speed_dating_pipeline_steps` to use our custom transformer'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openml_speed_dating_pipeline_steps` 用于使用我们的自定义转换器。'
- en: '`imbalanced-learn` to work with imbalanced classes'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` 用于处理不平衡的类别'
- en: '`shap` to show us the importance of features'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shap` 用于显示特征的重要性'
- en: 'In order to install them, we can use `pip` again:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装它们，我们可以再次使用`pip`：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: OpenML is an organization that intends to make data science and machine learning
    reproducible and  therefore  more conducive to research. The OpenML website not
    only hosts datasets, but also allows the uploading of machine learning results
    to public leaderboards under the condition that the implementation relies solely
    on open source. These results and how they were obtained can be inspected in complete
    detail by anyone who's interested.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: OpenML 是一个旨在使数据科学和机器学习可复制的组织，因此更有利于研究。OpenML 网站不仅托管数据集，还允许将机器学习结果上传到公共排行榜，条件是实现必须完全依赖开源。有兴趣的任何人都可以查看这些结果及其详细获取方式。
- en: 'In order to retrieve the data, we will use the OpenML Python API. The `get_dataset()`
    method will download the dataset; with `get_data()`, we can get pandas DataFrames
    for features and target, and we''ll conveniently get the information on categorical
    and numerical feature types:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检索数据，我们将使用OpenML Python API。`get_dataset()` 方法将下载数据集；使用 `get_data()`，我们可以获取特征和目标的
    pandas DataFrames，并且方便地获取分类和数值特征类型的信息：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the original version of the dataset, as presented in the paper, there was
    a lot more work to do. However, the version of the dataset on OpenML already has
    missing values represented as `numpy.nan`, which lets us skip this conversion.
    You can see this preprocessor on GitHub if you are interested: [https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始数据集的版本中，正如论文中所述，有很多工作要做。但是，在OpenML版本的数据集中，缺失值已表示为`numpy.nan`，这让我们可以跳过此转换。如果您感兴趣，可以在GitHub上查看此预处理器：[https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating)
- en: Alternatively, you can use a download link from the OpenML dataset web page
    at [https://www.openml.org/data/get_csv/13153954/speeddating.arff](https://www.openml.org/data/get_csv/13153954/speeddating.arff).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用来自OpenML数据集网页的下载链接，网址为[https://www.openml.org/data/get_csv/13153954/speeddating.arff](https://www.openml.org/data/get_csv/13153954/speeddating.arff)。
- en: With the dataset loaded, and the libraries installed, we are ready to start
    cracking.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集加载完毕，并安装了库，我们已经准备好开始了。
- en: How to do it...
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: Pipelines are a way of describing how machine learning algorithms, including
    preprocessing steps, can follow one another in a sequence of transformations on
    top of the raw dataset before applying a final predictor. We will see examples
    of these concepts in this recipe and throughout this book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 管道是描述机器学习算法如何按顺序进行转换的一种方式，包括预处理步骤，然后应用最终预测器之前的原始数据集。我们将在本配方和本书中的其他概念示例中看到这些概念。
- en: A few things stand out pretty quickly looking at this dataset. We have a lot
    of categorical features. So, for modeling, we will need to encode them numerically, as
    in the *Modeling and predicting in Keras* recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml),
    *Getting Started with Artificial Intelligence in Python*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此数据集很快就会发现几个显著的特点。我们有很多分类特征。因此，在建模时，我们需要对它们进行数字编码，如第1章中的*Keras中的建模和预测*配方中所述，*Python人工智能入门*。
- en: Encoding ranges numerically
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码范围以数字形式显示
- en: 'Some of these are actually encoded ranges. This means these are ordinal, in
    other words, they are categories that are ordered; for example, the `d_interests_correlate`
    feature contains strings like these:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些实际上是编码范围。这意味着这些是有序的序数类别；例如，`d_interests_correlate`特征包含如下字符串：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we were to treat these ranges as categorical variables, we'd lose the information
    about the order, and we would lose information about how different two values
    are. However, if we convert them to numbers, we will keep this information and
    we would be able to apply other numerical transformations on top.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把这些范围视为分类变量，我们将失去有关顺序的信息，以及有关两个值之间差异的信息。但是，如果我们将它们转换为数字，我们将保留这些信息，并能够在其上应用其他数值转换。
- en: 'We are going to implement a transformer to plug into an sklearn pipeline in
    order to convert these range features to numerical features. The basic idea of
    the conversion is to extract the upper and lower bounds of these ranges as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个转换器插入到sklearn管道中，以便将这些范围特征转换为数值特征。转换的基本思想是提取这些范围的上限和下限，如下所示：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll see this for our example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以示例为例：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In order to get numerical features, we can then take the mean between the two
    bounds. As we''ve mentioned before, on OpenML, not only are results shown, but also the models are transparent. Therefore, if
    we want to submit our model, we can only use published modules. We created a module and published
    it in the `pypi` Python package repository, where you can find the package with
    the complete code: [https://pypi.org/project/openml-speed-dating-pipeline-steps/](https://pypi.org/project/openml-speed-dating-pipeline-steps/).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得数字特征，我们可以取两个边界的平均值。正如我们之前提到的，在OpenML上，不仅显示结果，还透明显示模型。因此，如果我们要提交我们的模型，我们只能使用已发布的模块。我们在`pypi`
    Python包存储库中创建了一个模块，并发布了它，您可以在此处找到完整代码包：[https://pypi.org/project/openml-speed-dating-pipeline-steps/](https://pypi.org/project/openml-speed-dating-pipeline-steps/)。
- en: 'Here is the simplified code for `RangeTransformer`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`RangeTransformer`的简化代码：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a shortened snippet of the custom transformer for ranges. Please see
    the full implementation on GitHub at [https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是范围的自定义转换器的简短片段。请在GitHub上查看完整实现，网址为[https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating)。
- en: Please pay attention to how the `fit()` and `transform()` methods are used. We
    don't need to do anything in the `fit()` method, because we always apply the same
    static rule. The `transfer()` method applies this rule. We've seen the examples
    previously. What we do in the `transfer()` method is to iterate over columns. This
    transformer also shows the use of the parallelization pattern typical to scikit-learn.
    Additionally, since these ranges repeat a lot, and there aren't so many, we'll
    use a cache so that, instead of doing costly string transformations, the range
    value can be retrieved from memory once the range has been processed once.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意如何使用 `fit()` 和 `transform()` 方法。在 `fit()` 方法中，我们不需要做任何事情，因为我们总是应用相同的静态规则。`transform()`
    方法应用这个规则。我们之前已经看过例子。在 `transform()` 方法中，我们会迭代列。这个转换器还展示了典型的 scikit-learn 并行化模式的使用。另外，由于这些范围经常重复，并且数量并不多，我们将使用缓存，以便不必进行昂贵的字符串转换，而是可以在处理完范围后从内存中检索范围值。
- en: An important thing about custom transformers in scikit-learn is that they should
    inherit from `BaseEstimator` and `TransformerMixin`, and implement the `fit()` and
    `transform()`methods. Later on, we will require `get_feature_names()` so we can
    find out the names of the features generated.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中自定义转换器的一个重要事项是，它们应该继承自 `BaseEstimator` 和 `TransformerMixin`，并实现
    `fit()` 和 `transform()` 方法。稍后，我们将需要 `get_feature_names()` 方法来获取生成特征的名称。
- en: Deriving higher-order features
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 派生高阶特征
- en: 'Let''s implement another transformer. As you may have noticed, we have different
    types of features that seem to refer to the same personal attributes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现另一个转换器。您可能已经注意到，我们有不同类型的特征，看起来涉及相同的个人属性：
- en: Personal preferences
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人偏好
- en: Self-assessment
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我评估
- en: Assessment of the other person
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对另一个人的评估
- en: It seems clear that differences between any of these features could be significant,
    such as the importance of sincerity versus how sincere someone assesses a potential
    partner. Therefore, our next transformer is going to calculate the differences
    between numerical features. This is supposed to help highlight these differences.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎很明显，任何这些特征之间的差异可能是显著的，比如真诚的重要性与某人评估潜在伴侣的真诚程度之间的差异。因此，我们的下一个转换器将计算数值特征之间的差异，这有助于突出这些差异。
- en: 'These features are derived from other features, and combine information from
    two (or potentially more features). Let''s see what the `NumericDifferenceTransformer` feature
    looks like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征是从其他特征派生的，并结合了来自两个（或更多特征）的信息。让我们看看 `NumericDifferenceTransformer` 特征是什么样的：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is a custom transformer that calculates differences between numerical features.
    Please refer to the full implementation in the repository of the OpenML-Speed-Dating
    library at [https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个自定义转换器，用于计算数值特征之间的差异。请参考 OpenML-Speed-Dating 库的完整实现，位于 [https://github.com/benman1/OpenML-Speed-Dating](https://github.com/benman1/OpenML-Speed-Dating)。
- en: 'This transformer has a very similar structure to `RangeTransformer`. Please
    note the parallelization between columns. One of the arguments to the `__init__()`
    method is the function that is used to calculate the difference. This is `operator.sub()`
    by default. The operator library is part of the Python standard library and implements
    basic operators as functions. The `sub()` function does what it sounds like:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器与 `RangeTransformer` 结构非常相似。请注意列之间的并行化。`__init__()` 方法的一个参数是用于计算差异的函数，默认情况下是
    `operator.sub()`。operator 库是 Python 标准库的一部分，它将基本运算符实现为函数。`sub()` 函数做的就是它听起来像：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This gives us a prefix or functional syntax for standard operators. Since we
    can pass functions as arguments, this gives us the flexibility to specify different
    operators between columns.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了标准操作符的前缀或功能语法。由于我们可以将函数作为参数传递，这使我们能够指定列之间的不同操作符。
- en: The `fit()` method this time just collects the names of numerical columns, and
    we'll use these names in the `transform()` method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次的 `fit()` 方法只是收集数值列的名称，我们将在 `transform()` 方法中使用这些名称。
- en: Combining transformations
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合转换
- en: 'We will put these transformers together with `ColumnTransformer` and the pipeline.
    However, we''ll need to make the association between columns and their transformations.
    We''ll define different groups of columns:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `ColumnTransformer` 和管道将这些转换器组合在一起。但是，我们需要将列与它们的转换关联起来。我们将定义不同的列组：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we have columns that are ranges, columns that are categorical, and columns
    that are numerical, and we can assign pipeline steps to them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有范围列、分类列和数值列，我们可以为它们分配管道步骤。
- en: 'In our case, we put this together as follows, first in a preprocessor:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将其组合如下，首先是预处理器：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And then we''ll put the preprocessing in a pipeline, together with the estimator:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将预处理放入管道中，与估算器一起：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the performance in the test set:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是测试集的表现：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We get the following performance as an output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下性能作为输出：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is a very good performance, as you can see comparing it to the leaderboard
    on OpenML.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常好的性能，您可以将其与 OpenML 排行榜进行比较看到。
- en: How it works...
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'It is time to explain basic scikit-learn terminology relevant to this recipe.
    Neither of these concepts corresponds to existing machine learning algorithms,
    but to composable modules:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候解释与此配方相关的基本 scikit-learn 术语了。这两个概念都不对应现有的机器学习算法，而是可组合的模块：
- en: 'Transformer (in scikit-learn): A class that is derived from `sklearn.base.TransformerMixin`;
    it has `fit()` and `transform()` methods. These involve preprocessing steps or
    feature selection.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器（在 scikit-learn 中）：从 `sklearn.base.TransformerMixin` 派生的类；它具有 `fit()` 和 `transform()`
    方法。这些涉及预处理步骤或特征选择。
- en: 'Predictor: A class that is derived from either `sklearn.base.ClassifierMixin`
    or `sklearn.base.RegressorMixin`; it has `fit()` and `predict()` methods. These
    are machine learning estimators, in other words, classifiers or regressors.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测器：从 `sklearn.base.ClassifierMixin` 或 `sklearn.base.RegressorMixin` 派生的类；它具有
    `fit()` 和 `predict()` 方法。这些是机器学习估计器，换句话说，分类器或回归器。
- en: 'Pipeline: An interface that wraps all steps together and gives you a single
    interface for all steps of the transformation and the resulting estimator. A pipeline
    again has `fit()` and `predict()` methods.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道：一个接口，将所有步骤包装在一起，并为转换的所有步骤和结果估计器提供单一接口。管道再次具有 `fit()` 和 `predict()` 方法。
- en: There are a few things to point out regarding our approach. As we said before,
    we have missing values, so we have to impute (meaning replace) missing values
    with other values. In this case, we replace missing values with -1\. In the case
    of categorical variables, this will be a new category, while in the case of numerical
    variables, it will become a special value that the classifier will have to handle.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要指出关于我们的方法。正如我们之前所说，我们有缺失值，因此必须用其他值填充（意思是替换）缺失值。在这种情况下，我们用 -1 替换缺失值。对于分类变量来说，这将成为一个新类别，而对于数值变量来说，这将成为分类器必须处理的特殊值。
- en: '`ColumnTransformer` came with version 0.20 of scikit-learn and was a long-awaited
    feature. Since then, `ColumnTransformer` can often be seen like this, for example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnTransformer` 是 scikit-learn 版本 0.20 中引入的一个期待已久的功能。从那时起，`ColumnTransformer`
    经常可以像这样看到，例如：'
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`feature_preprocessing` can then be used as usual with the `fit()`, `transform()`,
    and `fit_transform()` methods:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`feature_preprocessing` 可以像往常一样使用 `fit()`、`transform()` 和 `fit_transform()`
    方法：'
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, `X` means our features.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`X` 意味着我们的特征。
- en: 'Alternatively, we can put `ColumnTransformer` as a step into a pipeline, for
    example, like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以像这样将 `ColumnTransformer` 作为管道的一步：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our classifier is a modified form of the random forest classifier. A random
    forest is a collection of decision trees, each trained on random subsets of the
    training data. The balanced random forest classifier (Chen *et al.*: [https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf](https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf))
    makes sure that each random subset is balanced between the two classes.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器是修改版的随机森林分类器。随机森林是一组决策树，每棵树都在训练数据的随机子集上进行训练。平衡的随机森林分类器（Chen *等人*：[https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf](https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)）确保每个随机子集在两个类别之间平衡。
- en: Since `NumericDifferenceTransformer` can provide lots of features, we will incorporate
    an extra step of model-based feature selection.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `NumericDifferenceTransformer` 可以提供大量特征，我们将增加一步基于模型的特征选择。
- en: There's more...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: You can see the complete example with the speed dating dataset, a few more custom
    transformers, and an extended imputation class in the GitHub repository of the
    `openml_speed_dating_pipeline_steps` library and notebook, on GitHub at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter02/Transforming%20Data%20in%20Scikit-Learn.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter02/Transforming%20Data%20in%20Scikit-Learn.ipynb).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的`openml_speed_dating_pipeline_steps`库和笔记本中查看使用速配数据集、几个自定义转换器和扩展填充类的完整示例，链接如下：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter02/Transforming%20Data%20in%20Scikit-Learn.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter02/Transforming%20Data%20in%20Scikit-Learn.ipynb)。
- en: Both `RangeTransformer` and `NumericDifferenceTransformer` could also have been
    implemented using `FunctionTransformer` in scikit-learn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`RangeTransformer` 和 `NumericDifferenceTransformer` 也可以使用 scikit-learn 中的 `FunctionTransformer`
    实现。'
- en: '`ColumnTransformer` is especially handy for pandas DataFrames or NumPy arrays
    since it allows the specification of different operations for different subsets
    of the features. However, another option is `FeatureUnion`, which allows concatenation
    of the results from different transformations. For yet another way to chain our
    operations together, please have a look at `PandasPicker` in our repository.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnTransformer` 对于 pandas DataFrames 或 NumPy 数组特别方便，因为它允许为不同特征子集指定不同操作。然而，另一个选项是
    `FeatureUnion`，它允许将来自不同转换的结果连接在一起。要了解另一种方法如何将我们的操作链在一起，请查看我们存储库中的 `PandasPicker`。'
- en: See also
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: In this recipe, we used ANOVA f-values for univariate feature selection, which
    is relatively simple, yet effective. Univariate feature selection methods are
    usually simple filters or statistical tests that measure the relevance of a feature
    with regard to the target. There are, however, many different methods for feature
    selection, and scikit-learn implements a lot of them: [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在此食谱中，我们使用ANOVA F值进行单变量特征选择，这相对简单但有效。单变量特征选择方法通常是简单的过滤器或统计测试，用于衡量特征与目标的相关性。然而，有许多不同的特征选择方法，scikit-learn
    实现了很多：[https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html)。
- en: Predicting house prices in PyTorch
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中预测房价
- en: In this recipe, the aim of the problem is to predict house prices in Ames, Iowa,
    given 81 features describing the house, area, land, infrastructure, utilities,
    and much more. The Ames dataset has a nice combination of categorical and continuous
    features, a good size, and, perhaps most importantly, it doesn't suffer from problems
    of potential redlining or data entry like other, similar datasets, such as Boston
    Housing. We'll concentrate on the main aspects of PyTorch modeling here. We'll
    do online learning, analogous to Keras, in the *Modeling and predicting in Keras *recipe
    in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting Started with
    Artificial Intelligence in Python*. If you want to see more details on some of
    the steps, please look at our notebook on GitHub.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，问题的目标是预测艾奥瓦州埃姆斯的房价，给定描述房屋、区域、土地、基础设施、公用设施等81个特征。埃姆斯数据集具有良好的分类和连续特征组合，适量适度，并且最重要的是，不像其他类似数据集（例如波士顿房价数据集）那样受潜在的红线问题或数据输入问题的困扰。我们将在此处集中讨论PyTorch建模的主要方面。我们将进行在线学习，类似于Keras，在[第1章](87098651-b37f-4b05-b0ee-878193f28b95.xhtml)中的*Keras中的建模和预测*食谱中。如果您想查看某些步骤的更多详细信息，请查看我们在GitHub上的笔记本。
- en: As a little extra, we will also demonstrate neuron importance for the models
    developed in PyTorch. You can try out different network architectures in PyTorch or
    model types. The focus in this recipe is on the methodology, not an exhaustive
    search for the best solution.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个额外的内容，我们还将演示在PyTorch中开发的模型的神经元重要性。您可以在PyTorch中尝试不同的网络架构或模型类型。这个食谱的重点是方法论，而不是对最佳解决方案的详尽搜索。
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In order to prepare for the recipe, we need to do a few things. We''ll download
    the data as in the previous recipe, *Transforming data in scikit-learn*, and perform
    some preprocessing by following these steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备这个食谱，我们需要做一些准备工作。我们将像之前的食谱一样下载数据，*在 scikit-learn 中转换数据*，并按以下步骤进行一些预处理：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see the full dataset description at OpenML: [https://www.openml.org/d/42165](https://www.openml.org/d/42165).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 OpenML 上查看完整的数据集描述：[https://www.openml.org/d/42165](https://www.openml.org/d/42165)。
- en: 'Let''s look at the features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些特征：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the DataFrame information:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 DataFrame 的信息：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: PyTorch and seaborn are installed by default in Colab. We will assume, even
    if you are working with your self-hosted install by now, that you'll have the
    libraries installed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中，默认安装了 PyTorch 和 seaborn。我们假设，即使您现在使用自己托管的安装工作，也会安装这些库。
- en: 'We''ll use one more library, however, `captum`, which allows the inspection
    of PyTorch models for feature and neuron importance:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们还会使用一个库，`captum`，它允许检查 PyTorch 模型的特征和神经元重要性：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There is one more thing. We'll assume you have a GPU available. If you don't
    have a GPU in your computer, we'd recommend you try this recipe on Colab. In Colab,
    you'll have to choose a runtime type with GPU.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。我们假设您的计算机有 GPU。如果您的计算机没有 GPU，我们建议您在 Colab 上尝试此方法。在 Colab 中，您需要选择一个带 GPU
    的运行时类型。
- en: After all these preparations, let's see how we can predict house prices.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些准备工作之后，让我们看看如何预测房屋价格。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: The Ames Housing dataset is a small- to mid-sized dataset (1,460 rows) with
    81 features, both categorical and numerical. There are no missing values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Ames 房屋数据集是一个小到中等规模的数据集（1,460 行），包含 81 个特征，既包括分类特征又包括数值特征。没有缺失值。
- en: In the Keras recipe previously, we've seen how to scale the variables. Scaling
    is important here because all variables have different scales. Categorical variables
    need to be converted to numerical types in order to feed them into our model.
    We have the choice of one-hot encoding, where we create dummy variables for each
    categorical factor, or ordinal encoding, where we number all factors and replace
    the strings with these numbers. We could feed the dummy variables in like any
    other float variable, while ordinal encoding would require the use of embeddings,
    linear neural network projections that re-order the categories in a multi-dimensional
    space.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 Keras 配方中，我们已经看到了如何缩放变量。在这里缩放很重要，因为所有变量具有不同的尺度。分类变量需要转换为数值类型，以便将它们输入到我们的模型中。我们可以选择独热编码，其中我们为每个分类因子创建虚拟变量，或者序数编码，其中我们对所有因子进行编号，并用这些编号替换字符串。我们可以像处理任何其他浮点变量一样输入虚拟变量，而序数编码则需要使用嵌入，线性神经网络投影，重新排列多维空间中的类别。
- en: 'We take the embedding route here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择嵌入路线：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We go through the data analysis, such as correlation and distribution plots,
    in a lot more detail in the notebook on GitHub.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GitHub 上的笔记本中更详细地进行数据分析，例如相关性和分布图。
- en: 'Now we can split the data into training and test sets, as we did in previous
    recipes. Here, we add a stratification of the numerical variable. This makes sure
    that different sections (five of them) are included at equal measure in both training
    and test sets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将数据分割为训练集和测试集，就像我们在之前的示例中所做的那样。在这里，我们还添加了一个数值变量的分层。这确保了不同部分（五个部分）在训练集和测试集中的等量包含：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Before going ahead, let's look at the importance of the features using a model-independent
    technique.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们使用一个与模型无关的技术来查看特征的重要性。
- en: 'Before we run anything, however, let''s make sure we are running on the GPU:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在运行任何东西之前，让我们确保我们在 GPU 上运行：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let's build our PyTorch model, similar to the *Classifying in scikit-learn*,
    *Keras*, and *PyTorch* recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting
    Started with Artificial Intelligence in Python*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的 PyTorch 模型，类似于《Python 中的人工智能入门》第 1 章中的 *在 scikit-learn、Keras 和 PyTorch
    中分类* 的配方。
- en: 'We''ll implement a neural network regression with batch inputs using PyTorch.
    This will involve the following steps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 PyTorch 实现一个带批量输入的神经网络回归。这将涉及以下步骤：
- en: Converting data to torch tensors
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为 torch 张量：
- en: Defining the model architecture
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: Defining the loss criterion and optimizer
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失标准和优化器：
- en: Creating a data loader for batches
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建批量数据加载器：
- en: Running the training
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行训练：
- en: 'Without further preamble, let''s get to it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 没有进一步的序言，让我们开始吧：
- en: 'Begin by converting the data to torch tensors:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为 torch 张量：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This makes sure we load our numerical and categorical data into separate variables,
    similar to NumPy. If you mix data types in a single variable (array/matrix), they'll
    become objects. We want to get our numerical variables as floats, and the categorical
    variables as long (or int) indexing our categories. We also separate the training
    and test sets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保我们将数值和分类数据加载到不同的变量中，类似于NumPy。如果在单个变量（数组/矩阵）中混合数据类型，它们将变成对象。我们希望将数值变量作为浮点数加载，并将分类变量作为长整型（或整型）索引类别。我们还要将训练集和测试集分开。
- en: Clearly, an ID variable should not be important in a model. In the worst case,
    it could introduce a target leak if there's any correlation of the ID with the
    target. We've removed it from further processing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，ID变量在模型中不应该很重要。在最坏的情况下，如果ID与目标变量有任何相关性，它可能会引入目标泄漏。因此，我们已经将其从进一步处理中移除。
- en: 'Define the model architecture:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our activation function on the two linear layers (dense layers, in Keras terminology)
    is the **rectified linear unit activation** (**ReLU**) function. Please note that
    we couldn't have encapsulated the same architecture (easily) as a sequential model
    because of the different operations occurring on categorical and numerical types.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个线性层（Keras术语中的密集层）上使用的激活函数是**修正线性单元激活**（**ReLU**）函数。请注意，由于在分类和数值类型上发生了不同的操作，我们不能轻松地将相同的架构封装为顺序模型。
- en: 'Next, define the loss criterion and optimizer. We take the **mean square error**
    (**MSE**) as the loss and stochastic gradient descent as our optimization algorithm:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义损失标准和优化器。我们以**均方误差**（**MSE**）作为损失函数，使用随机梯度下降作为优化算法：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, create a data loader to input a batch of data at a time:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个数据加载器以一次输入一个数据批次：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We set a batch size of 10\. Now we can do our training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置批量大小为10。现在我们可以进行训练。
- en: Run the training!
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始训练！
- en: Since this seems so much more verbose than what we saw in Keras in the *Classifying
    in scikit-learn, Keras, and PyTorch *recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting
    Started with Artificial Intelligence in Python*, we commented this code quite
    heavily. Basically, we have to loop over epochs, and within each epoch an inference
    is performance, an error is calculated, and the optimizer applies the adjustments
    according to the error.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这似乎比我们在《Python人工智能入门第一章中看到的Keras中的分类》一书中的示例要冗长得多，我们对此代码进行了详细的注释。基本上，我们必须在每个epoch上进行循环，并在每个epoch内执行推断、计算误差，并根据误差应用优化器进行调整。
- en: 'This is the loop over epochs without the inner loop for training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是没有内部训练循环的epoch循环：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The training is performed inside this loop over all the batches of the training
    data. This looks as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是在所有训练数据的批次循环内执行的。它看起来如下所示：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the output we get. TQDM provides us with a helpful progress bar. At
    every tenth epoch, we print an update to show training and validation performance:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的输出。TQDM为我们提供了一个有用的进度条。在每个第十个epoch，我们打印一个更新，显示训练和验证性能：
- en: '![](img/e7b44a72-a1c4-4bdd-aad4-5df7c46cf88b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7b44a72-a1c4-4bdd-aad4-5df7c46cf88b.png)'
- en: 'Please note that we take the square root of `nn.MSELoss` because `nn.MSELoss`
    in PyTorch is defined as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们对`nn.MSELoss`取平方根，因为PyTorch中的`nn.MSELoss`定义如下：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s plot how our model performs for training and validation datasets during
    training:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型在训练和验证数据集上的表现情况：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following diagram shows the resulting plot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了结果绘图：
- en: '![](img/c4127906-e287-4283-ba89-2775c68f147c.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4127906-e287-4283-ba89-2775c68f147c.png)'
- en: We stopped our training just in time before our validation loss stopped decreasing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在验证损失停止减少之前及时停止了训练。
- en: 'We can also rank and bin our target variable and plot the predictions against
    it in order to see how the model is performing across the whole spectrum of house
    prices. This is to avoid the situation in regression, especially with MSE as the
    loss, that you only predict well for a mid-range of values, close to the mean,
    but don''t do well for anything else. You can find the code for this in the notebook
    on GitHub. This is called a lift chart (here with 10 bins):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对目标变量进行排名和分箱，并将预测结果绘制在其上，以便查看模型在整个房价范围内的表现。这是为了避免在回归中出现的情况，特别是当损失函数为MSE时，只能在接近均值的中等价值范围内进行良好预测，而对其他任何值都表现不佳。您可以在GitHub笔记本中找到此代码。这称为提升图表（这里有10个分箱）：
- en: '![](img/410cf6a0-9d27-419b-b9b1-6cdb765196ec.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/410cf6a0-9d27-419b-b9b1-6cdb765196ec.png)'
- en: We can see that the model, in fact, predicts very closely across the whole range
    of house prices. In fact, we get a Spearman rank correlation of about 93% with
    very high significance, which confirms that this model performs with high accuracy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，事实上，模型在整个房价范围内都预测得非常接近。事实上，我们得到了约 93% 的斯皮尔曼等级相关性，非常显著，这证实了这个模型具有很高的准确性。
- en: How it works...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运作...
- en: The deep learning neural network frameworks use different optimization algorithms.
    Popular among them are **Stochastic Gradient Descent **(**SGD**), **Root Mean
    Square Propogation** (**RMSProp**), and **Adaptive Moment Estimation** (**ADAM**).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习神经网络框架使用不同的优化算法。其中流行的有**随机梯度下降**（**SGD**）、**均方根传播**（**RMSProp**）和**自适应矩估计**（**ADAM**）。
- en: 'We defined stochastic gradient descent as our optimization algorithm. Alternatively,
    we could have defined other optimizers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随机梯度下降定义为我们的优化算法。或者，我们也可以定义其他优化器：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: SGD works the same as gradient descent except that it works on a single example
    at a time. The interesting part is that the convergence is similar to the gradient
    descent and is easier on the computer memory.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 与梯度下降的工作方式相同，只是每次只对单个示例进行操作。有趣的是，其收敛性类似于梯度下降，并且对计算机内存的要求更低。
- en: RMSProp works by adapting the learning rates of the algorithm according to the
    gradient signs. The simplest of the variants checks the last two gradient signs
    and then adapts the learning rate by increasing it by a fraction if they are the
    same, or decreases it by a fraction if they are different.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 的工作原理是根据梯度的符号来调整算法的学习率。最简单的变体检查最后两个梯度的符号，然后根据它们是否相同增加或减少学习率的一小部分。
- en: ADAM is one of the most popular optimizers. It's an adaptive learning algorithm
    that changes the learning rate according to the first and second moments of the
    gradients.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM 是最流行的优化器之一。它是一种自适应学习算法，根据梯度的一阶和二阶矩来调整学习率。
- en: 'Captum is a tool that can help us understand the ins and outs of the neural
    network model learned on the datasets. It can assist in learning the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Captum 是一个工具，可以帮助我们理解在数据集上学习的神经网络模型的细枝末节。它可以协助学习以下内容：
- en: Feature importance
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Layer importance
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层重要性
- en: Neuron importance
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元重要性
- en: This is very important in learning interpretable neural networks. Here, integrated
    gradients have been applied to understand feature importance. Later, neuron importance
    is also demonstrated by using the layer conductance method.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这在学习可解释神经网络中非常重要。在这里，使用了集成梯度来理解特征重要性。后来，还通过使用层导纳方法展示了神经元的重要性。
- en: There's more...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Given that we have our neural network defined and trained, let''s find the
    important features and neurons using the captum library:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经定义并训练了神经网络，让我们使用 captum 库找出重要的特征和神经元：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we have a NumPy array of feature importances.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个特征重要性的 NumPy 数组。
- en: 'Layer and neuron importance can also be obtained using this tool. Let''s look
    at the neuron importances of our first layer. We can pass on `house_model.act1`,
    which is the ReLU activation function on top of the first linear layer:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用这个工具获取层和神经元的重要性。让我们看看我们第一层神经元的重要性。我们可以传递 `house_model.act1`，这是第一线性层上的 ReLU
    激活函数：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is how it looks:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来是这样的：
- en: '![](img/a4f50954-7c9a-45c2-8d56-c406848fe859.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f50954-7c9a-45c2-8d56-c406848fe859.png)'
- en: The diagram shows the neuron importances. Apparently, one neuron is just not
    important.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示神经元的重要性。显然，有一个神经元并不重要。
- en: 'We can also see the most important variables by sorting the NumPy array we''ve
    obtained earlier:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过对我们之前获得的 NumPy 数组进行排序来查看最重要的变量：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'So here''s a list of the 10 most important variables:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最重要的 10 个变量的列表：
- en: '![](img/8f4f1815-2c75-49cc-8641-2690cc20965b.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f4f1815-2c75-49cc-8641-2690cc20965b.png)'
- en: Often, feature importances can help us to both understand the model and prune
    our model to become less complex (and hopefully less overfitted).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性通常有助于我们理解模型，并且剪枝模型以使其变得不那么复杂（希望也不那么过拟合）。
- en: See also
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The PyTorch documentation includes everything you need to know about layer types, data
    loading, losses, metrics, and training: [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 文档包含了关于层类型、数据加载、损失、度量和训练的一切信息：[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)
- en: A detailed discussion about optimization algorithms can be found in the following
    article: [https://imaddabbura.github.io/post/gradient-descent-algorithm/](https://imaddabbura.github.io/post/gradient-descent-algorithm/).
    Geoffrey Hinton and others explain mini-batch gradient descent in a presentation
    slide deck: [https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).
    Finally, you can find all the details on ADAM in the article that introduced it: [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有关优化算法的详细讨论可以在以下文章中找到：[https://imaddabbura.github.io/post/gradient-descent-algorithm/](https://imaddabbura.github.io/post/gradient-descent-algorithm/)。Geoffrey
    Hinton和其他人在演示幻灯片中详细解释了小批量梯度下降：[https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)。最后，您可以在介绍它的文章中找到有关ADAM的所有细节：[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)。
- en: Captum provides a lot of functionality as regards the interpretability and model
    inspection of PyTorch models. It's worth having a look at its comprehensive documentation
    at [https://captum.ai/](https://captum.ai/). Details can be found in the original
    paper at [https://arxiv.org/pdf/1703.01365.pdf](https://arxiv.org/pdf/1703.01365.pdf).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Captum为PyTorch模型的可解释性和模型检查提供了丰富的功能。您可以在其详尽的文档中找到更多信息：[https://captum.ai/](https://captum.ai/)。详细信息可以在原始论文中找到：[https://arxiv.org/pdf/1703.01365.pdf](https://arxiv.org/pdf/1703.01365.pdf)。
- en: Live decisioning customer values
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时决策客户价值
- en: 'Let''s assume we have the following scenario: we have a list of customers to
    call in order to sell them our product. Each phone call costs money in call center
    personal salaries, so we want to reduce these costs as much as possible. We have
    certain information about each customer that could help us determine whether they
    are likely to buy. After every call, we can update our model. The main goal is
    to call only the most promising customers and to improve our insights into which
    customers are more likely to pay for our product.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下情景：我们有一份顾客名单，需要给他们打电话来销售我们的产品。每次电话都需要支付呼叫中心人员的工资，因此我们希望尽可能地减少这些成本。我们对每个顾客都有一些信息，这些信息可以帮助我们确定他们是否有可能购买。每次通话后，我们可以更新我们的模型。主要目标是只打给最有潜力的顾客，并提高我们对哪些顾客更可能购买我们产品的洞察力。
- en: In this recipe, we will approach this with active learning, a strategy where
    we actively decide what to explore (and learn) next. Our model will help decide
    whom to call. Because we will update our model after each query (phone call),
    we will use online learning models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将采用主动学习的方法，这是一种策略，我们可以主动决定接下来要探索（和学习）什么。我们的模型将帮助决定打给谁。因为我们将在每次查询（电话）后更新我们的模型，所以我们将使用在线学习模型。
- en: Getting ready
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll prepare for our recipe by downloading our dataset and installing a few
    libraries.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过下载数据集和安装几个库来准备我们的配方。
- en: 'Again, we will get the data from OpenML:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将从OpenML获取数据：
- en: '[PRE35]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This dataset is called `bank-marketing`, and you can see a description on OpenML
    at [https://www.openml.org/d/1461](https://www.openml.org/d/1461).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集被称为`bank-marketing`，您可以在OpenML上查看其描述：[https://www.openml.org/d/1461](https://www.openml.org/d/1461)。
- en: For each row, describing a single person, we have different features, numerical
    and categorical, that tell us about demographics and customer history.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一行，描述一个人，我们有不同的特征，数值和分类的，告诉我们关于人口统计和顾客历史的信息。
- en: 'To model the likelihood of customers signing up for our product, we will use
    the scikit-multiflow package that specializes in online models. We will also use
    the `category_encoders` package again:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟顾客签署我们的产品的可能性，我们将使用专门用于在线模型的scikit-multiflow包。我们还将再次使用`category_encoders`包：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: With these two libraries in place, we can start the recipe.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这两个库，我们可以开始这个配方了。
- en: How to do it...
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We need to implement an exploration strategy and a model that is being continuously
    updated. We are using the online version of the random forest, the Hoeffding Tree,
    as our model. We are estimating the uncertainties at every step, and based on
    that we will return a candidate to call next.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现一个探索策略和一个正在不断更新的模型。我们正在使用在线版本的随机森林，Hoeffding Tree，作为我们的模型。我们正在估计每一步的不确定性，并基于此返回下一个要呼叫的候选人。
- en: 'As always, we will need to define a few preprocessing steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，我们需要定义一些预处理步骤：
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then we come to our active learning approach itself. This is inspired by `modAL.models.ActiveLearner`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们来到我们的主动学习方法本身。这受到了`modAL.models.ActiveLearner`的启发：
- en: '[PRE38]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Again, we create a scikit-learn-compatible class. It basically holds a machine
    learning model and a data preprocessor. We implement `fit()` and `predict()`,
    but also `score()` to get a model performance. We also implement an `update()`
    method that calls `partial_fit()` of the machine learning model. Calling `partial_fit()`
    instead of `fit()` considerably speeds up the computations, because we don't have
    to start from scratch every time we get new data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们创建了一个与scikit-learn兼容的类。它基本上保存了一个机器学习模型和一个数据预处理器。我们实现了`fit()`和`predict()`，还有`score()`来获取模型的性能。我们还实现了一个`update()`方法，调用机器学习模型的`partial_fit()`。调用`partial_fit()`而不是`fit()`大大加快了计算速度，因为我们不必每次获取新数据时都从头开始。
- en: 'Here''s how to create the active learning pipeline:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 创建主动学习管道的方法如下：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can run different simulations on our dataset with this setup. For example,
    we can compare a lot of experimentation (0.5 exploitation) against only exploitation
    (1.0), or no learning at all after the first batch. We basically go through a
    loop:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个设置在我们的数据集上运行不同的模拟。例如，我们可以比较大量实验（0.5开发利用）与仅开发利用（1.0），或者在第一批之后根本不学习。基本上我们通过一个循环进行：
- en: Via `active_pipeline.``max_margin_uncertainty()`, we present data to the active
    pipeline, and get a number of data points that integrate uncertainty and target
    predictions according to our preferences of the integration method.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`active_pipeline.max_margin_uncertainty()`，我们向主动管道呈现数据，并根据我们集成方法的偏好获取不确定性和目标预测的数据点数量。
- en: 'Once we get the actual results for these data points, we can update our model:
    `active_pipeline.update()`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们获得这些数据点的实际结果，我们就可以更新我们的模型：`active_pipeline.update()`。
- en: You can see an example of this in the notebook on GitHub.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub的笔记本中看到一个例子。
- en: 'We can see that curious wins out after the first few examples. Exploitation
    is actually the least successful scheme. By not updating the model, performance
    deteriorates over time:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在几个示例之后，好奇心获胜。实际上，开发利用是最不成功的方案。通过不更新模型，性能随时间而恶化：
- en: '![](img/9ea01c94-9b57-4dc3-afb0-8d8b1fc21d2c.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ea01c94-9b57-4dc3-afb0-8d8b1fc21d2c.png)'
- en: This is an ideal scenario for active learning or reinforcement learning, because,
    not unlike in reinforcement learning, uncertainty can be an additional criterion,
    apart from positive expectation, from a customer. Over time, this entropy reduction-seeking
    behavior reduces as the model's understanding of customers improves.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是主动学习或强化学习的理想场景，因为，与强化学习类似，不确定性可以成为除了客户正面期望外的附加标准。随着模型对客户理解的提升，随时间减少这种熵寻求行为。
- en: How it works...
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理如下…
- en: It's worth delving a bit more into a few of the concepts and strategies employed
    in this recipe.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 值得深入探讨这个配方中使用的一些概念和策略。
- en: Active learning
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主动学习
- en: Active learning means that we can actively query for more information; in other
    words, exploration is part of our strategy. This can be useful in scenarios where
    we have to actively decide what to learn, and where what we learn influences not
    only how much our model learns and how well, but also how much return on an investment
    we can get.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习意味着我们可以积极查询更多信息；换句话说，探索是我们策略的一部分。在我们必须主动决定学习什么以及我们学到了什么不仅影响我们的模型学习量和质量，还影响我们可以获得的投资回报的场景中，这是非常有用的。
- en: Hoeffding Tree
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 霍夫丁树
- en: The Hoeffding Tree (also known as the *Very Fast Decision Tree*, *VFDT* for
    short) was introduced in 2001 by Geoff Hulten and others (*Mining time-changing
    data streams*). It is an incrementally growing decision tree for streamed data.
    Tree nodes are expanded based on the Hoeffding bound (or additive Chernoff bound).
    It was theoretically shown that, given sufficient training data, a model learned
    by the Hoeffding tree converges very closely to the one built by a non-incremental
    learner.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 霍夫丁树（也称为*非常快速决策树*，简称*VFDT*）由Geoff Hulten等人于2001年引入（*挖掘时间变化数据流*）。它是一个增量增长的用于数据流的决策树。树节点基于霍夫丁边界（或加法切尔诺夫边界）进行扩展。理论上已经证明，给定足够的训练数据，由霍夫丁树学习的模型与非增量学习者构建的模型非常接近。
- en: 'The Hoeffding bound is defined as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 霍夫丁边界定义如下：
- en: '![](img/85a026e0-2e21-40a4-ac43-2037dec07219.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85a026e0-2e21-40a4-ac43-2037dec07219.png)'
- en: It's important to note that the Hoeffding Tree doesn't deal with data distributions
    that change over time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，霍夫丁树不处理随时间变化的数据分布。
- en: Class weighting
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类加权
- en: Since we are dealing with an imbalanced dataset, let's use class weights. This
    basically means that we are upsampling the minority (signing up) class and downsampling
    the majority class (not signing up).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for the class weights is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c582bcf-2fbc-4e0e-8237-dbeae535d90d.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, in Python, we can write the following:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can then use these class weights for sampling.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: We'll close the recipe with a few more pointers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only a few models in scikit-learn allow incremental or online learning. Refer
    to the list at [https://scikit-learn.org/stable/modules/computing.html](https://scikit-learn.org/stable/modules/computing.html).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: A few linear models include the `partial_fit()` method. The scikit-multiflow library specializes
    in incremental and online/streaming models: [https://scikit-multiflow.github.io/](https://scikit-multiflow.github.io/)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: You can find more resources and ideas regarding active learning from a recent
    review that concentrates on biomedical image processing (Samuel Budd and others,
    *A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image
    Analysis*, 2019; [https://arxiv.org/abs/1910.02923](https://arxiv.org/abs/1910.02923)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach is inspired by the modalAI Python active learning package, which
    you can find at [https://modal-python.readthedocs.io/](https://modal-python.readthedocs.io/).
    We recommend you check it out if you are interested in active learning approaches.
    A few more Python packages are available, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Alipy: Active Learning in Python: [http://parnec.nuaa.edu.cn/huangsj/alipy/](http://parnec.nuaa.edu.cn/huangsj/alipy/)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Active Learning: A Google repo about active learning: [https://github.com/google/active-learning](https://github.com/google/active-learning)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the main decisions in active learning is the trade-off between exploration
    and exploitation. You can find out more about this in a paper called *Exploration
    versus exploitation in active learning: a Bayesian approach*: [http://www.vincentlemaire-labs.fr/publis/ijcnn_2_2010_camera_ready.pdf](http://www.vincentlemaire-labs.fr/publis/ijcnn_2_2010_camera_ready.pdf)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Battling algorithmic bias
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Correctional Offender Management Profiling for Alternative Sanctions** (**COMPAS**)
    is a commercial algorithm that assigns risk scores to criminal defendants based
    on their case records. This risk score corresponds to the likelihood of reoffending
    (recidivism) and of committing violent crimes, and this score is used in court
    to help determine sentences. The ProPublica organization obtained scores and data
    in 1 county in Florida, containing data on about 7,000 people. After waiting for
    2 years to see who reoffended, they audited the COMPAS model in 2016 and found
    very concerning issues with the model. Among the ProPublica findings was discrimination
    according to gender, race, and ethnicity, particularly in the case of over-predicting
    recidivism for ethnic minorities.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Discrimination presents a major problem for AI systems, and illustrates the
    importance of auditing your model and the data you feed into your model. Models
    built on human decisions will amplify human biases if this bias is ignored. Not
    just from a legal perspective, but also ethically, we want to build models that
    don't disadvantage certain groups. This poses an interesting challenge for model
    building.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 歧视对AI系统构成重大问题，说明了审核模型及其输入数据的重要性。如果忽视了这种偏见，基于人类决策建立的模型将放大人类偏见。我们不仅从法律角度考虑，而且从道德角度来说，我们希望构建不会给某些群体带来不利的模型。这为模型构建提出了一个有趣的挑战。
- en: Generally, we would think that justice should be blind to gender or race. This
    means that court decisions should not take these sensitive variables like race
    or gender into account. However, even if we omit them from our model training,
    these sensitive variables might be correlated to some of the other variables,
    and therefore they can still affect decisions, to the detriment of protected groups
    such as minorities or women.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们认为司法应对性别或种族视而不见。这意味着法院的决定不应考虑这些敏感变量如种族或性别。然而，即使我们在模型训练中省略了它们，这些敏感变量可能与其他变量相关，并因此仍然可能影响决策，对受保护群体如少数族裔或妇女造成损害。
- en: In this section, we are going to work with the COMPAS modeling dataset as provided
    by ProPublica. We are going to check for racial bias, and then create a model
    to remove it. You can find the original analysis by ProPublica at [https://github.com/propublica/compas-analysis](https://github.com/propublica/compas-analysis).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用ProPublica提供的COMPAS建模数据集。我们将检查是否存在种族偏见，并创建一个模型来消除它。您可以在ProPublica的原始分析中找到这些信息：[https://github.com/propublica/compas-analysis](https://github.com/propublica/compas-analysis)。
- en: Getting ready
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Before we can start, we'll first download the data, mention issues in preprocessing,
    and install the libraries.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们首先会下载数据，提及预处理中的问题，并安装所需的库。
- en: 'Let''s get the data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取数据：
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Each row represents the risk of violence and the risk of recidivism scores for
    an inmate. The final column, `two_year_recid`, indicates our target.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 每行代表犯人的暴力风险和累犯风险评分。最后一列`two_year_recid`表示我们的目标。
- en: 'ProPublica compiled their dataset from different sources, which they matched
    up according to the names of offenders:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ProPublica从不同来源编制了他们的数据集，他们根据罪犯的名字进行了匹配：
- en: Criminal records from the Broward County Clerk's Office website
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自布罗沃德县法院书记处网站的刑事记录。
- en: Public incarceration records from the Florida Department of Corrections website
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自佛罗里达监狱部门网站的公共监禁记录。
- en: COMPAS scores, which they obtained through a public record information request
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们通过公开记录信息请求获取的COMPAS评分。
- en: 'We can highlight a few issues in the dataset:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以突出几个数据集中的问题：
- en: The column race is a protected category. It should not be used as a feature
    for model training, but as a control.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 种族一栏是受保护的类别。不应作为模型训练的特征，而应作为控制变量。
- en: There are full names in the dataset, which will not be useful, or might even
    give away the ethnicity of the inmates.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中有全名，这些全名可能没有用处，甚至可能透露出犯人的种族。
- en: There are case numbers in the dataset. These will likely not be useful for training
    a model, although they might have some target leakage in the sense that increasing
    case numbers might give an indication of the time, and there could be a drift
    effect in the targets over time.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中有案件编号。这些可能不适合用于训练模型，尽管它们可能具有某种目标泄漏，即案件编号的增加可能表明时间，并且在目标上可能存在漂移效应。
- en: There are missing values. We will need to carry out imputation.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存在缺失值。我们需要进行插补。
- en: There are date stamps. These will probably not be useful and might even come
    with associated problems (see point 3). However, we can convert these features
    into UNIX epochs, which indicates the number of seconds that have elapsed since
    1970, and then calculate time periods between date stamps, for example, by repurposing
    `NumericDifferenceTransformer` that we saw in an earlier recipe. We can then use
    these periods as model features rather than the date stamps.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存在日期时间戳。这些可能不会有用，甚至可能会带来相关问题（见第3点）。然而，我们可以将这些特征转换为UNIX时间戳，即自1970年以来经过的秒数，然后计算日期时间戳之间的时间段，例如通过重新使用我们在之前示例中看到的`NumericDifferenceTransformer`。然后，我们可以将这些时间段用作模型特征，而不是日期时间戳。
- en: We have several categorical variables.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有几个分类变量。
- en: The charge description (`c_charge_desc`) might need to be cleaned up.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计费描述（`c_charge_desc`）可能需要清理。
- en: 'Mathias Barenstein has pointed out ([https://arxiv.org/abs/1906.04711](https://arxiv.org/abs/1906.04711))
    a data processing error in ProPublica''s cutoff that resulted in keeping 40% more
    recidivists than they should have. We''ll apply his correction to the two-year
    cutoff:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Mathias Barenstein 指出（[https://arxiv.org/abs/1906.04711](https://arxiv.org/abs/1906.04711)）ProPublica
    在处理数据时出现了一个错误，导致保留了比他们本应保留的再犯者多 40% 的数据。我们将他的修正应用于两年的截止日期：
- en: '[PRE42]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will use a few libraries in this recipe, which can be installed as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中我们将使用几个库，可以按以下方式安装：
- en: '[PRE43]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`category-encoders` is a library that provides functionality for categorical
    encoding beyond what scikit-learn provides.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`category-encoders` 是一个提供超出 scikit-learn 所提供的分类编码功能的库。'
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: Let's get some basic terminology out of the way first. We need to come up with
    metrics for fairness. But what does fairness (or, if we look at unfairness, bias)
    mean?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一些基本术语。我们需要为公平性制定度量标准。但是公平性（或者，如果我们看不公平性，偏见）意味着什么？
- en: 'Fairness can refer to two very different concepts:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性可以指两个非常不同的概念：
- en: '[**equal opportunity**]: There should be no difference in the relative ratios
    of predicted by the model versus actually true; or'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**平等机会**]：模型预测与实际情况之间不应有差异；或'
- en: '[**equal outcome**]: There should be no difference between model outcomes at
    all.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**等结果**]：模型的结果应完全相同。'
- en: The first is also called **equal odds**, while the latter refers to **equal
    false positive rates**. While equal opportunity means that each group should be
    given the same chance regardless of their group, the equal outcome strategy implies
    that the underperforming group should be given more lenience or chances relative
    to the other group(s).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个也被称为**等几率**，而后者指的是**等假阳性率**。平等机会意味着每个群体都应该有同样的机会，而平等结果策略则意味着表现不佳的群体应该相对其他群体更加宽容或者有更多机会。
- en: We'll go with the idea of false positive rates, which intuitively appeals, and
    which is enshrined in law in many jurisdictions in the case of equal employment
    opportunities. We'll provide a few resources about these terms in the *See also* section.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用假阳性率的概念，这在直觉上具有吸引力，并且在许多司法管辖区的平等就业机会案例中被确立为法律。关于这些术语，我们将在*参见*部分提供一些资源。
- en: 'Therefore, the logic for the impact calculation is based on values in the confusion
    matrix, most importantly, false positives, which we''ve just mentioned. These
    cases are predicted positive even though they are actually negative; in our case,
    people predicted as reoffenders, who are not reoffenders. Let''s write a function
    for this:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，影响计算的逻辑基于混淆矩阵中的数值，最重要的是假阳性，我们刚刚提到的。这些情况被预测为阳性，实际上却是阴性；在我们的情况下，被预测为再犯者的人，实际上不是再犯者。让我们为此编写一个函数：
- en: '[PRE44]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now use this function in order to summarize the impact on particular
    groups with this code:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个函数来总结特定群体的影响，代码如下：
- en: '[PRE45]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This first calculates the confusion matrix with true positives and false negatives,
    and then encodes the **adverse impact ratio** (**AIR**), known in statistics also
    as the **Relative Risk Ratio** (**RRR**). Given any performance metric, we can
    write the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这首先计算混淆矩阵，其中包括真阳性和假阴性，然后编码**不利影响比率**（**AIR**），在统计学中也被称为**相对风险比率**（**RRR**）。鉴于任何性能指标，我们可以写成以下形式：
- en: '![](img/4f66e08a-0870-484f-9281-89c9302ac84f.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f66e08a-0870-484f-9281-89c9302ac84f.png)'
- en: This expresses an expectation that the metric for the protected group (African-Americans)
    should be the same as the metric for the norm group (Caucasians). In this case,
    we'll get 1.0\. If the metric of the protected group is more than 20 percentage
    points different to the norm group (that is, lower than 0.8 or higher than 1.2),
    we'll flag it as a significant discrimination.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这表达了一个期望，即受保护群体（非裔美国人）的指标应该与常规群体（白种人）的指标相同。在这种情况下，我们将得到 1.0\. 如果受保护群体的指标与常规群体相差超过
    20 个百分点（即低于 0.8 或高于 1.2），我们将标记它为显著的歧视。
- en: '**Norm group**: **a norm group**, also known as a **standardization sample**
    or **norming group**, is a sample of the dataset that represents the population
    to which the statistic is intended to be compared. In the context of bias, its
    legal definition is the group with the highest success, but in some contexts,
    the entire dataset or the most frequent group are taken as the baseline instead. Pragmatically,
    we take the white group, since they are the biggest group, and the group for which
    the model works best.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding function, we calculate the false positive rates by sensitive
    group. We can then check whether the false positive rates for African-Americans
    versus Caucasians are disproportionate, or rather whether the false positive rates
    for African-Americans are much higher. This would mean that African-Americans
    get flagged much more often as repeat offenders than they should be. We find that
    this is indeed the case:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e72da81-d801-4426-a642-1bb22a2b7a19.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'A short explanation about this table follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'reoffend: frequencies of reoffending'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'score: average score for the group'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'N: the total number of people in the group'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FPR: false positive rates'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FNR: false negative rates'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DFP: disproportionate false positive'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DFN: disproportionate false negative'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last FPR and FNR columns together can give an idea about the general quality
    of the model. If both are high, the model just doesn't perform well for the particular
    group. The last two columns express the adverse impact ratio of FPR and FNR ratios,
    respectively, which is what we'll mostly focus on. We need to reduce the racial
    bias in the model by reducing the FPR of African-Americans to a tolerable level.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do some preprocessing and then we''ll build the model:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`CountVectorizer` counts a vocabulary of words, indicating how many times each
    word is used. This is called a bag-of-words representation, and we apply it to
    the charge description column. We exclude English stop words, which are very common
    words such as prepositions (such as **on** or **at**) and personal pronouns (for
    example **I **or **me**); we also limit the vocabulary to 100 words and words
    that don''t appear in more than 85% of the fields.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We apply dummy encoding (one-hot encoding) to the charge degree.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use two different transformations? Basically, the description is a
    textual description of why someone was charged with a crime. Every field is different.
    If we used one-hot encoding, every field would get their own dummy variable, and
    we wouldn't be able to see any commonalities between fields.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we create a new variable for stratification in order to make sure
    that we have similar proportions in the training and test datasets for both recidivism
    (our target variable) and whether someone is African-American. This will help
    us to calculate metrics to check for discrimination:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We do some data engineering, deriving variables to record how many days someone
    has spent in jail, has waited for a trial, or has waited for an arrest.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行一些数据工程，导出变量来记录某人在监狱中度过了多少天，等待了多久的审判，或者等待了多久的逮捕。
- en: 'We''ll build a neural network model using jax similar to the one we''ve encountered
    in the *Classifying in scikit-learn, Keras, and PyTorch *recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml),
    *Getting Started with Artificial Intelligence in Python*. This time, we''ll do
    a fully fledged implementation:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用类似于我们在《Python中的人工智能入门》[第1章](87098651-b37f-4b05-b0ee-878193f28b95.xhtml)中遇到的方法来构建一个jax神经网络模型。这次，我们将进行一个完整的实现：
- en: '[PRE48]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is a scikit-learn wrapper of a JAX neural network. For scikit-learn compatibility,
    we inherit from `ClassifierMixin` and implement `fit()` and `predict()`. The most
    important part here is the penalized MSE method, which, in addition to model predictions
    and targets, takes into account a sensitive variable.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个JAX神经网络的scikit-learn包装器。为了与scikit-learn兼容，我们从`ClassifierMixin`继承并实现`fit()`和`predict()`。这里最重要的部分是惩罚MSE方法，除了模型预测和目标之外，还考虑了一个敏感变量。
- en: 'Let''s train it and check the performance. Please note that we feed in `X`,
    `y`, and `sensitive_train`, which we define as the indicator variable for African-American
    for the training dataset:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练它并检查性能。请注意，我们输入`X`、`y`和`敏感训练`，它是用于训练数据集的非洲裔美国人的指示变量：
- en: '[PRE49]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We visualize the statistics as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将统计数据可视化如下：
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This is the table we get:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的表格：
- en: '![](img/1970d764-9568-4537-aa6f-38e18a5acd15.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1970d764-9568-4537-aa6f-38e18a5acd15.png)'
- en: We can see that the disproportionate false positive rate for African-Americans
    is very close to (and even lower than) 1.0, which is what we wanted. The test
    set is small and doesn't contain enough samples for Asians and native Americans,
    so we can't calculate meaningful statistics for those groups. We could, however,
    extend our approach to encompass these two groups as well if we wanted to ensure
    that these two groups had equal false positive rates.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到非洲裔美国人的不成比例的误报率非常接近（甚至低于）1.0，这正是我们想要的。测试集很小，没有足够的样本来计算亚裔和美洲原住民的有意义统计数据。然而，如果我们希望确保这两个群体的误报率相等，我们可以扩展我们的方法来涵盖这两个群体。
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: The keys for this to work are custom objective functions or loss functions.
    This is far from straightforward in scikit-learn, although we will show an implementation
    in the following section.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法能起作用的关键是自定义的目标函数或损失函数。在scikit-learn中，这远非易事，尽管我们将在接下来的部分展示一个实现。
- en: Generally, there are different possibilities for implementing your own cost
    or loss functions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有不同的可能性来实现自己的成本或损失函数。
- en: LightGBM, Catboost, and XGBoost each provide an interface with many loss functions
    and the ability to define custom loss functions.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM、Catboost和XGBoost都提供了许多损失函数的接口和定义自定义损失函数的能力。
- en: PyTorch and Keras (TensorFlow) provide an interface.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch和Keras（TensorFlow）提供了一个接口。
- en: You can implement your model from scratch (this is what we've done in the main
    recipe).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从头开始实现你的模型（这就是我们在主食谱中所做的）。
- en: Scikit-learn generally does not provide a public API for defining your own loss
    functions. For many algorithms, there is only a single choice, and sometimes there
    are a couple of alternatives. The rationale in the case of split criteria with
    trees is that loss functions have to be performant, and only Cython implementations
    will guarantee this. This is only available in a non-public API, which means it
    is more difficult to use.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于scikit-learn，通常没有公共API来定义自己的损失函数。对于许多算法，只有一个选择，有时候还有几个备选项。在使用树的分裂标准时，损失函数必须是高效的，只有Cython实现能保证这一点。这仅在非公共API中可用，这意味着使用起来更加困难。
- en: Finally, when there's no (straightforward) way to implement a custom loss, you
    can wrap your algorithms in a general optimization scheme such as genetic algorithms.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当没有（直接）方法来实现自定义损失时，可以将算法包装在如遗传算法等通用优化方案中。
- en: In neural networks, as long as you provide a differentiable loss function, you
    can plug in anything you want.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，只要提供一个可微的损失函数，你可以插入任何你想要的东西。
- en: 'Basically, we were able to encode the adverse impact as a penalty term with
    the **Mean Squared Error** (**MSE**) function. It is based on the MSE that we''ve
    mentioned before, but has a penalty term for adverse impact. Let''s look again
    at the loss function:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们能够将不利影响编码为具有**均方误差（MSE）**函数的惩罚项。这基于我们之前提到的MSE，但具有不利影响的惩罚项。让我们再看一下损失函数：
- en: '[PRE51]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The first thing to note is that instead of two variables, we pass three variables.
    `sensitive` is the variable relevant to the adverse impact, indicating if we have
    a person from a protected group.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，我们传递了三个变量而不是两个变量。`sensitive`是与不利影响相关的变量，指示我们是否有来自受保护群体的人。
- en: 'The calculation works as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 计算方法如下：
- en: We calculate the MSE overall, err, from model predictions and targets.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从模型预测和目标计算总体均方误差err。
- en: We calculate the MSE for the protected group, `err_s`.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算受保护群体的均方误差，`err_s`。
- en: We take the ratio of the MSE for the protected group over the MSE overall (AIR)
    and limit it to between 1.0 and 2.0\. We don't want values lower than 1, because
    we are only interested in the AIR if it's negatively affecting the protected group.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取受保护群体的均方误差与总体均方误差（AIR）的比值，并将其限制在1.0到2.0之间。我们不希望值低于1，因为我们只关心AIR是否对受保护群体产生负面影响。
- en: We then multiply AIR by the overall MSE.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将AIR乘以总体MSE。
- en: As for 2, the MSE can simply be calculated by multiplying the predictions and
    targets, each by `sensitive`. That would cancel out all points, where sensitive
    is equal to 0.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 2，MSE可以简单地通过将预测和目标分别乘以`sensitive`来计算。这将取消所有`sensitive`等于0的点。
- en: As for 4, it might seem that this would cancel out the overall error, but we
    see that it actually seems to work. We probably could have added the two terms
    as well to give both errors a similar importance.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 4，看似可以取消总体误差，但实际上我们发现它似乎确实有效。我们可能也可以添加这两项，以便给两种错误赋予类似的重要性。
- en: We use the autograd functionality in Jax to differentiate this.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Jax中的autograd功能来进行微分。
- en: There's more...
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'In the following, we''ll use the non-public scikit-learn API to implement a
    custom split criterion for decision trees. We''ll use this to train a random forest
    model with the COMPAS dataset:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将使用非公开的scikit-learn API来为决策树实现自定义分裂标准。我们将使用这个来训练一个带有COMPAS数据集的随机森林模型：
- en: This extends the implementation of the Hellinger criterion by Evgeni Dubov ([https://github.com/EvgeniDubov/hellinger-distance-criterion](https://github.com/EvgeniDubov/hellinger-distance-criterion)).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这扩展了Evgeni Dubov的Hellinger准则的实现（[https://github.com/EvgeniDubov/hellinger-distance-criterion](https://github.com/EvgeniDubov/hellinger-distance-criterion)）。
- en: '[PRE52]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s use this for training and test it:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个来训练并测试它：
- en: '[PRE53]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This gives us an AUC of 0.62:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个AUC值为0.62：
- en: '![](img/3d393169-1317-4c60-8c3e-28c201f86e29.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d393169-1317-4c60-8c3e-28c201f86e29.png)'
- en: We can see that, although we came a long way, we didn't completely remove all
    bias. 30% (DFP for African-Americans) would still be considered unacceptable.
    We could try different refinements or sampling strategies to improve the result.
    Unfortunately, we wouldn't be able to use this model in practice.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，尽管我们已经取得了很大进展，但我们并没有完全消除所有偏见。30%（非洲裔美国人的DFP）仍然被认为是不可接受的。我们可以尝试不同的改进或采样策略来改善结果。不幸的是，我们不能在实践中使用这个模型。
- en: As an example, a way to address this is to do model selection within the random
    forest. Since each of the trees would have their own way of classifying people,
    we could calculate the adverse impact statistics for each of the individual trees
    or combinations of trees. We could remove trees until we are left with a subset
    of trees that satisfy our adverse impact conditions. This is beyond the scope
    of this chapter.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，解决此问题的一种方法是在随机森林中进行模型选择。由于每棵树都有其独特的分类方式，我们可以计算每棵单独树或树组合的不利影响统计数据。我们可以删除树，直到剩下一组满足我们不利影响条件的树。这超出了本章的范围。
- en: See also
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can read up more on algorithmic fairness in different places. There''s
    a wide variety of literature available on fairness:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在不同的地方进一步了解算法公平性。有大量关于公平性的文献可供参考：
- en: A Science Magazine article about the COMPAS model (Julia Dressel and Hany Farid,
    2018, *The accuracy, fairness, and limits of predicting recidivism*): [https://advances.sciencemag.org/content/4/1/eaao5580](https://advances.sciencemag.org/content/4/1/eaao5580)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇关于COMPAS模型的《科学》杂志文章（Julia Dressel 和 Hany Farid，2018，《预测累犯的准确性、公平性和限制》，[https://advances.sciencemag.org/content/4/1/eaao5580](https://advances.sciencemag.org/content/4/1/eaao5580)）：
- en: '*A comparative study of fairness-enhancing interventions in machine learning*
    (Sorelle Friedler and others, 2018): [https://arxiv.org/pdf/1802.04422.pdf](https://arxiv.org/pdf/1802.04422.pdf)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Survey on Bias and Fairness in Machine Learning* (Mehrabi and others, 2019): [https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effect of explaining fairness (Jonathan Dodge and others, 2019): [https://arxiv.org/pdf/1901.07694.pdf](https://arxiv.org/pdf/1901.07694.pdf)
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different Python libraries are available for tackling bias (or, inversely,
    algorithmic fairness):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: fairlearn: [https://github.com/fairlearn/fairlearn](https://github.com/fairlearn/fairlearn)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AIF360: [https://github.com/IBM/AIF360](https://github.com/IBM/AIF360)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FairML:[ https://github.com/adebayoj/fairml](https://github.com/adebayoj/fairml)
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BlackBoxAuditing: [https://github.com/algofairness/BlackBoxAuditing](https://github.com/algofairness/BlackBoxAuditing)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balanced Committee Election: [https://github.com/huanglx12/Balanced-Committee-Election](https://github.com/huanglx12/Balanced-Committee-Election)
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, Scikit-Lego includes functionality for fairness: [https://scikit-lego.readthedocs.io/en/latest/fairness.html](https://scikit-lego.readthedocs.io/en/latest/fairness.html)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: While you can find many datasets on recidivism by performing a Google dataset
    search ([https://toolbox.google.com/datasetsearch](https://toolbox.google.com/datasetsearch)),
    there are many more applications and corresponding datasets where fairness is
    important, such as credit scoring, face recognition, recruitment, or predictive
    policing, to name just a few.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: There are different places to find out more about custom losses. The article
    *Custom loss versus custom scoring* ([https://kiwidamien.github.io/custom-loss-vs-custom-scoring.html](https://kiwidamien.github.io/custom-loss-vs-custom-scoring.html))
    affords a good overview. For implementations of custom loss functions in gradient
    boosting, towardsdatascience ([https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d](https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d))
    is a good place to start.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting CO[2] time series
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will test out some well-known models (ARIMA, SARIMA) and
    signal decomposition by forecasting using Facebook's Prophet library on the time
    series data, in order to check their performance at forecasting our time series
    of CO[2] values.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to prepare for this recipe, we'll install libraries and download a
    dataset.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `statsmodels` package and prophet:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We will analyze the CO[2] concentration data in this recipe. You can see the
    data loading in the notebook on GitHub accompanying this recipe, or in the scikit-learn
    **Gaussian process regression** (**GPR**) example regarding Mauna Loa CO[2] data: [https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-co2-py](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-co2-py)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is one of the earliest recordings available on atmospheric recordings
    of CO[2]. As it will be later observed, this data follows a sinusoidal pattern,
    with the CO[2] concentration rising in winter and falling in the summer owing
    to the decreasing quantity of plants and vegetation in the winter season:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The dataset contains the average CO[2] concentration measured at Mauna Loa Observatory
    in Hawaii from 1958 to 2001\. We will model the CO[2] concentration with respect
    to that.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we'll get to forecasting our time series of CO[2] data. We'll first explore
    the dataset, and then we'll apply the ARIMA and SARIMA techniques.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the time series:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here''s the graph:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8234b20c-db4d-477c-973a-6bde1c1cc10a.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'The script here shows the time series seasonal decomposition of the CO[2] data,
    showing a clear seasonal variation in the CO[2] concentration, which can be traced
    back to the biology:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now that we have preprocessed data for decomposition, let''s go ahead with
    it:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here, we see the decomposition: the observed time series, its trend, seasonal
    components, and what remains unexplained, the residual element:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6df3900f-8db2-4062-8acc-9af6f992bb32.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: Now, let's analyze the time series.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing time series using ARIMA and SARIMA
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will fit ARIMA and SARIMA models to the dataset.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: We'll define our two models and apply them to each point in the test dataset.
    Here, we iteratively fit the model on all the points and predict the next point,
    as a one-step-ahead.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we split the data:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This leaves us with 468 samples for training and 53 for testing.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the models:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then we train the models:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We get an MSE in the test of 0.554 and 0.405 for ARIMA and SARIMA models, respectively. Let''s
    see how the models fit graphically:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b50a0712-4290-488e-a50a-5790bb09df7a.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: We could do a parameter exploration using the **Akaike information criterion**
    (**AIC**), which expresses the quality of the model relative to the number of
    parameters in the model. The model object returned by the fit function in statsmodels
    includes the AIC, so we could do a grid search over a range of parameters, and
    then select the model that minimizes the AIC.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series data is a collection of observations *x(t)*, where each data point
    is recorded at time *t*. In most cases, time is a discrete variable, that is, ![](img/baa09adc-90fb-44ca-b96e-087c92c3cf22.png).
    We are looking at forecasting, which is the task of predicting future values based
    on the previous observations in the time series.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to explain the models that we''ve used, ARIMA and SARIMA, we''ll have
    to go step by step, and explain each in turn:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregression** (**AR**)'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving Average** (**MA**)'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive Moving Average** (**ARMA**)'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive Integrated Moving Average** (**ARIMA**) and'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonal Autoregressive Integrated Moving Average** (**SARIMA**)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMA and SARIMA are based on the ARMA model, which is an **autoregressive moving
    average** model. Let's briefly go through some of the basics.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'ARMA is a linear model, defined in two parts. First, the autoregressive linear
    model:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8d3ef6c-4bde-4615-8fea-de6c11f13222.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/75d01535-5147-4d3d-81e3-44ed4e4b3e51.png) are parameters and ![](img/fb710b5e-aa6e-4bcc-a4d6-2d241bf69636.png) is
    a constant, ![](img/503d11f9-a902-4347-a467-f99a479cae45.png) is white noise,
    and ![](img/47929613-e8f3-428c-9f03-cab92c370125.png) is the order of the model
    (or the window size of the linear model). The second part of ARMA is the moving
    average, and this is again a linear regression, but of non-observable, lagged
    error terms, defined as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce04acdd-ae4c-43dd-8593-0e24cffbf671.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/da090912-380c-4843-81e6-3a4a5535fbf4.png) is the order of the
    moving average, ![](img/17c2294b-a3e3-4c3a-98c9-5f55f1273173.png) are the parameters,
    and ![](img/20426613-f884-4151-b919-a318c9bc0bb1.png) the expectation or the mean
    of the time series ![](img/5b764349-1447-468f-ae94-a061b2fe1d10.png). The ARMA(p, q)
    model is then the composite of both of these models, AR(p) and MA(q):'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c17c537-7b95-4202-acd1-d1126b2c5bc2.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: The fitting procedure is a bit involved, particularly because of the MA part.
    You can read up on the Box-Jenkins method on Wikipedia if you are interested: [https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few limitations to note, however. The time series has to be the
    following:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Stationary: Basically, mean and covariance across observations have to be constant
    over time.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nonperiodic: Although bigger values for p and q could be used to model seasonality,
    it is not part of the model.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear: Each value for ![](img/1bd795c4-8671-4f00-b53b-7e68a6247202.png)can
    be modeled as a linear combination of previous values and error terms.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different extensions of ARMA to address the first two limitations,
    and that's where ARIMA and SARIMA come in.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'ARIMA (*p*, *d*, *q*) stands for **autoregressive integrated moving average**.
    It comes with three parameters:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '**p**: The number of autoregressive terms (autoregression)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d**: The number of non-seasonal differences needed for stationarity (integration)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d**：需要使时间序列平稳化的非季节性差分数（积分）'
- en: '**q**: The number of lagged forecast errors (moving average)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**q**：滞后的预测误差数（移动平均）'
- en: 'The *integration* refers to differencing. In order to stabilize the mean, we
    can take the difference between consecutive observations. This can also remove
    a trend or eliminate seasonality. It can be written as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '*积分*是指差异化。为了稳定均值，我们可以取连续观测之间的差异。这也可以去除趋势或消除季节性。可以写成如下形式：'
- en: '![](img/0430a833-0c30-420f-9959-192925196e54.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0430a833-0c30-420f-9959-192925196e54.png)'
- en: This can be repeated several times, and this is what the parameter d describes
    that ARIMA comes with. Please note that ARIMA can handle drifts and non-stationary
    time series. However, it is still unable to handle seasonality.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重复多次，并且这就是参数d描述的ARIMA。请注意，ARIMA可以处理漂移和非平稳时间序列。但是，它仍然无法处理季节性。
- en: SARIMA stands for seasonal ARIMA, and is an extension of ARIMA in that it also
    takes into account the seasonality of the data.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: SARIMA代表季节性ARIMA，是ARIMA的扩展，因为它还考虑了数据的季节性。
- en: '*SARIMA*(*p*, *d*, *q*)(*P*, *D*, *Q*)*m* contains the non-seasonal parameters
    of ARIMA and additional seasonal parameters. Uppercase letters P, D, and Q annotate
    the seasonal moving average and autoregressive components, where *m* is the number
    of periods in each season. Often this is the number of periods in a year; for
    example *m=4* would stand for a quarterly seasonal effect, meaning that *D* stands
    for seasonal differencing between observations *Xt* and *Xt-m*, and *P* and *Q*
    stand for linear models with backshifts of *m*.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '*SARIMA*(*p*, *d*, *q*)(*P*, *D*, *Q*)*m*包含ARIMA的非季节性参数和额外的季节性参数。大写字母P、D和Q注释了季节性移动平均和自回归分量，其中*m*是每个季节中的周期数。通常这是一年中的周期数；例如*m=4*表示季度季节效应，意味着*D*表示观测*Xt*和*Xt-m*之间的季节性差异，*P*和*Q*表示具有m个滞后的线性模型。'
- en: In Python, the statsmodels library provides a method to perform the decomposition
    of the signal based on the seasonality of the data.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，statsmodels库提供了基于数据季节性的信号分解方法。
- en: There's more...
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Prophet is a library provided by Facebook for forecasting time series data.
    It works on an additive model and fits non-linear models. The library works best
    when the data has strong seasonal effects and has enough historic trends available.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: Prophet是Facebook提供的用于预测时间序列数据的库。它采用加法模型并拟合非线性模型。当数据具有强烈的季节性效应并且有足够的历史趋势时，该库表现最佳。
- en: 'Let''s see how we can use it:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用它：
- en: '[PRE62]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here are our model forecasts:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的模型预测：
- en: '![](img/1abc025e-b3cb-4f9c-a7e9-dfd148ef05db.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1abc025e-b3cb-4f9c-a7e9-dfd148ef05db.png)'
- en: 'We get a similar decomposition as before with the ARIMA/SARIMA models, namely,
    the trend and the seasonal components:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到与ARIMA/SARIMA模型相似的分解，即趋势和季节性组成部分：
- en: '![](img/746fb1c2-b7c0-4737-98b6-7c389548db42.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](img/746fb1c2-b7c0-4737-98b6-7c389548db42.png)'
- en: The yearly variation nicely shows the rise and fall of the CO[2] concentration
    according to the seasons. The trend clearly goes up over time, which could be
    worrisome if you think about global warming.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 每年的变化明显显示了二氧化碳浓度随季节变化的升降。趋势随时间显著上升，这可能在考虑全球变暖时令人担忧。
- en: See also
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'We''ve used the following libraries in this recipe:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个示例中使用了以下库：
- en: 'Statsmodels: [http://statsmodels.sourceforge.net/stable/](http://statsmodels.sourceforge.net/stable/)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Statsmodels：[http://statsmodels.sourceforge.net/stable/](http://statsmodels.sourceforge.net/stable/)
- en: 'Prophet: [https://facebook.github.io/prophet/](https://facebook.github.io/prophet/)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prophet：[https://facebook.github.io/prophet/](https://facebook.github.io/prophet/)
- en: 'There are many more interesting libraries relating to time series, including
    the following:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多与时间序列相关的有趣库，包括以下内容：
- en: 'Time series modeling using state space models in statsmodels:'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在statsmodels中使用状态空间模型进行时间序列建模：
- en: '[https://www.statsmodels.org/dev/statespace.html](https://www.statsmodels.org/dev/statespace.html)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.statsmodels.org/dev/statespace.html](https://www.statsmodels.org/dev/statespace.html)'
- en: 'GluonTS: Probabilistic Time Series Models in MXNet (Python): [https://gluon-ts.mxnet.io/](https://gluon-ts.mxnet.io/)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GluonTS：MXNet中的概率时间序列模型（Python）：[https://gluon-ts.mxnet.io/](https://gluon-ts.mxnet.io/)
- en: 'SkTime: A unified interface for time series modeling: [https://github.com/alan-turing-institute/sktime](https://github.com/alan-turing-institute/sktime)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SkTime：时间序列建模的统一接口：[https://github.com/alan-turing-institute/sktime](https://github.com/alan-turing-institute/sktime)
