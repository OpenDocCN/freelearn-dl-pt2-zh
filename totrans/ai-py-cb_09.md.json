["```py\n!pip install librosa\n```", "```py\n!pip install wget\n```", "```py\nimport os\nimport wget\nimport tarfile\n\nDATA_DIR = 'sound_commands'\nDATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'\nARCHIVE = os.path.basename(DATASET_URL)\nos.mkdir(DATA_DIR)\nos.chdir(DATA_DIR)\nwget.download(DATASET_URL)\nwith tarfile.open(ARCHIVE, 'r:gz') as tar:\n  tar.extractall(path='data/train')\nos.remove(ARCHIVE)\n```", "```py\n_background_noise_  five     marvin        right             tree\nbed                 four     nine       seven             two\nbird                go       no         sheila            up\ncat                 happy    off        six               validation_list.txt\ndog                 house    on         stop              wow\ndown                left     one        testing_list.txt  yes\neight               LICENSE  README.md  three             zero\n```", "```py\nimport librosa\nx, sr = librosa.load('data/train/bed/58df33b5_nohash_0.wav')\n```", "```py\nimport IPython.display as ipd\nipd.Audio(x, rate=sr)\n```", "```py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr, alpha=0.8)\n```", "```py\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()\n```", "```py\nfrom tqdm.notebook import tqdm\n\ndef vectorize_directory(dirpath, label=0):\n features = []\n  labels = [0]\n  files = os.listdir(dirpath)\n  for filename in tqdm(files):\n    x, _ = librosa.load(\n        os.path.join(dirpath, filename)\n    )\n    if len(x) == 22050:\n      features.append(x)\n  return features, [label] * len(features)\n\nfeatures, labels = vectorize_directory('data/train/bed/')\nf, l = vectorize_directory('data/train/bird/', 1)\nfeatures.extend(f)\nlabels.extend(l)\nf, l = vectorize_directory('data/train/tree/', 2)\nfeatures.extend(f)\nlabels.extend(l)\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfeatures = np.concatenate([f.reshape(1, -1) for f in features], axis=0)\nlabels = np.array(labels)\nX_train, X_test, y_train, y_test = train_test_split(\n features, labels, test_size=0.33, random_state=42\n)\n```", "```py\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\n\ndef preprocess(x):\n    x = (x + 0.8) / 7.0\n    x = K.clip(x, -5, 5)\n    return x\n\nPreprocess = Lambda(preprocess)\n```", "```py\ndef relu6(x):\n    return K.relu(x, max_value=6)\n\ndef conv_layer(x, num_filters=100, k=3, strides=2):\n    x = Conv1D(\n          num_filters,\n          (k),\n          padding='valid',\n          use_bias=False,\n          kernel_regularizer=l2(1e-6)\n        )(x)\n    x = BatchNormalization()(x)\n    x = Activation(relu6)(x)\n    x = MaxPool1D(pool_size=num_filters, strides=None, padding='valid')(x)\n    return x\n\ndef create_model(classes, nlayers=1, filters=100, k=100):\n    input_layer = Input(shape=[features.shape[1]])\n    x = Preprocess(input_layer)\n    x = Reshape([features.shape[1], 1])(x)\n    for _ in range(nlayers):\n        x = conv_layer(x, num_filters=filters, k=k)\n        x = Reshape([219 * filters])(x)\n        x = Dense(\n            units=len(classes), activation='softmax',\n            kernel_regularizer=l2(1e-2)\n        )(x)\n    model = Model(input_layer, x, name='conv1d_sound')\n    model.compile(\n        optimizer=keras.optimizers.Adam(lr=3e-4),\n        loss=keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[keras.metrics.sparse_categorical_accuracy])\n    model.summary()\n    return model\n\nmodel = create_model(classes\n```", "```py\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_46 (InputLayer)        [(None, 22050)]           0         \n_________________________________________________________________\nlambda_44 (Lambda)           (None, 22050)             0         \n_________________________________________________________________\nreshape_86 (Reshape)         (None, 22050, 1)          0         \n_________________________________________________________________\nconv1d_56 (Conv1D)           (None, 21951, 100)        10000     \n_________________________________________________________________\nbatch_normalization_43 (Batc (None, 21951, 100)        400       \n_________________________________________________________________\nactivation_43 (Activation)   (None, 21951, 100)        0         \n_________________________________________________________________\nmax_pooling1d_29 (MaxPooling (None, 219, 100)          0         \n_________________________________________________________________\nreshape_87 (Reshape)         (None, 21900)             0         \n_________________________________________________________________\ndense_33 (Dense)             (None, 3)                 65703     \n=================================================================\nTotal params: 76,103\nTrainable params: 75,903\nNon-trainable params: 200\n_________________________________________________________________\n```", "```py\nimport sklearn\n\nmodel.fit(X_train, y_train, epochs=30)\npredicted = model.predict(X_test)\nprint('accuracy: {:.3f}'.format(\n    sklearn.metrics.accuracy_score(y_test, predicted.argmax(axis=1))\n))\n```", "```py\n!pip install wget\n```", "```py\nfrom os.path import exists\n\nif not exists('pytorch-dc-tts'):\n !git clone --quiet https://github.com/tugstugi/pytorch-dc-tts\n\n!pip install --ignore-installed librosa\n```", "```py\nimport wget\n\nif not exists('ljspeech-text2mel.pth'):\n    wget.download(      'https://www.dropbox.com/s/4t13ugxzzgnocbj/step-300K.pth',\n        'ljspeech-text2mel.pth'\n    )\n\nif not exists('ljspeech-ssrn.pth'):\n    wget.download(\n   'https://www.dropbox.com/s/gw4aqrgcvccmg0g/step-100K.pth',\n        'ljspeech-ssrn.pth'\n    )\n```", "```py\nimport sys\nsys.path.append('pytorch-dc-tts')\nimport numpy as np\nimport torch\nimport IPython\nfrom IPython.display import Audio\nfrom hparams import HParams as hp\nfrom audio import save_to_wav\nfrom models import Text2Mel, SSRN\nfrom datasets.lj_speech import vocab, idx2char, get_test_data\n```", "```py\ntorch.set_grad_enabled(False)\ntext2mel = Text2Mel(vocab)\ntext2mel.load_state_dict(torch.load('ljspeech-text2mel.pth').state_dict())\ntext2mel = text2mel.eval()\nssrn = SSRN()\nssrn.load_state_dict(torch.load('ljspeech-ssrn.pth').state_dict())\nssrn = ssrn.eval()\n```", "```py\nSENTENCES = [\n 'The horse raced past the barn fell.',\n 'The old man the boat.',\n 'The florist sent the flowers was pleased.',\n 'The cotton clothing is made of grows in Mississippi.',\n 'The sour drink from the ocean.',\n 'Have the students who failed the exam take the supplementary.',\n 'We painted the wall with cracks.',\n 'The girl told the story cried.',\n 'The raft floated down the river sank.',\n 'Fat people eat accumulates.'\n]\n```", "```py\nfor i in range(len(SENTENCES)):    \n    sentences = [SENTENCES[i]]\n    max_N = len(sentences[0])\n    L = torch.from_numpy(get_test_data(sentences, max_N))\n    zeros = torch.from_numpy(np.zeros((1, hp.n_mels, 1), np.float32))\n    Y = zeros\n    A = None\n\n    for t in range(hp.max_T):\n      _, Y_t, A = text2mel(L, Y, monotonic_attention=True)\n      Y = torch.cat((zeros, Y_t), -1)\n      _, attention = torch.max(A[0, :, -1], 0)\n      attention = attention.item()\n      if L[0, attention] == vocab.index('E'): # EOS\n          break\n\n    _, Z = ssrn(Y)\n    Z = Z.cpu().detach().numpy()\n    save_to_wav(Z[0, :, :].T, '%d.wav' % (i + 1))\n    IPython.display.display(Audio('%d.wav' % (i + 1), rate=hp.sr))\n```", "```py\nimport wget\n\nwget.download(\n  'https://s3.amazonaws.com/wavegan-v1/models/timit.ckpt.index',\n  'model.ckpt.index'\n)\nwget.download(\n  'https://s3.amazonaws.com/wavegan-v1/models/timit.ckpt.data-00000-of-00001',\n  'model.ckpt.data-00000-of-00001')\nwget.download(\n  'https://s3.amazonaws.com/wavegan-v1/models/timit_infer.meta',\n  'infer.meta'\n);\n```", "```py\nimport tensorflow as tf\n\ntf.reset_default_graph()\nsaver = tf.train.import_meta_graph('infer.meta')\ngraph = tf.get_default_graph()\nsess = tf.InteractiveSession()\nsaver.restore(sess, 'model.ckpt');\n```", "```py\nimport numpy as np\nimport PIL.Image\nfrom IPython.display import display, Audio\nimport time as time\n\n_z = (np.random.rand(2, 100) * 2.) - 1.\nz = graph.get_tensor_by_name('z:0')\nG_z = graph.get_tensor_by_name('G_z:0')[:, :, 0]\nG_z_spec = graph.get_tensor_by_name('G_z_spec:0')\n\nstart = time.time()\n_G_z, _G_z_spec = sess.run([G_z, G_z_spec], {z: _z})\nprint('Finished! (Took {} seconds)'.format(time.time() - start))\n\nfor i in range(2):\n    display(Audio(_G_z[i], rate=16000))\n```", "```py\n!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n!pip install -qU pyfluidsynth pretty_midi\n!pip install -qU magenta\n```", "```py\nimport ctypes.util\norig_ctypes_util_find_library = ctypes.util.find_library\ndef proxy_find_library(lib):\n  if lib == 'fluidsynth':\n    return 'libfluidsynth.so.1'\n  else:\n    return orig_ctypes_util_find_library(lib)\nctypes.util.find_library = proxy_find_library\n```", "```py\nfrom note_seq.protobuf import music_pb2\n\ntwinkle_twinkle = music_pb2.NoteSequence()\ntwinkle_twinkle.notes.add(pitch=60, start_time=0.0, end_time=0.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=60, start_time=0.5, end_time=1.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=67, start_time=1.0, end_time=1.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=67, start_time=1.5, end_time=2.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=69, start_time=2.0, end_time=2.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=69, start_time=2.5, end_time=3.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=67, start_time=3.0, end_time=4.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=65, start_time=4.0, end_time=4.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=65, start_time=4.5, end_time=5.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=64, start_time=5.0, end_time=5.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=64, start_time=5.5, end_time=6.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=62, start_time=6.0, end_time=6.5, velocity=80)\ntwinkle_twinkle.notes.add(pitch=62, start_time=6.5, end_time=7.0, velocity=80)\ntwinkle_twinkle.notes.add(pitch=60, start_time=7.0, end_time=8.0, velocity=80) \ntwinkle_twinkle.total_time = 8\ntwinkle_twinkle.tempos.add(qpm=60);\n```", "```py\nimport note_seq\n\nnote_seq.plot_sequence(twinkle_twinkle)\nnote_seq.play_sequence(twinkle_twinkle,synth=note_seq.fluidsynth)\n```", "```py\nfrom magenta.models.melody_rnn import melody_rnn_sequence_generator\nfrom magenta.models.shared import sequence_generator_bundle\nfrom note_seq.protobuf import generator_pb2\nfrom note_seq.protobuf import music_pb2\n\nnote_seq.notebook_utils.download_bundle('attention_rnn.mag', '/content/')\nbundle = sequence_generator_bundle.read_bundle_file('/content/basic_rnn.mag')\ngenerator_map = melody_rnn_sequence_generator.get_generator_map()\nmelody_rnn = generator_map['basic_rnn'](checkpoint=None, bundle=bundle)\nmelody_rnn.initialize()\n```", "```py\ndef get_options(input_sequence, num_steps=128, temperature=1.0):\n    last_end_time = (max(n.end_time for n in input_sequence.notes)\n                      if input_sequence.notes else 0)\n    qpm = input_sequence.tempos[0].qpm \n    seconds_per_step = 60.0 / qpm / melody_rnn.steps_per_quarter\n    total_seconds = num_steps * seconds_per_step\n\n    generator_options = generator_pb2.GeneratorOptions()\n    generator_options.args['temperature'].float_value = temperature\n    generate_section = generator_options.generate_sections.add(\n      start_time=last_end_time + seconds_per_step,\n      end_time=total_seconds)\n    return generator_options\n\nsequence = melody_rnn.generate(input_sequence, get_options(twinkle_twinkle))\n```", "```py\nnote_seq.plot_sequence(sequence)\nnote_seq.play_sequence(sequence, synth=note_seq.fluidsynth)\n```", "```py\nnote_seq.sequence_proto_to_midi_file(sequence, 'twinkle_continued.mid')\n```", "```py\nfrom google.colab import files\nfiles.download('twinkle_continued.mid')\n```"]