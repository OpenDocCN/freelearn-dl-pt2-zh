- en: Machine Learning for IoT
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning has dramatically altered what manufacturers are able to do
    with IoT. Today, there are numerous industries that have specific IoT needs. For
    example, the **internet of medical things** (**IoMT**) has devices such as outpatient
    heart monitors that can be worn at home. These devices often require large amounts
    of data to be sent over the network or large compute capacity on the edge to process
    heart-related events. Another example is **agricultural IoT** (**AIoT**) devices
    that are often placed in locations where there is no Wi-Fi or cellular network.
    Prescriptions or models are pushed down to these semi-connected devices. Many
    of these devices require that decisions be made on the edge. When connectivity
    is finally established using technology such as LoRAWAN or TV, white space models
    are downloaded to the devices.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to discuss using machine learning models such
    as logistic regression and decision trees to solve common IoT issues such as classifying
    medical results, detecting unsafe drivers, and classifying chemical readings.
    We are also going to look at techniques for working with constrained devices,
    and we are going to look at using unsupervised learning to gain insights on devices
    with little data, such as prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing chemical sensors with anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression with IoMT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying chemical sensors with decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple predictive maintenance with XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting unsafe drivers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection on constrained devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing chemical sensors with anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accurate predictive models require a large number of devices in the field to
    have failed so that they have enough fail data to use for predictions. For some
    well-crafted industrial devices, failures on this scale can take years. Anomaly
    detection can identify devices that are not behaving like the other devices in
    the fleet. It can also be used to wade through thousands of similar messages and
    pinpoint the messages that are not like the others.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection in machine learning can be **unsupervised**, **supervised**,
    or **semi-supervised**. Usually, it starts by using an unsupervised machine learning
    algorithm to cluster data into patterns of behavior or groups. This presents a
    series of data in buckets. When the machines are examined, some of the buckets
    identify behavior while some identify an issue with the device. The device may
    have exhibited different patterns of behavior in a resting state, an in-use state,
    a cold state, or something that represents a state that needs to be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe presumes the use of a dataset where not much is known about the
    data. The process of anomaly detection is used as part of the discovery process
    and is often used with prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomaly detection is one of the easiest machine learning models to implement.
    In this recipe, we are going to use a dataset drawn from chemical sensors that
    are detecting either neutral, banana, or wine. To get ready, you will need to
    import the `numpy`, `sklearn` and `matplotlib` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps need to be observed to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload the data file to a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'View the dataset to see if the grouping of data correlates to the number of
    clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/544d4537-bf2d-494c-8706-9b2ba46bddf3.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding chart shows three different groups of data. Tight clusters represent
    data with well-defined boundaries. If we adjust the number of clusters to `10`,
    we may be able to get better separation of different groups. These cluster segments
    help us identify different segments of data. This, in turn, may help us determine
    optimal sensor placement for a prototype or perform feature engineering in a machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are using `numpy` for data manipulation, `sklearn` for the
    machine learning algorithm, and `matplotlib` for viewing the results. Next, we
    pull the tab-separated file into a Spark dataframe. In this step, we convert the
    data into a pandas DataFrame. Then we run the k-means algorithm with three clusters,
    which gives the chart as the output.
  prefs: []
  type: TYPE_NORMAL
- en: K-means is an algorithm that helps group data into clusters. K-means is a popular
    clustering algorithm for examining data without labels. K-means first randomly
    initializes cluster centroids. In our example, it had three cluster centroids.
    It then assigns the centroids to the nearest data points. Next, it moves each
    centroid to the spot that is in the middle of its respective cluster. It repeats
    these steps until it achieves an appropriate division of data points.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the chart, you may have noticed outliers. These are very important to note
    when looking at prototypes. Outliers can represent power fluctuations within a
    machine, bad sensor placement, or a number of other issues. The following example
    shows a simple standard deviation calculation on our data. From here, we are able
    to see two values that fall outside three standard deviations from the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression with the IoMT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we're going to talk about using logistic regression to classify
    data from mammography machines. Recently, the IoMT has expanded greatly. Many
    devices are being worn by patients when they go home from their doctor, providing
    an in-home medical monitoring solution, while others are in hospitals, giving
    the doctors additional feedback on medical tests being run. In many cases, machine
    learning algorithms are able to spot diseases and issues that doctors may miss,
    or give them additional recommendations. In this recipe, we are going to work
    with a breast cancer dataset and determine whether a mammogram record is malignant
    or benign.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset, along with the Databricks notebooks, is available in the GitHub
    repository. The dataset is unwieldy. It has bad columns with a high degree of
    correlation, which is another way of saying some sensors are duplicates, and there
    are unused columns and extraneous data. For the sake of readability, there will
    be two notebooks in the GitHub repository. The first does all of the data manipulation
    and puts the data into a data table. The second notebook does the machine learning.
    We will focus this recipe on the data manipulation notebook. At the end of the
    recipe, we will talk about two other notebooks to show an example of MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: One other thing you will need in this recipe is an MLflow workspace. To set
    up an MLflow workspace, you will need to go into Databricks and create the workspace
    for this experiment. We will write the results of our experiment there.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Test our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the `precision`, `recall`, and `f1-score` of malignant (`M`)
    and benign (`B`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/172496b3-10f8-4f30-90d2-17161861a437.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Evaluate the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1a59574-ec79-4f3b-9000-7e11e38b0a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The results show that out of 171 records in our testing set, 112 were true
    negatives and 49 were true positives, meaning that out of 171 records it was able
    to correctly identify 161 records. 10 of those predictions were wrong: 5 false
    negatives and 5 false positives.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used logistic regression. Logistic regression is a technique
    that can be used for traditional statistics as well as machine learning. Due to
    its simplicity and power, many data scientists use logistic regression as their
    first model and use it as a benchmark to beat. Logistic regression is a binary
    classifier, meaning it can classify something as `true` or `false`. In our case,
    the classifications are benign or malignant.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import `koalas` for data manipulation and `sklearn` for our model
    and analysis. Next, we import data from our data table and put it into a Pandas
    DataFrame. Then we split the data into testing and training datasets. Next, we
    create a formula that will describe for the model the data columns being used.
    Next, we give the model the formula, the training dataset, and the algorithm it
    will use. We then output a model that we can use to evaluate new data. We now
    create a DataFrame called `predictions_nominal`, which we can use to compare against
    our testing results dataset. The classification report gives us `precision`, `recall`,
    and `f1-score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: The ratio of correctly reported positive predictions to the
    expected positive predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: The ratio of correctly reported positive predictions compared to
    the total population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-score**: A blended score of precision and recall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we can look at the results of the model and determine how accurately
    it predicted the real values. Some factors that we will examine are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Negatives: **The predicted negatives that were actually negative in
    the test set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives**: The number that the trained model predicted would be positive
    in the training set but were not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives**: The number of false negatives predicted in the test set
    that were actually positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Positives**: The amount the model actually got correct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will record the outcome in MLflow to be compared against other algorithms.
    We will also save other parameters, such as the main formula used and the family
    of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Classifying chemical sensors with decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to use chemical sensor data from **Metal-Oxide**
    (**MOx**) sensors to determine whether there is wine in the air. This type of
    sensor is commonly used to determine whether food or chemical particulates are
    in the air. Chemical sensors can detect gasses that would be poisonous to people
    or food spillage at a warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Encode the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Test/train the split data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Train and predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, we import the libraries we need for this project. Next, we import
    data from our Spark data table into a Pandas DataFrame. One-hot encoding can change
    categorical values, such as our example of *Wine* and *No Wine*, into encoded
    values that machine learning algorithms can use better. In *step 4*, we take our
    feature columns and our one-hot encoded column and perform a split, splitting
    them into a testing and training set. In *step 5*, we create a decision tree classifier,
    use the `X_train` and `y_train` data to train the model, and then use the `X_test` data
    to create a `y_prediction` dataset. In other words, in the end, we will have a
    set of predictions called `y_pred` based on the predictions the dataset had on
    the `X_test` set. In *step 6*, we evaluate the accuracy of the model and the **area
    under the curve** (**AUC**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree classifiers are used when the data is complex. In the same way,
    you can use a decision tree to follow a set of logical rules using yes/no questions,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94bdd679-d272-412c-bdc5-6cca165ac814.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A machine learning algorithm can train a decision tree model to use numeric
    data, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5d63031-bd06-4991-8043-f43b70f72d32.png)'
  prefs: []
  type: TYPE_IMG
- en: The machine learning algorithm trains the model to accurately pick the best
    path given the available data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `sklearn` decision tree classifier has two hyperparameters that we can
    tune: **criterion** and **max depth**. Hyperparameters are often changed to see
    if accuracy can be increased. The criterion is either gini or entropy. Both of
    these criteria evaluate impurities in the child nodes. The next one is max depth.
    The max depth of the decision tree can affect over- and underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting versus overfitting**'
  prefs: []
  type: TYPE_NORMAL
- en: Models that underfit are inaccurate and poorly represent the data they were
    trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Models that overfit are unable to generalize from the data trained on. It misses
    similar data to the training set because it only works on exactly the same data
    it was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Simple predictive maintenance with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every device has an end of life or will require maintenance from time to time.
    Predictive maintenance is one of the most commonly used machine learning algorithms
    in IoT. The next chapter will cover predictive maintenance in depth, looking at
    sequential data and how that data changes with seasonality. This recipe will look
    at predictive maintenance from the simpler perspective of classification.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to use the NASA *Turbofan engine degradation simulation*
    dataset. We are going to be looking at having three classifications. Green means
    the engine does not need maintenance; yellow, the engine needs maintenance within
    the next 14 maintenance cycles; or red, the engine needs maintenance within the
    next cycle. For an algorithm, we are going to use **extreme gradient boosting**
    (**XGBoost**). XGBoost has become popular in recent years because it tends to
    win more Kaggle competitions than other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get ready you will need the NASA *Turbofan engine degradation simulation*
    dataset. The data, along with a Spark notebook, can be found in the companion
    GitHub repository for this book or on the NASA website. Next, you will need to
    make sure you install XGBoost as a library in Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a table view on the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Test, train, and split the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we import `pandas`, `pyspark`, and `numpy` for data wrangling, `xgboost`
    for our algorithm, `sklearn` for scoring our results, and finally `mlflow` and
    `pickle` for saving those results. In *step 2*, we specify a schema in Spark.
    The inferred schema feature of Databricks can often get the schema wrong. Often
    we need to specify data types. In the next step, we create a temp view of the
    data so that we can use the SQL tools in Databricks. In *step 4*, we use the magic `%sql`
    tag at the top of the page to change the language to SQL. We then create a table
    call, `engine`, that has the engine data plus a new column that gives `0` if the
    engine has more than 14 cycles left, `1` if it has only one cycle left, and `2`
    if it has 14 cycles left. We then switch back to the default Python language and
    split the data into test and training datasets. In *step 6*, we specify the columns
    in the model as well as the hyperparameters. From here we train the model. We
    then test our model and print the precision score. Next, we will store the results
    in MLflow. In [Chapter 4](a40f2c07-0e51-46c6-a3cf-66d5c46477c4.xhtml), *Deep Learning
    for Predictive Maintenance*, we will conduct other experiments against this dataset
    to see which one performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has a large number of parameters you can tune. These tuning parameters
    can be the number of threads that algorithms are allowed to use, and tree parameters
    that help increase accuracy or prevent over and underfitting. Some of these include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate is the step size of the algorithm to update
    its nodes. It helps prevent overfitting but can also negatively affect how long
    it takes to complete the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: A deep tree tends to overfit and a shallow tree tends to underfit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictor`: This is a flag to tell the program to do its computations on either
    the CPU or GPU. GPUs can dramatically increase performance time but not all computers
    have a GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a dozen more parameters that can be tuned in XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoosted decision trees under the hood take weak learners or shallow trees
    and combine them into a strong learner using a dimension scoring system. It is
    similar to getting a bad diagnosis from a doctor and getting a second and third
    opinion. The first doctor could be wrong but it is less likely that all three
    doctors are wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting unsafe drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision in machine learning has allowed us to tell if there are accidents
    on roads or unsafe work environments and can be used in conjunction with complex
    systems such as smart sales assistants. Computer vision has opened up many possibilities
    in IoT. Computer vision is also one of the most challenging from a cost perspective.
    In the next two recipes, we are going to discuss two different ways of using computer
    vision. The first one takes in large amounts of images generated from IoT devices
    and performs predictions and analysis on them using the high-performance distributed
    Databricks format. In the next recipe, we are going to use a technique for performing
    machine learning on edge devices with a small amount of compute using a low compute
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get ready, you will need Databricks. In the example of this recipe, we are
    going to pull images from Azure Blob Storage.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries and configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Query the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create testing and training datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Record the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we are defining where the files are located. For this recipe, we are
    using Azure Blob Storage, but any storage system, such as S3 or HDFS, would work
    as well. Replace the `storage_account_name` and `storage_account_access_key` fields
    with the keys of your Blob Storage account. Read both *safe* and *unsafe* images
    in from our storage account into a Spark image DataFrame. In our example, we have
    placed safe images in one folder and unsafe images in another. Query the image
    DataFrame to see if it got the images. Create safe and unsafe test and training
    sets. We then union our datasets into a training set and a testing set. Next,
    we create a machine learning pipeline. We use the ResNet-50 algorithm as a featurizer.
    Next, we use logistic regression as our classifier. We then put it into a pipeline
    and train our model. Next, we take our pipeline and run our training DataFrame
    through it to come out with a trained model. We then evaluate the accuracy of
    our model. Finally, we store the results in MLflow so that we can compare it against
    other models.
  prefs: []
  type: TYPE_NORMAL
- en: There are many image classification models that have been developed, such as
    ResNet-50 and Inception v3\. In our example, we used ResNet-50, which is a type
    of tuned convolutional neural network. ResNet-50 is a powerful machine learning
    model for image featurization. In machine learning, there is the *no free lunch
    theorem*, which states that no one model will outperform the rest. For this reason,
    data scientists will test different algorithms. This can simply be done by changing
    a parameter such as a metric name.
  prefs: []
  type: TYPE_NORMAL
- en: We also used a Spark ML pipeline. Pipelines allow data scientists to declare
    different steps of a process and implement them independently. In our case, we
    used ResNet-50 to featurize the image. ResNet-50 outputs a vector of features
    that can be classified by a classifier. In our case, we use logistic regression,
    but we could have used XGBoost or a different neural network.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To change our pipeline to use Inception instead of `ResNet50`, we simply need
    to change the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `Inception v3`, we are able to test the accuracy of different models
    on the image set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We could use an array of models and record the results in MLflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Face detection on constrained devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks tend to outperform other classification techniques. However,
    with IoT devices, there is not a large amount of RAM, compute, or storage. On
    constrained devices, RAM and storage are often in MB and not in GB, making traditional
    classifiers not possible. Some video classification services in the cloud charge
    over $10,000 per device for live streaming video. OpenCV's Haar classifiers have
    the same underlying principles as a convolutional neural network but at a fraction
    of the compute and storage. OpenCV is available in multiple languages and runs
    on some of the most constrained devices.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to set up a Haar Cascade to detect if a person
    is close to the camera. This is often used in Kiosk and other interactive smart
    devices. The Haar Cascade can be run at a high rate of speed and when it finds
    a face that is close to the machine it can send that image via a cloud service
    or a different onboard machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we need to do is install the OpenCV framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Next, we download the model. The model can be downloaded from the OpenCV GitHub
    page or the book's GitHub page. The file is `haarcascade_frontalface_default.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a new folder by importing the `haarcascade_frontalface_default.xml`
    file and creating a Python file for the code. Finally, if the device does not
    have a camera attached to it, attach one. In the following recipe, we are going
    to implement a Haar Cascade using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries and settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Capture and transform the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Classify the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Debug the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Detect the face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, import the libraries and set the settings. In the next step, we import
    the `opencv` and `python` libraries and we also import `time` so we can wait if
    the camera is not ready. Next, we set some debugging flags so we can test the
    output visually if we are debugging. Then we import the Haar Cascade XML file
    into our classifier. Finally, we open the first video camera attached to the machine.
    In *step 2*, we wait for the camera to become ready. This is often not a problem
    when developing the software as the system has already recognized the camera.
    Then we set this program to run automatically; the camera may not be available
    for up to a minute when the system is restarted. We are also starting an infinite
    loop of processing the camera images. In the next step, we capture and transform
    the image into black and white. Next, we run the classifier. The `detectMultiScale`
    classifier allows faces of different sizes to be detected. The `minNeighbors` parameter
    specifies how many collaborating neighbors the detection needs before detecting
    a face. Making the `minNeighbors` parameter small could result in a false positive.
    Setting it too large may not detect a face at all. Finally, there is the minimum
    size in pixels that the face needs to be. To debug the code and make sure the
    camera is working accurately we have put in some debug code that outputs the video
    and a bounding box to the attached monitor. On a deployed device, this adds a
    considerable load. But for testing, this can reveal issues and allow tuning. If
    a face has been detected, then you are ready to perform tasks such as onboard
    sentiment analysis or send it to an external service such as the Azure Face API
    to identify people through a face ID.
  prefs: []
  type: TYPE_NORMAL
- en: A Haar Cascade is a highly efficient face detection classifier. Under the hood,
    it takes rectangular sections of the image and compares them against another part
    of the image to come up with something that has the characteristics of a face.
    In our recipe, we used the camera on the device, transformed it, and then used
    the Haar Cascade to classify it.
  prefs: []
  type: TYPE_NORMAL
