- en: Deep Learning for Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml), *Diving Deep into
    Neural Networks,* we built an image classifier using a popular **convolutional
    neural network** (**CNN**) architecture called **ResNet**, but we used this model
    as a black box. In this chapter, we will explore how we can build an architecture
    from scratch to solve image classification problems, which are the most common
    use cases. We will also learn how to use transfer learning, which will help us
    build image classifiers using a very small dataset. Apart from learning how to
    use CNNs, we will also explore what these convolutional networks learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the important building blocks of convolutional
    networks. Some of the important topics that we will be covering in this chapter
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN model from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and exploring a VGG16 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating pre-convoluted features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding what a CNN model learns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the weights of the CNN layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last few years, CNNs have become popular in image recognition, object
    detection, segmentation, and many other areas in the field of computer vision.
    They are also becoming popular in the field of **natural language processing**
    (**NLP**), though they are not commonly used yet. The fundamental difference between
    fully connected layers and convolution layers is the way the weights are connected
    to each other in the intermediate layers. Let''s take a look at the following
    diagram, which shows how fully connected, or linear, layers work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25c3a354-5f47-40df-a2ba-813d044132f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the biggest challenges of using a linear layer or fully connected layers
    for computer vision is that they lose all spatial information, and the complexity
    in terms of the number of weights that are used by fully connected layers is too
    big. For example, when we represent a 224-pixel image as a flat array, we would
    end up with 150, 528 (224 x 224 x 3 channels). When the image is flattened, we
    lose all the spatial information. Let''s look at what a simplified version of
    a CNN looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7865455-c3d0-4191-896f-d970ce006af4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All the convolution layer is doing is applying a window of weights called **filters**
    across the image. Before we try to understand convolutions and other building
    blocks in detail, let''s build a simple yet powerful image classifier for the
    MNIST dataset. Once we''ve built this, we will walk through each component of
    the network. We will break down building our image classifier into the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a validation dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building our CNN model from scratch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MNIST – getting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MNIST dataset contains 60,000 handwritten digits from 0 to 9 for training
    and 10,000 images for a test set. The PyTorch `torchvision` library provides us
    with an MNIST dataset, which downloads the data and provides it in a readily usable
    format. Let''s use the dataset MNIST function to pull the dataset to our local
    machine and then wrap it around `DataLoader`. We will use `torchvision` transformations
    to convert the data into PyTorch tensors and do data normalization. The following
    code takes care of downloading, wrapping data around `DataLoader`, and normalizing
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code provides us with a `DataLoader` variable for the train and
    test datasets. Let''s visualize a few images to get an understanding of what we
    are dealing with. The following code will help us visualize the MNIST images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can pass the `plot_img` method to visualize our dataset. We will pull
    a batch of records from the `DataLoader` variable using the following code and
    plot the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The images can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61281deb-f80b-457e-8974-83367ab71034.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a CNN model from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll build our own architecture from scratch. Our network
    architecture will contain a combination of different layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Conv2d
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MaxPool2d
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectified linear unit** (**ReLU**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at a pictorial representation of the architecture we are going
    to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70e1ba7b-d4dc-4563-af8a-cbd06265bbc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s implement this architecture in PyTorch and then walk through what each
    individual layer does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's understand what each layer does in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Conv2d
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conv2d takes care of applying a convolutional filter on our MNIST images. Let''s
    try to understand how convolution is applied on a one-dimensional array and then
    learn how a two-dimensional convolution is applied to an image. Take a look at
    the following diagram. Here, we will apply a **Conv1d** of a filter (or kernel)
    that''s size 3 to a tensor of length 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74b21364-00fa-46d5-af2a-92ad2317df10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The bottom boxes represent our input tensor of seven values, while the connected
    boxes represent the output after we apply our convolution filter of size three.
    At the top right corner of the image, the three boxes represent the weights and
    parameters of the **Conv1d** layer. The convolution filter is applied like a window
    and it moves to the following values by skipping one value. The number of values
    to be skipped is called the **stride** and is set to 1 by default. Let''s understand
    how the output values are being calculated by writing down the calculation for
    the first and last outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Output 1 –> (*-0.5209 x 0.2286*) + (*-0.0147 x 2.4488*) + (*-0.321 x -0.9498*)
  prefs: []
  type: TYPE_NORMAL
- en: Output 5 –> (*-0.5209 x -0.6791*) + (*-0.0147 x -0.6535*) + (*-0.321 x 0.6437*)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, by now, it should be clear what a convolution does. It applies a filter
    (or kernel), that is, a bunch of weights, on the input by moving it based on the
    value of the stride. In the preceding example, we moved our filter one point at
    a time. If the stride value is 2, then we would move two points at a time. Let''s
    look at a PyTorch implementation of this to understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There is another important parameter, called **padding,** that is often used
    with convolutions. As shown in the previous example, if the filter is not applied
    until the end of the data, that is, when there are not enough elements for the
    data to stride, it stops. Padding prevents this by adding zeros to both ends of
    a tensor. Let''s look at a one-dimensional example of how padding works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04aef5fe-05f4-40b2-b860-4b04ad55278d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we applied a **Conv1d** layer with padding 2 and stride
    1\. Let's look at how Conv2d works on an image.
  prefs: []
  type: TYPE_NORMAL
- en: Before we understand how Conv2d works, I would strongly recommend that you check
    out an amazing blog ([http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/))
    that contains a live demo of how convolution works. After you have spent a few
    minutes playing with the demo, continue reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand what happened in the demo. In the center box of the image,
    we have two different sets of numbers: one represented in the boxes and the other
    beneath the boxes. The ones represented in the boxes are pixel values, as highlighted
    by the white box on the left-hand photo in the demo. The numbers denoted beneath
    the boxes are the filter (or kernel) values that are being used to sharpen the
    image. The numbers are handpicked to do a particular job. In this case, they are
    sharpening the image. Just like in our previous example, we are doing an element-to-element
    multiplication and summing up all the values to generate the value of the pixel
    in the right-hand image. The generated value is highlighted by the white box on
    the right- hand side of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the values in the kernel are handpicked in this example, in CNNs, we
    do not handpick the values; instead, we initialize them randomly and let gradient
    descent and backpropagation tune the values of the kernels. The learned kernels
    will be responsible for identifying different features, such as lines, curves,
    and eyes. Take a look at the following screenshot, where we can see a matrix of
    numbers and see how convolution works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d54b8108-1fd5-4b42-8c4c-c7d371f0cc5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we assume that the 6 x 6 matrix represents an
    image and we apply the convolution filter of size 3 x 3\. Then, we show how the
    output is generated. To keep it simple, we are just calculating for the highlighted
    portion of the matrix. The output is generated by doing the following calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: Output –> *0.86 x 0 + -0.92 x 0 + -0.61 x 1 + -0.32 x -1 + -1.69 x -1 + ........*
  prefs: []
  type: TYPE_NORMAL
- en: The other important parameter that's used in the Conv2d function is `kernel_size`,
    which decides the size of the kernel. Some of the commonly used kernel sizes are
    *1*, *3*, *5*, and *7*. The larger the kernel's size, the larger the area that
    a filter can cover, so it is common to observe filters of *7* or *9* being applied
    to the input data in the early layers.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a common practice to add pooling layers after convolution layers since
    they reduce the size of feature maps and the outcomes of convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling offers two different features: one is reducing the size of the data
    to process and the other is forcing the algorithm to not focus on small changes
    in the position of an image. For example, a face detection algorithm should be
    able to detect a face in the picture, irrespective of the position of the face
    in the photo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how MaxPool2d works. It also uses the same concept of kernel
    size and strides. It differs from convolutions as it does not have any weights
    and just acts on the data generated by each filter from the previous layer. If
    the kernel size is *2 x 2,* then it considers that size in the image and picks
    the maximum of that area. Let''s look at the following diagram, which will make
    it clear how MaxPool2d works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95f76a92-ac50-401a-bf5d-5500e09e1509.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The box on the left-hand side contains the values of feature maps. After applying
    max pooling, the output is stored on the right-hand side of the box. Let''s look
    at how the output is calculated by writing down the calculation for the values
    in the first row of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7285839-6b38-4e1c-9901-097262274765.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The other commonly used pooling technique is **average pooling**. The maximum
    function is replaced with the average function. The following diagram explains
    how average pooling works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af03c571-cf8b-48a6-8159-23132edc61d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, instead of taking a maximum of four values, we are taking
    the average four values. Let''s write down the calculation to make it easier to
    understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5926b79-2ee8-488a-8cf2-073ae4305415.png)'
  prefs: []
  type: TYPE_IMG
- en: Nonlinear activation – ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a common and best practice to have a nonlinear layer after max pooling,
    or after convolution is applied. Most network architectures tend to use ReLU or
    different flavors of ReLU. Whatever nonlinear function we choose, it gets applied
    to each element of the feature map. To make it more intuitive, let''s look at
    an example where we apply ReLU on the same feature map that we applied max pooling
    and average pooling to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b67dbb6c-be51-48f4-a3f0-aa0d557d1165.png)'
  prefs: []
  type: TYPE_IMG
- en: View
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a common practice to use a fully connected, or linear, layer at the end
    of most networks for image classification problems. Here, we are using a two-dimensional
    convolution that takes a matrix of numbers as input, and outputs another matrix
    of numbers. To apply a linear layer, we need to flatten the matrix, which is a
    tensor of two dimensions, into a vector of one dimension. The following diagram
    shows how view works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/843840b3-e32a-4e73-beab-0d371811788b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the code that''s used in our network, which does exactly the
    same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw earlier, the view method will flatten an *n*-dimension tensor into
    a one-dimensional tensor. In our network, the first dimension is of each image.
    The input data after batching will have a dimension of *32 x 1 x 28 x 28,* where
    the first number, *32,* will denote that there are *32* images of size *28* height,
    *28* width, and *1* channel since it is a black and white image. When we flatten,
    we don''t want to flatten or mix the data for different images. So, the first
    argument that we pass to the view function will instruct PyTorch to avoid flattening
    the data on the first dimension. The following diagram shows how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9463a041-acb1-40bb-be9a-67ba4382b0b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we have data of size *2 x 1 x 2 x 2*; after we apply
    the view function, it converts it into a tensor of size *2 x 1 x 4*. Let''s look
    at another example where we don''t mention the *- 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9de8126-872d-4205-af00-77cb095097d7.png)'
  prefs: []
  type: TYPE_IMG
- en: If we ever forget to mention which dimension to flatten, we may end up with
    unexpected results, so be extra careful at this step.
  prefs: []
  type: TYPE_NORMAL
- en: Linear layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we have converted the data from a two-dimensional tensor into a one-dimensional
    tensor, we pass the data through a linear layer, followed by a nonlinear activation
    layer. In our architecture, we have two linear layers, one followed by ReLU and
    the other followed by a `log_softmax` function, which predicts what digit is contained
    in the given image.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the model, we need to follow the same process that we followed for
    our previous dogs and cats image classification problem. The following code snippet
    trains our model on the provided dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This method has different logic for training and validation. There are primarily
    two reasons for using different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: In training mode, dropout removes a percentage of values, which shouldn't happen
    in the validation or testing phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In training mode, we calculate gradients and change the model's parameter value,
    but backpropagation is not required during the testing or validation phases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the code in the previous function is self-explanatory. At the end of
    the function, we return the loss and accuracy of the model for that particular
    epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the model through the preceding function for 20 iterations and plot
    the loss and accuracy of train and validation to understand how our network performed.
    The following code runs the `fit` method for the train and test dataset for 20
    iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code plots the training and test loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47b5dd6a-ec5d-4e0e-8e88-fdc77d63df08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code plots the training and test accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69897521-e770-43fb-a494-52a86fd48428.png)'
  prefs: []
  type: TYPE_IMG
- en: At the end of the 20th epoch, we achieve a test accuracy of 98.9%. We have got
    our simple convolutional model working and almost achieving state of the art results.
    Let's take a look at what happens when we try the same network architecture on
    our dogs versus cats dataset. We will use the data from the previous chapter,
    Chapter 3, *Building Blocks of Neural Networks,* and the architecture from the
    MNIST example with some minor changes. Once we've trained the model, we can evaluate
    it to understand how well our simple architecture performs.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying dogs and cats – CNN from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same architecture but with a few minor changes, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The input dimensions for the first linear layer will need to change, since the
    dimensions for our cat and dog images are *256, 256*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will add another linear layer to allow the model to learn more flexibly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the code that implements the network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will use the same training function that we used for the MNIST example, so
    I'm not going to include the code here. However, let's look at the plots that
    are generated when the model is trained for 20 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss of the training and validation datasets is plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc0cc48b-993d-4894-b15a-1646ad638b5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The accuracy for the training and validation datasets is plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/368de14d-b697-4367-9f35-4272c130ae3f.png)'
  prefs: []
  type: TYPE_IMG
- en: From these plots, it is clear that the training loss is decreasing for every
    iteration, but the validation loss gets worse. Accuracy also increases during
    the training, but almost saturates at 75%. This is a clear example showing that
    the model is not generalizing. In the next section, we will look at another technique
    called **transfer learning**, which helps us train more accurate models and provides
    tricks that we can use to make the training faster.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying dogs and cats using transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning is the ability to reuse a trained algorithm on a similar dataset
    without training it from scratch. We humans don't learn to recognize new images
    by analyzing thousands of similar images. We just understand the different features
    that actually differentiate a particular animal, say, a fox, from a dog. We don't
    need to learn what a fox is from understanding what lines, eyes, and other smaller
    features are like. Therefore, we will learn how to use a pretrained model to build
    state of the art image classifiers with very little data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first few layers of a CNN architecture focus on smaller features, such
    as how a line or curve looks. The filters in the later layers of a CNN learn higher-level
    features, such as eyes and fingers, and the last few layers learn to identify
    the exact category. A pretrained model is an algorithm that is trained on a similar
    dataset. Most of the popular algorithms are pretrained on the popular ImageNet
    dataset to identify 1,000 different categories. Such a pretrained model will have
    filter weights tuned to identify various patterns. So, let''s understand how can
    we take advantage of these pretrained weights. We will look into an algorithm
    called **VGG16**, which was one of the earliest algorithms to find success in
    ImageNet competitions. Though there are more modern algorithms, this algorithm
    is still popular as it is simple to understand and use for transfer learning.
    Let''s take a look at the architecture of the VGG16 model and then try to understand
    the architecture and how we can use it to train our image classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13583159-3f95-4e41-afa9-201400628a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: The VGG16 architecture contains five VGG blocks. A block is a set of convolution
    layers, a nonlinear activation function, and a max-pooling function. All the algorithm
    parameters are tuned to achieve state of the art results when it comes to classifying
    1,000 categories. The algorithm takes input data in the form of batches, which
    are normalized by the mean and standard deviation of the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In transfer learning, we try to capture what the algorithm learns by freezing
    the learned parameters of most of the layers of the architecture. It is often
    good practice to fine-tune only the last layers of the network. In this example,
    we'll train only the last few linear layers and leave the convolutional layers
    intact since the features that are learned by the convolutional features are mostly
    used for all kinds of image problems where the images share similar properties.
    Let's train a VGG16 model using transfer learning to classify dogs and cats. We'll
    walk through the steps to implement this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and exploring a VGG16 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides a set of trained models in its `torchvision` library. Most
    of them accept an argument called `pretrained` when `True`, which downloads the
    weights that have been tuned for the **ImageNet** classification problem. We can
    use the following code to create a VGG16 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have our VGG16 model and all the pretrained weights ready to be used.
    When the code is run for the first time, it could take several minutes, depending
    on your internet speed. The size of the weights could be around 500 MB. We can
    take a quick look at the VGG16 model by printing it. Understanding how these networks
    are implemented turns out to be very useful when we use modern architectures.
    Let''s take a look at the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The model summary contains two sequential models: `features` and `classifiers`.
    The `features` sequential model contains the layers that we are going to freeze.
  prefs: []
  type: TYPE_NORMAL
- en: Freezing the layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's freeze all the layers of the features model, which contains the convolutional
    block. Freezing the weights in the layers will prevent the weights of these convolutional
    blocks from updating. Since the weights of the model are trained to recognize
    a lot of important features, our algorithm will be able to do the same from the
    very first iteration. The ability to use model's weights, which were initially
    trained for a different use case, is called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how we can freeze the weights, or parameters, of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This code prevents the optimizer from updating the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning VGG16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VGG16 model has been trained to classify 1,000 categories, but not trained
    to classify dogs and cats. Therefore, we need to change the output features of
    the last layer from 1,000 to 2\. We can use the following code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vgg.classifier` function gives us access to all the layers inside the
    sequential model, and the sixth element will contain the last layer. When we train
    the VGG16 model, we only need the classifier parameters to be trained. Therefore,
    we only pass `classifier.parameters` to the optimizer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Training the VGG16 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have created the model and optimizer. Since we are using the Dogs
    versus Cats dataset, we can use the same data loaders and the train function to
    train our model. Remember: when we train the model, only the parameters inside
    the classifier change. The following code snippet will train the model for 20
    epochs, thereby reaching a validation accuracy of 98.45%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the training and validation loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64929940-0c34-4bb1-9fad-2e81bb80cf7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize the training and validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba85c972-cae6-4c39-8559-60a335dc3f3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply a couple of tricks, such as data augmentation, and play with different
    values of the dropout to improve the model''s generalization capabilities. The
    following code snippet changes the dropout value in the classifier module of VGG
    to 0.2 from 0.5 and trains the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Training this for a few epochs gave me a slight improvement; you can try playing
    with the different dropout values yourself and see if you can get better results.
    Another important trick we can use to improve model generalization is to add more
    data or do data augmentation. We can perform data augmentation by randomly flipping
    the image horizontally or rotating the image by a small angle. The `torchvision
    transforms` module provides different functionalities for doing data augmentation
    and they do so dynamically, changing for every epoch. We can implement data augmentation
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Training the model with augmented data improved the model's accuracy by 0.1%
    by running just two epochs; we can run it for a few more epochs to improve further.
    If you have been training these models while reading this book, you will have
    realized that training each epoch could take more than a couple of minutes, depending
    on the GPU you are running. Let's look at a technique where we can train each
    epoch in a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating pre-convoluted features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we freeze the convolution layers and the training model, the input to the
    fully connected layers, or dense layers, (`vgg.classifier`) is always the same.
    To understand this better, let's treat the convolution block – which in our example
    is the `vgg.features` block – as a function that has learned weights and doesn't
    change during training. So, calculating the convolution features and storing them
    will help us improve the training speed. The time to train the model will decrease
    since we only calculate these features once, instead of calculating for each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visually understand this and implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27b0f4e9-1526-45ff-b025-57f6078dd457.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first box depicts how training is done in general, which could be slow
    since we calculate the convolutional features for every epoch, though the values
    don''t change. In the bottom box, we calculate the convolutional features once
    and train only the linear layers. To calculate the pre-convoluted features, we
    need to pass all the training data through the convolution blocks and store them.
    To perform this, we need to select the convolution blocks of the VGG model. Fortunately,
    the PyTorch implementation of VGG16 has two sequential models, so just picking
    the first sequential model''s features is enough. The following code does this
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `preconvfeat` method takes in the dataset and the `vgg`
    model and returns the convoluted features, along with the labels associated with
    it. The rest of the code is similar to what we used in the preceding examples
    to create data loaders and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the convolutional features for the train and validation sets,
    we can create a PyTorch dataset and `DataLoader` classes, which will ease up our
    training process. The following code creates the dataset and `DataLoader` for
    our convolutional features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have our new data loaders, which generate batches of convoluted features
    along with labels, we can use the same train function that we have been using
    in the other examples. Now, we will use `vgg.classifier` as the model to create
    the optimizer and fit methods. The following code trains the classifier module
    to identify dogs and cats. On a Titan X GPU, each epoch takes less than five seconds,
    which would otherwise take a few minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Understanding what a CNN model learns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning models are often said to not be interpretable. However, there
    are different techniques that we can use to interpret what happens inside these
    models. For images, the features that are learned by convents are interpretable.
    In this section, we will explore two popular techniques so that we can understand
    convents.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing outputs from intermediate layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing the outputs from intermediate layers will help us understand how
    the input image is being transformed across different layers. Often, the output
    from each layer is called an **activation**. To do this, we should extract the
    output from intermediate layers, which can be done in different ways. PyTorch
    provides a method called `register_forward_hook`, which allows us to pass a function
    that can extract the outputs of a particular layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, PyTorch models only store the output of the last layer so that
    they use memory optimally. So, before we inspect what the activations from the
    intermediate layers look like, let''s learn how to extract outputs from the model.
    Take a look at the following code snippet, which extracts outputs. We will walk
    through it to understand what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We start by creating a pre-trained VGG model, from which we extract the outputs
    of a particular layer. The `LayerActivations` class instructs PyTorch to store
    the output of a layer in the features variable. Let's walk through each function
    inside the `LayerActivations` class.
  prefs: []
  type: TYPE_NORMAL
- en: The `_init_` function takes a model and the layer number of the layer that the
    outputs need to be extracted from as arguments. We call the `register_forward_hook`
    method on the layer and pass in a function. PyTorch, when doing a forward pass
    – that is, when the images are passed through the layers – calls the function
    that is passed to the `register_forward_hook` method. This method returns a handle,
    which can be used to deregister the function that is passed to the `register_forward_hook`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The `register_forward_hook` method passes three values to the function that
    we pass to it. The `module` parameter allows us to access the layer itself. The
    second parameter is `input`, which refers to the data that is flowing through
    the layer. The third parameter is `output`, which allows us to access the transformed
    inputs, or activations, of the layer. We store the output of the `features` variable
    in the `LayerActivations` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third function takes the hook from the `_init_` function and deregisters
    the function. Now, we can pass the model and the layer numbers of the activations
    we are looking for. Let''s look at the activations that will be created for the
    following image for different layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b52b298-91e3-43a8-a623-b828e727415f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize some of the activations that are created by the first convolution
    layer and the code that''s used for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize some of the activations that are created by the fifth convolutional
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b30d86fd-298a-4867-bd10-02ba5e7a3209.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the last CNN layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d84ef366-2e9b-4c37-a15c-ef24748a92cc.png)'
  prefs: []
  type: TYPE_IMG
- en: By looking at what the different layers generate, we can see that the early
    layers detect lines and edges, while the final layers tend to learn high-level
    features and are less interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to visualizing weights, let's learn how the features maps
    or activations represent themselves after the ReLU layer. So, let's visualize
    the outputs of the second layer.
  prefs: []
  type: TYPE_NORMAL
- en: If you take a quick look at the fifth image in the second row of the preceding
    image, it looks like the filter is detecting the eyes in the image. When the models
    do not perform, these visualization tricks can help us understand why the model
    may not be working.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the weights of the CNN layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting the model weights for a particular layer is straightforward. All the
    model weights can be accessed through the `state_dict` function. The `state_dict`
    function returns a dictionary, with `keys` as its layers and `weights` as its
    values. The following code demonstrates how we can pull weights for a particular
    layer and visualize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides us with the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeebe3c9-3388-483b-a64a-fd6b9ae61753.png)'
  prefs: []
  type: TYPE_IMG
- en: Each box represents the weights of a filter that are *3* x *3* in size. Each
    filter is trained to identify certain patterns in the images.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to build an image classifier using convents,
    as well as how to use a pretrained model. We covered tricks on how to speed up
    the process of training by using pre-convoluted features. We also looked at the
    different techniques we can use to understand what goes on inside a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to handle sequential data using recurrent
    neural networks.
  prefs: []
  type: TYPE_NORMAL
