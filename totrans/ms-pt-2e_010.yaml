- en: 16 PyTorch and AutoML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 PyTorch和AutoML
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区，在Discord上交流讨论。
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file133.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file133.png)'
- en: '**Automated machine learning** (**AutoML**) provides methods to find the optimal
    neural architecture and the best hyperparameter settings for a given neural network.
    We have already covered neural architecture search in detail while discussing
    the `RandWireNN` model in *Chapter 5*, *Hybrid Advanced Models*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动化机器学习**（**AutoML**）为给定神经网络提供了寻找最佳神经架构和最佳超参数设置的方法。在讨论*第五章*，*混合高级模型*中详细介绍了神经架构搜索，例如`RandWireNN`模型。'
- en: In this chapter, we will look more broadly at the AutoML tool for PyTorch—**Auto-PyTorch**—which
    performs both neural architecture search and hyperparameter search. We will also
    look at another AutoML tool called **Optuna** that performs hyperparameter search
    for a PyTorch model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更广泛地探讨用于PyTorch的AutoML工具——**Auto-PyTorch**——它既执行神经架构搜索又执行超参数搜索。我们还将研究另一个名为**Optuna**的AutoML工具，它专门为PyTorch模型执行超参数搜索。
- en: At the end of this chapter, non-experts will be able to design machine learning
    models with little domain experience, and experts will drastically speed up their
    model selection process.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，非专家将能够设计具有少量领域经验的机器学习模型，而专家将大大加快其模型选择过程。
- en: 'This chapter is broken down into the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分解为以下主题：
- en: Finding the best neural architectures with AutoML
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoML寻找最佳神经架构
- en: Using Optuna for hyperparameter search
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Optuna进行超参数搜索
- en: Finding the best neural architectures with AutoML
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoML寻找最佳神经架构
- en: 'One way to think of machine learning algorithms is that they automate the process
    of learning relationships between given inputs and outputs. In traditional software
    engineering, we would have to explicitly write/code these relationships in the
    form of functions that take in input and return output. In the machine learning
    world, machine learning models find such functions for us. Although we automate
    to a certain extent, there is still a lot to be done. Besides mining and cleaning
    data, here are a few routine tasks to be performed in order to get those functions:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下机器学习算法的一种方式是它们自动化了学习给定输入和输出之间关系的过程。在传统软件工程中，我们必须明确地编写/编码这些关系，以函数形式接受输入并返回输出。在机器学习世界中，机器学习模型为我们找到这样的函数。尽管我们在一定程度上实现了自动化，但还有很多工作要做。除了挖掘和清理数据外，还有一些例行任务需要完成以获得这些函数：
- en: Choosing a machine learning model (or a model family and then a model)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习模型（或者模型家族，然后再选择模型）
- en: Deciding the model architecture (especially in the case of deep learning)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定模型架构（特别是在深度学习情况下）
- en: Choosing hyperparameters
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择超参数
- en: Adjusting hyperparameters based on validation set performance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据验证集性能调整超参数
- en: Trying different models (or model families)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的模型（或者模型家族）
- en: These are the kinds of tasks that justify the requirement of a human machine
    learning expert. Most of these steps are manual and either take a lot of time
    or need a lot of expertise to discount the required time, and we have far fewer
    machine learning experts than needed to create and deploy machine learning models
    that are increasingly popular, valuable, and useful across both industries and
    academia.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是需要人类机器学习专家的任务类型。大多数步骤都是手动的，要么耗时很长，要么需要大量专业知识以缩短所需时间，而我们缺少足够数量的机器学习专家来创建和部署越来越受欢迎、有价值且有用的机器学习模型，这在工业界和学术界都如此。
- en: This is where AutoML comes to the rescue. AutoML has become a discipline within
    the field of machine learning that aims to automate the previously listed steps
    and beyond.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是AutoML发挥作用的地方。AutoML已成为机器学习领域内的一个学科，旨在自动化前述步骤及更多内容。
- en: In this section, we will take a look at Auto-PyTorch—an AutoML tool created
    to work with PyTorch. In the form of an exercise, we will find an optimal neural
    network along with the hyperparameters to perform handwritten digit classification—a
    task that we worked on in *Chapter 1*, *Overview of Deep Learning Using PyTorch*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看Auto-PyTorch——一个专为与PyTorch配合使用而创建的AutoML工具。通过一项练习，我们将找到一个最优的神经网络以及执行手写数字分类的超参数——这是我们在*第一章*，*使用PyTorch进行深度学习概述*中进行的任务。
- en: The difference from the first chapter will be that this time, we do not decide
    the architecture or the hyperparameters, and instead let Auto-PyTorch figure that
    out for us. We will first load the dataset, then define an Auto-PyTorch model
    search instance, and finally run the model searching routine, which will provide
    us with a best-performing model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一章的不同之处在于，这一次我们不决定架构或超参数，而是让Auto-PyTorch为我们找出最佳方案。我们将首先加载数据集，然后定义一个Auto-PyTorch模型搜索实例，最后运行模型搜索例程，以提供最佳性能模型。
- en: '**Tool citation**'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**工具引用**'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Auto-PyTorch [16.1] *Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for
    Efficient and Robust AutoDL*, *Lucas Zimmer*, *Marius Lindauer*, and *Frank Hutter
    [16.2]*'
  id: totrans-23
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Auto-PyTorch [16.1] *Auto-PyTorch Tabular: 多精度元学习以实现高效和稳健的AutoDL*，*Lucas Zimmer*，*Marius
    Lindauer* 和 *Frank Hutter [16.2]*'
- en: Using Auto-PyTorch for optimal MNIST model search
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Auto-PyTorch进行最佳MNIST模型搜索
- en: We will execute the model search in the form of a Jupyter Notebook. In the text,
    we only show the important parts of the code. The full code can be found in our
    github repository [16.3]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以Jupyter Notebook的形式执行模型搜索。在文本中，我们只展示代码的重要部分。完整的代码可以在我们的github代码库中找到 [16.3]
- en: Loading the MNIST dataset
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载MNIST数据集
- en: 'We will now discuss the code for loading the dataset step by step, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将逐步讨论加载数据集的代码，如下所示：
- en: 'First, we import the relevant libraries, like this:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入相关的库，如下所示：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The last line is crucial, as we import the relevant Auto-PyTorch module here.
    This will help us set up and execute a model search session.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行非常关键，因为我们在这里导入相关的Auto-PyTorch模块。这将帮助我们设置和执行模型搜索会话。
- en: 'Next, we load the training and test datasets using Torch **application programming
    interfaces** (**APIs**), as follows:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用Torch的**应用程序编程接口** (**APIs**)加载训练和测试数据集，如下所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then convert these dataset tensors into training and testing input (`X`)
    and output (`y`) arrays, like this:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将这些数据集张量转换为训练和测试的输入（`X`）和输出（`y`）数组，如下所示：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we are reshaping the images into flattened vectors of size 784\. In
    the next section, we will be defining an Auto-PyTorch model searcher that expects
    a flattened feature vector as input, and hence we do the reshaping.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在将图像重塑为大小为784的扁平化向量。在下一节中，我们将定义一个期望扁平化特征向量作为输入的Auto-PyTorch模型搜索器，因此我们进行了重塑。
- en: Auto-PyTorch currently (at the time of writing) only provides support for featurized
    and image data in the form of `AutoNetClassification` and `AutoNetImageClassification`
    respectively. While we are using featurized data in this exercise, we leave it
    as an exercise for the reader to use image data instead[16.4] .
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Auto-PyTorch目前仅支持以特征化和图像数据的形式提供支持，分别为`AutoNetClassification`和`AutoNetImageClassification`。虽然在本练习中我们使用的是特征化数据，但我们留给读者的练习是改用图像数据[16.4]
    。
- en: Running a neural architecture search with Auto-PyTorch
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行使用Auto-PyTorch进行神经架构搜索
- en: 'Having loaded the dataset in the preceding section, we will now use Auto-PyTorch
    to define a model search instance and use it to perform the tasks of neural architecture
    search and hyperparameter search. We''ll proceed as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节加载了数据集之后，我们现在将使用Auto-PyTorch定义一个模型搜索实例，并使用它来执行神经架构搜索和超参数搜索的任务。我们将按以下步骤进行：
- en: 'This is the most important step of the exercise, where we define an `autoPyTorch`
    model search instance, like this:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是练习中最重要的一步，我们在此定义一个`autoPyTorch`模型搜索实例，如下所示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The configs here are derived from the examples provided in the Auto-PyTorch
    repository [16.5] . But generally, `tiny_cs` is used for faster searches with
    fewer hardware requirements.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的配置是从Auto-PyTorch仓库提供的示例中衍生出来的 [16.5] 。但通常情况下，`tiny_cs`用于更快速的搜索，且硬件要求较少。
- en: The budget argument is all about setting constraints on resource consumption
    by the Auto-PyTorch process. As a default, the unit of a budget is time—that is,
    how much **central processing unit**/**graphics processing unit** (**CPU**/**GPU**)
    time we are comfortable spending on the model search.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 预算参数主要是为了设置对Auto-PyTorch过程资源消耗的限制。默认情况下，预算的单位是时间，即我们愿意在模型搜索上花费多少**中央处理单元**/**图形处理单元**（**CPU**/**GPU**）时间。
- en: 'After instantiating an Auto-PyTorch model search instance, we execute the search
    by trying to fit the instance on the training dataset, as follows:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化了一个Auto-PyTorch模型搜索实例后，我们通过尝试将实例适配到训练数据集上来执行搜索，如下所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Internally, Auto-PyTorch will run several `trials` of different model architectures
    and hyperparameter settings based on methods mentioned in the original paper [16.2]
    .
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，Auto-PyTorch 将基于原始论文中提到的方法运行多个`试验`，尝试不同的模型架构和超参数设置 [16.2] 。
- en: 'The different `trials` will be benchmarked against the 10% validation dataset,
    and the best-performing `trial` will be returned as output. The command in the
    preceding code snippet should output the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的`试验`将与10%的验证数据集进行基准测试，并将最佳性能的`试验`作为输出返回。前述代码片段中的命令应该会输出以下内容：
- en: '![Figure 16 .1 – Auto-PyTorch model accuracy](img/file134.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .1 – Auto-PyTorch 模型准确性](img/file134.jpg)'
- en: Figure 16 .1 – Auto-PyTorch model accuracy
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .1 – Auto-PyTorch 模型准确性
- en: '*Figure 16* *.1* basically shows the hyperparameter setting that Auto-PyTorch
    finds optimal for the given task—for example, the learning rate is `0.068`, momentum
    is `0.934`, and so on. The preceding screenshot also shows the training and validation
    set accuracy for the chosen optimal model configuration.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16* *.1* 基本上展示了 Auto-PyTorch 为给定任务找到的最佳超参数设置，例如学习率为`0.068`，动量为`0.934`等。前面的截图还显示了所选最佳模型配置的训练集和验证集准确性。'
- en: 'Having converged to an optimal trained model, we can now make predictions on
    our test set using that model, as follows:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 已经收敛到最佳训练模型后，我们现在可以使用该模型对测试集进行预测，如下所示：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It should output something like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该输出类似于这样的内容：
- en: '![Figure 16 .2 – Auto-PyTorch model accuracy](img/file135.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .2 – Auto-PyTorch 模型准确性](img/file135.jpg)'
- en: Figure 16 .2 – Auto-PyTorch model accuracy
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .2 – Auto-PyTorch 模型准确性
- en: As we can see, we have obtained a model with a decent test-set performance of
    96.4%. For context, a random choice on this task would lead to a performance rate
    of 10%. We have obtained this good performance without defining either the model
    architecture or the hyperparameters. Upon setting a higher budget, a more extensive
    search could lead to an even better performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们获得了一个测试集性能达到了96.4%的模型。为了对比，随机选择将导致10%的性能水平。我们在没有定义模型架构或超参数的情况下获得了这样的良好性能。在设置更高预算后，更广泛的搜索可能会导致更好的性能。
- en: Also, the performance will vary based on the hardware (machine) on which the
    search is being performed. Hardware with more compute power and memory can run
    more searches in the same time budget, and hence can lead to a better performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，性能将根据执行搜索的硬件（机器）而变化。具有更多计算能力和内存的硬件可以在相同的时间预算内运行更多搜索，因此可能导致更好的性能。
- en: Visualizing the optimal AutoML model
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化最优 AutoML 模型
- en: 'In this section, we will look at the best-performing model that we have obtained
    by running the model search routine in the previous section. We''ll proceed as
    follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看通过在前一节中运行模型搜索例程获得的最佳性能模型。我们将按以下步骤进行：
- en: 'Having already looked at the hyperparameters in the preceding section, let''s
    look at the optimal model architecture that Auto-PyTorch has devised for us, as
    follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的章节中已经查看了超参数，现在让我们看一下 Auto-PyTorch 为我们设计的最佳模型架构，如下所示：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It should output something like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该输出类似于这样的内容：
- en: '![Figure 16 .3 – Auto-PyTorch model architecture](img/file136.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .3 – Auto-PyTorch 模型架构](img/file136.jpg)'
- en: Figure 16 .3 – Auto-PyTorch model architecture
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .3 – Auto-PyTorch 模型架构
- en: The model consists of some structured residual blocks containing fully connected
    layers, batch normalization layers, and ReLU activations. At the end, we see a
    final fully connected layer with 10 outputs—one for each digit from 0 to 9.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由一些结构化的残差块组成，其中包含全连接层、批量归一化层和ReLU激活函数。最后，我们看到一个最终的全连接层，具有10个输出，每个输出对应于从0到9的一个数字。
- en: 'We can also visualize the actual model graph using `torchviz`, as shown in
    the next code snippet:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用`torchviz`来可视化实际的模型图，如下代码片段所示：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This should save a `convnet_arch.pdf` file in the current working directory,
    which should look like this upon opening:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会在当前工作目录中保存一个`convnet_arch.pdf`文件，在打开时应该看起来像这样：
- en: '![Figure 16 .4 – Auto-PyTorch model diagram](img/file137.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .4 – Auto-PyTorch 模型图示](img/file137.jpg)'
- en: Figure 16 .4 – Auto-PyTorch model diagram
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .4 – Auto-PyTorch 模型图示
- en: 'To peek into how the model converged to this solution, we can look at the search
    space that was used during the model-finding process with the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看模型如何收敛到此解决方案，我们可以查看在模型查找过程中使用的搜索空间代码如下：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This should output the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会输出以下内容：
- en: '![Figure 16 .5 – Auto-PyTorch model search space](img/file138.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .5 – Auto-PyTorch 模型搜索空间](img/file138.jpg)'
- en: Figure 16 .5 – Auto-PyTorch model search space
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .5 – Auto-PyTorch 模型搜索空间
- en: It essentially lists the various ingredients required to build the model, with
    an allocated range per ingredient. For instance, the learning rate is allocated
    a range of **0.0001** to **0.1** and this space is sampled in a log scale—this
    is not linear but logarithmic sampling.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上列出了构建模型所需的各种要素，并为每个要素分配了一个范围。例如，学习率被分配了**0.0001**到**0.1**的范围，并且这个空间是以对数尺度进行采样——这不是线性采样而是对数采样。
- en: In *Figure 16* *.1*, we have already seen the exact hyperparameter values that
    Auto-PyTorch samples from these ranges as optimal values for the given task. We
    can also alter these hyperparameter ranges manually, or even add more hyperparameters,
    using the `HyperparameterSearchSpaceUpdates` sub-module under the Auto-PyTorch
    module [16.6] .
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 16 .1*中，我们已经看到了Auto-PyTorch从这些范围中采样的确切超参数值作为给定任务的最优值。我们还可以手动更改这些超参数范围，甚至添加更多超参数，使用Auto-PyTorch模块下的`HyperparameterSearchSpaceUpdates`子模块
    [16.6] 。
- en: This concludes our exploration of Auto-PyTorch—an AutoML tool for PyTorch. We
    successfully built an MNIST digit classification model using Auto-PyTorch, without
    specifying either the model architecture or the hyperparameters. This exercise
    will help you to get started with using this and other AutoML tools to build PyTorch
    models in an automated fashion. Some other similar tools are listed here - Hyperopt
    [16.7], Tune [16.8], Hypersearch [16.9], Skorcj [16.10], BoTorch [16.11] and Optuna
    [16.12]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Auto-PyTorch的探索——一个用于PyTorch的自动机器学习工具。我们成功地使用Auto-PyTorch构建了一个MNIST数字分类模型，而无需指定模型架构或超参数。此练习将帮助您开始使用此类和其他自动机器学习工具以自动化方式构建PyTorch模型。这里列出了一些类似的其他工具
    - Hyperopt [16.7]、Tune [16.8]、Hypersearch [16.9]、Skorcj [16.10]、BoTorch [16.11]
    和 Optuna [16.12]。
- en: While we cannot cover all of these tools in this chapter, in the next section
    we will discuss Optuna, which is a tool focused exclusively on finding an optimal
    set of hyperparameters and one that works well with PyTorch.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本章中无法涵盖所有这些工具，在下一节中我们将讨论Optuna，这是一个专注于查找最佳超参数集的工具，并且与PyTorch兼容良好。
- en: Using Optuna for hyperparameter search
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Optuna进行超参数搜索
- en: Optuna is one of the hyperparameter search tools that supports PyTorch. You
    can read in detail about the search strategies used by the tool, such as **TPE**
    (**Tree-Structured Parzen Estimation**) and **CMA-ES** (**Covariance Matrix Adaptation
    Evolution Strategy**) in the *Optuna* paper [16.13] . Besides the advanced hyperparameter
    search methodologies, the tool provides a sleek API, which we will explore in
    a moment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna是支持PyTorch的超参数搜索工具之一。您可以详细了解该工具使用的搜索策略，如*TPE*（树形结构帕尔森估计）和*CMA-ES*（协方差矩阵适应进化策略），在*Optuna*论文
    [16.13] 中。除了先进的超参数搜索方法，该工具还提供了一个简洁的API，我们将在下一节中探讨。
- en: '**Tool citation**'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**工具引用**'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Optuna: A Next-Generation Hyperparameter Optimization Framework.*'
  id: totrans-83
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Optuna: 下一代超参数优化框架。*'
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Takuya Akiba*, *Shotaro Sano*, *Toshihiko Yanase*, *Takeru Ohta*, and *Masanori
    Koyama* (2019, in KDD).'
  id: totrans-85
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Takuya Akiba*, *Shotaro Sano*, *Toshihiko Yanase*, *Takeru Ohta* 和 *Masanori
    Koyama*（2019年，KDD）。'
- en: In this section, we will once again build and train the `MNIST` model, this
    time using Optuna to figure out the optimal hyperparameter setting. We will discuss
    important parts of the code step by step, in the form of an exercise. The full
    code can be found in our github [16.14].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次构建和训练`MNIST`模型，这次使用Optuna来找出最佳的超参数设置。我们将逐步讨论代码的重要部分，以练习的形式进行。完整的代码可以在我们的github
    [16.14]上找到。
- en: Defining the model architecture and loading dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型架构和加载数据集
- en: 'First, we will define an Optuna-compliant model object. By Optuna-compliant,
    we mean adding APIs within the model definition code that are provided by Optuna
    to enable the parameterization of the model hyperparameters. To do this, we''ll
    proceed as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个符合Optuna要求的模型对象。所谓Optuna兼容，是指在模型定义代码中添加Optuna提供的API，以便对模型超参数进行参数化。为此，我们将按照以下步骤进行：
- en: 'First, we import the necessary libraries, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库，如下所示：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `optuna` library will manage the hyperparameter search for us throughout
    the exercise.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`optuna`库将在整个练习中管理超参数搜索。'
- en: 'Next, we define the model architecture. Because we want to be flexible with
    some of the hyperparameters—such as the number of layers and the number of units
    in each layer—we need to include some logic in the model definition code. So,
    first, we have declared that we need anywhere in between `1` to `4` convolutional
    layers and `1` to `2` fully connected layers thereafter, as illustrated in the
    following code snippet:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型架构。因为我们希望对一些超参数（如层数和每层单位数）保持灵活，所以需要在模型定义代码中包含一些逻辑。因此，首先，我们声明需要在 `1`
    到 `4` 个卷积层和之后的 `1` 到 `2` 个全连接层，如下面的代码片段所示：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We then successively append the convolutional layers, one by one. Each convolutional
    layer is instantly followed by a `ReLU` activation layer, and for each convolutional
    layer, we declare the depth of that layer to be between `16` and `64`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们逐个添加卷积层。每个卷积层紧接着一个 `ReLU` 激活层，对于每个卷积层，我们声明该层的深度在 `16` 到 `64` 之间。
- en: 'The stride and padding are fixed to `3` and `True` respectively, and the whole
    convolutional block is then followed by a `MaxPool` layer, then a `Dropout` layer,
    with dropout probability ranging anywhere between `0.1` to `0.4` (another hyperparameter),
    as illustrated in the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 步幅和填充分别固定为 `3` 和 `True`，整个卷积块之后是一个 `MaxPool` 层，然后是一个 `Dropout` 层，dropout 概率范围在
    `0.1` 到 `0.4` 之间（另一个超参数），如下面的代码片段所示：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we add a flattening layer so that fully connected layers can follow. We
    have to define a `_get_flatten_shape` function to derive the shape of the flattening
    layer output. We then successively add fully connected layers, where the number
    of units is declared to be between `16` and `64`. A `Dropout` layer follows each
    fully connected layer, again with the probability range of `0.1` to `0.4`.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个展平层，以便后续可以添加全连接层。我们必须定义一个 `_get_flatten_shape` 函数来推导展平层输出的形状。然后，我们逐步添加全连接层，其中单位数声明为介于
    `16` 和 `64` 之间。每个全连接层后面跟着一个 `Dropout` 层，再次使用概率范围为 `0.1` 到 `0.4`。
- en: 'Finally, we append a fixed fully connected layer that outputs `10` numbers
    (one for each class/digit), followed by a `LogSoftmax` layer. Having defined all
    the layers, we then instantiate our model object, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们附加一个固定的全连接层，输出 `10` 个数字（每个类别/数字一个），然后是一个 `LogSoftmax` 层。定义了所有层之后，我们实例化我们的模型对象，如下所示：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This model initialization function is conditioned on the `trial` object, which
    is facilitated by Optuna and which will decide the hyperparameter setting for
    our model. Finally, the `forward` method is quite straightforward, as can be seen
    in the following code snippet:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型初始化函数是依赖于 `trial` 对象的条件设置，该对象由 Optuna 轻松处理，并决定我们模型的超参数设置。最后，`forward` 方法非常简单，可以在下面的代码片段中看到：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Thus, we have defined our model object and we can now move on to loading the
    dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经定义了我们的模型对象，现在可以继续加载数据集。
- en: 'The code for dataset loading is the same as in *Chapter 1,* *Overview of Deep
    Learning Using PyTorch* and is shown again in the following snippet:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集加载的代码与 *第一章，使用 PyTorch 进行深度学习概述* 中相同，并在下面的代码片段中再次显示：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this section, we have successfully defined our parameterized model object
    as well as loaded the dataset. We will now define the model training and testing
    routines, along with the optimization schedule.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功地定义了我们的参数化模型对象，并加载了数据集。现在，我们将定义模型训练和测试程序，以及优化调度。
- en: Defining the model training routine and optimization schedule
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型训练程序和优化调度
- en: 'Model training itself involves hyperparameters such as optimizer, learning
    rate, and so on. In this part of the exercise, we will define the model training
    procedure while utilizing Optuna''s parameterization capabilities. We''ll proceed
    as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练本身涉及超参数，如优化器、学习率等。在本练习的这一部分中，我们将定义模型训练过程，同时利用 Optuna 的参数化能力。我们将按以下步骤进行：
- en: 'First, we define the training routine. Once again, the code is the same as
    the training routine code we had for this model in the exercise found in *Chapter
    1*, *Overview of Deep Learning Using PyTorch*, and is shown again here:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义训练例程。再次强调，这段代码与 *第一章，使用 PyTorch 进行深度学习概述* 中此模型的训练例程代码相同，并在此处再次显示：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The model testing routine needs to be slightly augmented. To operate as per
    Optuna API requirements, the test routine needs to return a model performance
    metric—accuracy, in this case—so that Optuna can compare different hyperparameter
    settings based on this metric, as illustrated in the following code snippet:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型测试例程需要稍作调整。为了按照Optuna API的要求操作，测试例程需要返回一个模型性能指标——在本例中是准确率，以便Optuna可以根据这一指标比较不同的超参数设置，如以下代码片段所示：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Previously, we would instantiate the model and the optimization function with
    the learning rate, and start the training loop outside of any function. But to
    follow the Optuna API requirements, we do all that under an `objective` function,
    which takes in the same `trial` object that was fed as an argument to the `__init__`
    method of our model object.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以前，我们会使用学习率来实例化模型和优化函数，并在任何函数外部启动训练循环。但是为了遵循Optuna API的要求，我们现在将所有这些都放在一个`objective`函数中进行，该函数接受与我们模型对象的`__init__`方法中传递的`trial`对象相同的参数。
- en: 'The `trial` object is needed here too because there are hyperparameters associated
    with deciding the learning rate value and choosing an optimizer, as illustrated
    in the following code snippet:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也需要`trial`对象，因为涉及到决定学习率值和选择优化器的超参数，如以下代码片段所示：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For each epoch, we record the accuracy returned by the model testing routine.
    Additionally, at each epoch, we check if we will prune—that is, if we will skip—the
    current epoch. This is another feature offered by Optuna to speed up the hyperparameter
    search process so that we don't waste time on poor hyperparameter settings.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个epoch，我们记录模型测试例程返回的准确率。此外，在每个epoch，我们还检查是否会剪枝——即是否会跳过当前epoch。这是Optuna提供的另一个功能，用于加速超参数搜索过程，以避免在糟糕的超参数设置上浪费时间。
- en: Running Optuna's hyperparameter search
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行Optuna的超参数搜索
- en: 'In this final part of the exercise, we will instantiate what is called an **Optuna
    study** and, using the model definition and the training routine, we will execute
    Optuna''s hyperparameter search process for the given model and the given dataset.
    We''ll proceed as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习的最后部分，我们将实例化所谓的**Optuna study**，并利用模型定义和训练例程，为给定的模型和给定的数据集执行Optuna的超参数搜索过程。我们将按如下步骤进行：
- en: 'Having prepared all the necessary components in the preceding sections, we
    are ready to start the hyperparameter search process—something that is called
    a `study` in Optuna terminology. A `trial` is one hyperparameter-search iteration
    in a `study`. The code can be seen in the following snippet:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的章节中准备了所有必要的组件后，我们已经准备好开始超参数搜索过程——在Optuna术语中称为`study`。一个`trial`是`study`中的一个超参数搜索迭代。代码可以在以下代码片段中看到：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `direction` argument helps Optuna compare different hyperparameter settings.
    Because our metric is accuracy, we will need to `maximize` the metric. We allow
    a maximum of `2000` seconds for the `study` or a maximum of `10` different searches—whichever
    finishes first. The preceding command should output the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`direction`参数帮助Optuna比较不同的超参数设置。因为我们的指标是准确率，我们将需要`maximize`这个指标。我们允许最多2000秒的`study`或最多10个不同的搜索——以先完成者为准。前述命令应输出以下内容：'
- en: '![Figure 16 .6 – Optuna logs](img/file139.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .6 – Optuna日志](img/file139.jpg)'
- en: Figure 16 .6 – Optuna logs
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .6 – Optuna日志
- en: As we can see, the third `trial` is the most optimal trial, producing a test
    set accuracy of 98.77%, and the last three `trials` are pruned. In the logs, we
    also see the hyperparameters for each non-pruned `trial`. For the most optimal
    `trial`, for example, there are three convolutional layers with 27, 28, and 46
    feature maps respectively, and then there are two fully connected layers with
    57 and 54 units/neurons respectively, and so on.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，第三个`trial`是最优的试验，产生了98.77%的测试集准确率，最后三个`trials`被剪枝。在日志中，我们还可以看到每个未剪枝`trial`的超参数。例如，在最优的`trial`中，有三个分别具有27、28和46个特征映射的卷积层，然后有两个分别具有57和54个单元/神经元的全连接层，等等。
- en: 'Each `trial` is given a completed or a pruned status. We can demarcate those
    with the following code:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个`trial`都有一个完成或被剪枝的状态。我们可以用以下代码标记它们：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And finally, we can specifically look at all the hyperparameters of the most
    successful `trial` with the following code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以具体查看最成功`trial`的所有超参数，使用以下代码：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You will see the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '![Figure 16 .7 – Optuna optimal hyperparameters](img/file140.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 16 .7 – Optuna最优超参数](img/file140.jpg)'
- en: Figure 16 .7 – Optuna optimal hyperparameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16 .7 – Optuna最优超参数
- en: As we can see, the output shows us the total number of `trials` and the number
    of successful `trials` performed. It further shows us the model hyperparameters
    for the most successful `trial`, such as the number of layers, the number of neurons
    in layers, learning rate, optimization schedule, and so on.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，输出显示了总`trials`数和执行的成功`trials`数。它进一步显示了最成功`trial`的模型超参数，如层数、层中神经元数量、学习率、优化进度等。
- en: This brings us to the end of the exercise. We have managed to use Optuna to
    define a range of hyperparameter values for different kinds of hyperparameters
    for a handwritten digit classification model. Using Optuna's hyperparameter search
    algorithm, we ran 10 different `trials` and managed to obtain the highest accuracy
    of 98.77% in one of those `trials`. The model (architecture and hyperparameters)
    from the most successful `trial` can be used for training with larger datasets,
    thereby serving in a production system.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了练习的尾声。我们成功地使用 Optuna 定义了不同类型超参数的值范围，适用于手写数字分类模型。利用 Optuna 的超参数搜索算法，我们运行了
    10 个不同的`trials`，在其中一个`trial`中获得了 98.77% 的最高准确率。最成功`trial`中的模型（架构和超参数）可以用于在更大数据集上进行训练，从而服务于生产系统。
- en: Using the lessons from this section, you can use Optuna to find the optimal
    hyperparameters for any neural network model written in PyTorch. Optuna can also
    be used in a distributed fashion if the model is extremely large and/or there
    are way too many hyperparameters to tune [16.15] .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本节的教训，您可以使用 Optuna 找到任何用 PyTorch 编写的神经网络模型的最佳超参数。如果模型非常庞大和/或需要调整的超参数过多，Optuna
    也可以在分布式环境中使用 [16.15]。
- en: Lastly, Optuna supports not only PyTorch but other popular machine learning
    libraries too, such as `TensorFlow`, `Sklearn`, `MXNet`, and so on.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Optuna 不仅支持 PyTorch，还支持其他流行的机器学习库，如`TensorFlow`、`Sklearn`、`MXNet`等等。
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed AutoML, which aims to provide methods for model
    selection and hyperparameter optimization. AutoML is useful for beginners who
    have little expertise on making decisions such as how many layers to put in a
    model, which optimizer to use, and so on. AutoML is also useful for experts to
    both speed up the model training process and discover superior model architectures
    for a given task that would be nearly impossible to figure manually.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了自动机器学习（AutoML），旨在提供模型选择和超参数优化的方法。AutoML 对于初学者非常有用，他们在做出诸如模型中应放置多少层、使用哪种优化器等决策时缺乏专业知识。AutoML
    对于专家也很有用，可以加快模型训练过程，发现给定任务的优越模型架构，这些任务手动推断几乎是不可能的。
- en: In the next chapter, we will study another increasingly important and crucial
    aspect of machine learning, especially deep learning. We will closely look at
    how to interpret output produced by PyTorch models—a field popularly known as
    model interpretability or explainability.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究另一个越来越重要和关键的机器学习方面，特别是深度学习。我们将密切关注如何解释由 PyTorch 模型生成的输出——这一领域通常被称为模型可解释性或可解释性。
