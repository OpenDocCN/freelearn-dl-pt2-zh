- en: Deep Q-Networks in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Q-learning, or using deep Q-networks, is considered the most modern reinforcement
    learning technique. In this chapter, we will develop various deep Q-network models
    step by step and apply them to solve several reinforcement learning problems.
    We will start with vanilla Q-networks and enhance them with experience replay.
    We will improve robustness by using an additional target network and demonstrate
    how to fine-tune a Deep Q-Network. We will also experiment with dueling deep Q-networks
    and see how their value functions differs from other types of Deep Q-Networks.
    In the last two recipes, we will solve complex Atari game problems by incorporating
    convolutional neural networks into Deep Q-Networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing deep Q-networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving DQNs with experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing double deep Q-Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning double DQN hyperparameters for CartPole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing Dueling deep Q-Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying deep Q-Networks to Atari games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using convolutional neural networks for Atari games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing deep Q-networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will recall that **Function Approximation** (**FA**) approximates the state
    space using a set of features generated from the original states. **Deep Q-Networks**
    (**DQNs**) are very similar to FA with neural networks, but they use neural networks
    to map the states to action values directly instead of using a set of generated
    features as media.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Deep Q-learning, a neural network is trained to output the appropriate *Q(s,a)*
    values for each action given the input state, s. The action, a, of the agent is
    chosen based on the output Q(s,a) values following the epsilon-greedy policy.
    The structure of a DQN with two hidden layers is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4217aa3a-58c0-4827-b4b2-4fbbd85d55ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will recall that Q-learning is an off-policy learning algorithm and that
    it updates the Q-function based on the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2932660-72b6-40ed-9471-d558e4357238.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *s''* is the resulting state after taking action, *a*, in state, *s*;
    *r* is the associated reward; α is the learning rate; and γ is the discount factor.
    Also, [![](img/781b37bb-fdcf-4f8f-a59e-7438e8d6cf73.png)] means that the behavior
    policy is greedy where the highest Q-value among those in state *s''* is selected
    to generate learning data. Similarly, DQNs learn to minimize the following error
    term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e80a59e-1381-409a-a0fd-e477bf2e485f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the goal becomes one of finding the optimal network model to best approximate
    the state-value function, *Q(s, a)*, for each possible action. The loss function
    we are trying to minimize in this case is similar to that in a regression problem,
    which is the mean squared error between the actual value and the estimated value.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will develop a DQN model to solve the Mountain Car ([https://gym.openai.com/envs/MountainCar-v0/](https://gym.openai.com/envs/MountainCar-v0/))
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We develop deep Q-learning using DQN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The variable wraps a tensor and supports backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the `__init__` method of the `DQN` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now develop the training method, which updates the neural network with a
    data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is the prediction of the state value for each action given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That's all for the `DQN` class! And now we can move on to develop the learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by creating a Mountain Car environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the epsilon-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the deep Q-learning algorithm with DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then specify the size of the hidden layer and the learning rate, and create
    a `DQN` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then perform Deep Q-learning with the DQN we just developed for 1,000 episodes
    and also keep track of the total (original) rewards for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s display the plot of episode reward over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 2*, the `DQN` class takes in four parameters: the number of input
    states and output actions, the number of hidden nodes (we herein just use one
    hidden layer as an example), and the learning rate. It initializes a neural network
    with one hidden layer, followed by a ReLU activation function. It takes in `n_state`
    units and generates one `n_action` output, which are the predicted state values
    for individual actions. An optimizer, Adam, is initialized along with each linear
    model. The loss function is the mean squared error.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3* is for updating the network: given a training data point, the predictive
    result, along with the target value, is used to compute the loss and gradients.
    The neural network model is then updated via backpropagation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 7*, the deep Q-learning function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: In each episode, creates an epsilon-greedy policy with an epsilon factor decayed
    to 99% (for example, if epsilon in the first episode is 0.1, it will be 0.099
    in the second episode). We also set 0.01 as the lower epsilon limit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runs an episode: In each step in state *s*, takes an action, a, by following
    the epsilon-greedy policy; then, compute the *Q* values `q_value`, of the previous
    state using the `predict` method from DQN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the *Q* values, `q_values_next`, for the new state, *s'*; then, computes
    the target value by updating the old *Q* values, `q_values`, for the action, ![](img/0f9dd526-9c3d-4a3c-b2de-b0a019e3a40d.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the data point *(s, Q(s))*, to train the neural network. Note that *Q(s)*
    is composed of the values for all actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs `n_episode` episodes and records the total reward for each episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may notice that we use a modified version of reward in training the model.
    It is based on the position of the car since we want it to reach the position
    of +0.5\. And we also give tiered incentives for positions greater than or equal
    to +0.5, +0.25, +0.1, and 0, respectively. This modified reward setting differentiates
    different car positions and favors positions that are closer to the goal; hence,
    it largely speeds up learning compared to the original monotonous -1 reward for
    each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in *Step 10*, you will see the resulting plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88967993-441e-40e8-b5ce-597a690045a0.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that in the last 200 episodes, the car reaches the mountain top
    after around 170 to 180 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Q-learning approximates the state values with a more direct model, a neural
    network, than using a set of intermediate artificial features. Given one step,
    where an old state transfers to a new state by taking an action and receives a
    reward, training the DQN involves the following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the neural network model to estimate the *Q* values of the old state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the neural network model to estimate the *Q* values of the new state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the target Q value for the action using the reward and the new *Q* values,
    as in [![](img/e7041f0b-3588-40ff-a5db-dc73000512f9.png)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if it is a terminal state, the target Q value is updated as r.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the neural network model with the old state as the input, and the target
    *Q* values as the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It updates the weights for the network via gradient descent and can predict
    the Q values given a state.
  prefs: []
  type: TYPE_NORMAL
- en: DQN dramatically reduces the number of states to learn, where learning millions
    of states is not feasible in the TD method. Moreover, it directly maps the input
    state to the Q values, and does not require any additional functions to generate
    artificial features.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are not familiar with the Adam optimizer as an advanced gradient descent
    method, please check out the following material:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving DQNs with experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approximation of Q-values using neural networks with one sample at a time
    is not very stable. You will recall that, in FA, we incorporated experience replay
    to improve stability. Similarly, in this recipe, we will apply experience replay
    to DQNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With experience replay, we store the agent''s experiences (an experience is
    composed of an old state, a new state, an action, and a reward) during episodes
    in a training session in a memory queue. Every time we gain sufficient experience,
    batches of experiences are randomly sampled from the memory and are used to train
    the neural network. Learning with experience replay becomes two phases: gaining
    experience, and updating models based on the past experiences randomly selected.
    Otherwise, the model will keep learning from the most recent experience and the
    neural network model could get stuck in a local minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: We will develop DQN with experience replay to solve the Mountain Car problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll develop a DQN with experience replay as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Mountain Car environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To incorporate experience replay, we add a `replay` method to the `DQN` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the `DQN` class remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipeand will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set 0.001 as the learning rate, and create
    a DQN accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the buffer holding the experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we define the deep Q-learning function that performs experience replay:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then perform deep Q-learning with experience replay for `600` episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `20` as the replay sample size for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We also keep track of the total rewards for each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to display the plot of episode rewards over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 2*, the experience replay function first randomly selects `replay_size`
    samples of experience. It then converts each experience into a training sample
    composed of the input state and output target values. And finally, it updates
    the neural network using the selected batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 6*, deep Q-learning with experience replay is performed with the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: In each episode, create an epsilon-greedy policy with an epsilon factor decayed
    to 99%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run an episode: in each step, take an action, *a*, following the epsilon-greedy
    policy; store this experience (old state, action, new state, reward) in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each step, conduct experience replay to train the neural network, provided
    we have sufficient training samples to randomly pick from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `n_episode` episodes and record the total reward for each episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Executing the lines of code in *Step 8* will result in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5b6cc17-88d7-40e7-9a0d-529a6f7ee760.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that, in most episodes in the last 200 episodes, the car reaches
    the mountain top in around 120 to 160 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In deep Q-learning, experience replay means we store the agent''s experience
    for each step and randomly draw some samples of past experience to train the DQN.
    The learning in this case is split into two phases: accumulating experience, and
    updating models based on batches of past experience. Specifically, the experience
    (also called the **buffer**, or **memory**) includes the past state, the action
    taken, the reward received, and the next state. Experience replay can stabilize
    training by providing a set of samples with low correlation, which, as a result,
    increases learning efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing double deep Q-Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the deep Q-learning algorithms we have developed so far, the same neural
    network is used to calculate the predicted values and the target values. This
    may cause a lot of divergence as the target values keep on changing and the prediction
    has to chase it. In this recipe, we will develop a new algorithm using two neural
    networks instead of one.
  prefs: []
  type: TYPE_NORMAL
- en: In **double DQNs**, we use a separate network to estimate the target rather
    than the prediction network. The separate network has the same structure as the
    prediction network. And its weights are fixed for every *T* episode (*T* is a
    hyperparameter we can tune), which means they are only updated after every *T*
    episode. The update is simply done by copying the weights of the prediction network.
    In this way, the target function is fixed for a while, which results in a more
    stable training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, double DQNs are trained to minimize the following error term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/083d4882-dad9-48b9-bb1c-7389e117c3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *s'* is the resulting state after taking action, *a*, in state *s*; *r*
    is the associated reward; α is the learning rate; and γ is the discount factor.
    Also, [![](img/86da76aa-fe81-4d29-868c-a2dac845bea2.png)] is the function for
    the target network, and Q is the function for the prediction network.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now solve the Mountain Car problem using double DQNs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We develop deep Q-learning using double DQNs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Mountain Car environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To incorporate the target network in the experience replay phase, we first
    initialize it in the `__init__` method of the `DQN` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The target network has the same structure as the prediction network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, we add the calculation of values using the target network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We also add the method to synchronize the weights of the target network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In experience replay, we use the target network to calculate the target value
    instead of the prediction network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the `DQN` class remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set `0.01` as the learning rate, and
    create a DQN accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the buffer holding the experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll develop deep Q-learning with double DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform deep Q-learning with double DQN for `1000` episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `20` as the replay sample size for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We update the target network for every 10 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We also keep track of the total rewards for each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We then display the plot of episode rewards over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 5*, the experience replay function first randomly selects `replay_size`
    samples of experience. It then converts each experience into a training sample
    composed of the input state and output target values. And finally, it updates
    the prediction network using the selected batch.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 9* is the most important step in a double DQN: it uses a different network
    to calculate the target values and then this network is updated periodically.
    The rest of the function is similar to deep Q-learning with experience replay.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization function in *Step 11* will result in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec4c795b-ebcd-4389-a3c5-8c0431c6f9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that, in most episodes, after the first **400** episodes, the car
    reaches the mountain top in around **80** to **160** steps.
  prefs: []
  type: TYPE_NORMAL
- en: In deep Q-learning with a double DQN, we create two separate networks for prediction
    and target calculation, respectively. The first one is used to predict and retrieve
    *Q* values, while the second one is used to provide a stable target *Q* value.
    And, after a while (let's say every 10 episodes, or 1,500 training steps), we
    synchronize the prediction network with the target network. In this double network
    setting, the target values are temporarily fixed instead of being modified constantly,
    so the prediction network has more stable targets to learn against. The results
    we obtained show that double DQNs outperform single DQNs.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning double DQN hyperparameters for CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's solve the CartPole environment using double DQNs. We will
    demonstrate how to fine-tune the hyperparameters in a double DQN to achieve the
    best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to fine-tune the hyperparameters, we can apply the **grid search**
    technique to explore a set of different combinations of values and pick the one
    achieving the best average performance. We can start with a coarse range of values
    and continue to narrow it down gradually. And don’t forget to fix the random number
    generators for all of the following in order to ensure reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: The Gym environment random number generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The epsilon-greedy random number generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial weights for the neural network in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the CartPole environment using double DQNs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a CartPole environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We will reuse the `DQN` class developed in the last, *Developing double deep
    Q-networks* recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we''ll develop deep Q-learning with a double DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, the hidden layer, and the number of episodes, as well as the
    number of episodes used to evaluate the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a few values for the following hyperparameters to explore in
    a grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we perform a grid search where, in each iteration, we create a DQN
    according to the set of hyperparameters and allow it to learn for 600 episodes.
    We then evaluate its performance by averaging the total rewards for the last 200
    episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having executed *Step 7*, we get the following grid search results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the best average reward, `192.77`, is achieved with the combination
    of `n_hidden=30`, `lr=0.001`, `replay_size=25`, and `target_update=35`.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to fine-tune the hyperparameters further in order to get a better
    DQN model.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we solved the CartPole problem with double DQNs. We fine-tuned
    the values of hyperparameters using a grid search. In our example, we optimized
    the size of the hidden layer, the learning rate, the replay batch size, and the
    target network update frequency. There are other hyperparameters we could also
    explore, such as the number of episodes, the initial epsilon, and the value of
    epsilon decay. For each experiment, we kept the random seeds fixed so that the
    randomness of the Gym environment, and the epsilon-greedy action, as well as the
    weight initialization of the neural network, remain the same. This is to ensure
    the reproducibility and comparability of performance. The performance of each
    DQN model is measured by the average total reward for the last few episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Developing Dueling deep Q-Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to develop another advanced type of DQNs, **Dueling
    DQNs** (**DDQNs**). In particularly, we will see how the computation of the Q
    value is split into two parts in DDQNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DDQNs, the Q value is computed with the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c71596f-ff5e-49b2-9522-e0181e979d78.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *V(s)* is the state-value function, calculating the value of being at
    state *s*; *A(s, a)* is the state-dependent action advantage function, estimating
    how much better it is to take an action, *a*, rather than taking other actions
    at a state, *s*. By decoupling the `value` and `advantage` functions, we are able
    to accommodate the fact that our agent may not necessarily look at both the value
    and advantage at the same time during the learning process. In other words, the
    agent using DDQNs can efficiently optimize either or both functions as it prefers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the Mountain Car problem using DDQNs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Mountain Car environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the DDQN model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Accordingly, we use the DDQN model in the `DQN` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the `DQN` class remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing deep Q-Networks* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will reuse the `q_learning` function we developed in the, *Improving DQNs
    with experience replay* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set `0.001` as the learning rate, and
    create a DQN accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the buffer holding the experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then perform Deep Q-learning with DDQN for `600` episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `20` as the replay sample size for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We also keep track of the total rewards for each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can display the plot of episode rewards over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Step 2* is the core part of the Dueling DQN. It is composed of two parts,
    the action **advantage** (**adv**), and the **state-value** (**val**). Again,
    we use one hidden layer as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing *Step 9* will result in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b252d17-a5a3-4320-b706-357a6844d87a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In DDQNs, the predicted Q value is the result of two elements: state-value
    and action advantage. The first one estimates how good it is to be at a certain
    state. The second one indicates how much better it is to take a particular action
    as opposed to the alternatives. These two elements are computed separately and
    combined into the last layer of the DQN. You will recall that traditional DQNs
    update the Q value for a given action at a state only. DDQNs update the state-value
    that all actions (not just the given one) can take advantage of, as well as the
    advantage for the action. Hence, DDQNs are thought to be more robust.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying Deep Q-Networks to Atari games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problems we have worked with so far are fairly simple, and applying DQNs
    is sometimes overkill. In this and the next recipe, we'll use DQNs to solve Atari
    games, which are far more complicated problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Pong ([https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/))
    as an example in this recipe. It simulates the Atari 2600 game Pong, where the
    agent plays table tennis with another player. The observation in this environment
    is an RGB image of the screen (refer to the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a405812-c25f-4905-8952-45313c8aeccf.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a matrix of shape (210, 160, 3), which means that the image is of size
    *210 * 160* and in three RGB channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent (on the right-hand side) moves up and down during the game to hit
    the ball. If it misses it, the other player (on the left-hand side) will get 1
    point; similarly, if the other player misses it, the agent will get 1 point. The
    winner of the game is whoever scores 21 points first. The agent can take the following
    6 possible actions in the Pong environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0: NOOP**: The agent stays still'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1: FIRE**: Not a meaningful action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2: RIGHT**: The agent moves up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3: LEFT**: The agent moves down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4: RIGHTFIRE**: The same as 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5: LEFTFIRE**: The same as 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each action is repeatedly performed for a duration of *k* frames (*k* can be
    2, 3, 4, or 16, depending on the specific variant of the Pong environment). The
    reward is any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*-1*: The agent misses the ball.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1*: The opponent misses the ball.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*0*: Otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The observation space *210 * 160 * 3* in Pong is a lot larger than what we are
    used to dealing with. Therefore, we will downsize the image to *84 * 84* and convert
    it to grayscale, before using DQNs to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll begin by exploring the Pong environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Pong environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this Pong environment variant, an action is deterministic and is repeatedly
    performed for a duration of 16 frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the observation space and action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify three actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: These actions are not moving, moving up, and moving down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take random actions and render the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: You will see in the screen that two players are playing table tennis, even though
    the agent is losing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we develop a screen processing function to downsize the image and convert
    it to grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just define a resizer that downsizes the image to *84 * 84*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This function reshapes the resized image to size (1, 84, 84):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start solving the environment using double DQNs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a larger neural network with two hidden layers at this time, as
    the input size is around 21,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the `DQN` class is the same as the one in the, *Developing double
    deep Q-networks* recipe, with a small change to the `replay` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we''ll develop deep Q-learning with a double DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Given an observation of size *[210, 160, 3]*, this transforms it to a grayscale
    matrix of a smaller size *[84, 84]* and flattens it so that we can feed it into
    our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we specify the shape of the neural network, including the size of the
    input and hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining hyperparameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a DQN accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the buffer holding the experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we perform deep Q-learning and also keep track of the total rewards
    for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The observation in Pong is much more complicated than the environments we have
    worked with so far in this chapter. It is a three-channel image of *210 * 160*
    screen size. So, we first transform it into a grayscale image, downsize it to
    *84 * 84*, and then flatten it so that it can be fed into the fully connected
    neural network. As we have inputs of around 6,000 dimensions, we use two hidden
    layers to accommodate the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional neural networks for Atari games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we treated each observed image in the Pong environment
    as a grayscale array and fed it to a fully connected neural network. Flattening
    an image may actually result in information loss. Why don’t we use the image as
    input instead? In this recipe, we will incorporate **convolutional neural networks**
    (**CNNs**) into the DQN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN is one of the best neural network architectures to deal with image inputs.
    In a CNN, the convolutional layers are able to effectively extract features from
    images, which will be passed on to downstream, fully connected layers. An example
    of a CNN with two convolutional layers is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67e6e37a-cbce-468f-bbd7-d89a59f2355e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can imagine, if we simply flatten an image into a vector, we will lose
    some information on where the ball is located, and where the two players are.
    Such information is significant to the model learning. With convolutional operations
    in a CNN, such information about represented by a set of feature maps generated
    from multiple filters.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we downsize the image from *210 * 160* to *84 * 84*, but retain three
    RGB channels without flattening them into an array this time.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s solve the Pong environment using a CNN-based DQN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules and create a Pong environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we specify three actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: These actions are not moving, moving up, and moving down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we develop an image processing function to downsize the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We now define a resizer that downsizes the image to *84 * 84*, and then we
    reshape the image to (*3, 84, 84*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we start to solve the Pong environment by developing the CNN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use the CNN model we just defined in our `DQN` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the `DQN` class is the same as the one in the, *Developing double
    deep Q-networks* recipe, with a small change to the `replay` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we develop deep Q-learning with a double DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We then specify the remaining hyperparameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a DQN accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the buffer holding the experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we perform deep Q-learning and also keep track of the total rewards
    for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The image preprocessing function in *Step 3* first downsizes the image for each
    channel to *84 * 84*, and then it changes its dimensions to *(3, 84, 84)*. This
    is to make sure that the image with the right dimensions is fed to the network.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, the CNN model has three convolutional layers and a ReLU activation
    function that follows each convolutional layer. The resulting feature maps from
    the last convolutional layer are then flattened and fed to a fully connected hidden
    layer with 512 nodes, followed by the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating CNNs in DQNs was first introduced by DeepMind, as published in
    *Playing Atari with Deep Reinforcement Learning* *(*[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)).
    The model takes in image pixels as inputs and outputs estimated future reward
    values. It also works well for other Atari game environments where the observation
    is an image of the game screen. The convolutional components are a set of effective
    hierarchical feature extractors. They can learn the feature representations from
    raw image data in complex environments and feed them the fully connected layers
    to learn successful control policies.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the training in the preceding example usually takes a couple
    of days, even on a GPU, and around 90 hours on a 2.9 GHz Intel i7 Quad-Core CPU.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are not familiar with CNNs, please check out the following material:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4, *CNN Architecture* from *Hands-On Deep Learning Architectures with
    Python* (Packt Publishing, by Yuxi (Hayden) Liu and Saransh Mehta)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 1, *Handwritten Digit Recognition Using Convolutional Neural Networks**,*and
    Chapter 2, *Traffic Sign Recognition for Intelligent Vehicles* from *R Deep Learning
    Projects* (Packt Publishing, by Yuxi (Hayden) Liu and Pablo Maldonado)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
