- en: Deep Q-Networks in Action
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 网络的实际应用
- en: Deep Q-learning, or using deep Q-networks, is considered the most modern reinforcement
    learning technique. In this chapter, we will develop various deep Q-network models
    step by step and apply them to solve several reinforcement learning problems.
    We will start with vanilla Q-networks and enhance them with experience replay.
    We will improve robustness by using an additional target network and demonstrate
    how to fine-tune a Deep Q-Network. We will also experiment with dueling deep Q-networks
    and see how their value functions differs from other types of Deep Q-Networks.
    In the last two recipes, we will solve complex Atari game problems by incorporating
    convolutional neural networks into Deep Q-Networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习，或使用深度 Q 网络，被认为是最现代的强化学习技术。在本章中，我们将逐步开发各种深度 Q 网络模型，并将其应用于解决几个强化学习问题。我们将从基本的
    Q 网络开始，并通过经验重播来增强它们。我们将通过使用额外的目标网络来提高鲁棒性，并演示如何微调深度 Q 网络。我们还将尝试决斗深度 Q 网络，并看看它们的价值函数如何与其他类型的深度
    Q 网络不同。在最后两个实例中，我们将通过将卷积神经网络整合到深度 Q 网络中来解决复杂的 Atari 游戏问题。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: Developing deep Q-networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发深度 Q 网络
- en: Improving DQNs with experience replay
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过经验重播改进 DQN
- en: Developing double deep Q-Networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发双重深度 Q 网络
- en: Tuning double DQN hyperparameters for CartPole
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 CartPole 的双重 DQN 超参数
- en: Developing Dueling deep Q-Networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发决斗深度 Q 网络
- en: Applying deep Q-Networks to Atari games
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度 Q 网络应用于 Atari 游戏
- en: Using convolutional neural networks for Atari games
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络玩 Atari 游戏
- en: Developing deep Q-networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发深度 Q 网络
- en: You will recall that **Function Approximation** (**FA**) approximates the state
    space using a set of features generated from the original states. **Deep Q-Networks**
    (**DQNs**) are very similar to FA with neural networks, but they use neural networks
    to map the states to action values directly instead of using a set of generated
    features as media.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您将回忆起**函数逼近**（**FA**）是使用从原始状态生成的一组特征来逼近状态空间。**深度 Q 网络**（**DQN**）与使用神经网络进行特征逼近非常相似，但它们直接使用神经网络将状态映射到动作值，而不是使用生成的特征作为媒介。
- en: 'In Deep Q-learning, a neural network is trained to output the appropriate *Q(s,a)*
    values for each action given the input state, s. The action, a, of the agent is
    chosen based on the output Q(s,a) values following the epsilon-greedy policy.
    The structure of a DQN with two hidden layers is depicted in the following diagram:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度 Q 学习中，神经网络被训练以输出每个动作给定输入状态 s 下的适当 *Q(s,a)* 值。根据 epsilon-greedy 策略选择代理的动作
    a，基于输出 Q(s,a) 值。具有两个隐藏层的 DQN 结构如下图所示：
- en: '![](img/4217aa3a-58c0-4827-b4b2-4fbbd85d55ab.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4217aa3a-58c0-4827-b4b2-4fbbd85d55ab.png)'
- en: 'You will recall that Q-learning is an off-policy learning algorithm and that
    it updates the Q-function based on the following equation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您将回忆起 Q 学习是一种离线学习算法，并且它根据以下方程更新 Q 函数：
- en: '![](img/d2932660-72b6-40ed-9471-d558e4357238.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2932660-72b6-40ed-9471-d558e4357238.png)'
- en: 'Here, *s''* is the resulting state after taking action, *a*, in state, *s*;
    *r* is the associated reward; α is the learning rate; and γ is the discount factor.
    Also, [![](img/781b37bb-fdcf-4f8f-a59e-7438e8d6cf73.png)] means that the behavior
    policy is greedy where the highest Q-value among those in state *s''* is selected
    to generate learning data. Similarly, DQNs learn to minimize the following error
    term:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s'* 是在状态 *s* 中采取动作 *a* 后得到的结果状态；*r* 是相关的奖励；α 是学习率；γ 是折扣因子。同时，[![](img/781b37bb-fdcf-4f8f-a59e-7438e8d6cf73.png)]
    表示行为策略是贪婪的，其中在状态 *s'* 中选择最高的 Q 值以生成学习数据。类似地，DQN 学习以最小化以下误差项：
- en: '![](img/2e80a59e-1381-409a-a0fd-e477bf2e485f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e80a59e-1381-409a-a0fd-e477bf2e485f.png)'
- en: Now, the goal becomes one of finding the optimal network model to best approximate
    the state-value function, *Q(s, a)*, for each possible action. The loss function
    we are trying to minimize in this case is similar to that in a regression problem,
    which is the mean squared error between the actual value and the estimated value.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，目标变成寻找最佳网络模型以最好地逼近每个可能动作的状态值函数 *Q(s, a)*。在这种情况下，我们试图最小化的损失函数类似于回归问题中的均方误差，即实际值和估计值之间的均方误差。
- en: Now, we will develop a DQN model to solve the Mountain Car ([https://gym.openai.com/envs/MountainCar-v0/](https://gym.openai.com/envs/MountainCar-v0/))
    problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开发一个 DQN 模型来解决 Mountain Car（[https://gym.openai.com/envs/MountainCar-v0/](https://gym.openai.com/envs/MountainCar-v0/)）问题。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做……
- en: 'We develop deep Q-learning using DQN as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 DQN 开发深度 Q 学习如下：
- en: 'Import all the necessary packages:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The variable wraps a tensor and supports backpropagation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 变量包装了一个张量并支持反向传播。
- en: 'Let''s start with the `__init__` method of the `DQN` class:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从 `DQN` 类的 `__init__` 方法开始：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now develop the training method, which updates the neural network with a
    data point:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开发训练方法，用于更新神经网络与数据点：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next is the prediction of the state value for each action given a state:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是给定状态预测每个动作的状态值：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's all for the `DQN` class! And now we can move on to develop the learning
    algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `DQN` 类的全部内容！现在我们可以继续开发学习算法了。
- en: 'We begin by creating a Mountain Car environment:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始创建一个 Mountain Car 环境：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we define the epsilon-greedy policy:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义 epsilon-greedy 策略：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, define the deep Q-learning algorithm with DQN:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用 DQN 定义深度 Q 学习算法：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then specify the size of the hidden layer and the learning rate, and create
    a `DQN` instance accordingly:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定隐藏层的大小和学习率，并相应地创建一个 `DQN` 实例：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then perform Deep Q-learning with the DQN we just developed for 1,000 episodes
    and also keep track of the total (original) rewards for each episode:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用刚开发的 DQN 进行 1,000 个回合的深度 Q 学习，并且还跟踪每个回合的总（原始）奖励：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s display the plot of episode reward over time:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们展示随时间变化的回合奖励图：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理是...
- en: 'In *Step 2*, the `DQN` class takes in four parameters: the number of input
    states and output actions, the number of hidden nodes (we herein just use one
    hidden layer as an example), and the learning rate. It initializes a neural network
    with one hidden layer, followed by a ReLU activation function. It takes in `n_state`
    units and generates one `n_action` output, which are the predicted state values
    for individual actions. An optimizer, Adam, is initialized along with each linear
    model. The loss function is the mean squared error.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 2* 中，`DQN` 类接受四个参数：输入状态数和输出动作数，隐藏节点数（我们这里只使用一个隐藏层作为示例），以及学习率。它初始化一个具有一个隐藏层的神经网络，并使用
    ReLU 激活函数。它接收 `n_state` 个单位并生成一个 `n_action` 的输出，这些是各个动作的预测状态值。一个优化器 Adam 与每个线性模型一起初始化。损失函数是均方误差。
- en: '*Step 3* is for updating the network: given a training data point, the predictive
    result, along with the target value, is used to compute the loss and gradients.
    The neural network model is then updated via backpropagation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 3* 用于更新网络：给定一个训练数据点，使用预测结果和目标值计算损失和梯度。然后通过反向传播更新神经网络模型。'
- en: 'In *Step 7*, the deep Q-learning function does the following tasks:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 7* 中，深度 Q 学习函数执行以下任务：
- en: In each episode, creates an epsilon-greedy policy with an epsilon factor decayed
    to 99% (for example, if epsilon in the first episode is 0.1, it will be 0.099
    in the second episode). We also set 0.01 as the lower epsilon limit.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个回合中，创建一个 epsilon-greedy 策略，其中 epsilon 的因子衰减到 99%（例如，如果第一个回合中的 epsilon 是 0.1，则第二个回合中将为
    0.099）。我们还将 0.01 设置为较低的 epsilon 限制。
- en: 'Runs an episode: In each step in state *s*, takes an action, a, by following
    the epsilon-greedy policy; then, compute the *Q* values `q_value`, of the previous
    state using the `predict` method from DQN.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个回合：在状态 *s* 的每一步中，根据 epsilon-greedy 策略选择动作 *a*；然后，使用 DQN 的 `predict` 方法计算上一个状态的
    *Q* 值 `q_value`。
- en: Computes the *Q* values, `q_values_next`, for the new state, *s'*; then, computes
    the target value by updating the old *Q* values, `q_values`, for the action, ![](img/0f9dd526-9c3d-4a3c-b2de-b0a019e3a40d.png).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算新状态 *s'* 的 *Q* 值 `q_values_next`；然后，通过更新旧的 *Q* 值 `q_values` 来计算目标值，用于动作的 ![](img/0f9dd526-9c3d-4a3c-b2de-b0a019e3a40d.png)。
- en: Use the data point *(s, Q(s))*, to train the neural network. Note that *Q(s)*
    is composed of the values for all actions.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据点 *(s, Q(s))* 来训练神经网络。注意 *Q(s)* 包含所有动作的值。
- en: Runs `n_episode` episodes and records the total reward for each episode.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 `n_episode` 个回合并记录每个回合的总奖励。
- en: You may notice that we use a modified version of reward in training the model.
    It is based on the position of the car since we want it to reach the position
    of +0.5\. And we also give tiered incentives for positions greater than or equal
    to +0.5, +0.25, +0.1, and 0, respectively. This modified reward setting differentiates
    different car positions and favors positions that are closer to the goal; hence,
    it largely speeds up learning compared to the original monotonous -1 reward for
    each step.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到，我们在训练模型时使用了修改版奖励。它基于汽车的位置，因为我们希望它达到+0.5的位置。而且，我们还为大于或等于+0.5、+0.25、+0.1和0的位置提供分层激励。这种修改后的奖励设置区分了不同的汽车位置，并偏爱接近目标的位置；因此，与每步-1的原始单调奖励相比，它大大加速了学习过程。
- en: 'Finally, in *Step 10*, you will see the resulting plot as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*第10步*，您将看到如下的结果图：
- en: '![](img/88967993-441e-40e8-b5ce-597a690045a0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88967993-441e-40e8-b5ce-597a690045a0.png)'
- en: You can see that in the last 200 episodes, the car reaches the mountain top
    after around 170 to 180 steps.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在最后的200个回合中，汽车在大约170到180步后达到山顶。
- en: 'Deep Q-learning approximates the state values with a more direct model, a neural
    network, than using a set of intermediate artificial features. Given one step,
    where an old state transfers to a new state by taking an action and receives a
    reward, training the DQN involves the following phases:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习用神经网络而不是一组中间人工特征更直接地逼近状态值。给定一个步骤，其中旧状态通过采取行动转换为新状态，并接收奖励，训练DQN涉及以下阶段：
- en: Use the neural network model to estimate the *Q* values of the old state.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络模型估计旧状态的*Q*值。
- en: Use the neural network model to estimate the *Q* values of the new state.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络模型估计新状态的*Q*值。
- en: Update the target Q value for the action using the reward and the new *Q* values,
    as in [![](img/e7041f0b-3588-40ff-a5db-dc73000512f9.png)]
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用奖励和新的*Q*值更新动作的目标Q值，如[![](img/e7041f0b-3588-40ff-a5db-dc73000512f9.png)]
- en: Note that if it is a terminal state, the target Q value is updated as r.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，如果是终端状态，目标Q值将更新为r。
- en: Train the neural network model with the old state as the input, and the target
    *Q* values as the output.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用旧状态作为输入，并将目标*Q*值作为输出训练神经网络模型。
- en: It updates the weights for the network via gradient descent and can predict
    the Q values given a state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过梯度下降更新网络的权重，并能预测给定状态的Q值。
- en: DQN dramatically reduces the number of states to learn, where learning millions
    of states is not feasible in the TD method. Moreover, it directly maps the input
    state to the Q values, and does not require any additional functions to generate
    artificial features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DQN显著减少了需要学习的状态数量，在TD方法中学习数百万个状态是不可行的。此外，它直接将输入状态映射到Q值，并不需要生成人工特征的额外函数。
- en: See also
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you are not familiar with the Adam optimizer as an advanced gradient descent
    method, please check out the following material:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对Adam优化器作为高级梯度下降方法不熟悉，请查看以下资料：
- en: '[https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)'
- en: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
- en: Improving DQNs with experience replay
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用经验重播改进DQNs
- en: The approximation of Q-values using neural networks with one sample at a time
    is not very stable. You will recall that, in FA, we incorporated experience replay
    to improve stability. Similarly, in this recipe, we will apply experience replay
    to DQNs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络逐个样本逼近Q值的近似并不非常稳定。您将回忆起，在FA中，我们通过经验重播来提高稳定性。同样，在这个配方中，我们将应用经验重播到DQNs中。
- en: 'With experience replay, we store the agent''s experiences (an experience is
    composed of an old state, a new state, an action, and a reward) during episodes
    in a training session in a memory queue. Every time we gain sufficient experience,
    batches of experiences are randomly sampled from the memory and are used to train
    the neural network. Learning with experience replay becomes two phases: gaining
    experience, and updating models based on the past experiences randomly selected.
    Otherwise, the model will keep learning from the most recent experience and the
    neural network model could get stuck in a local minimum.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用经验回放，我们在训练会话的每个周期内存储代理的经验（一个经验由旧状态、新状态、动作和奖励组成）到内存队列中。每当我们积累到足够的经验时，从内存中随机抽取一批经验，并用于训练神经网络。学习经验回放分为两个阶段：积累经验和基于随机选择的过去经验更新模型。否则，模型将继续从最近的经验中学习，神经网络模型可能会陷入局部最小值。
- en: We will develop DQN with experience replay to solve the Mountain Car problem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发具有经验回放功能的DQN来解决山车问题。
- en: How to do it...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll develop a DQN with experience replay as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如下开发具有经验回放的DQN：
- en: 'Import the necessary modules and create a Mountain Car environment:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个山车环境：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To incorporate experience replay, we add a `replay` method to the `DQN` class:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要添加经验回放功能，我们将在`DQN`类中添加一个`replay`方法：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The rest of the `DQN` class remains unchanged.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`DQN`类的其余部分保持不变。'
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipeand will not repeat it here.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用我们在*开发深度Q网络*配方中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set 0.001 as the learning rate, and create
    a DQN accordingly:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定神经网络的形状，包括输入的大小、输出和隐藏层的大小，将学习率设置为0.001，并相应地创建一个DQN：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we define the buffer holding the experience:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义存储经验的缓冲区：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果队列中的样本超过`10000`个，则将新样本附加到队列中，并删除旧样本。
- en: 'Now, we define the deep Q-learning function that performs experience replay:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义执行经验回放的深度Q学习函数：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then perform deep Q-learning with experience replay for `600` episodes:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为`600`个周期执行具有经验回放的深度Q学习：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We set `20` as the replay sample size for each step:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`20`设置为每步的重放样本大小：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We also keep track of the total rewards for each episode:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还跟踪每个周期的总奖励：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, it is time to display the plot of episode rewards over time:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候显示随时间变化的周期奖励图了：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In *Step 2*, the experience replay function first randomly selects `replay_size`
    samples of experience. It then converts each experience into a training sample
    composed of the input state and output target values. And finally, it updates
    the neural network using the selected batch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 2 步*中，经验回放函数首先随机选择`replay_size`个经验样本。然后，将每个经验转换为由输入状态和输出目标值组成的训练样本。最后，使用选定的批量更新神经网络。
- en: 'In *Step 6*, deep Q-learning with experience replay is performed with the following
    tasks:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 6 步*中，执行具有经验回放的深度Q学习，包括以下任务：
- en: In each episode, create an epsilon-greedy policy with an epsilon factor decayed
    to 99%.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个周期中，创建一个带有衰减到99%的ε贪婪策略。
- en: 'Run an episode: in each step, take an action, *a*, following the epsilon-greedy
    policy; store this experience (old state, action, new state, reward) in memory.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个周期：在每一步中，根据ε-贪婪策略选择一个动作*a*；将这一经验（旧状态、动作、新状态、奖励）存储在内存中。
- en: In each step, conduct experience replay to train the neural network, provided
    we have sufficient training samples to randomly pick from.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步中，进行经验回放来训练神经网络，前提是我们有足够的训练样本可以随机选择。
- en: Run `n_episode` episodes and record the total reward for each episode.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`n_episode`个周期，并记录每个周期的总奖励。
- en: 'Executing the lines of code in *Step 8* will result in the following plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*第 8 步*中的代码行将产生以下图表：
- en: '![](img/a5b6cc17-88d7-40e7-9a0d-529a6f7ee760.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5b6cc17-88d7-40e7-9a0d-529a6f7ee760.png)'
- en: You can see that, in most episodes in the last 200 episodes, the car reaches
    the mountain top in around 120 to 160 steps.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在最后200个周期的大多数周期中，车在大约120到160步内达到了山顶。
- en: 'In deep Q-learning, experience replay means we store the agent''s experience
    for each step and randomly draw some samples of past experience to train the DQN.
    The learning in this case is split into two phases: accumulating experience, and
    updating models based on batches of past experience. Specifically, the experience
    (also called the **buffer**, or **memory**) includes the past state, the action
    taken, the reward received, and the next state. Experience replay can stabilize
    training by providing a set of samples with low correlation, which, as a result,
    increases learning efficiency.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度Q学习中，经验重放意味着我们为每一步存储代理的经验，并随机抽取过去经验的一些样本来训练DQN。 在这种情况下，学习分为两个阶段：积累经验和基于过去经验批次更新模型。
    具体而言，经验（也称为**缓冲区**或**内存**）包括过去的状态、采取的动作、获得的奖励和下一个状态。 经验重放可以通过提供一组低相关性的样本来稳定训练，从而增加学习效率。
- en: Developing double deep Q-Networks
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发双深度Q网络
- en: In the deep Q-learning algorithms we have developed so far, the same neural
    network is used to calculate the predicted values and the target values. This
    may cause a lot of divergence as the target values keep on changing and the prediction
    has to chase it. In this recipe, we will develop a new algorithm using two neural
    networks instead of one.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今开发的深度Q学习算法中，同一个神经网络用于计算预测值和目标值。 这可能导致很多发散，因为目标值不断变化，而预测必须追赶它。 在这个配方中，我们将开发一种新的算法，使用两个神经网络代替一个。
- en: In **double DQNs**, we use a separate network to estimate the target rather
    than the prediction network. The separate network has the same structure as the
    prediction network. And its weights are fixed for every *T* episode (*T* is a
    hyperparameter we can tune), which means they are only updated after every *T*
    episode. The update is simply done by copying the weights of the prediction network.
    In this way, the target function is fixed for a while, which results in a more
    stable training process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在**双重DQN**中，我们使用一个单独的网络来估计目标，而不是预测网络。 这个单独的网络与预测网络具有相同的结构。 其权重在每个*T*集后固定（*T*是我们可以调整的超参数），这意味着它们仅在每个*T*集后更新。
    更新是通过简单地复制预测网络的权重来完成的。 这样，目标函数在一段时间内保持不变，从而导致更稳定的训练过程。
- en: 'Mathematically, double DQNs are trained to minimize the following error term:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，双重DQN被训练来最小化以下误差项：
- en: '![](img/083d4882-dad9-48b9-bb1c-7389e117c3eb.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/083d4882-dad9-48b9-bb1c-7389e117c3eb.png)'
- en: Here, *s'* is the resulting state after taking action, *a*, in state *s*; *r*
    is the associated reward; α is the learning rate; and γ is the discount factor.
    Also, [![](img/86da76aa-fe81-4d29-868c-a2dac845bea2.png)] is the function for
    the target network, and Q is the function for the prediction network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s'*是采取行动*a*后的结果状态，*r*是相关的奖励；α是学习率；γ是折扣因子。 另外，[![](img/86da76aa-fe81-4d29-868c-a2dac845bea2.png)]是目标网络的函数，而Q是预测网络的函数。
- en: Let’s now solve the Mountain Car problem using double DQNs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用双重DQN来解决Mountain Car问题。
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We develop deep Q-learning using double DQNs as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式开发使用双重DQN的深度Q学习：
- en: 'Import the necessary modules and create a Mountain Car environment:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个Mountain Car环境：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To incorporate the target network in the experience replay phase, we first
    initialize it in the `__init__` method of the `DQN` class:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在经验重放阶段，为了整合目标网络，我们首先在`DQN`类的`__init__`方法中对其进行初始化：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The target network has the same structure as the prediction network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络与预测网络具有相同的结构。
- en: 'Accordingly, we add the calculation of values using the target network:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们添加了使用目标网络计算值的计算：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We also add the method to synchronize the weights of the target network:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还添加了同步目标网络权重的方法：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In experience replay, we use the target network to calculate the target value
    instead of the prediction network:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在经验重放中，我们使用目标网络来计算目标值，而不是预测网络：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The rest of the `DQN` class remains unchanged.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`DQN` 类的其余部分保持不变。'
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用我们在《深度Q网络开发》配方中开发的`gen_epsilon_greedy_policy`函数，并且这里不会重复它。
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set `0.01` as the learning rate, and
    create a DQN accordingly:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们指定神经网络的形状，包括输入的大小、输出和隐藏层的大小，将`0.01`作为学习率，并相应地创建一个DQN：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we define the buffer holding the experience:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义保存经验的缓冲区：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 只要队列中有超过`10000`个样本，新样本将被追加到队列中，并移除旧样本。
- en: 'Now, we''ll develop deep Q-learning with double DQN:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开发双DQN的深度Q学习：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We perform deep Q-learning with double DQN for `1000` episodes:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行双DQN的深度Q学习，共进行`1000`个回合：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We set `20` as the replay sample size for each step:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每一步的回放样本大小设置为`20`：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We update the target network for every 10 episodes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每10个回合更新一次目标网络：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also keep track of the total rewards for each episode:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会跟踪每个回合的总奖励：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then display the plot of episode rewards over time:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们显示随时间变化的回合奖励图：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In *Step 5*, the experience replay function first randomly selects `replay_size`
    samples of experience. It then converts each experience into a training sample
    composed of the input state and output target values. And finally, it updates
    the prediction network using the selected batch.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Step 5*中，经验回放函数首先随机选择`replay_size`个样本经验。然后将每个经验转换为由输入状态和输出目标值组成的训练样本。最后，使用选定的批次更新预测网络。
- en: '*Step 9* is the most important step in a double DQN: it uses a different network
    to calculate the target values and then this network is updated periodically.
    The rest of the function is similar to deep Q-learning with experience replay.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 9*是双DQN中最重要的步骤：它使用不同的网络计算目标值，然后定期更新这个网络。函数的其余部分类似于带经验回放的深度Q学习。'
- en: 'The visualization function in *Step 11* will result in the following plot:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 11*中的可视化函数将生成以下图表：'
- en: '![](img/ec4c795b-ebcd-4389-a3c5-8c0431c6f9bb.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec4c795b-ebcd-4389-a3c5-8c0431c6f9bb.png)'
- en: You can see that, in most episodes, after the first **400** episodes, the car
    reaches the mountain top in around **80** to **160** steps.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在大多数情况下，经过第一个**400**个回合后，小车在大约**80**到**160**步内到达山顶。
- en: In deep Q-learning with a double DQN, we create two separate networks for prediction
    and target calculation, respectively. The first one is used to predict and retrieve
    *Q* values, while the second one is used to provide a stable target *Q* value.
    And, after a while (let's say every 10 episodes, or 1,500 training steps), we
    synchronize the prediction network with the target network. In this double network
    setting, the target values are temporarily fixed instead of being modified constantly,
    so the prediction network has more stable targets to learn against. The results
    we obtained show that double DQNs outperform single DQNs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在双DQN的深度Q学习中，我们分别创建两个用于预测和目标计算的网络。第一个网络用于预测和检索*Q*值，而第二个网络用于提供稳定的目标*Q*值。并且，经过一段时间（比如每10个回合或1500个训练步骤），我们同步预测网络和目标网络。在这种双网络设置中，目标值是暂时固定的，而不是被不断修改的，因此预测网络有更稳定的目标来学习。我们获得的结果表明，双DQN优于单DQN。
- en: Tuning double DQN hyperparameters for CartPole
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为CartPole调优双DQN超参数
- en: In this recipe, let's solve the CartPole environment using double DQNs. We will
    demonstrate how to fine-tune the hyperparameters in a double DQN to achieve the
    best performance.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，让我们使用双DQN解决CartPole环境。我们将展示如何调优双DQN的超参数以达到最佳性能。
- en: 'In order to fine-tune the hyperparameters, we can apply the **grid search**
    technique to explore a set of different combinations of values and pick the one
    achieving the best average performance. We can start with a coarse range of values
    and continue to narrow it down gradually. And don’t forget to fix the random number
    generators for all of the following in order to ensure reproducibility:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调优超参数，我们可以应用**网格搜索**技术来探索一组不同的值组合，并选择表现最佳的那一组。我们可以从粗范围的值开始，并逐渐缩小范围。并且不要忘记为所有后续的随机数生成器固定种子，以确保可重现性：
- en: The Gym environment random number generator
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gym环境随机数生成器
- en: The epsilon-greedy random number generator
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ε-贪心随机数生成器
- en: The initial weights for the neural network in PyTorch
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch神经网络的初始权重
- en: How to do it...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'We solve the CartPole environment using double DQNs as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用双DQN解决CartPole环境如下：
- en: 'Import the necessary modules and create a CartPole environment:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个CartPole环境：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We will reuse the `DQN` class developed in the last, *Developing double deep
    Q-networks* recipe.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用上一个*开发双深度Q网络*示例中开发的`DQN`类。
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用在《深度Q网络开发》食谱中开发的`gen_epsilon_greedy_policy`函数，并且不在这里重复。
- en: 'Now, we''ll develop deep Q-learning with a double DQN:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用双重DQN开发深度Q学习：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, the hidden layer, and the number of episodes, as well as the
    number of episodes used to evaluate the performance:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定神经网络的形状，包括输入的大小、输出的大小、隐藏层的大小和周期数，以及用于评估性能的周期数：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then define a few values for the following hyperparameters to explore in
    a grid search:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为以下超参数定义了几个值，以便在网格搜索中探索：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we perform a grid search where, in each iteration, we create a DQN
    according to the set of hyperparameters and allow it to learn for 600 episodes.
    We then evaluate its performance by averaging the total rewards for the last 200
    episodes:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行一个网格搜索，在每次迭代中，我们根据一组超参数创建一个DQN，并允许其学习600个周期。然后，通过对最后200个周期的总奖励进行平均来评估其性能：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Having executed *Step 7*, we get the following grid search results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 执行了*第7步*后，我们得到了以下网格搜索结果：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can see that the best average reward, `192.77`, is achieved with the combination
    of `n_hidden=30`, `lr=0.001`, `replay_size=25`, and `target_update=35`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过`n_hidden=30`、`lr=0.001`、`replay_size=25`和`target_update=35`的组合，我们获得了最佳的平均奖励，`192.77`。
- en: Feel free to fine-tune the hyperparameters further in order to get a better
    DQN model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 随意进一步微调超参数，以获得更好的DQN模型。
- en: In this recipe, we solved the CartPole problem with double DQNs. We fine-tuned
    the values of hyperparameters using a grid search. In our example, we optimized
    the size of the hidden layer, the learning rate, the replay batch size, and the
    target network update frequency. There are other hyperparameters we could also
    explore, such as the number of episodes, the initial epsilon, and the value of
    epsilon decay. For each experiment, we kept the random seeds fixed so that the
    randomness of the Gym environment, and the epsilon-greedy action, as well as the
    weight initialization of the neural network, remain the same. This is to ensure
    the reproducibility and comparability of performance. The performance of each
    DQN model is measured by the average total reward for the last few episodes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用双重DQNs解决了CartPole问题。我们使用网格搜索对超参数的值进行了微调。在我们的示例中，我们优化了隐藏层的大小、学习率、回放批量大小和目标网络更新频率。还有其他超参数我们也可以探索，如周期数、初始epsilon值和epsilon衰减值。为了确保实验的可重复性和可比性，我们保持了随机种子固定，使得Gym环境的随机性、epsilon-greedy动作以及神经网络的权重初始化保持不变。每个DQN模型的性能是通过最后几个周期的平均总奖励来衡量的。
- en: Developing Dueling deep Q-Networks
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发对抗深度Q网络
- en: In this recipe, we are going to develop another advanced type of DQNs, **Dueling
    DQNs** (**DDQNs**). In particularly, we will see how the computation of the Q
    value is split into two parts in DDQNs.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将开发另一种高级DQN类型，**对抗DQNs**（**DDQNs**）。特别是，我们将看到在DDQNs中如何将Q值的计算分为两部分。
- en: 'In DDQNs, the Q value is computed with the following two functions:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDQNs中，Q值由以下两个函数计算：
- en: '![](img/2c71596f-ff5e-49b2-9522-e0181e979d78.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c71596f-ff5e-49b2-9522-e0181e979d78.png)'
- en: Here, *V(s)* is the state-value function, calculating the value of being at
    state *s*; *A(s, a)* is the state-dependent action advantage function, estimating
    how much better it is to take an action, *a*, rather than taking other actions
    at a state, *s*. By decoupling the `value` and `advantage` functions, we are able
    to accommodate the fact that our agent may not necessarily look at both the value
    and advantage at the same time during the learning process. In other words, the
    agent using DDQNs can efficiently optimize either or both functions as it prefers.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V(s)*是状态值函数，计算处于状态*s*时的值；*A(s, a)*是状态相关的动作优势函数，估计采取动作*a*相比于在状态*s*下采取其他动作更好多少。通过解耦`value`和`advantage`函数，我们能够适应我们的代理在学习过程中可能不一定同时查看值和优势的事实。换句话说，使用DDQNs的代理可以根据其偏好有效地优化任一或两个函数。
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We solve the Mountain Car problem using DDQNs as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用DDQNs解决Mountain Car问题如下：
- en: 'Import the necessary modules and create a Mountain Car environment:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个Mountain Car环境：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we define the DDQN model as follows:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们按以下方式定义DDQN模型：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Accordingly, we use the DDQN model in the `DQN` class:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们在`DQN`类中使用DDQN模型：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The rest of the `DQN` class remains unchanged.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`DQN`类的其余部分保持不变。'
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing deep Q-Networks* recipe, and will not repeat it here.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用我们在*开发深度Q-Networks*配方中开发的`gen_epsilon_greedy_policy`函数，并且这里不会重复。
- en: We will reuse the `q_learning` function we developed in the, *Improving DQNs
    with experience replay* recipe, and will not repeat it here.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用我们在*通过经验重播改进DQNs*配方中开发的`q_learning`函数，并且这里不会重复。
- en: 'We then specify the shape of the neural network, including the size of the
    input, the output, and the hidden layer, set `0.001` as the learning rate, and
    create a DQN accordingly:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后指定神经网络的形状，包括输入的大小，输出和隐藏层，将`0.001`设置为学习率，并相应创建一个DQN模型：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we define the buffer holding the experience:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义保存经验的缓冲区：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than `10000` samples in the queue.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 新样本将被添加到队列中，并且只要队列中有超过`10000`个样本，旧样本就会被删除。
- en: 'We then perform Deep Q-learning with DDQN for `600` episodes:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们执行包含DDQN的Deep Q-learning，进行了`600`个剧集：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We set `20` as the replay sample size for each step:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每步设置为`20`作为回放样本大小：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We also keep track of the total rewards for each episode:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会跟踪每一集的总奖励：
- en: '[PRE45]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, we can display the plot of episode rewards over time:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以显示随时间变化的剧集奖励的绘图：
- en: '[PRE46]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '*Step 2* is the core part of the Dueling DQN. It is composed of two parts,
    the action **advantage** (**adv**), and the **state-value** (**val**). Again,
    we use one hidden layer as an example.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 2*是Dueling DQN的核心部分。它由两部分组成，动作**优势**（**adv**）和**状态值**（**val**）。同样，我们使用一个隐藏层作为示例。'
- en: 'Executing *Step 9* will result in the following plot:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*Step 9*将导致以下绘图：
- en: '![](img/1b252d17-a5a3-4320-b706-357a6844d87a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b252d17-a5a3-4320-b706-357a6844d87a.png)'
- en: 'In DDQNs, the predicted Q value is the result of two elements: state-value
    and action advantage. The first one estimates how good it is to be at a certain
    state. The second one indicates how much better it is to take a particular action
    as opposed to the alternatives. These two elements are computed separately and
    combined into the last layer of the DQN. You will recall that traditional DQNs
    update the Q value for a given action at a state only. DDQNs update the state-value
    that all actions (not just the given one) can take advantage of, as well as the
    advantage for the action. Hence, DDQNs are thought to be more robust.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDQNs中，预测的Q值由两个元素组成：状态值和动作优势。第一个估计在某个状态下的表现有多好。第二个指示相比其他选择，采取特定动作有多好。这两个元素分别计算并结合到DQN的最后一层。您将记得，传统的DQNs只更新给定状态下某个动作的Q值。DDQNs更新所有动作（而不仅仅是给定动作）可以利用的状态值，以及动作的优势。因此，认为DDQNs更加稳健。
- en: Applying Deep Q-Networks to Atari games
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Deep Q-Networks应用于Atari游戏
- en: The problems we have worked with so far are fairly simple, and applying DQNs
    is sometimes overkill. In this and the next recipe, we'll use DQNs to solve Atari
    games, which are far more complicated problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的问题相当简单，有时应用DQNs可能有点过头了。在这个和下一个配方中，我们将使用DQNs来解决Atari游戏，这些游戏问题要复杂得多。
- en: 'We will use Pong ([https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/))
    as an example in this recipe. It simulates the Atari 2600 game Pong, where the
    agent plays table tennis with another player. The observation in this environment
    is an RGB image of the screen (refer to the following screenshot):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个配方中以Pong ([https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/))为例。它模拟了Atari
    2600游戏Pong，代理与另一玩家打乒乓球。这个环境的观察是屏幕的RGB图像（参考下面的截图）：
- en: '![](img/8a405812-c25f-4905-8952-45313c8aeccf.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a405812-c25f-4905-8952-45313c8aeccf.png)'
- en: This is a matrix of shape (210, 160, 3), which means that the image is of size
    *210 * 160* and in three RGB channels.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个形状为（210，160，3）的矩阵，意味着图像的大小为*210 * 160*，有三个RGB通道。
- en: 'The agent (on the right-hand side) moves up and down during the game to hit
    the ball. If it misses it, the other player (on the left-hand side) will get 1
    point; similarly, if the other player misses it, the agent will get 1 point. The
    winner of the game is whoever scores 21 points first. The agent can take the following
    6 possible actions in the Pong environment:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 代理（右侧）在比赛中上下移动以击打球。如果错过了，另一名玩家（左侧）将获得1分；同样，如果另一名玩家错过了球，代理将获得1分。比赛的胜者是首先得到21分的人。代理可以在Pong环境中采取以下6种可能的动作：
- en: '**0: NOOP**: The agent stays still'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0: NOOP**: 代理保持静止'
- en: '**1: FIRE**: Not a meaningful action'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1: FIRE**: 不是一个有意义的动作'
- en: '**2: RIGHT**: The agent moves up'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2: RIGHT**: 代理向上移动'
- en: '**3: LEFT**: The agent moves down'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3: LEFT**: 代理向下移动'
- en: '**4: RIGHTFIRE**: The same as 2'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4: RIGHTFIRE**: 与2相同'
- en: '**5: LEFTFIRE**: The same as 5'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5: LEFTFIRE**: 与5相同'
- en: 'Each action is repeatedly performed for a duration of *k* frames (*k* can be
    2, 3, 4, or 16, depending on the specific variant of the Pong environment). The
    reward is any of the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 每个动作都会在*k*帧的持续时间内重复执行（*k*可以是2、3、4或16，取决于Pong环境的具体变体）。奖励可以是以下任意一种：
- en: '*-1*: The agent misses the ball.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*-1*: 代理错过球。'
- en: '*1*: The opponent misses the ball.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1*: 对手错过球。'
- en: '*0*: Otherwise.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*0*: 否则。'
- en: The observation space *210 * 160 * 3* in Pong is a lot larger than what we are
    used to dealing with. Therefore, we will downsize the image to *84 * 84* and convert
    it to grayscale, before using DQNs to solve it.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Pong中的观察空间*210 * 160 * 3*比我们通常处理的要大得多。因此，我们将把图像缩小到*84 * 84*并转换为灰度，然后使用DQNs来解决它。
- en: How to do it...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做…
- en: 'We''ll begin by exploring the Pong environment as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从以下内容开始探索Pong环境：
- en: 'Import the necessary modules and create a Pong environment:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建一个Pong环境：
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this Pong environment variant, an action is deterministic and is repeatedly
    performed for a duration of 16 frames.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Pong环境的变体中，一个动作是确定性的，并且在16帧的持续时间内重复执行。
- en: 'Take a look at the observation space and action space:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看观察空间和动作空间：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Specify three actions:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定三个动作：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: These actions are not moving, moving up, and moving down.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这些动作分别是不移动、向上移动和向下移动。
- en: 'Let''s take random actions and render the screen:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们采取随机动作并渲染屏幕：
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You will see in the screen that two players are playing table tennis, even though
    the agent is losing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在屏幕上看到两名玩家在打乒乓球，即使代理正在输。
- en: 'Now, we develop a screen processing function to downsize the image and convert
    it to grayscale:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开发一个屏幕处理函数来缩小图像并将其转换为灰度：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we just define a resizer that downsizes the image to *84 * 84*:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需定义一个调整图像大小至*84 * 84*的调整器：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This function reshapes the resized image to size (1, 84, 84):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将调整大小后的图像重塑为大小为(1, 84, 84)：
- en: '[PRE53]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we can start solving the environment using double DQNs as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用双DQNs开始解决环境，如下所示：
- en: 'We will use a larger neural network with two hidden layers at this time, as
    the input size is around 21,000:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们将使用一个较大的神经网络，有两个隐藏层，因为输入大小约为21,000：
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The rest of the `DQN` class is the same as the one in the, *Developing double
    deep Q-networks* recipe, with a small change to the `replay` method:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DQN`类的其余部分与*Developing double deep Q-networks*食谱中的一样，只是对`replay`方法进行了小的更改：'
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用我们在*Developing Deep Q-Networks*食谱中开发的`gen_epsilon_greedy_policy`函数，并且这里不再重复。
- en: 'Now, we''ll develop deep Q-learning with a double DQN:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将开发带有双DQN的深度Q学习：
- en: '[PRE56]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Given an observation of size *[210, 160, 3]*, this transforms it to a grayscale
    matrix of a smaller size *[84, 84]* and flattens it so that we can feed it into
    our network.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 给定大小为*[210, 160, 3]*的观察结果，将其转换为更小尺寸的灰度矩阵*[84, 84]*并将其扁平化，以便我们可以将其馈送到我们的网络中。
- en: 'Now, we specify the shape of the neural network, including the size of the
    input and hidden layers:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们指定神经网络的形状，包括输入和隐藏层的大小：
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The remaining hyperparameters are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的超参数如下：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, we create a DQN accordingly:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们相应地创建一个DQN：
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we define the buffer holding the experience:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义保存经验的缓冲区：
- en: '[PRE60]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Finally, we perform deep Q-learning and also keep track of the total rewards
    for each episode:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行深度Q学习，并跟踪每个episode的总奖励：
- en: '[PRE61]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works...
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The observation in Pong is much more complicated than the environments we have
    worked with so far in this chapter. It is a three-channel image of *210 * 160*
    screen size. So, we first transform it into a grayscale image, downsize it to
    *84 * 84*, and then flatten it so that it can be fed into the fully connected
    neural network. As we have inputs of around 6,000 dimensions, we use two hidden
    layers to accommodate the complexity.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Pong 中的观察情况比本章中迄今为止我们处理过的环境复杂得多。它是一个 *210 * 160* 屏幕尺寸的三通道图像。因此，我们首先将其转换为灰度图像，将其缩小为
    *84 * 84*，然后展平，以便馈送到全连接神经网络中。由于输入维度约为 6000，我们使用两个隐藏层来适应复杂性。
- en: Using convolutional neural networks for Atari games
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Atari 游戏中使用卷积神经网络
- en: In the previous recipe, we treated each observed image in the Pong environment
    as a grayscale array and fed it to a fully connected neural network. Flattening
    an image may actually result in information loss. Why don’t we use the image as
    input instead? In this recipe, we will incorporate **convolutional neural networks**
    (**CNNs**) into the DQN model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们将 Pong 环境中的每个观察图像视为灰度数组，并将其馈送到全连接神经网络中。将图像展平可能会导致信息丢失。为什么不直接使用图像作为输入呢？在这篇文章中，我们将**卷积神经网络**（**CNNs**）集成到
    DQN 模型中。
- en: 'A CNN is one of the best neural network architectures to deal with image inputs.
    In a CNN, the convolutional layers are able to effectively extract features from
    images, which will be passed on to downstream, fully connected layers. An example
    of a CNN with two convolutional layers is depicted here:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是处理图像输入的最佳神经网络架构之一。在 CNN 中，卷积层能够有效地从图像中提取特征，这些特征将传递给下游的全连接层。下图展示了一个具有两个卷积层的
    CNN 示例：
- en: '![](img/67e6e37a-cbce-468f-bbd7-d89a59f2355e.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67e6e37a-cbce-468f-bbd7-d89a59f2355e.png)'
- en: As you can imagine, if we simply flatten an image into a vector, we will lose
    some information on where the ball is located, and where the two players are.
    Such information is significant to the model learning. With convolutional operations
    in a CNN, such information about represented by a set of feature maps generated
    from multiple filters.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的，如果我们简单地将图像展平成一个向量，我们将丢失一些关于球和两名玩家位置的信息。这些信息对于模型学习至关重要。在 CNN 中的卷积操作中，多个滤波器生成的一组特征映射可以捕捉到这些信息。
- en: Again, we downsize the image from *210 * 160* to *84 * 84*, but retain three
    RGB channels without flattening them into an array this time.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将图像从 *210 * 160* 缩小到 *84 * 84*，但这次保留三个 RGB 通道，而不是将它们展平成数组。
- en: How to do it...
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'Let''s solve the Pong environment using a CNN-based DQN as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用基于 CNN 的 DQN 来解决 Pong 环境，如下所示：
- en: 'Import the necessary modules and create a Pong environment:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块并创建 Pong 环境：
- en: '[PRE62]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, we specify three actions:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定三个动作：
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: These actions are not moving, moving up, and moving down.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这些动作是不动、向上移动和向下移动。
- en: 'Now, we develop an image processing function to downsize the image:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开发一个图像处理函数来缩小图像：
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We now define a resizer that downsizes the image to *84 * 84*, and then we
    reshape the image to (*3, 84, 84*):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个调整器，将图像缩小为 *84 * 84*，然后将图像重塑为 (*3, 84, 84*)：
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, we start to solve the Pong environment by developing the CNN model:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开始通过开发 CNN 模型来解决 Pong 环境：
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We will now use the CNN model we just defined in our `DQN` model:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用刚刚在我们的`DQN`模型中定义的 CNN 模型：
- en: '[PRE67]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The rest of the `DQN` class is the same as the one in the, *Developing double
    deep Q-networks* recipe, with a small change to the `replay` method:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DQN`类的其余部分与“开发双重深度 Q 网络”一章中的相同，只是`replay`方法略有改变：'
- en: '[PRE68]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We will reuse the `gen_epsilon_greedy_policy` function we developed in the,
    *Developing Deep Q-Networks* recipe, and will not repeat it here.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用我们在“开发深度 Q 网络”一章中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
- en: 'Now, we develop deep Q-learning with a double DQN:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用双重 DQN 开发深度 Q-learning：
- en: '[PRE69]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We then specify the remaining hyperparameters as follows:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将剩余的超参数指定如下：
- en: '[PRE70]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Create a DQN accordingly:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需要创建一个 DQN：
- en: '[PRE71]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, we define the buffer holding the experience:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义保存经验的缓冲区：
- en: '[PRE72]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Finally, we perform deep Q-learning and also keep track of the total rewards
    for each episode:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行深度 Q-learning，并追踪每个周期的总奖励：
- en: '[PRE73]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: How it works...
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The image preprocessing function in *Step 3* first downsizes the image for each
    channel to *84 * 84*, and then it changes its dimensions to *(3, 84, 84)*. This
    is to make sure that the image with the right dimensions is fed to the network.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 3* 中的图像预处理函数首先将每个通道的图像缩小到 *84 * 84*，然后将其尺寸更改为 *(3, 84, 84)*。这是为了确保将具有正确尺寸的图像输入到网络中。'
- en: In *Step 4*, the CNN model has three convolutional layers and a ReLU activation
    function that follows each convolutional layer. The resulting feature maps from
    the last convolutional layer are then flattened and fed to a fully connected hidden
    layer with 512 nodes, followed by the output layer.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中，CNN 模型有三个卷积层和一个 ReLU 激活函数，每个卷积层后都跟随着。最后一个卷积层产生的特征映射然后被展平并输入到具有 512
    个节点的全连接隐藏层，然后是输出层。
- en: Incorporating CNNs in DQNs was first introduced by DeepMind, as published in
    *Playing Atari with Deep Reinforcement Learning* *(*[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)).
    The model takes in image pixels as inputs and outputs estimated future reward
    values. It also works well for other Atari game environments where the observation
    is an image of the game screen. The convolutional components are a set of effective
    hierarchical feature extractors. They can learn the feature representations from
    raw image data in complex environments and feed them the fully connected layers
    to learn successful control policies.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 将 CNNs 结合到 DQNs 中首先由 DeepMind 提出，并发表在 *Playing Atari with Deep Reinforcement
    Learning*（[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)）。该模型以图像像素作为输入，并输出估计的未来奖励值。它还适用于其他
    Atari 游戏环境，其中观察是游戏屏幕的图像。卷积组件是一组有效的分层特征提取器，它们可以从复杂环境中的原始图像数据中学习特征表示，并通过全连接层学习成功的控制策略。
- en: Keep in mind that the training in the preceding example usually takes a couple
    of days, even on a GPU, and around 90 hours on a 2.9 GHz Intel i7 Quad-Core CPU.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，即使在 GPU 上，前面示例中的训练通常也需要几天时间，在 2.9 GHz 英特尔 i7 四核 CPU 上大约需要 90 小时。
- en: See also
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you are not familiar with CNNs, please check out the following material:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对 CNN 不熟悉，请查看以下资料：
- en: Chapter 4, *CNN Architecture* from *Hands-On Deep Learning Architectures with
    Python* (Packt Publishing, by Yuxi (Hayden) Liu and Saransh Mehta)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hands-On Deep Learning Architectures with Python*（Packt Publishing，作者：刘宇熙（Hayden）和萨兰什·梅塔），第
    4 章，*CNN 架构*。'
- en: Chapter 1, *Handwritten Digit Recognition Using Convolutional Neural Networks**,*and
    Chapter 2, *Traffic Sign Recognition for Intelligent Vehicles* from *R Deep Learning
    Projects* (Packt Publishing, by Yuxi (Hayden) Liu and Pablo Maldonado)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R Deep Learning Projects*（Packt Publishing，作者：刘宇熙（Hayden）和帕布罗·马尔多纳多），第 1 章，*使用卷积神经网络识别手写数字*，和第
    2 章，*智能车辆的交通标志识别*。'
