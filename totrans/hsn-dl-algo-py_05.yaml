- en: Gradient Descent and Its Variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent is one of the most popular and widely used optimization algorithms,
    and is a first-order optimization algorithm. First-order optimization means that
    we calculate only the first-order derivative. As we saw in [Chapter 1](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml),
    *Introduction to Deep Learning*, we used gradient descent and calculated the first-order
    derivative of the loss function with respect to the weights of the network to
    minimize the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is not only applicable to neural networks—it is also used in
    situations where we need to find the minimum of a function. In this chapter, we
    will go deeper into gradient descent, starting with the basics, and learn several
    variants of gradient descent algorithms. There are various flavors of gradient
    descent that are used for training neural networks. First, we will understand
    **Stochastic Gradient Descent** (**SGD**) and mini-batch gradient descent. Then,
    we'll explore how momentum is used to speed up gradient descent to attain convergence.
    Later in this chapter, we will learn about how to perform gradient descent in
    an adaptive manner by using various algorithms, such as Adagrad, Adadelta, RMSProp,
    Adam, Adamax, AMSGrad, and Nadam. We will take a simple linear regression equation
    and see how we can find the minimum of a linear regression's cost function using
    various types of gradient descent algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent versus stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Momentum and Nesterov accelerated gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive methods of gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the details, let's understand the basics. What is a function
    in mathematics? A function represents the relation between input and output. We
    generally use ![](img/340fb1cd-80df-4c80-9d3e-cf73319d7dec.png) to denote a function.
    For instance, ![](img/0fae6909-055b-4274-a8b0-10c208a48914.png) implies a function
    that takes ![](img/ec3068c5-68a5-4d37-8497-2c7198921ff6.png) as an input and returns
    ![](img/bcec9718-a32d-4cbb-8bd2-be81646ed456.png) as an output. It can also be
    represented as ![](img/d9a2a1f7-670d-459c-8d69-2bbf1bda9919.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have a function, ![](img/f858029e-d5a6-42b2-adb1-08c259e11ed5.png),
    and we can plot and see what our function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/713266f0-94bb-470f-aede-dca33212cf32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The smallest value of a function is called the **minimum of a function**. As
    you can see in the preceding plot, the minimum of the ![](img/a3b99c72-2a57-45df-804e-1b5a5a245e50.png)
    function lies at 0\. The previous function is called a **convex function**, and
    is where we have only one minimum value. A function is called a **non-convex function**
    when there is more than one minimum value. As we can see in the following diagram,
    a non-convex function can have many local minima and one global minimum value,
    whereas a convex function has only one global minimum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a80ce143-5782-4b43-bdad-98ed35b8d159.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By looking at the graph of the ![](img/fc61e336-b53b-4577-b381-3270a8b3d136.png)
    function, we can easily say that it has its minimum value at ![](img/907d9105-3f08-40a8-948f-b85295a9d4db.png).
    But how can we find the minimum value of a function mathematically? First, let''s
    assume *x = 0.7*. Thus, we are at a position where *x = 0.7,* as shown in the
    following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf53ffce-63b6-4901-a23d-29c3f99031a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we need to go to zero, which is our minimum value, but how can we reach
    it? We can reach it by calculating the derivative of the function, ![](img/9004f0b2-5eb0-428a-85ad-65d0ff49998b.png).
    So, the derivative of the function, ![](img/8954bb0f-8f5f-4858-8c74-3eb891de5534.png),
    with respect to ![](img/ff9989d9-4558-42e3-bd59-73106825b532.png), is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0d41b34-b742-47eb-9e2c-41704d1c8f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5685c5da-2d00-4003-9e4a-0bc98bb1cc83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we are at *x = 0.7* and substituting this in the previous equation, we
    get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e909932d-23a8-4f59-bff5-e166f37f083d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After calculating the derivative, we update our position of ![](img/82b7a97b-5bfc-4af7-acec-9a68bcf9d2ca.png)
    according to the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe5a13ba-b2b8-4c5f-9ecb-ee86b960dba0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5c84f501-8cf9-4a1d-963d-a65c840c8165.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b27d8d74-0525-466b-840f-0789c16f516e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see in the following graph, we were at *x = 0.7* initially, but,
    after computing the gradient, we are now at the updated position of *x = -0.7*.
    However, this is something we don''t want because we missed our minimum value,
    which is *x = 0*, and reached somewhere else:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88c482a0-2098-4adb-95cc-953ca861c6ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To avoid this, we introduce a new parameter called learning rate, ![](img/2a4a3822-b1b0-4520-8ef5-350a15b5f201.png),
    in the update rule. It helps us to slow down our gradient steps so that we won''t
    miss out the minimal point. We multiply the gradients by the learning rate and
    update the ![](img/ed14be6e-8dfd-4224-8830-90929ef2bbf3.png) value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d9aa0e7-19cc-4a64-9c4a-fbe7c95975c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say that ![](img/70c9a5ba-c40e-4ca5-89f7-268009b00abb.png); now, we
    can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd07f2c3-8d87-47c1-8f28-9681313481dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/60110808-4e9c-4040-8c90-16e12b6ea20f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see in the following graph, after multiplying the gradients by the
    learning rate with the updated *x* value, we descended from the initial position,
    *x = 0.7,* to *x = 0.49*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a29a1581-00d6-465b-86cc-512afe8ee727.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this still isn''t our optimal minimum value. We need to go further
    down until we reach the minimum value; that is, *x = 0*. So, for some *n* number
    of iterations, we have to repeat the same process until we reach the minimal point.
    That is, for some *n* number of iterations, we update the value of *x* using the
    following update rule until we reach the minimal point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ab193ae-0792-4fa9-9fea-4898400c84ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay – why is there a minus in the preceding equation? That is, why we are subtracting
    ![](img/805ab6e1-eaf0-49d0-9b00-8a0cfeac074f.png) from *x*? Why can't we add them
    and have our equation as ![](img/21191f6f-8453-4570-ad5e-8db748a0fd65.png)?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because we are finding the minimum of a function, so we need to go
    downward. If we add *x* to ![](img/ce311d68-a009-4bb9-b9f7-f2f1eaa930d9.png),
    then we go upward on every iteration, and we cannot find the minimum value, as
    shown in the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/75ed568d-5953-4ce8-a6c2-9734dd515c26.png) | ![](img/590de2fe-7d7c-4dab-b585-19e1ec5cff25.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/3cdb85c9-c1d6-4893-8216-b8456584ee25.png) | ![](img/f4945d58-7c63-4fcf-a3a3-c19632de3d3a.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, on every iteration, we compute gradients of *y* with respect to *x,*
    that is, ![](img/4494ff4f-e578-4cb8-bb05-570ca8f80c2d.png), multiply the gradients
    by the learning rate, that is, ![](img/81159fea-9eb1-4e14-9b44-8a4d6364db92.png),
    and subtract it from the *x* value to get the updated *x* value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1573f8a-e46d-4af8-b06c-d0235a921102.png)'
  prefs: []
  type: TYPE_IMG
- en: By repeating this step on every iteration, we go downward from the cost function
    and reach the minimum point. As we can see in the following graph, we moved downward
    from the initial position of 0.7 to 0.49, and then, from there, we reached 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, after several iterations, we reach the minimum point, which is 0.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef6d023b-aef5-4fd5-ade2-c6cd0fc63fc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We say we attained **convergence** when we reach the minimum of the function.
    But the question is: how do we know that we attained convergence? In our example,
    ![](img/0bc8c52b-d12f-4341-ae71-a5d95cf472ce.png), we know that the minimum value
    is 0\. So, when we reach 0, we can say that we found the minimum value that we
    attained convergence. But how can we mathematically say that 0 is the minimum
    value of the function, ![](img/6aecabbf-04a4-4771-89af-4d9332c7ed27.png)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at the following graph, which shows how the value
    of *x* changes on every iteration. As you may notice, the value of *x* is 0.009
    in the fifth iteration, 0.008 in the sixth iteration, and 0.007 in the seventh
    iteration. As you can see, there''s not much difference between the fifth, sixth,
    and seventh iterations. When there is little change in the value of *x* over iterations,
    then we can conclude that we have attained convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85e06a85-c2e2-46b9-863b-db9f71f33379.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, but what is the use of all this? Why are we trying to find the minimum
    of a function? When we''re training a model, our goal is to minimize the loss
    function of the model. Thus, with gradient descent, we can find the minimum of
    the cost function. Finding the minimum of the cost function gives us an optimal
    parameter of the model with which we can obtain the minimal loss. In general,
    we denote the parameters of the model by ![](img/870699d5-07f1-45c0-9e0c-522b85807918.png).
    The following equation is called the parameter update rule or weight update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85b286b3-5625-4888-b6c2-c01b4b5ed3d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c47bd99-cc4d-4d46-af9a-6d91ce95c3ac.png)is the parameter of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/63d79fb5-00d3-4d7c-8959-770bb6f73a5a.png)is the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2d536939-b2e6-44c5-9ba4-5e87c2b4e5ec.png)is the gradient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We update the parameter of the model for several iterations according to the
    parameter update rule until we attain convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Performing gradient descent in regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have understood how the gradient descent algorithm finds the optimal
    parameters of the model. In this section, we will understand how we can use gradient
    descent in linear regression and find the optimal parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation of a simple linear regression can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97c861f0-d573-433a-b334-83319c22e8e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have two parameters, ![](img/949a79f2-a8f5-4c59-97b3-81e5a3e0fc9d.png)
    and ![](img/1134aef2-52c6-496a-972f-0db902f14e5f.png). Now, we will see how can
    we use gradient descent and find the optimal values for these two parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will generate some random data points with `500` rows and `2` columns
    (*x* and *y*) and use them for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, our data has two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first column indicates the ![](img/66590c20-dfc2-4b48-839f-cc63a9ba1b58.png)
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The second column indicates the ![](img/ab063c3a-cce6-4f37-8037-e30c9a093fff.png)
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that the equation of a simple linear regression is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b41dfbb1-d9c9-4016-9c74-1396b9451bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we have two parameters, ![](img/bc7de407-91aa-4d28-9b68-8a105971bd42.png)
    and ![](img/1b7b75f5-1fd9-47bd-997f-85319ea76537.png). We store both of these
    parameters in an array called `theta`. First, we initialize `theta` with zeros,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `theta[0]` function represents the value of ![](img/e9a82001-7e29-40fa-be76-cacaebe1c975.png),
    while the `theta[1]` function represents the value of ![](img/3f8557ef-0294-4299-b920-1fcaf33f9a02.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Defining the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **mean squared error** (**MSE**) of regression is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25a25165-374f-48bb-828f-ccc02eacebf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/98dad91a-ee90-4832-94da-20e58657f1cd.png) is the number of training
    samples, ![](img/214c49e5-d122-4a2a-bc14-36a6d572dbd3.png) is the actual value,
    and ![](img/a8e11bef-a35b-46b7-a034-e848dea6ae7f.png) is the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the preceding loss function is shown here. We feed the
    `data` and the model parameter, `theta`, to the loss function, which returns the
    MSE. Remember that `data[,0]` has an ![](img/c54f2a3b-c050-4d6b-aa49-5369f43303c4.png)
    value and that `data[,1]` has a ![](img/c3a91968-5b14-4f57-9cb6-a15db386a494.png)
    value. Similarly, `theta [0]` has a value of `m` and `theta[1]` has a value of
    ![](img/d0fe8134-2a5f-4033-8df2-e59a6bdb531e.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to get the value of ![](img/4b627c78-968a-4c5b-8894-06f178e5fddd.png)
    and ![](img/5087fbda-762c-449f-9e60-1d5cfb52b799.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We do this for each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we get the value of ![](img/c723faa4-5fbf-4295-b391-8033c2fd1267.png)
    and ![](img/01de849d-be74-4d24-ae46-84e33f534b8c.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we predict the value of ![](img/e112d877-4c86-4eb5-8a57-207d77879881.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we compute the loss as given in equation *(3)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the mean squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When we feed our randomly initialized `data` and model parameter, `theta`,
    `loss_function` returns the mean squared loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to minimize this loss. In order to minimize the loss, we need to
    calculate the gradient of the loss function, ![](img/45fcd047-b7d1-48af-9c70-f6f1b39ddc7f.png),
    with respect to the model parameters, ![](img/3b49a75c-ac98-4deb-9794-d54352e54182.png)
    and ![](img/7c7dcfb5-e088-4ddc-a962-f3de556ea04e.png), and update the parameter
    according to the parameter update rule. First, we will calculate the gradients
    of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the gradients of the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The gradients of the loss function, ![](img/b211122e-b4e6-4b47-8167-609a67e805e9.png),
    with respect to the parameter ![](img/92d4859f-5356-498f-af86-ca5a144e87ff.png),
    are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3df6bc2d-615e-4d85-990c-70c6f37a9e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of the loss function, ![](img/5f103b31-1faf-47d1-a933-6b5aed4a6f25.png),
    with respect to the parameter ![](img/1baea87c-c5e2-4509-8c22-a0d897bb8a4c.png),
    are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dbfc095-8767-4469-9b9e-0a2eee952e30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We define a function called `compute_gradients`, which takes the parameters, `data`
    and `theta` as input and returns the computed gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to initialize the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to save the total number of data points in `N`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can get the value of ![](img/b59cf398-1332-48c2-99fb-1226fb8719b3.png)
    and ![](img/8037f57c-68af-4026-b3ce-55f7b47f76c9.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same for each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we get the value of ![](img/095dfe47-f3a5-491d-9442-dd60338e16a7.png)
    and ![](img/0836a358-33f7-433e-b93b-6cb493da3be0.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the gradient of the loss with respect to ![](img/77bc2cf8-0682-4bfb-b6d6-66e98369310e.png),
    as given in equation *(4)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the gradient of the loss with respect to ![](img/76230e49-4358-4f6b-a92c-a882cf6b66d2.png),
    as given in equation *(5)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to add `epsilon` to avoid division by zero error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When we feed our randomly initialized `data` and `theta` model parameter, the
    `compute_gradients` function returns the gradients with respect to ![](img/f22c0dc5-90aa-43a5-9066-4d535fe01ef6.png),
    that is, ![](img/8aac270f-aeba-4a18-b172-5ba9b9ae9873.png), and gradients with
    respect to ![](img/8f3c9fe0-900e-493a-992f-e0962ea44bfc.png), that is, ![](img/bdece2b4-a275-4ac0-9929-27fa8b00ec1a.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Updating the model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve computed the gradients, we need to update our model parameters
    according to our update rule, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76e81629-8d88-4126-82ac-ba628b371785.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f46ada57-bf52-4003-8dd5-13571e32e5f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we stored ![](img/9414fad6-994b-4f63-beed-5d4a2d3aa118.png) in `theta[0]`
    and ![](img/ee192345-3284-48bc-907c-1203e34d5266.png) in `theta[1]`, we can write
    our update equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb0718ae-0508-4563-aa40-75297f1ab863.png)'
  prefs: []
  type: TYPE_IMG
- en: As we learned in the previous section, updating gradients on just one iteration
    will not lead us to convergence, that is, the minimum of the cost function, so
    we need to compute gradients and update the model parameter for several iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to define the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a list called `loss` for storing the loss on every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'On each iteration, we will calculate and update the gradients according to
    our parameter update rule from equation *(8)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to plot the `loss` (`Cost`) function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows how the loss (**Cost**) decreases over the training
    iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39abba35-483b-4ff6-b7e7-e898034b0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we learned that gradient descent can be used to find the optimal parameters
    of the model, which we can then use to minimize the loss. In the next section,
    we will learn about several variants of the gradient descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent versus stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We update the parameter of the model multiple times with our parameter update
    equation *(1)* until we find the optimal parameter value. In gradient descent,
    to perform a single parameter update, we iterate through all the data points in
    our training set. So, every time we update the parameters of the model, we iterate
    through all the data points in the training set. Updating the parameters of the
    model only after iterating through all the data points in the training set makes
    gradient descent very slow and it will increase the training time, especially
    when we have a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a training set with 1 million data points. We know that we
    update the parameters of the model multiple times to find the optimal parameter
    value. So, even to perform a single parameter update, we go through all 1 million
    data points in our training set and then update the model parameters. This will
    definitely make the training slow. This is because we can't just find the optimal
    parameter with a single update; we need to update the parameters of the model
    several times to find the optimal value. So, if we iterate through all 1 million
    data points in our training set for every parameter update, it will definitely
    slow down our training.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to combat this, we introduce **stochastic gradient descent** (**SGD**).
    Unlike gradient descent, we don't have to wait to update the parameter of the
    model after iterating all the data points in our training set; we just update
    the parameters of the model after iterating through every single data point in
    our training set.
  prefs: []
  type: TYPE_NORMAL
- en: Since we update the parameters of the model in SGD after iterating every single
    data point, it will learn the optimal parameter of the model faster compared to
    gradient descent, and this will minimize the training time.
  prefs: []
  type: TYPE_NORMAL
- en: How is SGD useful? When we have a huge dataset, by using the vanilla gradient
    descent method, we update the parameters only after iterating through all the
    data points in that huge dataset. So, after many iterations over the whole dataset,
    we reach convergence and, apparently, it takes a long time. But, in SGD, we update
    the parameters after iterating through every single training sample. That is,
    we are learning to find the optimal parameters right from the first training sample,
    which helps to attain convergence faster compared to the vanilla gradient descent
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the epoch specifies the number of times the neural network sees
    the whole training data. Thus, in gradient descent, on each epoch, we perform
    the parameter update. This means that, after every epoch, the neural networks
    see the whole training data. We perform the parameter update for each epoch as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f74c110a-5011-45ad-a74a-258cc6163d99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in stochastic gradient descent, we don''t have to wait until the completion
    of each epoch to update the parameters. That is, we don''t have to wait until
    the neural network sees the whole training data to update the parameters. Instead,
    we update the parameters of the network right from seeing a single training sample
    for each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adc3a4b7-55bb-445a-8144-371d292009e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following contour plot shows how gradient descent and stochastic gradient
    descent perform the parameter updates and find the minimum cost. The star symbol
    in the center of the plot denotes the position where we have a minimum cost. As
    you can see, SGD reaches convergence faster than vanilla gradient descent. You
    can also observe the oscillations in the gradient steps on SGD; this is because
    we are updating the parameter for every training sample, so, the gradient step
    in SGD changes frequently compared to the vanilla gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d57ee66-4d0d-4b1d-a9b9-ece39b2d86b0.png)'
  prefs: []
  type: TYPE_IMG
- en: There is also another variant of gradient descent called **mini-batch gradient
    descent**. It takes the pros of both vanilla gradient descent and stochastic gradient
    descent. In SGD, we saw that we update the parameter of the model for every training
    sample. However, in mini-batch gradient descent, instead of updating the parameters
    after iterating each training sample, we update the parameters after iterating
    some batches of data points. Let's say the batch size is 50, which means that
    we update the parameter of the model after iterating through 50 data points instead
    of updating the parameter after iterating through each individual data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the contour plot of SGD and mini-batch gradient
    descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db95290c-235f-4926-a23f-e78df9cc9651.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the differences between these types of gradient descent in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent**: Updates the parameters of the model after iterating through
    all the data points in the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent**: Updates the parameter of the model after iterating
    through every single data point in the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch gradient descent**: Updates the parameters of the model after
    iterating *n* number of data points in the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is preferred over vanilla gradient descent and SGD
    for large datasets since mini-batch gradient descent outperforms the other two.
  prefs: []
  type: TYPE_NORMAL
- en: The code for mini-batch gradient descent is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the `minibatch` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define `minibatch_size` by multiplying the length of data by
    `minibatch_ratio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, on each iteration, we perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, select `sample_size`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, sample the data based on `sample_size`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the gradients for `sample_data` with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After computing the gradients for the sampled data with the given mini-batch
    size, we update the model parameter, `theta`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Momentum-based gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about two new variants of gradient descent, called
    **momentum** and Nesterov accelerated gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent with momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have a problem with SGD and mini-batch gradient descent due to the oscillations
    in the parameter update. Take a look at the following plot, which shows how mini-batch
    gradient descent is attaining convergence. As you can see, there are oscillations
    in the gradient steps. The oscillations are shown by the dotted line. As you may
    notice, it is making a gradient step toward one direction, and then taking a different
    direction, and so on, until it reaches convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3701eba-0ceb-4d34-81c4-474e065d9c03.png)'
  prefs: []
  type: TYPE_IMG
- en: This oscillation occurs because, since we update the parameters after iterating
    every *n* number of data points, the direction of the update will have some variance,
    and this leads to oscillations in every gradient step. Due to this oscillation,
    it is hard to reach convergence, and it slows down the process of attaining it.
  prefs: []
  type: TYPE_NORMAL
- en: To alleviate this, we'll introduce a new technique called **momentum**. If we
    can understand what the right direction is for the gradient steps to attain convergence
    faster, then we can make our gradient steps navigate in that direction and reduce
    the oscillation in the irrelevant directions; that is, we can reduce taking directions
    that do not lead us to convergence.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we do this? We basically take a fraction of the parameter update
    from the previous gradient step and add it to the current gradient step. In physics,
    momentum keeps an object moving after a force is applied. Here, the momentum keeps
    our gradient moving toward the direction that leads to convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a look at the following equation, you can see we are basically
    taking the parameter update from the previous step, ![](img/e5b41e74-20a9-4e47-af2c-8986f1a87c88.png),
    and adding it to the current gradient step, ![](img/1e7bcf4d-985f-4ad8-b1f9-e1ac00461878.png).
    How much information we want to take from the previous gradient step depends on
    the factor, that is, ![](img/51a6304d-576d-4ec3-a26e-664ad0e61caf.png), and the
    learning rate, which is denoted by ![](img/954de68e-2d2f-4380-9aaa-00deeac3e008.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb83f676-d492-4491-8ab1-614bdfba32e4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/b2c9306e-e113-449b-a0ff-e55c2109e875.png)
    is called velocity, and it accelerates gradients in the direction that leads to
    convergence. It also reduces oscillations in an irrelevant direction by adding
    a fraction of a parameter update from the previous step to the current step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the parameter update equation with momentum is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53ac9a89-4fca-4b09-bbce-267e5ad12953.png)'
  prefs: []
  type: TYPE_IMG
- en: By doing this, performing mini-batch gradient descent with momentum helps us
    to reduce oscillations in gradient steps and attain convergence faster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the implementation of momentum.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `momentum` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize `vt` with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is executed to cover the range for each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute `gradients` with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we update `vt` to be ![](img/477c810a-9d08-43a5-ab14-d4fde01d0850.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we update the model parameter, `theta`, as ![](img/7f818a1c-0e65-4f8a-a21f-0adceb0497c5.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Nesterov accelerated gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One problem with momentum is that it might miss out the minimum value. That
    is, as we move closer toward convergence (the minimum point), the value for momentum
    will be high. When the value of momentum is high while we are near to attaining
    convergence, then the momentum actually pushes the gradient step high and it might
    miss out on the actual minimum value; that is, it might overshoot the minimum
    value when the momentum is high when we are near to convergence, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65dae543-ef11-42d3-9c8c-5bcea24c3d81.png)'
  prefs: []
  type: TYPE_IMG
- en: To overcome this, Nesterov introduced a new method called **Nesterov accelerated
    gradient** (**NAG**).
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental motivation behind Nesterov momentum is that, instead of calculating
    the gradient at the current position, we calculate gradients at the position where
    the momentum would take us to, and we call that position the lookahead position.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does this mean, though? In the *Gradient descent with momentum* section,
    we learned about the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/059fe057-0350-4fb5-85fe-51f27aee3500.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation tells us that we are basically pushing the current gradient
    step, ![](img/abcf414f-c3e2-49c4-b08e-4b57132297c0.png), to a new position using
    a fraction of the parameter update from the previous step, ![](img/ca74fb91-69b3-40a4-b3bc-a3ade6e543c8.png),
    which will help us to attain convergence. However, when the momentum is high,
    this new position will actually overshoot the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, before making a gradient step with momentum and reaching a new position,
    if we understand which position the momentum will take us to, then we can avoid
    overshooting the minimum value. If we find out that momentum will take us to the
    position that actually misses the minimum value, then we can slow down the momentum
    and try to reach the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: But how can we find the position that the momentum will take us to? In equation
    *(2)*, instead of calculating gradients with respect to the current gradient step,
    ![](img/d0ad33ba-05e6-4e68-af6e-0fb4d120ac05.png), we calculate gradients with
    respect to ![](img/8c413a95-a3a6-4d0e-b2d9-a6c990ffe441.png). The term, ![](img/eafe1ca8-2fa1-4d49-8c41-ec9d829f547c.png),
    basically tells us the approximate position of where our next gradient step is
    going to be. We call this the lookahead position. This gives us an idea of where
    our next gradient step is going to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can rewrite our ![](img/92d548ad-7fe9-4902-8393-cf2e0cf0be58.png) equation
    according to NAG as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2801d3ce-6ad0-4ac9-894f-db0adbdd79d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We update our parameter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b08cded8-6f68-4913-b47e-ac42ad3b9c74.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the parameters with the preceding equation prevents us from missing
    the minimum value by slowing down the momentum when the gradient steps are near
    to convergence. The Nesterov accelerated method is implemented as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `NAG` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the value of `vt` with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to compute the gradients with respect to ![](img/c47a48e3-6f6b-4ff0-8ae5-b215be5228bc.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update `vt` as ![](img/8ce44c3a-e42b-4a47-b1ab-728842010ff6.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we update the model parameter, `theta`, to ![](img/6f0cfa20-7bf8-4250-8dd8-d9bd7ae53190.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Adaptive methods of gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about several adaptive versions of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Setting a learning rate adaptively using Adagrad
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we build a deep neural network, we have many parameters. Parameters are
    basically the weights of the network, so when we build a network with many layers,
    we will have many weights, say, ![](img/921bdcf6-8faf-4582-98bd-5aa0143a103e.png).
    Our goal is to find the optimal values for all these weights. In all of the previous
    methods we learned about, the learning rate was a common value for all the parameters
    of the network. However **Adagrad** (short for **adaptive gradient**) adaptively
    sets the learning rate according to a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters that have frequent updates or high gradients will have a slower learning
    rate, while a parameter that has an infrequent update or small gradients will
    also have a slower learning rate. But why do we have to do this? It is because
    parameters that have infrequent updates implies that they are not trained enough,
    so we set a high learning rate for them, and parameters that have frequent updates
    implies that they are trained enough, so we set their learning rate to a low value
    so that we don't overshoot the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how Adagrad adaptively changes the learning rate. Previously,
    we represented the gradient with ![](img/0eb9264a-0aff-4e98-9416-98201ca0f47b.png).
    For simplicity, from now on in this chapter, we''ll represent gradients with ![](img/8e969c03-6112-4f27-bef1-7152b8e194c9.png).
    So, the gradient of a parameter, ![](img/a03c809e-24b3-4c69-998b-b13ef1878d0d.png),
    at an iteration, ![](img/8343d84a-b952-4bbf-b4fb-e81e0f63bcad.png), can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e696fced-9b54-41eb-9cc6-ae64180154bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can rewrite our update equation with ![](img/dfde1993-0b27-4d94-908e-44d8452e8bc6.png)
    as the gradient notation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a17fafe-c7e0-4222-a3db-e6a25d8483c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, for every iteration, ![](img/b848153f-c88b-42de-abb7-fa2f30ed7c90.png),
    to update a parameter, ![](img/9954b549-05fa-45fa-b92c-c9cbcd3d9dca.png), we divide
    the learning rate by the sum of squares of all previous gradients of the parameter,
    ![](img/e2bdd119-534b-4584-8954-5cf72f9c4495.png), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/143ea471-82eb-4eaf-82e3-a9a5e35b60ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/a030af86-9b17-4871-972a-a78ca31c12b6.png) implies the sum of squares
    of all previous gradients of the parameter ![](img/545b96da-57b3-4919-9e41-50617a0db779.png).
    We added ![](img/2bb4c014-84ed-47e4-9403-7740622fd273.png) just to avoid the division
    by zero error. We typically set the value of ![](img/82a50656-7a02-443d-bd1c-03998473e2b3.png)
    to a small number. The question that arises here is, why are we dividing the learning
    rate by a sum of squares of all the previous gradients?
  prefs: []
  type: TYPE_NORMAL
- en: We learned that parameters that have frequent updates or high gradients will
    have a slower learning rate, while parameters that have an infrequent update or
    small gradients will also have a high learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: The sum,![](img/daa6a6d7-af78-45ce-988d-e3e226db103a.png), actually scales our
    learning rate. That is, when the sum of the squared past gradients has a high
    value, we are basically dividing the learning rate by a high value, so our learning
    rate will become less. Similarly, if the sum of the squared past gradients has
    a low value, we are dividing the learning rate by a lower value, so our learning
    rate value will become high. This implies that the learning rate is inversely
    proportional to the sum of the squares of all the previous gradients of the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, our update equation is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca780a96-e7a8-4e50-9f0f-1abcc686d073.png)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, in Adagrad, we set the learning rate to a low value when the
    previous gradient value is high, and to a high value when the past gradient value
    is lower. This means that our learning rate value changes according to the past
    gradient updates of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how the Adagrad algorithm works, let's strengthen our
    knowledge by implementing it. The code for the Adagrad algorithm is given as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the `AdaGrad` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the variable called `gradients_sum` to hold the sum of gradients and
    initialize them with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the `gradients` of loss with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we calculate the sum of the gradients squared, that is, ![](img/8f690939-5865-4088-9bf5-13525f26c9af.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we compute the gradient updates, that is, ![](img/b934756d-4983-4ce3-995e-5886ca0adc58.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, update the `theta` model parameter so that it''s ![](img/a76a05a7-2b37-4d98-86f5-6305bc9a7c31.png)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Again, there is a shortcoming associated with the Adagrad method. For every
    iteration, we are accumulating and summing all the past squared gradients. So,
    on every iteration, our sum of the squared past gradients value will increase.
    When the sum of the squared past gradient value is high, we will have a large
    number in the denominator. When we divide the learning rate by a very large number,
    then the learning rate will become very small. So, over several iterations, the
    learning rate starts decaying and becomes an infinitesimally small number – that
    is, our learning rate will be monotonically decreasing. When the learning rate
    reaches a very low value, then it takes a long time to attain convergence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how Adadelta tackles this shortcoming.
  prefs: []
  type: TYPE_NORMAL
- en: Doing away with the learning rate using Adadelta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adadelta is an enhancement of the Adagrad algorithm. In Adagrad, we noticed
    the problem of the learning rate diminishing to a very low number. Although Adagrad
    learns the learning rate adaptively, we still need to set the initial learning
    rate manually. However, in Adadelta, we don't need the learning rate at all. So
    how does the Adadelta algorithm learn?
  prefs: []
  type: TYPE_NORMAL
- en: In Adadelta, instead of taking the sum of all the squared past gradients, we
    can set a window of size ![](img/d5c1b640-6064-4614-94c3-4711517041e8.png) and
    take the sum of squared past gradients only from that window. In Adagrad, we took
    the sum of all the squared past gradients and it led to the learning rate diminishing
    to a low number. To avoid that, we take the sum of the squared past gradients
    only from a window.
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![](img/7501c19e-7f5b-4007-9c3d-8bdd403bf972.png) is the window size, then
    our parameter update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d472342-c741-47d0-a9f3-c5df6d65ba19.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the problem is that, although we are taking gradients only from within
    a window, ![](img/2ea21248-7d34-45bb-9ad0-c97848dfee51.png), squaring and storing
    all the gradients from the window in each iteration is inefficient. So, instead
    of doing that, we can take the running average of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the running average of gradients at an iteration, *t*, ![](img/5a963575-25d4-4ca2-b5fe-065ad4351973.png),
    by adding the previous running average of gradients, ![](img/bbb28974-77d5-48b2-92bb-f82995761522.png),
    and current gradients, ![](img/cc6925a0-661f-4248-90ba-21adadf5485b.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12f4e9d1-20aa-49d7-903a-a0315ba86207.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of just taking the running average, we take the exponentially decaying
    running average of gradients, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1b0f509-4621-4956-a49d-15a19723991a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/77fc07fe-f0d6-4f74-a7f3-0270b2ff4fcf.png) is called the exponential
    decaying rate and is similar to the one we saw in momentum – that is, it is used
    for deciding how much information from the previous running average of gradients
    should be added.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, our update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d800a845-64f6-4472-bc96-7b8153deccaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For notation simplicity, let''s denote ![](img/7e80684e-53c8-4895-a6ea-c820e2bd63c1.png)
    as ![](img/8ca829a5-4620-440f-996e-b7b673987d5c.png) so that we can rewrite the
    previous update equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaa3abba-608c-4f8d-9597-72c0697b0e6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous equation, we can infer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/383befa7-159b-41d5-aa26-194b0765a082.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the denominator in the previous equation, we are basically computing
    the root mean squared of gradients up to an iteration, ![](img/c2272d39-a5b1-438a-8358-fa14ea1c5c1f.png),
    so we can simply write that in shorthand as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4725e93c-7d92-4c8b-8064-632079ceb665.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By substituting equation *(13)* in equation *(12)*, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f5ec229-a979-4ab0-9ebf-b780c0189982.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, we still have the learning rate, ![](img/7db8a465-4cf4-48dc-a7f4-e4273fee1c6b.png),
    term in our equation. How can we do away with that? We can do so by making the
    units of the parameter update in accordance with the parameter. As you may have
    noticed, the units of ![](img/9c736c49-b827-4714-ae7e-662a26876719.png) and ![](img/43cfa07c-dafd-46b8-bd4b-7003ded49a8e.png)
    don''t really match. To combat this, we compute the exponentially decaying average
    of the parameter updates,![](img/dc80e9c3-6926-4ca2-92fc-a7eaacbd810e.png), as
    we computed an exponentially decaying average of gradients, ![](img/6efc0ba3-2eb1-4a95-ba1f-75088f554f52.png),
    in equation *(10)*. So, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1b38486-b79e-4a08-8954-b46cecc10789.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s like the RMS of gradients, ![](img/74127744-93af-4dfc-bec6-57299b903588.png),
    which is similar to equation *(13)*. We can write the RMS of the parameter update
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce6f12d2-613a-4583-be31-93c4005b9be2.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the RMS value for the parameter update, ![](img/1f7ce6e6-cada-49fe-9014-08621044a329.png)
    is not known, that is, ![](img/5bdfb640-f031-4aad-bff5-860c7b834fbd.png) is not
    known, so we can just approximate it by considering until the previous update,
    ![](img/947ae65b-23c0-423b-a8c1-35c3ebe5a374.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we just replace our learning rate with the RMS value of the parameter
    updates. That is, we replace ![](img/076e95dd-a926-4ef4-911b-cf48ceb58891.png)
    with ![](img/f5044369-2aeb-4887-9afd-0098598987d9.png) in equation *(14)* and
    write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2751d98e-b867-4cfa-9244-8c3a0821fbe9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(15)* in equation *(11)*, our final update equation
    becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17dd5b3c-fba9-4b5d-b575-fd73ac204e68.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9bd169af-8202-43f9-991a-167b7682fa27.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's understand the Adadelta algorithm by implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `AdaDelta` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the `E_grad2` variable with zero for storing the running
    average of gradients, and `E_delta_theta2` with zero for storing the running average
    of the parameter update, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to compute the `gradients` with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can compute the running average of gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will compute `delta_theta`, that is, ![](img/6082dff2-d77b-4a67-8d43-094b9c2dc17f.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the running average of the parameter update, ![](img/fd64714f-4f7f-477d-a2e2-e9796e2ea107.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will update the parameter of the model, `theta`, so that it''s ![](img/e0a32cbd-f87c-4f52-981d-7fa487762814.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Overcoming the limitations of Adagrad using RMSProp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to Adadelta, RMSProp was introduced to combat the decaying learning
    rate problem of Adagrad. So, in RMSProp, we compute the exponentially decaying
    running average of gradients as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70a39013-89e8-4d74-b5d2-af6198ba6446.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of taking the sum of the square of all the past gradients, we use this
    running average of gradients. This means that our update equation becomes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95aacb08-98ff-4112-af96-544e87f5422e.png)'
  prefs: []
  type: TYPE_IMG
- en: It is recommended to assign a value of learning ![](img/0b5604c4-d979-49d8-ab84-f266667632fb.png)
    to `0.9`. Now, we will learn how to implement RMSProp in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the `RMSProp` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to initialize the `E_grad2` variable with zeros to store the running
    average of gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the `gradients` with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the running average of the gradients, that is, ![](img/8b2a51a3-3e9f-4e6b-9390-7dc65e8dc37a.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we update the parameter of the model, `theta`, so that it''s ![](img/9dbe5cdf-01b4-4668-afcf-6cbb72003eba.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Adaptive moment estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adaptive moment estimation**, known as **Adam** for short, is one of the
    most popularly used algorithms for optimizing a neural network. While reading
    about RMSProp, we learned that we compute the running average of squared gradients
    to avoid the diminishing learning rate problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3f51f1d-6ade-492e-aefc-61e6c6ed5c25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final updated equation of RMSprop is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1ea583f-7ebd-46e7-97c9-c7e426a35c2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to this, in Adam, we also compute the running average of the squared
    gradients. However, along with computing the running average of the squared gradients,
    we also compute the running average of the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The running average of gradients is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34ab5642-f504-4752-961d-143f1c859e11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The running average of squared gradients is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2a20842-c858-4d22-95d1-998e5419c757.png)'
  prefs: []
  type: TYPE_IMG
- en: Since a lot of literature and libraries represent the decaying rate in Adam
    as ![](img/df2124c0-d034-4ed9-96db-054189b46924.png) instead of ![](img/363da8bd-bf34-46d1-b780-f2e525aacceb.png),
    we'll also use ![](img/24d59526-7084-423f-a6a1-08fea1dccecc.png) to represent
    the decaying rate in Adam. Thus, ![](img/c6812bc9-7617-4c73-8196-42cbf1358429.png)
    and ![](img/eba646cb-bdf2-4046-94a7-179bf57eda6f.png) in equations *(16)* and
    *(17)* denote the exponential decay rates for the running average of the gradients
    and the squared gradients, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our updated equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9e421e2-9551-4936-9545-325f7c2063d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The running average of the gradients and running average of the squared gradients
    are basically the first and second moments of those gradients. That is, they are
    the mean and uncentered variance of our gradients, respectively. So, for notation
    simplicity, let's denote ![](img/43b1f7ac-39b2-40e2-8878-a79457c93ed5.png) as
    ![](img/05c11c4a-cfa8-481c-a11e-2a93745c3ab9.png) and ![](img/251767bb-33e2-4ed5-87b6-b50ac865576d.png)
    as ![](img/bfa9ed56-26be-4646-92d5-db97d0355fe6.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can rewrite equations *(16)* and *(17)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b55ff174-f556-4654-b015-5ceb1446b637.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/13ed75a5-2581-47ee-93e8-aa092bd94151.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We begin by setting the initial moments estimates to zero. That is, we initialize
    ![](img/010c97de-58b7-4b89-83fa-792dd5765de4.png) and ![](img/a3c4b67f-174d-4436-958e-a7cc63f150c4.png)
    with zeros. When the initial estimates are set to 0, they remain very small, even
    after many iterations. This means that they would be biased toward 0, especially
    when ![](img/e97e4a53-38c4-4acd-b55a-73192dbe7cd5.png) and ![](img/238ebaf3-b706-4feb-8165-4e4385a92e20.png)
    are close to 1\. So, to combat this, we compute the bias-corrected estimates of
    ![](img/5b150a4c-513b-4491-8122-771f492ba611.png) and ![](img/2be81116-58a9-4bd2-9006-7893608d63f6.png)
    by just dividing them by ![](img/3afa1ee2-0ab9-41cf-b67b-57c433323f37.png), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1aa781fc-0e50-4598-b403-635dbb989ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7eae202a-5884-43c6-be3f-a99dad9c0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b5328637-a5cd-4996-afa9-b53378a30465.png) and ![](img/fcade50e-ce09-46db-8c7e-e2f5080ff36a.png)
    are the bias-corrected estimates of ![](img/8b11788d-7089-4728-8c3c-b8f31f713db4.png)
    and ![](img/98cb351b-581c-4cee-8c75-873a17945df2.png), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our final update equation is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6246a7f0-a18e-4932-90f1-fc6bc3558b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's understand how to implement Adam in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define the `Adam` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    `zeros`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the `gradients` with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the first moment, `mt`, so that it''s ![](img/6479d95b-8005-4966-984d-3200dab96bed.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we update the second moment, `vt`, so that it''s ![](img/dcdf171a-595f-4074-8ec9-25495fc6f7af.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the bias-corrected estimate of `mt`, that is, ![](img/9fed01d0-11bd-44c4-88d8-27ebc5ae8b3a.png)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the bias-corrected estimate of `vt`, that is, ![](img/5452eea8-5aa0-493d-ad9b-dc39f12b950d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we update the model parameter, `theta`, so that it''s ![](img/182717ec-ad2c-4820-8c06-bca033b45355.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Adamax – Adam based on infinity-norm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will look at a small variant of the Adam algorithm called **Adamax**.
    Let''s recall the equation of the second-order moment in Adam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09810da8-e2a4-4c52-80b6-218dcf0b869f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you may have noticed from the preceding equation, we scale the gradients
    inversely proportional to the ![](img/96e6a50e-e40a-4999-a9cb-73c4e08807cc.png)
    norm of the current and past gradients (![](img/396097e1-c5af-4252-9fcf-173eb004ebc0.png)
    norm basically means the square of values):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7361a21-98a0-45a1-9359-0241675a7e6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of having just ![](img/9272cabb-f165-45d8-b836-a0f844855bb2.png), can
    we generalize it to the ![](img/b4a2e9d9-e01b-4d9e-a6ad-db8ab7a35230.png) norm?
    In general, when we have a large ![](img/9bec027b-eb90-4c01-abcd-ed54ea819ef3.png)
    for norm, our update would become unstable. However, when we set the ![](img/f0c92e11-97fb-4943-bfc2-631b36c7afc7.png)
    value to ![](img/c8cc7775-1fde-4ee3-993b-dcfebe9e3e6e.png), that is, when ![](img/33a528ff-3c8c-4150-a60b-c4ff0dc4e906.png),
    the ![](img/c4f4e684-6be1-4562-a631-f666c519f1dd.png) equation becomes simple
    and stable. Instead of just parameterizing the gradients, ![](img/004e5fdb-7b24-42e9-b848-3be63db54ee7.png),
    alone, we also parameterize the decay rate, ![](img/daf1f97c-fa96-4224-965a-36f4aea4133b.png).
    Thus, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6a4894a-9edd-4f41-b99a-1c73cef1beb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we set the limits, ![](img/afef1e2b-a4ff-4a68-8b56-c5ec148182a2.png) tends
    to reach infinity, and then we get the following final equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb48ebc1-f94e-47ee-91c0-dfaf45ac8dbb.png)'
  prefs: []
  type: TYPE_IMG
- en: You can check the paper listed in the *Further reading* section at the end of
    this chapter to see how exactly this is derived.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the preceding equation as a simple recursive equation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d41169c-0374-4b9f-9805-cf672f7ce1fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Computing ![](img/b3f21b89-9eaa-4591-838a-fdc425011972.png) is similar to what
    we saw in the *Adaptive moment estimation* section, so we can write the following
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d31edaf5-271b-488a-9cb6-00a78ee21f87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By doing this, we can compute the bias-corrected estimate of ![](img/dfe1e273-e827-4d73-9a5c-e77be289e10d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b1a70b5-d3e7-4dcc-95cb-85d1caa57a86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the final update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86e9d4cf-a048-4356-9a9a-3ddc03595084.png)'
  prefs: []
  type: TYPE_IMG
- en: To better understand the Adamax algorithm, let's code it, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `Adamax` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the gradients with respect to `theta`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the first moment, `mt`, as ![](img/85791c82-e801-4ef9-bb1c-602c9489ba17.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the second moment, `vt`, as ![](img/e05778c1-2bf0-4f8d-be76-cc5ebd085043.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the bias-corrected estimate of `mt`; that is, ![](img/b27f19b9-3357-4b19-a9cf-367b59e880b3.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the model parameter, `theta`, so that it''s ![](img/e61b483b-34ae-4351-940e-eda822970d60.png)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Adaptive moment estimation with AMSGrad
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One problem with the Adam algorithm is that it sometimes fails to attain optimal
    convergence, or it reaches a suboptimal solution. It has been noted that, in some
    settings, Adam fails to attain convergence or reach the suboptimal solution instead
    of a global optimal solution. This is due to exponentially moving the averages
    of gradients. Remember when we used the exponential moving averages of gradients
    in Adam to avoid the problem of learning rate decay?
  prefs: []
  type: TYPE_NORMAL
- en: However, the problem is that since we are taking an exponential moving average
    of gradients, we miss out information about the gradients that occur infrequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'To resolve this issue, the authors of AMSGrad made a small change to the Adam
    algorithm. Recall the second-order moment estimates we saw in Adam, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50fd89e6-3336-416a-8acd-4b361c5dd11d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In AMSGrad, we use a slightly modified version of ![](img/c60f3dab-10e9-4e71-88ef-16e13e8b209e.png).
    Instead of using ![](img/b3dc98f8-8596-48c3-a11c-16c8d704963b.png) directly, we
    take the maximum value of ![](img/b64abff4-419a-48b5-993f-305d0f29bc14.png) until
    the previous step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e77b55f-f6b1-4821-92d6-3ba7af3de434.png)'
  prefs: []
  type: TYPE_IMG
- en: This will retain the informative gradients instead of being phased out due to
    the exponential moving average.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our final update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/924a11d1-0148-42c3-90d2-759d475eb0e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's understand how to code AMSGrad in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `AMSGrad` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the first moment, `mt`, the second moment, `vt`, and the
    modified version of `vt`, that is, `vt_hat`, with `zeros`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the gradients with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the first moment, `mt`, as ![](img/db05e300-35f9-4180-abc3-a9d733e16abe.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we update the second moment, `vt`, as ![](img/456c4420-54a1-40dc-b7ce-3b79876a84c7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'In AMSGrad, we use a slightly modified version of ![](img/b905b063-1b90-4268-a01b-685c63321a23.png).
    Instead of using ![](img/cf6ace96-be36-43a2-9fb2-e8617e10e286.png) directly, we
    take the maximum value of ![](img/db094c80-9b62-49d5-a88a-169a38891439.png) until
    the previous step. Thus, ![](img/103a5249-9849-463d-b5ab-a3dad1bb5ad9.png) is
    implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will compute the bias-corrected estimate of `mt`, that is, ![](img/1f438fc5-4a60-4d4f-ac85-e24b8754e614.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can update the model parameter, `theta`, so that it''s ![](img/882d17d3-e9e7-4100-9dbb-294a078c3e94.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Nadam – adding NAG to ADAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nadam is another small extension of the Adam method. As the name suggests, here,
    we incorporate NAG into Adam. First, let's recall what we learned about in Adam.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculated the first and second moments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1e7996d-a2e6-4053-b993-2d597c4dae22.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e478ac82-f544-4b2f-9723-02d6999951c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we calculated the bias-corrected estimates of the first and second moments,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1e3821e-6433-432d-9150-6cbf08fc361b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/cd9f08a6-3fca-4e28-84d8-d718285b8670.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our final update equation of Adam is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3bed132-5f38-4a36-8c7d-94ba42e4f4de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will see how Nadam modifies Adam to use Nesterov momentum. In Adam,
    we compute the first moment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80c74cbe-8d42-4148-a948-92f11e10163b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We change this first moment so that it''s Nesterov accelerated momentum. That
    is, instead of using the previous momentum, we use the current momentum and use
    that as a lookahead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76be277c-6543-4968-9bd4-aa8d1044a6a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can''t compute the bias-corrected estimates in the same way as we computed
    them in Adam because, here, ![](img/d6171a5c-f432-4255-b665-a24aaa0d2540.png)
    comes from the current step, and ![](img/3431aced-2e2a-4450-8d1e-c280f09ba27f.png)
    comes from the subsequent step. Therefore, we change the bias-corrected estimate
    step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2deb0295-245d-4ab1-9fe1-2c613d4a16b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b7d13f32-8f97-4090-9910-4c759de7458a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can rewrite our first-moment equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da91ed55-f650-49c7-bbd6-9bf7b70d3d9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, our final update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/182c9f72-90ce-4b54-9413-164cbda3e08d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's see how we can implement the Nadam algorithm in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `nadam` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set `beta_prod` to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the gradients with respect to `theta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we compute the first moment, `mt`, so that it''s ![](img/a90248e7-e1f2-4d29-98c6-1faff1bdd3f9.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can update the second moment, `vt`, so that its'' ![](img/656e0070-28a6-413d-aea9-44b30bd53e98.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute `beta_prod`; that is, ![](img/25815526-3f96-4e89-91a9-0c89924cfbb6.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the bias-corrected estimate of `mt` so that it''s![](img/dbc9d845-d338-4041-af28-17b321535aaf.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the bias-corrected estimate of `gt` so that it''s ![](img/90ea0452-143c-417c-8d03-65ee10e2ae87.png)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we compute the bias-corrected estimate of `vt` so that it''s![](img/0fef7a9a-f152-4f65-bea8-a9166841b801.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute `mt_tilde` so that it''s ![](img/cb21eefa-02cd-4aaf-a638-d45f05ca767d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we update the model parameter, `theta`, by using ![](img/7026ecc8-19aa-4955-a1a3-9285b9b341df.png)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: By doing this, we have learned about various popular variants of gradient descent
    algorithms that are used for training neural networks. The complete code to perform
    regression with all the variants of regression is available as a Jupyter Notebook
    at [http://bit.ly/2XoW0vH](http://bit.ly/2XoW0vH).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off this chapter by learning about what convex and non-convex functions
    are. Then, we explored how we can find the minimum of a function using gradient
    descent. We learned how gradient descent minimizes a loss function by computing
    optimal parameters through gradient descent. Later, we looked at SGD, where we
    update the parameters of the model after iterating through each and every data
    point, and then we learned about mini-batch SGD, where we update the parameters
    after iterating through a batch of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we learned how momentum is used to reduce oscillations in gradient
    steps and attain convergence faster. Following this, we understood Nesterov momentum,
    where, instead of calculating the gradient at the current position, we calculate
    the gradient at the position the momentum will take us to.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about the Adagrad method, where we set the learning rate low
    for parameters that have frequent updates, and high for parameters that have infrequent
    updates. Next, we learned about the Adadelta method, where we completely do away
    with the learning rate and use an exponentially decaying average of gradients.
    We then learned about the Adam method, where we use both first and second momentum
    estimates to update gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we explored variants of Adam, such as Adamax, where we generalized
    the ![](img/a4a9a09c-4873-4dd0-a000-91b81420b218.png)norm of Adam to ![](img/d99a407c-5386-45d9-b34b-f61814aa6b12.png),
    and AMSGrad, where we combated the problem of Adam reaching a suboptimal solution.
    At the end of this chapter, we learned about Nadam, where we incorporated Nesterov
    momentum into the Adam algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about one of the most widely used deep learning
    algorithms, called **recurrent neural networks** (**RNNs**), and how to use them
    to generate song lyrics.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s recap on gradient descent by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How does SGD differ from vanilla gradient descent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain mini-batch gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need momentum?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the motivation behind NAG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Adagrad set the learning rate adaptively?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the update rule of Adadelta?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does RMSProp overcome the limitations of Adagrad?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the update equation of Adam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further information, refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adaptive Subgradient Methods for Online Learning and Stochastic Optimization*,
    by John Duchi et al., [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adadelta: An Adaptive Learning Rate Method*, by Matthew D. Zeiler, [https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adam: A Method For Stochastic Optimization*, by Diederik P. Kingma and Jimmy
    Lei Ba, [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On the Convergence of Adam and Beyond*, by Sashank J. Reddi, Satyen Kale,
    and Sanjiv Kumar, [https://openreview.net/pdf?id=ryQu7f-RZ](https://openreview.net/pdf?id=ryQu7f-RZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Incorporating Nesterov Momentum into Adam*, by Timothy Dozat, [http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
