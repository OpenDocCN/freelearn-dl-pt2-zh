- en: Gradient Descent and Its Variants
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降及其变体
- en: Gradient descent is one of the most popular and widely used optimization algorithms,
    and is a first-order optimization algorithm. First-order optimization means that
    we calculate only the first-order derivative. As we saw in [Chapter 1](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml),
    *Introduction to Deep Learning*, we used gradient descent and calculated the first-order
    derivative of the loss function with respect to the weights of the network to
    minimize the loss.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是最流行和广泛使用的优化算法之一，是一种一阶优化算法。一阶优化意味着我们只计算一阶导数。正如我们在[第1章](92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml)中看到的，*深度学习简介*，我们使用梯度下降计算损失函数相对于网络权重的一阶导数以最小化损失。
- en: Gradient descent is not only applicable to neural networks—it is also used in
    situations where we need to find the minimum of a function. In this chapter, we
    will go deeper into gradient descent, starting with the basics, and learn several
    variants of gradient descent algorithms. There are various flavors of gradient
    descent that are used for training neural networks. First, we will understand
    **Stochastic Gradient Descent** (**SGD**) and mini-batch gradient descent. Then,
    we'll explore how momentum is used to speed up gradient descent to attain convergence.
    Later in this chapter, we will learn about how to perform gradient descent in
    an adaptive manner by using various algorithms, such as Adagrad, Adadelta, RMSProp,
    Adam, Adamax, AMSGrad, and Nadam. We will take a simple linear regression equation
    and see how we can find the minimum of a linear regression's cost function using
    various types of gradient descent algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降不仅适用于神经网络，它还用于需要找到函数最小值的情况。在本章中，我们将深入探讨梯度下降，从基础知识开始，并学习几种梯度下降算法的变体。有各种各样的梯度下降方法用于训练神经网络。首先，我们将了解**随机梯度下降**（**SGD**）和小批量梯度下降。然后，我们将探讨如何通过动量加速梯度下降以达到收敛。本章后期，我们将学习如何使用各种算法（如Adagrad、Adadelta、RMSProp、Adam、Adamax、AMSGrad和Nadam）以自适应方式执行梯度下降。我们将使用简单的线性回归方程，并看看如何使用各种梯度下降算法找到线性回归成本函数的最小值。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Demystifying gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析梯度下降
- en: Gradient descent versus stochastic gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降与随机梯度下降的区别
- en: Momentum and Nesterov accelerated gradient
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量和Nesterov加速梯度
- en: Adaptive methods of gradient descent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降的自适应方法
- en: Demystifying gradient descent
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析梯度下降
- en: Before we get into the details, let's understand the basics. What is a function
    in mathematics? A function represents the relation between input and output. We
    generally use ![](img/340fb1cd-80df-4c80-9d3e-cf73319d7dec.png) to denote a function.
    For instance, ![](img/0fae6909-055b-4274-a8b0-10c208a48914.png) implies a function
    that takes ![](img/ec3068c5-68a5-4d37-8497-2c7198921ff6.png) as an input and returns
    ![](img/bcec9718-a32d-4cbb-8bd2-be81646ed456.png) as an output. It can also be
    represented as ![](img/d9a2a1f7-670d-459c-8d69-2bbf1bda9919.png).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，让我们先了解基础知识。数学中的函数是什么？函数表示输入和输出之间的关系。我们通常用![](img/340fb1cd-80df-4c80-9d3e-cf73319d7dec.png)表示一个函数。例如，![](img/0fae6909-055b-4274-a8b0-10c208a48914.png)
    表示一个以![](img/ec3068c5-68a5-4d37-8497-2c7198921ff6.png) 为输入并返回![](img/bcec9718-a32d-4cbb-8bd2-be81646ed456.png)
    为输出的函数。它也可以表示为![](img/d9a2a1f7-670d-459c-8d69-2bbf1bda9919.png)。
- en: 'Here, we have a function, ![](img/f858029e-d5a6-42b2-adb1-08c259e11ed5.png),
    and we can plot and see what our function looks like:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个函数，![](img/f858029e-d5a6-42b2-adb1-08c259e11ed5.png)，我们可以绘制并查看函数的形状：
- en: '![](img/713266f0-94bb-470f-aede-dca33212cf32.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/713266f0-94bb-470f-aede-dca33212cf32.png)'
- en: 'The smallest value of a function is called the **minimum of a function**. As
    you can see in the preceding plot, the minimum of the ![](img/a3b99c72-2a57-45df-804e-1b5a5a245e50.png)
    function lies at 0\. The previous function is called a **convex function**, and
    is where we have only one minimum value. A function is called a **non-convex function**
    when there is more than one minimum value. As we can see in the following diagram,
    a non-convex function can have many local minima and one global minimum value,
    whereas a convex function has only one global minimum value:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的最小值称为**函数的最小值**。正如您在上图中看到的，![](img/a3b99c72-2a57-45df-804e-1b5a5a245e50.png)
    函数的最小值位于 0 处。前述函数称为**凸函数**，在这种函数中只有一个最小值。当存在多个最小值时，函数称为**非凸函数**。如下图所示，非凸函数可以有许多局部最小值和一个全局最小值，而凸函数只有一个全局最小值：
- en: '![](img/a80ce143-5782-4b43-bdad-98ed35b8d159.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a80ce143-5782-4b43-bdad-98ed35b8d159.png)'
- en: 'By looking at the graph of the ![](img/fc61e336-b53b-4577-b381-3270a8b3d136.png)
    function, we can easily say that it has its minimum value at ![](img/907d9105-3f08-40a8-948f-b85295a9d4db.png).
    But how can we find the minimum value of a function mathematically? First, let''s
    assume *x = 0.7*. Thus, we are at a position where *x = 0.7,* as shown in the
    following graph:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察 ![](img/fc61e336-b53b-4577-b381-3270a8b3d136.png) 函数的图表，我们可以很容易地说它在 ![](img/907d9105-3f08-40a8-948f-b85295a9d4db.png)
    处有其最小值。但是，如何在数学上找到函数的最小值？首先，让我们假设 *x = 0.7*。因此，我们处于一个 *x = 0.7* 的位置，如下图所示：
- en: '![](img/cf53ffce-63b6-4901-a23d-29c3f99031a8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf53ffce-63b6-4901-a23d-29c3f99031a8.png)'
- en: 'Now, we need to go to zero, which is our minimum value, but how can we reach
    it? We can reach it by calculating the derivative of the function, ![](img/9004f0b2-5eb0-428a-85ad-65d0ff49998b.png).
    So, the derivative of the function, ![](img/8954bb0f-8f5f-4858-8c74-3eb891de5534.png),
    with respect to ![](img/ff9989d9-4558-42e3-bd59-73106825b532.png), is as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要走向零，这是我们的最小值，但我们如何达到它？我们可以通过计算函数的导数，![](img/9004f0b2-5eb0-428a-85ad-65d0ff49998b.png)，来达到它。因此，关于
    ![](img/ff9989d9-4558-42e3-bd59-73106825b532.png)，函数的导数为：
- en: '![](img/f0d41b34-b742-47eb-9e2c-41704d1c8f4c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0d41b34-b742-47eb-9e2c-41704d1c8f4c.png)'
- en: '![](img/5685c5da-2d00-4003-9e4a-0bc98bb1cc83.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5685c5da-2d00-4003-9e4a-0bc98bb1cc83.png)'
- en: 'Since we are at *x = 0.7* and substituting this in the previous equation, we
    get the following equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在 *x = 0.7* 处，并将其代入前述方程，我们得到以下方程：
- en: '![](img/e909932d-23a8-4f59-bff5-e166f37f083d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e909932d-23a8-4f59-bff5-e166f37f083d.png)'
- en: 'After calculating the derivative, we update our position of ![](img/82b7a97b-5bfc-4af7-acec-9a68bcf9d2ca.png)
    according to the following update rule:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算导数后，我们根据以下更新规则更新 ![](img/82b7a97b-5bfc-4af7-acec-9a68bcf9d2ca.png) 的位置：
- en: '![](img/fe5a13ba-b2b8-4c5f-9ecb-ee86b960dba0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe5a13ba-b2b8-4c5f-9ecb-ee86b960dba0.png)'
- en: '![](img/5c84f501-8cf9-4a1d-963d-a65c840c8165.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c84f501-8cf9-4a1d-963d-a65c840c8165.png)'
- en: '![](img/b27d8d74-0525-466b-840f-0789c16f516e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b27d8d74-0525-466b-840f-0789c16f516e.png)'
- en: 'As we can see in the following graph, we were at *x = 0.7* initially, but,
    after computing the gradient, we are now at the updated position of *x = -0.7*.
    However, this is something we don''t want because we missed our minimum value,
    which is *x = 0*, and reached somewhere else:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在下图中所见，我们最初位于 *x = 0.7* 处，但在计算梯度后，我们现在处于更新位置 *x = -0.7*。然而，这并不是我们想要的，因为我们错过了我们的最小值
    *x = 0*，而是达到了其他位置：
- en: '![](img/88c482a0-2098-4adb-95cc-953ca861c6ad.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88c482a0-2098-4adb-95cc-953ca861c6ad.png)'
- en: 'To avoid this, we introduce a new parameter called learning rate, ![](img/2a4a3822-b1b0-4520-8ef5-350a15b5f201.png),
    in the update rule. It helps us to slow down our gradient steps so that we won''t
    miss out the minimal point. We multiply the gradients by the learning rate and
    update the ![](img/ed14be6e-8dfd-4224-8830-90929ef2bbf3.png) value, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们引入了一个称为学习率的新参数，![](img/2a4a3822-b1b0-4520-8ef5-350a15b5f201.png)，在更新规则中。它帮助我们减慢梯度下降的步伐，以便我们不会错过最小点。我们将梯度乘以学习率，并更新
    ![](img/ed14be6e-8dfd-4224-8830-90929ef2bbf3.png) 的值，如下所示：
- en: '![](img/6d9aa0e7-19cc-4a64-9c4a-fbe7c95975c9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d9aa0e7-19cc-4a64-9c4a-fbe7c95975c9.png)'
- en: 'Let''s say that ![](img/70c9a5ba-c40e-4ca5-89f7-268009b00abb.png); now, we
    can write the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/70c9a5ba-c40e-4ca5-89f7-268009b00abb.png)；现在，我们可以写出以下内容：
- en: '![](img/dd07f2c3-8d87-47c1-8f28-9681313481dd.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd07f2c3-8d87-47c1-8f28-9681313481dd.png)'
- en: '![](img/60110808-4e9c-4040-8c90-16e12b6ea20f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60110808-4e9c-4040-8c90-16e12b6ea20f.png)'
- en: 'As we can see in the following graph, after multiplying the gradients by the
    learning rate with the updated *x* value, we descended from the initial position,
    *x = 0.7,* to *x = 0.49*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在下图中看到的那样，在将更新后的 *x* 值的梯度乘以学习率后，我们从初始位置 *x = 0.7* 下降到 *x = 0.49*：
- en: '![](img/a29a1581-00d6-465b-86cc-512afe8ee727.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a29a1581-00d6-465b-86cc-512afe8ee727.png)'
- en: 'However, this still isn''t our optimal minimum value. We need to go further
    down until we reach the minimum value; that is, *x = 0*. So, for some *n* number
    of iterations, we have to repeat the same process until we reach the minimal point.
    That is, for some *n* number of iterations, we update the value of *x* using the
    following update rule until we reach the minimal point:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仍然不是我们的最优最小值。我们需要进一步下降，直到达到最小值，即 *x = 0*。因此，对于一些 *n* 次迭代，我们必须重复相同的过程，直到达到最小点。也就是说，对于一些
    *n* 次迭代，我们使用以下更新规则更新 *x* 的值，直到达到最小点：
- en: '![](img/0ab193ae-0792-4fa9-9fea-4898400c84ee.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ab193ae-0792-4fa9-9fea-4898400c84ee.png)'
- en: Okay – why is there a minus in the preceding equation? That is, why we are subtracting
    ![](img/805ab6e1-eaf0-49d0-9b00-8a0cfeac074f.png) from *x*? Why can't we add them
    and have our equation as ![](img/21191f6f-8453-4570-ad5e-8db748a0fd65.png)?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，为什么在前面的方程中有一个减号？也就是说，为什么我们要从*x*中减去![](img/805ab6e1-eaf0-49d0-9b00-8a0cfeac074f.png)？为什么不能将它们加起来，将我们的方程变为![](img/21191f6f-8453-4570-ad5e-8db748a0fd65.png)？
- en: 'This is because we are finding the minimum of a function, so we need to go
    downward. If we add *x* to ![](img/ce311d68-a009-4bb9-b9f7-f2f1eaa930d9.png),
    then we go upward on every iteration, and we cannot find the minimum value, as
    shown in the following graphs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们在寻找函数的最小值，所以我们需要向下。如果我们将*x*加上![](img/ce311d68-a009-4bb9-b9f7-f2f1eaa930d9.png)，那么我们在每次迭代时都会向上移动，无法找到最小值，如下图所示：
- en: '| ![](img/75ed568d-5953-4ce8-a6c2-9734dd515c26.png) | ![](img/590de2fe-7d7c-4dab-b585-19e1ec5cff25.png)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/75ed568d-5953-4ce8-a6c2-9734dd515c26.png) | ![](img/590de2fe-7d7c-4dab-b585-19e1ec5cff25.png)
    |'
- en: '| ![](img/3cdb85c9-c1d6-4893-8216-b8456584ee25.png) | ![](img/f4945d58-7c63-4fcf-a3a3-c19632de3d3a.png)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/3cdb85c9-c1d6-4893-8216-b8456584ee25.png) | ![](img/f4945d58-7c63-4fcf-a3a3-c19632de3d3a.png)
    |'
- en: 'Thus, on every iteration, we compute gradients of *y* with respect to *x,*
    that is, ![](img/4494ff4f-e578-4cb8-bb05-570ca8f80c2d.png), multiply the gradients
    by the learning rate, that is, ![](img/81159fea-9eb1-4e14-9b44-8a4d6364db92.png),
    and subtract it from the *x* value to get the updated *x* value, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每次迭代中，我们计算*y*相对于*x*的梯度，即![](img/4494ff4f-e578-4cb8-bb05-570ca8f80c2d.png)，将梯度乘以学习率，即![](img/81159fea-9eb1-4e14-9b44-8a4d6364db92.png)，然后从*x*值中减去它以获得更新后的*x*值，如下所示：
- en: '![](img/f1573f8a-e46d-4af8-b06c-d0235a921102.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1573f8a-e46d-4af8-b06c-d0235a921102.png)'
- en: By repeating this step on every iteration, we go downward from the cost function
    and reach the minimum point. As we can see in the following graph, we moved downward
    from the initial position of 0.7 to 0.49, and then, from there, we reached 0.2.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每次迭代中重复此步骤，我们从成本函数向下移动，并达到最小点。正如我们在下图中所看到的，我们从初始位置0.7向下移动到0.49，然后从那里到达0.2。
- en: 'Then, after several iterations, we reach the minimum point, which is 0.0:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在多次迭代之后，我们达到了最小点，即0.0：
- en: '![](img/ef6d023b-aef5-4fd5-ade2-c6cd0fc63fc7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef6d023b-aef5-4fd5-ade2-c6cd0fc63fc7.png)'
- en: 'We say we attained **convergence** when we reach the minimum of the function.
    But the question is: how do we know that we attained convergence? In our example,
    ![](img/0bc8c52b-d12f-4341-ae71-a5d95cf472ce.png), we know that the minimum value
    is 0\. So, when we reach 0, we can say that we found the minimum value that we
    attained convergence. But how can we mathematically say that 0 is the minimum
    value of the function, ![](img/6aecabbf-04a4-4771-89af-4d9332c7ed27.png)?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们达到函数的最小值时，我们称之为**收敛**。但问题是：我们如何知道我们已经达到了收敛？在我们的例子中，![](img/0bc8c52b-d12f-4341-ae71-a5d95cf472ce.png)，我们知道最小值是0。因此，当我们达到0时，我们可以说我们找到了最小值，即我们已经达到了收敛。但是我们如何在数学上表达0是函数![](img/6aecabbf-04a4-4771-89af-4d9332c7ed27.png)的最小值呢？
- en: 'Let''s take a closer look at the following graph, which shows how the value
    of *x* changes on every iteration. As you may notice, the value of *x* is 0.009
    in the fifth iteration, 0.008 in the sixth iteration, and 0.007 in the seventh
    iteration. As you can see, there''s not much difference between the fifth, sixth,
    and seventh iterations. When there is little change in the value of *x* over iterations,
    then we can conclude that we have attained convergence:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细观察下面的图表，显示了*x*在每次迭代中的变化。正如您可能注意到的那样，第五次迭代时*x*的值为0.009，第六次迭代时为0.008，第七次迭代时为0.007。正如您所见，第五、六和七次迭代之间几乎没有什么差异。当*x*在迭代中的值变化很小时，我们可以得出我们已经达到了收敛：
- en: '![](img/85e06a85-c2e2-46b9-863b-db9f71f33379.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85e06a85-c2e2-46b9-863b-db9f71f33379.png)'
- en: 'Okay, but what is the use of all this? Why are we trying to find the minimum
    of a function? When we''re training a model, our goal is to minimize the loss
    function of the model. Thus, with gradient descent, we can find the minimum of
    the cost function. Finding the minimum of the cost function gives us an optimal
    parameter of the model with which we can obtain the minimal loss. In general,
    we denote the parameters of the model by ![](img/870699d5-07f1-45c0-9e0c-522b85807918.png).
    The following equation is called the parameter update rule or weight update rule:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，但这一切有什么用？我们为什么要找到一个函数的最小值？当我们训练模型时，我们的目标是最小化模型的损失函数。因此，通过梯度下降，我们可以找到成本函数的最小值。找到成本函数的最小值给我们提供了模型的最优参数，从而可以获得最小的损失。一般来说，我们用![](img/870699d5-07f1-45c0-9e0c-522b85807918.png)来表示模型的参数。以下方程称为参数更新规则或权重更新规则：
- en: '![](img/85b286b3-5625-4888-b6c2-c01b4b5ed3d4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85b286b3-5625-4888-b6c2-c01b4b5ed3d4.png)'
- en: 'Here, we have the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '![](img/7c47bd99-cc4d-4d46-af9a-6d91ce95c3ac.png)is the parameter of the model'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/7c47bd99-cc4d-4d46-af9a-6d91ce95c3ac.png)是模型的参数'
- en: '![](img/63d79fb5-00d3-4d7c-8959-770bb6f73a5a.png)is the learning rate'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/63d79fb5-00d3-4d7c-8959-770bb6f73a5a.png)是学习率'
- en: '![](img/2d536939-b2e6-44c5-9ba4-5e87c2b4e5ec.png)is the gradient'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/2d536939-b2e6-44c5-9ba4-5e87c2b4e5ec.png)是梯度'
- en: We update the parameter of the model for several iterations according to the
    parameter update rule until we attain convergence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据参数更新规则多次迭代更新模型的参数，直到达到收敛。
- en: Performing gradient descent in regression
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在回归中执行梯度下降
- en: So far, we have understood how the gradient descent algorithm finds the optimal
    parameters of the model. In this section, we will understand how we can use gradient
    descent in linear regression and find the optimal parameter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经理解了梯度下降算法如何找到模型的最优参数。在本节中，我们将了解如何在线性回归中使用梯度下降，并找到最优参数。
- en: 'The equation of a simple linear regression can be expressed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归的方程可以表达如下：
- en: '![](img/97c861f0-d573-433a-b334-83319c22e8e2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97c861f0-d573-433a-b334-83319c22e8e2.png)'
- en: Thus, we have two parameters, ![](img/949a79f2-a8f5-4c59-97b3-81e5a3e0fc9d.png)
    and ![](img/1134aef2-52c6-496a-972f-0db902f14e5f.png). Now, we will see how can
    we use gradient descent and find the optimal values for these two parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个参数，![](img/949a79f2-a8f5-4c59-97b3-81e5a3e0fc9d.png)和![](img/1134aef2-52c6-496a-972f-0db902f14e5f.png)。现在，我们将看看如何使用梯度下降找到这两个参数的最优值。
- en: Importing the libraries
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入库
- en: 'First, we need to import the required libraries:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的库：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preparing the dataset
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'Next, we will generate some random data points with `500` rows and `2` columns
    (*x* and *y*) and use them for training:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成一些随机数据点，有`500`行和`2`列（*x*和*y*），并将它们用于训练：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, our data has two columns:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的数据有两列：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first column indicates the ![](img/66590c20-dfc2-4b48-839f-cc63a9ba1b58.png)
    value:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列表示![](img/66590c20-dfc2-4b48-839f-cc63a9ba1b58.png)值：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The second column indicates the ![](img/ab063c3a-cce6-4f37-8037-e30c9a093fff.png)
    value:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第二列表示![](img/ab063c3a-cce6-4f37-8037-e30c9a093fff.png)值：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We know that the equation of a simple linear regression is expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道简单线性回归方程的表达如下：
- en: '![](img/b41dfbb1-d9c9-4016-9c74-1396b9451bbc.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b41dfbb1-d9c9-4016-9c74-1396b9451bbc.png)'
- en: 'Thus, we have two parameters, ![](img/bc7de407-91aa-4d28-9b68-8a105971bd42.png)
    and ![](img/1b7b75f5-1fd9-47bd-997f-85319ea76537.png). We store both of these
    parameters in an array called `theta`. First, we initialize `theta` with zeros,
    as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个参数，![](img/bc7de407-91aa-4d28-9b68-8a105971bd42.png)和![](img/1b7b75f5-1fd9-47bd-997f-85319ea76537.png)。我们将这两个参数都存储在名为`theta`的数组中。首先，我们将`theta`初始化为零，如下所示：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `theta[0]` function represents the value of ![](img/e9a82001-7e29-40fa-be76-cacaebe1c975.png),
    while the `theta[1]` function represents the value of ![](img/3f8557ef-0294-4299-b920-1fcaf33f9a02.png):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`theta[0]`表示![](img/e9a82001-7e29-40fa-be76-cacaebe1c975.png)的值，而函数`theta[1]`表示![](img/3f8557ef-0294-4299-b920-1fcaf33f9a02.png)的值：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining the loss function
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: 'The **mean squared error** (**MSE**) of regression is given as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的**均方误差**（**MSE**）如下所示：
- en: '![](img/25a25165-374f-48bb-828f-ccc02eacebf9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25a25165-374f-48bb-828f-ccc02eacebf9.png)'
- en: Here, ![](img/98dad91a-ee90-4832-94da-20e58657f1cd.png) is the number of training
    samples, ![](img/214c49e5-d122-4a2a-bc14-36a6d572dbd3.png) is the actual value,
    and ![](img/a8e11bef-a35b-46b7-a034-e848dea6ae7f.png) is the predicted value.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/98dad91a-ee90-4832-94da-20e58657f1cd.png) 是训练样本的数量，![](img/214c49e5-d122-4a2a-bc14-36a6d572dbd3.png)
    是实际值，![](img/a8e11bef-a35b-46b7-a034-e848dea6ae7f.png) 是预测值。
- en: The implementation of the preceding loss function is shown here. We feed the
    `data` and the model parameter, `theta`, to the loss function, which returns the
    MSE. Remember that `data[,0]` has an ![](img/c54f2a3b-c050-4d6b-aa49-5369f43303c4.png)
    value and that `data[,1]` has a ![](img/c3a91968-5b14-4f57-9cb6-a15db386a494.png)
    value. Similarly, `theta [0]` has a value of `m` and `theta[1]` has a value of
    ![](img/d0fe8134-2a5f-4033-8df2-e59a6bdb531e.png).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述损失函数的实现如下所示。我们将`data`和模型参数`theta`输入损失函数，返回均方误差（MSE）。请记住，`data[,0]`具有一个 ![](img/c54f2a3b-c050-4d6b-aa49-5369f43303c4.png)
    值，而`data[,1]`具有一个 ![](img/c3a91968-5b14-4f57-9cb6-a15db386a494.png) 值。类似地，`theta
    [0]`的值为`m`，而`theta[1]`的值为 ![](img/d0fe8134-2a5f-4033-8df2-e59a6bdb531e.png)。
- en: 'Let''s define the loss function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义损失函数：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we need to get the value of ![](img/4b627c78-968a-4c5b-8894-06f178e5fddd.png)
    and ![](img/5087fbda-762c-449f-9e60-1d5cfb52b799.png):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要获取 ![](img/4b627c78-968a-4c5b-8894-06f178e5fddd.png) 和 ![](img/5087fbda-762c-449f-9e60-1d5cfb52b799.png)
    的值：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We do this for each iteration:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个迭代执行此操作：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we get the value of ![](img/c723faa4-5fbf-4295-b391-8033c2fd1267.png)
    and ![](img/01de849d-be74-4d24-ae46-84e33f534b8c.png):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得到 ![](img/c723faa4-5fbf-4295-b391-8033c2fd1267.png) 和 ![](img/01de849d-be74-4d24-ae46-84e33f534b8c.png)
    的值：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we predict the value of ![](img/e112d877-4c86-4eb5-8a57-207d77879881.png):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们预测 ![](img/e112d877-4c86-4eb5-8a57-207d77879881.png) 的值：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we compute the loss as given in equation *(3)*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们按照方程 *(3)* 计算损失：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we compute the mean squared error:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算均方误差：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we feed our randomly initialized `data` and model parameter, `theta`,
    `loss_function` returns the mean squared loss, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将随机初始化的`data`和模型参数`theta`输入`loss_function`时，会返回均方损失，如下所示：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, we need to minimize this loss. In order to minimize the loss, we need to
    calculate the gradient of the loss function, ![](img/45fcd047-b7d1-48af-9c70-f6f1b39ddc7f.png),
    with respect to the model parameters, ![](img/3b49a75c-ac98-4deb-9794-d54352e54182.png)
    and ![](img/7c7dcfb5-e088-4ddc-a962-f3de556ea04e.png), and update the parameter
    according to the parameter update rule. First, we will calculate the gradients
    of the loss function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要最小化这个损失。为了最小化损失，我们需要计算损失函数 ![](img/45fcd047-b7d1-48af-9c70-f6f1b39ddc7f.png)
    关于模型参数 ![](img/3b49a75c-ac98-4deb-9794-d54352e54182.png) 和 ![](img/7c7dcfb5-e088-4ddc-a962-f3de556ea04e.png)
    的梯度，并根据参数更新规则更新参数。首先，我们将计算损失函数的梯度。
- en: Computing the gradients of the loss function
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失函数的梯度
- en: 'The gradients of the loss function, ![](img/b211122e-b4e6-4b47-8167-609a67e805e9.png),
    with respect to the parameter ![](img/92d4859f-5356-498f-af86-ca5a144e87ff.png),
    are given as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数关于参数 ![](img/92d4859f-5356-498f-af86-ca5a144e87ff.png) 的梯度如下：
- en: '![](img/3df6bc2d-615e-4d85-990c-70c6f37a9e7e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3df6bc2d-615e-4d85-990c-70c6f37a9e7e.png)'
- en: 'The gradients of the loss function, ![](img/5f103b31-1faf-47d1-a933-6b5aed4a6f25.png),
    with respect to the parameter ![](img/1baea87c-c5e2-4509-8c22-a0d897bb8a4c.png),
    are given as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数关于参数 ![](img/1baea87c-c5e2-4509-8c22-a0d897bb8a4c.png) 的梯度如下：
- en: '![](img/4dbfc095-8767-4469-9b9e-0a2eee952e30.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dbfc095-8767-4469-9b9e-0a2eee952e30.png)'
- en: 'We define a function called `compute_gradients`, which takes the parameters, `data`
    and `theta` as input and returns the computed gradients:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`compute_gradients`的函数，该函数接受输入参数`data`和`theta`，并返回计算得到的梯度：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we need to initialize the gradients:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要初始化梯度：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we need to save the total number of data points in `N`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要保存数据点的总数`N`：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can get the value of ![](img/b59cf398-1332-48c2-99fb-1226fb8719b3.png)
    and ![](img/8037f57c-68af-4026-b3ce-55f7b47f76c9.png):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以得到 ![](img/b59cf398-1332-48c2-99fb-1226fb8719b3.png) 和 ![](img/8037f57c-68af-4026-b3ce-55f7b47f76c9.png)
    的值：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We do the same for each iteration:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个迭代执行相同操作：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we get the value of ![](img/095dfe47-f3a5-491d-9442-dd60338e16a7.png)
    and ![](img/0836a358-33f7-433e-b93b-6cb493da3be0.png):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到 ![](img/095dfe47-f3a5-491d-9442-dd60338e16a7.png) 和 ![](img/0836a358-33f7-433e-b93b-6cb493da3be0.png)
    的值：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we compute the gradient of the loss with respect to ![](img/77bc2cf8-0682-4bfb-b6d6-66e98369310e.png),
    as given in equation *(4)*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们按照方程 *(4)* 计算损失关于 ![](img/77bc2cf8-0682-4bfb-b6d6-66e98369310e.png) 的梯度：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, we compute the gradient of the loss with respect to ![](img/76230e49-4358-4f6b-a92c-a882cf6b66d2.png),
    as given in equation *(5)*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据方程 *(5)* 计算损失相对于 ![](img/76230e49-4358-4f6b-a92c-a882cf6b66d2.png) 的梯度：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We need to add `epsilon` to avoid division by zero error:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加`epsilon`以避免零除错误：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When we feed our randomly initialized `data` and `theta` model parameter, the
    `compute_gradients` function returns the gradients with respect to ![](img/f22c0dc5-90aa-43a5-9066-4d535fe01ef6.png),
    that is, ![](img/8aac270f-aeba-4a18-b172-5ba9b9ae9873.png), and gradients with
    respect to ![](img/8f3c9fe0-900e-493a-992f-e0962ea44bfc.png), that is, ![](img/bdece2b4-a275-4ac0-9929-27fa8b00ec1a.png),
    as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们输入我们随机初始化的`data`和`theta`模型参数时，`compute_gradients`函数返回相对于 ![](img/f22c0dc5-90aa-43a5-9066-4d535fe01ef6.png)
    即 ![](img/8aac270f-aeba-4a18-b172-5ba9b9ae9873.png)，以及相对于 ![](img/8f3c9fe0-900e-493a-992f-e0962ea44bfc.png)
    即 ![](img/bdece2b4-a275-4ac0-9929-27fa8b00ec1a.png) 的梯度，如下所示：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Updating the model parameters
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新模型参数
- en: 'Now that we''ve computed the gradients, we need to update our model parameters
    according to our update rule, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出梯度，我们需要根据我们的更新规则更新我们的模型参数，如下所示：
- en: '![](img/76e81629-8d88-4126-82ac-ba628b371785.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76e81629-8d88-4126-82ac-ba628b371785.png)'
- en: '![](img/f46ada57-bf52-4003-8dd5-13571e32e5f5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f46ada57-bf52-4003-8dd5-13571e32e5f5.png)'
- en: 'Since we stored ![](img/9414fad6-994b-4f63-beed-5d4a2d3aa118.png) in `theta[0]`
    and ![](img/ee192345-3284-48bc-907c-1203e34d5266.png) in `theta[1]`, we can write
    our update equation as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在`theta[0]`中存储了 ![](img/9414fad6-994b-4f63-beed-5d4a2d3aa118.png)，在`theta[1]`中存储了
    ![](img/ee192345-3284-48bc-907c-1203e34d5266.png)，所以我们可以写出我们的更新方程如下：
- en: '![](img/cb0718ae-0508-4563-aa40-75297f1ab863.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb0718ae-0508-4563-aa40-75297f1ab863.png)'
- en: As we learned in the previous section, updating gradients on just one iteration
    will not lead us to convergence, that is, the minimum of the cost function, so
    we need to compute gradients and update the model parameter for several iterations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中学到的，仅在一个迭代中更新梯度不会导致我们收敛到成本函数的最小值，因此我们需要计算梯度并对模型参数进行多次迭代更新。
- en: 'First, we need to set the number of iterations:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置迭代次数：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we need to define the learning rate:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义学习率：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will define a list called `loss` for storing the loss on every iteration:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个名为`loss`的列表来存储每次迭代的损失：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'On each iteration, we will calculate and update the gradients according to
    our parameter update rule from equation *(8)*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们将根据我们的参数更新规则从方程 *(8)* 计算并更新梯度：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we need to plot the `loss` (`Cost`) function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要绘制`loss`（`Cost`）函数：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following plot shows how the loss (**Cost**) decreases over the training
    iterations:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了损失（**Cost**）随训练迭代次数减少的情况：
- en: '![](img/39abba35-483b-4ff6-b7e7-e898034b0412.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39abba35-483b-4ff6-b7e7-e898034b0412.png)'
- en: Thus, we learned that gradient descent can be used to find the optimal parameters
    of the model, which we can then use to minimize the loss. In the next section,
    we will learn about several variants of the gradient descent algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们学会了梯度下降可以用来找到模型的最优参数，然后我们可以用它来最小化损失。在下一节中，我们将学习梯度下降算法的几种变体。
- en: Gradient descent versus stochastic gradient descent
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降与随机梯度下降的对比
- en: We update the parameter of the model multiple times with our parameter update
    equation *(1)* until we find the optimal parameter value. In gradient descent,
    to perform a single parameter update, we iterate through all the data points in
    our training set. So, every time we update the parameters of the model, we iterate
    through all the data points in the training set. Updating the parameters of the
    model only after iterating through all the data points in the training set makes
    gradient descent very slow and it will increase the training time, especially
    when we have a large dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的参数更新方程 *(1)* 多次更新模型参数，直到找到最优参数值。在梯度下降中，为了执行单个参数更新，我们遍历训练集中的所有数据点。因此，每次更新模型参数时，我们都要遍历训练集中的所有数据点。仅在遍历训练集中的所有数据点后更新模型参数使得梯度下降非常缓慢，并且会增加训练时间，特别是当数据集很大时。
- en: Let's say we have a training set with 1 million data points. We know that we
    update the parameters of the model multiple times to find the optimal parameter
    value. So, even to perform a single parameter update, we go through all 1 million
    data points in our training set and then update the model parameters. This will
    definitely make the training slow. This is because we can't just find the optimal
    parameter with a single update; we need to update the parameters of the model
    several times to find the optimal value. So, if we iterate through all 1 million
    data points in our training set for every parameter update, it will definitely
    slow down our training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含100万数据点的训练集。我们知道，我们需要多次更新模型的参数才能找到最优参数值。因此，即使是执行单次参数更新，我们也需要遍历训练集中的所有100万数据点，然后更新模型参数。这无疑会使训练变慢。这是因为我们不能仅通过单次更新找到最优参数；我们需要多次更新模型参数才能找到最优值。因此，如果我们对训练集中的每个参数更新都进行100万次数据点的迭代，这肯定会减慢我们的训练速度。
- en: Thus, to combat this, we introduce **stochastic gradient descent** (**SGD**).
    Unlike gradient descent, we don't have to wait to update the parameter of the
    model after iterating all the data points in our training set; we just update
    the parameters of the model after iterating through every single data point in
    our training set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了应对这一情况，我们引入了**随机梯度下降**（**SGD**）。与梯度下降不同，我们无需等待在训练集中迭代所有数据点后更新模型参数；我们只需在遍历训练集中的每一个数据点后更新模型参数。
- en: Since we update the parameters of the model in SGD after iterating every single
    data point, it will learn the optimal parameter of the model faster compared to
    gradient descent, and this will minimize the training time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在随机梯度下降中在遍历每个单独数据点后更新模型参数，相比梯度下降，它将更快地学习到模型的最优参数，从而缩短训练时间。
- en: How is SGD useful? When we have a huge dataset, by using the vanilla gradient
    descent method, we update the parameters only after iterating through all the
    data points in that huge dataset. So, after many iterations over the whole dataset,
    we reach convergence and, apparently, it takes a long time. But, in SGD, we update
    the parameters after iterating through every single training sample. That is,
    we are learning to find the optimal parameters right from the first training sample,
    which helps to attain convergence faster compared to the vanilla gradient descent
    method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 的作用是什么？当我们有一个巨大的数据集时，使用传统的梯度下降方法，我们只在遍历完整个数据集中的所有数据点后更新参数。因此，在整个数据集上进行多次迭代后，我们才能达到收敛，并且显然这需要很长时间。但是，在随机梯度下降中，我们在遍历每个单独的训练样本后更新参数。也就是说，我们从第一个训练样本开始就学习到寻找最优参数，这有助于相比传统的梯度下降方法更快地达到收敛。
- en: 'We know that the epoch specifies the number of times the neural network sees
    the whole training data. Thus, in gradient descent, on each epoch, we perform
    the parameter update. This means that, after every epoch, the neural networks
    see the whole training data. We perform the parameter update for each epoch as
    follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，周期指的是神经网络查看整个训练数据的次数。因此，在梯度下降中，每个周期我们执行参数更新。这意味着，在每个周期结束后，神经网络看到整个训练数据。我们按以下方式每个周期执行参数更新：
- en: '![](img/f74c110a-5011-45ad-a74a-258cc6163d99.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f74c110a-5011-45ad-a74a-258cc6163d99.png)'
- en: 'However, in stochastic gradient descent, we don''t have to wait until the completion
    of each epoch to update the parameters. That is, we don''t have to wait until
    the neural network sees the whole training data to update the parameters. Instead,
    we update the parameters of the network right from seeing a single training sample
    for each epoch:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在随机梯度下降中，我们无需等到每个周期完成后才更新参数。也就是说，我们不需要等到神经网络看到整个训练数据后才更新参数。相反，我们在每个周期中从看到单个训练样本开始就更新网络的参数：
- en: '![](img/adc3a4b7-55bb-445a-8144-371d292009e2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adc3a4b7-55bb-445a-8144-371d292009e2.png)'
- en: 'The following contour plot shows how gradient descent and stochastic gradient
    descent perform the parameter updates and find the minimum cost. The star symbol
    in the center of the plot denotes the position where we have a minimum cost. As
    you can see, SGD reaches convergence faster than vanilla gradient descent. You
    can also observe the oscillations in the gradient steps on SGD; this is because
    we are updating the parameter for every training sample, so, the gradient step
    in SGD changes frequently compared to the vanilla gradient descent:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了梯度下降和随机梯度下降如何执行参数更新并找到最小成本。图中心的星号符号表示我们最小成本的位置。正如您所见，随机梯度下降比普通梯度下降更快地达到收敛。您还可以观察到随机梯度下降中梯度步骤的振荡；这是因为我们在每个训练样本上更新参数，因此与普通梯度下降相比，SGD中的梯度步骤变化频繁：
- en: '![](img/9d57ee66-4d0d-4b1d-a9b9-ece39b2d86b0.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d57ee66-4d0d-4b1d-a9b9-ece39b2d86b0.png)'
- en: There is also another variant of gradient descent called **mini-batch gradient
    descent**. It takes the pros of both vanilla gradient descent and stochastic gradient
    descent. In SGD, we saw that we update the parameter of the model for every training
    sample. However, in mini-batch gradient descent, instead of updating the parameters
    after iterating each training sample, we update the parameters after iterating
    some batches of data points. Let's say the batch size is 50, which means that
    we update the parameter of the model after iterating through 50 data points instead
    of updating the parameter after iterating through each individual data point.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种称为**小批量梯度下降**的梯度下降变体。它吸取了普通梯度下降和随机梯度下降的优点。在SGD中，我们看到我们为每个训练样本更新模型的参数。然而，在小批量梯度下降中，我们不是在每个训练样本迭代后更新参数，而是在迭代一些数据点的批次后更新参数。假设批量大小为50，这意味着我们在迭代50个数据点后更新模型的参数，而不是在迭代每个单独数据点后更新模型的参数。
- en: 'The following diagram shows the contour plot of SGD and mini-batch gradient
    descent:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了SGD和小批量梯度下降的轮廓图：
- en: '![](img/db95290c-235f-4926-a23f-e78df9cc9651.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db95290c-235f-4926-a23f-e78df9cc9651.png)'
- en: 'Here are the differences between these types of gradient descent in a nutshell:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些梯度下降类型之间的差异如下：
- en: '**Gradient descent**: Updates the parameters of the model after iterating through
    all the data points in the training set'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**：在训练集中迭代所有数据点后更新模型参数'
- en: '**Stochastic gradient descent**: Updates the parameter of the model after iterating
    through every single data point in the training set'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**：在训练集中迭代每个单独数据点后更新模型的参数'
- en: '**Mini-batch gradient descent**: Updates the parameters of the model after
    iterating *n* number of data points in the training set'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**：在训练集中迭代*n*个数据点后更新模型参数'
- en: Mini-batch gradient descent is preferred over vanilla gradient descent and SGD
    for large datasets since mini-batch gradient descent outperforms the other two.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，小批量梯度下降优于普通梯度下降和SGD，因为小批量梯度下降的表现优于其他两种方法。
- en: The code for mini-batch gradient descent is as follows.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降的代码如下所示。
- en: 'First, we need to define the `minibatch` function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义`minibatch`函数：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will define `minibatch_size` by multiplying the length of data by
    `minibatch_ratio`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过将数据长度乘以`minibatch_ratio`来定义`minibatch_size`：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, on each iteration, we perform the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每次迭代中，我们执行以下操作：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, select `sample_size`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，选择`sample_size`：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, sample the data based on `sample_size`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于`sample_size`对数据进行抽样：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Compute the gradients for `sample_data` with respect to `theta`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`sample_data`相对于`theta`的梯度：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After computing the gradients for the sampled data with the given mini-batch
    size, we update the model parameter, `theta`, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了给定小批量大小的抽样数据的梯度后，我们按以下方式更新模型参数`theta`：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Momentum-based gradient descent
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于动量的梯度下降
- en: In this section, we will learn about two new variants of gradient descent, called
    **momentum** and Nesterov accelerated gradient.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习两种新的梯度下降变体，称为**动量**和Nesterov加速梯度。
- en: Gradient descent with momentum
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有动量的梯度下降
- en: 'We have a problem with SGD and mini-batch gradient descent due to the oscillations
    in the parameter update. Take a look at the following plot, which shows how mini-batch
    gradient descent is attaining convergence. As you can see, there are oscillations
    in the gradient steps. The oscillations are shown by the dotted line. As you may
    notice, it is making a gradient step toward one direction, and then taking a different
    direction, and so on, until it reaches convergence:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在SGD和小批量梯度下降中遇到了问题，因为参数更新中存在振荡。看看下面的图表，展示了小批量梯度下降是如何达到收敛的。正如您所见，梯度步骤中存在振荡。振荡由虚线表示。您可能注意到，它朝一个方向迈出梯度步骤，然后朝另一个方向，依此类推，直到达到收敛：
- en: '![](img/b3701eba-0ceb-4d34-81c4-474e065d9c03.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3701eba-0ceb-4d34-81c4-474e065d9c03.png)'
- en: This oscillation occurs because, since we update the parameters after iterating
    every *n* number of data points, the direction of the update will have some variance,
    and this leads to oscillations in every gradient step. Due to this oscillation,
    it is hard to reach convergence, and it slows down the process of attaining it.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种振荡是由于我们在迭代每个*n*个数据点后更新参数，更新方向会有一定的变化，从而导致每个梯度步骤中的振荡。由于这种振荡，很难达到收敛，并且减慢了达到收敛的过程。
- en: To alleviate this, we'll introduce a new technique called **momentum**. If we
    can understand what the right direction is for the gradient steps to attain convergence
    faster, then we can make our gradient steps navigate in that direction and reduce
    the oscillation in the irrelevant directions; that is, we can reduce taking directions
    that do not lead us to convergence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，我们将介绍一种称为**动量**的新技术。如果我们能够理解梯度步骤达到更快收敛的正确方向，那么我们可以使我们的梯度步骤在那个方向导航，并减少不相关方向上的振荡；也就是说，我们可以减少采取不导致收敛的方向。
- en: So, how can we do this? We basically take a fraction of the parameter update
    from the previous gradient step and add it to the current gradient step. In physics,
    momentum keeps an object moving after a force is applied. Here, the momentum keeps
    our gradient moving toward the direction that leads to convergence.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何做呢？基本上，我们从前一梯度步骤的参数更新中获取一部分，并添加到当前梯度步骤中。在物理学中，动量在施加力后使物体保持运动。在这里，动量使我们的梯度保持朝向导致收敛的方向运动。
- en: 'If you take a look at the following equation, you can see we are basically
    taking the parameter update from the previous step, ![](img/e5b41e74-20a9-4e47-af2c-8986f1a87c88.png),
    and adding it to the current gradient step, ![](img/1e7bcf4d-985f-4ad8-b1f9-e1ac00461878.png).
    How much information we want to take from the previous gradient step depends on
    the factor, that is, ![](img/51a6304d-576d-4ec3-a26e-664ad0e61caf.png), and the
    learning rate, which is denoted by ![](img/954de68e-2d2f-4380-9aaa-00deeac3e008.png):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看一下以下方程，您会看到我们基本上是从上一步的参数更新中取参数更新，![](img/e5b41e74-20a9-4e47-af2c-8986f1a87c88.png)，并将其添加到当前梯度步骤，![](img/1e7bcf4d-985f-4ad8-b1f9-e1ac00461878.png)。我们希望从前一个梯度步骤中获取多少信息取决于因子，即，![](img/51a6304d-576d-4ec3-a26e-664ad0e61caf.png)，以及学习率，用![](img/954de68e-2d2f-4380-9aaa-00deeac3e008.png)表示：
- en: '![](img/eb83f676-d492-4491-8ab1-614bdfba32e4.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb83f676-d492-4491-8ab1-614bdfba32e4.png)'
- en: In the preceding equation, ![](img/b2c9306e-e113-449b-a0ff-e55c2109e875.png)
    is called velocity, and it accelerates gradients in the direction that leads to
    convergence. It also reduces oscillations in an irrelevant direction by adding
    a fraction of a parameter update from the previous step to the current step.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，![](img/b2c9306e-e113-449b-a0ff-e55c2109e875.png) 被称为速度，它加速梯度朝向收敛方向的更新。它还通过在当前步骤中添加来自上一步参数更新的一部分，来减少不相关方向上的振荡。
- en: 'Thus, the parameter update equation with momentum is expressed as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有动量的参数更新方程如下表达：
- en: '![](img/53ac9a89-4fca-4b09-bbce-267e5ad12953.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53ac9a89-4fca-4b09-bbce-267e5ad12953.png)'
- en: By doing this, performing mini-batch gradient descent with momentum helps us
    to reduce oscillations in gradient steps and attain convergence faster.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，使用动量进行小批量梯度下降有助于减少梯度步骤中的振荡，并更快地达到收敛。
- en: Now, let's look at the implementation of momentum.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看动量的实现。
- en: 'First, we define the `momentum` function, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`momentum`函数，如下所示：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we initialize `vt` with zeros:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们用零初始化`vt`：
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following code is executed to cover the range for each iteration:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码用于每次迭代覆盖范围：
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we compute `gradients` with respect to `theta`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算相对于`theta`的`gradients`：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we update `vt` to be ![](img/477c810a-9d08-43a5-ab14-d4fde01d0850.png):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更新`vt`为![](img/477c810a-9d08-43a5-ab14-d4fde01d0850.png)：
- en: '[PRE41]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we update the model parameter, `theta`, as ![](img/7f818a1c-0e65-4f8a-a21f-0adceb0497c5.png):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们更新模型参数`theta`，如下所示：
- en: '[PRE42]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Nesterov accelerated gradient
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nesterov加速梯度
- en: 'One problem with momentum is that it might miss out the minimum value. That
    is, as we move closer toward convergence (the minimum point), the value for momentum
    will be high. When the value of momentum is high while we are near to attaining
    convergence, then the momentum actually pushes the gradient step high and it might
    miss out on the actual minimum value; that is, it might overshoot the minimum
    value when the momentum is high when we are near to convergence, as shown in the
    following diagram:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法的一个问题是可能会错过最小值。也就是说，当我们接近收敛（最小点）时，动量的值会很高。当动量值在接近收敛时很高时，实际上动量会推动梯度步长变高，并且可能错过实际的最小值；也就是说，当动量在接近收敛时很高时，它可能会超出最小值，如下图所示：
- en: '![](img/65dae543-ef11-42d3-9c8c-5bcea24c3d81.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65dae543-ef11-42d3-9c8c-5bcea24c3d81.png)'
- en: To overcome this, Nesterov introduced a new method called **Nesterov accelerated
    gradient** (**NAG**).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一问题，Nesterov引入了一种新的方法，称为**Nesterov加速梯度**（**NAG**）。
- en: The fundamental motivation behind Nesterov momentum is that, instead of calculating
    the gradient at the current position, we calculate gradients at the position where
    the momentum would take us to, and we call that position the lookahead position.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov动量背后的基本动机是，我们不是在当前位置计算梯度，而是在动量将带我们到的位置计算梯度，我们称之为前瞻位置。
- en: 'What does this mean, though? In the *Gradient descent with momentum* section,
    we learned about the following equation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这到底意味着什么呢？在*带动量的梯度下降*部分，我们了解到以下方程：
- en: '![](img/059fe057-0350-4fb5-85fe-51f27aee3500.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/059fe057-0350-4fb5-85fe-51f27aee3500.png)'
- en: The preceding equation tells us that we are basically pushing the current gradient
    step, ![](img/abcf414f-c3e2-49c4-b08e-4b57132297c0.png), to a new position using
    a fraction of the parameter update from the previous step, ![](img/ca74fb91-69b3-40a4-b3bc-a3ade6e543c8.png),
    which will help us to attain convergence. However, when the momentum is high,
    this new position will actually overshoot the minimum value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程告诉我们，我们基本上是用前一步参数更新的一部分来推动当前的梯度步长`vt`到一个新位置，这将帮助我们达到收敛。然而，当动量很高时，这个新位置实际上会超过最小值。
- en: Thus, before making a gradient step with momentum and reaching a new position,
    if we understand which position the momentum will take us to, then we can avoid
    overshooting the minimum value. If we find out that momentum will take us to the
    position that actually misses the minimum value, then we can slow down the momentum
    and try to reach the minimum value.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用动量进行梯度步进并到达新位置之前，如果我们理解动量将带我们到哪个位置，我们就可以避免超出最小值。如果我们发现动量会带我们到实际错过最小值的位置，那么我们可以减缓动量并尝试达到最小值。
- en: But how can we find the position that the momentum will take us to? In equation
    *(2)*, instead of calculating gradients with respect to the current gradient step,
    ![](img/d0ad33ba-05e6-4e68-af6e-0fb4d120ac05.png), we calculate gradients with
    respect to ![](img/8c413a95-a3a6-4d0e-b2d9-a6c990ffe441.png). The term, ![](img/eafe1ca8-2fa1-4d49-8c41-ec9d829f547c.png),
    basically tells us the approximate position of where our next gradient step is
    going to be. We call this the lookahead position. This gives us an idea of where
    our next gradient step is going to be.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何找到动量将带我们到的位置呢？在方程*(2)*中，我们不是根据当前梯度步长`vt`计算梯度，而是根据`vt`的前瞻位置计算梯度。术语`1`基本上告诉我们我们下一个梯度步长的大致位置，我们称之为前瞻位置。这给了我们一个关于下一个梯度步长将在哪里的想法。
- en: 'So, we can rewrite our ![](img/92d548ad-7fe9-4902-8393-cf2e0cf0be58.png) equation
    according to NAG as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以根据NAG重新编写我们的方程`vt`如下：
- en: '![](img/2801d3ce-6ad0-4ac9-894f-db0adbdd79d7.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2801d3ce-6ad0-4ac9-894f-db0adbdd79d7.png)'
- en: 'We update our parameter as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新我们的参数如下：
- en: '![](img/b08cded8-6f68-4913-b47e-ac42ad3b9c74.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b08cded8-6f68-4913-b47e-ac42ad3b9c74.png)'
- en: Updating the parameters with the preceding equation prevents us from missing
    the minimum value by slowing down the momentum when the gradient steps are near
    to convergence. The Nesterov accelerated method is implemented as follows.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述方程更新参数可以通过在梯度步骤接近收敛时减慢动量来防止错过最小值。Nesterov加速方法的实现如下。
- en: 'First, we define the `NAG` function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`NAG`函数：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, we initialize the value of `vt` with zeros:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们用零初始化`vt`的值：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们执行以下步骤：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, we need to compute the gradients with respect to ![](img/c47a48e3-6f6b-4ff0-8ae5-b215be5228bc.png):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算相对于![](img/c47a48e3-6f6b-4ff0-8ae5-b215be5228bc.png)的梯度：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we update `vt` as ![](img/8ce44c3a-e42b-4a47-b1ab-728842010ff6.png):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`vt`更新为![](img/8ce44c3a-e42b-4a47-b1ab-728842010ff6.png)：
- en: '[PRE47]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we update the model parameter, `theta`, to ![](img/6f0cfa20-7bf8-4250-8dd8-d9bd7ae53190.png):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将模型参数`theta`更新为![](img/6f0cfa20-7bf8-4250-8dd8-d9bd7ae53190.png)：
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Adaptive methods of gradient descent
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的自适应方法
- en: In this section, we will learn about several adaptive versions of gradient descent.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习几种梯度下降的自适应版本。
- en: Setting a learning rate adaptively using Adagrad
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Adagrad自适应设置学习率
- en: When we build a deep neural network, we have many parameters. Parameters are
    basically the weights of the network, so when we build a network with many layers,
    we will have many weights, say, ![](img/921bdcf6-8faf-4582-98bd-5aa0143a103e.png).
    Our goal is to find the optimal values for all these weights. In all of the previous
    methods we learned about, the learning rate was a common value for all the parameters
    of the network. However **Adagrad** (short for **adaptive gradient**) adaptively
    sets the learning rate according to a parameter.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建深度神经网络时，会有很多参数。参数基本上是网络的权重，所以当我们构建具有多层的网络时，会有很多权重，比如![](img/921bdcf6-8faf-4582-98bd-5aa0143a103e.png)。我们的目标是找到所有这些权重的最优值。在我们之前学到的所有方法中，网络参数的学习率是一个常见值。然而，**Adagrad**（自适应梯度的简称）会根据参数自适应地设置学习率。
- en: Parameters that have frequent updates or high gradients will have a slower learning
    rate, while a parameter that has an infrequent update or small gradients will
    also have a slower learning rate. But why do we have to do this? It is because
    parameters that have infrequent updates implies that they are not trained enough,
    so we set a high learning rate for them, and parameters that have frequent updates
    implies that they are trained enough, so we set their learning rate to a low value
    so that we don't overshoot the minimum.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 经常更新或梯度较大的参数将具有较慢的学习率，而更新不频繁或梯度较小的参数也将具有较慢的学习率。但是我们为什么要这样做？这是因为更新不频繁的参数意味着它们没有被充分训练，所以我们为它们设置较高的学习率；而频繁更新的参数意味着它们已经足够训练，所以我们为它们设置较低的学习率，以防止超出最小值。
- en: 'Now, let''s see how Adagrad adaptively changes the learning rate. Previously,
    we represented the gradient with ![](img/0eb9264a-0aff-4e98-9416-98201ca0f47b.png).
    For simplicity, from now on in this chapter, we''ll represent gradients with ![](img/8e969c03-6112-4f27-bef1-7152b8e194c9.png).
    So, the gradient of a parameter, ![](img/a03c809e-24b3-4c69-998b-b13ef1878d0d.png),
    at an iteration, ![](img/8343d84a-b952-4bbf-b4fb-e81e0f63bcad.png), can be represented
    as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看Adagrad如何自适应地改变学习率。之前，我们用![](img/0eb9264a-0aff-4e98-9416-98201ca0f47b.png)表示梯度。为简单起见，在本章中，我们将用![](img/8e969c03-6112-4f27-bef1-7152b8e194c9.png)代表梯度。因此，参数![](img/a03c809e-24b3-4c69-998b-b13ef1878d0d.png)在迭代![](img/8343d84a-b952-4bbf-b4fb-e81e0f63bcad.png)时的梯度可以表示为：
- en: '![](img/e696fced-9b54-41eb-9cc6-ae64180154bb.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e696fced-9b54-41eb-9cc6-ae64180154bb.png)'
- en: 'Therefore, we can rewrite our update equation with ![](img/dfde1993-0b27-4d94-908e-44d8452e8bc6.png)
    as the gradient notation as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用![](img/dfde1993-0b27-4d94-908e-44d8452e8bc6.png)来重新编写我们的更新方程，作为梯度符号表示如下：
- en: '![](img/8a17fafe-c7e0-4222-a3db-e6a25d8483c8.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a17fafe-c7e0-4222-a3db-e6a25d8483c8.png)'
- en: 'Now, for every iteration, ![](img/b848153f-c88b-42de-abb7-fa2f30ed7c90.png),
    to update a parameter, ![](img/9954b549-05fa-45fa-b92c-c9cbcd3d9dca.png), we divide
    the learning rate by the sum of squares of all previous gradients of the parameter,
    ![](img/e2bdd119-534b-4584-8954-5cf72f9c4495.png), as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每次迭代![](img/b848153f-c88b-42de-abb7-fa2f30ed7c90.png)，要更新参数![](img/9954b549-05fa-45fa-b92c-c9cbcd3d9dca.png)，我们将学习率除以参数![](img/e2bdd119-534b-4584-8954-5cf72f9c4495.png)的所有先前梯度的平方和，如下所示：
- en: '![](img/143ea471-82eb-4eaf-82e3-a9a5e35b60ee.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/143ea471-82eb-4eaf-82e3-a9a5e35b60ee.png)'
- en: Here, ![](img/a030af86-9b17-4871-972a-a78ca31c12b6.png) implies the sum of squares
    of all previous gradients of the parameter ![](img/545b96da-57b3-4919-9e41-50617a0db779.png).
    We added ![](img/2bb4c014-84ed-47e4-9403-7740622fd273.png) just to avoid the division
    by zero error. We typically set the value of ![](img/82a50656-7a02-443d-bd1c-03998473e2b3.png)
    to a small number. The question that arises here is, why are we dividing the learning
    rate by a sum of squares of all the previous gradients?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/a030af86-9b17-4871-972a-a78ca31c12b6.png) 表示参数![](img/545b96da-57b3-4919-9e41-50617a0db779.png)所有先前梯度的平方和。我们添加![](img/2bb4c014-84ed-47e4-9403-7740622fd273.png)
    只是为了避免除以零的错误。通常将![](img/82a50656-7a02-443d-bd1c-03998473e2b3.png) 的值设置为一个小数。这里出现的问题是，为什么我们要把学习率除以所有先前梯度的平方和？
- en: We learned that parameters that have frequent updates or high gradients will
    have a slower learning rate, while parameters that have an infrequent update or
    small gradients will also have a high learning rate.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，具有频繁更新或高梯度的参数将具有较慢的学习率，而具有不频繁更新或小梯度的参数也将具有较高的学习率。
- en: The sum,![](img/daa6a6d7-af78-45ce-988d-e3e226db103a.png), actually scales our
    learning rate. That is, when the sum of the squared past gradients has a high
    value, we are basically dividing the learning rate by a high value, so our learning
    rate will become less. Similarly, if the sum of the squared past gradients has
    a low value, we are dividing the learning rate by a lower value, so our learning
    rate value will become high. This implies that the learning rate is inversely
    proportional to the sum of the squares of all the previous gradients of the parameter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 总和，![](img/daa6a6d7-af78-45ce-988d-e3e226db103a.png)，实际上缩放了我们的学习率。也就是说，当过去梯度的平方和值较高时，我们基本上将学习率除以一个高值，因此我们的学习率将变得较低。类似地，如果过去梯度的平方和值较低，则我们将学习率除以一个较低的值，因此我们的学习率值将变高。这意味着学习率与参数的所有先前梯度的平方和成反比。
- en: 'Here, our update equation is expressed as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的更新方程表示如下：
- en: '![](img/ca780a96-e7a8-4e50-9f0f-1abcc686d073.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca780a96-e7a8-4e50-9f0f-1abcc686d073.png)'
- en: In a nutshell, in Adagrad, we set the learning rate to a low value when the
    previous gradient value is high, and to a high value when the past gradient value
    is lower. This means that our learning rate value changes according to the past
    gradient updates of the parameters.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在Adagrad中，当先前梯度值高时，我们将学习率设置为较低的值，当过去梯度值较低时，我们将学习率设置为较高的值。这意味着我们的学习率值根据参数的过去梯度更新而变化。
- en: Now that we have learned how the Adagrad algorithm works, let's strengthen our
    knowledge by implementing it. The code for the Adagrad algorithm is given as follows.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了Adagrad算法的工作原理，让我们通过实现它来加深我们的知识。Adagrad算法的代码如下所示。
- en: 'First, define the `AdaGrad` function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义`AdaGrad`函数：
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define the variable called `gradients_sum` to hold the sum of gradients and
    initialize them with zeros:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 定义名为`gradients_sum`的变量来保存梯度和，并将它们初始化为零：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们执行以下步骤：
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we compute the `gradients` of loss with respect to `theta`:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算损失对`theta`的梯度：
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, we calculate the sum of the gradients squared, that is, ![](img/8f690939-5865-4088-9bf5-13525f26c9af.png):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算梯度平方和，即![](img/8f690939-5865-4088-9bf5-13525f26c9af.png)：
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Afterward, we compute the gradient updates, that is, ![](img/b934756d-4983-4ce3-995e-5886ca0adc58.png):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们计算梯度更新，即![](img/b934756d-4983-4ce3-995e-5886ca0adc58.png)：
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, update the `theta` model parameter so that it''s ![](img/a76a05a7-2b37-4d98-86f5-6305bc9a7c31.png)
    :'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，更新`theta`模型参数，使其为![](img/a76a05a7-2b37-4d98-86f5-6305bc9a7c31.png) ：
- en: '[PRE55]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Again, there is a shortcoming associated with the Adagrad method. For every
    iteration, we are accumulating and summing all the past squared gradients. So,
    on every iteration, our sum of the squared past gradients value will increase.
    When the sum of the squared past gradient value is high, we will have a large
    number in the denominator. When we divide the learning rate by a very large number,
    then the learning rate will become very small. So, over several iterations, the
    learning rate starts decaying and becomes an infinitesimally small number – that
    is, our learning rate will be monotonically decreasing. When the learning rate
    reaches a very low value, then it takes a long time to attain convergence.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Adagrad 方法存在一个缺点。在每次迭代中，我们都会累积和求和所有过去的平方梯度。因此，在每次迭代中，过去平方梯度值的总和将会增加。当过去平方梯度值的总和较高时，分母中会有一个较大的数。当我们将学习率除以一个非常大的数时，学习率将变得非常小。因此，经过几次迭代后，学习率开始衰减并变成一个无限小的数值——也就是说，我们的学习率将单调递减。当学习率降至一个非常低的值时，收敛需要很长时间。
- en: In the next section, we will see how Adadelta tackles this shortcoming.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到Adadelta如何解决这个缺点。
- en: Doing away with the learning rate using Adadelta
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用Adadelta方法摒弃学习率
- en: Adadelta is an enhancement of the Adagrad algorithm. In Adagrad, we noticed
    the problem of the learning rate diminishing to a very low number. Although Adagrad
    learns the learning rate adaptively, we still need to set the initial learning
    rate manually. However, in Adadelta, we don't need the learning rate at all. So
    how does the Adadelta algorithm learn?
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta 是Adagrad算法的增强版。在Adagrad中，我们注意到学习率减小到一个非常低的数字的问题。虽然Adagrad能够自适应地学习学习率，但我们仍然需要手动设置初始学习率。然而，在Adadelta中，我们根本不需要学习率。那么，Adadelta算法是如何学习的呢？
- en: In Adadelta, instead of taking the sum of all the squared past gradients, we
    can set a window of size ![](img/d5c1b640-6064-4614-94c3-4711517041e8.png) and
    take the sum of squared past gradients only from that window. In Adagrad, we took
    the sum of all the squared past gradients and it led to the learning rate diminishing
    to a low number. To avoid that, we take the sum of the squared past gradients
    only from a window.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在Adadelta中，我们不是取所有过去的平方梯度的总和，而是设定一个大小为 ![](img/d5c1b640-6064-4614-94c3-4711517041e8.png)
    的窗口，并仅从该窗口中取过去的平方梯度的总和。在Adagrad中，我们取所有过去的平方梯度的总和，并导致学习率减小到一个低数字。为了避免这种情况，我们只从一个窗口内取过去的平方梯度的总和。
- en: 'If ![](img/7501c19e-7f5b-4007-9c3d-8bdd403bf972.png) is the window size, then
    our parameter update equation becomes the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/7501c19e-7f5b-4007-9c3d-8bdd403bf972.png) 是窗口大小，则我们的参数更新方程如下所示：
- en: '![](img/7d472342-c741-47d0-a9f3-c5df6d65ba19.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d472342-c741-47d0-a9f3-c5df6d65ba19.png)'
- en: However, the problem is that, although we are taking gradients only from within
    a window, ![](img/2ea21248-7d34-45bb-9ad0-c97848dfee51.png), squaring and storing
    all the gradients from the window in each iteration is inefficient. So, instead
    of doing that, we can take the running average of gradients.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，问题在于，虽然我们仅从一个窗口内取梯度，![](img/2ea21248-7d34-45bb-9ad0-c97848dfee51.png)，但在每次迭代中将窗口内所有梯度进行平方并存储是低效的。因此，我们可以采取梯度的运行平均值，而不是这样做。
- en: 'We compute the running average of gradients at an iteration, *t*, ![](img/5a963575-25d4-4ca2-b5fe-065ad4351973.png),
    by adding the previous running average of gradients, ![](img/bbb28974-77d5-48b2-92bb-f82995761522.png),
    and current gradients, ![](img/cc6925a0-661f-4248-90ba-21adadf5485b.png):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将先前的梯度的运行平均值 ![](img/bbb28974-77d5-48b2-92bb-f82995761522.png) 和当前梯度 ![](img/cc6925a0-661f-4248-90ba-21adadf5485b.png)
    相加来计算迭代 *t* 的梯度的运行平均值 ![](img/5a963575-25d4-4ca2-b5fe-065ad4351973.png)：
- en: '![](img/12f4e9d1-20aa-49d7-903a-a0315ba86207.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12f4e9d1-20aa-49d7-903a-a0315ba86207.png)'
- en: 'Instead of just taking the running average, we take the exponentially decaying
    running average of gradients, as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是取运行平均值，我们还采取梯度的指数衰减运行平均值，如下所示：
- en: '![](img/c1b0f509-4621-4956-a49d-15a19723991a.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1b0f509-4621-4956-a49d-15a19723991a.png)'
- en: Here, ![](img/77fc07fe-f0d6-4f74-a7f3-0270b2ff4fcf.png) is called the exponential
    decaying rate and is similar to the one we saw in momentum – that is, it is used
    for deciding how much information from the previous running average of gradients
    should be added.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/77fc07fe-f0d6-4f74-a7f3-0270b2ff4fcf.png) 被称为指数衰减率，类似于我们在动量中看到的那个——用于决定从前一次梯度的运行平均值中添加多少信息。
- en: 'Now, our update equation becomes the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的更新方程如下所示：
- en: '![](img/d800a845-64f6-4472-bc96-7b8153deccaf.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d800a845-64f6-4472-bc96-7b8153deccaf.png)'
- en: 'For notation simplicity, let''s denote ![](img/7e80684e-53c8-4895-a6ea-c820e2bd63c1.png)
    as ![](img/8ca829a5-4620-440f-996e-b7b673987d5c.png) so that we can rewrite the
    previous update equation as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，让我们将![](img/7e80684e-53c8-4895-a6ea-c820e2bd63c1.png)记作![](img/8ca829a5-4620-440f-996e-b7b673987d5c.png)，这样我们可以将前一个更新方程重写为如下形式：
- en: '![](img/aaa3abba-608c-4f8d-9597-72c0697b0e6e.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaa3abba-608c-4f8d-9597-72c0697b0e6e.png)'
- en: 'From the previous equation, we can infer the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述方程，我们可以推断如下：
- en: '![](img/383befa7-159b-41d5-aa26-194b0765a082.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/383befa7-159b-41d5-aa26-194b0765a082.png)'
- en: 'If you look at the denominator in the previous equation, we are basically computing
    the root mean squared of gradients up to an iteration, ![](img/c2272d39-a5b1-438a-8358-fa14ea1c5c1f.png),
    so we can simply write that in shorthand as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察前一个方程中的分母，我们基本上在计算迭代过程中梯度的均方根，![](img/c2272d39-a5b1-438a-8358-fa14ea1c5c1f.png)，因此我们可以简单地用以下方式写出：
- en: '![](img/4725e93c-7d92-4c8b-8064-632079ceb665.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4725e93c-7d92-4c8b-8064-632079ceb665.png)'
- en: 'By substituting equation *(13)* in equation *(12)*, we can write the following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将方程 *(13)* 代入方程 *(12)*，我们可以写出如下：
- en: '![](img/4f5ec229-a979-4ab0-9ebf-b780c0189982.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f5ec229-a979-4ab0-9ebf-b780c0189982.png)'
- en: 'However, we still have the learning rate, ![](img/7db8a465-4cf4-48dc-a7f4-e4273fee1c6b.png),
    term in our equation. How can we do away with that? We can do so by making the
    units of the parameter update in accordance with the parameter. As you may have
    noticed, the units of ![](img/9c736c49-b827-4714-ae7e-662a26876719.png) and ![](img/43cfa07c-dafd-46b8-bd4b-7003ded49a8e.png)
    don''t really match. To combat this, we compute the exponentially decaying average
    of the parameter updates,![](img/dc80e9c3-6926-4ca2-92fc-a7eaacbd810e.png), as
    we computed an exponentially decaying average of gradients, ![](img/6efc0ba3-2eb1-4a95-ba1f-75088f554f52.png),
    in equation *(10)*. So, we can write the following:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的方程中仍然有学习率，![](img/7db8a465-4cf4-48dc-a7f4-e4273fee1c6b.png)，术语。我们如何摆脱它？我们可以通过使参数更新的单位与参数相符来实现。正如你可能注意到的那样，![](img/9c736c49-b827-4714-ae7e-662a26876719.png)和![](img/43cfa07c-dafd-46b8-bd4b-7003ded49a8e.png)的单位并不完全匹配。为了解决这个问题，我们计算参数更新的指数衰减平均值，![](img/dc80e9c3-6926-4ca2-92fc-a7eaacbd810e.png)，正如我们在方程
    *(10)* 中计算了梯度的指数衰减平均值，![](img/6efc0ba3-2eb1-4a95-ba1f-75088f554f52.png)。因此，我们可以写出如下：
- en: '![](img/d1b38486-b79e-4a08-8954-b46cecc10789.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1b38486-b79e-4a08-8954-b46cecc10789.png)'
- en: 'It''s like the RMS of gradients, ![](img/74127744-93af-4dfc-bec6-57299b903588.png),
    which is similar to equation *(13)*. We can write the RMS of the parameter update
    as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 它类似于梯度的RMS，![](img/74127744-93af-4dfc-bec6-57299b903588.png)，类似于方程 *(13)*。我们可以将参数更新的RMS写成如下形式：
- en: '![](img/ce6f12d2-613a-4583-be31-93c4005b9be2.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce6f12d2-613a-4583-be31-93c4005b9be2.png)'
- en: However, the RMS value for the parameter update, ![](img/1f7ce6e6-cada-49fe-9014-08621044a329.png)
    is not known, that is, ![](img/5bdfb640-f031-4aad-bff5-860c7b834fbd.png) is not
    known, so we can just approximate it by considering until the previous update,
    ![](img/947ae65b-23c0-423b-a8c1-35c3ebe5a374.png).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，参数更新的RMS值，![](img/1f7ce6e6-cada-49fe-9014-08621044a329.png)，是未知的，即![](img/5bdfb640-f031-4aad-bff5-860c7b834fbd.png)是未知的，因此我们可以通过考虑直到上一个更新，![](img/947ae65b-23c0-423b-a8c1-35c3ebe5a374.png)，来近似计算它。
- en: 'Now, we just replace our learning rate with the RMS value of the parameter
    updates. That is, we replace ![](img/076e95dd-a926-4ef4-911b-cf48ceb58891.png)
    with ![](img/f5044369-2aeb-4887-9afd-0098598987d9.png) in equation *(14)* and
    write the following:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需用参数更新的RMS值取代学习率。也就是说，我们在方程 *(14)* 中用![](img/f5044369-2aeb-4887-9afd-0098598987d9.png)取代![](img/076e95dd-a926-4ef4-911b-cf48ceb58891.png)，并写出如下：
- en: '![](img/2751d98e-b867-4cfa-9244-8c3a0821fbe9.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2751d98e-b867-4cfa-9244-8c3a0821fbe9.png)'
- en: 'Substituting equation *(15)* in equation *(11)*, our final update equation
    becomes the following:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 *(15)* 代入方程 *(11)*，我们的最终更新方程如下：
- en: '![](img/17dd5b3c-fba9-4b5d-b575-fd73ac204e68.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17dd5b3c-fba9-4b5d-b575-fd73ac204e68.png)'
- en: '![](img/9bd169af-8202-43f9-991a-167b7682fa27.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bd169af-8202-43f9-991a-167b7682fa27.png)'
- en: Now, let's understand the Adadelta algorithm by implementing it.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过实现了解Adadelta算法。
- en: 'First, we define the `AdaDelta` function:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`AdaDelta`函数：
- en: '[PRE56]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then, we initialize the `E_grad2` variable with zero for storing the running
    average of gradients, and `E_delta_theta2` with zero for storing the running average
    of the parameter update, as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`E_grad2`变量初始化为零，用于存储梯度的运行平均值，并将`E_delta_theta2`初始化为零，用于存储参数更新的运行平均值，如下所示：
- en: '[PRE57]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代，我们执行以下步骤：
- en: '[PRE58]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, we need to compute the `gradients` with respect to `theta`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算相对于`theta`的`gradients`：
- en: '[PRE59]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Then, we can compute the running average of gradients:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算梯度的运行平均值：
- en: '[PRE60]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here, we will compute `delta_theta`, that is, ![](img/6082dff2-d77b-4a67-8d43-094b9c2dc17f.png):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将计算`delta_theta`，即，![](img/6082dff2-d77b-4a67-8d43-094b9c2dc17f.png)：
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, we can compute the running average of the parameter update, ![](img/fd64714f-4f7f-477d-a2e2-e9796e2ea107.png):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算参数更新的运行平均值，![](img/fd64714f-4f7f-477d-a2e2-e9796e2ea107.png)：
- en: '[PRE62]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we will update the parameter of the model, `theta`, so that it''s ![](img/e0a32cbd-f87c-4f52-981d-7fa487762814.png):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更新模型参数`theta`，使其变为![](img/e0a32cbd-f87c-4f52-981d-7fa487762814.png)：
- en: '[PRE63]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Overcoming the limitations of Adagrad using RMSProp
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过RMSProp克服Adagrad的限制
- en: 'Similar to Adadelta, RMSProp was introduced to combat the decaying learning
    rate problem of Adagrad. So, in RMSProp, we compute the exponentially decaying
    running average of gradients as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 与Adadelta类似，RMSProp被引入来解决Adagrad的学习率衰减问题。因此，在RMSProp中，我们如下计算梯度的指数衰减运行平均值：
- en: '![](img/70a39013-89e8-4d74-b5d2-af6198ba6446.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70a39013-89e8-4d74-b5d2-af6198ba6446.png)'
- en: 'Instead of taking the sum of the square of all the past gradients, we use this
    running average of gradients. This means that our update equation becomes the
    following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是取所有过去梯度的平方和，我们使用这些梯度的运行平均值。这意味着我们的更新方程如下：
- en: '![](img/95aacb08-98ff-4112-af96-544e87f5422e.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95aacb08-98ff-4112-af96-544e87f5422e.png)'
- en: It is recommended to assign a value of learning ![](img/0b5604c4-d979-49d8-ab84-f266667632fb.png)
    to `0.9`. Now, we will learn how to implement RMSProp in Python.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 建议将学习率![](img/0b5604c4-d979-49d8-ab84-f266667632fb.png)设置为`0.9`。现在，我们将学习如何在Python中实现RMSProp。
- en: 'First, we need to define the `RMSProp` function:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义`RMSProp`函数：
- en: '[PRE64]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we need to initialize the `E_grad2` variable with zeros to store the running
    average of gradients:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要用零来初始化`E_grad2`变量，以存储梯度的运行平均值：
- en: '[PRE65]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代时，我们执行以下步骤：
- en: '[PRE66]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Then, we compute the `gradients` with respect to `theta`:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算相对于`theta`的`gradients`：
- en: '[PRE67]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we compute the running average of the gradients, that is, ![](img/8b2a51a3-3e9f-4e6b-9390-7dc65e8dc37a.png):'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算梯度的运行平均值，即，![](img/8b2a51a3-3e9f-4e6b-9390-7dc65e8dc37a.png)：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, we update the parameter of the model, `theta`, so that it''s ![](img/9dbe5cdf-01b4-4668-afcf-6cbb72003eba.png):'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们更新模型参数`theta`，使其变为![](img/9dbe5cdf-01b4-4668-afcf-6cbb72003eba.png)：
- en: '[PRE69]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Adaptive moment estimation
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应矩估计
- en: '**Adaptive moment estimation**, known as **Adam** for short, is one of the
    most popularly used algorithms for optimizing a neural network. While reading
    about RMSProp, we learned that we compute the running average of squared gradients
    to avoid the diminishing learning rate problem:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计**，简称**Adam**，是优化神经网络中最常用的算法之一。在阅读有关RMSProp的内容时，我们了解到，为了避免学习率衰减问题，我们计算了平方梯度的运行平均值：'
- en: '![](img/b3f51f1d-6ade-492e-aefc-61e6c6ed5c25.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3f51f1d-6ade-492e-aefc-61e6c6ed5c25.png)'
- en: 'The final updated equation of RMSprop is given as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop的最终更新方程如下所示：
- en: '![](img/d1ea583f-7ebd-46e7-97c9-c7e426a35c2e.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1ea583f-7ebd-46e7-97c9-c7e426a35c2e.png)'
- en: Similar to this, in Adam, we also compute the running average of the squared
    gradients. However, along with computing the running average of the squared gradients,
    we also compute the running average of the gradients.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于此，在Adam中，我们还计算平方梯度的运行平均值。然而，除了计算平方梯度的运行平均值外，我们还计算梯度的运行平均值。
- en: 'The running average of gradients is given as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的运行平均值如下所示：
- en: '![](img/34ab5642-f504-4752-961d-143f1c859e11.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34ab5642-f504-4752-961d-143f1c859e11.png)'
- en: 'The running average of squared gradients is given as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 平方梯度的运行平均值如下所示：
- en: '![](img/d2a20842-c858-4d22-95d1-998e5419c757.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2a20842-c858-4d22-95d1-998e5419c757.png)'
- en: Since a lot of literature and libraries represent the decaying rate in Adam
    as ![](img/df2124c0-d034-4ed9-96db-054189b46924.png) instead of ![](img/363da8bd-bf34-46d1-b780-f2e525aacceb.png),
    we'll also use ![](img/24d59526-7084-423f-a6a1-08fea1dccecc.png) to represent
    the decaying rate in Adam. Thus, ![](img/c6812bc9-7617-4c73-8196-42cbf1358429.png)
    and ![](img/eba646cb-bdf2-4046-94a7-179bf57eda6f.png) in equations *(16)* and
    *(17)* denote the exponential decay rates for the running average of the gradients
    and the squared gradients, respectively.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our updated equation becomes the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9e421e2-9551-4936-9545-325f7c2063d9.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: The running average of the gradients and running average of the squared gradients
    are basically the first and second moments of those gradients. That is, they are
    the mean and uncentered variance of our gradients, respectively. So, for notation
    simplicity, let's denote ![](img/43b1f7ac-39b2-40e2-8878-a79457c93ed5.png) as
    ![](img/05c11c4a-cfa8-481c-a11e-2a93745c3ab9.png) and ![](img/251767bb-33e2-4ed5-87b6-b50ac865576d.png)
    as ![](img/bfa9ed56-26be-4646-92d5-db97d0355fe6.png).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can rewrite equations *(16)* and *(17)* as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b55ff174-f556-4654-b015-5ceb1446b637.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: '![](img/13ed75a5-2581-47ee-93e8-aa092bd94151.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: 'We begin by setting the initial moments estimates to zero. That is, we initialize
    ![](img/010c97de-58b7-4b89-83fa-792dd5765de4.png) and ![](img/a3c4b67f-174d-4436-958e-a7cc63f150c4.png)
    with zeros. When the initial estimates are set to 0, they remain very small, even
    after many iterations. This means that they would be biased toward 0, especially
    when ![](img/e97e4a53-38c4-4acd-b55a-73192dbe7cd5.png) and ![](img/238ebaf3-b706-4feb-8165-4e4385a92e20.png)
    are close to 1\. So, to combat this, we compute the bias-corrected estimates of
    ![](img/5b150a4c-513b-4491-8122-771f492ba611.png) and ![](img/2be81116-58a9-4bd2-9006-7893608d63f6.png)
    by just dividing them by ![](img/3afa1ee2-0ab9-41cf-b67b-57c433323f37.png), as
    follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1aa781fc-0e50-4598-b403-635dbb989ab4.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: '![](img/7eae202a-5884-43c6-be3f-a99dad9c0401.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b5328637-a5cd-4996-afa9-b53378a30465.png) and ![](img/fcade50e-ce09-46db-8c7e-e2f5080ff36a.png)
    are the bias-corrected estimates of ![](img/8b11788d-7089-4728-8c3c-b8f31f713db4.png)
    and ![](img/98cb351b-581c-4cee-8c75-873a17945df2.png), respectively.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our final update equation is given as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6246a7f0-a18e-4932-90f1-fc6bc3558b6a.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Now, let's understand how to implement Adam in Python.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define the `Adam` function, as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    `zeros`:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Next, we compute the `gradients` with respect to `theta`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Then, we update the first moment, `mt`, so that it''s ![](img/6479d95b-8005-4966-984d-3200dab96bed.png):'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Next, we update the second moment, `vt`, so that it''s ![](img/dcdf171a-595f-4074-8ec9-25495fc6f7af.png):'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更新第二矩`vt`，使其为![](img/dcdf171a-595f-4074-8ec9-25495fc6f7af.png)：
- en: '[PRE75]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now, we compute the bias-corrected estimate of `mt`, that is, ![](img/9fed01d0-11bd-44c4-88d8-27ebc5ae8b3a.png)
    :'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算偏差校正估计的`mt`，即![](img/9fed01d0-11bd-44c4-88d8-27ebc5ae8b3a.png)：
- en: '[PRE76]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Next, we compute the bias-corrected estimate of `vt`, that is, ![](img/5452eea8-5aa0-493d-ad9b-dc39f12b950d.png):'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算偏差校正估计的`vt`，即![](img/5452eea8-5aa0-493d-ad9b-dc39f12b950d.png)：
- en: '[PRE77]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Finally, we update the model parameter, `theta`, so that it''s ![](img/182717ec-ad2c-4820-8c06-bca033b45355.png):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新模型参数`theta`，使其为![](img/182717ec-ad2c-4820-8c06-bca033b45355.png)：
- en: '[PRE78]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Adamax – Adam based on infinity-norm
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adamax – 基于无穷范数的Adam
- en: 'Now, we will look at a small variant of the Adam algorithm called **Adamax**.
    Let''s recall the equation of the second-order moment in Adam:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看一个叫**Adamax**的Adam算法的一个小变体。让我们回忆一下Adam中的二阶矩方程：
- en: '![](img/09810da8-e2a4-4c52-80b6-218dcf0b869f.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09810da8-e2a4-4c52-80b6-218dcf0b869f.png)'
- en: 'As you may have noticed from the preceding equation, we scale the gradients
    inversely proportional to the ![](img/96e6a50e-e40a-4999-a9cb-73c4e08807cc.png)
    norm of the current and past gradients (![](img/396097e1-c5af-4252-9fcf-173eb004ebc0.png)
    norm basically means the square of values):'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从上述方程看到的那样，我们将梯度按当前和过去梯度的![](img/396097e1-c5af-4252-9fcf-173eb004ebc0.png)范数的倒数进行缩放（![](img/96e6a50e-e40a-4999-a9cb-73c4e08807cc.png)范数基本上是值的平方）：
- en: '![](img/d7361a21-98a0-45a1-9359-0241675a7e6b.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7361a21-98a0-45a1-9359-0241675a7e6b.png)'
- en: 'Instead of having just ![](img/9272cabb-f165-45d8-b836-a0f844855bb2.png), can
    we generalize it to the ![](img/b4a2e9d9-e01b-4d9e-a6ad-db8ab7a35230.png) norm?
    In general, when we have a large ![](img/9bec027b-eb90-4c01-abcd-ed54ea819ef3.png)
    for norm, our update would become unstable. However, when we set the ![](img/f0c92e11-97fb-4943-bfc2-631b36c7afc7.png)
    value to ![](img/c8cc7775-1fde-4ee3-993b-dcfebe9e3e6e.png), that is, when ![](img/33a528ff-3c8c-4150-a60b-c4ff0dc4e906.png),
    the ![](img/c4f4e684-6be1-4562-a631-f666c519f1dd.png) equation becomes simple
    and stable. Instead of just parameterizing the gradients, ![](img/004e5fdb-7b24-42e9-b848-3be63db54ee7.png),
    alone, we also parameterize the decay rate, ![](img/daf1f97c-fa96-4224-965a-36f4aea4133b.png).
    Thus, we can write the following:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 而不仅仅是有![](img/9272cabb-f165-45d8-b836-a0f844855bb2.png)，我们能将它推广到![](img/b4a2e9d9-e01b-4d9e-a6ad-db8ab7a35230.png)范数吗？一般情况下，当我们有大![](img/9bec027b-eb90-4c01-abcd-ed54ea819ef3.png)范数时，我们的更新会变得不稳定。然而，当我们设置![](img/f0c92e11-97fb-4943-bfc2-631b36c7afc7.png)值为![](img/c8cc7775-1fde-4ee3-993b-dcfebe9e3e6e.png)，即![](img/33a528ff-3c8c-4150-a60b-c4ff0dc4e906.png)时，![](img/c4f4e684-6be1-4562-a631-f666c519f1dd.png)方程变得简单且稳定。我们不仅仅是对梯度参数化，![](img/004e5fdb-7b24-42e9-b848-3be63db54ee7.png)，我们还对衰减率，![](img/daf1f97c-fa96-4224-965a-36f4aea4133b.png)，进行参数化。因此，我们可以写出以下内容：
- en: '![](img/f6a4894a-9edd-4f41-b99a-1c73cef1beb3.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6a4894a-9edd-4f41-b99a-1c73cef1beb3.png)'
- en: 'When we set the limits, ![](img/afef1e2b-a4ff-4a68-8b56-c5ec148182a2.png) tends
    to reach infinity, and then we get the following final equation:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设置限制时，![](img/afef1e2b-a4ff-4a68-8b56-c5ec148182a2.png)趋向于无穷大，然后我们得到以下最终方程：
- en: '![](img/cb48ebc1-f94e-47ee-91c0-dfaf45ac8dbb.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb48ebc1-f94e-47ee-91c0-dfaf45ac8dbb.png)'
- en: You can check the paper listed in the *Further reading* section at the end of
    this chapter to see how exactly this is derived.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查阅本章末尾列出的*进一步阅读*部分的论文，了解这是如何推导出来的。
- en: 'We can rewrite the preceding equation as a simple recursive equation, as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上述方程重写为一个简单的递归方程，如下所示：
- en: '![](img/0d41169c-0374-4b9f-9805-cf672f7ce1fe.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d41169c-0374-4b9f-9805-cf672f7ce1fe.png)'
- en: 'Computing ![](img/b3f21b89-9eaa-4591-838a-fdc425011972.png) is similar to what
    we saw in the *Adaptive moment estimation* section, so we can write the following
    directly:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 计算![](img/b3f21b89-9eaa-4591-838a-fdc425011972.png)类似于我们在*自适应动量估计*部分看到的，因此我们可以直接写出以下内容：
- en: '![](img/d31edaf5-271b-488a-9cb6-00a78ee21f87.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d31edaf5-271b-488a-9cb6-00a78ee21f87.png)'
- en: 'By doing this, we can compute the bias-corrected estimate of ![](img/dfe1e273-e827-4d73-9a5c-e77be289e10d.png):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们可以计算偏差校正估计的![](img/dfe1e273-e827-4d73-9a5c-e77be289e10d.png)：
- en: '![](img/3b1a70b5-d3e7-4dcc-95cb-85d1caa57a86.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b1a70b5-d3e7-4dcc-95cb-85d1caa57a86.png)'
- en: 'Therefore, the final update equation becomes the following:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的更新方程变为以下形式：
- en: '![](img/86e9d4cf-a048-4356-9a9a-3ddc03595084.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86e9d4cf-a048-4356-9a9a-3ddc03595084.png)'
- en: To better understand the Adamax algorithm, let's code it, step by step.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解Adamax算法，让我们逐步编写代码。
- en: 'First, we define the `Adamax` function, as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`Adamax`函数，如下所示：
- en: '[PRE79]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    zeros:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们用零来初始化第一时刻`mt`和第二时刻`vt`：
- en: '[PRE80]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们执行以下步骤：
- en: '[PRE81]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, we can compute the gradients with respect to `theta`, as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算关于`theta`的梯度，如下所示：
- en: '[PRE82]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Then, we compute the first moment, `mt`, as ![](img/85791c82-e801-4ef9-bb1c-602c9489ba17.png):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算第一时刻`mt`，如下所示：
- en: '[PRE83]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we compute the second moment, `vt`, as ![](img/e05778c1-2bf0-4f8d-be76-cc5ebd085043.png):'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算第二时刻`vt`，如下所示：
- en: '[PRE84]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, we can compute the bias-corrected estimate of `mt`; that is, ![](img/b27f19b9-3357-4b19-a9cf-367b59e880b3.png):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算`mt`的偏差校正估计，即![](img/b27f19b9-3357-4b19-a9cf-367b59e880b3.png)：
- en: '[PRE85]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Update the model parameter, `theta`, so that it''s ![](img/e61b483b-34ae-4351-940e-eda822970d60.png)
    :'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 更新模型参数`theta`，使其为![](img/e61b483b-34ae-4351-940e-eda822970d60.png)：
- en: '[PRE86]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Adaptive moment estimation with AMSGrad
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AMSGrad的自适应矩估计
- en: One problem with the Adam algorithm is that it sometimes fails to attain optimal
    convergence, or it reaches a suboptimal solution. It has been noted that, in some
    settings, Adam fails to attain convergence or reach the suboptimal solution instead
    of a global optimal solution. This is due to exponentially moving the averages
    of gradients. Remember when we used the exponential moving averages of gradients
    in Adam to avoid the problem of learning rate decay?
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Adam算法的一个问题是有时无法达到最优收敛，或者它达到次优解。已经注意到，在某些情况下，Adam无法实现收敛，或者达到次优解，而不是全局最优解。这是由于指数移动平均梯度。记得我们在Adam中使用梯度的指数移动平均来避免学习率衰减的问题吗？
- en: However, the problem is that since we are taking an exponential moving average
    of gradients, we miss out information about the gradients that occur infrequently.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，问题在于由于我们采用梯度的指数移动平均，我们错过了不经常出现的梯度信息。
- en: 'To resolve this issue, the authors of AMSGrad made a small change to the Adam
    algorithm. Recall the second-order moment estimates we saw in Adam, as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，AMSGrad的作者对Adam算法进行了微小的修改。回想一下我们在Adam中看到的二阶矩估计，如下所示：
- en: '![](img/50fd89e6-3336-416a-8acd-4b361c5dd11d.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50fd89e6-3336-416a-8acd-4b361c5dd11d.png)'
- en: 'In AMSGrad, we use a slightly modified version of ![](img/c60f3dab-10e9-4e71-88ef-16e13e8b209e.png).
    Instead of using ![](img/b3dc98f8-8596-48c3-a11c-16c8d704963b.png) directly, we
    take the maximum value of ![](img/b64abff4-419a-48b5-993f-305d0f29bc14.png) until
    the previous step, as follows:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMSGrad中，我们使用稍微修改过的![](img/c60f3dab-10e9-4e71-88ef-16e13e8b209e.png)的版本。我们不直接使用![](img/b3dc98f8-8596-48c3-a11c-16c8d704963b.png)，而是取直到前一步的![](img/b64abff4-419a-48b5-993f-305d0f29bc14.png)的最大值，如下所示：
- en: '![](img/1e77b55f-f6b1-4821-92d6-3ba7af3de434.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e77b55f-f6b1-4821-92d6-3ba7af3de434.png)'
- en: This will retain the informative gradients instead of being phased out due to
    the exponential moving average.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 保留了由于指数移动平均而保留信息梯度，而不是逐步淘汰。
- en: 'So, our final update equation becomes the following:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终更新方程式如下所示：
- en: '![](img/924a11d1-0148-42c3-90d2-759d475eb0e8.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/924a11d1-0148-42c3-90d2-759d475eb0e8.png)'
- en: Now, let's understand how to code AMSGrad in Python.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解如何在Python中编写AMSGrad。
- en: 'First, we define the `AMSGrad` function, as follows:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`AMSGrad`函数，如下所示：
- en: '[PRE87]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Then, we initialize the first moment, `mt`, the second moment, `vt`, and the
    modified version of `vt`, that is, `vt_hat`, with `zeros`, as follows:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化第一时刻`mt`、第二时刻`vt`以及修改后的`vt`，即`vt_hat`，均为`zeros`，如下所示：
- en: '[PRE88]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们执行以下步骤：
- en: '[PRE89]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now, we can compute the gradients with respect to `theta`:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算关于`theta`的梯度：
- en: '[PRE90]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Then, we compute the first moment, `mt`, as ![](img/db05e300-35f9-4180-abc3-a9d733e16abe.png):'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算第一时刻`mt`，如下所示：
- en: '[PRE91]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Next, we update the second moment, `vt`, as ![](img/456c4420-54a1-40dc-b7ce-3b79876a84c7.png):'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更新第二时刻`vt`，如下所示：
- en: '[PRE92]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'In AMSGrad, we use a slightly modified version of ![](img/b905b063-1b90-4268-a01b-685c63321a23.png).
    Instead of using ![](img/cf6ace96-be36-43a2-9fb2-e8617e10e286.png) directly, we
    take the maximum value of ![](img/db094c80-9b62-49d5-a88a-169a38891439.png) until
    the previous step. Thus, ![](img/103a5249-9849-463d-b5ab-a3dad1bb5ad9.png) is
    implemented as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMSGrad中，我们使用稍微修改过的![](img/b905b063-1b90-4268-a01b-685c63321a23.png)的版本。我们不直接使用![](img/cf6ace96-be36-43a2-9fb2-e8617e10e286.png)，而是取直到前一步的![](img/db094c80-9b62-49d5-a88a-169a38891439.png)的最大值。因此，![](img/103a5249-9849-463d-b5ab-a3dad1bb5ad9.png)的实现如下：
- en: '[PRE93]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Here, we will compute the bias-corrected estimate of `mt`, that is, ![](img/1f438fc5-4a60-4d4f-ac85-e24b8754e614.png):'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将计算`mt`的偏差校正估计，即![](img/1f438fc5-4a60-4d4f-ac85-e24b8754e614.png)：
- en: '[PRE94]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Now, we can update the model parameter, `theta`, so that it''s ![](img/882d17d3-e9e7-4100-9dbb-294a078c3e94.png):'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以更新模型参数`theta`，使其为![](img/882d17d3-e9e7-4100-9dbb-294a078c3e94.png)：
- en: '[PRE95]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Nadam – adding NAG to ADAM
  id: totrans-440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nadam – 将NAG添加到ADAM中
- en: Nadam is another small extension of the Adam method. As the name suggests, here,
    we incorporate NAG into Adam. First, let's recall what we learned about in Adam.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Nadam是Adam方法的另一个小扩展。正如其名称所示，在这里，我们将NAG合并到Adam中。首先，让我们回顾一下我们在Adam中学到的内容。
- en: 'We calculated the first and second moments as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式计算第一和第二时刻：
- en: '![](img/a1e7996d-a2e6-4053-b993-2d597c4dae22.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1e7996d-a2e6-4053-b993-2d597c4dae22.png)'
- en: '![](img/e478ac82-f544-4b2f-9723-02d6999951c4.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e478ac82-f544-4b2f-9723-02d6999951c4.png)'
- en: 'Then, we calculated the bias-corrected estimates of the first and second moments,
    as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算第一和第二时刻的偏差校正估计，如下：
- en: '![](img/e1e3821e-6433-432d-9150-6cbf08fc361b.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1e3821e-6433-432d-9150-6cbf08fc361b.png)'
- en: '![](img/cd9f08a6-3fca-4e28-84d8-d718285b8670.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd9f08a6-3fca-4e28-84d8-d718285b8670.png)'
- en: 'Our final update equation of Adam is expressed as follows:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们Adam的最终更新方程式表达如下：
- en: '![](img/d3bed132-5f38-4a36-8c7d-94ba42e4f4de.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3bed132-5f38-4a36-8c7d-94ba42e4f4de.png)'
- en: 'Now, we will see how Nadam modifies Adam to use Nesterov momentum. In Adam,
    we compute the first moment as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看Nadam如何修改Adam以使用Nesterov动量。在Adam中，我们计算第一时刻如下：
- en: '![](img/80c74cbe-8d42-4148-a948-92f11e10163b.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80c74cbe-8d42-4148-a948-92f11e10163b.png)'
- en: 'We change this first moment so that it''s Nesterov accelerated momentum. That
    is, instead of using the previous momentum, we use the current momentum and use
    that as a lookahead:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个第一时刻更改为Nesterov加速动量。也就是说，我们不再使用先前的动量，而是使用当前的动量，并将其用作前瞻：
- en: '![](img/76be277c-6543-4968-9bd4-aa8d1044a6a3.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76be277c-6543-4968-9bd4-aa8d1044a6a3.png)'
- en: 'We can''t compute the bias-corrected estimates in the same way as we computed
    them in Adam because, here, ![](img/d6171a5c-f432-4255-b665-a24aaa0d2540.png)
    comes from the current step, and ![](img/3431aced-2e2a-4450-8d1e-c280f09ba27f.png)
    comes from the subsequent step. Therefore, we change the bias-corrected estimate
    step, as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法像在Adam中计算偏差校正估计那样在这里计算，因为这里的![](img/d6171a5c-f432-4255-b665-a24aaa0d2540.png)来自当前步骤，而![](img/3431aced-2e2a-4450-8d1e-c280f09ba27f.png)来自后续步骤。因此，我们改变偏差校正估计步骤如下：
- en: '![](img/2deb0295-245d-4ab1-9fe1-2c613d4a16b8.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2deb0295-245d-4ab1-9fe1-2c613d4a16b8.png)'
- en: '![](img/b7d13f32-8f97-4090-9910-4c759de7458a.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7d13f32-8f97-4090-9910-4c759de7458a.png)'
- en: 'Thus, we can rewrite our first-moment equation as follows:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将我们的第一时刻方程重写为以下形式：
- en: '![](img/da91ed55-f650-49c7-bbd6-9bf7b70d3d9e.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da91ed55-f650-49c7-bbd6-9bf7b70d3d9e.png)'
- en: 'Therefore, our final update equation becomes the following:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终更新方程变为以下形式：
- en: '![](img/182c9f72-90ce-4b54-9413-164cbda3e08d.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/182c9f72-90ce-4b54-9413-164cbda3e08d.png)'
- en: Now let's see how we can implement the Nadam algorithm in Python.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在Python中实现Nadam算法。
- en: 'First, we define the `nadam` function:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`nadam`函数：
- en: '[PRE96]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Then, we initialize the first moment, `mt`, and the second moment, `vt`, with
    zeros:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们用零初始化第一时刻`mt`和第二时刻`vt`：
- en: '[PRE97]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Next, we set `beta_prod` to `1`:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`beta_prod`设置为`1`：
- en: '[PRE98]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'For every iteration, we perform the following steps:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们执行以下步骤：
- en: '[PRE99]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Then, we compute the gradients with respect to `theta`:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算相对于`theta`的梯度：
- en: '[PRE100]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Afterward, we compute the first moment, `mt`, so that it''s ![](img/a90248e7-e1f2-4d29-98c6-1faff1bdd3f9.png):'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算第一时刻`mt`，使其为![](img/a90248e7-e1f2-4d29-98c6-1faff1bdd3f9.png)：
- en: '[PRE101]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Now, we can update the second moment, `vt`, so that its'' ![](img/656e0070-28a6-413d-aea9-44b30bd53e98.png):'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以更新第二时刻`vt`，使其为![](img/656e0070-28a6-413d-aea9-44b30bd53e98.png)：
- en: '[PRE102]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Now, we compute `beta_prod`; that is, ![](img/25815526-3f96-4e89-91a9-0c89924cfbb6.png):'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算`beta_prod`，即![](img/25815526-3f96-4e89-91a9-0c89924cfbb6.png)：
- en: '[PRE103]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Next, we compute the bias-corrected estimate of `mt` so that it''s![](img/dbc9d845-d338-4041-af28-17b321535aaf.png):'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算`mt`的偏差校正估计，使其为![](img/dbc9d845-d338-4041-af28-17b321535aaf.png)：
- en: '[PRE104]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Then, we compute the bias-corrected estimate of `gt` so that it''s ![](img/90ea0452-143c-417c-8d03-65ee10e2ae87.png)
    :'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算`gt`的偏差校正估计，使其为![](img/90ea0452-143c-417c-8d03-65ee10e2ae87.png)：
- en: '[PRE105]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'From here, we compute the bias-corrected estimate of `vt` so that it''s![](img/0fef7a9a-f152-4f65-bea8-a9166841b801.png):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们计算`vt`的偏差校正估计，使其为![](img/0fef7a9a-f152-4f65-bea8-a9166841b801.png)：
- en: '[PRE106]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Now, we compute `mt_tilde` so that it''s ![](img/cb21eefa-02cd-4aaf-a638-d45f05ca767d.png):'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算`mt_tilde`，使其为![](img/cb21eefa-02cd-4aaf-a638-d45f05ca767d.png)：
- en: '[PRE107]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Finally, we update the model parameter, `theta`, by using ![](img/7026ecc8-19aa-4955-a1a3-9285b9b341df.png)
    :'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过使用![](img/7026ecc8-19aa-4955-a1a3-9285b9b341df.png)来更新模型参数`theta`：
- en: '[PRE108]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: By doing this, we have learned about various popular variants of gradient descent
    algorithms that are used for training neural networks. The complete code to perform
    regression with all the variants of regression is available as a Jupyter Notebook
    at [http://bit.ly/2XoW0vH](http://bit.ly/2XoW0vH).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们学习了用于训练神经网络的各种流行的梯度下降算法变体。执行包含所有回归变体的完整代码的Jupyter Notebook可以在[http://bit.ly/2XoW0vH](http://bit.ly/2XoW0vH)找到。
- en: Summary
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started off this chapter by learning about what convex and non-convex functions
    are. Then, we explored how we can find the minimum of a function using gradient
    descent. We learned how gradient descent minimizes a loss function by computing
    optimal parameters through gradient descent. Later, we looked at SGD, where we
    update the parameters of the model after iterating through each and every data
    point, and then we learned about mini-batch SGD, where we update the parameters
    after iterating through a batch of data points.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习什么是凸函数和非凸函数开始本章。然后，我们探讨了如何使用梯度下降找到函数的最小值。我们学习了梯度下降通过计算最优参数来最小化损失函数。后来，我们看了SGD，其中我们在迭代每个数据点之后更新模型的参数，然后我们学习了小批量SGD，其中我们在迭代一批数据点之后更新参数。
- en: Going forward, we learned how momentum is used to reduce oscillations in gradient
    steps and attain convergence faster. Following this, we understood Nesterov momentum,
    where, instead of calculating the gradient at the current position, we calculate
    the gradient at the position the momentum will take us to.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，我们学习了如何使用动量来减少梯度步骤中的振荡并更快地达到收敛。在此之后，我们了解了Nesterov动量，其中我们不是在当前位置计算梯度，而是在动量将我们带到的位置计算梯度。
- en: We also learned about the Adagrad method, where we set the learning rate low
    for parameters that have frequent updates, and high for parameters that have infrequent
    updates. Next, we learned about the Adadelta method, where we completely do away
    with the learning rate and use an exponentially decaying average of gradients.
    We then learned about the Adam method, where we use both first and second momentum
    estimates to update gradients.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了Adagrad方法，其中我们为频繁更新的参数设置了低学习率，对不经常更新的参数设置了高学习率。接下来，我们了解了Adadelta方法，其中我们完全放弃了学习率，而是使用梯度的指数衰减平均值。然后，我们学习了Adam方法，其中我们使用第一和第二动量估计来更新梯度。
- en: Following this, we explored variants of Adam, such as Adamax, where we generalized
    the ![](img/a4a9a09c-4873-4dd0-a000-91b81420b218.png)norm of Adam to ![](img/d99a407c-5386-45d9-b34b-f61814aa6b12.png),
    and AMSGrad, where we combated the problem of Adam reaching a suboptimal solution.
    At the end of this chapter, we learned about Nadam, where we incorporated Nesterov
    momentum into the Adam algorithm.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们探讨了Adam的变体，如Adamax，我们将Adam的![](img/a4a9a09c-4873-4dd0-a000-91b81420b218.png)范数泛化为![](img/d99a407c-5386-45d9-b34b-f61814aa6b12.png)，以及AMSGrad，我们解决了Adam达到次优解的问题。在本章的最后，我们学习了Nadam，其中我们将Nesterov动量整合到Adam算法中。
- en: In the next chapter, we will learn about one of the most widely used deep learning
    algorithms, called **recurrent neural networks** (**RNNs**), and how to use them
    to generate song lyrics.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一种最广泛使用的深度学习算法之一，称为**循环神经网络**（**RNNs**），以及如何使用它们来生成歌词。
- en: Questions
  id: totrans-495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s recap on gradient descent by answering the following questions:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题来回顾梯度下降：
- en: How does SGD differ from vanilla gradient descent?
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SGD与普通梯度下降有什么区别？
- en: Explain mini-batch gradient descent.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释小批量梯度下降。
- en: Why do we need momentum?
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么需要动量？
- en: What is the motivation behind NAG?
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NAG背后的动机是什么？
- en: How does Adagrad set the learning rate adaptively?
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adagrad如何自适应地设置学习率？
- en: What is the update rule of Adadelta?
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adadelta的更新规则是什么？
- en: How does RMSProp overcome the limitations of Adagrad?
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RMSProp如何克服Adagrad的局限性？
- en: Define the update equation of Adam.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义Adam的更新方程。
- en: Further reading
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For further information, refer to the following links:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下链接：
- en: '*Adaptive Subgradient Methods for Online Learning and Stochastic Optimization*,
    by John Duchi et al., [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于在线学习和随机优化的自适应次梯度方法*，作者：John Duchi 等，[http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)'
- en: '*Adadelta: An Adaptive Learning Rate Method*, by Matthew D. Zeiler, [https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Adadelta：一种自适应学习率方法*，作者：Matthew D. Zeiler，[https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
- en: '*Adam: A Method For Stochastic Optimization*, by Diederik P. Kingma and Jimmy
    Lei Ba, [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Adam：一种用于随机优化的方法*，作者：Diederik P. Kingma 和 Jimmy Lei Ba，[https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
- en: '*On the Convergence of Adam and Beyond*, by Sashank J. Reddi, Satyen Kale,
    and Sanjiv Kumar, [https://openreview.net/pdf?id=ryQu7f-RZ](https://openreview.net/pdf?id=ryQu7f-RZ)'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Adam 及其扩展算法的收敛性研究*，作者：Sashank J. Reddi, Satyen Kale, 和 Sanjiv Kumar，[https://openreview.net/pdf?id=ryQu7f-RZ](https://openreview.net/pdf?id=ryQu7f-RZ)'
- en: '*Incorporating Nesterov Momentum into Adam*, by Timothy Dozat, [http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将 Nesterov 动量整合到 Adam 中*，作者：Timothy Dozat，[http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)'
