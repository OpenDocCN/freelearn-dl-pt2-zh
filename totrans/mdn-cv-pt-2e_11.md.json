["```py\n    import torch\n    import torch.nn as nn \n    ```", "```py\n    m = nn.ConvTranspose2d(1, 1, kernel_size=(2,2), stride=2, padding = 0) \n    ```", "```py\n    input = torch.ones(1, 1, 3, 3)\n    output = m(input)\n    output.shape \n    ```", "```py\n    import os\n    if not os.path.exists('dataset1'):\n        !wget -q https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip\n        !unzip -q dataset1.zip\n        !rm dataset1.zip\n        !pip install -q torch_snippets pytorch_model_summary\n    from torch_snippets import *\n    from torchvision import transforms\n    from sklearn.model_selection import train_test_split\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    tfms = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])\n            ]) \n    ```", "```py\n    class SegData(Dataset):\n        def __init__(self, split):\n            self.items=stems(f'dataset1/images_prepped_{split}')\n            self.split = split \n    ```", "```py\n     def __len__(self):\n            return len(self.items) \n    ```", "```py\n     def __getitem__(self, ix):\n            image =read(f'dataset1/images_prepped_{self.split}/\\\n                                   {self.items[ix]}.png', 1)\n            image = cv2.resize(image, (224,224))\n            mask=read(f'dataset1/annotations_prepped_{self.split}\\\n                                /{self.items[ix]}.png')[:,:,0]\n            mask = cv2.resize(mask, (224,224))\n            return image, mask \n    ```", "```py\n     def choose(self): return self[randint(len(self))] \n    ```", "```py\n     def collate_fn(self, batch):\n            ims, masks = list(zip(*batch))\n            ims = torch.cat([tfms(im.copy()/255.)[None] \\\n                         for im in ims]).float().to(device)\n            ce_masks = torch.cat([torch.Tensor(mask[None]) for\\\n                         mask in masks]).long().to(device)\n            return ims, ce_masks \n    ```", "```py\n    trn_ds = SegData('train')\n    val_ds = SegData('test')\n    trn_dl = DataLoader(trn_ds, batch_size=4, shuffle=True,\n                        collate_fn=trn_ds.collate_fn)\n    val_dl = DataLoader(val_ds, batch_size=1, shuffle=True,\n                        collate_fn=val_ds.collate_fn) \n    ```", "```py\n    def conv(in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size=3,\n                        stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ) \n    ```", "```py\n    def up_conv(in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels,\n                               kernel_size=2, stride=2),\n            nn.ReLU(inplace=True)\n        ) \n    ```", "```py\n    from torchvision.models import vgg16_bn\n    class UNet(nn.Module):\n        def __init__(self, pretrained=True, out_channels=12):\n            super().__init__()\n            self.encoder= vgg16_bn(pretrained=pretrained).features\n            self.block1 = nn.Sequential(*self.encoder[:6])\n            self.block2 = nn.Sequential(*self.encoder[6:13])\n            self.block3 = nn.Sequential(*self.encoder[13:20])\n            self.block4 = nn.Sequential(*self.encoder[20:27])\n            self.block5 = nn.Sequential(*self.encoder[27:34])\n            self.bottleneck = nn.Sequential(*self.encoder[34:])\n            self.conv_bottleneck = conv(512, 1024)\n            self.up_conv6 = up_conv(1024, 512)\n            self.conv6 = conv(512 + 512, 512)\n            self.up_conv7 = up_conv(512, 256)\n            self.conv7 = conv(256 + 512, 256)\n            self.up_conv8 = up_conv(256, 128)\n            self.conv8 = conv(128 + 256, 128)\n            self.up_conv9 = up_conv(128, 64)\n            self.conv9 = conv(64 + 128, 64)\n            self.up_conv10 = up_conv(64, 32)\n            self.conv10 = conv(32 + 64, 32)\n            self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1) \n    ```", "```py\n     def forward(self, x):\n            block1 = self.block1(x)\n            block2 = self.block2(block1)\n            block3 = self.block3(block2)\n            block4 = self.block4(block3)\n            block5 = self.block5(block4)\n            bottleneck = self.bottleneck(block5)\n            x = self.conv_bottleneck(bottleneck)\n            x = self.up_conv6(x)\n            x = torch.cat([x, block5], dim=1)\n            x = self.conv6(x)\n            x = self.up_conv7(x)\n            x = torch.cat([x, block4], dim=1)\n            x = self.conv7(x)\n            x = self.up_conv8(x)\n            x = torch.cat([x, block3], dim=1)\n            x = self.conv8(x)\n            x = self.up_conv9(x)\n            x = torch.cat([x, block2], dim=1)\n            x = self.conv9(x)\n            x = self.up_conv10(x)\n            x = torch.cat([x, block1], dim=1)\n            x = self.conv10(x)\n            x = self.conv11(x)\n            return x \n    ```", "```py\n    ce = nn.CrossEntropyLoss()\n    def UnetLoss(preds, targets):\n        ce_loss = ce(preds, targets)\n        acc = (torch.max(preds,1)[1] == targets).float().mean()\n        return ce_loss, acc \n    ```", "```py\n    def train_batch(model, data, optimizer, criterion):\n        model.train()\n        ims, ce_masks = data\n        _masks = model(ims)\n        optimizer.zero_grad()\n        loss, acc = criterion(_masks, ce_masks)\n        loss.backward()\n        optimizer.step()\n        return loss.item(), acc.item()\n    @torch.no_grad()\n    def validate_batch(model, data, criterion):\n        model.eval()\n        ims, masks = data\n        _masks = model(ims)\n        loss, acc = criterion(_masks, masks)\n        return loss.item(), acc.item() \n    ```", "```py\n    model = UNet().to(device)\n    criterion = UnetLoss\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    n_epochs = 20 \n    ```", "```py\n    log = Report(n_epochs)\n    for ex in range(n_epochs):\n        N = len(trn_dl)\n        for bx, data in enumerate(trn_dl):\n            loss, acc = train_batch(model, data, optimizer, criterion)\n            log.record(ex+(bx+1)/N,trn_loss=loss,trn_acc=acc, end='\\r')\n        N = len(val_dl)\n        for bx, data in enumerate(val_dl):\n            loss, acc = validate_batch(model, data, criterion)\n            log.record(ex+(bx+1)/N,val_loss=loss,val_acc=acc, end='\\r')\n\n        log.report_avgs(ex+1) \n    ```", "```py\n    log.plot_epochs(['trn_loss','val_loss']) \n    ```", "```py\n    im, mask = next(iter(val_dl))\n    _mask = model(im) \n    ```", "```py\n    _, _mask = torch.max(_mask, dim=1) \n    ```", "```py\n    subplots([im[0].permute(1,2,0).detach().cpu()[:,:,0],\n              mask.permute(1,2,0).detach().cpu()[:,:,0],\n              _mask.permute(1,2,0).detach().cpu()[:,:,0]],nc=3,\n              titles=['Original image','Original mask',\n              'Predicted mask'])\n    ```", "```py\n    !wget --quiet \\\n     http://sceneparsing.csail.mit.edu/data/ChallengeData2017/images.tar\n    !wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar\n    !tar -xf images.tar\n    !tar -xf annotations_instance.tar\n    !rm images.tar annotations_instance.tar\n    !pip install -qU torch_snippets\n    !wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/engine.py\n    !wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/utils.py\n    !wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/transforms.py\n    !wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/coco_eval.py\n    !wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/coco_utils.py\n    !pip install -q -U \\\n    'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' \n    ```", "```py\n    from torch_snippets import *\n    import torchvision\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n    from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n    from engine import train_one_epoch, evaluate\n    import utils\n    import transforms as T\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    all_images = Glob('images/training')\n    all_annots = Glob('annotations_instance/training') \n    ```", "```py\n    f = 'ADE_train_00014301'\n    im = read(find(f, all_images), 1)\n    an = read(find(f, all_annots), 1).transpose(2,0,1)\n    r,g,b = an\n    nzs = np.nonzero(r==4) # 4 stands for person\n    instances = np.unique(g[nzs])\n    masks = np.zeros((len(instances), *r.shape))\n    for ix,_id in enumerate(instances):\n        masks[ix] = g==_id\n    subplots([im, *masks], sz=20) \n    ```", "```py\n    annots = []\n    for ann in Tqdm(all_annots):\n        _ann = read(ann, 1).transpose(2,0,1)\n        r,g,b = _ann\n        if 4 not in np.unique(r): continue\n        annots.append(ann) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    _annots = stems(annots)\n    trn_items,val_items=train_test_split(_annots,random_state=2) \n    ```", "```py\n    def get_transform(train):\n        transforms = []\n        transforms.append(T.PILToTensor())\n        if train:\n            transforms.append(T.RandomHorizontalFlip(0.5))\n        return T.Compose(transforms) \n    ```", "```py\n    class MasksDataset(Dataset):\n        def __init__(self, items, transforms, N):\n            self.items = items\n            self.transforms = transforms\n            self.N = N \n    ```", "```py\n     def get_mask(self, path):\n            an = read(path, 1).transpose(2,0,1)\n            r,g,b = an\n            nzs = np.nonzero(r==4)\n            instances = np.unique(g[nzs])\n            masks = np.zeros((len(instances), *r.shape))\n            for ix,_id in enumerate(instances):\n                masks[ix] = g==_id\n            return masks \n    ```", "```py\n     def __getitem__(self, ix):\n            _id = self.items[ix]\n            img_path = f'images/training/{_id}.jpg'\n            mask_path=f'annotations_instance/training/{_id}.png'\n            masks = self.get_mask(mask_path)\n            obj_ids = np.arange(1, len(masks)+1)\n            img = Image.open(img_path).convert(\"RGB\")\n            num_objs = len(obj_ids) \n    ```", "```py\n     boxes = []\n            for i in range(num_objs):\n                obj_pixels = np.where(masks[i])\n                xmin = np.min(obj_pixels[1])\n                xmax = np.max(obj_pixels[1])\n                ymin = np.min(obj_pixels[0])\n                ymax = np.max(obj_pixels[0])\n                if (((xmax-xmin)<=10) | (ymax-ymin)<=10):\n                    xmax = xmin+10\n                    ymax = ymin+10\n                boxes.append([xmin, ymin, xmax, ymax]) \n    ```", "```py\n     boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.ones((num_objs,), dtype=torch.int64)\n            masks = torch.as_tensor(masks, dtype=torch.uint8)\n            area = (boxes[:, 3] - boxes[:, 1]) *(boxes[:, 2] - boxes[:, 0])\n            iscrowd = torch.zeros((num_objs,),dtype=torch.int64)\n            image_id = torch.tensor([ix]) \n    ```", "```py\n     target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n            target[\"masks\"] = masks\n            target[\"image_id\"] = image_id\n            target[\"area\"] = area\n            target[\"iscrowd\"] = iscrowd \n    ```", "```py\n     if self.transforms is not None:\n                img, target = self.transforms(img, target)\n            if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :\n                 img = img/255.\n            return img, target \n    ```", "```py\n     def __len__(self):\n            return self.N \n    ```", "```py\n     def choose(self):\n            return self[randint(len(self))] \n    ```", "```py\n    x = MasksDataset(trn_items, get_transform(train=True),N=100)\n    im,targ = x[0]\n    inspect(im,targ)\n    subplots([im, *targ['masks']], sz=10) \n    ```", "```py\n    def get_model_instance_segmentation(num_classes):\n        # load an instance segmentation model pre-trained on\n        # COCO\n        model = torchvision.models.detection\\\n                         .maskrcnn_resnet50_fpn(pretrained=True)\n        # get number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(\\\n                                        in_features,num_classes)\n        in_features_mask = model.roi_heads\\\n                          .mask_predictor.conv5_mask.in_channels\n        hidden_layer = 256\n        # and replace the mask predictor with a new one\n        model.roi_heads.mask_predictor = MaskRCNNPredictor(\\\n                                          in_features_mask,\n                                          hidden_layer, \n                                          num_classes)\n        return model \n    ```", "```py\n    model = get_model_instance_segmentation(2).to(device)\n    model \n    ```", "```py\n    dataset = MasksDataset(trn_items, get_transform(train=True), N=3000)\n    dataset_test = MasksDataset(val_items, get_transform(train=False), N=800)\n    # define training and validation data loaders\n    data_loader=torch.utils.data.DataLoader(dataset,\n                batch_size=2, shuffle=True, num_workers=0, \n                              collate_fn=utils.collate_fn)\n    data_loader_test = torch.utils.data.DataLoader(dataset_test, \n                      batch_size=1, shuffle=False, \n                      num_workers=0,collate_fn=utils.collate_fn) \n    ```", "```py\n    num_classes = 2\n    model = get_model_instance_segmentation(num_classes).to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,                             \n                               momentum=0.9,weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n                                     step_size=3, gamma=0.1) \n    ```", "```py\n    # The following code is for illustration purpose only\n    model.eval()\n    pred = model(dataset[0][0][None].to(device))\n    inspect(pred[0]) \n    ```", "```py\n    # The following code is for illustration purpose only\n    pred[0]['masks'].shape\n    # torch.Size([100, 1, 536, 559]) \n    ```", "```py\n    num_epochs = 5\n    trn_history = []\n    for epoch in range(num_epochs):\n        # train for one epoch, printing every 10 iterations\n        res = train_one_epoch(model, optimizer, data_loader,\n                              device, epoch, print_freq=10)\n        trn_history.append(res)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        res = evaluate(model, data_loader_test, device=device) \n    ```", "```py\n    import matplotlib.pyplot as plt\n    plt.title('Training Loss')\n    losses=[np.mean(list(trn_history[i].meters['loss'].deque)) \\\n                for i in range(len(trn_history))]\n    plt.plot(losses) \n    ```", "```py\n    model.eval()\n    im = dataset_test[0][0]\n    show(im)\n    with torch.no_grad():\n        prediction = model([im.to(device)])\n        for i in range(len(prediction[0]['masks'])):\n            plt.imshow(Image.fromarray(prediction[0]['masks']\\\n                          [i, 0].mul(255).byte().cpu().numpy()))\n            plt.title('Class: '+str(prediction[0]['labels']\\\n                       [i].cpu().numpy())+' Score:'+str(\\\n                      prediction[0]['scores'][i].cpu().numpy()))\n            plt.show() \n    ```", "```py\n    !wget https://www.dropbox.com/s/e92sui3a4kt/Hema18.JPG\n    img = Image.open('Hema18.JPG').convert(\"RGB\")\n    from torchvision import transforms\n    pil_to_tensor = transforms.ToTensor()(img).unsqueeze_(0)\n    Image.fromarray(pil_to_tensor[0].mul(255)\\\n                            .permute(1, 2, 0).byte().numpy()) \n    ```", "```py\n    model.eval()\n    with torch.no_grad():\n        prediction = model([pil_to_tensor[0].to(device)])\n        for i in range(len(prediction[0]['masks'])):\n            plt.imshow(Image.fromarray(prediction[0]['masks']\\\n                        [i, 0].mul(255).byte().cpu().numpy()))\n            plt.title('Class: '+str(prediction[0]\\\n                     ['labels'][i].cpu().numpy())+ \\       \n         'Score:'+str(prediction[0]['scores'][i].cpu().numpy()))\n            plt.show() \n    ```", "```py\n    classes_list = [4,6]\n    annots = []\n    for ann in Tqdm(all_annots):\n        _ann = read(ann, 1).transpose(2,0,1)\n        r,g,b = _ann\n        if np.array([num in np.unique(r) for num in \\\n                    classes_list]).sum()==0: continue\n        annots.append(ann)\n    from sklearn.model_selection import train_test_split\n    _annots = stems(annots)\n    trn_items, val_items = train_test_split(_annots, random_state=2) \n    ```", "```py\n     def get_mask(self,path):\n            an = read(path, 1).transpose(2,0,1)\n            r,g,b = an\n            cls = list(set(np.unique(r)).intersection({4,6}))\n            masks = []\n            labels = []\n            for _cls in cls:\n                nzs = np.nonzero(r==_cls)\n                instances = np.unique(g[nzs])\n                for ix,_id in enumerate(instances):\n                    masks.append(g==_id)\n                    labels.append(classes_list.index(_cls)+1)\n            return np.array(masks), np.array(labels) \n    ```", "```py\n     def __getitem__(self, ix):\n            _id = self.items[ix]\n            img_path = f'images/training/{_id}.jpg'\n            mask_path = \\ f'annotations_instance/training/{_id}.png'\n            **masks, labels = self.get_mask(mask_path)**\n            #print(labels)\n            obj_ids = np.arange(1, len(masks)+1)\n            img = Image.open(img_path).convert(\"RGB\")\n            num_objs = len(obj_ids)\n            boxes = []\n            for i in range(num_objs):\n                obj_pixels = np.where(masks[i])\n                xmin = np.min(obj_pixels[1])\n                xmax = np.max(obj_pixels[1])\n                ymin = np.min(obj_pixels[0])\n                ymax = np.max(obj_pixels[0])\n                if (((xmax-xmin)<=10) | (ymax-ymin)<=10):\n                    xmax = xmin+10\n                    ymax = ymin+10\n                boxes.append([xmin, ymin, xmax, ymax])\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            **labels = torch.as_tensor(labels, dtype=torch.int64)**\n            masks = torch.as_tensor(masks, dtype=torch.uint8)\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n            image_id = torch.tensor([ix])\n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n            target[\"masks\"] = masks\n            target[\"image_id\"] = image_id\n            target[\"area\"] = area\n            target[\"iscrowd\"] = iscrowd\n            if self.transforms is not None:\n                img, target = self.transforms(img, target)\n             if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :\n                 img = img/255.\n            return img, target\n        def __len__(self):\n            return self.N\n        def choose(self):\n            return self[randint(len(self))] \n    ```", "```py\n    num_classes = 3\n    model=get_model_instance_segmentation(num_classes).to(device) \n    ```"]