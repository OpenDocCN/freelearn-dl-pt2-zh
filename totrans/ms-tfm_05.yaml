- en: '*Chapter 3*: Autoencoding Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：自动编码语言模型'
- en: In the previous chapter, we looked at and studied how a typical Transformer
    model can be used by HuggingFace's Transformers. So far, all the topics have included
    how to use pre-defined or pre-built models and less information has been given
    about specific models and their training.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们查看并研究了如何使用HuggingFace的Transformers的典型Transformer模型。到目前为止，所有主题都包括如何使用预定义或预构建模型，而对于特定模型及其训练的信息较少。
- en: In this chapter, we will gain knowledge of how we can train autoencoding language
    models on any given language from scratch. This training will include pre-training
    and task-specific training of the models. First, we will start with basic knowledge
    about the BERT model and how it works. Then we will train the language model using
    a simple and small corpus. Afterward, we will look at how the model can be used
    inside any Keras model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解如何从头开始在任何给定语言上训练自动编码语言模型。这种训练将包括模型的预训练和任务特定训练。首先，我们将从BERT模型的基本知识和其工作原理开始。然后，我们将使用一个简单且小型的语料库来训练语言模型。之后，我们将看看如何将该模型用于任何Keras模型内。
- en: 'For an overview of what will be learned in this chapter, we will discuss the
    following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解本章将学到的内容，我们将讨论以下主题：
- en: BERT – one of the autoencoding language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT——其中之一自动编码语言模型
- en: Autoencoding language model training for any language
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何语言的自动编码语言模型训练
- en: Sharing models with the community
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与社区共享模型
- en: Understanding other autoencoding models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解其他自动编码模型
- en: Working with tokenization algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记化算法工作
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technical requirements for this chapter are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: Anaconda
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Transformers >= 4.0.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers >= 4.0.0
- en: PyTorch >= 1.0.2
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch >= 1.0.2
- en: TensorFlow >= 2.4.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow >= 2.4.0
- en: Datasets >= 1.4.1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集 >= 1.4.1
- en: Tokenizers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记器
- en: 'Please also check the corresponding GitHub code of `chapter 03`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请还要检查`第03章`对应的GitHub代码：
- en: '[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03).'
- en: 'Check out the following link to see Code in Action Video: [https://bit.ly/3i1ycdY](https://bit.ly/3i1ycdY)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看代码实战视频：[https://bit.ly/3i1ycdY](https://bit.ly/3i1ycdY)
- en: BERT – one of the autoencoding language models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT——其中之一自动编码语言模型
- en: '**Bidirectional Encoder Representations from Transformers**, also known as
    **BERT**, was one of the first autoencoding language models to utilize the encoder
    Transformer stack with slight modifications for language modeling.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自变换器的双向编码器表示**，也被称为**BERT**，是最早使用编码器Transformer堆栈的自动编码语言模型之一，稍作修改用于语言建模。'
- en: The BERT architecture is a multilayer Transformer encoder based on the Transformer
    original implementation. The Transformer model itself was originally for machine
    translation tasks, but the main improvement made by BERT is the utilization of
    this part of the architecture to provide better language modeling. This language
    model, after pretraining, is able to provide a global understanding of the language
    it is trained on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BERT架构是基于Transformer原始实现的多层Transformer编码器。Transformer模型本身最初用于机器翻译任务，但BERT所做的主要改进是利用该体系结构的这一部分来提供更好的语言建模。这种语言模型在预训练之后，能够提供对其训练语言的全局理解。
- en: BERT language model pretraining tasks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT语言模型预训练任务
- en: 'To have a clear understanding of the masked language modeling used by BERT,
    let''s define it with more details. **Masked language modeling** is the task of
    training a model on input (a sentence with some masked tokens) and obtaining the
    output as the whole sentence with the masked tokens filled. But how and why does
    it help a model to obtain better results on downstream tasks such as classification?
    The answer is simple: if the model can do a cloze test (a linguistic test for
    evaluating language understanding by filling in blanks), then it has a general
    understanding of the language itself. For other tasks, it has been pretrained
    (by language modeling) and will perform better.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要清楚了解 BERT 所使用的遮罩语言建模，让我们更详细地定义它。**遮罩语言建模**是训练模型的任务，输入是一句话，其中有一些遮罩标记，输出是填满遮罩标记的完整句子。但是这样做为什么能帮助模型在分类等下游任务中获得更好的结果呢？答案很简单：如果模型能够完成完形填空测试（一种通过填写空白来评估语言理解能力的语言测试），那么它就对语言本身有了一般的理解。对于其他任务，它已经进行了预训练（通过语言建模），并且将表现更好。
- en: 'Here''s an example of a cloze test:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一道完形填空的例子：
- en: George Washington was the first President of the ___ States.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·华盛顿是___州的第一任总统。
- en: It is expected that *United* should fill in the blank. For a masked language
    model, the same task is applied, and it is required to fill in the masked tokens.
    However, masked tokens are selected randomly from a sentence.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 预期 *United* 应该填入空白处。对于遮罩语言模型，应用了同样的任务，需要填补遮罩标记。不过，遮罩标记是从一句话中随机选择的。
- en: Another task that BERT is trained on is **Next Sentence Prediction** (**NSP**).
    This pretraining task ensures that BERT learns not only the relations of all tokens
    to each other in predicting masked ones but also helps it understand the relation
    between two sentences. A pair of sentences is selected and given to BERT with
    a *[SEP]* splitter token in between. It is also known from the dataset whether
    the second sentence comes after the first one or not.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 受训的另一个任务是**下一句预测**（**NSP**）。这个预训练任务确保 BERT 不仅学习了预测遮罩标记中所有令牌之间的关系，还帮助其理解两个句子之间的关系。会选择一对句子，并在它们之间放上一个*[SEP]*
    分隔符令牌。数据集中还知道第二个句子是在第一个句子之后还是之前。
- en: 'The following is an example of NSP:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 NSP 的示例：
- en: '*It is required from reader to fill the blank. Bitcoin price is way over too
    high compared to other altcoins.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*读者需要填写空白。比特币价格相比其他替代币高得太多了。*'
- en: In this example, the model is required to predict it as negative (the sentences
    are not related to each other).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型需要预测为否定（这两个句子之间没有关联）。
- en: 'These two pretraining tasks enable BERT to have an understanding of the language
    itself. BERT token embeddings provide a contextual embedding for each token. **Contextual
    embedding** means each token has an embedding that is completely related to the
    surrounding tokens. Unlike Word2Vec and such models, BERT provides better information
    for each token embedding. NSP tasks, on the other hand, enable BERT to have better
    embeddings for *[CLS]* tokens. This token, as was discussed in the first chapter,
    provides information about the whole input. *[CLS]* is used for classification
    tasks and in the pretraining part learns the overall embedding of the whole input.
    The following figure shows an overall look at the BERT model. *Figure 3.1* shows
    the respective input and output of the BERT model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种预训练任务使 BERT 能够对语言本身有所了解。BERT 令牌嵌入为每个令牌提供上下文嵌入。**上下文嵌入**意味着每个令牌的嵌入与周围令牌完全相关。与
    Word2Vec 和其他模型不同，BERT 为每个令牌嵌入提供更好的信息。另一方面，NSP 任务使 BERT 能够为*[CLS]* 令牌提供更好的嵌入。正如在第一章中讨论的那样，此令牌提供关于整个输入的信息。*[CLS]*
    用于分类任务，并且在预训练部分学习整个输入的总体嵌入。下图显示了 BERT 模型的整体外观。*图3.1* 显示了 BERT 模型的相应输入和输出：
- en: '![Figure 3.1 – The BERT model ](img/B17123_03_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – BERT 模型](img/B17123_03_01.jpg)'
- en: Figure 3.1 – The BERT model
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – BERT 模型
- en: Let's move on to the next section!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一部分！
- en: A deeper look into the BERT language model
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解 BERT 语言模型
- en: Tokenizers are one of the most important parts of many NLP applications in their
    respective pipelines. For BERT, WordPiece tokenization is used. Generally, **WordPiece**,
    **SentencePiece**, and **BytePairEncoding** (**BPE**) are the three most widely
    known tokenizers, used by different Transformer-based architectures, which are
    also covered in the next sections. The main reason that BERT or any other Transformer-based
    architecture uses subword tokenization is the ability of such tokenizers to deal
    with unknown tokens.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器是许多NLP应用程序中各自流水线中最重要的部分之一。 对于BERT，使用的是WordPiece标记。 通常，**WordPiece**，**SentencePiece**和**BytePairEncoding**（**BPE**）是最广为人知的三种标记器，由不同的基于Transformer的架构使用，也将在接下来的部分中介绍。
    BERT或任何其他基于Transformer的架构使用子词标记化的主要原因是这些标记器处理未知标记的能力。
- en: BERT also uses positional encoding to ensure the position of the tokens is given
    to the model. If you recall from [*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016),
    *From Bag-of-Words to the Transformers*, BERT and similar models use non-sequential
    operations such as dense neural layers. Conventional models such as LSTM- and
    RNN-based models get the position by the order of the tokens in the sequence.
    In order to provide this extra information to BERT, positional encoding comes
    in handy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BERT还使用位置编码来确保将标记的位置提供给模型。如果您还记得[*章节1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)，*从词袋模型到Transformer*，BERT和类似的模型使用非顺序操作，如密集神经层。
    传统模型，如基于LSTM和RNN的模型，通过序列中标记的顺序获得位置。 为了为BERT提供这些额外信息，位置编码非常有用。
- en: Pretraining of BERT such as autoencoding models provides language-wise information
    for the model, but in practice, when dealing with different problems such as sequence
    classification, token classification, or question answering, different parts of
    the model output are used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练（如自动编码模型）为模型提供了语言信息，但在实践中，当处理不同的问题，如序列分类，标记分类或问题回答时，会使用模型输出的不同部分。
- en: For example, in the case of a sequence classification task, such as sentiment
    analysis or sentence classification, it is proposed by the original BERT article
    that *[CLS]* embedding from the last layer must be used. However, there is other
    research that performs classification using different techniques using BERT (using
    average token embedding from all tokens, deploying an LSTM over the last layer,
    or even using a CNN on top of the last layer). The last *[CLS]* embedding for
    sequence classification can be used by any classifier, but the proposed, and the
    most common one, is a dense layer with an input size equal to the final token
    embedding size and an output size equal to the number of classes with a softmax
    activation function. Using sigmoid is also another alternative when the output
    could be multilabel and the problem itself is a multilabel classification problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在序列分类任务（如情感分析或句子分类）的情况下，原始BERT文章提出了必须使用最后一层的*[CLS]*嵌入。然而，还有其他研究使用BERT进行分类，使用不同的技术（使用所有标记的平均标记嵌入，在最后一层部署LSTM，甚至在最后一层之上使用CNN）。
    序列分类的最后一个*[CLS]*嵌入可以被任何分类器使用，但提出的，也是最常见的方法是使用具有输入大小等于最终标记嵌入大小和输出大小等于类数量的softmax激活函数的密集层。
    当输出可能是多标签并且问题本身是多标签分类问题时，使用sigmoid也是另一种选择。
- en: 'To give you more detailed information about how BERT actually works, the following
    illustration shows an example of an NSP task. Note that the tokenization is simplified
    here for better understanding:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给您更详细的关于BERT如何实际工作的信息，以下说明显示了一个NSP任务的示例。请注意，这里对标记化进行了简化，以便更好地理解：
- en: '![Figure 3.2 – BERT example for an NSP task ](img/B17123_03_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 - 用于NSP任务的BERT示例](img/B17123_03_02.jpg)'
- en: Figure 3.2 – BERT example for an NSP task
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 - 用于NSP任务的BERT示例
- en: The BERT model has different variations, with different settings. For example,
    the size of the input is variable. In the preceding example, it is set to *512*
    and the maximum sequence size that model can get as input is *512*. However, this
    size includes special tokens, *[CLS]* and *[SEP]*, so it will be reduced to *510*.
    On the other hand, using WordPiece as a tokenizer yields subword tokens, and the
    sequence size before we can have fewer words, and after tokenization, the size
    will increase because the tokenizer breaks words into subwords if they are not
    commonly seen in the pretrained corpus.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型有不同的变体，具有不同的设置。例如，输入大小是可变的。在前面的示例中，它被设置为*512*，而模型可以接受的最大序列大小是*512*。但是，这个大小包括特殊标记*[CLS]*和*[SEP]*，因此它会被缩减为*510*。另一方面，使用WordPiece作为标记器会产生子词标记，作为序列输入之前可以有较少的词，标记化之后，大小会增加，因为标记器会将词分解为子词，如果在预训练语料库中没有看到它们常见。
- en: 'The following figure shows an illustration of BERT for different tasks. For
    an NER task, the output of each token is used instead of *[CLS]*. In the case
    of question answering, the question and the answer are concatenated using the
    *[SEP]* delimiter token and the answer is annotated using *Start/End* and the
    *Span* output from the last layer. In this case, the *Paragraph* is the context
    that the *Question* is asked about it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了BERT用于不同任务的示例。对于NER任务，使用每个令牌的输出，而不是*[CLS]*。在问答情景中，使用*[SEP]*分隔符令牌将问题和答案连接起来，然后使用最后一层的*Start/End*和*Span*输出标记答案。在这种情况下，*Paragraph*是*Question*所询问的*Context*：
- en: '![Figure 3.3 – BERT model for various NLP tasks ](img/B17123_03_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 用于各种NLP任务的BERT模型](img/B17123_03_03.jpg)'
- en: Figure 3.3 – BERT model for various NLP tasks
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 用于各种NLP任务的BERT模型
- en: Regardless of all of these tasks, the most important ability of BERT is its
    contextual representation of text. The reason it is successful in various tasks
    is because of the Transformer encoder architecture that represents input in the
    form of dense vectors. These vectors can be easily transformed into output by
    very simple classifiers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不管这些任务如何，BERT最重要的能力是对文本的上下文表示。它成功的原因在于Transformer编码器架构，它以密集向量的形式表示输入。这些向量可以通过非常简单的分类器轻松转换为输出。
- en: Up to this point, you have learned about BERT and how it works. You have learned
    detailed information on various tasks that BERT can be used for and the important
    points of this architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经了解了BERT以及它的工作原理。您已经详细了解了BERT可以用于的各种任务的重要信息以及这种架构的重要要点。
- en: In the next section, you will learn how you can pre-train BERT and use it after
    training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何预先训练BERT，并在训练后使用它。
- en: Autoencoding language model training for any language
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任何语言的自编码语言模型训练
- en: We have discussed how BERT works and that it is possible to use the pretrained
    version of it provided by the HuggingFace repository. In this section, you will
    learn how to use the HuggingFace library to train your own BERT.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了BERT的工作原理以及可以使用HuggingFace库提供的预训练版本。在本节中，您将学习如何使用HuggingFace库来训练您自己的BERT。
- en: Before we start, it is essential to have good training data, which will be used
    for the language modeling. This data is called the **corpus**, which is normally
    a huge pile of data (sometimes it is preprocessed and cleaned). This unlabeled
    corpus must be appropriate for the use case you wish to have your language model
    trained on; for example, if you are trying to have a special BERT for, let's say,
    the English language. Although there are tons of huge, good datasets, such as
    Common Crawl ([https://commoncrawl.org/](https://commoncrawl.org/)), we would
    prefer a small one for faster training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，有一个很重要的问题，那就是需要有良好的训练数据，这将用于语言建模。这些数据称为**语料库**，通常是一大堆数据（有时经过预处理和清理）。这些无标签的语料库必须适合您希望训练语言模型的用例；例如，如果您尝试为英语单独创建一个特殊的BERT。尽管有成千上万的巨大优秀数据集，比如Common
    Crawl（[https://commoncrawl.org/](https://commoncrawl.org/)），我们更倾向于一个小一点的数据集，以便更快地训练。
- en: 'The IMDB dataset of 50K movie reviews (available at [https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews))
    is a large dataset for sentiment analysis, but small if you use it as a corpus
    for training your language model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 50K电影评论的IMDB数据集（可在[https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)找到）是一个用于情感分析的大型数据集，但如果您将其用作语料库来训练语言模型，则算是小型的：
- en: 'You can easily download and save it in `.txt` format for language model and
    tokenizer training by using the following code:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下代码轻松下载并保存为`.txt`格式，用于语言模型和分词器训练：
- en: '[PRE0]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After preparing the corpus, the tokenizer must be trained. The `tokenizers`
    library provides fast and easy training for the WordPiece tokenizer. In order
    to train it on your corpus, it is required to run the following code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在准备语料库之后，必须训练分词器。`tokenizers`库提供了快速简单的WordPiece分词器训练。为了在你的语料库上训练它，需要运行以下代码：
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will train the tokenizer. You can access the trained vocabulary by using
    the `get_vocab()` function of the trained `tokenizer` object. You can get the
    vocabulary by using the following code:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将训练分词器。你可以通过使用训练好的`tokenizer`对象的`get_vocab()`函数来访问训练好的词汇表。你可以通过以下代码获取词汇表：
- en: '[PRE2]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It is essential to save the tokenizer to be used afterwards. Using the `save_model()`
    function of the object and providing the directory will save the tokenizer vocabulary
    for further usage:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存分词器以供以后使用是必不可少的。使用对象的`save_model()`函数并提供目录将保存分词器词汇表供以后使用：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And you can reload it by using the `from_file()` function:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`from_file()`函数重新加载它：
- en: '[PRE5]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can use the tokenizer by following this example:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以按照以下示例使用分词器：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The special tokens `[CLS]` and `[SEP]` will be automatically added to the list
    of tokens because BERT needs them for processing input.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特殊的标记`[CLS]`和`[SEP]`将自动添加到标记列表中，因为BERT需要它们来处理输入。
- en: 'Let''s try another sentence using our tokenizer:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们的分词器来另一个句子：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Seems like a good tokenizer for noisy and misspelled text. Now that you have
    your tokenizer ready and saved, you can train your own BERT. The first step is
    to use `BertTokenizerFast` from the `Transformers` library. You are required to
    load the trained tokenizer from the previous step by using the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于嘈杂和拼写错误的文本，似乎是一个很好的分词器。现在你已经准备好并保存了你的分词器，你可以训练你自己的BERT。第一步是使用`Transformers`库中的`BertTokenizerFast`。你需要使用以下命令加载上一步训练好的分词器：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have used `BertTokenizerFast` because it is suggested by the HuggingFace
    documentation. There is also `BertTokenizer`, which, according to the definition
    from the library documentation, is not implemented as fast as the fast version.
    In most of the pretrained models' documentations and cards, it is highly recommended
    to use the `BertTokenizerFast` version.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了`BertTokenizerFast`，因为它是由HuggingFace文档建议使用的。还有`BertTokenizer`，根据库文档中的定义，它没有实现快速版本那么快。在大多数预训练模型的文档和卡片中，强烈建议使用`BertTokenizerFast`版本。
- en: 'The next step is preparing the corpus for faster training by using the following
    command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是通过以下命令准备语料库以加快训练速度：
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And it is required to provide a data collator for masked language modeling:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且需要为掩码语言建模提供数据收集器：
- en: '[PRE10]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The data collator gets the data and prepares it for the training. For example,
    the data collator above takes data and prepares it for masked language modeling
    with a probability of `0.15`. The purpose of using such a mechanism is to do the
    preprocessing on the fly, which makes it possible to use fewer resources. On the
    other hand, it slows down the training process because each sample has to be preprocessed
    on the fly at training time.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集器获取数据并为训练准备好。例如，上面的数据收集器获取数据并准备好使用概率为`0.15`的掩码语言建模。使用这种机制的目的是在运行时进行预处理，这样可以使用更少的资源。另一方面，它会减慢训练过程，因为每个样本都必须在训练时动态进行预处理。
- en: 'The training arguments also provide information for the trainer in the training
    phase, which can be set by using the following command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练参数还为训练器在训练阶段提供信息，可以使用以下命令设置：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll now make the BERT model itself, which we are going to use with the default
    configuration (the number of attention heads, Transformer encoder layers, and
    so on):'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建BERT模型本身，我们将使用默认配置（注意力头数、Transformer编码器层数等）：
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And the final step is to make a trainer object:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是创建一个训练器对象：
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, you can train your language model using the following command:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用以下命令训练你的语言模型：
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It will show you a progress bar indicating the progress made in training:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它会显示一个进度条，指示训练的进度：
- en: '![Figure 3.4 – BERT model training progress ](img/B17123_03_04.jpg)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.4 - BERT模型训练进度](img/B17123_03_04.jpg)'
- en: Figure 3.4 – BERT model training progress
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4 - BERT模型训练进度
- en: 'During the model training, a log directory called `runs` will be used to store
    the checkpoint in steps:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型训练过程中，将使用名为`runs`的日志目录存储步骤检查点：
- en: '![Figure 3.5 – BERT model checkpoints ](img/B17123_03_05.jpg)'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.5 – BERT 模型检查点](img/B17123_03_05.jpg)'
- en: Figure 3.5 – BERT model checkpoints
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.5 – BERT 模型检查点
- en: 'After the training is finished, you can easily save the model using the following
    command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束后，您可以使用以下命令轻松保存模型：
- en: '[PRE15]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Up to this point, you have learned how you can train BERT from scratch for any
    specific language that you desire. You've learned how to train the tokenizer and
    BERT model using the corpus you have prepared.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直到目前为止，您已经学会了如何训练您希望的任何特定语言的 BERT。您已经学会了如何训练标记器和 BERT 模型，使用您准备的语料库。
- en: 'The default configuration that you provided for BERT is the most essential
    part of this training process, which defines the architecture of BERT and its
    hyperparameters. You can take a peek at these parameters by using the following
    code:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您提供的 BERT 默认配置是此训练过程中最关键的部分，它定义了 BERT 的架构和超参数。您可以使用以下代码查看这些参数：
- en: '[PRE16]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.6 – BERT model configuration ](img/B17123_03_06.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.6 – BERT 模型配置](img/B17123_03_06.jpg)'
- en: Figure 3.6 – BERT model configuration
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.6 – BERT 模型配置
- en: If you wish to replicate `max_position_embedding`, `num_attention_heads`, `num_hidden_layers`,
    `intermediate_size`, and `hidden_size`, directly affects the training time. Increasing
    them dramatically increases the training time for a large corpus.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您希望复制`max_position_embedding`、`num_attention_heads`、`num_hidden_layers`、`intermediate_size`和`hidden_size`，直接影响训练时间。将它们增加会显著增加大型语料库的训练时间。
- en: 'For example, you can easily make a new configuration for a tiny version of
    BERT for faster training using the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，您可以使用以下代码轻松为小型 BERT 制作新配置以加快训练速度：
- en: '[PRE17]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the result of the code:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的结果：
- en: '![Figure 3.8 – Tiny BERT model configuration ](img/B17123_03_08.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.8 – 小型 BERT 模型配置](img/B17123_03_08.jpg)'
- en: Figure 3.8 – Tiny BERT model configuration
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.8 – 小型 BERT 模型配置
- en: 'By using the same method, we can make a tiny BERT model using this configuration:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的方法，我们可以使用这个配置制作一个微小的 BERT 模型：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And using the same parameters for training, you can train this tiny new BERT:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且使用相同的参数进行训练，您可以训练这个微小的新 BERT：
- en: '[PRE19]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following is the output:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.9 – Tiny BERT model configuration ](img/B17123_03_09.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.9 – 小型 BERT 模型配置](img/B17123_03_09.jpg)'
- en: Figure 3.9 – Tiny BERT model configuration
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.9 – 小型 BERT 模型配置
- en: It is clear that the training time is dramatically decreased, but you should
    be aware that this is a tiny version of BERT with fewer layers and parameters,
    which is not as good as BERT Base.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显然，训练时间显著减少，但您应该意识到这是一个具有更少层和参数的微小版 BERT，不如 BERT Base 好。
- en: Up to this point, you have learned how to train your own model from scratch,
    but it is essential to note that using the `datasets` library is a better choice
    when dealing with datasets for training language models or leveraging it to perform
    task-specific training.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何从头开始训练自己的模型，但需要注意的是在处理用于训练语言模型的数据集或利用它执行特定任务的数据集时，使用`datasets`库是更好的选择。
- en: 'The BERT language model can also be used as an embedding layer combined with
    any deep learning model. For example, you can load any pretrained BERT model or
    your own version that has been trained in the previous step. The following code
    shows how you must load it to be used in a Keras model:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 语言模型也可以作为嵌入层与任何深度学习模型结合使用。例如，您可以加载任何预训练的 BERT 模型或您在上一步中训练过的自己的版本。以下代码显示了如何加载它以在
    Keras 模型中使用：
- en: '[PRE20]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'But you do not need the whole model; instead, you can access the layers by
    using the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但您不需要整个模型；相反，您可以使用以下代码访问层：
- en: '[PRE21]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, there is just a single layer from `TFBertMainLayer`, which
    you can access within your Keras model. But before using it, it is nice to test
    it and see what kind of output it provides:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，只有一个来自`TFBertMainLayer`的单层，您可以在 Keras 模型中访问它。但在使用之前，最好先测试它，看看它提供了什么样的输出：
- en: '[PRE22]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.10 – BERT model output ](img/B17123_03_10.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.10 – BERT 模型输出](img/B17123_03_10.jpg)'
- en: Figure 3.10 – BERT model output
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.10 – BERT 模型输出
- en: 'As can be seen from the result, there are two outputs: one for the last hidden
    state and one for the pooler output. The last hidden state provides all token
    embeddings from BERT with additional *[CLS]* and *[SEP]* tokens at the start and
    end, respectively.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have learned more about the TensorFlow version of BERT, you can
    make a Keras model using this new embedding:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model object, which is a Keras model, has two inputs: one for tokens and
    one for masks. Tokens has `token_ids` from the tokenizer output and the masks
    will have `attention_mask`. Let''s try it and see what happens:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is important to use `max_length`, `truncation`, and `pad_to_max_length`
    when using `tokenizer`. These parameters make sure you have the output in a usable
    shape by padding it to the maximum length of 256 that was defined before. Now
    you can run the model using this sample:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – BERT model classification output ](img/B17123_03_11.jpg)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.11 – BERT model classification output
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When training the model, you need to compile it using the `compile` function:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – BERT model summary ](img/B17123_03_12.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.12 – BERT model summary
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the model summary, you can see that the model has 109,483,778 trainable
    parameters including BERT. But if you have your BERT model pretrained and you
    want to freeze it in a task-specific training, you can do so with the following
    command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As far as we know, the layer index of the embedding layer is 2, so we can simply
    freeze it. If you rerun the summary function, you will see the trainable parameters
    are reduced to 1,538, which is the number of parameters of the last layer:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – BERT model summary with fewer trainable parameters ](img/B17123_03_13.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.13 – BERT model summary with fewer trainable parameters
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you recall, we used the IMDB sentiment analysis dataset for training the
    language model. Now you can use it for training the Keras-based model for sentiment
    analysis. But first, you need to prepare the input and output:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And finally, your data is ready, and you can fit your model:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: And after fitting the model, your model is ready to be used. Up to this point,
    you have learned how to perform model training for a classification task. You
    have learned how to save it, and in the next section, you will learn how it is
    possible to share the trained model with the community.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Sharing models with the community
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HuggingFace provides a very easy-to-use model-sharing mechanism:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'You can simply use the following `cli` tool to log in:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After you''ve logged in using your own credentials, you can create a repository:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can put any model name for the `a-fancy-model-name` parameter and then
    it is essential to make sure you have git-lfs:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Git LFS is a Git extension used for handling large files. HuggingFace pretrained
    models are usually large files that require extra libraries such as LFS to be
    handled by Git.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then you can clone the repository you have just created:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以克隆你刚刚创建的仓库：
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Afterward, you can add and remove from the repository as you like, and then,
    just like Git usage, you have to run the following command:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，您可以随意向仓库中添加和删除，然后，就像使用 Git 一样，您必须运行以下命令：
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Autoencoding models rely on the left encoder side of the original Transformer
    and are highly efficient at solving classification problems. Even though BERT
    is a typical example of autoencoding models, there are many alternatives discussed
    in the literature. Let's take a look at these important alternatives.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码模型依赖于原始 Transformer 左编码器一侧，非常有效地解决分类问题。尽管 BERT 是自编码模型的典型示例，但文献中讨论了许多替代方案。让我们看一下这些重要的替代方案。
- en: Understanding other autoencoding models
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解其他自编码模型
- en: 'In this part, we will review autoencoding model alternatives that slightly
    modify the original BERT. These alternative re-implementations have led to better
    downstream tasks by exploiting many sources: optimizing the pre-training process
    and the number of layers or heads, improving data quality, designing better objective
    functions, and so forth. The source of improvements roughly falls into two parts:
    *better architectural design choice* and *pre-training control*.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分中，我们将回顾略微修改原始 BERT 的自编码模型替代方案。这些替代方案的重新实现通过利用许多来源（优化预训练过程和层或头的数量、改进数据质量、设计更好的目标函数等）导致了更好的下游任务。改进的来源大致分为两部分：*更好的架构设计选择*
    和 *预训练控制*。
- en: Many effective alternatives have been shared lately, so it is impossible to
    understand and explain them all here. We can take a look at some of the most cited
    models in the literature and the most used ones on NLP benchmarks. Let's start
    with **Albert** as a re-implementation of BERT that focuses especially on architectural
    design choice.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最近共享了许多有效的替代方案，因此不可能在这里理解和解释它们全部。我们可以看一些文献中引用最多的模型和 NLP 基准测试中使用最多的模型。让我们从 **Albert**
    开始，作为对架构设计选择特别关注的 BERT 的重新实现。
- en: Introducing ALBERT
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入 ALBERT
- en: The performance of language models is considered to improve as their size gets
    bigger. However, training such models is getting more challenging due to both
    memory limitations and longer training times. To address these issues, the Google
    team proposed the **Albert model** (**A Lite BERT** for Self-Supervised Learning
    of Language Representations), which is indeed a reimplementation of the BERT architecture
    by utilizing several new techniques that reduce memory consumption and increase
    the training speed. The new design led to the language models scaling much better
    than the original BERT. Along with 18 times fewer parameters, Albert trains 1.7
    times faster than the original BERT-large model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的性能被认为随着其规模的增大而提高。然而，由于内存限制和较长的训练时间，训练这些模型变得更加具有挑战性。为了解决这些问题，Google 团队提出了
    **Albert 模型**（**A Lite BERT** 用于语言表示的自监督学习），这实际上是通过利用几种新技术对 BERT 架构进行重新实现，从而减少了内存消耗并增加了训练速度。新设计导致语言模型比原始
    BERT 更好地扩展。与原始 BERT-large 模型相比，Albert 参数减少了 18 倍，训练速度提高了 1.7 倍。
- en: 'The Albert model mainly consists of three modifications of the original BERT:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Albert 模型主要由对原始 BERT 的三种修改组成：
- en: Factorized embedding parameterization
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子化嵌入参数化
- en: Cross-layer parameter sharing
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨层参数共享
- en: Inter-sentence coherence loss
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句间连贯性损失
- en: 'The first two modifications are parameter-reduction methods that are related
    to the issue of model size and memory consumption in the original BERT. The third
    corresponds to a new objective function: **Sentence-Order Prediction** (**SOP**),
    replacing the **Next Sentence Prediction** (**NSP**) task of the original BERT,
    which led to a much thinner model and improved performance.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种修改是与原始 BERT 中模型大小和内存消耗问题相关的参数减少方法。第三种对应于一个新的目标函数：**句子顺序预测**（**SOP**），取代了原始
    BERT 的 **下一句预测**（**NSP**）任务，从而导致了一个更薄的模型和更好的性能。
- en: Factorized embedding parameterization is used to decompose the large vocabulary-embedding
    matrix into two small matrices, which separate the size of the hidden layers from
    the size of the vocabulary. This decomposition reduces the embedding parameters
    from *O(V × H) to O(V × E + E × H)* where *V* is *Vocabulary*, *H* is *Hidden
    Layer Size*, *E* is *Embedings*, which leads to more efficient usage of the total
    model parameters *if H >> E* is satisfied.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Cross-layer parameter sharing prevents the total number of parameters from increasing
    as the network gets deeper. The technique is considered another way to improve
    parameter efficiency since we can keep the parameter size smaller by sharing or
    copying. In the original paper, they experimented with many ways to share parameters,
    such as either sharing FF-only parameters across layers or sharing attention-only
    parameters or entire parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The other modification of Albert is inter-sentence coherence loss. As we already
    discussed, the BERT architecture takes advantage of two loss calculations, the
    **Masked Language Modeling** (**MLM**) loss and NSP. NSP comes with binary cross-entropy
    loss for predicting whether or not two segments appear in a row in the original
    text. The negative examples are obtained by selecting two segments from different
    documents. However, the Albert team criticized NSP for being a topic detection
    problem, which is considered a relatively easy problem. Therefore, the team proposed
    a loss based primarily on coherence rather than topic prediction. They utilized
    SOP loss, which focuses on modeling inter-sentence coherence instead of topic
    prediction. SOP loss uses the same positive examples technique as BERT, (which
    is two consecutive segments from the same document), and as negative examples,
    the same two consecutive segments but with their order swapped. The model is then
    forced to learn finer-grained distinctions between coherence properties at the
    discourse level.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the original BERT and Albert configuration with the `Transformers`
    library. The following piece of code shows how to configure a BERT-Base initial
    model. As you see in the output, the number of parameters is around 110 M:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And the following piece of code shows how to define the Albert model with two
    classes, `AlbertConfig` and `AlbertModel`, from the `Transformers` library:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Due to that, the default Albert configuration points to Albert-xxlarge. We
    need to set the hidden size, the number of attention heads, and the intermediate
    size to fit Albert-base. And the code shows the Albert-base mode as 11M, 10 times
    smaller than the BERT-base model. The original paper on ALBERT reported benchmarking
    as in the following table:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Albert model benchmarking ](img/B17123_03_14.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.14 – Albert model benchmarking
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From this point on, in order to train an Albert language model from scratch,
    we need to go through similar phases to those we already illustrated in BERT training
    in the previous sections by using the uniform Transformers API. There''s no need
    to explain the same steps here! Instead, let''s load an already trained Albert
    language model as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding pieces of code download the Albert model weights and its configuration
    from the HuggingFace hub or from our local cache directory if already cached,
    which means you''ve already called the `AlbertTokenizer.from_pretrained()` function
    before. Since that the model object is a pre-trained language model, the things
    we can do with this model are limited for now. We need to train it on a downstream
    task to able to use it for inference, which will be the main subject of further
    chapters. Instead, we can take advantage of its masked language model objective
    as follows:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the output:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15 – The fill-mask output results for albert-base-v2 ](img/B17123_03_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – The fill-mask output results for albert-base-v2
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The `fill-mask` pipeline computes the scores for each vocabulary token with
    the `SoftMax()` function and sorts the most probable tokens where `cute` is the
    winner with a probability score of 0.281\. You may notice that entries in the
    *token_str* column start with the `_` character, which is due to the metaspace
    component of the tokenizer of Albert.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the next alternative, *RoBERTa*, which mostly focuses on
    the pre-training phase.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Robustly Optimized BERT pre-training Approach** (**RoBERTa**) is another
    popular BERT reimplementation. It has provided many more improvements in training
    strategy than architectural design. It outperformed BERT in almost all individual
    tasks on GLUE. Dynamic masking is one of its original design choices. Although
    static masking is better for some tasks, the RoBERTa team showed that dynamic
    masking can perform well for overall performances. Let''s compare the changes
    from BERT and summarize all the features as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes in architecture are as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Removing the next sentence prediction training objective
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically changing the masking patterns instead of static masking, which is
    done by generating masking patterns whenever they feed a sequence to the model
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BPE** sub-word tokenizer'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The changes in training are as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Controlling the training data: More data is used, such as 160 GB instead of
    the 16 GB originally used in BERT. Not only the size of the data but the quality
    and diversity were taken into consideration in the study.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longer iterations of up to 500K pretraining steps.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A longer batch size.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longer sequences, which leads to less padding.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large 50K BPE vocabulary instead of a 30K BPE vocabulary.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thanks to the Transformers uniform API, as in the Albert model pipeline above,
    we initialize the RoBERTa model as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In order to load the pre-trained model, we execute the following pieces of
    code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'These lines illustrate how the model processes a given text. The output representation
    at the last layer is not useful at the moment. As we''ve mentioned several times,
    we need to fine-tune the main language models. The following execution applies
    the `fill-mask` function using the `roberta-base` model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – The fill-mask task results for roberta-base ](img/B17123_03_16.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – The fill-mask task results for roberta-base
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the previous ALBERT `fill-mask` model, this pipeline ranks the suitable
    candidate words. Please ignore the prefix `Ġ` in the tokens – that is an encoded
    space character produced by the byte-level BPE tokenizer, which we will discuss
    later. You should have noticed that we used the `[MASK]` and `<mask>` tokens in
    ALBERT and RoBERTa pipeline in order to hold place for masked token. This is because
    of the configuration of `tokenizer`. To learn which token expression will be used,
    you can check `tokenizer.mask_token`. Please see the following execution:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To ensure proper mask token use, we can add the `fillmask.tokenizer.mask_token`
    expression in the pipeline as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ELECTRA
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **ELECTRA** model (*proposed by Kevin Clark et al. in 2020*) focuses on
    a new masked language model utilizing the replaced token detection training objective.
    During pre-training, the model is forced to learn to distinguish real input tokens
    from synthetically generated replacements where the synthetic negative example
    is sampled from plausible tokens rather than randomly sampled tokens. The Albert
    model criticized the NSP objective of BERT for being a topic detection problem
    and using low-quality negative examples. ELECTRA trains two neural networks, a
    generator and a discriminator, so that the former produces high-quality negative
    examples, whereas the latter distinguishes the original token from the replaced
    token. We know GAN networks from the field of computer vision, in which the generator
    *G* produces fake images and tries to fool the discriminator *D*, and the discriminator
    network tries to avoid being fooled. The ELECTRA model applies almost the same
    generator-discriminator approach to replace original tokens with high-quality
    negative examples that are plausible replacements but synthetically generated.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'In order not to repeat the same code with other examples, we only provide a
    simple `fill-mask` example for the Electra generator as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can see the entire list of models at the following link: [https://huggingface.co/Transformers/model_summary.html](https://huggingface.co/transformers/model_summary.html).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The model checkpoints can be found at [https://huggingface.co/models](https://huggingface.co/models).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We've finally completed the autoencoding model part. Now we'll move
    on to tokenization algorithms, which have an important effect on the success of
    Transformers.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Working with tokenization algorithms
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the opening part of the chapter, we trained the BERT model using a specific
    tokenizer, namely `BertWordPieceTokenizer`. Now it is worth discussing the tokenization
    process in detail here. Tokenization is a way of splitting textual input into
    tokens and assigning an identifier to each token before feeding the neural network
    architecture. The most intuitive way is to split the sequence into smaller chunks
    in terms of space. However, such approaches do not meet the requirement of some
    languages, such as Japanese, and also may lead to huge vocabulary problems. Almost
    all Transformer models leverage subword tokenization not only for reducing dimensionality
    but also for encoding rare (or unknown) words not seen in training. The tokenization
    relies on the idea that every word, including rare words or unknown words, can
    be decomposed into meaningful smaller chunks that are widely seen symbols in the
    training corpus.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Some traditional tokenizers developed within Moses and the `nltk` library apply
    advanced rule-based techniques. But the tokenization algorithms that are used
    with Transformers are based on self-supervised learning and extract the rules
    from the corpus. Simple intuitive solutions for rule-based tokenization are based
    on using characters, punctuation, or whitespace. Character-based tokenization
    causes language models to lose the input meaning. Even though it can reduce the
    vocabulary size, which is good, it makes it hard for the model to capture the
    meaning of `cat` by means of the encodings of the characters `c`, `a`, and `t`.
    Moreover, the dimension of the input sequence becomes very large. Likewise, punctuation-based
    models cannot treat some expressions, such as *haven't* or *ain't*, properly.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, several advanced subword tokenization algorithms, such as BPE, have
    become an integral part of Transformer architectures. These modern tokenization
    procedures consist of two phases: The pre-tokenization phase simply splits the
    input into tokens either using space as or language-dependent rules. Second, the
    tokenization training phase is to train the tokenizer and build a base vocabulary
    of a reasonable size based on tokens. Before training our own tokenizers, let''s
    load a pre-trained tokenizer. The following code loads a Turkish tokenizer, which
    is of type `BertTokenizerFast`, from the `Transformers` library with a vocabulary
    size of 32K:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following code loads an English BERT tokenizer for the `bert-base-uncased`
    model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s see how they work! We tokenize the word `telecommunication` with these
    two tokenizers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `word_en` token is already in the vocabulary of the English tokenizer but
    not in that of the Turkish one. So let''s see what happens with the Turkish tokenizer:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Since the Turkish tokenizer model has no such a word in its vocabulary, it
    needs to break the word into parts that make sense to it. All these split tokens
    are already stored in the model vocabulary. Please notice the output of the following
    execution:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于土耳其分词器模型的词汇表中没有这样一个词，它需要将单词分解成对它有意义的部分。所有这些分割的标记已经存储在模型词汇表中。请注意以下执行的输出：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s tokenize the same word with the English tokenizer that we already loaded:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们已经加载的英语分词器对相同的单词进行分词：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Since the English model has the word `telecommunication` in the base vocabulary,
    it does not need to break it into parts but rather takes it as a whole. By learning
    from the corpus, the tokenizers are capable of transforming a word into mostly
    grammatically logical subcomponents. Let''s take a difficult example from Turkish.
    As an agglutinative language, Turkish allows us to add many suffixes to a word
    stem to construct very long words. Here is one of the longest words in the Turkish
    language used in a text ([https://en.wikipedia.org/wiki/Longest_word_in_Turkish](https://en.wikipedia.org/wiki/Longest_word_in_Turkish)):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于英语模型在基础词汇表中有单词`telecommunication`，它不需要将其分割成部分，而是将其作为一个整体。通过从语料库中学习，分词器能够将一个单词转换为大部分语法逻辑的子部分。让我们以土耳其语的一个难例为例。作为一种聚合语言，土耳其语允许我们在一个词干上加入许多后缀，构成非常长的单词。以下是土耳其语中使用的最长单词之一（来源于[https://en.wikipedia.org/wiki/Longest_word_in_Turkish](https://en.wikipedia.org/wiki/Longest_word_in_Turkish)）：
- en: '*Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine*'
- en: 'It means that *As though you happen to have been from among those whom we will
    not be able to easily/quickly make a maker of unsuccessful ones*. The Turkish
    BERT tokenizer may not have seen this word in training, but it has seen its pieces;
    *muvaffak (succesful) as the stem, ##iyet(successfulness), ##siz (unsuccessfulness),
    ##leş (become unsuccessful)*, and so forth. The Turkish tokenizer extracts components
    that seem to be grammatically logical for the Turkish language when comparing
    the results with a Wikipedia article:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 它的意思是*仿佛你是我们中那些我们不能轻易快速地使成为失败者的人之一*。土耳其BERT分词器可能在训练中没有见过这个单词，但它已经看到了它的部分；*muvaffak（成功）作为词干，##iyet（成功性），##siz（不成功），##leş（变得不成功）*，等等。当将结果与维基百科文章进行比较时，土耳其分词器提取出了在土耳其语中看起来是语法合乎逻辑的组件。
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The Turkish tokenizer is an example of the WordPiece algorithm since it works
    with a BERT model. Almost all language models including BERT, DistilBERT, and
    ELECTRA require a WordPiece tokenizer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 土耳其分词器是WordPiece算法的一个例子，因为它与BERT模型协同工作。几乎所有的语言模型，包括BERT、DistilBERT和ELECTRA，都需要一个WordPiece分词器。
- en: Now we are ready to take a look at the tokenization approaches used with Transformers.
    First, we'll discuss the widely used tokenizations of BPE, WordPiece, and SentencePiece
    a bit and then train them with HuggingFace's fast `tokenizers` library.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备研究用于Transformers的分词方法。首先，我们将简要讨论一下BPE、WordPiece和SentencePiece的广泛使用的分词，然后用HuggingFace的快速`分词器`库进行训练。
- en: Byte pair encoding
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码
- en: '**BPE** is a data compression technique. It scans the data sequence and iteratively
    replaces the most frequent pair of bytes with a single symbol. It was first adapted
    and proposed in *Neural Machine Translation of Rare Words with Subword Units,
    Sennrich et al. 2015,* to solve the problem of unknown words and rare words for
    machine translation. Currently, it is successfully being used within GPT-2 and
    many other state-of-the-art models. Many modern tokenization algorithms are based
    on such compression techniques.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**BPE** 是一种数据压缩技术。它会扫描数据序列，并迭代地用一个单一的符号替换最常见的字节对。它最初是在*Neural Machine Translation
    of Rare Words with Subword Units, Sennrich et al. 2015*中提出来解决机器翻译中未知单词和稀有单词的问题。目前，它成功应用于GPT-2和许多其他领先的模型中。许多现代分词算法都是基于这样的压缩技术。'
- en: It represents text as a sequence of character n-grams, which are also called
    character-level subwords. The training starts initially with a vocabulary of all
    Unicode characters (or symbols) seen in the corpus. This can be small for English
    but can be large for character-rich languages such as Japanese. Then, it iteratively
    computes character bigrams and replaces the most frequent ones with special new
    symbols. For example, *t* and *h* are frequently occurring symbols. We replace
    consecutive symbols with the *th* symbol. This process is kept iteratively running
    until the vocabulary has attained the desired vocabulary size. The most common
    vocabulary size is around 30K.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: BPE is particularly effective at representing unknown words. However, it may
    not guarantee the handling of rare words and/or words including rare subwords.
    In such cases, it associates rare characters with a special symbol, *<UNK>*, which
    may lead to losing meaning in words a bit. As a potential solution, **Byte-Level
    BPE** (**BBPE**) has been proposed, which uses a 256-byte set of vocabulary instead
    of Unicode characters to ensure that every base character is included in the vocabulary.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece tokenization
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**WordPiece** is another popular word segmentation algorithm widely used with
    BERT, DistilBERT, and Electra. It was proposed by Schuster and Nakajima to solve
    the Japanese and Korean voice problem in 2012\. The motivation behind this work
    was that, although not a big issue for the English language, word segmentation
    is important preprocessing for many Asian languages, because in these languages
    spaces are rarely used. Therefore, we come across word segmentation approaches
    in NLP studies in Asian languages more often. Similar to BPE, WordPiece uses a
    large corpus to learn vocabulary and merging rules. While BPE and BBPE learn the
    merging rules based on co-occurrence statistics, the WordPiece algorithm uses
    maximum likelihood estimation to extract the merging rules from a corpus. It first
    initializes the vocabulary with Unicode characters, which are also called vocabulary
    symbols. It treats each word in the training corpus as a list of symbols (initially
    Unicode characters), and then it iteratively produces a new symbol merging two
    symbols out of all the possible candidate symbol pairs based on the likelihood
    maximization rather than frequency. This production pipeline continues until the
    desired vocabulary size is reached.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Sentence piece tokenization
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previous tokenization algorithms treat text as a space-separated word list.
    This space-based splitting does not work in some languages. In the German language,
    compound nouns are written without spaces, for example, menschenrechte (human
    rights). The solution is to use language-specific pre-tokenizers. In German, an
    NLP pipeline leverages a compound-splitter module to check whether a word can
    be subdivided into smaller words. However, East Asian languages (for example,
    Chinese, Japanese, Korean, and Thai) do not use spaces between words. The `_`
    character, which is also why we saw `_` in the output of the Albert model example
    earlier. Other popular language models that use SentencePiece are XLNet, Marian,
    and T5\.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed subword tokenization approaches. It is time to start
    conducting experiments for training with the `tokenizers` library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizers library
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that the already-trained tokenizers for Turkish and English
    are part of the `Transformers` library in the previous code examples. On the other
    hand, the HuggingFace team provided the `tokenizers` library independently from
    the `Transformers` library to be fast and give us more freedom. The library was
    originally written in Rust, which makes multi-core parallel computations possible
    and is wrapped with Python ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `tokenizers` library, we use this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `tokenizers` library provides several components so that we can build an
    end-to-end tokenizer from preprocessing the raw text to decoding tokenized unit
    IDs:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '*Normalizer→ PreTokenizer → Modeling → Post-Processor → Decoding*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the tokenization pipeline:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Tokenization pipeline ](img/B17123_03_17.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Tokenization pipeline
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalizer** allows us to apply primitive text processing such as lowercasing,
    stripping, Unicode normalization, and removing accents.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PreTokenizer** prepares the corpus for the next training phase. It splits
    the input into tokens depending on the rules, such as whitespace.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Training** is a subword tokenization algorithm such as *BPE*, *BBPE*,
    and *WordPiece*, which we''ve discussed already. It discovers subwords/vocabulary
    and learns generation rules.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing** provides advanced class construction that is compatible
    with Transformers models such as BertProcessors. We mostly add special tokens
    such as *[CLS]* and *[SEP]* to the tokenized input just before feeding the architecture.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder** is in charge of converting token IDs back to the original string.
    It is just for inspecting what is going on.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training BPE
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s train a BPE tokenizer using Shakespeare''s plays:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'It is loaded as follows:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We will need a post-processor (`TemplateProcessing`) for all the tokenization
    algorithms ahead. We need to customize the post-processor to make the input convenient
    for a particular language model. For example, the following template will be suitable
    for the BERT model since it needs the *[CLS]* token at the beginning of the input
    and *[SEP]* tokens both at the end and in the middle.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We define the template as follows:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We import the necessary components to build an end-to-end tokenization pipeline:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We start by instantiating **BPE** as follows:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preprocessing part has two components: *normalizer* and *pre-tokenizer*.
    We may have more than one normalizer. So, we compose a `Sequence` of normalizer
    components that includes multiple normalizers where `NFD()` is a Unicode normalizer
    and `StripAccents()` removes accents. For pre-tokenization, `Whitespace()` gently
    breaks the text based on space. Since the decoder component must be compatible
    with the model, `BPEDecoder` is selected for the `BPE` model:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Well! We are ready to train the tokenizer on the data. The following execution
    instantiates `BpeTrainer()`, which helps us to organize the entire training process
    by setting hyperparameters. We set the vocabulary size parameter to 5K since our
    Shakespeare corpus is relatively small. For a large-scale project, we use a bigger
    corpus and normally set the vocabulary size to around 30K:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We have completed the training!
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Training from the filesystem: To start the training process, we passed an in-memory
    Shakespeare object as a list of strings to `tokenizer.train_from_iterator()`.
    For a large-scale project with a large corpus, we need to design a Python generator
    that yields string lines mostly by consuming the files from the filesystem rather
    than in-memory storage. You should also check `tokenizer.train()` to train from
    the filesystem storage as applied in the BERT training section above.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s grab a random sentence from the play Macbeth, name it `sen`, and tokenize
    it with our fresh tokenizer:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Thanks to the post-processor function above, we see additional *[CLS]* and
    *[SEP]* tokens in the proper position. There is only one split word, *handle*
    (*hand*, *le*), since we passed to the model a sentence from the play Macbeth
    that the model already knew. Besides, we used a small corpus, and the tokenizer
    is not forced to use compression. Let''s pass a challenging phrase, `Hugging Face`,
    that the tokenizer might not know:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The term `Hugging` is lowercased and split into three pieces `hu`, `gg`, `ing`,
    since the model vocabulary contains all other tokens but `Hugging`. Let''s pass
    two sentences now:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Notice that the post-processor injected the `[SEP]` token as an indicator.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is time to save the model. We can either save the sub-word tokenization
    model or the entire tokenization pipeline. First, let''s save the BPE model only:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The model saved two files regarding vocabulary and merging rules. The `merge.txt`
    file is composed of 4,948 merging rules:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The top five rules ranked are as shown in the following where we see that (`t`,
    `h`) is the first ranked rule due to that being the most frequent pair. For testing,
    the model scans the textual input and tries to merge these two symbols first if
    applicable:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The BPE algorithm ranks the rules based on frequency. When you manually calculate
    character bigrams in the Shakespeare corpus, you will find (`t`, `h`) the most
    frequent pair.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now save and load the entire tokenization pipeline:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We successfully reloaded the tokenizer!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Training the WordPiece model
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will train the WordPiece model:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary modules:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The following lines instantiate an empty WordPiece tokenizer and prepare it
    for training. `BertNormalizer` is a pre-defined normalizer sequence that includes
    the processes of cleaning the text, transforming accents, handling Chinese characters,
    and lowercasing:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, we instantiate a proper trainer, `WordPieceTrainer()` for `WordPiece()`,
    to organize the training process:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s use `WordPieceDecoder()` to treat the sentences properly:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We have not come across any `[UNK]` tokens in the output since the tokenizer
    somehow knows or splits the input for encoding. Let''s force the model to produce
    `[UNK]`tokens as in the following code. Let''s pass a Turkish sentence to our
    tokenizer:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Well done! We have a couple of unknown tokens since the tokenizer does not find
    a way to decompose the given word from the merging rules and the base vocabulary.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So far, we have designed our tokenization pipeline all the way from the normalizer
    component to the decoder component. On the other hand, the `tokenizers` library
    provides us with an already made (not trained) empty tokenization pipeline with
    proper components to build quick prototypes for production. Here are some pre-made
    tokenizers:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '`CharBPETokenizer`: The original BPE'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ByteLevelBPETokenizer`: The byte-level version of the BPE'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SentencePieceBPETokenizer`: A BPE implementation compatible with the one used
    by *SentencePiece*'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BertWordPieceTokenizer`: The famous BERT tokenizer, using WordPiece'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code imports these pipelines:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: All these pipelines are already designed for us. The rest of the process (such
    as training, saving the model, and using the tokenizer) is the same as our previous
    BPE and WordPiece training procedure.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We have made great progress and trained our first Transformer model
    as well as its tokenizer.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have experienced autoencoding models both theoretically
    and practically. Starting with basic knowledge about BERT, we trained it as well
    as a corresponding tokenizer from scratch. We also discussed how to work inside
    other frameworks, such as Keras. Besides BERT, we also reviewed other autoencoding
    models. To avoid excessive code repetition, we did not provide the full implementation
    for training other models. During the BERT training, we trained the WordPiece
    tokenization algorithm. In the last part, we examined other tokenization algorithms
    since it is worth discussing and understanding all of them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoding models use the left decoder side of the original Transformer and
    are mostly fine-tuned for classification problems. In the next chapter, we will
    discuss and learn about the right decoder part of Transformers to implement language
    generation models.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
