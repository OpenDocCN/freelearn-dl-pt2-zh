- en: '*Chapter 3*: Autoencoding Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at and studied how a typical Transformer
    model can be used by HuggingFace's Transformers. So far, all the topics have included
    how to use pre-defined or pre-built models and less information has been given
    about specific models and their training.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will gain knowledge of how we can train autoencoding language
    models on any given language from scratch. This training will include pre-training
    and task-specific training of the models. First, we will start with basic knowledge
    about the BERT model and how it works. Then we will train the language model using
    a simple and small corpus. Afterward, we will look at how the model can be used
    inside any Keras model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an overview of what will be learned in this chapter, we will discuss the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT – one of the autoencoding language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoding language model training for any language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing models with the community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding other autoencoding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with tokenization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technical requirements for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers >= 4.0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch >= 1.0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow >= 2.4.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets >= 1.4.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please also check the corresponding GitHub code of `chapter 03`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see Code in Action Video: [https://bit.ly/3i1ycdY](https://bit.ly/3i1ycdY)'
  prefs: []
  type: TYPE_NORMAL
- en: BERT – one of the autoencoding language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bidirectional Encoder Representations from Transformers**, also known as
    **BERT**, was one of the first autoencoding language models to utilize the encoder
    Transformer stack with slight modifications for language modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: The BERT architecture is a multilayer Transformer encoder based on the Transformer
    original implementation. The Transformer model itself was originally for machine
    translation tasks, but the main improvement made by BERT is the utilization of
    this part of the architecture to provide better language modeling. This language
    model, after pretraining, is able to provide a global understanding of the language
    it is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: BERT language model pretraining tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To have a clear understanding of the masked language modeling used by BERT,
    let''s define it with more details. **Masked language modeling** is the task of
    training a model on input (a sentence with some masked tokens) and obtaining the
    output as the whole sentence with the masked tokens filled. But how and why does
    it help a model to obtain better results on downstream tasks such as classification?
    The answer is simple: if the model can do a cloze test (a linguistic test for
    evaluating language understanding by filling in blanks), then it has a general
    understanding of the language itself. For other tasks, it has been pretrained
    (by language modeling) and will perform better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of a cloze test:'
  prefs: []
  type: TYPE_NORMAL
- en: George Washington was the first President of the ___ States.
  prefs: []
  type: TYPE_NORMAL
- en: It is expected that *United* should fill in the blank. For a masked language
    model, the same task is applied, and it is required to fill in the masked tokens.
    However, masked tokens are selected randomly from a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Another task that BERT is trained on is **Next Sentence Prediction** (**NSP**).
    This pretraining task ensures that BERT learns not only the relations of all tokens
    to each other in predicting masked ones but also helps it understand the relation
    between two sentences. A pair of sentences is selected and given to BERT with
    a *[SEP]* splitter token in between. It is also known from the dataset whether
    the second sentence comes after the first one or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of NSP:'
  prefs: []
  type: TYPE_NORMAL
- en: '*It is required from reader to fill the blank. Bitcoin price is way over too
    high compared to other altcoins.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the model is required to predict it as negative (the sentences
    are not related to each other).
  prefs: []
  type: TYPE_NORMAL
- en: 'These two pretraining tasks enable BERT to have an understanding of the language
    itself. BERT token embeddings provide a contextual embedding for each token. **Contextual
    embedding** means each token has an embedding that is completely related to the
    surrounding tokens. Unlike Word2Vec and such models, BERT provides better information
    for each token embedding. NSP tasks, on the other hand, enable BERT to have better
    embeddings for *[CLS]* tokens. This token, as was discussed in the first chapter,
    provides information about the whole input. *[CLS]* is used for classification
    tasks and in the pretraining part learns the overall embedding of the whole input.
    The following figure shows an overall look at the BERT model. *Figure 3.1* shows
    the respective input and output of the BERT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The BERT model ](img/B17123_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The BERT model
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next section!
  prefs: []
  type: TYPE_NORMAL
- en: A deeper look into the BERT language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenizers are one of the most important parts of many NLP applications in their
    respective pipelines. For BERT, WordPiece tokenization is used. Generally, **WordPiece**,
    **SentencePiece**, and **BytePairEncoding** (**BPE**) are the three most widely
    known tokenizers, used by different Transformer-based architectures, which are
    also covered in the next sections. The main reason that BERT or any other Transformer-based
    architecture uses subword tokenization is the ability of such tokenizers to deal
    with unknown tokens.
  prefs: []
  type: TYPE_NORMAL
- en: BERT also uses positional encoding to ensure the position of the tokens is given
    to the model. If you recall from [*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016),
    *From Bag-of-Words to the Transformers*, BERT and similar models use non-sequential
    operations such as dense neural layers. Conventional models such as LSTM- and
    RNN-based models get the position by the order of the tokens in the sequence.
    In order to provide this extra information to BERT, positional encoding comes
    in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining of BERT such as autoencoding models provides language-wise information
    for the model, but in practice, when dealing with different problems such as sequence
    classification, token classification, or question answering, different parts of
    the model output are used.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the case of a sequence classification task, such as sentiment
    analysis or sentence classification, it is proposed by the original BERT article
    that *[CLS]* embedding from the last layer must be used. However, there is other
    research that performs classification using different techniques using BERT (using
    average token embedding from all tokens, deploying an LSTM over the last layer,
    or even using a CNN on top of the last layer). The last *[CLS]* embedding for
    sequence classification can be used by any classifier, but the proposed, and the
    most common one, is a dense layer with an input size equal to the final token
    embedding size and an output size equal to the number of classes with a softmax
    activation function. Using sigmoid is also another alternative when the output
    could be multilabel and the problem itself is a multilabel classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you more detailed information about how BERT actually works, the following
    illustration shows an example of an NSP task. Note that the tokenization is simplified
    here for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – BERT example for an NSP task ](img/B17123_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – BERT example for an NSP task
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model has different variations, with different settings. For example,
    the size of the input is variable. In the preceding example, it is set to *512*
    and the maximum sequence size that model can get as input is *512*. However, this
    size includes special tokens, *[CLS]* and *[SEP]*, so it will be reduced to *510*.
    On the other hand, using WordPiece as a tokenizer yields subword tokens, and the
    sequence size before we can have fewer words, and after tokenization, the size
    will increase because the tokenizer breaks words into subwords if they are not
    commonly seen in the pretrained corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows an illustration of BERT for different tasks. For
    an NER task, the output of each token is used instead of *[CLS]*. In the case
    of question answering, the question and the answer are concatenated using the
    *[SEP]* delimiter token and the answer is annotated using *Start/End* and the
    *Span* output from the last layer. In this case, the *Paragraph* is the context
    that the *Question* is asked about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – BERT model for various NLP tasks ](img/B17123_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – BERT model for various NLP tasks
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of all of these tasks, the most important ability of BERT is its
    contextual representation of text. The reason it is successful in various tasks
    is because of the Transformer encoder architecture that represents input in the
    form of dense vectors. These vectors can be easily transformed into output by
    very simple classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you have learned about BERT and how it works. You have learned
    detailed information on various tasks that BERT can be used for and the important
    points of this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how you can pre-train BERT and use it after
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoding language model training for any language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed how BERT works and that it is possible to use the pretrained
    version of it provided by the HuggingFace repository. In this section, you will
    learn how to use the HuggingFace library to train your own BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, it is essential to have good training data, which will be used
    for the language modeling. This data is called the **corpus**, which is normally
    a huge pile of data (sometimes it is preprocessed and cleaned). This unlabeled
    corpus must be appropriate for the use case you wish to have your language model
    trained on; for example, if you are trying to have a special BERT for, let's say,
    the English language. Although there are tons of huge, good datasets, such as
    Common Crawl ([https://commoncrawl.org/](https://commoncrawl.org/)), we would
    prefer a small one for faster training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IMDB dataset of 50K movie reviews (available at [https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews))
    is a large dataset for sentiment analysis, but small if you use it as a corpus
    for training your language model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily download and save it in `.txt` format for language model and
    tokenizer training by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After preparing the corpus, the tokenizer must be trained. The `tokenizers`
    library provides fast and easy training for the WordPiece tokenizer. In order
    to train it on your corpus, it is required to run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will train the tokenizer. You can access the trained vocabulary by using
    the `get_vocab()` function of the trained `tokenizer` object. You can get the
    vocabulary by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is essential to save the tokenizer to be used afterwards. Using the `save_model()`
    function of the object and providing the directory will save the tokenizer vocabulary
    for further usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And you can reload it by using the `from_file()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the tokenizer by following this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The special tokens `[CLS]` and `[SEP]` will be automatically added to the list
    of tokens because BERT needs them for processing input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try another sentence using our tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Seems like a good tokenizer for noisy and misspelled text. Now that you have
    your tokenizer ready and saved, you can train your own BERT. The first step is
    to use `BertTokenizerFast` from the `Transformers` library. You are required to
    load the trained tokenizer from the previous step by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have used `BertTokenizerFast` because it is suggested by the HuggingFace
    documentation. There is also `BertTokenizer`, which, according to the definition
    from the library documentation, is not implemented as fast as the fast version.
    In most of the pretrained models' documentations and cards, it is highly recommended
    to use the `BertTokenizerFast` version.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is preparing the corpus for faster training by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And it is required to provide a data collator for masked language modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data collator gets the data and prepares it for the training. For example,
    the data collator above takes data and prepares it for masked language modeling
    with a probability of `0.15`. The purpose of using such a mechanism is to do the
    preprocessing on the fly, which makes it possible to use fewer resources. On the
    other hand, it slows down the training process because each sample has to be preprocessed
    on the fly at training time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The training arguments also provide information for the trainer in the training
    phase, which can be set by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now make the BERT model itself, which we are going to use with the default
    configuration (the number of attention heads, Transformer encoder layers, and
    so on):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the final step is to make a trainer object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can train your language model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It will show you a progress bar indicating the progress made in training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4 – BERT model training progress ](img/B17123_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.4 – BERT model training progress
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'During the model training, a log directory called `runs` will be used to store
    the checkpoint in steps:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5 – BERT model checkpoints ](img/B17123_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.5 – BERT model checkpoints
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After the training is finished, you can easily save the model using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Up to this point, you have learned how you can train BERT from scratch for any
    specific language that you desire. You've learned how to train the tokenizer and
    BERT model using the corpus you have prepared.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The default configuration that you provided for BERT is the most essential
    part of this training process, which defines the architecture of BERT and its
    hyperparameters. You can take a peek at these parameters by using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6 – BERT model configuration ](img/B17123_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.6 – BERT model configuration
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to replicate `max_position_embedding`, `num_attention_heads`, `num_hidden_layers`,
    `intermediate_size`, and `hidden_size`, directly affects the training time. Increasing
    them dramatically increases the training time for a large corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example, you can easily make a new configuration for a tiny version of
    BERT for faster training using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the result of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Tiny BERT model configuration ](img/B17123_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.8 – Tiny BERT model configuration
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By using the same method, we can make a tiny BERT model using this configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And using the same parameters for training, you can train this tiny new BERT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Tiny BERT model configuration ](img/B17123_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.9 – Tiny BERT model configuration
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is clear that the training time is dramatically decreased, but you should
    be aware that this is a tiny version of BERT with fewer layers and parameters,
    which is not as good as BERT Base.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Up to this point, you have learned how to train your own model from scratch,
    but it is essential to note that using the `datasets` library is a better choice
    when dealing with datasets for training language models or leveraging it to perform
    task-specific training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The BERT language model can also be used as an embedding layer combined with
    any deep learning model. For example, you can load any pretrained BERT model or
    your own version that has been trained in the previous step. The following code
    shows how you must load it to be used in a Keras model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'But you do not need the whole model; instead, you can access the layers by
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, there is just a single layer from `TFBertMainLayer`, which
    you can access within your Keras model. But before using it, it is nice to test
    it and see what kind of output it provides:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10 – BERT model output ](img/B17123_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.10 – BERT model output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As can be seen from the result, there are two outputs: one for the last hidden
    state and one for the pooler output. The last hidden state provides all token
    embeddings from BERT with additional *[CLS]* and *[SEP]* tokens at the start and
    end, respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have learned more about the TensorFlow version of BERT, you can
    make a Keras model using this new embedding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model object, which is a Keras model, has two inputs: one for tokens and
    one for masks. Tokens has `token_ids` from the tokenizer output and the masks
    will have `attention_mask`. Let''s try it and see what happens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is important to use `max_length`, `truncation`, and `pad_to_max_length`
    when using `tokenizer`. These parameters make sure you have the output in a usable
    shape by padding it to the maximum length of 256 that was defined before. Now
    you can run the model using this sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – BERT model classification output ](img/B17123_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.11 – BERT model classification output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When training the model, you need to compile it using the `compile` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – BERT model summary ](img/B17123_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.12 – BERT model summary
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the model summary, you can see that the model has 109,483,778 trainable
    parameters including BERT. But if you have your BERT model pretrained and you
    want to freeze it in a task-specific training, you can do so with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As far as we know, the layer index of the embedding layer is 2, so we can simply
    freeze it. If you rerun the summary function, you will see the trainable parameters
    are reduced to 1,538, which is the number of parameters of the last layer:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – BERT model summary with fewer trainable parameters ](img/B17123_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.13 – BERT model summary with fewer trainable parameters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you recall, we used the IMDB sentiment analysis dataset for training the
    language model. Now you can use it for training the Keras-based model for sentiment
    analysis. But first, you need to prepare the input and output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And finally, your data is ready, and you can fit your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And after fitting the model, your model is ready to be used. Up to this point,
    you have learned how to perform model training for a classification task. You
    have learned how to save it, and in the next section, you will learn how it is
    possible to share the trained model with the community.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing models with the community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HuggingFace provides a very easy-to-use model-sharing mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can simply use the following `cli` tool to log in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After you''ve logged in using your own credentials, you can create a repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can put any model name for the `a-fancy-model-name` parameter and then
    it is essential to make sure you have git-lfs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Git LFS is a Git extension used for handling large files. HuggingFace pretrained
    models are usually large files that require extra libraries such as LFS to be
    handled by Git.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then you can clone the repository you have just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Afterward, you can add and remove from the repository as you like, and then,
    just like Git usage, you have to run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Autoencoding models rely on the left encoder side of the original Transformer
    and are highly efficient at solving classification problems. Even though BERT
    is a typical example of autoencoding models, there are many alternatives discussed
    in the literature. Let's take a look at these important alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding other autoencoding models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, we will review autoencoding model alternatives that slightly
    modify the original BERT. These alternative re-implementations have led to better
    downstream tasks by exploiting many sources: optimizing the pre-training process
    and the number of layers or heads, improving data quality, designing better objective
    functions, and so forth. The source of improvements roughly falls into two parts:
    *better architectural design choice* and *pre-training control*.'
  prefs: []
  type: TYPE_NORMAL
- en: Many effective alternatives have been shared lately, so it is impossible to
    understand and explain them all here. We can take a look at some of the most cited
    models in the literature and the most used ones on NLP benchmarks. Let's start
    with **Albert** as a re-implementation of BERT that focuses especially on architectural
    design choice.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ALBERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of language models is considered to improve as their size gets
    bigger. However, training such models is getting more challenging due to both
    memory limitations and longer training times. To address these issues, the Google
    team proposed the **Albert model** (**A Lite BERT** for Self-Supervised Learning
    of Language Representations), which is indeed a reimplementation of the BERT architecture
    by utilizing several new techniques that reduce memory consumption and increase
    the training speed. The new design led to the language models scaling much better
    than the original BERT. Along with 18 times fewer parameters, Albert trains 1.7
    times faster than the original BERT-large model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Albert model mainly consists of three modifications of the original BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: Factorized embedding parameterization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-layer parameter sharing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inter-sentence coherence loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two modifications are parameter-reduction methods that are related
    to the issue of model size and memory consumption in the original BERT. The third
    corresponds to a new objective function: **Sentence-Order Prediction** (**SOP**),
    replacing the **Next Sentence Prediction** (**NSP**) task of the original BERT,
    which led to a much thinner model and improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Factorized embedding parameterization is used to decompose the large vocabulary-embedding
    matrix into two small matrices, which separate the size of the hidden layers from
    the size of the vocabulary. This decomposition reduces the embedding parameters
    from *O(V × H) to O(V × E + E × H)* where *V* is *Vocabulary*, *H* is *Hidden
    Layer Size*, *E* is *Embedings*, which leads to more efficient usage of the total
    model parameters *if H >> E* is satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-layer parameter sharing prevents the total number of parameters from increasing
    as the network gets deeper. The technique is considered another way to improve
    parameter efficiency since we can keep the parameter size smaller by sharing or
    copying. In the original paper, they experimented with many ways to share parameters,
    such as either sharing FF-only parameters across layers or sharing attention-only
    parameters or entire parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The other modification of Albert is inter-sentence coherence loss. As we already
    discussed, the BERT architecture takes advantage of two loss calculations, the
    **Masked Language Modeling** (**MLM**) loss and NSP. NSP comes with binary cross-entropy
    loss for predicting whether or not two segments appear in a row in the original
    text. The negative examples are obtained by selecting two segments from different
    documents. However, the Albert team criticized NSP for being a topic detection
    problem, which is considered a relatively easy problem. Therefore, the team proposed
    a loss based primarily on coherence rather than topic prediction. They utilized
    SOP loss, which focuses on modeling inter-sentence coherence instead of topic
    prediction. SOP loss uses the same positive examples technique as BERT, (which
    is two consecutive segments from the same document), and as negative examples,
    the same two consecutive segments but with their order swapped. The model is then
    forced to learn finer-grained distinctions between coherence properties at the
    discourse level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the original BERT and Albert configuration with the `Transformers`
    library. The following piece of code shows how to configure a BERT-Base initial
    model. As you see in the output, the number of parameters is around 110 M:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the following piece of code shows how to define the Albert model with two
    classes, `AlbertConfig` and `AlbertModel`, from the `Transformers` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Due to that, the default Albert configuration points to Albert-xxlarge. We
    need to set the hidden size, the number of attention heads, and the intermediate
    size to fit Albert-base. And the code shows the Albert-base mode as 11M, 10 times
    smaller than the BERT-base model. The original paper on ALBERT reported benchmarking
    as in the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Albert model benchmarking ](img/B17123_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.14 – Albert model benchmarking
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From this point on, in order to train an Albert language model from scratch,
    we need to go through similar phases to those we already illustrated in BERT training
    in the previous sections by using the uniform Transformers API. There''s no need
    to explain the same steps here! Instead, let''s load an already trained Albert
    language model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding pieces of code download the Albert model weights and its configuration
    from the HuggingFace hub or from our local cache directory if already cached,
    which means you''ve already called the `AlbertTokenizer.from_pretrained()` function
    before. Since that the model object is a pre-trained language model, the things
    we can do with this model are limited for now. We need to train it on a downstream
    task to able to use it for inference, which will be the main subject of further
    chapters. Instead, we can take advantage of its masked language model objective
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15 – The fill-mask output results for albert-base-v2 ](img/B17123_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – The fill-mask output results for albert-base-v2
  prefs: []
  type: TYPE_NORMAL
- en: The `fill-mask` pipeline computes the scores for each vocabulary token with
    the `SoftMax()` function and sorts the most probable tokens where `cute` is the
    winner with a probability score of 0.281\. You may notice that entries in the
    *token_str* column start with the `_` character, which is due to the metaspace
    component of the tokenizer of Albert.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the next alternative, *RoBERTa*, which mostly focuses on
    the pre-training phase.
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Robustly Optimized BERT pre-training Approach** (**RoBERTa**) is another
    popular BERT reimplementation. It has provided many more improvements in training
    strategy than architectural design. It outperformed BERT in almost all individual
    tasks on GLUE. Dynamic masking is one of its original design choices. Although
    static masking is better for some tasks, the RoBERTa team showed that dynamic
    masking can perform well for overall performances. Let''s compare the changes
    from BERT and summarize all the features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes in architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing the next sentence prediction training objective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically changing the masking patterns instead of static masking, which is
    done by generating masking patterns whenever they feed a sequence to the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BPE** sub-word tokenizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The changes in training are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Controlling the training data: More data is used, such as 160 GB instead of
    the 16 GB originally used in BERT. Not only the size of the data but the quality
    and diversity were taken into consideration in the study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longer iterations of up to 500K pretraining steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A longer batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longer sequences, which leads to less padding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large 50K BPE vocabulary instead of a 30K BPE vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thanks to the Transformers uniform API, as in the Albert model pipeline above,
    we initialize the RoBERTa model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to load the pre-trained model, we execute the following pieces of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines illustrate how the model processes a given text. The output representation
    at the last layer is not useful at the moment. As we''ve mentioned several times,
    we need to fine-tune the main language models. The following execution applies
    the `fill-mask` function using the `roberta-base` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – The fill-mask task results for roberta-base ](img/B17123_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – The fill-mask task results for roberta-base
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the previous ALBERT `fill-mask` model, this pipeline ranks the suitable
    candidate words. Please ignore the prefix `Ġ` in the tokens – that is an encoded
    space character produced by the byte-level BPE tokenizer, which we will discuss
    later. You should have noticed that we used the `[MASK]` and `<mask>` tokens in
    ALBERT and RoBERTa pipeline in order to hold place for masked token. This is because
    of the configuration of `tokenizer`. To learn which token expression will be used,
    you can check `tokenizer.mask_token`. Please see the following execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure proper mask token use, we can add the `fillmask.tokenizer.mask_token`
    expression in the pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ELECTRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **ELECTRA** model (*proposed by Kevin Clark et al. in 2020*) focuses on
    a new masked language model utilizing the replaced token detection training objective.
    During pre-training, the model is forced to learn to distinguish real input tokens
    from synthetically generated replacements where the synthetic negative example
    is sampled from plausible tokens rather than randomly sampled tokens. The Albert
    model criticized the NSP objective of BERT for being a topic detection problem
    and using low-quality negative examples. ELECTRA trains two neural networks, a
    generator and a discriminator, so that the former produces high-quality negative
    examples, whereas the latter distinguishes the original token from the replaced
    token. We know GAN networks from the field of computer vision, in which the generator
    *G* produces fake images and tries to fool the discriminator *D*, and the discriminator
    network tries to avoid being fooled. The ELECTRA model applies almost the same
    generator-discriminator approach to replace original tokens with high-quality
    negative examples that are plausible replacements but synthetically generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order not to repeat the same code with other examples, we only provide a
    simple `fill-mask` example for the Electra generator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the entire list of models at the following link: [https://huggingface.co/Transformers/model_summary.html](https://huggingface.co/transformers/model_summary.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The model checkpoints can be found at [https://huggingface.co/models](https://huggingface.co/models).
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We've finally completed the autoencoding model part. Now we'll move
    on to tokenization algorithms, which have an important effect on the success of
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Working with tokenization algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the opening part of the chapter, we trained the BERT model using a specific
    tokenizer, namely `BertWordPieceTokenizer`. Now it is worth discussing the tokenization
    process in detail here. Tokenization is a way of splitting textual input into
    tokens and assigning an identifier to each token before feeding the neural network
    architecture. The most intuitive way is to split the sequence into smaller chunks
    in terms of space. However, such approaches do not meet the requirement of some
    languages, such as Japanese, and also may lead to huge vocabulary problems. Almost
    all Transformer models leverage subword tokenization not only for reducing dimensionality
    but also for encoding rare (or unknown) words not seen in training. The tokenization
    relies on the idea that every word, including rare words or unknown words, can
    be decomposed into meaningful smaller chunks that are widely seen symbols in the
    training corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Some traditional tokenizers developed within Moses and the `nltk` library apply
    advanced rule-based techniques. But the tokenization algorithms that are used
    with Transformers are based on self-supervised learning and extract the rules
    from the corpus. Simple intuitive solutions for rule-based tokenization are based
    on using characters, punctuation, or whitespace. Character-based tokenization
    causes language models to lose the input meaning. Even though it can reduce the
    vocabulary size, which is good, it makes it hard for the model to capture the
    meaning of `cat` by means of the encodings of the characters `c`, `a`, and `t`.
    Moreover, the dimension of the input sequence becomes very large. Likewise, punctuation-based
    models cannot treat some expressions, such as *haven't* or *ain't*, properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, several advanced subword tokenization algorithms, such as BPE, have
    become an integral part of Transformer architectures. These modern tokenization
    procedures consist of two phases: The pre-tokenization phase simply splits the
    input into tokens either using space as or language-dependent rules. Second, the
    tokenization training phase is to train the tokenizer and build a base vocabulary
    of a reasonable size based on tokens. Before training our own tokenizers, let''s
    load a pre-trained tokenizer. The following code loads a Turkish tokenizer, which
    is of type `BertTokenizerFast`, from the `Transformers` library with a vocabulary
    size of 32K:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code loads an English BERT tokenizer for the `bert-base-uncased`
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how they work! We tokenize the word `telecommunication` with these
    two tokenizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The `word_en` token is already in the vocabulary of the English tokenizer but
    not in that of the Turkish one. So let''s see what happens with the Turkish tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the Turkish tokenizer model has no such a word in its vocabulary, it
    needs to break the word into parts that make sense to it. All these split tokens
    are already stored in the model vocabulary. Please notice the output of the following
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s tokenize the same word with the English tokenizer that we already loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the English model has the word `telecommunication` in the base vocabulary,
    it does not need to break it into parts but rather takes it as a whole. By learning
    from the corpus, the tokenizers are capable of transforming a word into mostly
    grammatically logical subcomponents. Let''s take a difficult example from Turkish.
    As an agglutinative language, Turkish allows us to add many suffixes to a word
    stem to construct very long words. Here is one of the longest words in the Turkish
    language used in a text ([https://en.wikipedia.org/wiki/Longest_word_in_Turkish](https://en.wikipedia.org/wiki/Longest_word_in_Turkish)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It means that *As though you happen to have been from among those whom we will
    not be able to easily/quickly make a maker of unsuccessful ones*. The Turkish
    BERT tokenizer may not have seen this word in training, but it has seen its pieces;
    *muvaffak (succesful) as the stem, ##iyet(successfulness), ##siz (unsuccessfulness),
    ##leş (become unsuccessful)*, and so forth. The Turkish tokenizer extracts components
    that seem to be grammatically logical for the Turkish language when comparing
    the results with a Wikipedia article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The Turkish tokenizer is an example of the WordPiece algorithm since it works
    with a BERT model. Almost all language models including BERT, DistilBERT, and
    ELECTRA require a WordPiece tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to take a look at the tokenization approaches used with Transformers.
    First, we'll discuss the widely used tokenizations of BPE, WordPiece, and SentencePiece
    a bit and then train them with HuggingFace's fast `tokenizers` library.
  prefs: []
  type: TYPE_NORMAL
- en: Byte pair encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**BPE** is a data compression technique. It scans the data sequence and iteratively
    replaces the most frequent pair of bytes with a single symbol. It was first adapted
    and proposed in *Neural Machine Translation of Rare Words with Subword Units,
    Sennrich et al. 2015,* to solve the problem of unknown words and rare words for
    machine translation. Currently, it is successfully being used within GPT-2 and
    many other state-of-the-art models. Many modern tokenization algorithms are based
    on such compression techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: It represents text as a sequence of character n-grams, which are also called
    character-level subwords. The training starts initially with a vocabulary of all
    Unicode characters (or symbols) seen in the corpus. This can be small for English
    but can be large for character-rich languages such as Japanese. Then, it iteratively
    computes character bigrams and replaces the most frequent ones with special new
    symbols. For example, *t* and *h* are frequently occurring symbols. We replace
    consecutive symbols with the *th* symbol. This process is kept iteratively running
    until the vocabulary has attained the desired vocabulary size. The most common
    vocabulary size is around 30K.
  prefs: []
  type: TYPE_NORMAL
- en: BPE is particularly effective at representing unknown words. However, it may
    not guarantee the handling of rare words and/or words including rare subwords.
    In such cases, it associates rare characters with a special symbol, *<UNK>*, which
    may lead to losing meaning in words a bit. As a potential solution, **Byte-Level
    BPE** (**BBPE**) has been proposed, which uses a 256-byte set of vocabulary instead
    of Unicode characters to ensure that every base character is included in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**WordPiece** is another popular word segmentation algorithm widely used with
    BERT, DistilBERT, and Electra. It was proposed by Schuster and Nakajima to solve
    the Japanese and Korean voice problem in 2012\. The motivation behind this work
    was that, although not a big issue for the English language, word segmentation
    is important preprocessing for many Asian languages, because in these languages
    spaces are rarely used. Therefore, we come across word segmentation approaches
    in NLP studies in Asian languages more often. Similar to BPE, WordPiece uses a
    large corpus to learn vocabulary and merging rules. While BPE and BBPE learn the
    merging rules based on co-occurrence statistics, the WordPiece algorithm uses
    maximum likelihood estimation to extract the merging rules from a corpus. It first
    initializes the vocabulary with Unicode characters, which are also called vocabulary
    symbols. It treats each word in the training corpus as a list of symbols (initially
    Unicode characters), and then it iteratively produces a new symbol merging two
    symbols out of all the possible candidate symbol pairs based on the likelihood
    maximization rather than frequency. This production pipeline continues until the
    desired vocabulary size is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence piece tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previous tokenization algorithms treat text as a space-separated word list.
    This space-based splitting does not work in some languages. In the German language,
    compound nouns are written without spaces, for example, menschenrechte (human
    rights). The solution is to use language-specific pre-tokenizers. In German, an
    NLP pipeline leverages a compound-splitter module to check whether a word can
    be subdivided into smaller words. However, East Asian languages (for example,
    Chinese, Japanese, Korean, and Thai) do not use spaces between words. The `_`
    character, which is also why we saw `_` in the output of the Albert model example
    earlier. Other popular language models that use SentencePiece are XLNet, Marian,
    and T5\.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed subword tokenization approaches. It is time to start
    conducting experiments for training with the `tokenizers` library.
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizers library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that the already-trained tokenizers for Turkish and English
    are part of the `Transformers` library in the previous code examples. On the other
    hand, the HuggingFace team provided the `tokenizers` library independently from
    the `Transformers` library to be fast and give us more freedom. The library was
    originally written in Rust, which makes multi-core parallel computations possible
    and is wrapped with Python ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `tokenizers` library, we use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tokenizers` library provides several components so that we can build an
    end-to-end tokenizer from preprocessing the raw text to decoding tokenized unit
    IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Normalizer→ PreTokenizer → Modeling → Post-Processor → Decoding*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the tokenization pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Tokenization pipeline ](img/B17123_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Tokenization pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalizer** allows us to apply primitive text processing such as lowercasing,
    stripping, Unicode normalization, and removing accents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PreTokenizer** prepares the corpus for the next training phase. It splits
    the input into tokens depending on the rules, such as whitespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Training** is a subword tokenization algorithm such as *BPE*, *BBPE*,
    and *WordPiece*, which we''ve discussed already. It discovers subwords/vocabulary
    and learns generation rules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing** provides advanced class construction that is compatible
    with Transformers models such as BertProcessors. We mostly add special tokens
    such as *[CLS]* and *[SEP]* to the tokenized input just before feeding the architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder** is in charge of converting token IDs back to the original string.
    It is just for inspecting what is going on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training BPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s train a BPE tokenizer using Shakespeare''s plays:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is loaded as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will need a post-processor (`TemplateProcessing`) for all the tokenization
    algorithms ahead. We need to customize the post-processor to make the input convenient
    for a particular language model. For example, the following template will be suitable
    for the BERT model since it needs the *[CLS]* token at the beginning of the input
    and *[SEP]* tokens both at the end and in the middle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We define the template as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import the necessary components to build an end-to-end tokenization pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We start by instantiating **BPE** as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preprocessing part has two components: *normalizer* and *pre-tokenizer*.
    We may have more than one normalizer. So, we compose a `Sequence` of normalizer
    components that includes multiple normalizers where `NFD()` is a Unicode normalizer
    and `StripAccents()` removes accents. For pre-tokenization, `Whitespace()` gently
    breaks the text based on space. Since the decoder component must be compatible
    with the model, `BPEDecoder` is selected for the `BPE` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Well! We are ready to train the tokenizer on the data. The following execution
    instantiates `BpeTrainer()`, which helps us to organize the entire training process
    by setting hyperparameters. We set the vocabulary size parameter to 5K since our
    Shakespeare corpus is relatively small. For a large-scale project, we use a bigger
    corpus and normally set the vocabulary size to around 30K:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have completed the training!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Training from the filesystem: To start the training process, we passed an in-memory
    Shakespeare object as a list of strings to `tokenizer.train_from_iterator()`.
    For a large-scale project with a large corpus, we need to design a Python generator
    that yields string lines mostly by consuming the files from the filesystem rather
    than in-memory storage. You should also check `tokenizer.train()` to train from
    the filesystem storage as applied in the BERT training section above.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s grab a random sentence from the play Macbeth, name it `sen`, and tokenize
    it with our fresh tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thanks to the post-processor function above, we see additional *[CLS]* and
    *[SEP]* tokens in the proper position. There is only one split word, *handle*
    (*hand*, *le*), since we passed to the model a sentence from the play Macbeth
    that the model already knew. Besides, we used a small corpus, and the tokenizer
    is not forced to use compression. Let''s pass a challenging phrase, `Hugging Face`,
    that the tokenizer might not know:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The term `Hugging` is lowercased and split into three pieces `hu`, `gg`, `ing`,
    since the model vocabulary contains all other tokens but `Hugging`. Let''s pass
    two sentences now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the post-processor injected the `[SEP]` token as an indicator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is time to save the model. We can either save the sub-word tokenization
    model or the entire tokenization pipeline. First, let''s save the BPE model only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model saved two files regarding vocabulary and merging rules. The `merge.txt`
    file is composed of 4,948 merging rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The top five rules ranked are as shown in the following where we see that (`t`,
    `h`) is the first ranked rule due to that being the most frequent pair. For testing,
    the model scans the textual input and tries to merge these two symbols first if
    applicable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The BPE algorithm ranks the rules based on frequency. When you manually calculate
    character bigrams in the Shakespeare corpus, you will find (`t`, `h`) the most
    frequent pair.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now save and load the entire tokenization pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We successfully reloaded the tokenizer!
  prefs: []
  type: TYPE_NORMAL
- en: Training the WordPiece model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will train the WordPiece model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following lines instantiate an empty WordPiece tokenizer and prepare it
    for training. `BertNormalizer` is a pre-defined normalizer sequence that includes
    the processes of cleaning the text, transforming accents, handling Chinese characters,
    and lowercasing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we instantiate a proper trainer, `WordPieceTrainer()` for `WordPiece()`,
    to organize the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s use `WordPieceDecoder()` to treat the sentences properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have not come across any `[UNK]` tokens in the output since the tokenizer
    somehow knows or splits the input for encoding. Let''s force the model to produce
    `[UNK]`tokens as in the following code. Let''s pass a Turkish sentence to our
    tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Well done! We have a couple of unknown tokens since the tokenizer does not find
    a way to decompose the given word from the merging rules and the base vocabulary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So far, we have designed our tokenization pipeline all the way from the normalizer
    component to the decoder component. On the other hand, the `tokenizers` library
    provides us with an already made (not trained) empty tokenization pipeline with
    proper components to build quick prototypes for production. Here are some pre-made
    tokenizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CharBPETokenizer`: The original BPE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ByteLevelBPETokenizer`: The byte-level version of the BPE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SentencePieceBPETokenizer`: A BPE implementation compatible with the one used
    by *SentencePiece*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BertWordPieceTokenizer`: The famous BERT tokenizer, using WordPiece'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code imports these pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: All these pipelines are already designed for us. The rest of the process (such
    as training, saving the model, and using the tokenizer) is the same as our previous
    BPE and WordPiece training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We have made great progress and trained our first Transformer model
    as well as its tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have experienced autoencoding models both theoretically
    and practically. Starting with basic knowledge about BERT, we trained it as well
    as a corresponding tokenizer from scratch. We also discussed how to work inside
    other frameworks, such as Keras. Besides BERT, we also reviewed other autoencoding
    models. To avoid excessive code repetition, we did not provide the full implementation
    for training other models. During the BERT training, we trained the WordPiece
    tokenization algorithm. In the last part, we examined other tokenization algorithms
    since it is worth discussing and understanding all of them.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoding models use the left decoder side of the original Transformer and
    are mostly fine-tuned for classification problems. In the next chapter, we will
    discuss and learn about the right decoder part of Transformers to implement language
    generation models.
  prefs: []
  type: TYPE_NORMAL
