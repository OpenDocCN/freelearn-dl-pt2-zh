["```py\n>>> import numpy as np\n>>> def conv1d(x, w, p=0, s=1):\n...     w_rot = np.array(w[::-1])\n...     x_padded = np.array(x)\n...     if p > 0:\n...         zero_pad = np.zeros(shape=p)\n...         x_padded = np.concatenate([\n...             zero_pad, x_padded, zero_pad\n...         ])\n...     res = []\n...     for i in range(0, int((len(x_padded) - len(w_rot))) + 1, s):\n...         res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n...     return np.array(res)\n>>> ## Testing:\n>>> x = [1, 3, 2, 4, 5, 6, 1, 3]\n>>> w = [1, 0, 3, 1, 2]\n>>> print('Conv1d Implementation:',\n...       conv1d(x, w, p=2, s=1))\nConv1d Implementation: [ 5\\. 14\\. 16\\. 26\\. 24\\. 34\\. 19\\. 22.]\n>>> print('NumPy Results:',\n...       np.convolve(x, w, mode='same'))\nNumPy Results: [ 5 14 16 26 24 34 19 22] \n```", "```py\n>>> import numpy as np\n>>> import scipy.signal\n>>> def conv2d(X, W, p=(0, 0), s=(1, 1)):\n...     W_rot = np.array(W)[::-1,::-1]\n...     X_orig = np.array(X)\n...     n1 = X_orig.shape[0] + 2*p[0]\n...     n2 = X_orig.shape[1] + 2*p[1]\n...     X_padded = np.zeros(shape=(n1, n2))\n...     X_padded[p[0]:p[0]+X_orig.shape[0],\n...              p[1]:p[1]+X_orig.shape[1]] = X_orig\n...\n...     res = []\n...     for i in range(0,\n...             int((X_padded.shape[0] - \\\n...             W_rot.shape[0])/s[0])+1, s[0]):\n...         res.append([])\n...         for j in range(0,\n...                 int((X_padded.shape[1] - \\\n...                 W_rot.shape[1])/s[1])+1, s[1]):\n...             X_sub = X_padded[i:i+W_rot.shape[0],\n...                              j:j+W_rot.shape[1]]\n...             res[-1].append(np.sum(X_sub * W_rot))\n...     return(np.array(res))\n>>> X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n>>> W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n>>> print('Conv2d Implementation:\\n',\n...       conv2d(X, W, p=(1, 1), s=(1, 1)))\nConv2d Implementation:\n[[ 11\\.  25\\.  32\\.  13.]\n [ 19\\.  25\\.  24\\.  13.]\n [ 13\\.  28\\.  25\\.  17.]\n [ 11\\.  17\\.  14\\.   9.]]\n>>> print('SciPy Results:\\n',\n...       scipy.signal.convolve2d(X, W, mode='same'))\nSciPy Results:\n[[11 25 32 13]\n [19 25 24 13]\n [13 28 25 17]\n [11 17 14  9]] \n```", "```py\n>>> import torch\n>>> from torchvision.io import read_image\n>>> img = read_image('example-image.png')\n>>> print('Image shape:', img.shape)\nImage shape: torch.Size([3, 252, 221])\n>>> print('Number of channels:', img.shape[0])\nNumber of channels: 3\n>>> print('Image data type:', img.dtype)\nImage data type: torch.uint8\n>>> print(img[:, 100:102, 100:102])\ntensor([[[179, 182],\n         [180, 182]],\n        [[134, 136],\n         [135, 137]],\n        [[110, 112],\n         [111, 113]]], dtype=torch.uint8) \n```", "```py\n>>> import torch.nn as nn\n>>> loss_func = nn.BCELoss()\n>>> loss = loss_func(torch.tensor([0.9]), torch.tensor([1.0]))\n>>> l2_lambda = 0.001\n>>> conv_layer = nn.Conv2d(in_channels=3,\n...                        out_channels=5,\n...                        kernel_size=5)\n>>> l2_penalty = l2_lambda * sum(\n...     [(p**2).sum() for p in conv_layer.parameters()]\n... )\n>>> loss_with_penalty = loss + l2_penalty\n>>> linear_layer = nn.Linear(10, 16)\n>>> l2_penalty = l2_lambda * sum(\n...     [(p**2).sum() for p in linear_layer.parameters()]\n... )\n>>> loss_with_penalty = loss + l2_penalty \n```", "```py\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    weight_decay=l2_lambda,\n    ...\n) \n```", "```py\n>>> ####### Binary Cross-entropy\n>>> logits = torch.tensor([0.8])\n>>> probas = torch.sigmoid(logits)\n>>> target = torch.tensor([1.0])\n>>> bce_loss_fn = nn.BCELoss()\n>>> bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n>>> print(f'BCE (w Probas): {bce_loss_fn(probas, target):.4f}')\nBCE (w Probas): 0.3711\n>>> print(f'BCE (w Logits): '\n...       f'{bce_logits_loss_fn(logits, target):.4f}')\nBCE (w Logits): 0.3711\n>>> ####### Categorical Cross-entropy\n>>> logits = torch.tensor([[1.5, 0.8, 2.1]])\n>>> probas = torch.softmax(logits, dim=1)\n>>> target = torch.tensor([2])\n>>> cce_loss_fn = nn.NLLLoss()\n>>> cce_logits_loss_fn = nn.CrossEntropyLoss()\n>>> print(f'CCE (w Probas): '\n...       f'{cce_logits_loss_fn(logits, target):.4f}')\nCCE (w Probas): 0.5996\n>>> print(f'CCE (w Logits): '\n...       f'{cce_loss_fn(torch.log(probas), target):.4f}')\nCCE (w Logits): 0.5996 \n```", "```py\n>>> import torchvision\n>>> from torchvision import transforms\n>>> image_path = './'\n>>> transform = transforms.Compose([\n...     transforms.ToTensor()\n... ])\n>>> mnist_dataset = torchvision.datasets.MNIST(\n...     root=image_path, train=True,\n...     transform=transform, download=True\n... )\n>>> from torch.utils.data import Subset\n>>> mnist_valid_dataset = Subset(mnist_dataset,\n...                              torch.arange(10000))\n>>> mnist_train_dataset = Subset(mnist_dataset,\n...                              torch.arange(\n...                                  10000, len(mnist_dataset)\n...                              ))\n>>> mnist_test_dataset = torchvision.datasets.MNIST(\n...     root=image_path, train=False,\n...     transform=transform, download=False\n... ) \n```", "```py\n>>> from torch.utils.data import DataLoader\n>>> batch_size = 64\n>>> torch.manual_seed(1)\n>>> train_dl = DataLoader(mnist_train_dataset,\n...                       batch_size,\n...                       shuffle=True)\n>>> valid_dl = DataLoader(mnist_valid_dataset,\n...                       batch_size,\n...                       shuffle=False) \n```", "```py\n>>> model = nn.Sequential()\n>>> model.add_module(\n...     'conv1',\n...     nn.Conv2d(\n...         in_channels=1, out_channels=32,\n...         kernel_size=5, padding=2\n...     )\n... )\n>>> model.add_module('relu1', nn.ReLU())\n>>> model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n>>> model.add_module(\n...     'conv2',\n...     nn.Conv2d(\n...         in_channels=32, out_channels=64,\n...         kernel_size=5, padding=2\n...     )\n... )\n>>> model.add_module('relu2', nn.ReLU())\n>>> model.add_module('pool2', nn.MaxPool2d(kernel_size=2)) \n```", "```py\n>>> x = torch.ones((4, 1, 28, 28))\n>>> model(x).shape\ntorch.Size([4, 64, 7, 7]) \n```", "```py\n>>> model.add_module('flatten', nn.Flatten())\n>>> x = torch.ones((4, 1, 28, 28))\n>>> model(x).shape\ntorch.Size([4, 3136]) \n```", "```py\n>>> model.add_module('fc1', nn.Linear(3136, 1024))\n>>> model.add_module('relu3', nn.ReLU())\n>>> model.add_module('dropout', nn.Dropout(p=0.5))\n>>> model.add_module('fc2', nn.Linear(1024, 10)) \n```", "```py\n>>> loss_fn = nn.CrossEntropyLoss()\n>>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n```", "```py\n>>> def train(model, num_epochs, train_dl, valid_dl):\n...     loss_hist_train = [0] * num_epochs\n...     accuracy_hist_train = [0] * num_epochs\n...     loss_hist_valid = [0] * num_epochs\n...     accuracy_hist_valid = [0] * num_epochs\n...     for epoch in range(num_epochs):\n...         model.train()\n...         for x_batch, y_batch in train_dl:\n...             pred = model(x_batch)\n...             loss = loss_fn(pred, y_batch)\n...             loss.backward()\n...             optimizer.step()\n...             optimizer.zero_grad()\n...             loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n...             is_correct = (\n...                 torch.argmax(pred, dim=1) == y_batch\n...             ).float()\n...             accuracy_hist_train[epoch] += is_correct.sum()\n...         loss_hist_train[epoch] /= len(train_dl.dataset)\n...         accuracy_hist_train[epoch] /= len(train_dl.dataset)\n...\n...         model.eval()\n...         with torch.no_grad():\n...             for x_batch, y_batch in valid_dl:\n...                 pred = model(x_batch)\n...                 loss = loss_fn(pred, y_batch)\n...                 loss_hist_valid[epoch] += \\\n...                     loss.item()*y_batch.size(0)\n...                 is_correct = (\n...                     torch.argmax(pred, dim=1) == y_batch\n...                 ).float()\n...                 accuracy_hist_valid[epoch] += is_correct.sum()\n...         loss_hist_valid[epoch] /= len(valid_dl.dataset)\n...         accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n...\n...         print(f'Epoch {epoch+1} accuracy: '\n...               f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n...               f'{accuracy_hist_valid[epoch]:.4f}')\n...     return loss_hist_train, loss_hist_valid, \\\n...            accuracy_hist_train, accuracy_hist_valid \n```", "```py\n>>> torch.manual_seed(1)\n>>> num_epochs = 20\n>>> hist = train(model, num_epochs, train_dl, valid_dl)\nEpoch 1 accuracy: 0.9503 val_accuracy: 0.9802\n...\nEpoch 9 accuracy: 0.9968 val_accuracy: 0.9892\n...\nEpoch 20 accuracy: 0.9979 val_accuracy: 0.9907 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> x_arr = np.arange(len(hist[0])) + 1\n>>> fig = plt.figure(figsize=(12, 4))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> ax.plot(x_arr, hist[0], '-o', label='Train loss')\n>>> ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n>>> ax.legend(fontsize=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n>>> ax.plot(x_arr, hist[3], '--<',\n...         label='Validation acc.')\n>>> ax.legend(fontsize=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.set_ylabel('Accuracy', size=15)\n>>> plt.show() \n```", "```py\n>>> pred = model(mnist_test_dataset.data.unsqueeze(1) / 255.)\n>>> is_correct = (\n...     torch.argmax(pred, dim=1) == mnist_test_dataset.targets\n... ).float()\n>>> print(f'Test accuracy: {is_correct.mean():.4f}')\nTest accuracy: 0.9914 \n```", "```py\n>>> fig = plt.figure(figsize=(12, 4))\n>>> for i in range(12):\n...     ax = fig.add_subplot(2, 6, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     img = mnist_test_dataset[i][0][0, :, :]\n...     pred = model(img.unsqueeze(0).unsqueeze(1))\n...     y_pred = torch.argmax(pred)\n...     ax.imshow(img, cmap='gray_r')\n...     ax.text(0.9, 0.1, y_pred.item(),\n...             size=15, color='blue',\n...             horizontalalignment='center',\n...             verticalalignment='center',\n...             transform=ax.transAxes)\n>>> plt.show() \n```", "```py\n>>> image_path = './'\n>>> celeba_train_dataset = torchvision.datasets.CelebA(\n...     image_path, split='train',\n...     target_type='attr', download=True\n... )\n>>> celeba_valid_dataset = torchvision.datasets.CelebA(\n...     image_path, split='valid',\n...     target_type='attr', download=True\n... )\n>>> celeba_test_dataset = torchvision.datasets.CelebA(\n...     image_path, split='test',\n...     target_type='attr', download=True\n... )\n>>>\n>>> print('Train set:', len(celeba_train_dataset))\nTrain set:  162770\n>>> print('Validation set:', len(celeba_valid_dataset))\nValidation: 19867\n>>> print('Test set:', len(celeba_test_dataset))\nTest set:   19962 \n```", "```py\n>>> fig = plt.figure(figsize=(16, 8.5))\n>>> ## Column 1: cropping to a bounding-box\n>>> ax = fig.add_subplot(2, 5, 1)\n>>> img, attr = celeba_train_dataset[0]\n>>> ax.set_title('Crop to a \\nbounding-box', size=15)\n>>> ax.imshow(img)\n>>> ax = fig.add_subplot(2, 5, 6)\n>>> img_cropped = transforms.functional.crop(img, 50, 20, 128, 128)\n>>> ax.imshow(img_cropped)\n>>> \n>>> ## Column 2: flipping (horizontally)\n>>> ax = fig.add_subplot(2, 5, 2)\n>>> img, attr = celeba_train_dataset[1]\n>>> ax.set_title('Flip (horizontal)', size=15)\n>>> ax.imshow(img)\n>>> ax = fig.add_subplot(2, 5, 7)\n>>> img_flipped = transforms.functional.hflip(img)\n>>> ax.imshow(img_flipped)\n>>> \n>>> ## Column 3: adjust contrast\n>>> ax = fig.add_subplot(2, 5, 3)\n>>> img, attr = celeba_train_dataset[2]\n>>> ax.set_title('Adjust constrast', size=15)\n>>> ax.imshow(img)\n>>> ax = fig.add_subplot(2, 5, 8)\n>>> img_adj_contrast = transforms.functional.adjust_contrast(\n...     img, contrast_factor=2\n... )\n>>> ax.imshow(img_adj_contrast)\n>>> \n>>> ## Column 4: adjust brightness\n>>> ax = fig.add_subplot(2, 5, 4)\n>>> img, attr = celeba_train_dataset[3]\n>>> ax.set_title('Adjust brightness', size=15)\n>>> ax.imshow(img)\n>>> ax = fig.add_subplot(2, 5, 9)\n>>> img_adj_brightness = transforms.functional.adjust_brightness(\n...     img, brightness_factor=1.3\n... )\n>>> ax.imshow(img_adj_brightness)\n>>> \n>>> ## Column 5: cropping from image center\n>>> ax = fig.add_subplot(2, 5, 5)\n>>> img, attr = celeba_train_dataset[4]\n>>> ax.set_title('Center crop\\nand resize', size=15)\n>>> ax.imshow(img)\n>>> ax = fig.add_subplot(2, 5, 10)\n>>> img_center_crop = transforms.functional.center_crop(\n...     img, [0.7*218, 0.7*178]\n... )\n>>> img_resized = transforms.functional.resize(\n...     img_center_crop, size=(218, 178)\n... )\n>>> ax.imshow(img_resized)\n>>> plt.show() \n```", "```py\n>>> torch.manual_seed(1)\n>>> fig = plt.figure(figsize=(14, 12))\n>>> for i, (img, attr) in enumerate(celeba_train_dataset):\n...     ax = fig.add_subplot(3, 4, i*4+1)\n...     ax.imshow(img)\n...     if i == 0:\n...         ax.set_title('Orig.', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+2)\n...     img_transform = transforms.Compose([\n...         transforms.RandomCrop([178, 178])\n...     ])\n...     img_cropped = img_transform(img)\n...     ax.imshow(img_cropped)\n...     if i == 0:\n...         ax.set_title('Step 1: Random crop', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+3)\n...     img_transform = transforms.Compose([\n...         transforms.RandomHorizontalFlip()\n...     ])\n...     img_flip = img_transform(img_cropped)\n...     ax.imshow(img_flip)\n...     if i == 0:\n...         ax.set_title('Step 2: Random flip', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+4)\n...     img_resized = transforms.functional.resize(\n...         img_flip, size=(128, 128)\n...     )\n...     ax.imshow(img_resized)\n...     if i == 0:\n...         ax.set_title('Step 3: Resize', size=15)\n...     if i == 2:\n...         break\n>>> plt.show() \n```", "```py\n>>> get_smile = lambda attr: attr[18] \n```", "```py\n>>> transform_train = transforms.Compose([\n...     transforms.RandomCrop([178, 178]),\n...     transforms.RandomHorizontalFlip(),\n...     transforms.Resize([64, 64]),\n...     transforms.ToTensor(),\n... ]) \n```", "```py\n>>> transform = transforms.Compose([\n...     transforms.CenterCrop([178, 178]),\n...     transforms.Resize([64, 64]),\n...     transforms.ToTensor(),\n... ]) \n```", "```py\n>>> from torch.utils.data import DataLoader\n>>> celeba_train_dataset = torchvision.datasets.CelebA(\n...     image_path, split='train',\n...     target_type='attr', download=False,\n...     transform=transform_train, target_transform=get_smile\n... )\n>>> torch.manual_seed(1)\n>>> data_loader = DataLoader(celeba_train_dataset, batch_size=2)\n>>> fig = plt.figure(figsize=(15, 6))\n>>> num_epochs = 5\n>>> for j in range(num_epochs):\n...     img_batch, label_batch = next(iter(data_loader))\n...     img = img_batch[0]\n...     ax = fig.add_subplot(2, 5, j + 1)\n...     ax.set_xticks([])\n...     ax.set_yticks([])\n...     ax.set_title(f'Epoch {j}:', size=15)\n...     ax.imshow(img.permute(1, 2, 0))\n...\n...     img = img_batch[1]\n...     ax = fig.add_subplot(2, 5, j + 6)\n...     ax.set_xticks([])\n...     ax.set_yticks([])\n...     ax.imshow(img.permute(1, 2, 0))\n>>> plt.show() \n```", "```py\n>>> celeba_valid_dataset = torchvision.datasets.CelebA(\n...     image_path, split='valid',\n...     target_type='attr', download=False,\n...     transform=transform, target_transform=get_smile\n... )\n>>> celeba_test_dataset = torchvision.datasets.CelebA(\n...     image_path, split='test',\n...     target_type='attr', download=False,\n...     transform=transform, target_transform=get_smile\n... ) \n```", "```py\n>>> from torch.utils.data import Subset\n>>> celeba_train_dataset = Subset(celeba_train_dataset,\n...                               torch.arange(16000))\n>>> celeba_valid_dataset = Subset(celeba_valid_dataset,\n...                               torch.arange(1000))\n>>> print('Train set:', len(celeba_train_dataset))\nTrain set: 16000\n>>> print('Validation set:', len(celeba_valid_dataset))\nValidation set: 1000 \n```", "```py\n>>> batch_size = 32\n>>> torch.manual_seed(1)\n>>> train_dl = DataLoader(celeba_train_dataset,\n...                       batch_size, shuffle=True)\n>>> valid_dl = DataLoader(celeba_valid_dataset,\n...                       batch_size, shuffle=False)\n>>> test_dl = DataLoader(celeba_test_dataset,\n...                      batch_size, shuffle=False) \n```", "```py\n>>> model = nn.Sequential()\n>>> model.add_module(\n...     'conv1',\n...     nn.Conv2d(\n...         in_channels=3, out_channels=32,\n...         kernel_size=3, padding=1\n...     )\n... )\n>>> model.add_module('relu1', nn.ReLU())\n>>> model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n>>> model.add_module('dropout1', nn.Dropout(p=0.5))\n>>> \n>>> model.add_module(\n...     'conv2',\n...     nn.Conv2d(\n...         in_channels=32, out_channels=64,\n...         kernel_size=3, padding=1\n...     )\n... )\n>>> model.add_module('relu2', nn.ReLU())\n>>> model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n>>> model.add_module('dropout2', nn.Dropout(p=0.5))\n>>> \n>>> model.add_module(\n...     'conv3',\n...     nn.Conv2d(\n...         in_channels=64, out_channels=128,\n...         kernel_size=3, padding=1\n...     )\n... )\n>>> model.add_module('relu3', nn.ReLU())\n>>> model.add_module('pool3', nn.MaxPool2d(kernel_size=2))\n>>> \n>>> model.add_module(\n...     'conv4',\n...     nn.Conv2d(\n...         in_channels=128, out_channels=256,\n...         kernel_size=3, padding=1\n...     )\n... )\n>>> model.add_module('relu4', nn.ReLU()) \n```", "```py\n>>> x = torch.ones((4, 3, 64, 64))\n>>> model(x).shape\ntorch.Size([4, 256, 8, 8]) \n```", "```py\n>>> model.add_module('pool4', nn.AvgPool2d(kernel_size=8))\n>>> model.add_module('flatten', nn.Flatten())\n>>> x = torch.ones((4, 3, 64, 64))\n>>> model(x).shape\ntorch.Size([4, 256]) \n```", "```py\n>>> model.add_module('fc', nn.Linear(256, 1))\n>>> model.add_module('sigmoid', nn.Sigmoid())\n>>> x = torch.ones((4, 3, 64, 64))\n>>> model(x).shape\ntorch.Size([4, 1])\n>>> model\nSequential(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu3): ReLU()\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu4): ReLU()\n  (pool4): AvgPool2d(kernel_size=8, stride=8, padding=0)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n) \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n```", "```py\n>>> def train(model, num_epochs, train_dl, valid_dl):\n...     loss_hist_train = [0] * num_epochs\n...     accuracy_hist_train = [0] * num_epochs\n...     loss_hist_valid = [0] * num_epochs\n...     accuracy_hist_valid = [0] * num_epochs\n...     for epoch in range(num_epochs):\n...         model.train()\n...         for x_batch, y_batch in train_dl:\n...             pred = model(x_batch)[:, 0]\n...             loss = loss_fn(pred, y_batch.float())\n...             loss.backward()\n...             optimizer.step()\n...             optimizer.zero_grad()\n...             loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n...             is_correct = ((pred>=0.5).float() == y_batch).float()\n...             accuracy_hist_train[epoch] += is_correct.sum()\n...         loss_hist_train[epoch] /= len(train_dl.dataset)\n...         accuracy_hist_train[epoch] /= len(train_dl.dataset)\n...\n...         model.eval()\n...         with torch.no_grad():\n...             for x_batch, y_batch in valid_dl:\n...                 pred = model(x_batch)[:, 0]\n...                 loss = loss_fn(pred, y_batch.float())\n...                 loss_hist_valid[epoch] += \\\n...                     loss.item() * y_batch.size(0)\n...                 is_correct = \\\n...                     ((pred>=0.5).float() == y_batch).float()\n...                 accuracy_hist_valid[epoch] += is_correct.sum()\n...         loss_hist_valid[epoch] /= len(valid_dl.dataset)\n...         accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n...\n...         print(f'Epoch {epoch+1} accuracy: '\n...               f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n...               f'{accuracy_hist_valid[epoch]:.4f}')\n...     return loss_hist_train, loss_hist_valid, \\\n...            accuracy_hist_train, accuracy_hist_valid \n```", "```py\n>>> torch.manual_seed(1)\n>>> num_epochs = 30\n>>> hist = train(model, num_epochs, train_dl, valid_dl)\nEpoch 1 accuracy: 0.6286 val_accuracy: 0.6540\n...\nEpoch 15 accuracy: 0.8544 val_accuracy: 0.8700\n...\nEpoch 30 accuracy: 0.8739 val_accuracy: 0.8710 \n```", "```py\n>>> x_arr = np.arange(len(hist[0])) + 1\n>>> fig = plt.figure(figsize=(12, 4))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> ax.plot(x_arr, hist[0], '-o', label='Train loss')\n>>> ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n>>> ax.legend(fontsize=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n>>> ax.plot(x_arr, hist[3], '--<',\n...         label='Validation acc.')\n>>> ax.legend(fontsize=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.set_ylabel('Accuracy', size=15)\n>>> plt.show() \n```", "```py\n>>> accuracy_test = 0\n>>> model.eval()\n>>> with torch.no_grad():\n...     for x_batch, y_batch in test_dl:\n...         pred = model(x_batch)[:, 0]\n...         is_correct = ((pred>=0.5).float() == y_batch).float()\n...         accuracy_test += is_correct.sum()\n>>> accuracy_test /= len(test_dl.dataset)\n>>> print(f'Test accuracy: {accuracy_test:.4f}')\nTest accuracy: 0.8446 \n```", "```py\n>>> pred = model(x_batch)[:, 0] * 100\n>>> fig = plt.figure(figsize=(15, 7))\n>>> for j in range(10, 20):\n...     ax = fig.add_subplot(2, 5, j-10+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(x_batch[j].permute(1, 2, 0))\n...     if y_batch[j] == 1:\n...         label='Smile'\n...     else:\n...         label = 'Not Smile'\n...     ax.text(\n...         0.5, -0.15,\n...         f'GT: {label:s}\\nPr(Smile)={pred[j]:.0f}%',\n...         size=16,\n...         horizontalalignment='center',\n...         verticalalignment='center',\n...         transform=ax.transAxes\n...     )\n>>> plt.show() \n```"]