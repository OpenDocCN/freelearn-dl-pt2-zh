["```py\n`…For it is in reality vain to profess _indifference_ in regard to such`\n`inquiries, the object of which cannot be indifferent to humanity.` \n```", "```py\n#@title Step 1: Loading the Dataset\n#1.Load kant.txt using the Colab file manager\n#2.Downloading the file from GitHub\n!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter04/kant.txt --output \"kant.txt\" \n```", "```py\n#@title Step 2:Installing Hugging Face Transformers\n# We won't need TensorFlow here\n!pip uninstall -y tensorflow\n# Install 'transformers' from master\n!pip install git+https://github.com/huggingface/transformers\n!pip list | grep -E 'transformers|tokenizers'\n# transformers version at notebook update --- 2.9.1\n# tokenizers version at notebook update --- 0.7.0 \n```", "```py\nSuccessfully built transformers\ntokenizers               0.7.0          \ntransformers             2.10.0 \n```", "```py\n...the tokenizer... \n```", "```py\n'Ġthe', 'Ġtoken',   'izer', \n```", "```py\n#@title Step 3: Training a Tokenizer\n%%time\nfrom pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer\npaths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()\n# Customize training\ntokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n]) \n```", "```py\nCPU times: user 14.8 s, sys: 14.2 s, total: 29 s\nWall time: 7.72 s \n```", "```py\n#@title Step 4: Saving the files to disk\nimport os\ntoken_dir = '/content/KantaiBERT'\nif not os.path.exists(token_dir):\n  os.makedirs(token_dir)\ntokenizer.save_model('KantaiBERT') \n```", "```py\n['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt'] \n```", "```py\n#version: 0.2 - Trained by 'huggingface/tokenizers'\nĠ t\nh e\nĠ a\no n\ni n\nĠ o\nĠt he\nr e\ni t\nĠo f \n```", "```py\n[…,\"Ġthink\":955,\"preme\":956,\"ĠE\":957,\"Ġout\":958,\"Ġdut\":959,\"aly\":960,\"Ġexp\":961,…] \n```", "```py\n#@title Step 5 Loading the Trained Tokenizer Files \nfrom tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\ntokenizer = ByteLevelBPETokenizer(\n    \"./KantaiBERT/vocab.json\",\n    \"./KantaiBERT/merges.txt\",\n) \n```", "```py\ntokenizer.encode(\"The Critique of Pure Reason.\").tokens \n```", "```py\n['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.'] \n```", "```py\ntokenizer.encode(\"The Critique of Pure Reason.\") \n```", "```py\nEncoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) \n```", "```py\ntokenizer._tokenizer.post_processor = BertProcessing(\n    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512) \n```", "```py\ntokenizer.encode(\"The Critique of Pure Reason.\") \n```", "```py\nEncoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) \n```", "```py\ntokenizer.encode(\"The Critique of Pure Reason.\").tokens \n```", "```py\n['<s>', 'The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.', '</s>'] \n```", "```py\n#@title Step 6: Checking Resource Constraints: GPU and NVIDIA \n!nvidia-smi \n```", "```py\n#@title Checking that PyTorch Sees CUDA\nimport torch\ntorch.cuda.is_available() \n```", "```py\nTrue \n```", "```py\n#@title Step 7: Defining the configuration of the Model\nfrom transformers import RobertaConfig\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n) \n```", "```py\n#@title Step 8: Re-creating the Tokenizer in Transformers\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512) \n```", "```py\n#@title Step 9: Initializing a Model From Scratch\nfrom transformers import RobertaForMaskedLM \n```", "```py\nmodel = RobertaForMaskedLM(config=config) \n```", "```py\nprint(model) \n```", "```py\nRobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n…/… \n```", "```py\nprint(model.num_parameters()) \n```", "```py\n84095008 \n```", "```py\n#@title Exploring the Parameters\nLP=list(model.parameters())\nlp=len(LP)\nprint(lp) \n```", "```py\n108 \n```", "```py\nfor p in range(0,lp):\n  print(LP[p]) \n```", "```py\nParameter containing:\ntensor([[-0.0175, -0.0210, -0.0334,  ...,  0.0054, -0.0113,  0.0183],\n        [ 0.0020, -0.0354, -0.0221,  ...,  0.0220, -0.0060, -0.0032],\n        [ 0.0001, -0.0002,  0.0036,  ..., -0.0265, -0.0057, -0.0352],\n        ...,\n        [-0.0125, -0.0418,  0.0190,  ..., -0.0069,  0.0175, -0.0308],\n        [ 0.0072, -0.0131,  0.0069,  ...,  0.0002, -0.0234,  0.0042],\n        [ 0.0008,  0.0281,  0.0168,  ..., -0.0113, -0.0075,  0.0014]],\n       requires_grad=True) \n```", "```py\n#@title Counting the parameters\nnp=0\nfor p in range(0,lp):#number of tensors \n```", "```py\n PL2=True\n  try:\n    L2=len(LP[p][0]) #check if 2D\n  except:\n    L2=1             #not 2D but 1D\n    PL2=False \n```", "```py\nL1=len(LP[p])      \nL3=L1*L2 \n```", "```py\nnp+=L3             # number of parameters per tensor \n```", "```py\n if PL2==True:\n    print(p,L1,L2,L3)  # displaying the sizes of the parameters\n  if PL2==False:\n    print(p,L1,L3)  # displaying the sizes of the parameters\nprint(np)              # total number of parameters \n```", "```py\n0 52000 768 39936000\n1 514 768 394752\n2 1 768 768\n3 768 768\n4 768 768\n5 768 768 589824\n6 768 768\n7 768 768 589824\n8 768 768\n9 768 768 589824\n10 768 768 \n```", "```py\n84,095,008 \n```", "```py\n#@title Step 10: Building the Dataset\n%%time\nfrom transformers import LineByLineTextDataset\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"./kant.txt\",\n    block_size=128,\n) \n```", "```py\nCPU times: user 8.48 s, sys: 234 ms, total: 8.71 s\nWall time: 3.88 s \n```", "```py\n#@title Step 11: Defining a Data Collator\nfrom transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n) \n```", "```py\n#@title Step 12: Initializing the Trainer\nfrom transformers import Trainer, TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./KantaiBERT\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n) \n```", "```py\n#@title Step 13: Pre-training the Model\n%%time\ntrainer.train() \n```", "```py\nEpoch: 100%\n1/1 [17:59<00:00, 1079.91s/it]\nIteration: 100%\n2672/2672 [17:59<00:00, 2.47it/s]\n{\"loss\": 5.6455852394104005, \"learning_rate\": 4.06437125748503e-05, \"epoch\": 0.18712574850299402, \"step\": 500}\n{\"loss\": 4.940259679794312, \"learning_rate\": 3.12874251497006e-05, \"epoch\": 0.37425149700598803, \"step\": 1000}\n{\"loss\": 4.639936000347137, \"learning_rate\": 2.1931137724550898e-05, \"epoch\": 0.561377245508982, \"step\": 1500}\n{\"loss\": 4.361462069988251, \"learning_rate\": 1.2574850299401197e-05, \"epoch\": 0.7485029940119761, \"step\": 2000}\n{\"loss\": 4.228510192394257, \"learning_rate\": 3.218562874251497e-06, \"epoch\": 0.9356287425149701, \"step\": 2500}\nCPU times: user 11min 36s, sys: 6min 25s, total: 18min 2s\nWall time: 17min 59s\nTrainOutput(global_step=2672, training_loss=4.7226536670130885) \n```", "```py\n#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\ntrainer.save_model(\"./KantaiBERT\") \n```", "```py\n#@title Step 15: Language Modeling with the FillMaskPipeline\nfrom transformers import pipeline\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"./KantaiBERT\",\n    tokenizer=\"./KantaiBERT\"\n) \n```", "```py\nfill_mask(\"Human thinking involves human <mask>.\") \n```", "```py\n[{'score': 0.022831793874502182,\n  'sequence': '<s> Human thinking involves human reason.</s>',\n  'token': 393},\n {'score': 0.011635891161859035,\n  'sequence': '<s> Human thinking involves human object.</s>',\n  'token': 394},\n {'score': 0.010641072876751423,\n  'sequence': '<s> Human thinking involves human priori.</s>',\n  'token': 575},\n {'score': 0.009517930448055267,\n  'sequence': '<s> Human thinking involves human conception.</s>',\n  'token': 418},\n {'score': 0.00923212617635727,\n  'sequence': '<s> Human thinking involves human experience.</s>',\n  'token': 531}] \n```", "```py\nHuman thinking involves human reason \n```"]