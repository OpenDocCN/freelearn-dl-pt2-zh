["```py\nfrom PIL import Image\nimport matplotlib.pyplot as pltimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvisiondvc = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n```", "```py\ndef image_to_tensor(image_filepath,  image_dimension=128):\n    img = Image.open(image_filepath).convert('RGB')\n    # display image\n    â€¦\n    torch_transformation =       torchvision.transforms.Compose([\n        torchvision.transforms.Resize(img_size),\n        torchvision.transforms.ToTensor()\n                              ])\n    img = torch_transformation(img).unsqueeze(0)\n    return img.to(dvc, torch.float)\nstyle_image = image_to_tensor(\"./images/style.jpg\")\ncontent_image =image_to_tensor(\"./images/content.jpg\")\n```", "```py\nvgg19_model = torchvision.models.vgg19(pretrained=True).to(dvc)\nprint(vgg19_model)\n```", "```py\nvgg19_model = vgg19_model.features\n```", "```py\nfor param in vgg19_model.parameters():\n    param.requires_grad_(False)\n```", "```py\nconv_indices = []for i in range(len(vgg19_model)):\n    if vgg19_model[i]._get_name() == 'MaxPool2d':\n        vgg19_model[i] =  nn.AvgPool2d(kernel_size=vgg19_model[i].kernel_size,\nstride=vgg19_model[i].stride, padding=vgg19_model[i].padding)\n    if vgg19_model[i]._get_name() == 'Conv2d':\n        conv_indices.append(i)\nconv_indices = dict(enumerate(conv_indices, 1))print(vgg19_model)\n```", "```py\nlayers = {1: 's', 2: 's', 3: 's', 4: 'sc', 5: 's'}\n```", "```py\nvgg_layers = nn.ModuleList(vgg19_model)\nlast_layer_idx = conv_indices[max(layers.keys())]\nvgg_layers_trimmed = vgg_layers[:last_layer_idx+1]\nneural_style_transfer_model = nn.Sequential(*vgg_layers_trimmed)\nprint(neural_style_transfer_model)\n```", "```py\n# initialize as the content image\n# ip_image = content_image.clone()\n# initialize as random noise:\nip_image = torch.randn(content_image.data.size(), device=dvc)\nplt.figure()\nplt.imshow(ip_image.squeeze(0).cpu().detach().numpy().transpose(1,2,0).clip(0,1));\n```", "```py\nnum_epochs=180\nwt_style=1e6\nwt_content=1\nstyle_losses = []\ncontent_losses = []\nopt = optim.Adam([ip_image.requires_grad_()], lr=0.1)\n```", "```py\nfor curr_epoch in range(1, num_epochs+1):    \n    ip_image.data.clamp_(0, 1)\n    opt.zero_grad()\n    epoch_style_loss = 0\n    epoch_content_loss = 0\n```", "```py\n for k in layers.keys():\n        if 'c' in layers[k]:\n            target = neural_style_transfer_model[:conv_indices[k]+1](content_image).detach()\n            ip = neural_style_transfer_model[:conv_indices[k]+1](ip_image)\n            epoch_content_loss += torch.nn.functional.mse_loss(ip, target)\n        if 's' in layers[k]:\n            target = gram_matrix(neural_style_transfer_model[:conv_indices[k]+1](style_image)).detach()\n            ip = gram_matrix(neural_style_transfer_model[:conv_indices[k]+1](ip_image))\n            epoch_style_loss += torch.nn.functional.mse_loss(ip, target)\n```", "```py\ndef gram_matrix(ip):\n    num_batch, num_channels, height, width = ip.size()\n    feats = ip.view(num_batch * num_channels, width *   height)\n    gram_mat = torch.mm(feats, feats.t())\n    return gram_mat.div(num_batch * num_channels *        width * height)\n```", "```py\n epoch_style_loss *= wt_style\n    epoch_content_loss *= wt_content\n    total_loss = epoch_style_loss + epoch_content_loss\n    total_loss.backward()\n```"]