- en: 'Chapter 1: The Large Language Model Revolution'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 章节1：大型语言模型革命
- en: '"art is the debris from the collision between the soul and the world" [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"艺术是灵魂和世界碰撞的残骸" [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)'
- en: '"technology is now the myth of the modern world" [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '"科技现在是现代世界的神话" [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)'
- en: '"revolutions begin with a question, but do not end with an answer" [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '"革命始于一个问题，但不以一个答案结束" [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)'
- en: '"nature decorates the world with variety" [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '"大自然用多样性装饰世界" [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)'
- en: Imagine waking up to a beautiful, sunny morning. It’s Monday, and you know the
    week will be hectic. Your company is about to launch a new personal-productivity
    app, Taskr, and start a social media campaign to inform the world about your ingenious
    product.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下醒来的美丽、阳光明媚的早晨。今天是星期一，你知道这一周将会很忙碌。你的公司即将推出一款新的个人生产力应用程序Taskr，并启动一场社交媒体宣传活动，告知世界你们的创造性产品。
- en: This week, your main task is writing and publishing a series of engaging blog
    posts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本周，你的主要任务是编写并发布一系列引人入胜的博客文章。
- en: 'You start by making a to-do list:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始列出一份待办事项清单：
- en: ●        Write an informative and fun article about productivity hacks, including
    Taskr. Keep it under 500 words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ●        写一篇关于提高生产力的有趣信息性的文章，包括Taskr。字数限制在500字以内。
- en: ●        Create a list of 5 catchy article titles.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ●        创建5个引人注目的文章标题清单。
- en: ●        Choose the visuals.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ●        选择图片。
- en: You hit Enter, take a sip of coffee, and watch an article unravel on your screen,
    sentence by sentence, paragraph by paragraph. In 30 seconds, you have a meaningful,
    high-quality blog post, a perfect starter for your social media series. The visual
    is fun and attention-grabbing. It’s done! You choose the best title and begin
    the publishing process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你按下回车键，抿一口咖啡，看着文章在屏幕上一句一句、一段一段地展开。30秒钟后，你就有了一篇有意义、高质量的博客文章，是社交媒体系列的完美开端。视觉效果有趣，能吸引注意。完成了！你选择最好的标题并开始发布流程。
- en: This is not a distant, futuristic fantasy but a glimpse of the new reality made
    possible by advancements in AI. As we write this book, many such applications
    are being created and deployed to a broader audience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个遥远的、未来的幻想，而是通过人工智能技术的进步实现的新现实的一瞥。在我们撰写本书的同时，许多这样的应用正在被创建和部署到更广泛的受众中。
- en: GPT-3 is a cutting-edge language model created by OpenAI, a company at the frontier
    of artificial intelligence R&D. OpenAI’s [research paper](https://arxiv.org/abs/2005.14165)
    announcing GPT-3 was released in May 2020, following a launch of access to GPT-3
    via [OpenAI API](https://openai.com/blog/openai-api/) in June 2020\. Since the
    GPT-3 release, people around the world coming from different backgrounds, including
    technology, art, literature, marketing, etc., have already found hundreds of exciting
    applications of the model that have the potential to elevate the ways we communicate,
    learn, and play.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是由OpenAI公司创建的尖端语言模型，该公司处于人工智能研发的前沿。OpenAI在2020年5月发布了宣布GPT-3的[研究论文](https://arxiv.org/abs/2005.14165)，并于2020年6月通过[OpenAI
    API](https://openai.com/blog/openai-api/)提供了对GPT-3的访问权限。自GPT-3发布以来，来自不同背景的人，包括技术、艺术、文学、市场营销等领域的人，已经发现了该模型数以百计令人兴奋的应用，这些应用有潜力提升我们沟通、学习和娱乐的方式。
- en: GPT-3 can solve general language-based tasks, like generating and classifying
    text, with unprecedented ease, moving freely between different text styles and
    purposes. The array of problems it can solve is vast.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3可以轻松解决一般的基于语言的任务，比如生成和分类文本，自由地在不同的文本风格和目的之间移动。它可以解决的问题范围是广阔的。
- en: In this book, we invite you to think of what problems you might solve with GPT-3
    yourself. We’ll show you what it is and how to use it, but first, we want to give
    you a bit of context. The rest of this chapter will discuss where this technology
    comes from, how it is built, what tasks it excels at, and its potential risks.
    Let’s dive right in by looking at the field of natural language processing (NLP)
    and how large language models (LLMs) and GPT-3 fit into it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们邀请你考虑用GPT-3解决什么问题。我们将向你展示它是什么以及如何使用它，但首先，我们想给你一些背景知识。本章的其余部分将讨论这项技术的来源，它是如何构建的，它擅长解决哪些任务，以及它的潜在风险。让我们直奔主题，先看一看自然语言处理（NLP）领域，以及大型语言模型（LLMs）和GPT-3是如何融入其中的。
- en: A Behind-the-Scenes Look at Natural Language Processing
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 背后的自然语言处理观察
- en: NLP is a subfield focusing on the interaction between computers and human languages.
    Its goal is to build systems that can process natural language, which is the way
    people communicate with each other. NLP combines linguistics, computer science,
    and artificial intelligence techniques to achieve this.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 是专注于计算机与人类语言交互的一个子领域。其目标是构建能够处理自然语言的系统，即人们相互交流的方式。NLP 结合了语言学、计算机科学和人工智能技术来实现这一目标。
- en: NLP combines the field of computational linguistics (rule-based modeling of
    human language) with machine learning to create intelligent machines capable of
    identifying the context and understanding the intent of natural language. Machine
    learning is a branch of AI that focuses on the study of how machines can improve
    their performance on tasks through experiences without being explicitly programmed
    to do so. Deep learning is a subfield of machine learning that involves using
    neural networks modeled after the human brain to perform complex tasks with minimal
    human intervention.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 将计算语言学（基于规则的人类语言建模）领域与机器学习相结合，创建出能够识别上下文并理解自然语言意图的智能机器。机器学习是人工智能的一个分支，专注于研究如何使机器通过经验提高其在任务上的表现，而不需要明确编程。深度学习是机器学习的一个子领域，涉及使用模仿人脑的神经网络来进行复杂任务，减少人为干预。
- en: The 2010s saw the advent of deep learning, and with the maturity of the field
    came large language models consisting of dense neural networks composed of thousands
    or even millions of simple processing units called artificial neurons. Neural
    networks became the first significant game-changer in the field of NLP by making
    it feasible to perform complex natural language tasks, which had been possible
    only in theory so far. The second major milestone was the introduction of pre-trained
    models (such as GPT-3) that could be fine-tuned on various downstream tasks, saving
    many hours of training. (We discuss pre-trained models later in this chapter.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2010 年代见证了深度学习的出现，随着该领域的成熟，出现了由数千甚至数百万个简单处理单元（称为人工神经元）组成的密集神经网络的大型语言模型。神经网络通过使得执行复杂自然语言任务成为可能，成为了
    NLP 领域的首个重大变革者，而这些任务迄今为止仅在理论上可能。第二个重要里程碑是引入了可以在各种下游任务上进行微调的预训练模型（如 GPT-3），从而节省了大量训练时间。（我们在本章后面讨论预训练模型。）
- en: 'NLP is at the core of many real-world AI applications, such as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 是许多现实世界人工智能应用的核心，例如：
- en: Spam detection
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件检测
- en: The spam filtering in your email inbox assigns a percentage of the incoming
    emails to the spam folder, using NLP to evaluate which emails look suspicious.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件收件箱中的垃圾邮件过滤将一部分收到的电子邮件分配到垃圾邮件文件夹中，使用 NLP 来评估哪些电子邮件看起来可疑。
- en: Machine translation
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Google Translate, DeepL, and other machine translation programs use NLP to assess
    millions of sentences translated by human speakers of different language pairs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Google 翻译、DeepL 等机器翻译程序使用自然语言处理（NLP）评估由不同语言对的人类讲话者翻译的数百万句子。
- en: Virtual assistants and chatbots
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟助手和聊天机器人
- en: All the Alexas, Siris, Google Assistant, and customer support chatbots of the
    world fall into this category. They use NLP to understand, analyze, prioritize
    user questions and requests, and respond to them quickly and correctly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所有世界上的 Alexa、Siri、Google Assistant 和客户支持聊天机器人都属于这一类。它们使用 NLP 来理解、分析、优先处理用户的问题和请求，并迅速正确地回复它们。
- en: Social media sentiment analysis
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体情感分析
- en: Marketers collect social media posts about specific brands, conversation subjects,
    and keywords, then use NLP to analyze how users feel about each topic, individually
    and collectively. It helps the brands with customer research, image evaluation,
    and social dynamics detection.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 市场营销人员收集特定品牌、话题和关键词的社交媒体帖子，然后使用 NLP 分析用户对每个主题的感受，以及整体感受。它有助于品牌进行客户调查、形象评估和社会动态检测。
- en: Text summarization
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Summarizing a text involves reducing its size while keeping key information
    and the essential meaning. Some everyday examples of text summarization are news
    headlines, movie previews, newsletter production, financial research, legal contract
    analysis, email summaries, and applications delivering news feeds, reports, and
    emails.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行摘要意味着减小其大小，同时保留关键信息和基本含义。文本摘要的一些日常示例包括新闻标题、电影预告片、通讯简报制作、金融研究、法律合同分析、电子邮件摘要以及提供新闻订阅、报告和电子邮件的应用程序。
- en: Semantic search
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索
- en: Semantic search leverages deep neural networks to intelligently search through
    the data. You interact with it every time you search on Google. Semantic search
    is helpful when searching for something based on the context rather than specific
    keywords.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索利用深度神经网络智能地搜索数据。每当您在Google上搜索时，您都在与之互动。语义搜索在基于上下文而不是特定关键词搜索某物时非常有用。
- en: '"The way we interact with other humans is through language,” says [Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew),
    one of the most popular YouTubers and influencers in the NLP space.  “Language
    is part of every business transaction, every other interaction we have with other
    humans, even with machines in part we interact with some sort of language, be
    that via programming or a user interface.” It’s no wonder, then, that NLP as a
    field has been the site of some of the most exciting AI discoveries and implementations
    of the past decade.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “我们与其他人类的交流方式是通过语言”，一位在自然语言处理领域最受欢迎的YouTuber和影响者[Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)说道。
    “语言是每一笔业务交易、我们与其他人类的每一次交互，甚至在与机器的部分交互中，我们也在某种程度上使用某种语言，无论是通过编程还是用户界面。” 因此，不足为奇，NLP作为一个领域已经成为过去十年中一些最激动人心的人工智能发现和实施的地方。
- en: Language Models Getting Bigger & Better
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型越来越大、越来越好
- en: Language modeling is the task of assigning a probability to a sequence of words
    in a text in a specific language. Based on a statistical analysis of existing
    text sequences, simple language models can look at a word and predict the next
    word (or words) most likely to follow. To create a language model that successfully
    predicts word sequences, you must train it on large data sets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是将文本中一系列单词赋予特定语言中的概率的任务。根据对现有文本序列的统计分析，简单的语言模型可以查看一个词并预测最有可能跟随的下一个词（或词组）。要创建一个成功预测单词序列的语言模型，您必须在大型数据集上对其进行训练。
- en: Language models are a vital component in natural language processing applications.
    You can think of them as statistical prediction machines, giving text as input
    and getting a prediction as the output. You’re probably familiar with this from
    the autocomplete feature on your phone. For instance, if you type good, autocomplete
    might come up with suggestions like “morning” or “luck.”
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是自然语言处理应用程序的重要组成部分。您可以将它们视为统计预测机器，给定文本作为输入并得到预测作为输出。您可能从手机上的自动完成功能中熟悉这一点。例如，如果您输入good，自动完成可能会提供“morning”或“luck”等建议。
- en: Before GPT-3 there was no general language model that could perform well on
    an array of NLP tasks. Language models were designed to perform one NLP task,
    such as text generation, summarization, or classification. So, in this book, we
    will discuss GPT-3’s extraordinary capabilities as a general language model. We’ll
    start this chapter by walking you through each letter of “GPT” to show what they
    stand for and what are the elements with which the famous model was built. We’ll
    give a brief overview of the model’s history and how the sequence-to-sequence
    models we see today came into the picture. After that, we will walk you through
    the importance of API access and how it evolved based on users' demands. We recommend
    signing up for an OpenAI account before you move on to the rest of the chapters.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-3之前，没有一种通用的语言模型能够在各种NLP任务上表现良好。语言模型被设计用于执行一种NLP任务，如文本生成、摘要或分类。因此，在本书中，我们将讨论GPT-3作为通用语言模型的非凡能力。我们将从介绍“GPT”每个字母开始，以展示它们代表什么以及构建这个著名模型的元素是什么。我们将简要概述该模型的历史以及今天我们看到的序列到序列模型是如何出现的。之后，我们将向您介绍API访问的重要性以及它如何根据用户需求进行了演进。我们建议在继续阅读本书的其余章节之前注册一个OpenAI账号。
- en: 'The Generative Pre-Trained Transformer: GPT-3'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式预训练转换器：GPT-3
- en: The name GPT-3 stands for “Generative Pre-trained Transformer 3.” Let's go through
    all these terms one by one to understand the making of GPT-3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3这个名字代表“生成式预训练转换器3”。让我们逐一通过所有这些术语来理解GPT-3的制作过程。
- en: Generative models
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型
- en: GPT-3 is a generative model because it generates text. Generative modeling is
    a branch of statistical modeling. It is a method for mathematically approximating
    the world.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是一种生成模型，因为它生成文本。生成建模是统计建模的一个分支。它是一种用数学方法近似世界的方法。
- en: We are surrounded by an incredible amount of easily accessible information—
    both in the physical and digital worlds. The tricky part is to develop intelligent
    models and algorithms that can analyze and understand this treasure trove of data.
    Generative models are one of the most promising approaches to achieving this goal.[[1]](xhtml-0-12.xhtml#aid_74)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们被一个难以置信的大量易于访问的信息所包围，无论是在物理世界还是数字世界中。棘手的部分是开发能够分析和理解这些宝库数据的智能模型和算法。生成模型是实现这一目标最有前途的方法之一。
- en: 'To train a model, you must prepare and preprocess a dataset, a collection of
    examples that helps the model learn to perform a given task. Usually, a dataset
    is a large amount of data in some specific domain: like millions of images of
    cars to teach a model what a car is, for example. Datasets can also take the form
    of sentences or audio samples. Once you have shown the model many examples, you
    must train it to generate similar data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，您必须准备和预处理一个数据集，这是一系列示例，帮助模型学习执行给定任务。通常，数据集是某个特定领域的大量数据：例如，数百万张车辆图像，以教会模型汽车是什么。数据集也可以采用句子或音频样本的形式。一旦向模型展示了许多示例，您必须训练它生成类似的数据。
- en: Pre-trained models
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Have you heard of the theory of 10,000 hours? In his book Outliers, Malcolm
    Gladwell suggests that practicing any skill for 10,000 hours is sufficient to
    make you an expert.[[2]](xhtml-0-12.xhtml#aid_68) This “expert” knowledge is reflected
    in the connections your human brain develops between its neurons. An AI model
    does something similar.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您听说过一万小时理论吗？在他的书《异类》中，马尔科姆·格拉德威尔建议练习任何技能一万小时就足以使您成为专家。这种“专家”知识体现在您的人脑发展的神经元之间的连接中。人工智能模型也在做类似的事情。
- en: To create a model that performs well, you need to train it using a specific
    set of variables called parameters. The process of determining the ideal parameters
    for your model is called training. The model assimilates parameter values through
    successive training iterations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个表现良好的模型，您需要使用一组特定的变量（称为参数）对其进行训练。确定模型的理想参数的过程称为训练。模型通过连续的训练迭代吸收参数值。
- en: A deep learning model takes a lot of time to find these ideal parameters. Training
    is a lengthy process that depending on the task, can last from a few hours to
    a few months and requires a tremendous amount of computing power. Reusing some
    of that long learning process for other tasks would significantly help. And this
    is where the pre-trained models come in.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型需要很长时间来找到这些理想参数。训练是一个漫长的过程，具体取决于任务，可能持续几个小时到几个月，并且需要大量的计算资源。重新利用部分长时间的学习过程以应对其他任务将会极大地帮助。这就是预训练模型发挥作用的地方。
- en: A pre-trained model, keeping with Gladwell’s 10,000 hours theory, is the first
    skill you develop to help you acquire another faster. For example, mastering the
    craft of solving math problems can allow you to acquire the skill of solving engineering
    problems faster. A pre-trained model is trained (by you or someone else) for a
    more general task and can be fine-tuned for different tasks. Instead of creating
    a brand new model to address your issue, you can use a pre-trained model that
    has already been trained on a more general problem. The pre-trained model can
    be fine-tuned to address your specific needs by providing additional training
    with a tailored dataset. This approach is faster and more efficient and allows
    for improved performance compared to building a model from scratch.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型，与格拉德威尔的“一万小时理论”保持一致，是帮助您更快地获得另一种技能的第一个技能。例如，掌握解决数学问题的技能可以让您更快地获得解决工程问题的技能。预训练模型是针对更一般任务进行训练的（由您或他人训练），可以针对不同的任务进行微调。您可以使用已经针对更一般问题进行了训练的预训练模型，而不是创建全新的模型来解决您的问题。通过使用定制的数据集提供额外的训练，可以微调预训练模型以满足您的特定需求。这种方法更快，更高效，并且与从头开始构建模型相比，可以实现更好的性能。
- en: 'In machine learning, a model is trained on a dataset. The size and type of
    data samples vary depending on the task you want to solve. GPT-3 is pre-trained
    on a corpus of text from five datasets: Common Crawl, WebText2, Books1, Books2,
    and Wikipedia.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型是在数据集上训练的。数据样本的大小和类型取决于您想要解决的任务。GPT-3是在五个数据集的文本语料库上预训练的：Common Crawl，WebText2，Books1，Books2和Wikipedia。
- en: Common Crawl
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl
- en: The Common Crawl corpus comprises petabytes of data, including raw web page
    data, metadata, and text data collected over eight years of web crawling. OpenAI
    researchers use a curated, filtered version of this dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl语料库包含了宠字节的数据，包括原始网页数据、元数据和八年网络爬虫收集的文本数据。OpenAI研究人员使用这个数据集的策划和过滤版本。
- en: WebText2
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: WebText2
- en: WebText2 is an expanded version of the WebText dataset, an internal OpenAI corpus
    created by scraping particularly high-quality web pages. To vet for quality, the
    authors scraped all outbound links from Reddit, which received at least three
    karma (an indicator for whether other users found the link interesting, educational,
    or just funny). WebText contains 40 gigabytes of text from these 45 million links,
    and over 8 million documents.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: WebText2是WebText数据集的扩展版本，是OpenAI的一个内部语料库，通过对特别高质量的网页进行抓取而创建。为了确保质量，作者们从Reddit抓取了所有至少有三个karma（指其他用户是否认为链接有趣、教育性或仅仅是有趣）的出站链接。WebText包含来自这4500万个链接的40
    GB文本，以及800多万个文档。
- en: Books1 and Books2
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Books1和Books2
- en: Books1 and Books2 are two corpora, or collections of text, that contain the
    text of tens of thousands of books on various subjects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Books1和Books2是两个语料库，包含了数以万计的各种主题的书籍文本。
- en: Wikipedia
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科
- en: A collection including all English-language articles from the crowdsourced online
    encyclopedia [Wikipedia](https://en.wikipedia.org/wiki/Main_Page) at the time
    of finalizing the GPT-3’s dataset in 2019\. This dataset has roughly [5.8 million](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia)
    English articles.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个集合，包括[GPT-3数据集](https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_document_count.csv)最终确定时的所有英语文章。这个数据集大约有[580万](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia)英语文章。
- en: This corpus includes nearly a trillion words altogether.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语料库总共包含近一万亿字。
- en: GPT-3 is capable of generating and successfully working with languages other
    than English as well. Table 1-1 shows the [top 10](https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_document_count.csv)
    other languages within the dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3还能够生成并成功处理英语以外的语言。表1-1展示了数据集中[排名前十位](https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_document_count.csv)的其他语言。
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Rank
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 排名
- en: '|'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Language
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 语言
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Number of documents
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文件数量
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '% of total documents'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总文档的百分比
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '1.'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: English
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 英语
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '235987420'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '235987420'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 93.68882%
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 93.68882%
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '2.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '2.'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: German
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 德语
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '3014597'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '3014597'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 1.19682%
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1.19682%
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '3.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '3.'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: French
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 法语
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '2568341'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '2568341'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 1.01965%
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 1.01965%
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '4.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '4.'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Portuguese
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄牙语
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1608428'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '1608428'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.63856%
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 0.63856%
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '5.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '5.'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Italian
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 意大利语
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1456350'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '1456350'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.57818%
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 0.57818%
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '6.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '6.'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Spanish
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 西班牙语
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1284045'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '1284045'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.50978%
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 0.50978%
- en: '|'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '7.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '7.'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Dutch
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 荷兰语
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '934788'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '934788'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.37112%
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 0.37112%
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '8.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '8.'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Polish
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 波兰语
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '632959'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '632959'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.25129%
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 0.25129%
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '9.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '9.'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Japanese
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 日语
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '619582'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '619582'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.24598%
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 0.24598%
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '10.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '10.'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Danish
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 丹麦语
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '396477'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '396477'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 0.15740%
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 0.15740%
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: ​Table 1-1\. Top ten languages in the GPT-3 dataset
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1\. GPT-3数据集中的前十种语言
- en: While the gap between English and other languages is dramatic - English is number
    one, with 93% of the dataset; German, at number two, accounts for just 1% - that
    1% is sufficient to create perfect text in German, with style transfer and other
    tasks. The same goes for other languages on the list.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管英语和其他语言之间的差距很大 - 英语位居榜首，占数据集的93％；德语排名第二，仅占1％ - 但这1％足以创建德语的完美文本，并进行风格转换和其他任务。对于列表中的其他语言也是一样。
- en: Since GPT-3 is pre-trained on an extensive and diverse corpus of text, it can
    successfully perform a surprising number of NLP tasks without users providing
    any additional example data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT-3是在广泛而多样的文本语料库上进行预训练的，因此它可以成功地执行令人惊讶的多个NLP任务，而无需用户提供任何额外的示例数据。
- en: Transformer Models
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型
- en: Neural networks are at the heart of deep learning, with their name and structure
    being inspired by the human brain. They are composed of a network or circuit of
    neurons that work together. Advances in neural networks can enhance the performance
    of AI models on various tasks, leading AI scientists to continually develop new
    architectures for these networks. One such advancement is the transformer, a machine
    learning model that processes a sequence of text all at once rather than one word
    at a time and has a strong ability to understand the relationship between those
    words. This invention has dramatically impacted the field of natural language
    processing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是深度学习的核心，其名称和结构受到了人脑的启发。它们由一组一起工作的神经元组成。神经网络的进展可以增强人工智能模型在各种任务上的性能，导致人工智能科学家不断为这些网络开发新的架构。其中一项进展是转换器，这是一个机器学习模型，可以一次处理一段文本的所有内容，而不是逐个单词处理，并且非常擅长理解这些单词之间的关系。这一发明对自然语言处理领域产生了巨大影响。
- en: Sequence-to-sequence models
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: 'Researchers at Google and the University of Toronto paper introduced a transformer
    model in a 2017 paper:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌和多伦多大学的研究人员在2017年的一篇论文中介绍了转换器模型：
- en: We propose a new simple network architecture, the Transformer, based solely
    on attention mechanisms, dispensing with recurrence and convolutions entirely.
    Experiments on two machine translation tasks show these models to be superior
    in quality while being more parallelizable and requiring significantly less time
    to train.[[3]](xhtml-0-12.xhtml#aid_55)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新的简单网络架构，即转换器，它仅基于注意机制，完全摒弃了循环和卷积。对两个机器翻译任务的实验表明，这些模型在质量上优于其他模型，同时更易并行化，需要的训练时间显著减少。[[3]](xhtml-0-12.xhtml#aid_55)
- en: The foundation of transformer models is sequence-to-sequence architecture. Sequence-
    to-sequence (Seq2Seq) models are useful for converting a sequence of elements,
    such as words in a sentence, into another sequence, such as a sentence in a different
    language. This is particularly effective in translation tasks, where a sequence
    of words in one language is translated into a sequence of words in another language.
    Google Translate started using a Seq2Seq-based model in 2016.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器模型的基础是序列到序列架构。序列到序列（Seq2Seq）模型对将一个序列，比如句子中的单词，转换成另一个序列，比如另一种语言中的句子，非常有用。这在翻译任务中特别有效，其中一个语言中的单词序列被翻译成另一种语言中的单词序列。谷歌翻译在2016年开始使用基于Seq2Seq模型的模型。
- en: '![](img/image-0-0.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-0.jpg)'
- en: Figure 1-1\. Seq-to-Seq Model (Neural Machine Translation)[[4]](xhtml-0-12.xhtml#aid_75)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-1。序列到序列模型（神经机器翻译）[[4]](xhtml-0-12.xhtml#aid_75)
- en: 'Seq2Seq models are comprised of two components: an Encoder and a Decoder. The
    Encoder can be thought of as a translator who speaks French as their first language
    and Korean as their second language. The Decoder is a translator who speaks English
    as their first language and Korean as their second language. To translate French
    to English, the Encoder converts the French sentence into Korean (also known as
    the context) and passes it on to the Decoder. Since the Decoder understands Korean,
    it can translate the sentence from Korean to English. The Encoder and Decoder
    can successfully translate from French to English[[5]](xhtml-0-12.xhtml#aid_17),
    as illustrated by Figure 1-1.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq模型由两个组件组成：编码器和解码器。编码器可以被看作是以法语为母语、以韩语为第二语言的翻译人员。解码器则是一位以英语为母语、以韩语为第二语言的翻译人员。要将法语翻译成英语，编码器将法语句子转换为韩语（也称为上下文），然后传递给解码器。由于解码器理解韩语，它可以将句子从韩语翻译成英语。编码器和解码器可以成功地将法语翻译成英语[[5]](xhtml-0-12.xhtml#aid_17)，如图1-1所示。
- en: Transformer attention mechanisms
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器注意机制
- en: Transformer architecture was invented to improve AIs’ performance on machine
    translation tasks. “Transformers started as language models,” Kilcher explains,
    “not even that large, but then they became large."
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器架构是为了改进人工智能在机器翻译任务上的表现而发明的。“转换器最初是作为语言模型出现的，”基尔彻解释说，“起初并不大，但后来变得更大了。”
- en: To use transformer models effectively, it is crucial to grasp the concept of
    attention. Attention mechanisms mimic how the human brain focuses on specific
    parts of an input sequence, using probabilities to determine which parts of the
    sequence are most relevant at each step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地使用转换器模型，了解注意力的概念至关重要。注意机制模仿人脑如何集中注意力于输入序列的特定部分，使用概率确定在每个步骤中哪些部分的序列最相关。
- en: For example, look at the sentence,” The cat sat on the mat once it ate the mouse.”
    Does it in this sentence refer to “the cat" or "the mat"? The transformer model
    can strongly connect "it" with "the cat." That's attention.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看这个句子，“猫坐在垫子上，一旦吃掉了老鼠。” 在这个句子中，"it" 是指“猫”还是“垫子”？Transformer 模型可以将 "it" 强烈地与
    "猫" 连接起来。这就是注意力。
- en: An example of how the Encoder and Decoder work together is when the Encoder
    writes down important keywords related to the meaning of the sentence and provides
    them to the Decoder along with the translation. These keywords make it easier
    for the Decoder to understand the translation, as it now has a better understanding
    of the critical parts of the sentence and the terms that provide context.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器共同工作的一个例子是，当编码器记录下与句子含义相关的重要关键词并将其与翻译一起提供给解码器时。这些关键词使得解码器更容易理解翻译，因为它现在更好地理解了句子的关键部分和提供上下文的术语。
- en: 'The transformer model has two types of attention: self-attention (the connection
    of words within a sentence) and Encoder-Decoder attention (the connection between
    words from the source sentence to words from the target sentence).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型有两种注意力机制：自注意力（句子内单词之间的连接）和编码器-解码器注意力（源句子中的单词与目标句子中的单词之间的连接）。
- en: 'The attention mechanism helps the transformer filter out the noise and focus
    on what’s relevant: connecting two words in semantic relationship to each other
    that do not carry any apparent markers pointing to one another.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制帮助 Transformer 模型过滤噪音，聚焦于相关内容：将两个语义关联的单词连接起来，而这些单词之间没有明显的指向彼此的标记。
- en: Transformer models benefit from larger architectures and larger quantities of
    data. Training on large datasets and fine-tuning for specific tasks improve results.
    Transformers better understand the context of words in a sentence than any other
    kind of neural network. GPT is just the Decoder part of the transformer.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型受益于更大的架构和更多的数据。在大型数据集上进行训练，并针对特定任务进行微调可以提高结果。相比于其他类型的神经网络，Transformer
    更好地理解句子中单词的语境。GPT 只是 Transformer 的解码器部分。
- en: Now that you know what “GPT” means, let’s talk about that “3”- as well as 1
    and 2.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道“GPT”的含义了，让我们谈谈那个“3”——以及 1 和 2。
- en: 'GPT-3: A Brief History'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3：简史
- en: 'GPT-3 was created by, and is a significant milestone for, OpenAI, a San Francisco-based
    pioneer of AI research. OpenAI’s [stated mission](https://openai.com/about/#:~:text=Our%20mission%20is%20to%20ensure,work%E2%80%94benefits%20all%20of%20humanity.)
    is “to ensure that artificial general intelligence benefits all of humanity,”
    as well as its vision of creating artificial general intelligence: a type of AI
    not confined to being specialized tasks, instead performing well at a variety
    of tasks, just like humans do.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 由旧金山的人工智能研究先驱 OpenAI 创建，并且是一个重要的里程碑。OpenAI 的 [声明任务](https://openai.com/about/#:~:text=Our%20mission%20is%20to%20ensure,work%E2%80%94benefits%20all%20of%20humanity.)
    是“确保人工智能造福全人类”，以及其创造人工通用智能的愿景：一种不仅限于特定任务，而是在各种任务中表现良好的人工智能类型，就像人类一样。
- en: GPT-1
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1
- en: OpenAI presented GPT-1 in June 2018\. The developers’ [key finding](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    was that combining the transformer architecture with unsupervised pre-training
    yielded promising results. GPT-1, they write, was fine-tuned for specific tasks
    to achieve “strong natural language understanding.”
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在 2018 年 6 月发布了 GPT-1。开发者的 [主要发现](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    是将 Transformer 架构与无监督的预训练相结合可以产生有希望的结果。他们写道，GPT-1 经过特定任务的微调，实现了“强大的自然语言理解”。
- en: GPT-1 was an essential stepping stone towards a language model with general
    language-based capabilities. It proved that language models can be effectively
    pre-trained, which could help them generalize well. The architecture could perform
    various NLP tasks with very little fine-tuning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1 是通向具有通用语言能力的语言模型的重要里程碑。它证明了语言模型可以被有效地预训练，这可以帮助它们很好地泛化。该架构可以通过极少的微调执行各种自然语言处理任务。
- en: The GPT-1 model used the [BooksCorpus](https://yknzhu.wixsite.com/mbweb) dataset,
    which contains some 7,000 unpublished books and self-attention in the transformer's
    decoder to train the model. The architecture was similar to the original transformer,
    with 117 million parameters. This model paved the way for future models with larger
    datasets and more parameters to utilize its potential better.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: One of its notable abilities was its decent performance on zero-shot tasks in
    natural language processing, such as question-answering and sentiment analysis,
    thanks to pre-training. Zero-shot learning is the ability of a model to perform
    a task without having previously seen examples of that task. In Zero-shot task
    transfer, the model is given little to no examples and must understand the task
    based on instructions and a few examples.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: In February 2019, OpenAI introduced GPT-2, which is bigger but otherwise very
    similar. The significant difference is that GPT-2 can multitask. It successfully
    [proved](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    that a language model could perform well on several tasks without receiving any
    training examples.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 showed that training on a larger dataset and having more parameters improves
    a language model’s capability to understand tasks and surpass the state-of-the-art
    of many tasks in zero-shot settings. It also showed that even larger language
    models would better understand natural language.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: To create an extensive, high-quality dataset, the authors scraped Reddit and
    pulled data from outbound links of upvoted articles on the platform. The resulting
    dataset, WebText, had 40GB of text data from over 8 million documents, far larger
    than GPT-1’s dataset. GPT-2 was trained on the WebText dataset and had 1.5 billion
    parameters, ten times more than GPT-1.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 was evaluated on several datasets of downstream tasks like reading comprehension,
    summarisation, translation, and question answering.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'In the quest to build an even more robust and powerful language model, OpenAI
    built the GPT-3 model. Both its dataset and the model are about two orders of
    magnitude larger than those used for GPT-2: GPT-3 has 175 billion parameters and
    was trained on a mix of five different text corpora, a much bigger dataset than
    the dataset used to train GPT-2\. The architecture of GPT-3 is largely the same
    as GPT-2\. It performs well on downstream NLP tasks in zero-shot and few-shot
    settings.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 has capabilities like writing articles that are indistinguishable from
    human-written articles. It can also perform on-the-fly tasks for which it was
    never explicitly trained, like summing numbers, writing SQL queries, and even
    writing React and JavaScript codes given a plain English description of the tasks.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Few, one, and zero-shot settings are specialized cases of zero-shot task
    transfer. In a few-shot setting, the model is provided with a task description
    and as many examples as fit into the context window of the model. The model is
    provided with exactly one example in a one-shot setting and in a zero-shot setting
    with no example.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：少量、一量和零量设置是零样本任务转移的特殊情况。在少量设置中，模型提供了任务描述和适合模型上下文窗口的尽可能多的示例。在一量设置中，模型提供了一个示例，而在零量设置中则没有示例。
- en: OpenAI focuses on the democratic and ethical implications of AI in its mission
    statement. This can be seen in their decision to make the third version of their
    model, GPT-3, available via a public API. Application programming interface allows
    for a software intermediary to facilitate communication between a website or app
    and the user.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在其使命声明中关注 AI 的民主和伦理影响。这可以从他们决定通过公共 API 提供其模型 GPT-3 的第三个版本中看出。应用程序编程接口允许软件中介在网站或应用程序与用户之间进行通信。
- en: APIs act as a means of communication between developers and applications, allowing
    them to build new programmatic interactions with users. Releasing GPT-3 via an
    API was a revolutionary move. Until 2020, the powerful AI models developed by
    leading research labs were available to only a select few researchers and engineers
    working on these projects. The OpenAI API gives users all over the world unprecedented
    access to the world's most powerful language model via simple sign-in. (OpenAI’s
    business rationale for this move is to create a new paradigm it calls “[Model-as-a-Service](https://arxiv.org/abs/1706.03762)”
    where developers can pay per API call; we will take a closer look at this in chapter
    3.)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: API 充当了开发者和应用程序之间通信的手段，使它们能够与用户建立新的程序化交互。通过 API 发布 GPT-3 是一项革命性的举措。直到 2020 年，由领先研究实验室开发的强大
    AI 模型仅供少数研究人员和工程师使用。OpenAI API 通过简单的登录，让全世界的用户首次获得了对世界上最强大的语言模型的无与伦比的访问权限。（OpenAI
    对此举的商业原因是创建一个称为“[模型即服务](https://arxiv.org/abs/1706.03762)”的新范式，在其中开发者可以按 API 调用付费；我们将在第三章中更仔细地研究这一点。）
- en: OpenAI researchers experimented with different model sizes while working on
    GPT-3\. They took the existing GPT-2 architecture and increased the number of
    parameters. What came out as a result of that experiment was a model with new
    and extraordinary capabilities in the form of GPT-3\. While GPT-2 displayed some
    zero-shot capabilities on downstream tasks, GPT-3  can carry out even more novel
    tasks when presented with example context.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在研发 GPT-3 时，OpenAI 的研究人员尝试了不同的模型大小。他们采用了现有的 GPT-2 架构，并增加了参数数量。实验结果是产生了一种具有新的和非凡能力的模型，即
    GPT-3。虽然 GPT-2 在下游任务上显示出了一些零样本能力，但是当提供示例上下文时，GPT-3 能够执行更多的新颖任务。
- en: '[OpenAI researchers found it remarkable](https://arxiv.org/abs/2102.02503)
    that merely scaling the model parameters and the size of the training dataset
    led to such extraordinary advances. They are generally optimistic that these trends
    will continue even for models much larger than GPT-3, enabling ever-stronger learning
    models capable of few-shot or zero-shot learning just by fine-tuning on a small
    sample size.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI 的研究人员发现令人吃惊的是](https://arxiv.org/abs/2102.02503)，仅仅通过扩大模型参数和训练数据集的规模就能取得如此非凡的进步。他们普遍乐观地认为，即使是比
    GPT-3 大得多的模型，这些趋势也将继续，从而实现通过对小样本进行微调即可进行少量或零量学习的越来越强大的学习模型。'
- en: As you read this book, experts [estimate](https://arxiv.org/abs/2101.03961)
    that well over a trillion parameter-based language models are probably being built
    and deployed. We have entered the golden age of Large Language Models, and now
    it's time for you to become a part of it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当您阅读本书时，专家们[估计](https://arxiv.org/abs/2101.03961)可能已经构建和部署了超过一万亿个基于参数的语言模型。我们已经进入了大型语言模型的黄金时代，现在是您成为其中一员的时候了。
- en: 'GPT-3 has captured a lot of public attention. The MIT Technology Review considered
    GPT-3 as one of the [10 Breakthrough Technologies of 2021](https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/).
    Its sheer flexibility in performing generalized tasks with near-human efficiency
    and accuracy makes it so exciting. As an early adopter, Arram Sabeti tweeted (Figure
    1-2):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 已经吸引了大量的公众关注。《麻省理工科技评论》认为 GPT-3 是[2021 年的十项突破技术之一](https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/)。它在执行通用任务方面的出色灵活性，几乎接近人类的效率和准确性，使人们感到非常激动。作为早期采用者，Arram
    Sabeti 发推文如下（见图 1-2）：
- en: '![](img/image-0-1.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-1.jpg)'
- en: Figure 1-2\. Tweet from [Arram Sabeti](https://twitter.com/arram/status/1281258647566217216?lang=en)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-2\. 来自 [Arram Sabeti](https://twitter.com/arram/status/1281258647566217216?lang=en)
    的推文
- en: The API release created a paradigm shift in NLP and attracted many beta testers.
    Innovations and startups followed at lightning speed, with many commentators calling
    GPT-3 a “[fifth Industrial Revolution](https://twitter.com/gpt_three)”.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: API 发布在自然语言处理领域引起了范式转变，并吸引了许多测试者。创新和初创公司以惊人的速度涌现，许多评论者称 GPT-3 为“[第五次工业革命](https://twitter.com/gpt_three)”。
- en: Within just nine months of the launch of the API, according to OpenAI, people
    were building more than three hundred businesses with it. Despite this suddenness,
    some experts argue that the excitement isn’t exaggerated. Bakz Awan is a developer
    turned entrepreneur and influencer and one of the major voices in the OpenAI API
    developer community. He has a [YouTube channel “Bakz T. Future”](https://www.youtube.com/user/bakztfuture)
    and a [podcast](https://open.spotify.com/show/7qrWSE7ZxFXYe8uoH8NIFV?si=0e1170c9d7944c9b).
    Awan argues that GPT-3 and other models are actually “underhyped for how usable
    and friendly and fun and powerful they really are. It’s almost shocking.”
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 表示，API 推出仅九个月后，就有超过三百家企业开始使用它。尽管如此突然，一些专家认为这种兴奋并不夸张。Bakz Awan 是一位从开发者转型为企业家和影响者的人，也是
    OpenAI API 开发者社区的主要声音之一。他拥有一个 [YouTube 频道“Bakz T. Future”](https://www.youtube.com/user/bakztfuture)
    和一个 [播客](https://open.spotify.com/show/7qrWSE7ZxFXYe8uoH8NIFV?si=0e1170c9d7944c9b)。Awan
    认为，GPT-3 和其他模型实际上“在可用性、友好性、趣味性和强大性方面被低估了。这几乎是令人震惊的。”
- en: 'Daniel Erickson, CEO of Viable, which has a GPT-3 powered product, praises
    the model’s ability to extract insights from large datasets through what he calls
    prompt-based development:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Viable 的首席执行官 Daniel Erickson 称赞了该模型通过所谓的基于提示的开发从大型数据集中提取见解的能力：
- en: 'Companies going down that path cover use cases such as generating copy for
    ads and websites. The design philosophy is relatively simple: the company takes
    your data in, sends it over into a prompt, and displays the API-generated result.
    It solves a task that is easily done by a single API prompt and wraps a UI around
    that to deliver it to the users.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 走上这条道路的公司涵盖了生成广告和网站文案等用例。设计哲学相对简单：公司将您的数据传入，将其发送到提示中，并显示 API 生成的结果。它解决了一个可以由单个
    API 提示轻松完成的任务，并将 UI 包装起来传递给用户。
- en: The problem Erickson sees with this category of use cases is that it is already
    overcrowded, attracting many ambitious startup founders competing with similar
    services. Instead, Erickson recommends looking at another use case, as Viable
    did. Data-driven use cases are not as crowded as prompt-generation use cases,
    but they are more profitable and allow you to create a moat easily.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Erickson 觉得这类用例的问题在于已经过度拥挤，吸引了许多雄心勃勃的初创公司创始人竞争相似的服务。相反，Erickson 建议看看另一个用例，就像
    Viable 做的那样。基于数据的用例并没有像提示生成用例那样拥挤，但它们更有利可图，而且可以轻松创建一道护城河。
- en: The key, Erickson says, Erickson says, is to build a large dataset that you
    can keep adding to and that can provide potential insights. GPT-3 will help you
    extract valuable insights from it. At Viable, this was the model that let them
    monetize easily. “People pay much more for data than they do for prompt output,”
    Erickson explains.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Erickson 表示，关键是建立一个可以不断添加数据并提供潜在见解的大型数据集。GPT-3 将帮助您从中提取有价值的见解。在 Viable，这是让他们轻松实现货币化的模型。“人们为数据付出的比为提示输出付出的多得多，”
    Erickson 解释道。
- en: It should be noted that technological revolutions also bring controversies and
    challenges. GPT-3 is a powerful tool in the hands of anyone trying to create a
    narrative. Without great care and benevolent intentions, one such challenge we
    will face is curbing the attempts to use the algorithm to spread misinformation
    campaigns. Another one would be eradicating its use for generating mass quantities
    of low-quality digital content that will then pollute the information available
    on the internet. Yet another one is the limitations of its datasets that are filled
    with various kinds of bias, which can be amplified by this technology. We will
    look closer at these and more challenges in Chapter 6, along with discussing the
    various efforts by OpenAI to address them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，技术革命也会引发争议和挑战。GPT-3在任何试图创造叙述的人手中都是一个强大的工具。没有充分的关怀和善意意图，我们将面临的挑战之一就是抑制试图使用该算法传播误导性宣传活动。另一个挑战是根除其用于生成大量低质量数字内容的用途，这将污染互联网上的可用信息。还有一个挑战就是其数据集的局限性，这些数据集充满各种偏见，而这些偏见可能会被这项技术放大。在第6章中，我们将更详细地讨论这些挑战以及OpenAI为解决这些挑战所做的各种努力。
- en: Accessing the OpenAI API
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 访问OpenAI API
- en: As of 2021, the market has already produced several proprietary AI models with
    more parameters than GPT-3\. However, access to them is limited to a handful of
    people within the company's R&D walls, making it impossible to evaluate their
    performance on real-world NLP tasks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2021年，市场已经生产了几种拥有比GPT-3更多参数的专有 AI 模型。然而，对这些模型的访问仅限于公司研发部门内的少数人，这使得不可能评估它们在真实的自然语言处理任务中的性能。
- en: Another factor that makes GPT-3 accessible is its simple and intuitive “text-in,
    text-out” user interface. It doesn’t require complex gradient fine-tuning or updates,
    and you don’t need to be an expert to use it. This combination of scalable parameters
    and relatively open access makes GPT-3 the most exciting and arguably the most
    relevant language model to date.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使GPT-3变得易于访问的另一个因素是其简单直观的“文本输入，文本输出”用户界面。它不需要复杂的梯度微调或更新，你也不需要是一个专家来使用它。这种可扩展参数与相对开放的访问结合使得GPT-3成为迄今为止最令人兴奋的、可能也是最相关的语言模型。
- en: Due to GPT-3's extraordinary capabilities, there are significant risks in terms
    of security and misuse associated with making it open-source, which we will cover
    in the last chapter—considering that, OpenAI decided not to publicly release the
    source code of GPT-3 and came up with a unique, never seen before access sharing
    model via an API.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPT-3的非凡能力，开源存在着与安全和滥用相关的重大风险，在最后一章中我们将会讨论到—考虑到这一点，OpenAI决定不公开发布GPT-3的源代码，而是通过API提出了一个独特的、前所未见的访问共享模式。
- en: The company initially decided to release API access in the form of a limited
    beta user list. There was an application process where people had to fill in a
    form detailing their background and the reasons for requesting API access. Only
    the approved users were granted access to a private beta of the API with an interface
    called Playground.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 公司最初决定以有限的测试版用户列表形式发布API访问。人们必须填写一份详细说明其背景和申请API访问原因的申请表。只有被批准的用户才被授予对名为Playground的API私人测试版的访问权限。
- en: In its early days, the GPT-3 beta access waitlist consisted of tens of thousands
    of people. OpenAI swiftly managed the applications pouring in and adding developers
    in batches. It also closely monitored their activity and feedback about the API
    user experience to improve it continuously.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，GPT-3的测试版访问等待列表包含数以万计的人。OpenAI 迅速处理了涌入的申请，并分批添加开发人员。它还密切监测他们对API用户体验的活动和反馈，以不断改进。
- en: Thanks to the progress with safeguards, OpenAI removed the waitlist in November
    2021\. GPT-3 is now openly accessible via a [simple sign-in](https://openai.com/blog/api-no-waitlist/)[.](https://openai.com/blog/api-no-waitlist/)
    This is a significant milestone in the history of GPT-3 and a highly requested
    move by the Community. To get API access, go to the [sign-up page](https://beta.openai.com/signup)[,](https://beta.openai.com/signup)
    sign up for a free account, and start experimenting with it right away.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有了保障措施的进展，OpenAI于2021年11月取消了等待列表。现在可以通过[简单的注册](https://openai.com/blog/api-no-waitlist)公开访问GPT-3。这是GPT-3历史上的一个重要里程碑，也是社区强烈要求的一步。要获得API访问权限，只需到[注册页面](https://beta.openai.com/signup)，注册一个免费帐户，立即开始尝试。[。](https://beta.openai.com/signup)
- en: New users initially get a pool of free credits that allows them to freely experiment
    with the API. The number of credits is equivalent to creating text content as
    long as three average-length novels. After the free credits are used, users start
    paying for usage or, if they have a need, they can request additional credits
    from OpenAI API customer support.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 新用户最初会获得一定数量的免费积分，可以自由地尝试 API。积分数量相当于创作长度为三部普通长度小说的文本内容。使用完免费积分后，用户开始支付使用费用，或者如果有需求的话，可以向
    OpenAI API 客户支持请求额外的积分。
- en: OpenAI strives to ensure that API-powered applications are built responsibly.
    For that reason, it provides [tools](https://beta.openai.com/docs/guides/moderation),
    [best practices](https://beta.openai.com/docs/guides/safety-best-practices), and
    [usage guidelines](https://beta.openai.com/docs/usage-policies) to help developers
    bring their applications to production quickly and safely.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 致力于确保基于 API 的应用程序的负责任构建。为此，它提供了[工具](https://beta.openai.com/docs/guides/moderation)、[最佳实践](https://beta.openai.com/docs/guides/safety-best-practices)和[使用指南](https://beta.openai.com/docs/usage-policies)，以帮助开发人员快速而安全地将其应用程序投入生产。
- en: The company has also created [content guidelines](https://beta.openai.com/docs/usage-policies/content-guidelines)
    to clarify what kind of content the OpenAI API can be used to generate. To help
    developers ensure their applications are used for the intended purpose, prevent
    potential misuse, and adhere to the content guidelines, OpenAI offers a free content
    filter. OpenAI policy prohibits the use of the API in ways that do not adhere
    to the principles described in its [charter](https://openai.com/charter/), including
    content that promotes hate, violence, or self-harm, or that intends to harass,
    influence political processes, spread misinformation, spam content, and so on.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 公司还创建了[内容指南](https://beta.openai.com/docs/usage-policies/content-guidelines)，以明确
    OpenAI API 可用于生成哪种类型的内容。为了帮助开发人员确保其应用程序用于预期目的，防止潜在的滥用，并遵守内容指南，OpenAI 提供了免费的内容过滤器。OpenAI
    政策禁止将 API 用于违反其[宪章](https://openai.com/charter/)中描述的原则的方式，包括宣扬仇恨、暴力或自残的内容，或者意图骚扰、影响政治进程、传播错误信息、发送垃圾内容等。
- en: Once you have signed up for an OpenAI account, you can move on to Chapter 2,
    where we will discuss the different components of the API, the GPT-3 Playground,
    and how to use the API to the best of its abilities for different use cases.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您注册了 OpenAI 账户，您可以继续阅读第二章，在那里我们将讨论 API 的不同组成部分、GPT-3 游乐场，以及如何针对不同的使用情况最大限度地利用
    API 的能力。
