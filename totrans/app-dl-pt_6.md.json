["```py\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers,\n        batch_first=True)\n        self.output = nn.Linear(hidden_size, 1)\n        def forward(self, x, hidden):\n            out, hidden = self.rnn(x, hidden)\n            out = out.view(-1, self.hidden_size)\n            out = self.output(out)\n            return out, hidden\n```", "```py\nfor i in range(1, epochs+1):\n\n    hidden = None\n\n    for inputs, targets in batches:\n       pred, hidden = model(inputs, hidden)\n\n       loss = loss_function(pred, targets)\n       optimizer.zero_grad()\n       loss.backward()\n       optimizer.step()\n```", "```py\n    torch.manual_seed(0)\n    ```", "```py\nclass LSTM(nn.Module):\n    def __init__(self, char_length, hidden_size, n_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.lstm = nn.LSTM(char_length, hidden_size, n_layers,                             batch_first=True)\n        self.output = nn.Linear(hidden_size, char_length)\n\n    def forward(self, x, states):\n        out, states = self.lstm(x, states)\n        out = out.contiguous().view(-1, self.hidden_size)\n        out = self.output(out)\n\n        return out, states\n\n    def init_states(self, batch_size):\n        hidden = next(self.parameters()).data.new(self.n_layers,                  batch_size, self.hidden_size).zero_()\n        cell = next(self.parameters()).data.new(self.n_layers,                batch_size, self.hidden_size).zero_()\n        states = (hidden, cell)\n\n        return states\n```", "```py\nfor e in range(1, epochs+1):\n    states = model.init_states(n_seq)\n\n    for b in range(0, x.shape[1], seq_length):\n        x_batch = x[:,b:b+seq_length]\n\n        if b == x.shape[1] - seq_length:\n            y_batch = x[:,b+1:b+seq_length]\n            y_batch = np.hstack((y_batch, indexer[\".\"] *                       np.ones((y_batch.shape[0],1))))\n        else:\n            y_batch = x[:,b+1:b+seq_length+1]\n\n        x_onehot = torch.Tensor(index2onehot(x_batch))\n        y = torch.Tensor(y_batch).view(n_seq * seq_length)\n\n        pred, states = model(x_onehot, states)\n        loss = loss_function(pred, y.long())\n        optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        optimizer.step()\n```", "```py\nclass LSTM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, n_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_size, n_layers\n        self.output = nn.Linear(hidden_size, 1)\n\n    def forward(self, x, states):\n        out = self.embedding(x)\n        out, states = self.lstm(out, states)\n        out = out.contiguous().view(-1, self.hidden_size)\n        out = self.output(out)\n\n        return out, states\n```"]