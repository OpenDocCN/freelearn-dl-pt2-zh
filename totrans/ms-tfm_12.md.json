["```py\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-base- \n                    multilingual-uncased')\nsentences = [\n\"Transformers changed the [MASK] language processing\",\n\"Transformerlar [MASK] dil işlemeyi değiştirdiler\",\n\"ترنسفرمرها پردازش زبان [MASK] را تغییر دادند\"\n]\nfor sentence in sentences:\n    print(sentence)\n    print(unmasker(sentence)[0][\"sequence\"])\n    print(\"=\"*50)\n```", "```py\nTransformers changed the [MASK] language processing\ntransformers changed the english language processing\n==================================================\nTransformerlar [MASK] dil işlemeyi değiştirdiler\ntransformerlar bu dil islemeyi degistirdiler\n==================================================\nترنسفرمرها پردازش زبان [MASK] را تغییر دادند\nترنسفرمرها پردازش زبانی را تغییر دادند\n==================================================\n```", "```py\n    unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n    ```", "```py\n    sentences = [\n    \"Transformers changed the <mask> language processing\",\n    \"Transformerlar <mask> dil işlemeyi değiştirdiler\",\n    \"ترنسفرمرها پردازش زبان <mask\" را تغییر دادند\n    ]\n    ```", "```py\n    for sentence in sentences:\n      print(sentence)\n      print(unmasker(sentence)[0][\"sequence\"])\n      print(\"=\"*50)\n    ```", "```py\n    Transformers changed the <mask> language processing\n    Transformers changed the human language processing\n    ==================================================\n    Transformerlar <mask> dil işlemeyi değiştirdiler\n    Transformerlar, dil işlemeyi değiştirdiler\n    ================================================== ترنسفرمرها پردازش زبان [MASK] را تغییر دادند\n     ترنسفرمرها پردازش زبانی را تغییر دادند\n    ==================================================\n    ```", "```py\n    print(unmasker(\"Transformers changed the natural language processing. </s> Transformerlar <mask> dil işlemeyi değiştirdiler.\")[0][\"sequence\"])\n    ```", "```py\n    Transformers changed the natural language processing. Transformerlar doğal dil işlemeyi değiştirdiler.\n    ```", "```py\n    print(unmasker(\"Earth is a great place to live in. </s> زمین جای خوبی برای <mask> کردن است.\")[0][\"sequence\"])\n    ```", "```py\n    Earth is a great place to live in. زمین جای خوبی برای زندگی کردن است.\n    ```", "```py\n    from sentence_transformers import SentenceTransformer, util\n    model = SentenceTransformer(\"stsb-xlm-r-multilingual\")\n    ```", "```py\n    azeri_sentences = ['Pişik çöldə oturur',\n                  'Bir adam gitara çalır',\n                  'Mən makaron sevirəm',\n                  'Yeni film möhtəşəmdir',\n                  'Pişik bağda oynayır',\n                  'Bir qadın televizora baxır',\n                  'Yeni film çox möhtəşəmdir',\n                  'Pizzanı sevirsən?']\n    english_sentences = ['The cat sits outside',\n                 'A man is playing guitar',\n                 'I love pasta',\n                 'The new movie is awesome',\n                 'The cat plays in the garden',\n                 'A woman watches TV',\n                 'The new movie is so great',\n                 'Do you like pizza?']\n    ```", "```py\n    azeri_representation = model.encode(azeri_sentences)\n    english_representation = \\ \n    model.encode(english_sentences)\n    ```", "```py\n    results = []\n    for azeri_sentence, query in zip(azeri_sentences, azeri_representation):\n      id_, score = util.semantic_search(\n              query,english_representation)[0][0].values()\n      results.append({\n          \"azeri\": azeri_sentence,\n          \"english\": english_sentences[id_],\n          \"score\": round(score, 4)\n      })\n    ```", "```py\n    import pandas as pd\n    pd.DataFrame(results)\n    ```", "```py\nmodel = SentenceTransformer(\"LaBSE\")\n```", "```py\n    !pip install sentence_transformers datasets transformers umap-learn\n    ```", "```py\n    from datasets import load_dataset\n    import pandas as pd\n    data=load_dataset(\"xtreme\",\"tatoeba.rus\", \n                       split=\"validation\")\n    pd.DataFrame(data)[[\"source_sentence\",\"target_sentence\"]]\n    ```", "```py\n    from sentence_transformers import SentenceTransformer \n    model = SentenceTransformer(\"stsb-xlm-r-multilingual\")\n    K=30\n    q=data[\"source_sentence\"][:K] + data[\"target_sentence\"][:K]\n    emb=model.encode(q)\n    len(emb), len(emb[0])\n    Output: (60, 768)\n    ```", "```py\n    source_emb=model.encode(data[\"source_sentence\"])\n    target_emb=model.encode(data[\"target_sentence\"])\n    ```", "```py\n    from scipy import spatial\n    sims=[ 1 - spatial.distance.cosine(s,t) \\\n            for s,t in zip(source_emb, target_emb)]\n    plt.hist(sims, bins=100, range=(0.8,1))\n    plt.show()\n    ```", "```py\n    >>> np.mean(sims), np.std(sims)\n    (0.946, 0.082)\n    ```", "```py\n    from datasets import load_dataset\n    sms_spam = load_dataset(\"imdb\")\n    ```", "```py\n    imdb = imdb.shuffle()\n    ```", "```py\n    imdb_x = [x for x in imdb['train'][:1000]['text']]\n    labels = [x for x in imdb['train'][:1000]['label']]\n    import pandas as pd  \n    pd.DataFrame(imdb_x,\n                 columns=[\"text\"]).to_excel(\n                                     \"imdb.xlsx\",\n                                      index=None)\n    ```", "```py\n    pd.read_excel(\"KHMER.xlsx\")\n    ```", "```py\n    imdb_khmer = list(pd.read_excel(\"KHMER.xlsx\").text)\n    ```", "```py\n    from sklearn.model_selection import train_test_split \n    train_x, test_x, train_y, test_y, khmer_train, khmer_test = train_test_split(imdb_x, labels, imdb_khmer, test_size = 0.2, random_state = 1)\n    ```", "```py\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer(\"stsb-xlm-r-multilingual\")\n    ```", "```py\n    encoded_train = model.encode(train_x)\n    encoded_test = model.encode(test_x)\n    encoded_khmer_test = model.encode(khmer_test)\n    ```", "```py\n    import numpy as np\n    train_y = np.array(train_y)\n    test_y = np.array(test_y)\n    ```", "```py\n    import tensorflow as tf\n    input_ = tf.keras.layers.Input((768,))\n    classification = tf.keras.layers.Dense(\n                           1,\n                          activation=\"sigmoid\")(input_)\n    classification_model = \\\n               tf.keras.Model(input_, classification)\n    classification_model.compile(\n             loss=tf.keras.losses.BinaryCrossentropy(),\n             optimizer=\"Adam\",\n             metrics=[\"accuracy\", \"Precision\", \"Recall\"])\n    ```", "```py\n    classification_model.fit(\n                         x = encoded_train,\n                         y = train_y,\n                 validation_data=(encoded_test, test_y),\n                         epochs = 10)\n    ```", "```py\n    val_loss: 0.5226\n    val_accuracy: 0.7150 \n    val_precision: 0.7600\n    val_recall: 0.6972\n    ```", "```py\n    classification_model.evaluate(x = encoded_khmer_test,\n                                  y = test_y)\n    ```", "```py\n    loss: 0.5949\n    accuracy: 0.7250\n    precision: 0.7014\n    recall: 0.8623\n    ```", "```py\n    from torch.nn.functional import softmax\n    from transformers import\\\n        MT5ForConditionalGeneration, MT5Tokenizer\n    model_name = \"alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli\"\n    tokenizer = MT5Tokenizer.from_pretrained(model_name)\n    model = MT5ForConditionalGeneration\\\n        .from_pretrained(model_name)\n    ```", "```py\n    sequence_to_classify = \\\n        \"Wen werden Sie bei der nächsten Wahl wählen? \"\n    candidate_labels = [\"spor\", \"ekonomi\", \"politika\"]\n    hypothesis_template = \"Dieses Beispiel ist {}.\"\n    ```", "```py\n    ENTAILS_LABEL = \"_0\"\n    NEUTRAL_LABEL = \"_1\"\n    CONTRADICTS_LABEL = \"_2\"\n    label_inds = tokenizer.convert_tokens_to_ids([\n                               ENTAILS_LABEL,\n                               NEUTRAL_LABEL,\n                               CONTRADICTS_LABEL])\n    ```", "```py\n    def process_nli(premise, hypothesis):\n        return f'xnli: premise: {premise} hypothesis: {hypothesis}'\n    ```", "```py\n    pairs =[(sequence_to_classify,\\  \n          hypothesis_template.format(label)) for label in\n          candidate_labels]\n    seqs = [process_nli(premise=premise,\n                        hypothesis=hypothesis)\n                        for premise, hypothesis in pairs]\n    ```", "```py\n    print(seqs)\n    ['xnli: premise: Wen werden Sie bei der nächsten Wahl wählen?  hypothesis: Dieses Beispiel ist spor.',\n    'xnli: premise: Wen werden Sie bei der nächsten Wahl wählen?  hypothesis: Dieses Beispiel ist ekonomi.',\n    'xnli: premise: Wen werden Sie bei der nächsten Wahl wählen?  hypothesis: Dieses Beispiel ist politika.']\n    ```", "```py\n    inputs = tokenizer.batch_encode_plus(seqs,  \n             return_tensors=\"pt\", padding=True)\n    out = model.generate(**inputs, output_scores=True, \n            return_dict_in_generate=True,num_beams=1)\n    ```", "```py\n    scores = out.scores[0]\n    scores = scores[:, label_inds]\n    ```", "```py\n    >>> print(scores)\n    tensor([[-0.9851,  2.2550, -0.0783],\n            [-5.1690, -0.7202, -2.5855],\n            [ 2.7442,  3.6727,  0.7169]])\n    ```", "```py\n    entailment_ind = 0\n    contradiction_ind = 2\n    entail_vs_contra_scores = scores[:, [entailment_ind, contradiction_ind]]\n    ```", "```py\n    entail_vs_contra_probas = softmax(entail_vs_contra_scores, dim=1)\n    ```", "```py\n    >>> print(entail_vs_contra_probas)\n    tensor([[0.2877, 0.7123],\n            [0.0702, 0.9298],\n            [0.8836, 0.1164]])\n    ```", "```py\n    entail_scores = scores[:, entailment_ind]\n    entail_probas = softmax(entail_scores, dim=0)\n    ```", "```py\n    >>> print(entail_probas)\n    tensor([2.3438e-02, 3.5716e-04, 9.7620e-01])\n    ```", "```py\n    >>> print(dict(zip(candidate_labels, entail_probas.tolist())))\n    {'ekonomi': 0.0003571564157027751,\n    'politika': 0.9762046933174133,\n    'spor': 0.023438096046447754}\n    ```", "```py\n    from transformers import BertTokenizerFast\n    tokenizer = BertTokenizerFast.from_pretrained(\n                       \"dbmdz/bert-base-turkish-uncased\")\n    from transformers import BertForSequenceClassification\n    model = \\ BertForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-uncased\",num_labels=NUM_LABELS, \n                         id2label=id2label, \n                         label2id=label2id)\n    ```", "```py\n    from transformers import \\ BertForSequenceClassification, AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n                       \"bert-base-multilingual-uncased\")\n    model = BertForSequenceClassification.from_pretrained(\n                        \"bert-base-multilingual-uncased\",\n                         num_labels=NUM_LABELS,\n                         id2label=id2label,\n                         label2id=label2id)\n    ```", "```py\n    from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n    tokenizer = AutoTokenizer.from_pretrained(\n                                   \"xlm-roberta-base\")\n    model = XLMRobertaForSequenceClassification\\\n                   .from_pretrained(\"xlm-roberta-base\",\n                   num_labels=NUM_LABELS,\n                  id2label=id2label,label2id=label2id)\n    ```"]