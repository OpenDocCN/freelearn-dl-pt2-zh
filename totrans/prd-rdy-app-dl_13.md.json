["```py\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\ntflite_quant_model = converter.convert()\n```", "```py\nimport tensorflow as tf\n# A set of data for estimating the range of numbers that the inference requires\nrepresentative_dataset = …\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8  # or tf.uint8\nconverter.inference_output_type = tf.int8  # or tf.uint8\ntflite_quant_model = converter.convert()\n```", "```py\nimport torch\nmodel = …\nquantized_model = torch.quantization.quantize_dynamic(\n    model,  # the original model\n    qconfig_spec={torch.nn.Linear},  # a set of layers to quantize\n    dtype=torch.qint8)  # data type which the quantized tensors will be\n```", "```py\nimport torch\n# A model with few layers\nclass OriginalModel(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts the incoming floating point tensors into a quantized tensor\n        self.quant = torch.quantization.QuantStub()\n        self.linear = torch.nn.Linear(10, 20)\n        # DeQuantStub converts the given quantized tensor into a tensor in floating point\n        self.dequant = torch.quantization.DeQuantStub()\n    def forward(self, x):\n        # using QuantStub and DeQuantStub operations, we can indicate the region for quantization\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.linear(x)\n        x = self.dequant(x)\n        return x\n```", "```py\n# model is instantiated and trained\nmodel_fp32 = OriginalModel()\n…\n# Prepare the model for static quantization\nmodel_fp32.eval()\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32)\n# Determine the best quantization settings by calibrating the model on a representative dataset.\ncalibration_dataset = …\nmodel_fp32_prepared.eval()\nfor data, label in calibration_dataset:\n    model_fp32_prepared(data)\n```", "```py\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n```", "```py\nimport tensorflow_model_optimization as tfmot\n# A TF model\nmodel = … \nq_aware_model = tfmot.quantization.keras.quantize_model(model)\nq_aware_model.compile(\n              optimizer=...,\n              loss=...,\n              metrics=['accuracy'])\nq_aware_model.fit(...)\n```", "```py\nmodel_fp32 = OriginalModel()\n# model must be set to train mode for QAT\nmodel_fp32.train()\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n# train the model\nfor data, label in train_dataset:\n    pred = model_fp32_prepared(data)\n    ...\n# Generate quantized version of the trained model\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n```", "```py\nimport tensorflow_model_optimization as tfmot\n# A trained model to compress\ntf_model = ...\nCentroidInitialization = tfmot.clustering.keras.CentroidInitialization\nclustering_params = {\n  'number_of_clusters': 10,\n  'cluster_centroids_init': CentroidInitialization.LINEAR\n}\nclustered_model = tfmot.clustering.keras.cluster_weights(tf_model, **clustering_params)\n```", "```py\nfinal_model = tfmot.clustering.keras.strip_clustering(clustered_model) \n```", "```py\nfrom torch.nn import Module\nclass SampleModel(Module):\n# in the case of PyTorch Lighting, we inherit pytorch_lightning.LightningModule class\n  def __init__(self):\n    self.layer = …\n    self.weights_cluster = … # cluster index for each weight\n    self.weights_mapping = … # mapping from a cluster index to a centroid value\n  def forward(self, input):\n    if self.training: # in training mode\n      output = self.layer(input)\n    else: # in eval mode\n      # update weights of the self.layer by reassigning each value based on self.weights_cluster and self.weights_mapping\n    output = self.layer(input)\n    return output\ndef cluster_weights(self):\n  # cluster weights of the layer \n  # construct a mapping from a cluster index to a centroid value and store at self.weights_mapping\n  # find cluster index for each weight value and store at self.weights_cluster\n  # drop the original weights to reduce the model size\n# First, we instantiate a model to train\nmodel = SampleModel()\n# train the model\n…\n# perform weight sharing\nmodel.cluster_weights()\nmodel.eval()\n```", "```py\nimport tensorflow_model_optimization as tfmot\n```", "```py\n# data and configurations for training\nx_train, y_train, x_text, y_test, x_valid, y_valid,  num_examples_train, num_examples_test, num_examples_valid  = …\nbatch_size = ...\nend_step = np.ceil(num_examples_train / batch_size).astype(np.int32) * epochs\n# pruning configuration\npruning_params = {\n      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.3,\nfinal_sparsity=0.5,\nbegin_step=0,\nend_step=end_step)}\n#  Prepare a model that will be pruned\nmodel = ...\nmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n```", "```py\npruning_params = {\n      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100) }\n```", "```py\nmodel_for_pruning.compile(…)\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.fit(x_train, y_train,\n    batch_size=batch_size, epochs=epochs,     validation_data=(x_valid, y_vallid),\n    callbacks=callbacks)\n```", "```py\nfinal_tf_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n```", "```py\ndef apply_pruning_to_dense(layer):\n    if isinstance(layer, tf.keras.layers.Dense):\n        return tfmot.sparsity.keras.prune_low_magnitude(layer)\n    return layer\nmodel_for_pruning = tf.keras.models.clone_model(model, clone_function=apply_pruning_to_dense)\n```", "```py\n# model is instantiated and trained\nmodel = …\nparameters_to_prune = (\n    (model.conv, 'weight'),\n    (model.fc, 'weight')\n)\nprune.global_unstructured(\n    parameters_to_prune,\n    pruning_method=prune.L1Unstructured, # L1-norm\n    amount=0.2\n)\n```"]