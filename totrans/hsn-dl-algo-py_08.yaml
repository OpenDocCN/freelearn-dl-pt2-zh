- en: Demystifying Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) are one of the most commonly used
    deep learning algorithms. They are widely used for image-related tasks, such as
    image recognition, object detection, image segmentation, and more. The applications
    of CNNs are endless, ranging from powering vision in self-driving cars to the
    automatic tagging of friends in our Facebook pictures. Although CNNs are widely
    used for image datasets, they can also be applied to textual datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at CNNs in detail and get the hang of CNNs and
    how they work. First, we will learn about CNNs intuitively, and then we will deep-dive
    into the underlying math behind them. Following this, we will come to understand
    how to implement a CNN in TensorFlow step by step. Moving ahead, we will explore
    different types of CNN architectures such as LeNet, AlexNet, VGGNet, and GoogleNet.
    At the end of the chapter, we will study the shortcomings of CNNs and how these
    can be resolved using Capsule networks. Also, we will learn how to build Capsule
    networks using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are CNNs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The math behind CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing CNNs in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different CNN architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capsule networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building Capsule networks in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are CNNs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A CNN, also known as a **ConvNet**, is one of the most widely used deep learning
    algorithms for computer vision tasks. Let''s say we are performing an image-recognition
    task. Consider the following image. We want our CNN to recognize that it contains
    a horse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c930822-cdbf-43ac-a5f2-2c8418f3039f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How can we do that? When we feed the image to a computer, it basically converts
    it into a matrix of pixel values. The pixel values range from 0 to 255, and the
    dimensions of this matrix will be of [*image* *width* x *image height* x *number
    of channels*]. A grayscale image has one channel, and colored images have three
    channels **red, green, and blue** (**RGB**).
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a colored input image with a width of 11 and a height of 11,
    that is 11 x 11, then our matrix dimension would be of *[11 x 11 x 3]*. As you
    can see in *[11 x 11 x 3]*, 11 x 11 represents the image width and height and
    3 represents the channel number, as we have a colored image. So, we will have
    a 3D matrix.
  prefs: []
  type: TYPE_NORMAL
- en: But it is hard to visualize a 3D matrix, so, for the sake of understanding,
    let's consider a grayscale image as our input. Since the grayscale image has only
    one channel, we will get a 2D matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, the input grayscale image will be converted
    into a matrix of pixel values ranging from 0 to 255, with the pixel values representing
    the intensity of pixels at that point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d93e2fb-b07f-4165-a395-80c9e0dbc857.png)'
  prefs: []
  type: TYPE_IMG
- en: The values given in the input matrix are just arbitrary values for our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, now we have an input matrix of pixel values. What happens next? How does
    the CNN come to understand that the image contains a horse? CNNs consists of the
    following three important layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of these three layers, the CNN recognizes that the image contains
    a horse. Now we will explore each of these layers in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The convolutional layer is the first and core layer of the CNN. It is one of
    the building blocks of a CNN and is used for extracting important features from
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: We have an image of a horse. What do you think are the features that will help
    us to understand that this is an image of a horse? We can say body structure,
    face, legs, tail, and so on. But how does the CNN understand these features? This
    is where we use a convolution operation that will extract all the important features
    from the image that characterize the horse. So, the convolution operation helps
    us to understand what the image is all about.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what exactly is this convolution operation? How it is performed? How does
    it extract the important features? Let's look at this in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, every input image is represented by a matrix of pixel values. Apart
    from the input matrix, we also have another matrix called the **filter matrix**.
    The filter matrix is also known as a **kernel**, or simply a **filter**, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdda8d40-64ac-4f3e-ad41-129f0ecb9c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We take the filter matrix, slide it over the input matrix by one pixel, perform
    element-wise multiplication, sum up the results, and produce a single number.
    That''s pretty confusing, isn''t it? Let''s understand this better with the aid
    of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9169595-1bfc-4898-8095-46e20d9e0efb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the previous diagram, we took the filter matrix and placed
    it on top of the input matrix, performed element-wise multiplication, summed their
    results, and produced the single number. This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bbdbdf4-a60e-4ff7-844a-e92b578c1518.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we slide the filter over the input matrix by one pixel and perform the
    same steps, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2a9f9f2-de3c-4bc6-a051-18eded4470d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f13d7db8-4d70-456e-b553-174debcd9340.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, we slide the filter matrix by one pixel and perform the same operation,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6ecbe08-83fe-47b9-811f-9f65df0d65a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98f7173b-8d73-4abf-a4bd-1b016a30b82d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, again, we slide the filter matrix over the input matrix by one pixel and
    perform the same operation, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4725aa5-08ef-4f96-9ded-b761cfc4e5a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f72ca299-aeaf-442d-97e8-102631737f49.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay. What are we doing here? We are basically sliding the filter matrix over
    the entire input matrix by one pixel, performing element-wise multiplication and
    summing their results, which creates a new matrix called a **feature map** or
    **activation map**. This is called the **convolution operation**.
  prefs: []
  type: TYPE_NORMAL
- en: As we've learned, the convolution operation is used to extract features, and
    the new matrix, that is, the feature maps, represents the extracted features.
    If we plot the feature maps, then we can see the features extracted by the convolution
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the actual image (the input image) and the convolved
    image (the feature map). We can see that our filter has detected the edges from
    the actual image as a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e7080aa-e990-41b5-8d2a-35f73e337d15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Various filters are used for extracting different features from the image.
    For instance, if we use a sharpen filter, ![](img/c26a4782-aa88-4916-a1bf-341300987e17.png),
    then it will sharpen our image, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/507a9d26-2aa8-4443-a4ea-6ba2adc37947.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we have learned that with filters, we can extract important features
    from the image using the convolution operation. So, instead of using one filter,
    we can use multiple filters for extracting different features from the image,
    and produce multiple feature maps. So, the depth of the feature map will be the
    number of filters. If we use seven filters to extract different features from
    the image, then the depth of our feature map will be seven:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75d06dec-64f7-4888-ad46-b0366fcb90df.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, we have learned that different filters extract different features from
    the image. But the question is, how can we set the correct values for the filter
    matrix so that we can extract the important features from the image? Worry not!
    We just initialize the filter matrix randomly, and the optimal values of the filter
    matrix, with which we can extract the important features from the images, will
    be learned through backpropagation. However, we just need to specify the size
    of the filter and the number of filters we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just learned how a convolution operation works. We slide over the input
    matrix with the filter matrix by one pixel and perform the convolution operation.
    But we can't only slide over the input matrix by one pixel. We can also slide
    over the input matrix by any number of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The number of pixels we slide over the input matrix by the filter matrix is
    called a **stride**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set the stride to 2, then we slide over the input matrix with the filter
    matrix by two pixels. The following diagram shows a convolution operation with
    a stride of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11bc6120-28bc-4d19-9ad4-5372e5bda4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: But how do we choose the stride number? We just learned that a stride is the
    number of pixels along that we move our filter matrix. So, when the stride is
    set to a small number, we can encode a more detailed representation of the image
    than when the stride is set to a large number. However, a stride with a high value
    takes less time to compute than one with a low value.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the convolution operation, we are sliding over the input matrix with a
    filter matrix. But in some cases, the filter does not perfectly fit the input
    matrix. What do we mean by that? For example, let''s say we are performing a convolution
    operation with a stride of 2\. There exists a situation where, when we move our
    filter matrix by two pixels, it reaches the border and the filter matrix does
    not fit the input matrix. That is, some part of our filter matrix is outside the
    input matrix, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7a24559-4882-4df0-b801-02065e8b3fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we perform padding. We can simply pad the input matrix with zeros
    so that the filter can fit the input matrix, as shown in the following diagram.
    Padding with zeros on the input matrix is called **same padding** or **zero padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/718bf23e-c425-4f9c-a2e6-6d9e37d070b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of padding them with zeros, we can also simply discard the region of
    the input matrix where the filter doesn''t fit in. This is called **valid padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3349d9e-becb-4495-8655-71014560c081.png)'
  prefs: []
  type: TYPE_IMG
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay. Now, we are done with the convolution operation. As a result of the convolution
    operation, we have some feature maps. But the feature maps are too large in dimension.
    In order to reduce the dimensions of feature maps, we perform a pooling operation.
    This reduces the dimensions of the feature maps and keeps only the necessary details
    so that the amount of computation can be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to recognize a horse from the image, we need to extract and keep
    only the features of the horse; we can simply discard unwanted features, such
    as the background of the image and more. A pooling operation is also called a
    **downsampling** or **subsampling** operation, and it makes the CNN translation
    invariant. Thus, the pooling layer reduces spatial dimensions by keeping only
    the important features.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation will not change the depth of the feature maps; it will
    only affect the height and width.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of pooling operations, including max pooling, average
    pooling, and sum pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In max pooling, we slide over the filter on the input matrix and simply take
    the maximum value from the filter window, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faf168e5-c7f9-438b-9dd3-70a27d03a408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the name suggests, in average pooling, we take the average value of the
    input matrix within the filter window, and in sum pooling, we sum all the values
    of the input matrix within the filter window, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a660315a-b637-4f49-8ed6-e118993a1386.png)'
  prefs: []
  type: TYPE_IMG
- en: Max pooling is one of the most commonly used pooling operations.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned how convolutional and pooling layers work. A CNN can have
    multiple convolutional layers and pooling layers. However, these layers will only
    extract features from the input image and produce the feature map; that is, they
    are just the feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given any image, convolutional layers extract features from the image and produce
    a feature map. Now, we need to classify these extracted features. So, we need
    an algorithm that can classify these extracted features and tell us whether the
    extracted features are the features of a horse, or something else. In order to
    make this classification, we use a feedforward neural network. We flatten the
    feature map and convert it into a vector, and feed it as an input to the feedforward
    network. The feedforward network takes this flattened feature map as an input,
    applies an activation function, such as sigmoid, and returns the output, stating
    whether the image contains a horse or not; this is called a fully connected layer
    and is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/affa9ab7-63a4-45f0-865d-a1bbc6c75062.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of a CNN is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4e34a47-e0c6-45d8-ad8c-0e03396efb57.png)'
  prefs: []
  type: TYPE_IMG
- en: As you will notice, first we feed the input image to the convolutional layer,
    where we apply the convolution operation to extract important features from the
    image and create the feature maps. We then pass the feature maps to the pooling
    layer, where the dimensions of the feature maps will be reduced. As shown in the
    previous diagram, we can have multiple convolutional and pooling layers, and we
    should also note that the pooling layer does not necessarily have to be there
    after every convolutional layer; there can be many convolutional layers followed
    by a pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: So, after the convolutional and pooling layers, we flatten the resultant feature
    maps and feed it to a fully connected layer, which is basically a feedforward
    neural network that classifies the given input image based on the feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: The math behind CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have intuitively understood how a CNN works. But how exactly does
    a CNN learn? How does it find the optimal values for the filter using backpropagation?
    To answer this question, we will explore mathematically how the CNN works. Unlike
    in the [Chapter 5](c8326380-001a-4ece-8a14-b0a1ea0010b5.xhtml), *Improvements
    to the RNN*, the math behind a CNN is pretty simple and very interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with the forward propagation. We have already seen how forward
    propagation works and how a CNN classifies the given input image. Let''s frame
    this mathematically. Let''s consider an input matrix, *X*, and filter, *W*, with
    values shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77c1def1-3668-4604-ac80-273dfb885ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: First, let's familiarize ourselves with the notations. Whenever we write ![](img/d683705e-7017-43b2-8fd2-31377943d59c.png),
    it implies the element in the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)
    row and the ![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png) column of the input
    matrix. The same applies to the filter and output matrix; that is, ![](img/d34f998a-542d-4bd0-b85e-3b6ede8d1626.png)
    and ![](img/401d921f-f24e-41ac-991c-3bf09f3573d2.png) represent the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png) row
    and the ![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png) column value in the
    filter and output matrix, respectively. In the previous figure, ![](img/4432ea26-b8cd-4fff-bcaa-80460506ea8d.png)
    = ![](img/e0c53dec-a173-4d9b-b7fe-332690e0ceae.png), that is, ![](img/0fa58a79-c956-40d3-8832-cc2c6d53d6e9.png)
    is the element in the first row and first column of the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, we take the filter, slide it over the input
    matrix, perform a convolution operation, and produce the output matrix (the feature
    map) just as we learned in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d7956a5-7923-4c51-a5c1-7c0e5fcb7e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, all the values in the output matrix (feature map) are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f7cb78b-c6e7-41a0-95de-d3cb4b3af679.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f9c00bd1-1088-409e-aa64-a1b4c452bedd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d40ab13d-bd4d-45fd-b80d-766865b9a3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ed7a8206-de1d-4958-be08-9056f1feee1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, so we know this is how a convolution operation is performed and how the
    output is computed. Can we represent this in a simple equation? Let''s say we
    have an input image, *X,* with a width of *W* and a height of *H*, and the filter
    of size *P* x *Q*, then the convolution operation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20f207a6-3090-4335-8e1f-c124e2aceeb1.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation basically represents how the output, ![](img/d17ac902-2af3-4c84-b127-3ba512459135.png)
    (that is, the element in the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)
    row and the ![](img/fbd568a7-6a06-437d-a179-d4c530797237.png) column of the output
    matrix), is computed using a convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the convolution operation is performed, we feed the result,![](img/d17ac902-2af3-4c84-b127-3ba512459135.png),
    to a feedforward network, ![](img/a9e5a9e6-0ec1-4ac5-a813-442c18dcabd9.png), and
    predict the output, ![](img/9913c405-9178-4da0-930c-5afa513240ca.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b833c3e0-2ef9-4d5d-b1bf-4d7dc11a450d.png)'
  prefs: []
  type: TYPE_IMG
- en: Backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have predicted the output, we compute the loss, ![](img/00f83254-228d-43a5-99df-8a8cf86d6931.png).
    We use the mean squared error as the loss function, that is, the mean of the squared
    difference between the actual output, ![](img/7b4f6183-c77d-4521-bb70-849f7bf94f51.png),
    and the predicted output, ![](img/21977035-1547-4a83-a0f4-a35d0e227cac.png), which
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8515dd01-439a-4cec-a247-73861ef4b010.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will see how can we use backpropagation to minimize the loss ![](img/6c8b8da5-079e-4817-8a65-336e26f89086.png).
    In order to minimize the loss, we need to find the optimal values for our filter
    *W*. Our filter matrix consists of four values, *w1*, *w2*, *w3*, and *w4*. To
    find the optimal filter matrix, we need to calculate the gradients of our loss
    function with respect to all these four values. How do we do that?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s recollect the equations of the output matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73f8263a-b92c-4929-ac10-7c461980d67b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b88649d8-8201-40e2-8672-2c53962903f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9cf41a2f-3084-4bfd-87af-56233d26714d.png)'
  prefs: []
  type: TYPE_IMG
- en: Don't get intimidated by the upcoming equations; they are actually pretty simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s calculate gradients with respect to ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png)*.*
    As you can see, ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png) appears in all
    the output equations; we calculate the partial derivatives of the loss with respect
    to ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63f9fe80-68b3-4d10-8565-1aed0af0fb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we calculate the partial derivative of the loss with respect to
    the ![](img/b7ea87fb-dbe7-4ca2-8006-eb5d0227fca7.png) weight as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c16a2cb-18bf-4d51-b67b-49b6eca0ae53.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of loss with respect to the ![](img/cc642919-e0bc-40be-ad22-3f8bd770015c.png)
    weights, are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a740913-b834-4054-908e-7febd562836d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of loss with respect to the ![](img/761a3045-a3d4-42c8-9f9b-38183d0bec79.png)
    weights, are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/986e1d83-6913-4c43-8bfc-670cd50e47b4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/51967733-80f6-45a7-b857-71f6bf2a5ada.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in a nutshell, our final equations for the gradients of loss with respect
    to all the weights are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/51967733-80f6-45a7-b857-71f6bf2a5ada.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It turns out that computing the derivatives of loss with respect to the filter
    matrix is very simple—it is just another convolution operation. If we look at
    the preceding equations closely, we will notice they look like the result of a
    convolution operation between the input matrix and the gradient of the loss with
    respect to the output as a filter matrix, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00d7add4-0916-49a8-9897-77e206151643.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, let''s see how the gradients of loss with respect to weight ![](img/ebe3ea15-3942-46c9-b837-2369257cad35.png)
    are computed by the convolution operation between the input matrix and the gradients
    of loss with respect to the output as a filter matrix, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d8b588c-5392-4ca1-9d78-7cb614c10765.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we understand that computing the gradients of loss with respect to the filter
    (that is, weights) is just the convolution operation between the input matrix
    and the gradient of loss with respect to the output as a filter matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from calculating the gradients of loss with respect to the filter, we
    also need to calculate the gradients of loss with respect to an input. But why
    do we do that? Because it is used for calculating the gradients of the filters
    present in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our input matrix consists of nine values, from ![](img/1a62ae19-7c7c-462f-8f2d-6a34dadb3021.png)
    to ![](img/89e4142d-a6fe-4b74-8fc9-5a030c915ac0.png), so we need to calculate
    the gradients of loss with respect to all these nine values. Let''s recollect
    how the output matrix is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73f8263a-b92c-4929-ac10-7c461980d67b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b88649d8-8201-40e2-8672-2c53962903f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9cf41a2f-3084-4bfd-87af-56233d26714d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, ![](img/fd93e013-64ad-487b-894b-19c26344a3cc.png) is present
    only in ![](img/56e5404b-91dd-49f5-946b-2a447d893e41.png), so we can calculate
    the gradients of loss with respect to ![](img/cf1784c6-b3a1-4629-828d-e88c17764212.png)
    alone, as other terms would be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdc411e1-040b-45a8-8aad-21ba41f61d86.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s calculate the gradients with respect to ![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png)*;*
    as ![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png) is present in only ![](img/b3a81d52-41a7-4d87-8012-881d3559401c.png)
    and ![](img/4c0f720d-ad3f-43a7-b52d-76f3d5083e02.png), we calculate the gradients
    with respect to ![](img/70fd3562-41b7-4080-b3d0-ac23f22d37ed.png) and ![](img/9f4c6517-1607-42a0-8d30-ac53f753119c.png)
    alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44bc61b2-09bf-4225-9468-0ac63294909a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/efae36f7-eb83-4461-8a1d-ed9ac9468b27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a very similar way, we calculate the gradients of loss with respect to all
    the inputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3721b26c-ea6f-4830-ab9b-be534e05ba6a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/31638cdf-1ee2-4670-9a53-2e0edf004b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7c44f895-a2e7-4d9c-9549-cddb214fb2d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9f822bc7-39ea-4bdb-a4b4-b3373a8c0965.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/87fb3292-de91-4957-acf4-f18a5f8bac3f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/58625695-d3df-4add-8a30-79bf551a8ed4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5060bb60-9610-4752-a75b-7bf372964d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Just as we represented the gradients of the loss with respect to the weights
    using the convolution operation, can we also do the same here? It turns out that
    the answer is yes. We can actually represent the preceding equations, that is,
    the gradients of loss with respect to the inputs, using a convolution operation
    between the filter matrix as an input matrix and the gradients of loss with respect
    to the output matrix as a filter matrix. But the trick is that, instead of using
    the filter matrix directly, we rotate them 180 degrees and, also, instead of performing
    convolution, we perform full convolution. We are doing this so that we can derive
    the previous equations using a convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows what the kernel rotated by 180 degrees looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e433f90c-42dc-4142-8f5e-317acf3308da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, so what is full convolution? In the same way as a convolution operation,
    in full convolution, we use a filter and slide it over the input matrix, but the
    way we slide the filter is different from the convolution operation we looked
    at before. The following figure shows how full convolution operations work. As
    we can see, the shaded matrix represents the filter matrix and the unshaded one
    represents the input matrix; we can see how the filter slides over the input matrix
    step by step, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51512ba8-b3ab-40d5-9792-229050fabd1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can say that the gradient of loss with respect to the input matrix can
    be calculated using a full convolution operation between a filter rotated by 180
    degrees as the input matrix and the gradient of the loss with respect to the output
    as a filter matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e9eb3a4-7b6b-403c-9ed9-368662189258.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, as shown in the following figure, we will notice how the gradients
    of loss with respect to the input, ![](img/0425842a-097d-4394-8a1b-4f81719f427f.png),
    is computed by the full convolution operation between the filter matrix rotated
    by 180 degrees, and the gradients of loss with respect to an output matrix as
    a filter matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39d37a5e-1442-43a1-bad9-c70594773ad1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we understand that computing the gradients of loss with respect to the
    input is just the full convolution operation. So, we can say that backpropagation
    in CNN is just another convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CNN in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will learn how to build a CNN using TensorFlow. We will use the MNIST
    handwritten digits dataset and understand how a CNN recognizes handwritten digits,
    and we will also visualize how the convolutional layers extract important features
    from the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Defining helper functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we define the functions for initializing weights and bias, and for performing
    the convolution and pooling operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the weights by drawing from a truncated normal distribution. Remember,
    the weights are actually the filter matrix that we use while performing the convolution
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the bias with a constant value of, say, `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function called convolution using `tf.nn.conv2d()`, which actually
    performs the convolution operation; that is, the element-wise multiplication of
    the input matrix (`x`) by the filter (`W`) with a the stride of `1` and the same
    padding. We set `strides = [1,1,1,1]`. The first and last values of strides are
    set to `1`, which implies that we don''t want to move between training samples
    and different channels. The second and third values of `strides` are also set
    to `1`, which implies that we move the filter by `1` pixel in both the height
    and width direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function called `max_pooling`, using `tf.nn.max_pool()` to perform
    the pooling operation. We perform max pooling with a `stride` of `2` and the same
    `padding` and `ksize` implies our pooling window shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Define the placeholders for the input and output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `placeholder` for the input image is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `placeholder` for a reshaped input image is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `placeholder` for the output label is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Defining the convolutional network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our network architecture consists of two convolutional layers. Each convolutional
    layer is followed by one pooling layer, and we use a fully connected layer that
    is followed by an output layer; that is, `conv1->pooling->conv2->pooling2->fully
    connected layer-> output layer`.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define the first convolutional layer and pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: The weights are actually the filters in the convolutional layers. So, the weight
    matrix will be initialized as `[ filter_shape[0], filter_shape[1], number_of_input_channel,
    filter_size ]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a `5 x 5` filter. Since we use grayscale images, the number of input
    channels will be `1` and we set the filter size as `32`. So, the weight matrix
    of the first convolution layer will be `[5,5,1,32]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the bias is just the filter size, which is `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the first convolution operation with ReLU activations followed by max
    pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define the second convolution layer and pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the second convolutional layer takes the input from the first convolutional
    layer, which has 32-channel output, the number of input channel, to the second
    convolutional layer becomes 32 and we use the 5 x 5 filter with a filter size
    of `64`. Thus, the weight matrix for the second convolutional layer becomes `[5,5,32,64]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the bias is just the filter size, which is `64`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the second convolution operation with ReLU activations, followed by
    max pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After two convolution and pooling layers, we need to flatten the output before
    feeding it to the fully connected layer. So, we flatten the result of the second
    pooling layer and feed it to the fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flatten the result of the second pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define the weights and bias for the fully connected layer. We know that
    we set the shape of the weight matrix as `[number of neurons in the current layer,
    number of neurons layer in the next layer]`. This is because the shape of the
    input image becomes `7x7x64` after flattening and we use `1024` neurons in the
    hidden layer. The shape of the weights becomes `[7x7x64, 1024]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a fully connected layer with ReLU activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the output layer. We have `1024` neurons in the current layer, and since
    we need to predict 10 classes, we have 10 neurons in the next layer, thus the
    shape of the weight matrix becomes `[1024 x 10]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the output with softmax activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Computing loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compute the loss using cross entropy. We know that the cross-entropy loss is
    given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69145fd8-d596-4fad-b1ca-7f4ac8fca30c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/b16d392d-d81a-4471-9a35-a41d748c2dee.png) is the actual label
    and ![](img/9455b313-8e3b-465f-8f71-c4f1d9e7286d.png) is the predicted label.
    Thus, the cross-entropy loss is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Minimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Starting the training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start a TensorFlow `Session` and initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model for `1000` epochs. Print the results for every `100` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that the loss decreases and the accuracy increases over epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing extracted features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have trained our CNN model, we can see what features our CNN has
    extracted to recognize the image. As we learned, each convolutional layer extracts
    important features from the image. We will see what features our first convolutional
    layer has extracted to recognize the handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s select one image from the training set, say, digit 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The input image is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/908dbc36-2c73-460d-aa03-07bb29122736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Feed this image to the first convolutional layer, that is, `conv1`, and get
    the feature maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following plot, the first convolutional layer has learned
    to extract edges from the given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/793fdc93-7882-47e6-90de-acbbca82a500.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, this is how the CNN uses multiple convolutional layers to extract important
    features from the image and feed these extracted features to a fully connected
    layer to classify the image. Now that we have learned how CNNs works, in the next
    section, we will learn about several interesting CNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: CNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore different interesting types of CNN architecture.
    When we say different types of CNN architecture, we basically mean how convolutional
    and pooling layers are stacked on each other. Additionally, we will learn how
    many numbers of convolutional, pooling, and fully connected layers are used, what
    the number of filters and filter sizes are, and more.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LeNet architecture is one of the classic architectures of a CNN. As shown
    in the following diagram, the architecture is very simple, and it consists of
    only seven layers. Out of these seven layers, there are three convolutional layers,
    two pooling layers, one fully connected layer, and one output layer. It uses a
    5 x 5 convolution with a stride of 1, and uses average pooling. What is 5 x 5
    convolution? It implies we are performing a convolution operation with a 5 x 5
    filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, LeNet consists of three convolutional layers
    (`C1`, `C3`, `C5`), two pooling layers (`S2`, `S4`), one fully connected layer
    (`F6`), and one output layer (`OUTPUT`), and each convolutional layer is followed
    by a pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffae1810-3122-412d-bfa0-554f2fabfa12.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding AlexNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AlexNet is a classic and powerful deep learning architecture. It won the ILSVRC
    2012 by significantly reducing the error rate from 26% to 15.3%. ILSVRC stands
    for ImageNet Large Scale Visual Recognition Competition, which is one of the biggest
    competitions focused on computer vision tasks, such as image classification, localization,
    object detection, and more. ImageNet is a huge dataset containing over 15 million
    labeled, high-resolution images, with over 22,000 categories. Every year, researchers
    compete to win the competition using innovative architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'AlexNet was designed by pioneering scientists, including Alex Krizhevsky, Geoffrey
    Hinton, and Ilya Sutskever. It consists of five convolutional layers and three
    fully connected layers, as shown in the following diagram. It uses the ReLU activation
    function instead of the tanh function, and ReLU is applied after every layer.
    It uses dropout to handle overfitting, and dropout is performed before the first
    and second fully connected layers. It uses data augmentation techniques, such
    as image translation, and is trained using batch stochastic gradient descent on
    two GTX 580 GPUs for 5 to 6 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1414b54-38c8-4512-8ba1-53f2acbe9dd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of VGGNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VGGNet is one of the most popularly used CNN architectures. It was invented
    by the **Visual Geometry Group** (**VGG**) at the University of Oxford. It started
    to get very popular when it became the first runner-up of ILSVRC 2014.
  prefs: []
  type: TYPE_NORMAL
- en: It is basically a deep convolutional network and is widely used for object-detection
    tasks. The weights and structure of the network are made available to the public
    by the Oxford team, so we can use these weights directly to carry out several
    computer vision tasks. It is also widely used as a good baseline feature extractor
    for images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the VGG network is very simple. It consists of convolutional
    layers followed by a pooling layer. It uses 3 x 3 convolution and 2 x 2 pooling
    throughout the network. It is referred to as VGG-*n*, where *n* corresponds to
    a number of layers, excluding the pooling and softmax layer. The following figure
    shows the architecture of the VGG-16 network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc6cc749-7c91-44dd-ab86-0682f7e7a216.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the following figure, the architecture of AlexNet is characterized
    by a pyramidal shape, as the initial layers are wide and the later layers are
    narrow. You will notice it consists of multiple convolutional layers followed
    by a pooling layer. Since the pooling layer reduces the spatial dimension, it
    narrows the network as we go deeper into the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e778b341-6235-4202-adcc-6716effb5248.png)'
  prefs: []
  type: TYPE_IMG
- en: The one shortcoming of VGGNet is that it is computationally expensive, and it
    has over 160 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: GoogleNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GoogleNet**, also known as **inception net**, was the winner of ILSVRC 2014\.
    It consists of various versions, and each version is an improved version of the
    previous one. We will explore each version one by one.'
  prefs: []
  type: TYPE_NORMAL
- en: Inception v1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inception v1 is the first version of the network. An object in an image appears
    in different sizes and in a different positions. For example, look at the first
    image; as you can see, the parrot, when viewed closer, takes up the whole portion
    of the image but in the second image, when the parrot is viewed from a distance,
    it takes up a smaller region of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00b99050-2195-47da-99e9-8ae8ec30e92f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we can say objects (in the given image, it's a parrot) can appear on any
    region of the image. It might be small or big. It might take up a whole region
    of the image, or just a very small portion. Our network has to exactly identify
    the object. But what's the problem here? Remember how we learned that we use a
    filter to extract features from the image? Now, because our object of interest
    varies in size and location in each image, choosing the right filter size is difficult.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a filter of a large size when the object size is large, but a large
    filter size is not suitable when we have to detect an object that is in a small
    corner of an image. Since we use a fixed receptive field that is a fixed filter
    size, it is difficult to recognize objects in the images whose position varies
    greatly. We can use deep networks, but they are more vulnerable to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this, instead of using a single filter of the same size, the inception
    network uses multiple filters of varying sizes on the same input. An inception
    network consists of nine inception blocks stacked over one another. A single inception
    block is shown in the following figure. As you will notice, we perform convolution
    operations on a given image with three different filters of varying size, that
    is, 1 x 1, 3 x 3, and 5 x 5\. Once the convolution operation is performed by all
    these different filters, we concatenate the results and feed it to the next inception
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f335394b-72c1-411c-9169-fccbf7e0b4ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we are concatenating output from multiple filters, the depth of the concatenated
    result will increase. Although we use padding that only matches the shape of the
    input and output to be the same but we will still have different depths. Since
    the result of one inception block is the feed to another, the depth keeps on increasing.
    So, to avoid the increase in the depth, we just add a 1 x 1 convolution before
    the 3 x 3 and 5 x 5 convolution, as shown in the following figure. We also perform
    a max pooling operation, and a 1 x 1 convolution is added after the max pooling
    operation as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6307804b-fdfa-4d8b-adc2-66b96100293d.png)'
  prefs: []
  type: TYPE_IMG
- en: Each inception block extracts some features and feeds them to the next inception
    block. Let's say we are trying to recognize a picture of a parrot. The inception
    block in the first few layers detects basic features, and the later inception
    blocks detect high-level features. As we saw, in a convolutional network, inception
    blocks will only extract features, and don't perform any classification. So, we
    feed the features extracted by the inception block to a classifier, which will
    predict whether the image contains a parrot or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the inception network is deep, with nine inception blocks, it is susceptible
    to the vanishing-gradient problem. To avoid this, we introduce classifiers between
    the inception blocks. Since each inception block learns the meaningful feature
    of the image, we try to perform classification and compute loss from the intermediate
    layers as well. As shown in the following figure, we have nine inception blocks.
    We take the result of the third inception block, ![](img/b06294bd-c05c-4396-8c5a-93105abd48e4.png),
    and feed it to an intermediate classifier, and also the result of the sixth inception
    block, ![](img/dcd2ad2a-d433-4347-8e03-acde05424092.png), to another intermediate
    classifier. There is also yet another classifier at the end of the final inception
    blocks. This classifier basically consists of average pooling, 1 x 1 convolutions,
    and a linear layer with softmax activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af8b6bd7-d29c-463b-985a-16cd92565a1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The intermediate classifiers are actually called auxiliary classifiers. So,
    the final loss of the inception network is the weighted sum of the auxiliary classifier''s
    loss and the loss of the final classifier (real loss), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb05a5d5-deb7-4763-a5c4-9da8477c8b9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception v2 and v3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inception v2 and v3 are introduced in the paper, *Going Deeper with Convolutions*
    by Christian Szegedy as mentioned in *Further reading* section. The authors suggest
    the use of factorized convolution, that is, we can break down a convolutional
    layer with a larger filter size into a stack of convolutional layers with a smaller
    filter size. So, in the inception block, a convolutional layer with a 5 x 5 filter
    can be broken down into two convolutional layers with 3 x 3 filters, as shown
    in the following diagram. Having a factorized convolution increases performance
    and speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4491e9b-d121-451b-8aa6-297fab8f2381.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The authors also suggest breaking down a convolutional layer of filter size
    *n* x *n* into a stack of convolutional layers with filter sizes *1* x *n* and
    *n* x *1*. For example, in the previous figure, we have *3* x *3* convolution,
    which is now broken down into *1* x *3* convolution, followed by *3* x *1* convolution,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fa9174f-ded7-4078-ba94-9df45bdfa063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you will notice in the previous diagram, we are basically expanding our
    network in a deeper fashion, which will lead us to lose information. So, instead
    of making it deeper, we make our network wider, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/679c32af-bc3b-47e1-95d7-72293c3627cb.png)'
  prefs: []
  type: TYPE_IMG
- en: In inception net v3, we use factorized 7 x 7 convolutions with RMSProp optimizers.
    Also, we apply batch normalization in the auxiliary classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Capsule networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Capsule networks** (**CapsNets**) were introduced by Geoffrey Hinton to overcome
    the limitations of convolutional networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hinton stated the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  prefs: []
  type: TYPE_NORMAL
- en: But what is wrong with the pooling operation? Remember when we used the pooling
    operation to reduce the dimension and to remove unwanted information? The pooling
    operation makes our CNN representation invariant to small translations in the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: This translation invariance property of a CNN is not always beneficial, and
    can be prone to misclassifications. For example, let's say we need to recognize
    whether an image has a face; the CNN will look for whether the image has eyes,
    a nose, a mouth, and ears. It does not care about which location they are in.
    If it finds all such features, then it classifies it as a face.
  prefs: []
  type: TYPE_NORMAL
- en: Consider two images, as shown in the following figure. The first image is the
    actual face, and in the second image, the eyes are placed on the left side, one
    above the another, and the ears and mouth are placed on the right. But the CNN
    will still classify both the images as a face as both images have all the features
    of a face, that is, ears, eyes, a mouth, and a nose. The CNN thinks that both
    images consist of a face. It does not learn the spatial relationship between each
    feature; that the eyes should be placed at the top and should be followed by a
    nose, and so on. All it checks for is the existence of the features that make
    up the face.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem will become worse when we have a deep network, as in the deep
    network, the features will become abstract, and it will also shrink in size due
    to the several pooling operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ed60e6a-2da0-4a5e-8179-3d1234e032d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To overcome this, Hinton introduced a new network called the Capsule network,
    which consists of capsules instead of neurons. Like a CNN, the Capsule network
    checks for the presence of certain features to classify the image, but apart from
    detecting the features, it will also check the spatial relationship between them.
    That is, it learns the hierarchy of the features. Taking our example of recognizing
    a face, the Capsule network will learn that the eyes should be at the top and
    the nose should be in the middle, followed by a mouth and so on. If the image
    does not follow this relationship, then the Capsule network will not classify
    it as a face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/934ce657-a389-4b18-9af9-a9655338c006.png)'
  prefs: []
  type: TYPE_IMG
- en: A Capsule network consists of several capsules connected together. But, wait.
    What is a capsule?
  prefs: []
  type: TYPE_NORMAL
- en: A capsule is a group of neurons that learn to detect a particular feature in
    the image; say, eyes. Unlike neurons, which return a scalar, capsules return a
    vector. The length of the vector tells us whether a particular feature exists
    in a given location, and the elements of the vector represent the properties of
    the features, such as, position, angle, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have a vector, ![](img/de2c431c-b1a4-4adf-8d7a-d5e7207e9b65.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61f11bd7-ee13-4d67-bb43-a5fbef5eba3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The length of the vector can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9683d9c-30b1-4c92-a117-2437ce209a93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have learned that the length of the vector represents the probability of
    the existence of the features. But the preceding length does not represent a probability,
    as it exceeds 1\. So, we convert this value into a probability using a function
    called the squash function. The squash function has an advantage. Along with calculating
    probability, it also preserves the direction of the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/482bbb73-5d3e-4ba8-8ae7-0d0df58c8b45.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like a CNN, capsules in the earlier layers detect basic features including
    eyes, a nose, and so on, and the capsules in the higher layers detect high-level
    features, such as the overall face. Thus, capsules in the higher layers take input
    from the capsules in the lower layers. In order for the capsules in the higher
    layers to detect a face, they not only check for the presence of features such
    as a nose and eyes, but also check their spatial relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of what a capsule is, we will go into
    this in more detail and see how exactly a Capsule network works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Capsule networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we have two layers, ![](img/c682874b-001b-47df-89c0-2a1e2c9c965d.png)
    and ![](img/d999d9de-a068-411b-a73a-e5bc97fe347d.png). ![](img/bcbdc041-43ec-438a-b9d0-75c0100912e1.png)
    will be the lower layer and it has ![](img/4d5b5876-e2fe-4c08-bb59-32eec8809635.png)
    capsules, and ![](img/b30eca11-e52d-408f-beba-503f68cc450e.png) will be the higher
    layer and it has ![](img/1ba6e654-dbcc-434f-818d-c1a76de69601.png) capsules. Capsules
    from the lower layer send their outputs to capsules in the higher layer. ![](img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png)
    will be the activations of the capsules from the lower layer, ![](img/523529a6-1330-4a87-bbfb-8e1131f32111.png).
    ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png) will be the activations of the
    capsules from the higher layer, ![](img/b30eca11-e52d-408f-beba-503f68cc450e.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents a capsule, ![](img/f4b54408-3d5c-4620-9a6b-ed0a95fcdac0.png),
    and as you can observe, it takes the outputs of the previous capsules, ![](img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png),
    as inputs and computes its output, ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8411ec40-4aa9-4263-ab6a-216ebba8edac.png)'
  prefs: []
  type: TYPE_IMG
- en: We will move on to learn how ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png)
    is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Computing prediction vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous figure, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png),
    ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png), and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png)
    represent the output vectors from the previous capsule. First, we multiply these
    vectors by the weight matrix and compute a prediction vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6c26383-1d9f-4096-b233-6f0c6e13e213.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, so what exactly are we doing here, and what are prediction vectors? Let's
    consider a simple example. Say that capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png)
    is trying to predict whether an image has a face. We have learned that capsules
    in the earlier layers detect basic features and send their results to the capsules
    in the higher layer. So, the capsules in the earlier layer, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png),
    ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png), and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png)
    detect basic low features, such as eyes, a nose, and a mouth, and send their results
    to the capsules in the high-level layer, that is, capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png),
    which detects the face.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png) takes the previous
    capsules, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png), ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png),
    and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png), as inputs and multiplies
    them by a weights matrix, ![](img/daf9bc01-bd76-48aa-866c-f4efcc275d91.png).
  prefs: []
  type: TYPE_NORMAL
- en: The weights matrix ![](img/39d372ef-547c-40ad-abd2-2b55badabb04.png) represents
    the spatial and other relationship between low-level features and high-level features.
    For instance, the weight ![](img/e5a11cc9-8a2c-4438-8599-a1e1fe25466e.png) tells
    us that eyes should be on the top. ![](img/2394add2-cdb3-40e9-8558-e722eb4df2f6.png)
    tells us that a nose should be in the middle.![](img/8ea16cdd-9ec9-43e3-a1de-ea08e508d657.png)
    tells us that a mouth should be on the bottom. Note that the weight matrix not
    only captures the position (that is, the spatial relationship), but also other
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, by multiplying the inputs by weights, we can predict the position of the
    face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d994f6ef-9071-44bd-8f97-0b6eb62bb99e.png) implies the predicted position
    of the face based on the eyes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a51b641b-3884-482b-b29c-9ded8b6f29aa.png) implies the predicted position
    of the face based on the nose'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c53a8493-8c9d-4610-aaf4-c0334f70087d.png) implies the predicted position
    of the face based on the mouth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When all the predicted positions of the face are the same, that is, in agreement
    with each other, then we can say that the image contains a face. We learn these
    weights using backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Coupling coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we multiply the prediction vectors ![](img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png)
    by the coupling coefficients ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png).
    The coupling coefficients exist between any two capsules. We know that capsules
    from the lower layer send their output to the capsules in the higher layer. The
    coupling coefficient helps the capsule in the lower layer to understand which
    capsule in the higher layer it has to send its output to.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's consider the same example, where we are trying to predict
    whether an image consists of a face. ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png)
    represents the agreement between ![](img/49647a36-9898-4c0a-99b4-23cf0fe2d5b4.png)
    and ![](img/4cb9f8f3-5451-46ee-a62f-bbc3cef70423.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c658d700-33da-4b71-a264-2eee36515b37.png) represents the agreement
    between an eye and a face. Since we know that the eye is on the face, the ![](img/c658d700-33da-4b71-a264-2eee36515b37.png)
    value will be increased. We know that the prediction vector ![](img/8aab866e-89b4-49b7-ae83-568a49f9f77d.png)
    implies the predicted position of the face based on the eyes. Multiplying ![](img/1ae779e1-3dad-46f3-a74f-2bfed0ea20c2.png)
    by ![](img/44a776bc-7835-4a04-b8e6-5a163beee271.png) implies that we are increasing
    the importance of the eyes, as the value of ![](img/44a776bc-7835-4a04-b8e6-5a163beee271.png)
    is high.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1757d487-16aa-422d-a8c8-421f92b96a62.png) represents the agreement
    between nose and face. Since we know that the nose is on the face, the ![](img/a0aa2114-5372-4e01-bb5b-3670a27b9225.png)
    value will be increased. We know that the prediction vector ![](img/50b80e08-9d68-43f4-93e1-9e863f302dd9.png)
    implies the predicted position of the face based on the nose. Multiplying ![](img/3d9ab4a0-f640-410f-9a28-7f56fb1d30e3.png)
    by ![](img/07ddd2de-d13f-47c7-a594-9c546cb1ec61.png) implies that we are increasing
    the importance of the nose, as the value of ![](img/6e19a055-6cb3-4a00-bf84-c50931a89ce5.png)
    is high.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another low-level feature, say, ![](img/1f7021c5-124b-4510-9045-ac9e08df3874.png),
    which detects a finger. Now, ![](img/29e60844-2b1a-4f13-b806-8d5dca8abd49.png)
    represents the agreement between a finger and a face, which will be low. Multiplying
    ![](img/b761a33c-c89b-487c-bfc4-478297e98a63.png) by ![](img/a44a4354-8d73-4870-8662-e46fc9b96e65.png)
    implies that we are decreasing the importance of the finger, as the value of ![](img/a44a4354-8d73-4870-8662-e46fc9b96e65.png)
    is low.
  prefs: []
  type: TYPE_NORMAL
- en: But how are these coupling coefficients learned? Unlike weights, the coupling
    coefficients are learned in the forward propagation itself, and they are learned
    using an algorithm called dynamic routing, which we will discuss later in an upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'After multiplying ![](img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png) by ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png),
    we sum them up, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d918e1b-d57e-42d2-98f8-a3daca4670cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can write our equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00a46584-ac9d-48a5-bac9-eba1cf110992.png)'
  prefs: []
  type: TYPE_IMG
- en: Squashing function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off saying that capsule ![](img/8117a7a5-ad1c-46b9-b9e4-3831995ce70e.png)
    tries to detect the face in the image. So, we need to convert ![](img/4475e081-5628-4b62-8ba3-d38f9e364a3f.png)
    into probabilities to get the probability of the existence of a face in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from calculating probabilities, we also need to preserve the direction
    of the vectors, so we use an activation function called the squash function. It
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3abea8b-efbf-4e38-9f43-a39c0b72adc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, ![](img/47acd07c-4abb-4824-9b7b-6a21fe18405a.png) (also referred to as
    the activity vector) gives us the probability of the existence of a face in a
    given image.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how the dynamic routing algorithm computes the coupling coefficients.
    Let's introduce a new variable called ![](img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png),
    which is just a temporary variable, and is the same as the coupling coefficients
    ![](img/0e8aeb81-9ded-4ea1-b1f9-6f431b657fcf.png). First, we initialize ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png)
    to 0\. It implies coupling coefficients between the capsules ![](img/1102139e-972c-446f-ba3b-4af96e8fde18.png)
    in the lower layer ![](img/0670a3bb-bb11-482d-b63f-a276da98c15d.png) and capsules
    ![](img/81a66aa8-aad8-4120-980d-d884d3d37c00.png) in the higher layer ![](img/57f70dd5-15ca-4d9e-b01d-b55f406c0d64.png)
    are set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/1e3a3e33-adda-4961-a733-f61ec467327a.png) be the vector representation
    of ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png). Given the prediction vectors
    ![](img/b84324a1-f6d6-49de-b61b-30b6343d6174.png), for some *n* number of iterations,
    we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For all the capsules ![](img/77b2dc9f-3926-4f32-89c0-d3425563f7a5.png) in the
    layer ![](img/fcc44e5a-e3ef-4c53-ba87-ebc12c074820.png), compute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7fde1dd7-8523-4f2d-817a-55f7310d647b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For all the capsules ![](img/9c4ef49d-c423-43d2-9d13-49e8d0e42eb5.png) in the
    layer ![](img/79a8569c-6df0-427f-a606-32fd6508d47a.png), compute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e90ff949-6b08-4f0c-8bd4-ea5c0b6f5d3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/26849c78-0b84-44c4-834d-7f41e2c656c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For all capsules ![](img/e529ebfd-14de-40ca-9270-29301931285b.png) in ![](img/c857672e-e8b6-4bc5-bec4-e03185379626.png)
    and for all capsules in ![](img/bd605ba5-adde-471d-bead-2749d794b71d.png), compute
    ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/302e217a-c547-46d2-be4a-d4ae7f42f078.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous equation has to be noted carefully. It is where we update our coupling
    coefficient. The dot product ![](img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png)
    implies the dot product between the prediction vectors ![](img/4e68442b-e295-4ea4-adba-21217e91221a.png)
    of the capsule in the lower layer and the output vector ![](img/19cc1d0c-f95d-4cd8-88fc-65a1d397c85f.png)
    of the capsule in the higher layer. If the dot product is high, ![](img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png)
    will increase the respective coupling coefficient ![](img/69f2393a-d5aa-4a94-9b21-8f386ffbb9c0.png),
    which makes the ![](img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png) stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the Capsule network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose our network is trying to predict handwritten digits. We know that
    capsules in the earlier layers detect basic features, and those in the later layers
    detect the digit. So, let's call the capsules in the earlier layers **primary
    capsules** and those in the later layers **digit capsules**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a Capsule network is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d14d16fb-2c2e-4f8e-b9a6-09a0026f2522.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we take the input image and feed it to a standard convolution layer,
    and we call the result convolutional inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we feed the convolutional inputs to the primary capsules layer and get
    the primary capsules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we compute digit capsules with primary capsules as input using the dynamic-routing
    algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The digit capsules consist of 10 rows, and each of the rows represents the probability
    of the predicted digit. That is, row 1 represents the probability of the input
    digit to be 0, row 2 represents the probability of the digit 1, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the input image is digit 3 in the preceding image, row 4, which represents
    the probability of digit 3, will be high in the digit capsules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will explore the loss function of the Capsule network. The loss function
    is the weighted sum of two loss functions called margin loss and reconstruction
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: Margin loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned that the capsule returns a vector and the length of a vector represents
    the probability of the existence of the features. Say our network is trying to
    recognize the handwritten digits in an image. To detect multiple digits in a given
    image, we use margin loss, ![](img/41912fb7-6561-4262-bdef-7252e7f5806d.png),
    for each digit capsule, ![](img/b9dd5c83-2949-44d7-986b-60a7c8c48982.png), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f038fbd8-a9ec-4841-b4d8-ddd2430b95aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/642d706d-8618-4d97-be58-ec1c5a724004.png), if the digit of a class
    ![](img/7f1c19ea-9906-42eb-9175-c4524ee9f281.png) is present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ba3ab593-1bde-4137-9de1-937379784247.png) is the margin, and ![](img/5739739c-5563-4d46-acfc-b602f1715e16.png)
    is set to 0.9 and ![](img/b83b1abf-cf5d-47c4-9ec1-3841703f993c.png) is set to
    0.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9993a5d8-1324-44e2-94d6-b443be02872e.png) prevents the initial learning
    from shrinking the lengths of the vectors of all the digit capsules and is usually
    set to 0.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The total margin loss is the sum of the loss of all classes, ![](img/2df2e71e-36eb-4fe7-b622-3b5d36e3e4cd.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1707881e-bf20-42cd-b127-55e5e71f7c7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Reconstruction loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to make sure that the network has learned the important features in
    the capsules, we use reconstruction loss. This means that we use a three-layer
    network called a decoder network, which tries to reconstruct the original image
    from the digit capsules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e714ade3-5b08-4d16-8bdd-5cc6438a31c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reconstruction loss is given as the squared difference between the reconstructed
    and original image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23dd475d-dd9e-4e17-9310-7b800aef04ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/106c782e-4b1e-49f0-ad58-d9e3ecddd21b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, alpha is a regularization term, because we don't want the reconstruction
    loss to have more priority than the margin loss. So, alpha is multiplied by the
    reconstruction loss to scale down its importance, and is usually set to 0.0005.
  prefs: []
  type: TYPE_NORMAL
- en: Building Capsule networks in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will learn how to implement Capsule networks in TensorFlow. We will use
    our favorite MNIST dataset to learn how a Capsule network recognizes the handwritten
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Defining the squash function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned that the squash function converts the length of the vector into
    probability, and it is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee00267a-bcf4-4a2d-92ee-bba5e1c162c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `squash` function can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Defining a dynamic routing algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will look at how the dynamic routing algorithm is implemented. We use
    variable names of the same notations that we learned in the dynamic routing algorithm,
    so that we can easily follow the steps. We will look at each line in our function
    step by step. You can also check the complete code on GitHub, at [http://bit.ly/2HQqDEZ](http://bit.ly/2HQqDEZ).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the function called `dynamic_routing`, which takes the previous
    capsules, `ui`, coupling coefficients, `bij`, and number of routing iterations,
    `num_routing` as inputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `wij` weights by drawing from a random normal distribution,
    and initialize `biases` with a constant value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the primary capsules `ui` (`tf.tile` replicates the tensor *n* times):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the prediction vector, ![](img/b4ca5bae-01ba-43f0-ba90-7543d6de315c.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the prediction vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Stop gradient computation in the prediction vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform dynamic routing for a number of routing iterations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Computing primary and digit capsules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will compute the primary capsules, which extract the basic features,
    and the digit capsules, which recognizes the digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the TensorFlow `Graph`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholders for input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the convolution operation and get the convolutional input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the primary capsules that extract the basic features, such as edges.
    First, compute the capsules using the convolution operation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate all the capsules and form the primary capsules, squash the primary
    capsules, and get the probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the `squash` function to the primary capsules and get the probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the digit capsules using a dynamic-routing algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Masking the digit capsule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we need to mask the digit capsule? We learned that in order to make sure
    that the network has learned the important features, we use a three-layer network
    called a decoder network, which tries to reconstruct the original image from the
    digit capsules. If the decoder is able to reconstruct the image successfully from
    the digit capsules, then it means the network has learned the important features
    of the image; otherwise, the network has not learned the correct features of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The digit capsules contain the activity vector for all the digits. But the
    decoder wants to reconstruct only the given input digit (the input image). So,
    we mask out the activity vector of all the digits, except for the correct digit.
    Then we use this masked digit capsule to reconstruct the given input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Defining the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the decoder network for reconstructing the image. It consists of three
    fully connected networks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Computing the accuracy of the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we compute the accuracy of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the length of each activity vector in the digit capsule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply `softmax` to the length and get the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the index that had the highest probability; this will give us the predicted
    digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the `accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Calculating loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, we compute two types of loss—margin loss and reconstruction loss.
  prefs: []
  type: TYPE_NORMAL
- en: Margin loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that margin loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7a02188-ebba-41ac-a44e-4172aabfef28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute the maximum value in the left and maximum value in the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Set ![](img/305c1497-9b9f-42e9-9869-d19d8e80c646.png) to ![](img/6c67bda9-c871-4a51-9eb9-c9503a77223c.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The total margin loss is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Reconstruction loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reshape and get the original image by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean of the squared difference between the reconstructed and the
    original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the reconstruction loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Total loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the total loss, which is the weighted sum of the margin loss and the
    reconstructed loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Training the Capsule network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Set the number of epochs and number of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now start the TensorFlow `Session` and perform training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see how the loss decreases over various iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have learned how Capsule networks work step by step, and how to build
    a Capsule network in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding CNNs. We learned about the different
    layers of a CNN, such as convolution and pooling; where the important features
    from the image will be extracted and are fed to the fully collected layer; and
    where the extracted feature will be classified. We also visualized the features
    extracted from the convolutional layer using TensorFlow by classifying handwritten
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about several architectures of CNN, including LeNet, AlexNet,
    VGGNet, and GoogleNet. At the end of the chapter, we studied Capsule networks,
    which overcome the shortcomings of a convolutional network. We learned that Capsule
    networks use a dynamic routing algorithm for classifying the image.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study the various algorithms used for learning
    text representations.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try answering the following questions to assess our knowledge of CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the different layers of a CNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define stride.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is padding required?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define pooling. What are the different types of pooling operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the architecture of VGGNet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is factorized convolution in the inception network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do Capsule networks differ from CNNs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the squash function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Very Deep Convolutional Networks for Large-Scale Image Recognition* by Karen
    Simonyan and Andrew Zisserman, available at [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paper on inception net, *Going Deeper with Convolutions* by Christian Szegedy
    et al., available at [https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamic Routing Between Capsules* by Sara Sabour, Nicholas Frosst, and Geoffrey
    E. Hinton, available at [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
