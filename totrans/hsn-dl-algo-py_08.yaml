- en: Demystifying Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘卷积网络
- en: '**Convolutional Neural Networks** (**CNNs**) are one of the most commonly used
    deep learning algorithms. They are widely used for image-related tasks, such as
    image recognition, object detection, image segmentation, and more. The applications
    of CNNs are endless, ranging from powering vision in self-driving cars to the
    automatic tagging of friends in our Facebook pictures. Although CNNs are widely
    used for image datasets, they can also be applied to textual datasets.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络** (**CNNs**) 是最常用的深度学习算法之一。它们广泛应用于与图像相关的任务，如图像识别、物体检测、图像分割等。CNNs 的应用无所不在，从自动驾驶汽车中的视觉处理到我们在
    Facebook 照片中自动标记朋友。尽管CNNs广泛用于图像数据集，但它们也可以应用于文本数据集。'
- en: In this chapter, we will look at CNNs in detail and get the hang of CNNs and
    how they work. First, we will learn about CNNs intuitively, and then we will deep-dive
    into the underlying math behind them. Following this, we will come to understand
    how to implement a CNN in TensorFlow step by step. Moving ahead, we will explore
    different types of CNN architectures such as LeNet, AlexNet, VGGNet, and GoogleNet.
    At the end of the chapter, we will study the shortcomings of CNNs and how these
    can be resolved using Capsule networks. Also, we will learn how to build Capsule
    networks using TensorFlow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细了解CNNs，并掌握CNNs及其工作原理。首先，我们将直观地了解CNNs，然后深入探讨其背后的数学原理。随后，我们将学习如何逐步在TensorFlow中实现CNN。接下来，我们将探索不同类型的CNN架构，如LeNet、AlexNet、VGGNet和GoogleNet。在本章末尾，我们将研究CNNs的不足之处，并学习如何使用胶囊网络解决这些问题。此外，我们还将学习如何使用TensorFlow构建胶囊网络。
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: What are CNNs?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是CNNs？
- en: The math behind CNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs 的数学原理
- en: Implementing CNNs in TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现CNNs
- en: Different CNN architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的CNN架构
- en: Capsule networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: Building Capsule networks in TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中构建胶囊网络
- en: What are CNNs?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是CNNs？
- en: 'A CNN, also known as a **ConvNet**, is one of the most widely used deep learning
    algorithms for computer vision tasks. Let''s say we are performing an image-recognition
    task. Consider the following image. We want our CNN to recognize that it contains
    a horse:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，也称为**ConvNet**，是用于计算机视觉任务的最常用的深度学习算法之一。假设我们正在执行一个图像识别任务。考虑以下图像。我们希望我们的CNN能够识别其中包含一匹马。
- en: '![](img/5c930822-cdbf-43ac-a5f2-2c8418f3039f.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c930822-cdbf-43ac-a5f2-2c8418f3039f.jpg)'
- en: How can we do that? When we feed the image to a computer, it basically converts
    it into a matrix of pixel values. The pixel values range from 0 to 255, and the
    dimensions of this matrix will be of [*image* *width* x *image height* x *number
    of channels*]. A grayscale image has one channel, and colored images have three
    channels **red, green, and blue** (**RGB**).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何做到这一点？当我们将图像输入计算机时，它基本上会将其转换为一个像素值矩阵。像素值的范围从0到255，而此矩阵的尺寸将是 [*图像宽度* x *图像高度*
    x *通道数*]。灰度图像有一个通道，彩色图像有三个通道 **红色、绿色和蓝色** (**RGB**)。
- en: Let's say we have a colored input image with a width of 11 and a height of 11,
    that is 11 x 11, then our matrix dimension would be of *[11 x 11 x 3]*. As you
    can see in *[11 x 11 x 3]*, 11 x 11 represents the image width and height and
    3 represents the channel number, as we have a colored image. So, we will have
    a 3D matrix.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个宽度为11、高度为11的彩色输入图像，即11 x 11，那么我们的矩阵维度将是 *[11 x 11 x 3]*。如你所见，*[11 x 11
    x 3]* 中，11 x 11表示图像的宽度和高度，3表示通道数，因为我们有一张彩色图像。因此，我们将得到一个3D矩阵。
- en: But it is hard to visualize a 3D matrix, so, for the sake of understanding,
    let's consider a grayscale image as our input. Since the grayscale image has only
    one channel, we will get a 2D matrix.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于理解来说，很难将3D矩阵可视化，因此让我们以灰度图像作为输入来考虑。由于灰度图像只有一个通道，我们将得到一个2D矩阵。
- en: 'As shown in the following diagram, the input grayscale image will be converted
    into a matrix of pixel values ranging from 0 to 255, with the pixel values representing
    the intensity of pixels at that point:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，输入的灰度图像将被转换为一个像素值矩阵，其像素值介于0到255之间，像素值表示该点的像素强度：
- en: '![](img/6d93e2fb-b07f-4165-a395-80c9e0dbc857.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d93e2fb-b07f-4165-a395-80c9e0dbc857.png)'
- en: The values given in the input matrix are just arbitrary values for our understanding.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输入矩阵中给出的值仅仅是为了帮助我们理解而随意给出的。
- en: 'Okay, now we have an input matrix of pixel values. What happens next? How does
    the CNN come to understand that the image contains a horse? CNNs consists of the
    following three important layers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pooling layer
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of these three layers, the CNN recognizes that the image contains
    a horse. Now we will explore each of these layers in detail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The convolutional layer is the first and core layer of the CNN. It is one of
    the building blocks of a CNN and is used for extracting important features from
    the image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: We have an image of a horse. What do you think are the features that will help
    us to understand that this is an image of a horse? We can say body structure,
    face, legs, tail, and so on. But how does the CNN understand these features? This
    is where we use a convolution operation that will extract all the important features
    from the image that characterize the horse. So, the convolution operation helps
    us to understand what the image is all about.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what exactly is this convolution operation? How it is performed? How does
    it extract the important features? Let's look at this in detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, every input image is represented by a matrix of pixel values. Apart
    from the input matrix, we also have another matrix called the **filter matrix**.
    The filter matrix is also known as a **kernel**, or simply a **filter**, as shown
    in the following diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdda8d40-64ac-4f3e-ad41-129f0ecb9c6c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'We take the filter matrix, slide it over the input matrix by one pixel, perform
    element-wise multiplication, sum up the results, and produce a single number.
    That''s pretty confusing, isn''t it? Let''s understand this better with the aid
    of the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9169595-1bfc-4898-8095-46e20d9e0efb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the previous diagram, we took the filter matrix and placed
    it on top of the input matrix, performed element-wise multiplication, summed their
    results, and produced the single number. This is demonstrated as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bbdbdf4-a60e-4ff7-844a-e92b578c1518.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Now, we slide the filter over the input matrix by one pixel and perform the
    same steps, as shown in the following diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2a9f9f2-de3c-4bc6-a051-18eded4470d2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f13d7db8-4d70-456e-b553-174debcd9340.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Again, we slide the filter matrix by one pixel and perform the same operation,
    as shown in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6ecbe08-83fe-47b9-811f-9f65df0d65a6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98f7173b-8d73-4abf-a4bd-1b016a30b82d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Now, again, we slide the filter matrix over the input matrix by one pixel and
    perform the same operation, as shown in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4725aa5-08ef-4f96-9ded-b761cfc4e5a0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'That is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f72ca299-aeaf-442d-97e8-102631737f49.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Okay. What are we doing here? We are basically sliding the filter matrix over
    the entire input matrix by one pixel, performing element-wise multiplication and
    summing their results, which creates a new matrix called a **feature map** or
    **activation map**. This is called the **convolution operation**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: As we've learned, the convolution operation is used to extract features, and
    the new matrix, that is, the feature maps, represents the extracted features.
    If we plot the feature maps, then we can see the features extracted by the convolution
    operation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the actual image (the input image) and the convolved
    image (the feature map). We can see that our filter has detected the edges from
    the actual image as a feature:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e7080aa-e990-41b5-8d2a-35f73e337d15.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'Various filters are used for extracting different features from the image.
    For instance, if we use a sharpen filter, ![](img/c26a4782-aa88-4916-a1bf-341300987e17.png),
    then it will sharpen our image, as shown in the following figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/507a9d26-2aa8-4443-a4ea-6ba2adc37947.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we have learned that with filters, we can extract important features
    from the image using the convolution operation. So, instead of using one filter,
    we can use multiple filters for extracting different features from the image,
    and produce multiple feature maps. So, the depth of the feature map will be the
    number of filters. If we use seven filters to extract different features from
    the image, then the depth of our feature map will be seven:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75d06dec-64f7-4888-ad46-b0366fcb90df.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Okay, we have learned that different filters extract different features from
    the image. But the question is, how can we set the correct values for the filter
    matrix so that we can extract the important features from the image? Worry not!
    We just initialize the filter matrix randomly, and the optimal values of the filter
    matrix, with which we can extract the important features from the images, will
    be learned through backpropagation. However, we just need to specify the size
    of the filter and the number of filters we want to use.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just learned how a convolution operation works. We slide over the input
    matrix with the filter matrix by one pixel and perform the convolution operation.
    But we can't only slide over the input matrix by one pixel. We can also slide
    over the input matrix by any number of pixels.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The number of pixels we slide over the input matrix by the filter matrix is
    called a **stride**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set the stride to 2, then we slide over the input matrix with the filter
    matrix by two pixels. The following diagram shows a convolution operation with
    a stride of 2:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11bc6120-28bc-4d19-9ad4-5372e5bda4d6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: But how do we choose the stride number? We just learned that a stride is the
    number of pixels along that we move our filter matrix. So, when the stride is
    set to a small number, we can encode a more detailed representation of the image
    than when the stride is set to a large number. However, a stride with a high value
    takes less time to compute than one with a low value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何选择步幅数字呢？我们刚学到步幅是我们移动滤波器矩阵的像素数。因此，当步幅设置为一个小数字时，我们可以编码比步幅设置为大数字更详细的图像表示。然而，具有高值步幅的步幅计算时间少于具有低值步幅的步幅。
- en: Padding
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充
- en: 'With the convolution operation, we are sliding over the input matrix with a
    filter matrix. But in some cases, the filter does not perfectly fit the input
    matrix. What do we mean by that? For example, let''s say we are performing a convolution
    operation with a stride of 2\. There exists a situation where, when we move our
    filter matrix by two pixels, it reaches the border and the filter matrix does
    not fit the input matrix. That is, some part of our filter matrix is outside the
    input matrix, as shown in the following diagram:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积操作中，我们通过滤波器矩阵滑动输入矩阵。但在某些情况下，滤波器并不完全适合输入矩阵。什么意思？例如，假设我们使用步幅为2进行卷积操作。存在这样一种情况，即当我们将我们的滤波器矩阵移动两个像素时，它达到边界，滤波器矩阵不适合输入矩阵。也就是说，我们的滤波器矩阵的某部分位于输入矩阵之外，如下图所示：
- en: '![](img/e7a24559-4882-4df0-b801-02065e8b3fd7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7a24559-4882-4df0-b801-02065e8b3fd7.png)'
- en: 'In this case, we perform padding. We can simply pad the input matrix with zeros
    so that the filter can fit the input matrix, as shown in the following diagram.
    Padding with zeros on the input matrix is called **same padding** or **zero padding**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们进行填充。我们可以简单地用零填充输入矩阵，以便滤波器可以适合输入矩阵，如下图所示。在输入矩阵上用零填充被称为**相同填充**或**零填充**：
- en: '![](img/718bf23e-c425-4f9c-a2e6-6d9e37d070b7.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/718bf23e-c425-4f9c-a2e6-6d9e37d070b7.png)'
- en: 'Instead of padding them with zeros, we can also simply discard the region of
    the input matrix where the filter doesn''t fit in. This is called **valid padding**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与其用零填充它们，我们也可以简单地丢弃输入矩阵中滤波器不适合的区域。这称为**有效填充**：
- en: '![](img/f3349d9e-becb-4495-8655-71014560c081.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3349d9e-becb-4495-8655-71014560c081.png)'
- en: Pooling layers
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: Okay. Now, we are done with the convolution operation. As a result of the convolution
    operation, we have some feature maps. But the feature maps are too large in dimension.
    In order to reduce the dimensions of feature maps, we perform a pooling operation.
    This reduces the dimensions of the feature maps and keeps only the necessary details
    so that the amount of computation can be reduced.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在，我们完成了卷积操作。作为卷积操作的结果，我们得到了一些特征映射。但是特征映射的维度太大了。为了减少特征映射的维度，我们进行池化操作。这样可以减少特征映射的维度，并保留必要的细节，从而减少计算量。
- en: For example, to recognize a horse from the image, we need to extract and keep
    only the features of the horse; we can simply discard unwanted features, such
    as the background of the image and more. A pooling operation is also called a
    **downsampling** or **subsampling** operation, and it makes the CNN translation
    invariant. Thus, the pooling layer reduces spatial dimensions by keeping only
    the important features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要从图像中识别出一匹马，我们需要提取并保留马的特征；我们可以简单地丢弃不需要的特征，比如图像的背景等。池化操作也称为**下采样**或**子采样**操作，使得卷积神经网络具有平移不变性。因此，池化层通过保留重要的特征来减少空间维度。
- en: The pooling operation will not change the depth of the feature maps; it will
    only affect the height and width.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作不会改变特征映射的深度；它只会影响高度和宽度。
- en: There are different types of pooling operations, including max pooling, average
    pooling, and sum pooling.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作有不同的类型，包括最大池化、平均池化和求和池化。
- en: 'In max pooling, we slide over the filter on the input matrix and simply take
    the maximum value from the filter window, as shown in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大池化中，我们在输入矩阵上滑动滤波器，并从滤波器窗口中简单地取最大值，如下图所示：
- en: '![](img/faf168e5-c7f9-438b-9dd3-70a27d03a408.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/faf168e5-c7f9-438b-9dd3-70a27d03a408.png)'
- en: 'As the name suggests, in average pooling, we take the average value of the
    input matrix within the filter window, and in sum pooling, we sum all the values
    of the input matrix within the filter window, as shown in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名，平均池化中，我们取滤波器窗口内输入矩阵的平均值，在求和池化中，我们对滤波器窗口内的输入矩阵的所有值求和，如下图所示：
- en: '![](img/a660315a-b637-4f49-8ed6-e118993a1386.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a660315a-b637-4f49-8ed6-e118993a1386.png)'
- en: Max pooling is one of the most commonly used pooling operations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化是最常用的池化操作之一。
- en: Fully connected layers
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: So far, we've learned how convolutional and pooling layers work. A CNN can have
    multiple convolutional layers and pooling layers. However, these layers will only
    extract features from the input image and produce the feature map; that is, they
    are just the feature extractors.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了卷积和池化层的工作原理。CNN可以拥有多个卷积层和池化层。然而，这些层只会从输入图像中提取特征并生成特征映射；也就是说，它们只是特征提取器。
- en: 'Given any image, convolutional layers extract features from the image and produce
    a feature map. Now, we need to classify these extracted features. So, we need
    an algorithm that can classify these extracted features and tell us whether the
    extracted features are the features of a horse, or something else. In order to
    make this classification, we use a feedforward neural network. We flatten the
    feature map and convert it into a vector, and feed it as an input to the feedforward
    network. The feedforward network takes this flattened feature map as an input,
    applies an activation function, such as sigmoid, and returns the output, stating
    whether the image contains a horse or not; this is called a fully connected layer
    and is shown in the following diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 针对任何图像，卷积层会从图像中提取特征并生成特征映射。现在，我们需要对这些提取出的特征进行分类。因此，我们需要一种算法来对这些提取出的特征进行分类，并告诉我们这些提取出的特征是否是马的特征，或者其他什么东西的特征。为了进行这种分类，我们使用一个前向神经网络。我们将特征映射展平并将其转换为向量，并将其作为输入馈送到前向网络中。前向网络将这个展平的特征映射作为输入，应用激活函数（如sigmoid），并返回输出，说明图像是否包含马；这称为全连接层，如下图所示：
- en: '![](img/affa9ab7-63a4-45f0-865d-a1bbc6c75062.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/affa9ab7-63a4-45f0-865d-a1bbc6c75062.png)'
- en: The architecture of CNNs
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的架构
- en: 'The architecture of a CNN is shown in the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的架构如下图所示：
- en: '![](img/b4e34a47-e0c6-45d8-ad8c-0e03396efb57.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4e34a47-e0c6-45d8-ad8c-0e03396efb57.png)'
- en: As you will notice, first we feed the input image to the convolutional layer,
    where we apply the convolution operation to extract important features from the
    image and create the feature maps. We then pass the feature maps to the pooling
    layer, where the dimensions of the feature maps will be reduced. As shown in the
    previous diagram, we can have multiple convolutional and pooling layers, and we
    should also note that the pooling layer does not necessarily have to be there
    after every convolutional layer; there can be many convolutional layers followed
    by a pooling layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所注意到的，首先我们将输入图像馈送到卷积层，其中我们对图像应用卷积操作以从图像中提取重要特征并创建特征映射。然后，我们将特征映射传递给池化层，其中特征映射的维度将被减少。正如前面的图所示，我们可以有多个卷积和池化层，并且还应注意到池化层并不一定需要在每个卷积层之后；可以有多个卷积层后跟一个池化层。
- en: So, after the convolutional and pooling layers, we flatten the resultant feature
    maps and feed it to a fully connected layer, which is basically a feedforward
    neural network that classifies the given input image based on the feature maps.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在卷积和池化层之后，我们将展平产生的特征映射，并将其馈送到全连接层，这基本上是一个前向神经网络，根据特征映射对给定的输入图像进行分类。
- en: The math behind CNNs
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的数学背后
- en: So far, we have intuitively understood how a CNN works. But how exactly does
    a CNN learn? How does it find the optimal values for the filter using backpropagation?
    To answer this question, we will explore mathematically how the CNN works. Unlike
    in the [Chapter 5](c8326380-001a-4ece-8a14-b0a1ea0010b5.xhtml), *Improvements
    to the RNN*, the math behind a CNN is pretty simple and very interesting.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经直观地理解了CNN的工作原理。但是CNN到底是如何学习的呢？它如何使用反向传播找到滤波器的最优值？为了回答这个问题，我们将从数学角度探讨CNN的工作原理。与《第5章》中的循环神经网络改进不同，CNN的数学背后非常简单而且非常有趣。
- en: Forward propagation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'Let''s begin with the forward propagation. We have already seen how forward
    propagation works and how a CNN classifies the given input image. Let''s frame
    this mathematically. Let''s consider an input matrix, *X*, and filter, *W*, with
    values shown as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从前向传播开始。我们已经看到了前向传播的工作原理以及CNN如何对给定的输入图像进行分类。让我们从数学角度来描述这个过程。让我们考虑一个输入矩阵*X*和滤波器*W*，其值如下所示：
- en: '![](img/77c1def1-3668-4604-ac80-273dfb885ff1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77c1def1-3668-4604-ac80-273dfb885ff1.png)'
- en: First, let's familiarize ourselves with the notations. Whenever we write ![](img/d683705e-7017-43b2-8fd2-31377943d59c.png),
    it implies the element in the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)
    row and the ![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png) column of the input
    matrix. The same applies to the filter and output matrix; that is, ![](img/d34f998a-542d-4bd0-b85e-3b6ede8d1626.png)
    and ![](img/401d921f-f24e-41ac-991c-3bf09f3573d2.png) represent the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png) row
    and the ![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png) column value in the
    filter and output matrix, respectively. In the previous figure, ![](img/4432ea26-b8cd-4fff-bcaa-80460506ea8d.png)
    = ![](img/e0c53dec-a173-4d9b-b7fe-332690e0ceae.png), that is, ![](img/0fa58a79-c956-40d3-8832-cc2c6d53d6e9.png)
    is the element in the first row and first column of the input matrix.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们熟悉符号。每当我们写![](img/d683705e-7017-43b2-8fd2-31377943d59c.png)，这意味着输入矩阵中第![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)行和第![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png)列的元素。滤波器和输出矩阵同理，即![](img/d34f998a-542d-4bd0-b85e-3b6ede8d1626.png)和![](img/401d921f-f24e-41ac-991c-3bf09f3573d2.png)分别表示滤波器和输出矩阵中第![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)行和第![](img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png)列的值。在前一图中，![](img/4432ea26-b8cd-4fff-bcaa-80460506ea8d.png)
    = ![](img/e0c53dec-a173-4d9b-b7fe-332690e0ceae.png)，即![](img/0fa58a79-c956-40d3-8832-cc2c6d53d6e9.png)是输入矩阵中第一行第一列的元素。
- en: 'As shown in the following diagram, we take the filter, slide it over the input
    matrix, perform a convolution operation, and produce the output matrix (the feature
    map) just as we learned in the previous section:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们取滤波器，在输入矩阵上滑动，执行卷积操作，并生成输出矩阵（特征图），就像我们在前一节中学到的那样：
- en: '![](img/2d7956a5-7923-4c51-a5c1-7c0e5fcb7e6c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d7956a5-7923-4c51-a5c1-7c0e5fcb7e6c.png)'
- en: 'Thus, all the values in the output matrix (feature map) are computed as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出矩阵（特征图）中的所有值计算如下：
- en: '![](img/0f7cb78b-c6e7-41a0-95de-d3cb4b3af679.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f7cb78b-c6e7-41a0-95de-d3cb4b3af679.png)'
- en: '![](img/f9c00bd1-1088-409e-aa64-a1b4c452bedd.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9c00bd1-1088-409e-aa64-a1b4c452bedd.png)'
- en: '![](img/d40ab13d-bd4d-45fd-b80d-766865b9a3eb.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d40ab13d-bd4d-45fd-b80d-766865b9a3eb.png)'
- en: '![](img/ed7a8206-de1d-4958-be08-9056f1feee1d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed7a8206-de1d-4958-be08-9056f1feee1d.png)'
- en: 'Okay, so we know this is how a convolution operation is performed and how the
    output is computed. Can we represent this in a simple equation? Let''s say we
    have an input image, *X,* with a width of *W* and a height of *H*, and the filter
    of size *P* x *Q*, then the convolution operation can be represented as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们知道了卷积操作的执行方式以及如何计算输出。我们可以用一个简单的方程表示这个过程吗？假设我们有一个输入图像*X*，宽度为*W*，高度为*H*，滤波器大小为*P*
    x *Q*，那么卷积操作可以表示如下：
- en: '![](img/20f207a6-3090-4335-8e1f-c124e2aceeb1.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20f207a6-3090-4335-8e1f-c124e2aceeb1.png)'
- en: This equation basically represents how the output, ![](img/d17ac902-2af3-4c84-b127-3ba512459135.png)
    (that is, the element in the ![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)
    row and the ![](img/fbd568a7-6a06-437d-a179-d4c530797237.png) column of the output
    matrix), is computed using a convolution operation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此方程基本表示了如何使用卷积操作计算输出，![](img/d17ac902-2af3-4c84-b127-3ba512459135.png)（即输出矩阵中第![](img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png)行和第![](img/fbd568a7-6a06-437d-a179-d4c530797237.png)列的元素）。
- en: 'Once the convolution operation is performed, we feed the result,![](img/d17ac902-2af3-4c84-b127-3ba512459135.png),
    to a feedforward network, ![](img/a9e5a9e6-0ec1-4ac5-a813-442c18dcabd9.png), and
    predict the output, ![](img/9913c405-9178-4da0-930c-5afa513240ca.png):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作完成后，我们将结果，![](img/d17ac902-2af3-4c84-b127-3ba512459135.png)，馈送给前馈网络，![](img/a9e5a9e6-0ec1-4ac5-a813-442c18dcabd9.png)，并预测输出，![](img/9913c405-9178-4da0-930c-5afa513240ca.png)：
- en: '![](img/b833c3e0-2ef9-4d5d-b1bf-4d7dc11a450d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b833c3e0-2ef9-4d5d-b1bf-4d7dc11a450d.png)'
- en: Backward propagation
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'Once we have predicted the output, we compute the loss, ![](img/00f83254-228d-43a5-99df-8a8cf86d6931.png).
    We use the mean squared error as the loss function, that is, the mean of the squared
    difference between the actual output, ![](img/7b4f6183-c77d-4521-bb70-849f7bf94f51.png),
    and the predicted output, ![](img/21977035-1547-4a83-a0f4-a35d0e227cac.png), which
    is given as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出后，我们计算损失，![](img/00f83254-228d-43a5-99df-8a8cf86d6931.png)。我们使用均方误差作为损失函数，即实际输出，![](img/7b4f6183-c77d-4521-bb70-849f7bf94f51.png)，与预测输出，![](img/21977035-1547-4a83-a0f4-a35d0e227cac.png)，之间差值的平均值，如下所示：
- en: '![](img/8515dd01-439a-4cec-a247-73861ef4b010.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8515dd01-439a-4cec-a247-73861ef4b010.png)'
- en: Now, we will see how can we use backpropagation to minimize the loss ![](img/6c8b8da5-079e-4817-8a65-336e26f89086.png).
    In order to minimize the loss, we need to find the optimal values for our filter
    *W*. Our filter matrix consists of four values, *w1*, *w2*, *w3*, and *w4*. To
    find the optimal filter matrix, we need to calculate the gradients of our loss
    function with respect to all these four values. How do we do that?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s recollect the equations of the output matrix, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73f8263a-b92c-4929-ac10-7c461980d67b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: '![](img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: '![](img/b88649d8-8201-40e2-8672-2c53962903f2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: '![](img/9cf41a2f-3084-4bfd-87af-56233d26714d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Don't get intimidated by the upcoming equations; they are actually pretty simple.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s calculate gradients with respect to ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png)*.*
    As you can see, ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png) appears in all
    the output equations; we calculate the partial derivatives of the loss with respect
    to ![](img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png) as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63f9fe80-68b3-4d10-8565-1aed0af0fb6a.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: '![](img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we calculate the partial derivative of the loss with respect to
    the ![](img/b7ea87fb-dbe7-4ca2-8006-eb5d0227fca7.png) weight as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c16a2cb-18bf-4d51-b67b-49b6eca0ae53.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: '![](img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of loss with respect to the ![](img/cc642919-e0bc-40be-ad22-3f8bd770015c.png)
    weights, are calculated as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a740913-b834-4054-908e-7febd562836d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of loss with respect to the ![](img/761a3045-a3d4-42c8-9f9b-38183d0bec79.png)
    weights, are given as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/986e1d83-6913-4c43-8bfc-670cd50e47b4.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: '![](img/51967733-80f6-45a7-b857-71f6bf2a5ada.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: 'So, in a nutshell, our final equations for the gradients of loss with respect
    to all the weights are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: '![](img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: '![](img/51967733-80f6-45a7-b857-71f6bf2a5ada.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: 'It turns out that computing the derivatives of loss with respect to the filter
    matrix is very simple—it is just another convolution operation. If we look at
    the preceding equations closely, we will notice they look like the result of a
    convolution operation between the input matrix and the gradient of the loss with
    respect to the output as a filter matrix, as depicted in the following diagram:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00d7add4-0916-49a8-9897-77e206151643.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'For example, let''s see how the gradients of loss with respect to weight ![](img/ebe3ea15-3942-46c9-b837-2369257cad35.png)
    are computed by the convolution operation between the input matrix and the gradients
    of loss with respect to the output as a filter matrix, as shown in the following
    diagram:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d8b588c-5392-4ca1-9d78-7cb614c10765.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d8b588c-5392-4ca1-9d78-7cb614c10765.png)'
- en: 'Thus, we can write the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成以下形式：
- en: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7e01557-af09-4335-8d95-3abafc555a57.png)'
- en: So, we understand that computing the gradients of loss with respect to the filter
    (that is, weights) is just the convolution operation between the input matrix
    and the gradient of loss with respect to the output as a filter matrix.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们了解到，计算损失对滤波器（即权重）的梯度，其实就是输入矩阵和损失对输出的梯度作为滤波器矩阵之间的卷积操作。
- en: Apart from calculating the gradients of loss with respect to the filter, we
    also need to calculate the gradients of loss with respect to an input. But why
    do we do that? Because it is used for calculating the gradients of the filters
    present in the previous layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算对滤波器的损失梯度之外，我们还需要计算对某个输入的损失梯度。但是为什么要这样做？因为它用于计算上一层中滤波器的梯度。
- en: 'Our input matrix consists of nine values, from ![](img/1a62ae19-7c7c-462f-8f2d-6a34dadb3021.png)
    to ![](img/89e4142d-a6fe-4b74-8fc9-5a030c915ac0.png), so we need to calculate
    the gradients of loss with respect to all these nine values. Let''s recollect
    how the output matrix is computed:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入矩阵包括从![](img/1a62ae19-7c7c-462f-8f2d-6a34dadb3021.png) 到![](img/89e4142d-a6fe-4b74-8fc9-5a030c915ac0.png)
    的九个值，因此我们需要计算对这九个值的损失梯度。让我们回顾一下输出矩阵是如何计算的：
- en: '![](img/73f8263a-b92c-4929-ac10-7c461980d67b.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f8263a-b92c-4929-ac10-7c461980d67b.png)'
- en: '![](img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png)'
- en: '![](img/b88649d8-8201-40e2-8672-2c53962903f2.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b88649d8-8201-40e2-8672-2c53962903f2.png)'
- en: '![](img/9cf41a2f-3084-4bfd-87af-56233d26714d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cf41a2f-3084-4bfd-87af-56233d26714d.png)'
- en: 'As you can see, ![](img/fd93e013-64ad-487b-894b-19c26344a3cc.png) is present
    only in ![](img/56e5404b-91dd-49f5-946b-2a447d893e41.png), so we can calculate
    the gradients of loss with respect to ![](img/cf1784c6-b3a1-4629-828d-e88c17764212.png)
    alone, as other terms would be zero:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，![](img/fd93e013-64ad-487b-894b-19c26344a3cc.png) 仅出现在![](img/56e5404b-91dd-49f5-946b-2a447d893e41.png)，因此我们可以单独计算对![](img/cf1784c6-b3a1-4629-828d-e88c17764212.png)
    的损失梯度，其他项为零：
- en: '![](img/bdc411e1-040b-45a8-8aad-21ba41f61d86.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdc411e1-040b-45a8-8aad-21ba41f61d86.png)'
- en: '![](img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png)'
- en: 'Now, let''s calculate the gradients with respect to ![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png)*;*
    as ![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png) is present in only ![](img/b3a81d52-41a7-4d87-8012-881d3559401c.png)
    and ![](img/4c0f720d-ad3f-43a7-b52d-76f3d5083e02.png), we calculate the gradients
    with respect to ![](img/70fd3562-41b7-4080-b3d0-ac23f22d37ed.png) and ![](img/9f4c6517-1607-42a0-8d30-ac53f753119c.png)
    alone:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算对![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png)*;* 的梯度；因为![](img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png)
    仅出现在![](img/b3a81d52-41a7-4d87-8012-881d3559401c.png) 和![](img/4c0f720d-ad3f-43a7-b52d-76f3d5083e02.png)
    中，我们仅计算对![](img/70fd3562-41b7-4080-b3d0-ac23f22d37ed.png) 和![](img/9f4c6517-1607-42a0-8d30-ac53f753119c.png)
    的梯度：
- en: '![](img/44bc61b2-09bf-4225-9468-0ac63294909a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44bc61b2-09bf-4225-9468-0ac63294909a.png)'
- en: '![](img/efae36f7-eb83-4461-8a1d-ed9ac9468b27.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efae36f7-eb83-4461-8a1d-ed9ac9468b27.png)'
- en: 'In a very similar way, we calculate the gradients of loss with respect to all
    the inputs as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以非常类似的方式，我们计算对所有输入的损失梯度如下：
- en: '![](img/3721b26c-ea6f-4830-ab9b-be534e05ba6a.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3721b26c-ea6f-4830-ab9b-be534e05ba6a.png)'
- en: '![](img/31638cdf-1ee2-4670-9a53-2e0edf004b7b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31638cdf-1ee2-4670-9a53-2e0edf004b7b.png)'
- en: '![](img/7c44f895-a2e7-4d9c-9549-cddb214fb2d5.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c44f895-a2e7-4d9c-9549-cddb214fb2d5.png)'
- en: '![](img/9f822bc7-39ea-4bdb-a4b4-b3373a8c0965.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f822bc7-39ea-4bdb-a4b4-b3373a8c0965.png)'
- en: '![](img/87fb3292-de91-4957-acf4-f18a5f8bac3f.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87fb3292-de91-4957-acf4-f18a5f8bac3f.png)'
- en: '![](img/58625695-d3df-4add-8a30-79bf551a8ed4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58625695-d3df-4add-8a30-79bf551a8ed4.png)'
- en: '![](img/5060bb60-9610-4752-a75b-7bf372964d4f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5060bb60-9610-4752-a75b-7bf372964d4f.png)'
- en: Just as we represented the gradients of the loss with respect to the weights
    using the convolution operation, can we also do the same here? It turns out that
    the answer is yes. We can actually represent the preceding equations, that is,
    the gradients of loss with respect to the inputs, using a convolution operation
    between the filter matrix as an input matrix and the gradients of loss with respect
    to the output matrix as a filter matrix. But the trick is that, instead of using
    the filter matrix directly, we rotate them 180 degrees and, also, instead of performing
    convolution, we perform full convolution. We are doing this so that we can derive
    the previous equations using a convolution operation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows what the kernel rotated by 180 degrees looks like:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e433f90c-42dc-4142-8f5e-317acf3308da.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'Okay, so what is full convolution? In the same way as a convolution operation,
    in full convolution, we use a filter and slide it over the input matrix, but the
    way we slide the filter is different from the convolution operation we looked
    at before. The following figure shows how full convolution operations work. As
    we can see, the shaded matrix represents the filter matrix and the unshaded one
    represents the input matrix; we can see how the filter slides over the input matrix
    step by step, as shown in this diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51512ba8-b3ab-40d5-9792-229050fabd1c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'So, we can say that the gradient of loss with respect to the input matrix can
    be calculated using a full convolution operation between a filter rotated by 180
    degrees as the input matrix and the gradient of the loss with respect to the output
    as a filter matrix:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e9eb3a4-7b6b-403c-9ed9-368662189258.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'For example, as shown in the following figure, we will notice how the gradients
    of loss with respect to the input, ![](img/0425842a-097d-4394-8a1b-4f81719f427f.png),
    is computed by the full convolution operation between the filter matrix rotated
    by 180 degrees, and the gradients of loss with respect to an output matrix as
    a filter matrix:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39d37a5e-1442-43a1-bad9-c70594773ad1.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'This is demonstrated as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Thus, we understand that computing the gradients of loss with respect to the
    input is just the full convolution operation. So, we can say that backpropagation
    in CNN is just another convolution operation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CNN in TensorFlow
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will learn how to build a CNN using TensorFlow. We will use the MNIST
    handwritten digits dataset and understand how a CNN recognizes handwritten digits,
    and we will also visualize how the convolutional layers extract important features
    from the image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the required libraries:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the MNIST dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Defining helper functions
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we define the functions for initializing weights and bias, and for performing
    the convolution and pooling operations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the weights by drawing from a truncated normal distribution. Remember,
    the weights are actually the filter matrix that we use while performing the convolution
    operation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过截断正态分布绘制来初始化权重。请记住，这些权重实际上是我们在执行卷积操作时使用的滤波器矩阵：
- en: '[PRE2]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize the bias with a constant value of, say, `0.1`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常量值（例如`0.1`）初始化偏置：
- en: '[PRE3]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We define a function called convolution using `tf.nn.conv2d()`, which actually
    performs the convolution operation; that is, the element-wise multiplication of
    the input matrix (`x`) by the filter (`W`) with a the stride of `1` and the same
    padding. We set `strides = [1,1,1,1]`. The first and last values of strides are
    set to `1`, which implies that we don''t want to move between training samples
    and different channels. The second and third values of `strides` are also set
    to `1`, which implies that we move the filter by `1` pixel in both the height
    and width direction:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个名为`convolution`的函数，使用`tf.nn.conv2d()`执行卷积操作；即输入矩阵（`x`）与滤波器（`W`）的逐元素乘法，步长为`1`，相同填充。我们设置`strides
    = [1,1,1,1]`。步长的第一个和最后一个值设为`1`，表示我们不希望在训练样本和不同通道之间移动。步长的第二个和第三个值也设为`1`，表示我们在高度和宽度方向上将滤波器移动`1`个像素：
- en: '[PRE4]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We define a function called `max_pooling`, using `tf.nn.max_pool()` to perform
    the pooling operation. We perform max pooling with a `stride` of `2` and the same
    `padding` and `ksize` implies our pooling window shape:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个名为`max_pooling`的函数，使用`tf.nn.max_pool()`执行池化操作。我们使用步长为`2`的最大池化，并且使用相同的填充和`ksize`指定我们的池化窗口形状：
- en: '[PRE5]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Define the placeholders for the input and output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入和输出的占位符。
- en: 'The `placeholder` for the input image is defined as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像的占位符定义如下：
- en: '[PRE6]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `placeholder` for a reshaped input image is defined as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑后输入图像的占位符定义如下：
- en: '[PRE7]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `placeholder` for the output label is defined as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出标签的占位符定义如下：
- en: '[PRE8]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the convolutional network
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义卷积网络
- en: Our network architecture consists of two convolutional layers. Each convolutional
    layer is followed by one pooling layer, and we use a fully connected layer that
    is followed by an output layer; that is, `conv1->pooling->conv2->pooling2->fully
    connected layer-> output layer`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络架构包括两个卷积层。每个卷积层后面跟着一个池化层，并且我们使用一个全连接层，其后跟一个输出层；即`conv1->pooling->conv2->pooling2->fully
    connected layer-> output layer`。
- en: First, we define the first convolutional layer and pooling layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义第一个卷积层和池化层。
- en: The weights are actually the filters in the convolutional layers. So, the weight
    matrix will be initialized as `[ filter_shape[0], filter_shape[1], number_of_input_channel,
    filter_size ]`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 权重实际上是卷积层中的滤波器。因此，权重矩阵将初始化为`[ filter_shape[0], filter_shape[1], number_of_input_channel,
    filter_size ]`。
- en: 'We use a `5 x 5` filter. Since we use grayscale images, the number of input
    channels will be `1` and we set the filter size as `32`. So, the weight matrix
    of the first convolution layer will be `[5,5,1,32]`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`5 x 5`的滤波器。由于我们使用灰度图像，输入通道数将为`1`，并且我们将滤波器大小设置为`32`。因此，第一个卷积层的权重矩阵将是`[5,5,1,32]`：
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The shape of the bias is just the filter size, which is `32`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置的形状只是滤波器大小，即`32`：
- en: '[PRE10]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Perform the first convolution operation with ReLU activations followed by max
    pooling:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ReLU激活执行第一个卷积操作，然后进行最大池化：
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we define the second convolution layer and pooling layer.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义第二个卷积层和池化层。
- en: 'As the second convolutional layer takes the input from the first convolutional
    layer, which has 32-channel output, the number of input channel, to the second
    convolutional layer becomes 32 and we use the 5 x 5 filter with a filter size
    of `64`. Thus, the weight matrix for the second convolutional layer becomes `[5,5,32,64]`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第二个卷积层从具有32通道输出的第一个卷积层接收输入，因此第二个卷积层的输入通道数变为32，并且我们使用尺寸为`5 x 5`的滤波器，因此第二个卷积层的权重矩阵变为`[5,5,32,64]`：
- en: '[PRE12]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The shape of the bias is just the filter size, which is `64`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置的形状只是滤波器大小，即`64`：
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Perform the second convolution operation with ReLU activations, followed by
    max pooling:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ReLU激活执行第二次卷积操作，然后进行最大池化：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After two convolution and pooling layers, we need to flatten the output before
    feeding it to the fully connected layer. So, we flatten the result of the second
    pooling layer and feed it to the fully connected layer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个卷积和池化层之后，我们需要在馈送到全连接层之前展平输出。因此，我们展平第二个池化层的结果并馈送到全连接层。
- en: 'Flatten the result of the second pooling layer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 展平第二个池化层的结果：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we define the weights and bias for the fully connected layer. We know that
    we set the shape of the weight matrix as `[number of neurons in the current layer,
    number of neurons layer in the next layer]`. This is because the shape of the
    input image becomes `7x7x64` after flattening and we use `1024` neurons in the
    hidden layer. The shape of the weights becomes `[7x7x64, 1024]`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为全连接层定义权重和偏置。我们设置权重矩阵的形状为 `[当前层中的神经元数，下一层中的神经元数]`。这是因为在展平之后，输入图像的形状变为 `7x7x64`，我们在隐藏层中使用
    `1024` 个神经元。权重的形状变为 `[7x7x64, 1024]`：
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is a fully connected layer with ReLU activations:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个具有 ReLU 激活函数的全连接层：
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the output layer. We have `1024` neurons in the current layer, and since
    we need to predict 10 classes, we have 10 neurons in the next layer, thus the
    shape of the weight matrix becomes `[1024 x 10]`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输出层。当前层有 `1024` 个神经元，由于我们需要预测 10 类，所以下一层有 10 个神经元，因此权重矩阵的形状变为 `[1024 x 10]`：
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compute the output with softmax activations:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 softmax 激活函数计算输出：
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Computing loss
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失
- en: 'Compute the loss using cross entropy. We know that the cross-entropy loss is
    given as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉熵计算损失。我们知道交叉熵损失如下所示：
- en: '![](img/69145fd8-d596-4fad-b1ca-7f4ac8fca30c.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69145fd8-d596-4fad-b1ca-7f4ac8fca30c.png)'
- en: 'Here, ![](img/b16d392d-d81a-4471-9a35-a41d748c2dee.png) is the actual label
    and ![](img/9455b313-8e3b-465f-8f71-c4f1d9e7286d.png) is the predicted label.
    Thus, the cross-entropy loss is implemented as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b16d392d-d81a-4471-9a35-a41d748c2dee.png) 是实际标签，![](img/9455b313-8e3b-465f-8f71-c4f1d9e7286d.png)
    是预测标签。因此，交叉熵损失实现如下：
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Minimize the loss using the Adam optimizer:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器最小化损失：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Calculate the accuracy:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 计算准确率：
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Starting the training
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始训练
- en: 'Start a TensorFlow `Session` and initialize all the variables:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 的 `Session` 并初始化所有变量：
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Train the model for `1000` epochs. Print the results for every `100` epochs:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行 `1000` 个 epochs 的训练。每 `100` 个 epochs 打印结果：
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You will notice that the loss decreases and the accuracy increases over epochs:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到随着 epochs 的增加，损失减少，准确率增加：
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Visualizing extracted features
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化提取的特征
- en: Now that we have trained our CNN model, we can see what features our CNN has
    extracted to recognize the image. As we learned, each convolutional layer extracts
    important features from the image. We will see what features our first convolutional
    layer has extracted to recognize the handwritten digits.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了我们的CNN模型，我们可以看看我们的CNN提取了哪些特征来识别图像。正如我们学到的，每个卷积层从图像中提取重要特征。我们将看看我们的第一个卷积层提取了什么特征来识别手写数字。
- en: 'First, let''s select one image from the training set, say, digit 1:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从训练集中选择一张图片，比如说数字 1：
- en: '[PRE26]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The input image is shown here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像显示如下：
- en: '![](img/908dbc36-2c73-460d-aa03-07bb29122736.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/908dbc36-2c73-460d-aa03-07bb29122736.png)'
- en: 'Feed this image to the first convolutional layer, that is, `conv1`, and get
    the feature maps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 将该图像馈送到第一个卷积层 `conv1` 中，并获取特征图：
- en: '[PRE27]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the feature map:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制特征图：
- en: '[PRE28]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can see in the following plot, the first convolutional layer has learned
    to extract edges from the given image:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在下图中看到的，第一个卷积层已经学会从给定图像中提取边缘：
- en: '![](img/793fdc93-7882-47e6-90de-acbbca82a500.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/793fdc93-7882-47e6-90de-acbbca82a500.png)'
- en: Thus, this is how the CNN uses multiple convolutional layers to extract important
    features from the image and feed these extracted features to a fully connected
    layer to classify the image. Now that we have learned how CNNs works, in the next
    section, we will learn about several interesting CNN architectures.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是CNN如何使用多个卷积层从图像中提取重要特征，并将这些提取的特征馈送到全连接层以对图像进行分类。现在我们已经学习了CNN的工作原理，在接下来的部分，我们将学习一些有趣的CNN架构。
- en: CNN architectures
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 架构
- en: In this section, we will explore different interesting types of CNN architecture.
    When we say different types of CNN architecture, we basically mean how convolutional
    and pooling layers are stacked on each other. Additionally, we will learn how
    many numbers of convolutional, pooling, and fully connected layers are used, what
    the number of filters and filter sizes are, and more.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索不同类型的CNN架构。当我们说不同类型的CNN架构时，我们基本上是指如何在一起堆叠卷积和池化层。此外，我们将了解使用的卷积层、池化层和全连接层的数量，以及滤波器数量和滤波器大小等信息。
- en: LeNet architecture
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LeNet 架构
- en: The LeNet architecture is one of the classic architectures of a CNN. As shown
    in the following diagram, the architecture is very simple, and it consists of
    only seven layers. Out of these seven layers, there are three convolutional layers,
    two pooling layers, one fully connected layer, and one output layer. It uses a
    5 x 5 convolution with a stride of 1, and uses average pooling. What is 5 x 5
    convolution? It implies we are performing a convolution operation with a 5 x 5
    filter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet架构是CNN的经典架构之一。如下图所示，该架构非常简单，仅包含七个层。在这七个层中，有三个卷积层、两个池化层、一个全连接层和一个输出层。它使用5
    x 5的卷积和步幅为1，并使用平均池化。什么是5 x 5卷积？这意味着我们正在使用一个5 x 5的滤波器进行卷积操作。
- en: 'As shown in the following diagram, LeNet consists of three convolutional layers
    (`C1`, `C3`, `C5`), two pooling layers (`S2`, `S4`), one fully connected layer
    (`F6`), and one output layer (`OUTPUT`), and each convolutional layer is followed
    by a pooling layer:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，LeNet由三个卷积层（`C1`、`C3`、`C5`）、两个池化层（`S2`、`S4`）、一个全连接层（`F6`）和一个输出层（`OUTPUT`）组成，每个卷积层后面都跟着一个池化层：
- en: '![](img/ffae1810-3122-412d-bfa0-554f2fabfa12.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffae1810-3122-412d-bfa0-554f2fabfa12.png)'
- en: Understanding AlexNet
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解AlexNet
- en: AlexNet is a classic and powerful deep learning architecture. It won the ILSVRC
    2012 by significantly reducing the error rate from 26% to 15.3%. ILSVRC stands
    for ImageNet Large Scale Visual Recognition Competition, which is one of the biggest
    competitions focused on computer vision tasks, such as image classification, localization,
    object detection, and more. ImageNet is a huge dataset containing over 15 million
    labeled, high-resolution images, with over 22,000 categories. Every year, researchers
    compete to win the competition using innovative architecture.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是一个经典而强大的深度学习架构。它通过将错误率从26%降低到15.3%而赢得了2012年ILSVRC竞赛。ILSVRC代表ImageNet大规模视觉识别竞赛，这是一个专注于图像分类、定位、物体检测等计算机视觉任务的重大竞赛。ImageNet是一个包含超过1500万标记高分辨率图像的巨大数据集，具有超过22000个类别。每年，研究人员竞争使用创新架构来赢得比赛。
- en: 'AlexNet was designed by pioneering scientists, including Alex Krizhevsky, Geoffrey
    Hinton, and Ilya Sutskever. It consists of five convolutional layers and three
    fully connected layers, as shown in the following diagram. It uses the ReLU activation
    function instead of the tanh function, and ReLU is applied after every layer.
    It uses dropout to handle overfitting, and dropout is performed before the first
    and second fully connected layers. It uses data augmentation techniques, such
    as image translation, and is trained using batch stochastic gradient descent on
    two GTX 580 GPUs for 5 to 6 days:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是由包括Alex Krizhevsky、Geoffrey Hinton和Ilya Sutskever在内的先驱科学家设计的。如下图所示，它由五个卷积层和三个全连接层组成。它使用ReLU激活函数而不是tanh函数，并且每一层之后都应用ReLU。它使用dropout来处理过拟合，在第一个和第二个全连接层之前执行dropout。它使用图像平移等数据增强技术，并使用两个GTX
    580 GPU进行5到6天的批次随机梯度下降训练：
- en: '![](img/a1414b54-38c8-4512-8ba1-53f2acbe9dd0.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1414b54-38c8-4512-8ba1-53f2acbe9dd0.png)'
- en: Architecture of VGGNet
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGGNet的架构
- en: VGGNet is one of the most popularly used CNN architectures. It was invented
    by the **Visual Geometry Group** (**VGG**) at the University of Oxford. It started
    to get very popular when it became the first runner-up of ILSVRC 2014.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet是最流行的CNN架构之一。它由牛津大学的**视觉几何组**（**VGG**）发明。当它成为2014年ILSVRC的亚军时，它开始变得非常流行。
- en: It is basically a deep convolutional network and is widely used for object-detection
    tasks. The weights and structure of the network are made available to the public
    by the Oxford team, so we can use these weights directly to carry out several
    computer vision tasks. It is also widely used as a good baseline feature extractor
    for images.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是一个深度卷积网络，广泛用于物体检测任务。该网络的权重和结构由牛津团队公开，因此我们可以直接使用这些权重来执行多个计算机视觉任务。它还广泛用作图像的良好基准特征提取器。
- en: 'The architecture of the VGG network is very simple. It consists of convolutional
    layers followed by a pooling layer. It uses 3 x 3 convolution and 2 x 2 pooling
    throughout the network. It is referred to as VGG-*n*, where *n* corresponds to
    a number of layers, excluding the pooling and softmax layer. The following figure
    shows the architecture of the VGG-16 network:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络的架构非常简单。它由卷积层和池化层组成。它在整个网络中使用3 x 3的卷积和2 x 2的池化。它被称为VGG-*n*，其中*n*对应于层数，不包括池化层和softmax层。以下图显示了VGG-16网络的架构：
- en: '![](img/fc6cc749-7c91-44dd-ab86-0682f7e7a216.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc6cc749-7c91-44dd-ab86-0682f7e7a216.png)'
- en: 'As you can see in the following figure, the architecture of AlexNet is characterized
    by a pyramidal shape, as the initial layers are wide and the later layers are
    narrow. You will notice it consists of multiple convolutional layers followed
    by a pooling layer. Since the pooling layer reduces the spatial dimension, it
    narrows the network as we go deeper into the network:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在下图中所看到的，AlexNet的架构以金字塔形状为特征，因为初始层宽度较大，而后续层次较窄。您会注意到它由多个卷积层和一个池化层组成。由于池化层减少了空间维度，随着网络深入，网络变窄：
- en: '![](img/e778b341-6235-4202-adcc-6716effb5248.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e778b341-6235-4202-adcc-6716effb5248.png)'
- en: The one shortcoming of VGGNet is that it is computationally expensive, and it
    has over 160 million parameters.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet的一个缺点是计算开销大，有超过1.6亿个参数。
- en: GoogleNet
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoogleNet
- en: '**GoogleNet**, also known as **inception net**, was the winner of ILSVRC 2014\.
    It consists of various versions, and each version is an improved version of the
    previous one. We will explore each version one by one.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**GoogleNet**，也被称为**Inception网络**，是2014年ILSVRC竞赛的获胜者。它包括各种版本，每个版本都是前一版本的改进版。我们将逐一探索每个版本。'
- en: Inception v1
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v1
- en: 'Inception v1 is the first version of the network. An object in an image appears
    in different sizes and in a different positions. For example, look at the first
    image; as you can see, the parrot, when viewed closer, takes up the whole portion
    of the image but in the second image, when the parrot is viewed from a distance,
    it takes up a smaller region of the image:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1是网络的第一个版本。图像中的对象以不同的大小和不同的位置出现。例如，看看第一张图像；正如您所看到的，当鹦鹉近距离观察时，它占据整个图像的一部分，但在第二张图像中，当鹦鹉从远处观察时，它占据了图像的一个较小区域：
- en: '![](img/00b99050-2195-47da-99e9-8ae8ec30e92f.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00b99050-2195-47da-99e9-8ae8ec30e92f.png)'
- en: Thus, we can say objects (in the given image, it's a parrot) can appear on any
    region of the image. It might be small or big. It might take up a whole region
    of the image, or just a very small portion. Our network has to exactly identify
    the object. But what's the problem here? Remember how we learned that we use a
    filter to extract features from the image? Now, because our object of interest
    varies in size and location in each image, choosing the right filter size is difficult.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说对象（在给定的图像中，是一只鹦鹉）可以出现在图像的任何区域。它可能很小，也可能很大。它可能占据整个图像的一个区域，也可能只占据一个非常小的部分。我们的网络必须精确识别对象。但是问题在哪里呢？记得我们学习过，我们使用滤波器从图像中提取特征吗？现在，因为我们感兴趣的对象在每个图像中的大小和位置都不同，所以选择合适的滤波器大小是困难的。
- en: We can use a filter of a large size when the object size is large, but a large
    filter size is not suitable when we have to detect an object that is in a small
    corner of an image. Since we use a fixed receptive field that is a fixed filter
    size, it is difficult to recognize objects in the images whose position varies
    greatly. We can use deep networks, but they are more vulnerable to overfitting.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当对象大小较大时，我们可以使用较大的滤波器大小，但是当我们需要检测图像角落中的对象时，较大的滤波器大小就不合适了。由于我们使用的是固定的感受野，即固定的滤波器大小，因此在图像中位置变化很大的图像中识别对象是困难的。我们可以使用深度网络，但它们更容易过拟合。
- en: 'To overcome this, instead of using a single filter of the same size, the inception
    network uses multiple filters of varying sizes on the same input. An inception
    network consists of nine inception blocks stacked over one another. A single inception
    block is shown in the following figure. As you will notice, we perform convolution
    operations on a given image with three different filters of varying size, that
    is, 1 x 1, 3 x 3, and 5 x 5\. Once the convolution operation is performed by all
    these different filters, we concatenate the results and feed it to the next inception
    block:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，Inception网络不使用相同大小的单个滤波器，而是在同一输入上使用多个不同大小的滤波器。一个Inception块由九个这样的块堆叠而成。下图显示了一个单独的Inception块。正如您将看到的，我们对给定图像使用三种不同大小的滤波器进行卷积操作，即1
    x 1、3 x 3和5 x 5。一旦所有这些不同的滤波器完成卷积操作，我们将结果连接起来并输入到下一个Inception块中：
- en: '![](img/f335394b-72c1-411c-9169-fccbf7e0b4ec.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f335394b-72c1-411c-9169-fccbf7e0b4ec.png)'
- en: 'As we are concatenating output from multiple filters, the depth of the concatenated
    result will increase. Although we use padding that only matches the shape of the
    input and output to be the same but we will still have different depths. Since
    the result of one inception block is the feed to another, the depth keeps on increasing.
    So, to avoid the increase in the depth, we just add a 1 x 1 convolution before
    the 3 x 3 and 5 x 5 convolution, as shown in the following figure. We also perform
    a max pooling operation, and a 1 x 1 convolution is added after the max pooling
    operation as well:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们连接多个滤波器的输出时，连接结果的深度将增加。虽然我们只使用填充来使输入和输出的形状相匹配，但我们仍然会有不同的深度。由于一个 Inception
    块的结果是另一个的输入，深度会不断增加。因此，为了避免深度增加，我们只需在 3 x 3 和 5 x 5 卷积之前添加一个 1 x 1 卷积，如下图所示。我们还执行最大池化操作，并且在最大池化操作后添加了一个
    1 x 1 卷积：
- en: '![](img/6307804b-fdfa-4d8b-adc2-66b96100293d.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6307804b-fdfa-4d8b-adc2-66b96100293d.png)'
- en: Each inception block extracts some features and feeds them to the next inception
    block. Let's say we are trying to recognize a picture of a parrot. The inception
    block in the first few layers detects basic features, and the later inception
    blocks detect high-level features. As we saw, in a convolutional network, inception
    blocks will only extract features, and don't perform any classification. So, we
    feed the features extracted by the inception block to a classifier, which will
    predict whether the image contains a parrot or not.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Inception 块提取一些特征并将其馈送到下一个 Inception 块。假设我们试图识别一张鹦鹉的图片。在前几层中，Inception 块检测基本特征，而后续的
    Inception 块则检测高级特征。正如我们所看到的，在卷积网络中，Inception 块仅提取特征，并不执行任何分类。因此，我们将 Inception
    块提取的特征馈送给分类器，该分类器将预测图像是否包含鹦鹉。
- en: 'As the inception network is deep, with nine inception blocks, it is susceptible
    to the vanishing-gradient problem. To avoid this, we introduce classifiers between
    the inception blocks. Since each inception block learns the meaningful feature
    of the image, we try to perform classification and compute loss from the intermediate
    layers as well. As shown in the following figure, we have nine inception blocks.
    We take the result of the third inception block, ![](img/b06294bd-c05c-4396-8c5a-93105abd48e4.png),
    and feed it to an intermediate classifier, and also the result of the sixth inception
    block, ![](img/dcd2ad2a-d433-4347-8e03-acde05424092.png), to another intermediate
    classifier. There is also yet another classifier at the end of the final inception
    blocks. This classifier basically consists of average pooling, 1 x 1 convolutions,
    and a linear layer with softmax activations:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Inception 网络很深，具有九个 Inception 块，因此容易受到梯度消失问题的影响。为了避免这种情况，我们在 Inception 块之间引入分类器。由于每个
    Inception 块学习图像的有意义特征，我们尝试在中间层进行分类并计算损失。如下图所示，我们有九个 Inception 块。我们将第三个 Inception
    块的结果 ![](img/b06294bd-c05c-4396-8c5a-93105abd48e4.png) 和第六个 Inception 块的结果 ![](img/dcd2ad2a-d433-4347-8e03-acde05424092.png)
    馈送到一个中间分类器，最终的 Inception 块后也有另一个分类器。这个分类器基本上由平均池化、1 x 1 卷积和具有 softmax 激活函数的线性层组成：
- en: '![](img/af8b6bd7-d29c-463b-985a-16cd92565a1c.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af8b6bd7-d29c-463b-985a-16cd92565a1c.png)'
- en: 'The intermediate classifiers are actually called auxiliary classifiers. So,
    the final loss of the inception network is the weighted sum of the auxiliary classifier''s
    loss and the loss of the final classifier (real loss), as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 中间分类器实际上被称为辅助分类器。因此，Inception 网络的最终损失是辅助分类器损失和最终分类器（真实损失）损失的加权和，如下所示：
- en: '![](img/cb05a5d5-deb7-4763-a5c4-9da8477c8b9b.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb05a5d5-deb7-4763-a5c4-9da8477c8b9b.png)'
- en: Inception v2 and v3
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v2 和 v3
- en: 'Inception v2 and v3 are introduced in the paper, *Going Deeper with Convolutions*
    by Christian Szegedy as mentioned in *Further reading* section. The authors suggest
    the use of factorized convolution, that is, we can break down a convolutional
    layer with a larger filter size into a stack of convolutional layers with a smaller
    filter size. So, in the inception block, a convolutional layer with a 5 x 5 filter
    can be broken down into two convolutional layers with 3 x 3 filters, as shown
    in the following diagram. Having a factorized convolution increases performance
    and speed:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 和 v3 是由 Christian Szegedy 在 *Going Deeper with Convolutions* 论文中介绍的，如
    *Further reading* 部分所述。作者建议使用分解卷积，即将具有较大滤波器大小的卷积层分解为具有较小滤波器大小的一组卷积层。因此，在 Inception
    块中，具有 5 x 5 滤波器的卷积层可以分解为两个具有 3 x 3 滤波器的卷积层，如下图所示。使用分解卷积可以提高性能和速度：
- en: '![](img/b4491e9b-d121-451b-8aa6-297fab8f2381.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4491e9b-d121-451b-8aa6-297fab8f2381.png)'
- en: 'The authors also suggest breaking down a convolutional layer of filter size
    *n* x *n* into a stack of convolutional layers with filter sizes *1* x *n* and
    *n* x *1*. For example, in the previous figure, we have *3* x *3* convolution,
    which is now broken down into *1* x *3* convolution, followed by *3* x *1* convolution,
    as shown in the following diagram:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还建议将大小为 *n* x *n* 的卷积层分解为大小为 *1* x *n* 和 *n* x *1* 的卷积层堆叠。例如，在前面的图中，我们有 *3*
    x *3* 的卷积，现在将其分解为 *1* x *3* 的卷积，然后是 *3* x *1* 的卷积，如下图所示：
- en: '![](img/8fa9174f-ded7-4078-ba94-9df45bdfa063.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fa9174f-ded7-4078-ba94-9df45bdfa063.png)'
- en: 'As you will notice in the previous diagram, we are basically expanding our
    network in a deeper fashion, which will lead us to lose information. So, instead
    of making it deeper, we make our network wider, shown as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的图表中所注意到的，我们基本上是以更深入的方式扩展我们的网络，这将导致我们丢失信息。因此，我们不是让网络更深，而是让我们的网络更宽，如下所示：
- en: '![](img/679c32af-bc3b-47e1-95d7-72293c3627cb.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/679c32af-bc3b-47e1-95d7-72293c3627cb.png)'
- en: In inception net v3, we use factorized 7 x 7 convolutions with RMSProp optimizers.
    Also, we apply batch normalization in the auxiliary classifiers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在 inception net v3 中，我们使用因子化的 7 x 7 卷积和 RMSProp 优化器。此外，我们在辅助分类器中应用批归一化。
- en: Capsule networks
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: '**Capsule networks** (**CapsNets**) were introduced by Geoffrey Hinton to overcome
    the limitations of convolutional networks.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**胶囊网络** (**CapsNets**) 是由 Geoffrey Hinton 提出的，旨在克服卷积网络的局限性。'
- en: 'Hinton stated the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 表示如下：
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '"卷积神经网络中使用的池化操作是一个大错误，而它如此有效地工作实际上是一场灾难。"'
- en: But what is wrong with the pooling operation? Remember when we used the pooling
    operation to reduce the dimension and to remove unwanted information? The pooling
    operation makes our CNN representation invariant to small translations in the
    input.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 但是池化操作有什么问题呢？记得当我们使用池化操作来减少维度和去除不必要的信息时吗？池化操作使我们的 CNN 表示对输入中的小平移具有不变性。
- en: This translation invariance property of a CNN is not always beneficial, and
    can be prone to misclassifications. For example, let's say we need to recognize
    whether an image has a face; the CNN will look for whether the image has eyes,
    a nose, a mouth, and ears. It does not care about which location they are in.
    If it finds all such features, then it classifies it as a face.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的这种平移不变性特性并不总是有益的，而且可能容易导致错误分类。例如，假设我们需要识别一幅图像是否有一个面部；CNN 将查找图像是否有眼睛、鼻子、嘴巴和耳朵。它不关心它们的位置。如果找到所有这些特征，它就将其分类为面部。
- en: Consider two images, as shown in the following figure. The first image is the
    actual face, and in the second image, the eyes are placed on the left side, one
    above the another, and the ears and mouth are placed on the right. But the CNN
    will still classify both the images as a face as both images have all the features
    of a face, that is, ears, eyes, a mouth, and a nose. The CNN thinks that both
    images consist of a face. It does not learn the spatial relationship between each
    feature; that the eyes should be placed at the top and should be followed by a
    nose, and so on. All it checks for is the existence of the features that make
    up the face.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两幅图像，如下图所示。第一幅图像是实际的面部图像，第二幅图像中，眼睛位于左侧，一个在另一个上方，耳朵和嘴巴位于右侧。但是 CNN 仍然会将这两幅图像都分类为面部，因为它们都具备面部的所有特征，即耳朵、眼睛、嘴巴和鼻子。CNN
    认为这两幅图像都包含一个面部。它并不学习每个特征之间的空间关系；例如眼睛应该位于顶部，并且应该跟随一个鼻子等等。它只检查构成面部的特征是否存在。
- en: 'This problem will become worse when we have a deep network, as in the deep
    network, the features will become abstract, and it will also shrink in size due
    to the several pooling operations:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个深度网络时，这个问题会变得更糟，因为在深度网络中，特征将变得抽象，并且由于多次池化操作，它的尺寸也会缩小：
- en: '![](img/0ed60e6a-2da0-4a5e-8179-3d1234e032d8.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ed60e6a-2da0-4a5e-8179-3d1234e032d8.png)'
- en: 'To overcome this, Hinton introduced a new network called the Capsule network,
    which consists of capsules instead of neurons. Like a CNN, the Capsule network
    checks for the presence of certain features to classify the image, but apart from
    detecting the features, it will also check the spatial relationship between them.
    That is, it learns the hierarchy of the features. Taking our example of recognizing
    a face, the Capsule network will learn that the eyes should be at the top and
    the nose should be in the middle, followed by a mouth and so on. If the image
    does not follow this relationship, then the Capsule network will not classify
    it as a face:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/934ce657-a389-4b18-9af9-a9655338c006.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: A Capsule network consists of several capsules connected together. But, wait.
    What is a capsule?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: A capsule is a group of neurons that learn to detect a particular feature in
    the image; say, eyes. Unlike neurons, which return a scalar, capsules return a
    vector. The length of the vector tells us whether a particular feature exists
    in a given location, and the elements of the vector represent the properties of
    the features, such as, position, angle, and so on.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have a vector, ![](img/de2c431c-b1a4-4adf-8d7a-d5e7207e9b65.png),
    as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61f11bd7-ee13-4d67-bb43-a5fbef5eba3b.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: 'The length of the vector can be calculated as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9683d9c-30b1-4c92-a117-2437ce209a93.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'We have learned that the length of the vector represents the probability of
    the existence of the features. But the preceding length does not represent a probability,
    as it exceeds 1\. So, we convert this value into a probability using a function
    called the squash function. The squash function has an advantage. Along with calculating
    probability, it also preserves the direction of the vector:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/482bbb73-5d3e-4ba8-8ae7-0d0df58c8b45.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: Just like a CNN, capsules in the earlier layers detect basic features including
    eyes, a nose, and so on, and the capsules in the higher layers detect high-level
    features, such as the overall face. Thus, capsules in the higher layers take input
    from the capsules in the lower layers. In order for the capsules in the higher
    layers to detect a face, they not only check for the presence of features such
    as a nose and eyes, but also check their spatial relationships.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of what a capsule is, we will go into
    this in more detail and see how exactly a Capsule network works.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Capsule networks
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we have two layers, ![](img/c682874b-001b-47df-89c0-2a1e2c9c965d.png)
    and ![](img/d999d9de-a068-411b-a73a-e5bc97fe347d.png). ![](img/bcbdc041-43ec-438a-b9d0-75c0100912e1.png)
    will be the lower layer and it has ![](img/4d5b5876-e2fe-4c08-bb59-32eec8809635.png)
    capsules, and ![](img/b30eca11-e52d-408f-beba-503f68cc450e.png) will be the higher
    layer and it has ![](img/1ba6e654-dbcc-434f-818d-c1a76de69601.png) capsules. Capsules
    from the lower layer send their outputs to capsules in the higher layer. ![](img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png)
    will be the activations of the capsules from the lower layer, ![](img/523529a6-1330-4a87-bbfb-8e1131f32111.png).
    ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png) will be the activations of the
    capsules from the higher layer, ![](img/b30eca11-e52d-408f-beba-503f68cc450e.png).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents a capsule, ![](img/f4b54408-3d5c-4620-9a6b-ed0a95fcdac0.png),
    and as you can observe, it takes the outputs of the previous capsules, ![](img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png),
    as inputs and computes its output, ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8411ec40-4aa9-4263-ab6a-216ebba8edac.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: We will move on to learn how ![](img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png)
    is computed.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Computing prediction vectors
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous figure, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png),
    ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png), and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png)
    represent the output vectors from the previous capsule. First, we multiply these
    vectors by the weight matrix and compute a prediction vector:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6c26383-1d9f-4096-b233-6f0c6e13e213.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Okay, so what exactly are we doing here, and what are prediction vectors? Let's
    consider a simple example. Say that capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png)
    is trying to predict whether an image has a face. We have learned that capsules
    in the earlier layers detect basic features and send their results to the capsules
    in the higher layer. So, the capsules in the earlier layer, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png),
    ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png), and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png)
    detect basic low features, such as eyes, a nose, and a mouth, and send their results
    to the capsules in the high-level layer, that is, capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png),
    which detects the face.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Thus, capsule ![](img/1ab28ef0-32dd-489e-983c-256897634e7a.png) takes the previous
    capsules, ![](img/7221635f-36de-465d-a298-4d78bdbe3005.png), ![](img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png),
    and, ![](img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png), as inputs and multiplies
    them by a weights matrix, ![](img/daf9bc01-bd76-48aa-866c-f4efcc275d91.png).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The weights matrix ![](img/39d372ef-547c-40ad-abd2-2b55badabb04.png) represents
    the spatial and other relationship between low-level features and high-level features.
    For instance, the weight ![](img/e5a11cc9-8a2c-4438-8599-a1e1fe25466e.png) tells
    us that eyes should be on the top. ![](img/2394add2-cdb3-40e9-8558-e722eb4df2f6.png)
    tells us that a nose should be in the middle.![](img/8ea16cdd-9ec9-43e3-a1de-ea08e508d657.png)
    tells us that a mouth should be on the bottom. Note that the weight matrix not
    only captures the position (that is, the spatial relationship), but also other
    relationships.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'So, by multiplying the inputs by weights, we can predict the position of the
    face:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d994f6ef-9071-44bd-8f97-0b6eb62bb99e.png) implies the predicted position
    of the face based on the eyes'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a51b641b-3884-482b-b29c-9ded8b6f29aa.png) implies the predicted position
    of the face based on the nose'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c53a8493-8c9d-4610-aaf4-c0334f70087d.png) implies the predicted position
    of the face based on the mouth'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When all the predicted positions of the face are the same, that is, in agreement
    with each other, then we can say that the image contains a face. We learn these
    weights using backward propagation.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Coupling coefficients
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we multiply the prediction vectors ![](img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png)
    by the coupling coefficients ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png).
    The coupling coefficients exist between any two capsules. We know that capsules
    from the lower layer send their output to the capsules in the higher layer. The
    coupling coefficient helps the capsule in the lower layer to understand which
    capsule in the higher layer it has to send its output to.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's consider the same example, where we are trying to predict
    whether an image consists of a face. ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png)
    represents the agreement between ![](img/49647a36-9898-4c0a-99b4-23cf0fe2d5b4.png)
    and ![](img/4cb9f8f3-5451-46ee-a62f-bbc3cef70423.png).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c658d700-33da-4b71-a264-2eee36515b37.png) represents the agreement
    between an eye and a face. Since we know that the eye is on the face, the ![](img/c658d700-33da-4b71-a264-2eee36515b37.png)
    value will be increased. We know that the prediction vector ![](img/8aab866e-89b4-49b7-ae83-568a49f9f77d.png)
    implies the predicted position of the face based on the eyes. Multiplying ![](img/1ae779e1-3dad-46f3-a74f-2bfed0ea20c2.png)
    by ![](img/44a776bc-7835-4a04-b8e6-5a163beee271.png) implies that we are increasing
    the importance of the eyes, as the value of ![](img/44a776bc-7835-4a04-b8e6-5a163beee271.png)
    is high.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1757d487-16aa-422d-a8c8-421f92b96a62.png) represents the agreement
    between nose and face. Since we know that the nose is on the face, the ![](img/a0aa2114-5372-4e01-bb5b-3670a27b9225.png)
    value will be increased. We know that the prediction vector ![](img/50b80e08-9d68-43f4-93e1-9e863f302dd9.png)
    implies the predicted position of the face based on the nose. Multiplying ![](img/3d9ab4a0-f640-410f-9a28-7f56fb1d30e3.png)
    by ![](img/07ddd2de-d13f-47c7-a594-9c546cb1ec61.png) implies that we are increasing
    the importance of the nose, as the value of ![](img/6e19a055-6cb3-4a00-bf84-c50931a89ce5.png)
    is high.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another low-level feature, say, ![](img/1f7021c5-124b-4510-9045-ac9e08df3874.png),
    which detects a finger. Now, ![](img/29e60844-2b1a-4f13-b806-8d5dca8abd49.png)
    represents the agreement between a finger and a face, which will be low. Multiplying
    ![](img/b761a33c-c89b-487c-bfc4-478297e98a63.png) by ![](img/a44a4354-8d73-4870-8662-e46fc9b96e65.png)
    implies that we are decreasing the importance of the finger, as the value of ![](img/a44a4354-8d73-4870-8662-e46fc9b96e65.png)
    is low.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: But how are these coupling coefficients learned? Unlike weights, the coupling
    coefficients are learned in the forward propagation itself, and they are learned
    using an algorithm called dynamic routing, which we will discuss later in an upcoming
    section.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'After multiplying ![](img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png) by ![](img/2d45e983-c3ec-464d-940b-75c71790fcec.png),
    we sum them up, as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d918e1b-d57e-42d2-98f8-a3daca4670cb.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can write our equation as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00a46584-ac9d-48a5-bac9-eba1cf110992.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: Squashing function
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off saying that capsule ![](img/8117a7a5-ad1c-46b9-b9e4-3831995ce70e.png)
    tries to detect the face in the image. So, we need to convert ![](img/4475e081-5628-4b62-8ba3-d38f9e364a3f.png)
    into probabilities to get the probability of the existence of a face in the image.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from calculating probabilities, we also need to preserve the direction
    of the vectors, so we use an activation function called the squash function. It
    is given as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3abea8b-efbf-4e38-9f43-a39c0b72adc2.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: Now, ![](img/47acd07c-4abb-4824-9b7b-6a21fe18405a.png) (also referred to as
    the activity vector) gives us the probability of the existence of a face in a
    given image.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing algorithm
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will see how the dynamic routing algorithm computes the coupling coefficients.
    Let's introduce a new variable called ![](img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png),
    which is just a temporary variable, and is the same as the coupling coefficients
    ![](img/0e8aeb81-9ded-4ea1-b1f9-6f431b657fcf.png). First, we initialize ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png)
    to 0\. It implies coupling coefficients between the capsules ![](img/1102139e-972c-446f-ba3b-4af96e8fde18.png)
    in the lower layer ![](img/0670a3bb-bb11-482d-b63f-a276da98c15d.png) and capsules
    ![](img/81a66aa8-aad8-4120-980d-d884d3d37c00.png) in the higher layer ![](img/57f70dd5-15ca-4d9e-b01d-b55f406c0d64.png)
    are set to 0.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/1e3a3e33-adda-4961-a733-f61ec467327a.png) be the vector representation
    of ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png). Given the prediction vectors
    ![](img/b84324a1-f6d6-49de-b61b-30b6343d6174.png), for some *n* number of iterations,
    we do the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'For all the capsules ![](img/77b2dc9f-3926-4f32-89c0-d3425563f7a5.png) in the
    layer ![](img/fcc44e5a-e3ef-4c53-ba87-ebc12c074820.png), compute the following:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7fde1dd7-8523-4f2d-817a-55f7310d647b.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: 'For all the capsules ![](img/9c4ef49d-c423-43d2-9d13-49e8d0e42eb5.png) in the
    layer ![](img/79a8569c-6df0-427f-a606-32fd6508d47a.png), compute the following:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e90ff949-6b08-4f0c-8bd4-ea5c0b6f5d3a.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: '![](img/26849c78-0b84-44c4-834d-7f41e2c656c3.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: 'For all capsules ![](img/e529ebfd-14de-40ca-9270-29301931285b.png) in ![](img/c857672e-e8b6-4bc5-bec4-e03185379626.png)
    and for all capsules in ![](img/bd605ba5-adde-471d-bead-2749d794b71d.png), compute
    ![](img/0c803f36-d666-496c-b10b-b0df922e17bd.png) as follows:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/302e217a-c547-46d2-be4a-d4ae7f42f078.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: The previous equation has to be noted carefully. It is where we update our coupling
    coefficient. The dot product ![](img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png)
    implies the dot product between the prediction vectors ![](img/4e68442b-e295-4ea4-adba-21217e91221a.png)
    of the capsule in the lower layer and the output vector ![](img/19cc1d0c-f95d-4cd8-88fc-65a1d397c85f.png)
    of the capsule in the higher layer. If the dot product is high, ![](img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png)
    will increase the respective coupling coefficient ![](img/69f2393a-d5aa-4a94-9b21-8f386ffbb9c0.png),
    which makes the ![](img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png) stronger.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the Capsule network
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose our network is trying to predict handwritten digits. We know that
    capsules in the earlier layers detect basic features, and those in the later layers
    detect the digit. So, let's call the capsules in the earlier layers **primary
    capsules** and those in the later layers **digit capsules**.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a Capsule network is shown here:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d14d16fb-2c2e-4f8e-b9a6-09a0026f2522.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can observe the following:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: First, we take the input image and feed it to a standard convolution layer,
    and we call the result convolutional inputs.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we feed the convolutional inputs to the primary capsules layer and get
    the primary capsules.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we compute digit capsules with primary capsules as input using the dynamic-routing
    algorithm.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The digit capsules consist of 10 rows, and each of the rows represents the probability
    of the predicted digit. That is, row 1 represents the probability of the input
    digit to be 0, row 2 represents the probability of the digit 1, and so on.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the input image is digit 3 in the preceding image, row 4, which represents
    the probability of digit 3, will be high in the digit capsules.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss function
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will explore the loss function of the Capsule network. The loss function
    is the weighted sum of two loss functions called margin loss and reconstruction
    loss.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Margin loss
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned that the capsule returns a vector and the length of a vector represents
    the probability of the existence of the features. Say our network is trying to
    recognize the handwritten digits in an image. To detect multiple digits in a given
    image, we use margin loss, ![](img/41912fb7-6561-4262-bdef-7252e7f5806d.png),
    for each digit capsule, ![](img/b9dd5c83-2949-44d7-986b-60a7c8c48982.png), as
    follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f038fbd8-a9ec-4841-b4d8-ddd2430b95aa.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following is the case:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/642d706d-8618-4d97-be58-ec1c5a724004.png), if the digit of a class
    ![](img/7f1c19ea-9906-42eb-9175-c4524ee9f281.png) is present'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ba3ab593-1bde-4137-9de1-937379784247.png) is the margin, and ![](img/5739739c-5563-4d46-acfc-b602f1715e16.png)
    is set to 0.9 and ![](img/b83b1abf-cf5d-47c4-9ec1-3841703f993c.png) is set to
    0.1'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9993a5d8-1324-44e2-94d6-b443be02872e.png) prevents the initial learning
    from shrinking the lengths of the vectors of all the digit capsules and is usually
    set to 0.5'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The total margin loss is the sum of the loss of all classes, ![](img/2df2e71e-36eb-4fe7-b622-3b5d36e3e4cd.png),
    as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1707881e-bf20-42cd-b127-55e5e71f7c7c.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: Reconstruction loss
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to make sure that the network has learned the important features in
    the capsules, we use reconstruction loss. This means that we use a three-layer
    network called a decoder network, which tries to reconstruct the original image
    from the digit capsules:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e714ade3-5b08-4d16-8bdd-5cc6438a31c2.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: 'Reconstruction loss is given as the squared difference between the reconstructed
    and original image, as follows:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23dd475d-dd9e-4e17-9310-7b800aef04ff.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
- en: 'The final loss is given as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/106c782e-4b1e-49f0-ad58-d9e3ecddd21b.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
- en: Here, alpha is a regularization term, because we don't want the reconstruction
    loss to have more priority than the margin loss. So, alpha is multiplied by the
    reconstruction loss to scale down its importance, and is usually set to 0.0005.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Building Capsule networks in TensorFlow
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will learn how to implement Capsule networks in TensorFlow. We will use
    our favorite MNIST dataset to learn how a Capsule network recognizes the handwritten
    image.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Load the MNIST dataset:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Defining the squash function
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned that the squash function converts the length of the vector into
    probability, and it is given as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee00267a-bcf4-4a2d-92ee-bba5e1c162c5.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: 'The `squash` function can be defined as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Defining a dynamic routing algorithm
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will look at how the dynamic routing algorithm is implemented. We use
    variable names of the same notations that we learned in the dynamic routing algorithm,
    so that we can easily follow the steps. We will look at each line in our function
    step by step. You can also check the complete code on GitHub, at [http://bit.ly/2HQqDEZ](http://bit.ly/2HQqDEZ).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the function called `dynamic_routing`, which takes the previous
    capsules, `ui`, coupling coefficients, `bij`, and number of routing iterations,
    `num_routing` as inputs as follows:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Initialize the `wij` weights by drawing from a random normal distribution,
    and initialize `biases` with a constant value:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the primary capsules `ui` (`tf.tile` replicates the tensor *n* times):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Compute the prediction vector, ![](img/b4ca5bae-01ba-43f0-ba90-7543d6de315c.png),
    as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Reshape the prediction vector:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Stop gradient computation in the prediction vector:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perform dynamic routing for a number of routing iterations, as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Computing primary and digit capsules
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will compute the primary capsules, which extract the basic features,
    and the digit capsules, which recognizes the digits.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the TensorFlow `Graph`:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the placeholders for input and output:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Perform the convolution operation and get the convolutional input:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Compute the primary capsules that extract the basic features, such as edges.
    First, compute the capsules using the convolution operation as follows:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Concatenate all the capsules and form the primary capsules, squash the primary
    capsules, and get the probability as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Apply the `squash` function to the primary capsules and get the probability:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Compute the digit capsules using a dynamic-routing algorithm as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Masking the digit capsule
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we need to mask the digit capsule? We learned that in order to make sure
    that the network has learned the important features, we use a three-layer network
    called a decoder network, which tries to reconstruct the original image from the
    digit capsules. If the decoder is able to reconstruct the image successfully from
    the digit capsules, then it means the network has learned the important features
    of the image; otherwise, the network has not learned the correct features of the
    image.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'The digit capsules contain the activity vector for all the digits. But the
    decoder wants to reconstruct only the given input digit (the input image). So,
    we mask out the activity vector of all the digits, except for the correct digit.
    Then we use this masked digit capsule to reconstruct the given input image:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Defining the decoder
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the decoder network for reconstructing the image. It consists of three
    fully connected networks, as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Computing the accuracy of the model
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we compute the accuracy of our model:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Compute the length of each activity vector in the digit capsule:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Apply `softmax` to the length and get the probabilities:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Select the index that had the highest probability; this will give us the predicted
    digit:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Compute the `accuracy`:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Calculating loss
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, we compute two types of loss—margin loss and reconstruction loss.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Margin loss
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that margin loss is given as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7a02188-ebba-41ac-a44e-4172aabfef28.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
- en: 'Compute the maximum value in the left and maximum value in the right:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Set ![](img/305c1497-9b9f-42e9-9869-d19d8e80c646.png) to ![](img/6c67bda9-c871-4a51-9eb9-c9503a77223c.png):'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The total margin loss is computed as follows:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Reconstruction loss
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reshape and get the original image by using the following code:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Compute the mean of the squared difference between the reconstructed and the
    original image:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Compute the reconstruction loss:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Total loss
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the total loss, which is the weighted sum of the margin loss and the
    reconstructed loss:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Optimize the loss using the Adam optimizer:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Training the Capsule network
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Set the number of epochs and number of steps:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now start the TensorFlow `Session` and perform training:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You can see how the loss decreases over various iterations:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Thus, we have learned how Capsule networks work step by step, and how to build
    a Capsule network in TensorFlow.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-478
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding CNNs. We learned about the different
    layers of a CNN, such as convolution and pooling; where the important features
    from the image will be extracted and are fed to the fully collected layer; and
    where the extracted feature will be classified. We also visualized the features
    extracted from the convolutional layer using TensorFlow by classifying handwritten
    digits.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about several architectures of CNN, including LeNet, AlexNet,
    VGGNet, and GoogleNet. At the end of the chapter, we studied Capsule networks,
    which overcome the shortcomings of a convolutional network. We learned that Capsule
    networks use a dynamic routing algorithm for classifying the image.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study the various algorithms used for learning
    text representations.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try answering the following questions to assess our knowledge of CNNs:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: What are the different layers of a CNN?
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define stride.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is padding required?
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define pooling. What are the different types of pooling operations?
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the architecture of VGGNet.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is factorized convolution in the inception network?
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do Capsule networks differ from CNNs?
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the squash function.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following for more information:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '*Very Deep Convolutional Networks for Large-Scale Image Recognition* by Karen
    Simonyan and Andrew Zisserman, available at [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paper on inception net, *Going Deeper with Convolutions* by Christian Szegedy
    et al., available at [https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamic Routing Between Capsules* by Sara Sabour, Nicholas Frosst, and Geoffrey
    E. Hinton, available at [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
