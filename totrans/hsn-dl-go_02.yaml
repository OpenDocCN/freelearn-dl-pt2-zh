- en: Introduction to Deep Learning in Go
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Go中深度学习介绍
- en: This book will very quickly jump into the practicalities of implementing **Deep
    Neural Networks** (**DNNs**) in Go. Simply put, this book's title contains its
    aim. This means there will be a lot of technical detail, a lot of code, and (not
    too much) math. By the time you finally close this book or turn off your Kindle,
    you'll know how (and why) to implement modern, scalable DNNs and be able to repurpose
    them for your needs in whatever industry or mad science project you're involved
    in.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将非常快速地进入在Go中实现**Deep Neural Networks**（**DNNs**）的实际操作方面。简单地说，本书的标题包含了它的目标。这意味着会有大量的技术细节、大量的代码以及（不是太多的）数学。当你最终关闭本书或关闭你的Kindle时，你将知道如何（以及为什么）实现现代可扩展的DNN，并能够根据你所从事的任何行业或疯狂的科学项目的需求重新利用它们。
- en: Our choice of Go reflects the maturing of the landscape of Go libraries built
    for the kinds of operations our DNNs perform. There is, of course, much debate
    about the trade-offs made when selecting languages or libraries, and we will devote
    a section of this chapter to our views and argue for the choices we've made.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择Go语言反映了我们的DNNs执行的操作类型所建立的Go库景观的成熟性。当然，在选择语言或库时进行的权衡存在很多争论，我们将在本章的一个部分专门讨论我们的观点，并为我们所做的选择辩护。
- en: However, what is code without context? Why do we care about this seemingly convoluted
    mix of linear algebra, calculus, statistics, and probability? Why use computers
    to recognize things in images or identify aberrant patterns in financial data?
    And, perhaps most importantly, what do the approaches to these tasks have in common?
    The initial sections of this book will try to provide some of this context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，没有上下文的代码意味着什么？为什么我们要关心这种看似混乱的线性代数、微积分、统计学和概率论的混合？为什么要使用计算机在图像中识别事物或在金融数据中识别异常模式？而且，也许最重要的是，这些任务的方法有什么共同之处？本书的初步部分将尝试提供一些这方面的背景信息。
- en: Scientific endeavor, when broken up into the disciplines that represent their
    institutional and industry specialization, is governed by an idea of progress.
    By this, we mean a kind of momentum, a moving forward, toward some end. For example,
    the ideal goal of medicine is to be able to identify and cure any ailment or disease.
    Physicists aim to understand completely the fundamental laws of nature. Progress
    trends in this general direction. Science is itself an optimization method. So,
    what might the ultimate goal of **Machine Learning** (**ML**) be?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 科学探索，当被分解为代表其机构和行业专业化的学科时，是由进步思想所统治的。通过这一点，我们指的是一种动力，一种向前推进，朝着某种目标前进。例如，医学的理想目标是能够识别和治愈任何疾病或疾病。物理学家的目标是完全理解自然界的基本法则。进步趋势是朝着这个方向的。科学本身就是一种优化方法。那么**Machine
    Learning**（**ML**）的最终目标可能是什么呢？
- en: 'We''ll be upfront. We think it''s the creation of **Artificial General Intelligence**
    (**AGI**). That''s the prize: a general-purpose learning computer to take care
    of the jobs and leave life to people. As we will see when we cover the history
    of **Deep Learning** (**DL**) in detail, founders of the top **Artificial Intelligence**
    (**AI**) labs agree that AGI represents a *meta-solution* to many of the complex
    problems in our world today, from economics to medicine to government.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接了当地说吧。我们认为这是创造**人工通用智能**（**AGI**）的过程。这就是我们的目标：一个通用学习计算机，可以处理工作，让人们留下生活。正如我们在详细讨论**Deep
    Learning**（**DL**）的历史时将看到的那样，顶尖人工智能实验室的创始人们一致认为AGI代表着今天世界上许多复杂问题的*元解决方案*，从经济学到医学再到政府。
- en: 'This chapter will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why DL?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择DL？
- en: DL—history applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL——历史及应用
- en: Overview of ML in Go
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Go中ML的概述
- en: Using Gorgonia
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Gorgonia
- en: Introducing DL
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DL
- en: We will now offer a high-level view of why DL is important and how it fits into
    the discussion about AI. Then, we will look at the historical development of DL,
    as well as current and future applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将提供深度学习（**DL**）为何重要及其如何融入人工智能（**AI**）讨论的高层视角。然后，我们将看一看DL的历史发展，以及当前和未来的应用。
- en: Why DL?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择DL？
- en: So, who are you, dear reader? Why are you interested in DL? Do you have your
    private vision for AI? Or do you have something more modest? What is your *origin
    story*?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，亲爱的读者，你是谁？你为何对DL感兴趣？你是否对AI有自己的私人愿景？还是有更为谦虚的目标？你的*起源故事*是什么？
- en: In our survey of colleagues, teachers, and meetup acquaintances, the origin
    story of someone with a more formal interest in machines has a few common features.
    It doesn't matter much if you grew up playing games against the computer, an invisible
    enemy who sometimes glitched out, or if you chased down actual bots in *id Software's Quake* back
    in the late 1990s; the idea of some combination of software and hardware thinking
    and acting independently had an impact on each of us early on in life.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对同事、老师和见面会熟人的调查中，有更为正式对机器感兴趣的人的起源故事有一些共同特征。无论你是在电脑上与看不见的敌人玩游戏，有时会出现故障，还是在上世纪90年代后期追踪*id
    Software's Quake*中的实际机器人；对软硬件结合思考和独立行动的概念在我们每个人的生活中都有影响。
- en: And then, as time passed, with age, education, and exposure to pop culture,
    your ideas grew refined and maybe you ended up as a researcher, engineer, hacker,
    or hobbyist, and now you're wondering how you might participate in booting up
    the grand machine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的流逝，随着年龄、教育和对流行文化的接触，你的想法逐渐精炼，也许你最终成为了研究员、工程师、黑客或者业余爱好者，现在你想知道如何参与启动这个宏大的机器。
- en: If your interests are more modest, say you are a data scientist looking to understand
    cutting-edge techniques, but are ambivalent about all of this talk of sentient
    software and science fiction, you are, in many ways, better prepared for the realities
    of ML in 2019 than most. Each of us, regardless of the scale of our ambition,
    must understand the logic of code and hard work through trial and error. Thankfully,
    we have very fast graphics cards.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的兴趣更为温和，比如你是一名数据科学家，想要了解前沿技术，但对这些关于有意识的软件和科幻的讨论并不感冒，那么在2019年，你在很多方面都比大多数人更为准备充足。不管我们的抱负大小如何，每个人都必须通过试验和错误理解代码的逻辑和辛勤工作。幸运的是，我们有非常快的显卡。
- en: And what is the result of these basic truths? Right now, in 2019, DL has had
    an impact on our lives in numerous ways. Hard problems are being solved. Some
    trivial, some not. Yes, Netflix has a model of your most embarrassing movie preferences,
    but Facebook has automatic image annotation for the visually impaired. Understanding
    the potential impact of DL is as simple as watching the expression of joy on the
    face of someone who has just seen a photo of a loved one for the first time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本真理的结果是什么呢？现在，在2019年，DL已经以多种方式影响了我们的生活。一些棘手的问题正在解决。有些微不足道，有些则不然。是的，Netflix有你最尴尬的电影喜好模型，但Facebook为视觉障碍者提供了自动图像标注。理解DL的潜力就像看到有人第一次看到爱人照片时脸上的喜悦表情一样简单。
- en: DL – a history
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习（DL）- 一个历史
- en: 'We will now briefly cover the history of DL and the historical context from
    which it emerged, including the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要介绍DL的历史以及它出现的历史背景，包括以下内容：
- en: The idea of **AI**
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI**的概念'
- en: The beginnings of computer science/information theory
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机科学/信息论的起源
- en: Current academic work about the state/future of DL systems
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前有关DL系统状态/未来的学术工作
- en: While we are specifically interested in DL, the field didn't emerge out of nothing.
    It is a group of models/algorithms within ML itself, a branch of computer science.
    It forms one approach to AI. The other, so-called **symbolic AI**, revolves around
    hand-crafted (rather than learned) features and rules written in code, rather
    than a weighted model that contains patterns extracted from data algorithmically.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们特别关注DL，但这个领域并非从无到有。它是机器学习中的一组模型/算法，是计算机科学的一个分支。它构成了AI的一种方法。另一种所谓的**符号AI**则围绕着手工制作（而不是学习得来的）特征和编码中的规则，而不是从数据中算法提取模式的加权模型。
- en: The idea of thinking machines, before becoming a science, was very much a fiction
    that began in antiquity. The Greek god of arms manufacturing, *Hephaestus*, built
    automatons out of gold and silver. They served his whims and are an early example
    of human imagination naturally considering what it might take to replicate an
    embodied form of itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在成为一门科学之前，思考机器的想法在古代就是虚构。希腊的冶金之神*赫菲斯托斯*用金银制造了自动机器人。它们为他服务，是人类想象力自然地考虑如何复制自身体现形式的早期例子。
- en: Bringing the history forward a few thousand years, there are several key figures
    in 20^(th)-century information theory and computer science that built the platform
    that allowed the development of AI as a distinct field, including the recent work
    in DL we will be covering.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将历史推进几千年，20世纪信息理论和计算机科学的几位关键人物建立了AI作为一个独特领域的平台，包括我们将要涵盖的DL最近的工作。
- en: The first major figure, Claude Shannon, offered us a general theory of communication.
    Specifically, he described, in his landmark paper, *A Mathematical Theory of Computation,* how
    to ensure against information loss when transmitting over an imperfect medium
    (like, say, using vacuum tubes to perform computation). This notion, particularly
    his noisy-channel coding theorem, proved crucial for handling arbitrarily large
    quantities of data and algorithms reliably, without the errors of the medium itself
    being introduced into the communications channel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个重要人物，Claude Shannon，为我们提供了通信的一般理论。具体地，在他的里程碑论文*A Mathematical Theory of Computation*中，他描述了如何在使用不完善介质（例如使用真空管进行计算）时避免信息丢失。特别是他的噪声信道编码定理，对于可靠地处理任意大量数据和算法而言至关重要，而不引入介质本身的错误到通信通道中。
- en: 'Alan Turing described his *Turing machine* in 1936, offering us a universal
    model of computation. With the fundamental building blocks he described, he defined
    the limits of what a machine might compute. He was influenced by John Von Neumann''s
    idea of the *stored-program*. The key insight from Turing''s work is that digital
    computers can simulate any process of formal reasoning (the *Church-Turing* hypothesis). The
    following diagram shows the Turing machine process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 1936年，Alan Turing描述了他的*Turing machine*，为我们提供了一个通用的计算模型。他所描述的基本构建块定义了机器可能计算的极限。他受到John
    Von Neumann关于*stored-program*的想法的影响。Turing工作的关键洞见在于数字计算机可以模拟任何形式推理过程（*Church-Turing*假设）。下图显示了图灵机的工作过程：
- en: '![](img/65ebbc44-c930-4920-a13f-f268400e5caf.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65ebbc44-c930-4920-a13f-f268400e5caf.png)'
- en: '*So, you mean to tell us, Mr. Turing, that computers might be made to reason…like
    us?!*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*所以，你是想告诉我们，图灵先生，计算机也可以像我们一样推理？！*'
- en: John Von Neumann was himself influenced by Turing's 1936 paper. Before the development
    of the transistor, when vacuum tubes were the only means of computation available
    (in systems such as ENIAC and its derivatives), John Von Neumann published his
    final work. It remained incomplete at his death and is entitled *The Computer
    and the Brain*. Despite remaining incomplete, it gave early consideration to how
    models of computation may operate in the brain as they do in machines, including
    observations from early neuroscience around the connections between neurons and
    synapses.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: John Von Neumann本人受到了图灵1936年论文的影响。在晶体管开发之前，当真空管是唯一的计算手段时（如ENIAC及其衍生系统），John
    Von Neumann发表了他的最终作品。在他去世时仍未完成，题为*The Computer and the Brain*。尽管未完成，但它初步考虑了计算模型如何在大脑中运作，正如它们在机器中一样，包括早期神经科学对神经元和突触之间连接的观察。
- en: Since AI was first conceived as a discrete field of research in 1956, with ML
    coined in 1959, the field has gone through a much-discussed ebb and flow—periods
    where hype and funding were plentiful, and periods where private sector money
    was non-existent and research conferences wouldn't even accept papers that emphasized
    neural network approaches to building AI systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自从1956年AI首次被定义为一个独立的研究领域，而ML的术语则是在1959年被创造出来，这个领域经历了一个被广泛讨论的起伏过程——既有兴奋和充裕的资金的时期，也有私营部门资金不存在且研究会议甚至不接受强调神经网络方法用于构建AI系统的论文的时期。
- en: Within AI itself, these competing approaches cannibalized research dollars and
    talent. Symbolic AI met its limitations in the sheer impossibility of handcrafting
    rules for advanced tasks such as image classification, speech recognition, and
    machine translation. ML sought to radically reconfigure this process. Instead
    of applying a bunch of human-written rules to data and hoping to get answers,
    human labor was, instead, to be spent on building a machine that could infer rules
    from data when the answers were known. This is an example of *supervised learning*,
    where the machine learns an essential *cat-ness* after processing thousands of
    example images with an associated *cat* label.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI领域内部，这些竞争方法消耗了大量的研究经费和人才。符号AI在手工制定规则以完成像图像分类、语音识别和机器翻译等高级任务的不可能性方面达到了其限制。ML试图根本性地重新配置这一过程。与其将一堆人类编写的规则应用于数据并希望得到答案，人类劳动力反而被用于构建一台可以在已知答案时从数据中推断规则的机器。这是*监督学习*的一个例子，其中机器在处理数千个带有关联*cat*标签的示例图像后学习了本质*cat-ness*。
- en: 'Quite simply, the idea was to have a system that could generalize. After all,
    the goal is AGI. Take a picture of your family''s newest furry feline and the
    computer, using its understanding of *cat-ness*, correctly identifies a *cat*! An
    active area of research within ML, one thought essential for building a general
    AI, is *transfer learning*, where we might take the machine that understands *cat-ness* and
    plug it into a machine that, in turn, acts when *cat-ness* is identified. This
    is the approach many AI labs around the world are taking: building systems out
    of systems, augmenting algorithmic weakness in one area with statistical near
    certainty in another, and, hopefully, building a system that better serves human
    (or business) needs.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这个想法是要建立一个能够推广的系统。毕竟，我们的目标是通用人工智能（AGI）。拍下家庭最新的毛茸茸的猫的照片，计算机利用其对*猫本质*的理解正确识别出一只*猫*！在机器学习中，一个被认为对构建通用人工智能至关重要的研究领域是*迁移学习*，其中我们可以将理解*猫本质*的机器转移到在识别*猫本质*时进行操作的机器上。这是世界各地许多人工智能实验室采取的方法：将系统建立在系统之上，利用在一个领域中的算法弱点与另一个领域中的统计学几乎确定性的增强，并希望构建一个更好地为人类（或企业）需求服务的系统。
- en: The notion of *serving human needs* brings us to an important point regarding
    the ethics of AI (and the DL approaches we will be looking at). There has been
    much discussion in the media and academic or industry circles around the ethical
    implications of these systems. What does it mean for our society if we have easy,
    automated, widespread surveillance thanks to advances in computer vision? What
    about automated weapons systems or manufacturing? It is no longer a stretch to
    imagine vast warehouses staffed by nothing with a pulse. What then for the people
    who used to do those jobs?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*为人类需求服务* 的概念引向了关于人工智能伦理（以及我们将要探讨的深度学习方法）的一个重要观点。媒体和学术或行业圈子已经就这些系统的伦理影响进行了大量讨论。如果由于计算机视觉的进步，我们可以轻松实现自动化广泛的监视，这对我们社会意味着什么？自动武器系统或制造业又如何？想象一下庞大的仓库不再有任何人类员工，这已不再是一种遥远的设想。那么，曾经从事这些工作的人们将何去何从？'
- en: Of course, full consideration of these important issues lies outside the scope
    of this book, but this is the context in which our work takes place. You will
    be one of the privileged few able to build these systems and move the field forward.
    The work of the **Future of Humanity Institute** at Oxford University, run by
    Nick Bostrom, and the **Future of Life Institute**, run by MIT physicist, Max
    Tegmark, are two examples of where the kind of academic debate around AI ethics
    issues is taking place. This debate is not limited to academic or non-profit circles,
    however; DeepMind, an Alphabet company whose goal is to be an *Apollo program
    for AGI*, launched *DeepMind Ethics & Society* in October 2017.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，全面考虑这些重要问题超出了本书的范围，但这正是我们工作的背景。你将成为为数不多能够构建这些系统并推动该领域发展的幸运之一。牛津大学未来人类研究所（由尼克·博斯特罗姆领导）和麻省理工学院未来生命研究所（由麦克斯·泰格马克领导）的工作是学术界围绕人工智能伦理问题进行辩论的两个例子。然而，这场辩论并不仅限于学术或非营利圈子；DeepMind，一个旗下的阿尔法母公司旨在成为“AGI的阿波罗计划”，于2017年10月推出了“DeepMind伦理与社会”。
- en: This may seem far removed from the world of code and CUDA and neural networks
    to recognize cat pictures, but, as progress is made and these systems become more
    advanced and have more wide-ranging applications, our societies will face real
    consequences. As researchers and developers, it behooves us to have some answers,
    or at least ideas of how we might deal with these challenges when we have to face
    them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可能看似与代码、CUDA和神经网络识别猫图片的世界毫无关系，但随着技术的进步和这些系统的日益先进及广泛应用，我们的社会将面临真实的后果。作为研究人员和开发者，我们有责任找到一些答案，或者至少有应对这些挑战的想法。
- en: DL – hype or breakthrough?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL – 炒作还是突破？
- en: 'DL and the associated hype is a relatively recent development. Most discussion
    of its *emergence* centers around the ImageNet benchmarks of 2012, where a deep
    convolutional neural network beat the previous year''s error rate by 9%, a significant
    improvement where previous winners had made incremental improvements at best with
    techniques that used hand-crafted features in their models. The following diagram
    shows this improvement:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习及其相关的炒作是近来的一个发展。大多数关于其*出现*的讨论集中在2012年的ImageNet基准测试上，深度卷积神经网络将错误率比上一年提高了9%，这是一个显著的改进，而以往的优胜者最多只能通过使用模型中手工制作的特征来进行增量改进。以下图表展示了这一改进：
- en: '![](img/1bcd2687-c8b6-4343-b5cf-6b58bc606a4c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bcd2687-c8b6-4343-b5cf-6b58bc606a4c.png)'
- en: Despite the recent hype, the components that make DL work, which allow us to
    train deep models, have proven very effective in image classification and various
    other tasks. These were developed in the 1980s by Geoffrey Hinton and his group
    at the University of Toronto. Their early work took place during one of the *flow* periods
    discussed earlier in this chapter. Indeed, they were wholly dependent on funding from the **Canadian
    Institute for Advanced Research** (**CIFAR**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近的炒作，使DL正常运转的组成部分——使我们能够训练深度模型的组成部分，在图像分类和各种其他任务中已被证明非常有效。这些是由Geoffrey Hinton及其在多伦多大学的团队于1980年代开发的。他们的早期工作发生在本章早些时候讨论的*流动*时期之一。事实上，他们完全依赖于来自**加拿大高级研究所**（**CIFAR**）的资助。
- en: As the 21^(st) century began in earnest, after the tech bubble that had burst
    in March 2000 began to inflate again, the availability of high-performance GPUs
    and the growth in computational power more generally meant that these techniques,
    which had been developed decades earlier but had gone unused due to a lack of
    funding and industry interest, suddenly became viable. Benchmarks that previously
    saw only incremental improvements in image recognition, speech recognition, natural
    language processing, and sequence modeling all had their *y-*axes adjusted.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着21世纪正式开始，2000年3月爆发的科技泡沫再度膨胀，高性能GPU的可用性以及计算能力的普遍增长意味着这些技术，尽管几十年前已经开发，但由于缺乏资金和行业兴趣而未被使用，突然变得可行。以往在图像识别、语音识别、自然语言处理和序列建模中只能看到渐进改进的基准都调整了它们的*y-*轴。
- en: It was not just massive advances in hardware paired with old algorithms that
    got us to this point. There have been algorithmic advances that have allowed us
    to train particularly deep networks. The most well-known of these is batch normalization,
    introduced in 2015\. It ensures numeric stabilization across layers and can prevent
    exploding gradients, reducing training time dramatically. There is still active
    debate about *why* batch normalization is so effective. An example of this is
    a paper published in May 2018 refuting the central premise of the original paper,
    namely, that it is not the *internal co-variant shift* that is reduced, rather
    it *makes the optimization landscape smoother*, that is, the gradients can more
    reliably propagate, and the effects of a learning rate on training time and stability
    are more predictable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是硬件的巨大进步与旧算法的结合使我们达到了这一点。还有算法上的进步，允许我们训练特别深的网络。其中最著名的是批归一化，于2015年引入。它确保各层之间的数值稳定，并可以防止梯度爆炸，显著减少训练时间。关于批归一化为何如此有效仍存在活跃的讨论。例如，2018年5月发表的一篇论文驳斥了原始论文的核心前提，即并非*内部协变移位*被减少，而是*使优化景观更加平滑*，即梯度可以更可靠地传播，学习率对训练时间和稳定性的影响更加可预测。
- en: Collectively, from the folk science of ancient Greek myths to the very real
    breakthroughs in information theory, neuroscience, and computer science, specifically
    in models of computation, have combined to produce network architectures and the
    algorithms needed to train them that scale well to solving many fundamental AI
    tasks in 2018 that had proven intractable for decades.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从古希腊神话的民间科学到信息理论、神经科学和计算机科学的实际突破，特别是计算模型，这些集合产生了网络架构和用于训练它们的算法，这些算法能够很好地扩展到解决2018年多个基础AI任务，这些任务几十年来一直难以解决。
- en: Defining deep learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义深度学习
- en: Now, let's take a step back and start with a simple, working definition of DL.
    As we work through this book, our understanding of this term will evolve, but,
    for now, let's consider a simple example. We have an image of a person. How can
    we *show* this image to a computer? How can we *teach* the computer to associate
    this image with the word *person*?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们退后一步，从一个简单且可操作的深度学习（DL）定义开始。当我们逐步阅读本书时，对这个术语的理解会逐渐加深，但现在，让我们考虑一个简单的例子。我们有一个人的图像。如何向计算机*展示*这个图像？如何*教*计算机将这个图像与*人*这个词关联起来？
- en: First, we figure out a representation of this image, say the RGB values for
    every pixel in the image. We then feed that array of values (together with several
    trainable parameters) into a series of operations we're quite familiar with (multiplication
    and addition). This produces a new representation that we can use to compare against
    a representation we know maps to the label, *person*. We automate this process
    of comparison and update the values of our parameters as we go.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们找出了这个图像的一个表示，比如图像中每个像素的RGB值。然后，我们将这个数值数组（连同几个可训练参数）输入到一系列我们非常熟悉的操作中（乘法和加法）。这样就产生了一个新的表示，我们可以用它来与我们知道的映射到标签“人”的表示进行比较。我们自动化这个比较过程，并随着进行更新我们参数的值。
- en: This description covers a simple, shallow ML system. We'll get into more detail
    in a later chapter devoted to neural networks but, for now, to make this system
    deep, we increase the number of operations on a greater number of parameters.
    This allows us to capture more information regarding the thing we're representing
    (the person's image). The biological model that influences the design of this
    system is the human nervous system, including neurons (the things we fill with
    our representations) and synapses (the trainable parameters).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个描述涵盖了一个简单的、浅层的机器学习系统。我们将在后面专门讨论神经网络的章节中进一步详细介绍，但是现在，为了使这个系统变得更深入，我们增加了对更多参数的操作。这使我们能够捕获更多关于我们所代表的事物（人的形象）的信息。影响这个系统设计的生物模型是人类神经系统，包括神经元（我们用我们的表现填充的东西）和突触（可训练的参数）。
- en: 'The following diagram shows the ML system in progress:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了正在进行中的机器学习系统：
- en: '![](img/128ef3d0-ba49-495c-a248-46d20f08cfd6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/128ef3d0-ba49-495c-a248-46d20f08cfd6.png)'
- en: So, DL is just an evolutionary twist on the 1957's *perceptron,* the simplest
    and the original binary classifier.This twist, together with dramatic increases
    in computing power, is the difference between a system that doesn't work and a
    system that allows a car to drive autonomously.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度学习只是对1957年的*感知器*的一种进化性变化，这是最简单和最原始的二元分类器。这种变化，再加上计算能力的显著增强，是使得一个系统不能工作和使得一辆车能够自主驾驶的差异。
- en: Beyond self-driving cars, there are numerous applications for DL and related
    approaches in farming, crop management, and satellite image analysis. Advanced
    computer vision powers machines that remove weeds and reduce pesticide use. We
    have near-real-time voice search that is fast and accurate. This is the fundamental
    stuff of society, from food production to communication. Additionally, we are
    also on the cusp of compelling, real-time video and audio generation, which will
    make today's privacy debates or drama about what is *fake news* look minor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动驾驶汽车，深度学习和相关方法在农业、作物管理和卫星图像分析中也有许多应用。先进的计算机视觉技术驱动能够除草和减少农药使用的机器。我们拥有几乎实时的快速准确语音搜索。这些是社会的基础，从食品生产到通信。此外，我们还处于引人注目的实时视频和音频生成的前沿，这将使当今关于“假新闻”等隐私争论或戏剧显得微不足道。
- en: Long before we get to AGI, we can improve the world around us using the discoveries
    we make along the way. DL is one of these discoveries. It will drive an increase
    in automation, which, as long as the political change that accompanies it is supportive,
    can offer improvements across any number of industries, meaning goods and services
    will get cheaper, faster, and more widely available. Ideally, this means people
    will be set increasingly free from the routines of their ancestors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们达到通用人工智能之前，我们可以利用我们沿途发现的发现来改善我们周围的世界。深度学习就是这些发现之一。它将推动自动化的增加，只要伴随其而来的政治变革是支持的，就能在任何行业中提供改进，意味着商品和服务将变得更便宜、更快速和更广泛地可用。理想情况下，这意味着人们将越来越多地从他们祖先的日常中获得自由。
- en: 'The darker side of progress is not to be forgotten either. Machine vision that
    can identify victims can also identify targets. Indeed, the Future of Life Institute''s
    open letter on autonomous weapons (*Autonomous Weapons: an Open Letter from AI
    & Robotics Researchers*), endorsed by science and tech luminaries such as Stephen
    Hawking and Elon Musk, is an example of the interplay and tensions between academic
    departments, industry labs, and governments about what the right kind of progress
    is. In our world, the nation-state has traditionally controlled the guns and the
    money. Advanced AI can be weaponized, and this is a race where perhaps one group
    wins and the rest of us lose.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 进步的阴暗面也不容忽视。可以识别受害者的机器视觉也可以识别目标。事实上，未来生命研究所关于自主武器的公开信（*自主武器：AI 和机器人研究人员的公开信*），由史蒂芬·霍金和埃隆·马斯克等科学技术名人支持，是学术部门、工业实验室和政府之间关于进步的正确方式的相互作用和紧张关系的一个例子。在我们的世界中，传统上国家控制着枪支和金钱。先进的
    AI 可以被武器化，这是一场也许一个组织赢，其余人输的竞赛。
- en: More concretely, the field of ML is progressing incredibly fast. How might we
    measure this? The premier ML conference **Neural Information Processing Systems**
    (**NIPS**) has over seven times the registrations in the year 2017 that it did
    in 2010.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，机器学习领域的发展速度非常快。我们如何衡量这一点？首屈一指的机器学习会议 **Neural Information Processing Systems**（**NIPS**）在
    2017 年的注册人数比 2010 年增加了七倍多。
- en: 'Registrations for 2018 happened more in the manner of a rock concert than a
    dry technical conference, reflected in the following statistic tweeted out by
    the organizers themselves:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年的注册活动更像是一场摇滚音乐会，而不是干燥的技术会议，这反映在组织者自己发布的以下统计数据中：
- en: '![](img/7352c9fb-222f-43e5-b5f8-cd56732a7ce2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7352c9fb-222f-43e5-b5f8-cd56732a7ce2.png)'
- en: The *de facto* central repository of ML preprints, **arXiv**, has a hockey-stick
    growth chart of such extremes, where tools have emerged to help researchers to
    track all of the new work. An example of this is the director of AI at Tesla,
    Andrej Karpathy's site, arxiv-sanity ([http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)).
    This site allows us to sort/group papers and organize an interface by which we
    can pull research we're interested in from the stream of publications with relative
    ease.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*de facto* 机器学习预印本的中心库 **arXiv**，其增长曲线呈现出极端的“曲棍球”形态，新工具的出现帮助研究人员追踪所有新工作。例如特斯拉的
    AI 主管 Andrej Karpathy 的网站 arxiv-sanity（[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)）。该网站允许我们对论文进行排序/分组，并组织一个接口，从中轻松地提取我们感兴趣的研究成果。'
- en: We cannot predict what will happen to the rate of progress over the next five
    years. The professional guesses of venture capitalists and pundits range from
    exponential to *the next AI winter is nigh*. But we have techniques and libraries
    and compute power *now,* and knowing how to use them at their limits for a natural
    language processing or computer vision task can help to solve real-world problems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法预测未来五年进展速度的变化。风险投资家和专家的专业猜测从指数增长到*下一个 AI 冬季即将来临*不等。但是，我们现在拥有技术、库和计算能力，知道如何在自然语言处理或计算机视觉任务中充分利用它们，可以帮助解决真实世界的问题。
- en: This is what our book aims to show you how to do.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的书的目的所在，展示如何实现这一点。
- en: Overview of ML in Go
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Go 中的机器学习概述
- en: This section will take a look at the ML ecosystem in Go, first discussing the
    essentials we want from a library, and then assessing each of the main Go ML libraries
    in turn.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将审视 Go 中的机器学习生态系统，首先讨论我们从一个库中期望的基本功能，然后依次评估每个主要的 Go 机器学习库。
- en: Go's ML ecosystem has historically been quite limited. The language was introduced
    in 2009, well before the DL revolution that has brought many new programmers into
    the fold. You might assume that Go has seen the growth in libraries and tools
    that other languages have. History, instead, determined that many of the higher-level
    APIs for the mathematical operations that underpin our networks have appeared
    as Python libraries (or have complete Python bindings). There are numerous well-known
    examples of this, including PyTorch, Keras, TensorFlow, Theano, and Caffe (you
    get the idea).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的 ML 生态系统在历史上一直相当有限。该语言于 2009 年推出，远早于深度学习革命，该革命吸引了许多新程序员加入。你可能会认为 Go 在库和工具的增长方面与其他语言一样。然而，历史决定了我们的网络基础数学操作的许多高级
    API 出现为 Python 库（或具有完整的 Python 绑定）。有许多著名的例子，包括 PyTorch、Keras、TensorFlow、Theano
    和 Caffe（你明白的）。
- en: Unfortunately, these libraries have either zero or incomplete bindings for Go.
    For example, TensorFlow can do inference (*Is this a cat or not?*), but not training
    (*What is a cat anyway? Oh, okay, I'll take a look at these examples and figure
    it out*). While this takes advantage of Go's strengths when it comes to deployment
    (compilation down to single binary, compiler speed, and low memory footprint),
    from a developer's perspective, you're then forced to work across two languages
    (Python for training your model and Go for running it).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些库要么没有Go的绑定，要么绑定不完整。例如，TensorFlow可以进行推断（*这是一只猫吗？*），但无法进行训练（*到底什么是猫？好的，我会看这些例子并找出来*）。虽然这利用了Go在部署时的优势（编译成单个二进制文件、编译速度快且内存占用低），但从开发者的角度来看，你将被迫在两种语言之间工作（用Python训练模型，用Go运行模型）。
- en: 'Issues you may face, beyond the cognitive hit of switching syntax when designing,
    implementing, or troubleshooting, extend to environment and configuration problems.
    These problems include questions such as: *Is my Go environment configured properly*? *Is
    my Python 2 binary symlinked to Python or is it Python 3*? *Is TensorFlow GPU
    working properly*? If our interest is in designing the best model and getting
    it trained and deployed in the minimum amount of time, none of the Python or Go
    bindings libraries are suitable.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在设计、实施或故障排除时语法转换的认知冲击之外，您可能会遇到的问题还涉及环境和配置问题。这些问题包括：*我的Go环境配置正确吗*？*我的Python
    2二进制链接到Python还是Python 3*？*TensorFlow GPU正常工作吗*？如果我们的兴趣在于设计最佳模型并在最短时间内进行训练和部署，那么Python或Go绑定库都不合适。
- en: 'It is important, at this point, to ask: so, what do we want out of a *DL library*
    in Go? Quite simply, we want as much unnecessary complication abstracted away
    as possible while preserving flexibility and control over our model and how it
    is trained.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此时很重要的一点是问：那么，我们希望从Go中的*DL库*中得到什么？简单来说，我们希望尽可能地摆脱不必要的复杂性，同时保留对模型及其训练方式的灵活性和控制。
- en: 'What does this mean in practice? The following list outlines the answers to
    this query:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中这意味着什么？以下清单概述了这个问题的答案：
- en: We do not want to interface with **Basic Linear Algebra Subprograms** (**BLAS**) directly
    to construct basic operations such as multiplication and addition.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不想直接与**基本线性代数子程序**（**BLAS**）接口，以构建乘法和加法等基本操作。
- en: We do not want to define a tensor type and associated function(s) each time
    we implement a new network.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不想在每次实现新网络时都定义张量类型和相关函数。
- en: We do not want to write an implementation of **Stochastic Gradient Descent**
    (**SGD**) from scratch each time we want to train our network.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不想每次训练网络时都从头开始实现**随机梯度下降**（**SGD**）。
- en: 'The following are some of the things that will be covered in this book:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将涵盖以下一些内容：
- en: '**Automatic or symbolic differentiation**: Our DNN is trying to learn some
    function. It iteratively *solves* the problem of *what is the function that will
    take an input image and output the label cat*? by calculating the gradient (the
    *gradient descent optimizations*) with respect to the loss (*how wrong is our
    function*?). This allows us to understand whether to change the weights in our
    network and by how much, with the specific mode of differentiation *breaking up*
    the calculation of these gradients (effectively using the chain rule), giving
    us the performance we need to be able to train deep networks with millions of
    parameters.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动或符号微分**：我们的深度神经网络试图学习某个函数。它通过计算梯度（*梯度下降优化*）来迭代地解决“如何设计一个函数，使其能够接受输入图像并输出标签猫”的问题，同时考虑到损失函数（*我们的函数有多错误*？）。这使我们能够理解是否需要改变网络中的权重以及改变的幅度，微分的具体方式是通过链式法则（*将梯度计算分解*），这给我们提供了训练数百万参数深度网络所需的性能。'
- en: '**N****umerical stabilization** **function****s**: This is essential for DL,
    as we will explore in later sections of this book. A primary example of this is
    **Batch Normalization** or BatchNorm, as the attendant function is often called.
    It aims to put our data on the same scale to increase training speed, and it reduces
    the possibility of maximum values cascading through the layers and causing gradient
    explosion (something we will discuss in greater detail in [Chapter 2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml),
    *What is a Neural Network and How Do I Train One?*).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值稳定化函数**：这对DL至关重要，正如本书后面章节中将会探讨的那样。一个主要的例子就是**批归一化**或者BatchNorm，常被称为附带函数。它的目标是将我们的数据放在同一尺度上，以增加训练速度，并减少最大值通过层级导致的梯度爆炸的可能性（这是我们将在[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)，《什么是神经网络以及如何训练一个？》中详细讨论的内容）。'
- en: '**Activation functions**: These are mathematical operations that introduce
    nonlinearities into the various layers in our neural network and help to determine
    which neurons in a layer will be *activated*, passing their values down to the
    next layer in the network. Examples include Sigmoid, **Rectified Linear Unit**
    (**ReLU**), and Softmax. These will be considered in greater detail in [Chapter
    2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml), *What is a Neural Network and
    How Do I Train One?*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：这些是引入非线性到我们神经网络各层的数学操作，帮助确定哪些层中的神经元将被*激活*，将它们的值传递到网络中的下一层。例如Sigmoid，**修正线性单元**（**ReLU**）和Softmax。这些将在[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)，《什么是神经网络以及如何训练一个？》中更详细地讨论。'
- en: '**Gradient descent optimizations**: We will also cover these extensively in
    [Chapter 2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml), *What is a Neural Network
    and How Do I Train One?* But, as the primary optimization method used in DNNs,
    we consider this a core function necessary for any library to have DL as its purpose.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降优化**：我们还将在[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)，《什么是神经网络以及如何训练一个？》中广泛讨论这些。但是作为DNN中主要的优化方法，我们认为这是任何以DL为目的的库必须具备的核心功能。'
- en: '**CUDA support**: Nvidia''s drivers allow us to offload the fundamental operations
    involved in our DNNs to our GPU. GPUs are really great for parallel workloads
    involving matrix transformations (indeed, this was their original purpose: computing
    the world-geometry of games) and can reduce the time it takes to train your model
    by an order of magnitude or more. Suffice to say, CUDA support is essential for
    modern DNN implementations and is therefore available in the major Python libraries
    described previously.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CUDA支持**：Nvidia的驱动程序允许我们将DNN中涉及的基本操作卸载到GPU上。GPU非常适合并行处理涉及矩阵变换的工作负载（事实上，这是它们最初的用途：计算游戏的世界几何），可以将训练模型的时间减少一个数量级甚至更多。可以说，CUDA支持对现代DNN实现至关重要，因此在前述主要Python库中是可用的。'
- en: '**Deployment tools**: As we will cover in [Chapter 9](8d4d0f34-db9c-4eba-a6ab-9892bc9c2bf8.xhtml),
    *Building a Deep Learning Pipeline*, deployment of a model for training or inference
    is often overlooked in discussions about DL. With neural network architectures
    growing more complex, and with the availability of vast amounts of data, training
    your network on, say, AWS GPUs, or deploying your trained model to other systems
    (for example, a recommendation system integrated into a news website) is a critical
    step. You will improve your training time and extend the amount of computing that
    can be used. This means being able to experiment with more complex models too.
    Ideally, we would want a library that makes it easy to integrate with existing
    tools or has tools of its own.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署工具**：正如我们将在[第9章](8d4d0f34-db9c-4eba-a6ab-9892bc9c2bf8.xhtml)，《构建深度学习流水线》中详细讨论的那样，模型的部署用于训练或推理经常被忽视。随着神经网络架构变得更加复杂，并且可用的数据量变得更大，例如在AWS
    GPU上训练网络，或将训练好的模型部署到其他系统（例如集成到新闻网站的推荐系统）是一个关键步骤。这将改进你的训练时间并扩展可以使用的计算量。这意味着也可以尝试更复杂的模型。理想情况下，我们希望一个库能够轻松地与现有工具集成或具备自己的工具。'
- en: Now that we've got a reasonable set of requirements for our ideal library, let's
    take a look at a number of the popular options out there in the community. The
    following list is by no means exhaustive; however, it covers most of the major
    ML-related Go projects on GitHub, from the most narrow to the most general.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们理想的库设定了一个合理的需求集，让我们来看看社区中的一些流行选项。以下列表并非详尽无遗，但它涵盖了GitHub上大多数主要的与ML相关的Go项目，从最狭窄到最通用。
- en: ML libraries
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML库
- en: We will now consider each of the main ML libraries, assessing their utility
    based on the criteria we defined earlier, including any negative aspects or shortcomings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑每个主要的ML库，根据我们之前定义的标准来评估它们的效用，包括任何负面方面或缺陷。
- en: Word-embeddings in Go
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Go中的词嵌入
- en: '**Word-embeddings in Go** is an example of a task-specific ML library. It implements
    the two-layer neural network necessary to generate word embeddings, using `Word2vec`
    and `GloVe`. It is a great implementation, fast, and clean. It implements a limited
    number of features very well and in ways specific to the task of generating word
    embeddings via `Word2vec` and `GloVe`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Go中的词嵌入** 是一个特定任务的ML库示例。它实现了生成词嵌入所需的两层神经网络，使用`Word2vec`和`GloVe`。它是一个出色的实现，快速而干净。它非常好地实现了有限数量的特性，特定于通过`Word2vec`和`GloVe`生成词嵌入的任务。'
- en: An example of this is a core feature required for training DNNs, an optimization
    method called SGD. This is used in the `GloVe` model, developed by a team at Stanford.
    However, the code is integrated specifically with the `GloVe` model, and the additional
    optimization methods used in `Word2Vec` (negative sampling and skip-gram) are
    not useful with DNNs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于训练DNNs的核心功能示例，称为SGD的优化方法。这在由斯坦福团队开发的`GloVe`模型中使用。然而，代码特定集成于`GloVe`模型中，而在DNNs中使用的其他优化方法（如负采样和skip-gram）对其无用。
- en: This can be useful for DL tasks, say, for generating an embedded layer or dense
    vector representation of a text corpus that can be used in **Long Short-Term Memory**
    (**LSTM**) networks, which we will cover in [Chapter 5](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml), *Next
    Word Prediction with Recurrent Neural Networks*. However, all of the advanced
    functions we would need (for example, gradient descent or backpropagation) and
    model features (LSTM units themselves) are absent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DL任务来说，这可能是有用的，例如，用于生成文本语料库的嵌入式层或密集向量表示，这可以在**长短期记忆**（**LSTM**）网络中使用，我们将在[第5章](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml)中讨论，*使用递归神经网络进行下一个单词预测*。然而，我们需要的所有高级功能（例如梯度下降或反向传播）和模型特性（LSTM单元本身）都不存在。
- en: Naive Bayesian classification and genetic algorithms for Go or Golang
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Go或Golang的朴素贝叶斯分类和遗传算法
- en: These two libraries form another set of task-specific examples of ML libraries
    in Go. Both are well-written and offer primitives specific to their features,
    but these primitives do not generalize. In the Naive Bayes classifier `lib`, matrices
    are built manually before they can be used, while the traditional approach to
    generic algorithms makes no use of matrices at all. There has been some work on
    incorporating them into GA; however, this work is yet to make it into the GA library
    we're referencing here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个库构成Go中ML库的另一组特定任务的示例。两者都写得很好，提供了特定功能的原语，但这些原语并不通用。在朴素贝叶斯分类器`lib`中，需要手动构建矩阵，然后才能使用，而传统的通用算法方法根本不使用矩阵。已经有一些工作正在将它们整合到GA中；然而，这项工作尚未进入我们在此引用的GA库。
- en: ML for Go
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Go中的ML**'
- en: A library that has a more general collection of useful features is GoLearn.
    While DL-specific features are on its wish list, it has the necessary primitives
    to implement simple neural networks, random forests, clustering, and other ML
    approaches. It relies heavily on Gonum, a library that provides implementations
    of `float64` and `complex128` matrix structures and linear algebra operations
    on them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有更通用集合有用功能的库是GoLearn。虽然DL特定的功能在其愿望清单上，但它具有实现简单神经网络、随机森林、聚类和其他ML方法所需的基本原语。它严重依赖于Gonum，这是一个提供`float64`和`complex128`矩阵结构以及对它们进行线性代数操作的库。
- en: 'Let''s look at what this means from a code perspective, as shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从代码的角度来看一下这意味着什么，如下所示：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we have GoLearn's primary definition of what a neural network looks like.
    It contains definitions for weights using Gonum's `mat` library to create the
    weights as dense matrices. It has biases, functions, size, and input, all of the
    essentials of a basic feedforward network. (We will cover feedforward networks
    in [Chapter 3](200c9784-4718-47d4-84ce-95e41854a151.xhtml), *Beyond Basic Neural
    Networks – Autoencoders and RBMs*).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有GoLearn对神经网络外观的主要定义。它使用Gonum的`mat`库来定义权重，以创建密集矩阵的权重。它有偏差、函数、大小和输入，所有这些都是基本前馈网络的基本要素。（我们将在[第3章](200c9784-4718-47d4-84ce-95e41854a151.xhtml)，*超越基本神经网络
    - 自编码器和RBM*中讨论前馈网络）。
- en: What is lacking is the ability to easily define connections within and across
    layers (for advanced network architectures, such as RNNs and their derivatives,
    and functions essential for DL, such as convolution operations and batch normalization).
    Coding these by hand would add a significant amount of development time to your
    project, which is to say nothing of the time needed to optimize their performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 缺少的是轻松定义高级网络架构内部和跨层次连接的能力（例如RNN及其派生物，以及DL中的必要函数，如卷积操作和批量归一化）。手动编码这些将会显著增加项目开发时间，更不用说优化它们的性能所需的时间了。
- en: Another big missing feature, and for training and scaling the network architectures
    used in DL, is CUDA support. We will go through CUDA in [Chapter 4](066629ff-35fe-4fc3-b10a-e110e79631b5.xhtml),
    *CUDA – GPU-Accelerated Training*, but without this support, we will be limited
    to simple models that do not use massive quantities of data, that is, the kind
    we are interested in for the purposes of this book.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的缺失特性是对DL中使用的网络架构进行训练和扩展的CUDA支持。我们将在[第4章](066629ff-35fe-4fc3-b10a-e110e79631b5.xhtml)，*CUDA
    - GPU加速训练*中介绍CUDA，但是如果没有这种支持，我们将局限于不使用大量数据的简单模型，也就是我们在本书的目的中感兴趣的模型类型。
- en: Machine learning libraries for Golang
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于Golang的机器学习库
- en: 'This library differs in that it implements its own matrix operations and does
    not rely on Gonum. It is really a collection of implementations that include the
    following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库的不同之处在于它实现了自己的矩阵操作，而不依赖于Gonum。实际上，它是一系列实现的集合，包括以下内容：
- en: Linear regression
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Neural networks
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Collaborative filtering
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协作过滤
- en: Gaussian multivariate distribution for anomaly detection systems
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于异常检测系统的高斯多变量分布
- en: Individually, these are powerful tools; indeed, linear regression is often described
    as one of the most important tools in the data scientist's toolkit, but, for our
    purposes, we really only care about the neural networks portion of the library.
    And here, we see limitations similar to those of GoLearn, such as limited activation
    functions and a lack of tools for intra- and interlayer connections (for example,
    LSTM units).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就个别而言，这些都是强大的工具；事实上，线性回归通常被描述为数据科学家工具箱中最重要的工具之一，但出于我们的目的，我们真正关心的只是该库的神经网络部分。在这里，我们看到类似于GoLearn的限制，例如有限的激活函数以及用于层内和层间连接的工具的缺乏（例如，LSTM单元）。
- en: The author has an additional library that implements CUDA matrix operations;
    however, both this and the `go_ml` library itself have not been updated in four
    years (at the time of writing), so this is not a project you could simply import
    and start building neural networks straightaway.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还有一个实现CUDA矩阵操作的额外库；然而，无论是这个库还是`go_ml`库本身，在撰写本文时已经有四年没有更新了，因此这不是一个你可以简单导入并立即开始构建神经网络的项目。
- en: GoBrain
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoBrain
- en: 'Another library that is not under active development is GoBrain. You might
    then ask: why bother reviewing it? Briefly, it is of interest because it is the
    only other library apart from Gorgonia that attempts to implement primitives from
    more advanced network architectures. Specifically, it extends its primary network,
    which is a basic feedforward neural network, to become something new, an **Elman
    recurrent neural network**, or **SRN**.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个目前未处于积极开发状态的库是GoBrain。你可能会问：为什么要去审查它？简而言之，它之所以引人关注，是因为除了Gorgonia之外，它是唯一尝试实现更高级网络架构原语的库。具体而言，它将其主要网络（一个基本的前馈神经网络）扩展为新的东西，即**Elman递归神经网络**或**SRN**。
- en: Introduced in 1990, this was the first network architecture to include recurrence,
    or loops, connecting hidden layers of a network and adjacent *context* units.
    This had the effect of allowing networks to learn sequence dependencies, such
    as the *context* of a word, or potentially the grammar and syntax of human language.
    Groundbreaking for its time, the SRN offered *the vision that these units might
    be emergent consequences of a learning process operating over the latent structure
    in the speech stream*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年引入以来，这是第一个包括循环或连接网络隐藏层和相邻*上下文*单元的网络架构。这使得网络能够学习序列依赖性，例如单词的*上下文*或潜在的语法和人类语言的句法。对当时来说具有开创性的是，SRN提供了*这些单元可能是通过作用于语音流中的潜在结构的学习过程而产生的*的愿景。
- en: SRNs have given way to more modern recurrent neural networks, which we will
    cover in detail in [Chapter 5](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml), *Next
    Word Prediction with Recurrent Neural Networks*. However, in GoBrain, we have
    an interesting example of a library that contains the beginnings of what we need
    for our work.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: SRN已经被更现代的递归神经网络取代，我们将在[第5章](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml)详细介绍*递归神经网络进行下一个单词预测*。然而，在GoBrain中，我们有一个有趣的例子，这是一个包含了我们工作所需内容开端的库。
- en: A set of numeric libraries for the Go programming language
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一组针对Go编程语言的数值库
- en: The most feature-complete library that could potentially be useful for DL (aside
    from Gorgonia, which we will cover in later sections) is Gonum. The simplest description
    would be that Gonum attempts to emulate much of the functionality of the well-known
    scientific computing libraries in Python, namely, NumPy and SciPy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 除了稍后将介绍的Gorgonia之外，可能对DL有用的最全面的库是Gonum。最简单的描述是，Gonum试图模拟Python中广为人知的科学计算库，即NumPy和SciPy的许多功能。
- en: Let's take a look at a code example for constructing a matrix we might use to
    represent inputs to a DNN.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个构建矩阵的代码示例，我们可以用它来表示DNN的输入。
- en: 'Initialize a matrix and back it with some numbers, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个矩阵，并用一些数字支持它，如下所示：
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Perform operations on the matrix, as shown in the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对矩阵执行操作，如下所示的代码：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we can see that Gonum offers us the primitives we need to manipulate the
    matrices exchanged between layers in DNNs, namely, `c.Mul` and `c.Add`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到Gonum为我们提供了我们在DNN中层间交换的矩阵操作所需的基本功能，即`c.Mul`和`c.Add`。
- en: When we decide to scale up our design ambitions, this is when we run into the
    limitations of Gonum. There are no GRU/LSTM cells and there is no SGD with backpropagation.
    If we are to reliably and efficiently construct DNNs that we want to carry all
    of the way through to production, we need to look elsewhere for a more complete
    library.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们决定扩展我们的设计野心时，这时我们遇到了Gonum的限制。它没有GRU/LSTM单元，也没有带有反向传播的SGD。如果我们要可靠且高效地构建我们希望完全推广的DNN，我们需要在其他地方寻找更完整的库。
- en: Using Gorgonia
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Gorgonia
- en: At the time of writing this book, there are two libraries that would typically
    be considered for DL in Go, TensorFlow and Gorgonia. However, while TensorFlow
    is definitely well regarded and has a full-featured API in Python, this is not
    the case in Go. As discussed previously, the Go bindings for TensorFlow are only
    suited to loading models that have already been created in Python, but not for
    creating models from scratch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，有两个通常被认为适用于Go中深度学习的库，分别是TensorFlow和Gorgonia。然而，虽然TensorFlow在Python中拥有全面的API并且广受好评，但在Go语言中并非如此。正如前面讨论的那样，TensorFlow的Go绑定仅适用于加载已在Python中创建的模型，而不能从头开始创建模型。
- en: Gorgonia has been built from the ground up to be a Go library that is able to
    both train ML models and perform inference. This is a particularly valuable property,
    especially if you have an existing Go application or you are looking to build
    a Go application. Gorgonia allows you to develop, train, and maintain your DL
    model in your existing Go environment. For this book, we will be using Gorgonia
    exclusively to build models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia是从头开始构建的Go库，能够训练ML模型并进行推理。这是一种特别有价值的特性，特别是如果你已经有现有的Go应用程序或者想要构建一个Go应用程序。Gorgonia允许你在现有的Go环境中开发、训练和维护你的DL模型。在本书中，我们将专门使用Gorgonia来构建模型。
- en: Before we go on to build models, let's go through some basics of Gorgonia and
    learn how to build simple equations in it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续构建模型之前，让我们先了解一些Gorgonia的基础知识，并学习如何在其中构建简单的方程。
- en: The basics of Gorgonia
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gorgonia的基础知识
- en: '**Gorgonia** is a lower-level library, which means that we need to build the
    equations and the architecture for models ourselves. This means that there isn''t
    a built-in DNN classifier function that will magically create an entire model
    with several hidden layers and immediately be ready to apply to your dataset.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gorgonia** 是一个较低级别的库，这意味着我们需要自己构建模型的方程和架构。这意味着没有一个内置的 DNN 分类器函数会像魔法一样创建一个具有多个隐藏层的完整模型，并立即准备好应用于您的数据集。'
- en: Gorgonia facilitates DL by being a library that makes working with multidimensional
    arrays easy. It does this by providing loads of operators to work with so you
    can build the underlying mathematical equations that make up layers in a DL model.
    We can then proceed to use these layers in our model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 通过提供大量运算符来简化多维数组的操作，从而促进 DL。我们可以利用这些层来构建模型。
- en: Another important feature of Gorgonia is performance. By removing the need to
    think about how to optimize tensor operations, we can focus on building the model
    and ensuring the architecture is correct, rather than worrying about whether or
    not our model will be performant.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 的另一个重要特性是性能。通过消除优化张量操作的需求，我们可以专注于构建模型和确保架构正确，而不是担心我们的模型是否高效。
- en: 'As Gorgonia is a little lower-level than a typical ML library, building a model
    takes a few more steps. However, this does not mean that building a model in Gorgonia
    is difficult. It requires the following three basic steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Gorgonia 比典型的 ML 库略低级，构建模型需要更多步骤。但这并不意味着在 Gorgonia 中构建模型是困难的。它需要以下三个基本步骤：
- en: Create a computation graph
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建计算图。
- en: Input the data
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据。
- en: Execute the graph
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行图。
- en: Wait, what's a computation graph? A **computation graph** is a directed graph
    where each of the nodes is either an operation or a variable. Variables can be
    fed into operations, which will then produce a value. This value can then be fed
    into another operation. In more familiar terms, a graph is like a function that
    takes all of the variables and then produces a result.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么是计算图？**计算图**是一个有向图，其中每个节点都是一个操作或一个变量。变量可以输入到操作中，操作将产生一个值。然后这个值可以输入到另一个操作中。更熟悉的术语来说，图像一个接收所有变量并产生结果的函数。
- en: A variable can be anything; we can make it a single scalar value, a vector (that
    is, an array), or a matrix. In DL, we typically work with a more generalized structure
    called a tensor; a tensor can be thought of as something similar to an *n*-dimensional
    matrix.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 变量可以是任何东西；我们可以将其设定为单个标量值、一个向量（即数组）或一个矩阵。在 DL 中，我们通常使用一个更一般化的结构称为张量；张量可以被视为类似于*n*维矩阵。
- en: 'The following screenshot shows a visual representation of *n*-dimensional tensors:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了*n*维张量的可视化表示：
- en: '![](img/906774f8-7b4b-4e6f-aabd-7b0e57cf7d16.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/906774f8-7b4b-4e6f-aabd-7b0e57cf7d16.png)'
- en: We represent equations as graphs because this makes it easier for us to optimize
    the performance of our model. This is enabled by the fact that, by putting each
    node in a directed graph, we have a good idea of what its dependencies are. Since
    we model each node as an independent piece of code, we know that all it needs
    to execute are its dependencies (which can be other nodes or other variables).
    Also, as we traverse the graph, we can know which nodes are independent of each
    other and run those concurrently.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将方程表示为图形，因为这样更容易优化我们模型的性能。这是通过将每个节点放入有向图中来实现的，这样我们就知道它的依赖关系。由于我们将每个节点建模为独立的代码片段，我们知道它执行所需的只是它的依赖关系（可以是其他节点或其他变量）。此外，当我们遍历图形时，我们可以知道哪些节点彼此独立，并可以并行运行它们。
- en: 'For example, take the following diagram:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下图示：
- en: '![](img/f71250c6-0790-48c1-8d43-48ace79d54e3.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f71250c6-0790-48c1-8d43-48ace79d54e3.png)'
- en: As **A**, **B**, and **C** are independent, we can easily compute these concurrently.
    The computation of **V** requires both **A** and **B** to be ready. However, **W**
    only requires **B** to be ready. This follows into the next level, and so on,
    up until we are ready to compute the final output in **Z**.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 **A**，**B** 和 **C** 是独立的，我们可以轻松并行计算它们。计算 **V** 需要 **A** 和 **B** 准备好。然而，**W**
    只需要 **B** 准备好。这一过程一直延续到下一个级别，直到我们准备计算最终输出 **Z**。
- en: Simple example – addition
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单示例 - 加法。
- en: The easiest way to understand how this all fits together is by building a simple
    example.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一切如何组合在一起的最简单方法是通过构建一个简单的示例。
- en: 'To start, let''s implement a simple graph to add two numbers together—basically,
    this would be: *c = a + b*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实现一个简单的图形来将两个数字加在一起 - 基本上是这样的：*c = a + b*：
- en: 'First, let''s import some libraries—most importantly, Gorgonia, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入一些库，最重要的是Gorgonia，如下所示：
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, let''s start our main function, like so:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们开始我们的主函数，如下所示：
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To that, let''s add our scalars, as shown here:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们添加我们的标量，如下所示：
- en: '[PRE5]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, very importantly, let''s define our operation node, as follows:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，非常重要的是，让我们定义我们的操作节点，如下所示：
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that `c` will not actually have a value now; we've just defined a new node
    of our computation graph, so we need to execute it before it will have a value.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在*c*实际上没有值；我们只是定义了我们计算图的一个新节点，因此在它有值之前我们需要执行它。
- en: 'To execute it, we need to create a virtual machine object for it to run in,
    as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行它，我们需要为其创建一个虚拟机对象，如下所示：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, set the initial values of `a` and `b`, and proceed to get the machine
    to execute our graph, as shown here:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，设置`a`和`b`的初始值，并继续让机器执行我们的图，如下所示：
- en: '[PRE8]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The complete code is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码如下：
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we have built our first computation graph in Gorgonia and executed it!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经在Gorgonia中构建并执行了我们的第一个计算图！
- en: Vectors and matrices
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量和矩阵
- en: Of course, being able to add to numbers isn't why we're here; we're here to
    work with tensors, and eventually, DL equations, so let's take the first step
    toward something a little more complicated.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不是为了能够加两个数字才来这里的；我们是为了处理张量，最终是深度学习方程，所以让我们迈出第一步，朝着更复杂的东西迈进。
- en: 'The goal here is to now create a graph that will compute the following simple
    equation:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是现在创建一个计算以下简单方程的图形：
- en: '*z = Wx*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = Wx*'
- en: Note that *W* is an *n* x *n* matrix, and *x* is a vector of size *n*. For the
    purposes of this example, we will use *n = 2*.1957.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*W*是一个*n* x *n*矩阵，而*x*是大小为*n*的向量。在本示例中，我们将使用*n = 2*。
- en: 'Again, we start with the same basic main function, as shown here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们从这里开始相同的基本主函数：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You'll notice that we've chosen to alias the Gorgonia package as `G`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，我们选择将Gorgonia包别名为`G`。
- en: 'We then create our first tensor, the matrix, `W`, like so:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像这样创建我们的第一个张量，矩阵`W`：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You''ll notice that we''ve done things a bit differently this time around,
    as listed here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，这一次我们做了一些不同的事情，如下所列：
- en: We've started by declaring an array with the values that we want in our matrix
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先声明一个带有我们想要的矩阵值的数组
- en: We've then created a tensor from that matrix with a shape of 2 x 2, as we want
    a 2 x 2 matrix
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从该矩阵创建一个张量，形状为2 x 2，因为我们希望是一个2 x 2的矩阵
- en: After all of that, we've then created a new node in our graph for the matrix,
    given it the name `W`, and initialized it with the value of the tensor
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一切之后，我们在图中创建了一个新的节点用于矩阵，并将其命名为`W`，并用张量的值初始化了它
- en: 'We then create our second tensor and input node the same way, the vector, `x`,
    as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们以相同的方式创建我们的第二个张量和输入节点，向量`x`，如下所示：
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Just like last time, we then add an operator node, `z`, that will multiply
    the two (instead of an addition operation):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 就像上次一样，我们接着添加一个操作节点`z`，它将两者相乘（而不是加法操作）：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, as last time, create a new tape machine and run it, as shown here, and
    then print the result:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，和上次一样，创建一个新的计算机并运行它，如下所示，然后打印结果：
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Visualizing the graph
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化图形
- en: 'In many cases, it is also very useful to visualize the graph; you can easily
    do this by adding `io` or `ioutil` to your imports and the following line to your
    code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，通过将`io`或`ioutil`添加到您的导入中，并将以下行添加到您的代码中，可以非常方便地可视化图形：
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will produce a DOT file; you can open this in GraphViz, or, more conveniently,
    convert it to an SVG. You can view it in most modern browsers by installing GraphViz
    and entering the following in the command line:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个DOT文件；您可以在GraphViz中打开它，或者更方便地将其转换为SVG。您可以通过安装GraphViz并在命令行中输入以下内容，在大多数现代浏览器中查看它：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will produce `simple_graph.dot.svg`; you can open this in your browser
    to see a rendering of the graph, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成`simple_graph.dot.svg`；您可以在浏览器中打开它以查看图的渲染，如下所示：
- en: '![](img/0514dde0-b38c-440b-84a4-4b1df0ffcacc.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0514dde0-b38c-440b-84a4-4b1df0ffcacc.png)'
- en: You can see, in our graph, that we have two inputs, `W` and `x`, and this then
    gets fed into our operator, being a matrix multiplication with a vector giving
    us the result as well—another vector.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在我们的图中，我们有两个输入，`W`和`x`，然后将其馈送到我们的运算符中，这是一个矩阵乘法和一个向量，给我们的结果同样是另一个向量。
- en: Building more complex expressions
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建更复杂的表达式
- en: 'Of course, we''ve mostly covered how to build simple equations; however, what
    happens if your equation is a little bit more complicated, for example, like the
    following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们已经大部分讲解了如何构建简单的方程；但是，如果你的方程稍微复杂一些，例如以下情况会发生：
- en: '*z = Wx + b*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = Wx + b*'
- en: 'We can also very easily do this by changing our code a bit to add the following
    line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过稍微改变我们的代码来轻松完成这一点，添加以下行：
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we can change our definition for `z` slightly, as shown here:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以稍微更改`z`的定义，如下所示：
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, we've created a multiplication operator node, and then created
    an addition operator node on top of that.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们创建了一个乘法运算符节点，然后在此基础上创建了一个加法运算符节点。
- en: 'Alternatively, you can also just do it in a single line, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以只在一行中完成，如下所示：
- en: '[PRE19]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that we use `Must` here to suppress the error object; we are merely doing
    it here for convenience, as we know that the operation to add this node to the
    graph will work. It is important to note that you may want to restructure this
    code to create the node for addition separately so that you can have error handling
    for each step.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里使用`Must`来抑制错误对象；我们这样做仅仅是为了方便，因为我们知道将此节点添加到图中的操作会成功。重要的是要注意，你可能希望重新构造此代码，以便单独创建用于添加节点的代码，以便每一步都能进行错误处理。
- en: 'If you now proceed to build and execute the code, you will find that it will
    produce the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在继续构建和执行代码，你会发现它将产生以下结果：
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The computation graph now looks like the following screenshot:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图现在看起来像以下的屏幕截图：
- en: '![](img/bac6b03b-238f-406a-9adf-57a510a7b559.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bac6b03b-238f-406a-9adf-57a510a7b559.png)'
- en: You can see that `W` and `x` both feed into the first operation (our multiplication
    operation) and then, later, it feeds into our addition operation to produce our
    results.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`W`和`x`都输入到第一个操作中（我们的乘法操作），稍后又输入到我们的加法操作中以生成我们的结果。
- en: That's an introduction to using Gorgonia! As you can now hopefully see, it is
    a library that contains the necessary primitives that will allow us to build the
    first simple, and then more complicated, neural networks in the following chapters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Gorgonia的简介！正如你现在希望看到的那样，它是一个包含了必要原语的库，它将允许我们在接下来的章节中构建第一个简单的，然后是更复杂的神经网络。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter included a brief introduction to DL, both its history and applications.
    It was followed by a discussion of why Go is a great language for DL and demonstrated
    how the library we use in Gorgonia compares to other libraries in Go.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了DL，包括其历史和应用。随后讨论了为什么Go语言非常适合DL，并演示了我们在Gorgonia中使用的库与Go中其他库的比较。
- en: The next chapter will cover the magic that makes neural networks and DL work,
    which includes activation functions, network structure, and training algorithms.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将涵盖使神经网络和DL工作的魔力，其中包括激活函数、网络结构和训练算法。
