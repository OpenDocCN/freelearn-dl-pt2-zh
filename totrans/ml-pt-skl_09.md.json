["```py\nimport pandas as pd\ncolumns = ['Overall Qual', 'Overall Cond', 'Gr Liv Area',\n           'Central Air', 'Total Bsmt SF', 'SalePrice']\ndf = pd.read_csv('http://jse.amstat.org/v19n3/decock/AmesHousing.txt', \n                 sep='\\t',\n                 usecols=columns)\ndf.head() \n```", "```py\n>>> df.shape\n(2930, 6) \n```", "```py\n>>> df['Central Air'] = df['Central Air'].map({'N': 0, 'Y': 1}) \n```", "```py\n>>> df.isnull().sum()\nOverall Qual     0\nOverall Cond     0\nTotal Bsmt SF    1\nCentral Air      0\nGr Liv Area      0\nSalePrice        0\ndtype: int64 \n```", "```py\n>>> df = df.dropna(axis=0)\n>>> df.isnull().sum()\nOverall Qual     0\nOverall Cond     0\nTotal Bsmt SF    0\nCentral Air      0\nGr Liv Area      0\nSalePrice        0\ndtype: int64 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> from mlxtend.plotting import scatterplotmatrix\n>>> scatterplotmatrix(df.values, figsize=(12, 10), \n...                   names=df.columns, alpha=0.5)\n>>> plt.tight_layout()\nplt.show() \n```", "```py\n>>> import numpy as np\n>>> from mlxtend.plotting import heatmap\n>>> cm = np.corrcoef(df.values.T)\n>>> hm = heatmap(cm, row_names=df.columns, column_names=df.columns)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nclass LinearRegressionGD:\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n    def fit(self, X, y):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = np.array([0.])\n        self.losses_ = []\n        for i in range(self.n_iter):\n            output = self.net_input(X)\n            errors = (y - output)\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean()\n            loss = (errors**2).mean()\n            self.losses_.append(loss)\n        return self\n    def net_input(self, X):\n        return np.dot(X, self.w_) + self.b_\n    def predict(self, X):\n        return self.net_input(X) \n```", "```py\n>>> X = df[['Gr Liv Area']].values\n>>> y = df['SalePrice'].values\n>>> from sklearn.preprocessing import StandardScaler\n>>> sc_x = StandardScaler()\n>>> sc_y = StandardScaler()\n>>> X_std = sc_x.fit_transform(X)\n>>> y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()\n>>> lr = LinearRegressionGD(eta=0.1)\n>>> lr.fit(X_std, y_std) \n```", "```py\n>>> plt.plot(range(1, lr.n_iter+1), lr.losses_)\n>>> plt.ylabel('MSE')\n>>> plt.xlabel('Epoch')\n>>> plt.show() \n```", "```py\n>>> def lin_regplot(X, y, model):\n...     plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n...     plt.plot(X, model.predict(X), color='black', lw=2) \n```", "```py\n>>> lin_regplot(X_std, y_std, lr)\n>>> plt.xlabel(' Living area above ground (standardized)')\n>>> plt.ylabel('Sale price (standardized)')\n>>> plt.show() \n```", "```py\n>>> feature_std = sc_x.transform(np.array([[2500]]))\n>>> target_std = lr.predict(feature_std)\n>>> target_reverted = sc_y.inverse_transform(target_std.reshape(-1, 1))\n>>> print(f'Sales price: ${target_reverted.flatten()[0]:.2f}')\nSales price: $292507.07 \n```", "```py\n>>> print(f'Slope: {lr.w_[0]:.3f}')\nSlope: 0.707\n>>> print(f'Intercept: {lr.b_[0]:.3f}')\nIntercept: -0.000 \n```", "```py\n>>> from sklearn.linear_model import LinearRegression\n>>> slr = LinearRegression()\n>>> slr.fit(X, y)\n>>> y_pred = slr.predict(X)\n>>> print(f'Slope: {slr.coef_[0]:.3f}')\nSlope: 111.666\n>>> print(f'Intercept: {slr.intercept_:.3f}')\nIntercept: 13342.979 \n```", "```py\n>>> lin_regplot(X, y, slr)\n>>> plt.xlabel('Living area above ground in square feet')\n>>> plt.ylabel('Sale price in U.S. dollars')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n# adding a column vector of \"ones\"\n>>> Xb = np.hstack((np.ones((X.shape[0], 1)), X))\n>>> w = np.zeros(X.shape[1])\n>>> z = np.linalg.inv(np.dot(Xb.T, Xb))\n>>> w = np.dot(z, np.dot(Xb.T, y))\n>>> print(f'Slope: {w[1]:.3f}')\nSlope: 111.666\n>>> print(f'Intercept: {w[0]:.3f}')\nIntercept: 13342.979 \n```", "```py\n>>> from sklearn.linear_model import RANSACRegressor\n>>> ransac = RANSACRegressor(\n...     LinearRegression(), \n...     max_trials=100, # default value\n...     min_samples=0.95, \n...     residual_threshold=None, # default value \n...     random_state=123)\n>>> ransac.fit(X, y) \n```", "```py\n>>> inlier_mask = ransac.inlier_mask_\n>>> outlier_mask = np.logical_not(inlier_mask)\n>>> line_X = np.arange(3, 10, 1)\n>>> line_y_ransac = ransac.predict(line_X[:, np.newaxis])\n>>> plt.scatter(X[inlier_mask], y[inlier_mask],\n...             c='steelblue', edgecolor='white',\n...             marker='o', label='Inliers')\n>>> plt.scatter(X[outlier_mask], y[outlier_mask],\n...             c='limegreen', edgecolor='white',\n...             marker='s', label='Outliers')\n>>> plt.plot(line_X, line_y_ransac, color='black', lw=2)\n>>> plt.xlabel('Living area above ground in square feet')\n>>> plt.ylabel('Sale price in U.S. dollars')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> print(f'Slope: {ransac.estimator_.coef_[0]:.3f}')\nSlope: 106.348\n>>> print(f'Intercept: {ransac.estimator_.intercept_:.3f}')\nIntercept: 20190.093 \n```", "```py\n>>> def mean_absolute_deviation(data):\n...     return np.mean(np.abs(data - np.mean(data)))\n>>> mean_absolute_deviation(y)\n58269.561754979375 \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> target = 'SalePrice'\n>>> features = df.columns[df.columns != target]\n>>> X = df[features].values\n>>> y = df[target].values\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=123) \n>>> slr = LinearRegression()\n>>> slr.fit(X_train, y_train)\n>>> y_train_pred = slr.predict(X_train)\n>>> y_test_pred = slr.predict(X_test) \n```", "```py\n>>> x_max = np.max(\n...     [np.max(y_train_pred), np.max(y_test_pred)])\n>>> x_min = np.min(\n...     [np.min(y_train_pred), np.min(y_test_pred)])\n>>> fig, (ax1, ax2) = plt.subplots(\n...     1, 2, figsize=(7, 3), sharey=True)\n>>> ax1.scatter(\n...     y_test_pred, y_test_pred - y_test,\n...     c='limegreen', marker='s',\n...     edgecolor='white',\n...     label='Test data')\n>>> ax2.scatter(\n...     y_train_pred, y_train_pred - y_train,\n...     c='steelblue', marker='o', edgecolor='white',\n...     label='Training data')\n>>> ax1.set_ylabel('Residuals')\n>>> for ax in (ax1, ax2):\n...     ax.set_xlabel('Predicted values')\n...     ax.legend(loc='upper left')\n...     ax.hlines(y=0, xmin=x_min-100, xmax=x_max+100,\\\n...         color='black', lw=2)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.metrics import mean_squared_error\n>>> mse_train = mean_squared_error(y_train, y_train_pred)\n>>> mse_test = mean_squared_error(y_test, y_test_pred)\n>>> print(f'MSE train: {mse_train:.2f}')\nMSE train: 1497216245.85\n>>> print(f'MSE test: {mse_test:.2f}')\nMSE test: 1516565821.00 \n```", "```py\n>>> from sklearn.metrics import mean_absolute_error\n>>> mae_train = mean_absolute_error(y_train, y_train_pred)\n>>> mae_test = mean_absolute_error(y_test, y_test_pred)\n>>> print(f'MAE train: {mae_train:.2f}')\nMAE train: 25983.03\n>>> print(f'MAE test: {mae_test:.2f}')\nMAE test: 24921.29 \n```", "```py\n>>> from sklearn.metrics import r2_score\n>>> train_r2 = r2_score(y_train, y_train_pred)>>> test_r2 = r2_score(y_test, y_test_pred)\n>>> print(f'R^2 train: {train_r2:.3f}, {test_r2:.3f}')\nR^2 train: 0.77, test: 0.75 \n```", "```py\n>>> from sklearn.linear_model import Ridge\n>>> ridge = Ridge(alpha=1.0) \n```", "```py\n>>> from sklearn.linear_model import Lasso\n>>> lasso = Lasso(alpha=1.0) \n```", "```py\n>>> from sklearn.linear_model import ElasticNet\n>>> elanet = ElasticNet(alpha=1.0, l1_ratio=0.5) \n```", "```py\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> X = np.array([ 258.0, 270.0, 294.0, 320.0, 342.0,\n    ...                368.0, 396.0, 446.0, 480.0, 586.0])\\\n    ...              [:, np.newaxis]\n    >>> y = np.array([ 236.4, 234.4, 252.8, 298.6, 314.2,\n    ...                342.2, 360.8, 368.0, 391.2, 390.8])\n    >>> lr = LinearRegression()\n    >>> pr = LinearRegression()\n    >>> quadratic = PolynomialFeatures(degree=2)\n    >>> X_quad = quadratic.fit_transform(X) \n    ```", "```py\n    >>> lr.fit(X, y)\n    >>> X_fit = np.arange(250, 600, 10)[:, np.newaxis]\n    >>> y_lin_fit = lr.predict(X_fit) \n    ```", "```py\n    >>> pr.fit(X_quad, y)\n    >>> y_quad_fit = pr.predict(quadratic.fit_transform(X_fit)) \n    ```", "```py\n    >>> plt.scatter(X, y, label='Training points')\n    >>> plt.plot(X_fit, y_lin_fit,\n    ...          label='Linear fit', linestyle='--')\n    >>> plt.plot(X_fit, y_quad_fit,\n    ...          label='Quadratic fit')\n    >>> plt.xlabel('Explanatory variable')\n    >>> plt.ylabel('Predicted or known target values')\n    >>> plt.legend(loc='upper left')\n    >>> plt.tight_layout()\n    >>> plt.show() \n    ```", "```py\n>>> y_lin_pred = lr.predict(X)\n>>> y_quad_pred = pr.predict(X_quad)\n>>> mse_lin = mean_squared_error(y, y_lin_pred)\n>>> mse_quad = mean_squared_error(y, y_quad_pred)\n>>> print(f'Training MSE linear: {mse_lin:.3f}'\n          f', quadratic: {mse_quad:.3f}')\nTraining MSE linear: 569.780, quadratic: 61.330\n>>> r2_lin = r2_score(y, y_lin_pred)\n>>> r2_quad = r2_score(y, y_quad_pred)\n>>> print(f'Training R^2 linear: {r2_lin:.3f}'\n          f', quadratic: {r2_quad:.3f}')\nTraining R^2 linear: 0.832, quadratic: 0.982 \n```", "```py\n>>> X = df[['Gr Liv Area']].values\n>>> y = df['SalePrice'].values\n>>> X = X[(df['Gr Liv Area'] < 4000)]\n>>> y = y[(df['Gr Liv Area'] < 4000)] \n```", "```py\n>>> regr = LinearRegression()\n>>> # create quadratic and cubic features\n>>> quadratic = PolynomialFeatures(degree=2)\n>>> cubic = PolynomialFeatures(degree=3)\n>>> X_quad = quadratic.fit_transform(X)\n>>> X_cubic = cubic.fit_transform(X)\n>>> # fit to features\n>>> X_fit = np.arange(X.min()-1, X.max()+2, 1)[:, np.newaxis]\n>>> regr = regr.fit(X, y)\n>>> y_lin_fit = regr.predict(X_fit)\n>>> linear_r2 = r2_score(y, regr.predict(X))\n>>> regr = regr.fit(X_quad, y)\n>>> y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))\n>>> quadratic_r2 = r2_score(y, regr.predict(X_quad))\n>>> regr = regr.fit(X_cubic, y)\n>>> y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))\n>>> cubic_r2 = r2_score(y, regr.predict(X_cubic))\n>>> # plot results\n>>> plt.scatter(X, y, label='Training points', color='lightgray')\n>>> plt.plot(X_fit, y_lin_fit, \n...          label=f'Linear (d=1), $R^2$={linear_r2:.2f}',\n...          color='blue', \n...          lw=2, \n...          linestyle=':')\n>>> plt.plot(X_fit, y_quad_fit, \n...          label=f'Quadratic (d=2), $R^2$={quadratic_r2:.2f}',\n...          color='red', \n...          lw=2,\n...          linestyle='-')\n>>> plt.plot(X_fit, y_cubic_fit, \n...          label=f'Cubic (d=3), $R^2$={cubic_r2:.2f}',\n...          color='green', \n...          lw=2,\n...          linestyle='--')\n>>> plt.xlabel('Living area above ground in square feet')\n>>> plt.ylabel('Sale price in U.S. dollars')\n>>> plt.legend(loc='upper left')\n>>> plt.show() \n```", "```py\n>>> X = df[['Overall Qual']].values\n>>> y = df['SalePrice'].values \n```", "```py\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> X = df[['Gr Liv Area']].values\n>>> y = df['SalePrice'].values\n>>> tree = DecisionTreeRegressor(max_depth=3)\n>>> tree.fit(X, y)\n>>> sort_idx = X.flatten().argsort()\n>>> lin_regplot(X[sort_idx], y[sort_idx], tree)\n>>> plt.xlabel('Living area above ground in square feet')\n>>> plt.ylabel('Sale price in U.S. dollars')>>> plt.show() \n```", "```py\n>>> target = 'SalePrice'\n>>> features = df.columns[df.columns != target]\n>>> X = df[features].values\n>>> y = df[target].values\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=123)\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> forest = RandomForestRegressor(\n...     n_estimators=1000, \n...     criterion='squared_error', \n...     random_state=1, \n...     n_jobs=-1)\n>>> forest.fit(X_train, y_train)\n>>> y_train_pred = forest.predict(X_train)\n>>> y_test_pred = forest.predict(X_test)\n>>> mae_train = mean_absolute_error(y_train, y_train_pred)\n>>> mae_test = mean_absolute_error(y_test, y_test_pred)\n>>> print(f'MAE train: {mae_train:.2f}')\nMAE train: 8305.18\n>>> print(f'MAE test: {mae_test:.2f}')\nMAE test: 20821.77\n>>> r2_train = r2_score(y_train, y_train_pred)\n>>> r2_test =r2_score(y_test, y_test_pred)\n>>> print(f'R^2 train: {r2_train:.2f}')\nR^2 train: 0.98\n>>> print(f'R^2 test: {r2_test:.2f}')\nR^2 test: 0.85 \n```", "```py\n>>> x_max = np.max([np.max(y_train_pred), np.max(y_test_pred)])\n>>> x_min = np.min([np.min(y_train_pred), np.min(y_test_pred)])\n>>> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3), sharey=True)\n>>> ax1.scatter(y_test_pred, y_test_pred - y_test,\n...             c='limegreen', marker='s', edgecolor='white',\n...             label='Test data')\n>>> ax2.scatter(y_train_pred, y_train_pred - y_train,\n...             c='steelblue', marker='o', edgecolor='white',\n...             label='Training data')\n>>> ax1.set_ylabel('Residuals')\n>>> for ax in (ax1, ax2):\n...     ax.set_xlabel('Predicted values')\n...     ax.legend(loc='upper left')\n...     ax.hlines(y=0, xmin=x_min-100, xmax=x_max+100,\n...               color='black', lw=2)\n>>> plt.tight_layout()\n>>> plt.show() \n```"]