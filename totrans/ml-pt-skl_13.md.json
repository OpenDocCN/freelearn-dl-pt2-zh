["```py\n>>> import torch\n>>> def compute_z(a, b, c):\n...     r1 = torch.sub(a, b)\n...     r2 = torch.mul(r1, 2)\n...     z = torch.add(r2, c)\n...     return z \n```", "```py\n>>> print('Scalar Inputs:', compute_z(torch.tensor(1),\n...       torch.tensor(2), torch.tensor(3)))\nScalar Inputs: tensor(1)\n>>> print('Rank 1 Inputs:', compute_z(torch.tensor([1]),\n...       torch.tensor([2]), torch.tensor([3])))\nRank 1 Inputs: tensor([1])\n>>> print('Rank 2 Inputs:', compute_z(torch.tensor([[1]]),\n...       torch.tensor([[2]]), torch.tensor([[3]])))\nRank 2 Inputs: tensor([[1]]) \n```", "```py\n>>> a = torch.tensor(3.14, requires_grad=True)\n>>> print(a)\ntensor(3.1400, requires_grad=True)\n>>> b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n>>> print(b)\ntensor([1., 2., 3.], requires_grad=True) \n```", "```py\n>>> w = torch.tensor([1.0, 2.0, 3.0])\n>>> print(w.requires_grad)\nFalse\n>>> w.requires_grad_()\n>>> print(w.requires_grad)\nTrue \n```", "```py\n>>> import torch.nn as nn\n>>> torch.manual_seed(1)\n>>> w = torch.empty(2, 3)\n>>> nn.init.xavier_normal_(w)\n>>> print(w)\ntensor([[ 0.4183,  0.1688,  0.0390],\n        [ 0.3930, -0.2858, -0.1051]]) \n```", "```py\n>>> class MyModule(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.w1 = torch.empty(2, 3, requires_grad=True)\n...         nn.init.xavier_normal_(self.w1)\n...         self.w2 = torch.empty(1, 2, requires_grad=True)\n...         nn.init.xavier_normal_(self.w2) \n```", "```py\n>>> w = torch.tensor(1.0, requires_grad=True)\n>>> b = torch.tensor(0.5, requires_grad=True)\n>>> x = torch.tensor([1.4])\n>>> y = torch.tensor([2.1])\n>>> z = torch.add(torch.mul(w, x), b)\n>>> loss = (y-z).pow(2).sum()\n>>> loss.backward()\n>>> print('dL/dw : ', w.grad)\n>>> print('dL/db : ', b.grad)\ndL/dw :  tensor(-0.5600)\ndL/db :  tensor(-0.4000) \n```", "```py\n>>> # verifying the computed gradient\n>>> print(2 * x * ((w * x + b) - y))\ntensor([-0.5600], grad_fn=<MulBackward0>) \n```", "```py\n>>> model = nn.Sequential(\n...     nn.Linear(4, 16),\n...     nn.ReLU(),\n...     nn.Linear(16, 32),\n...     nn.ReLU()\n... )\n>>> model\nSequential(\n  (0): Linear(in_features=4, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=16, out_features=32, bias=True)\n  (3): ReLU()\n) \n```", "```py\n>>> nn.init.xavier_uniform_(model[0].weight)\n>>> l1_weight = 0.01\n>>> l1_penalty = l1_weight * model[2].weight.abs().sum() \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.001) \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> torch.manual_seed(1)\n>>> np.random.seed(1)\n>>> x = np.random.uniform(low=-1, high=1, size=(200, 2))\n>>> y = np.ones(len(x))\n>>> y[x[:, 0] * x[:, 1]<0] = 0\n>>> n_train = 100\n>>> x_train = torch.tensor(x[:n_train, :], dtype=torch.float32)\n>>> y_train = torch.tensor(y[:n_train], dtype=torch.float32)\n>>> x_valid = torch.tensor(x[n_train:, :], dtype=torch.float32)\n>>> y_valid = torch.tensor(y[n_train:], dtype=torch.float32)\n>>> fig = plt.figure(figsize=(6, 6))\n>>> plt.plot(x[y==0, 0], x[y==0, 1], 'o', alpha=0.75, markersize=10)\n>>> plt.plot(x[y==1, 0], x[y==1, 1], '<', alpha=0.75, markersize=10)\n>>> plt.xlabel(r'$x_1$', size=15)\n>>> plt.ylabel(r'$x_2$', size=15)\n>>> plt.show() \n```", "```py\n>>> model = nn.Sequential(\n...     nn.Linear(2, 1),\n...     nn.Sigmoid()\n... )\n>>> model\nSequential(\n  (0): Linear(in_features=2, out_features=1, bias=True)\n  (1): Sigmoid()\n) \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.001) \n```", "```py\n>>> from torch.utils.data import DataLoader, TensorDataset\n>>> train_ds = TensorDataset(x_train, y_train)\n>>> batch_size = 2\n>>> torch.manual_seed(1)\n>>> train_dl = DataLoader(train_ds, batch_size, shuffle=True) \n```", "```py\n>>> torch.manual_seed(1)\n>>> num_epochs = 200\n>>> def train(model, num_epochs, train_dl, x_valid, y_valid):\n...     loss_hist_train = [0] * num_epochs\n...     accuracy_hist_train = [0] * num_epochs\n...     loss_hist_valid = [0] * num_epochs\n...     accuracy_hist_valid = [0] * num_epochs\n...     for epoch in range(num_epochs):\n...         for x_batch, y_batch in train_dl:\n...             pred = model(x_batch)[:, 0]\n...             loss = loss_fn(pred, y_batch)\n...             loss.backward()\n...             optimizer.step()\n...             optimizer.zero_grad()\n...             loss_hist_train[epoch] += loss.item()\n...             is_correct = ((pred>=0.5).float() == y_batch).float()\n...             accuracy_hist_train[epoch] += is_correct.mean()\n...         loss_hist_train[epoch] /= n_train\n...         accuracy_hist_train[epoch] /= n_train/batch_size\n...         pred = model(x_valid)[:, 0]\n...         loss = loss_fn(pred, y_valid)\n...         loss_hist_valid[epoch] = loss.item()\n...         is_correct = ((pred>=0.5).float() == y_valid).float()\n...         accuracy_hist_valid[epoch] += is_correct.mean()\n...     return loss_hist_train, loss_hist_valid, \\\n...            accuracy_hist_train, accuracy_hist_valid\n>>> history = train(model, num_epochs, train_dl, x_valid, y_valid) \n```", "```py\n>>> fig = plt.figure(figsize=(16, 4))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> plt.plot(history[0], lw=4)\n>>> plt.plot(history[1], lw=4)\n>>> plt.legend(['Train loss', 'Validation loss'], fontsize=15)\n>>> ax.set_xlabel('Epochs', size=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> plt.plot(history[2], lw=4)\n>>> plt.plot(history[3], lw=4)\n>>> plt.legend(['Train acc.', 'Validation acc.'], fontsize=15)\n>>> ax.set_xlabel('Epochs', size=15) \n```", "```py\n>>> model = nn.Sequential(\n...     nn.Linear(2, 4),\n...     nn.ReLU(),\n...     nn.Linear(4, 4),\n...     nn.ReLU(),\n...     nn.Linear(4, 1),\n...     nn.Sigmoid()\n... )\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.015)\n>>> model\nSequential(\n  (0): Linear(in_features=2, out_features=4, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=4, out_features=4, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=4, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n>>> history = train(model, num_epochs, train_dl, x_valid, y_valid) \n```", "```py\n>>> class MyModule(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         l1 = nn.Linear(2, 4)\n...         a1 = nn.ReLU()\n...         l2 = nn.Linear(4, 4)\n...         a2 = nn.ReLU()\n...         l3 = nn.Linear(4, 1)\n...         a3 = nn.Sigmoid()\n...         l = [l1, a1, l2, a2, l3, a3]\n...         self.module_list = nn.ModuleList(l)\n...\n...     def forward(self, x):\n...         for f in self.module_list:\n...             x = f(x)\n...         return x \n```", "```py\n>>> model = MyModule()\n>>> model\nMyModule(\n  (module_list): ModuleList(\n    (0): Linear(in_features=2, out_features=4, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=4, out_features=4, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=4, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.015)\n>>> history = train(model, num_epochs, train_dl, x_valid, y_valid) \n```", "```py\nconda install mlxtend -c conda-forge\npip install mlxtend \n```", "```py\n>>>     def predict(self, x):\n...         x = torch.tensor(x, dtype=torch.float32)\n...         pred = self.forward(x)[:, 0]\n...         return (pred>=0.5).float() \n```", "```py\n>>> from mlxtend.plotting import plot_decision_regions\n>>> fig = plt.figure(figsize=(16, 4))\n>>> ax = fig.add_subplot(1, 3, 1)\n>>> plt.plot(history[0], lw=4)\n>>> plt.plot(history[1], lw=4)\n>>> plt.legend(['Train loss', 'Validation loss'], fontsize=15)\n>>> ax.set_xlabel('Epochs', size=15)\n>>> ax = fig.add_subplot(1, 3, 2)\n>>> plt.plot(history[2], lw=4)\n>>> plt.plot(history[3], lw=4)\n>>> plt.legend(['Train acc.', 'Validation acc.'], fontsize=15)\n>>> ax.set_xlabel('Epochs', size=15)\n>>> ax = fig.add_subplot(1, 3, 3)\n>>> plot_decision_regions(X=x_valid.numpy(),\n...                       y=y_valid.numpy().astype(np.integer),\n...                       clf=model)\n>>> ax.set_xlabel(r'$x_1$', size=15)\n>>> ax.xaxis.set_label_coords(1, -0.025)\n>>> ax.set_ylabel(r'$x_2$', size=15)\n>>> ax.yaxis.set_label_coords(-0.025, 1)\n>>> plt.show() \n```", "```py\n>>> class NoisyLinear(nn.Module):\n...     def __init__(self, input_size, output_size,\n...                  noise_stddev=0.1):\n...         super().__init__()\n...         w = torch.Tensor(input_size, output_size)\n...         self.w = nn.Parameter(w)  # nn.Parameter is a Tensor\n...                                   # that's a module parameter.\n...         nn.init.xavier_uniform_(self.w)\n...         b = torch.Tensor(output_size).fill_(0)\n...         self.b = nn.Parameter(b)\n...         self.noise_stddev = noise_stddev\n...\n...     def forward(self, x, training=False):\n...         if training:\n...             noise = torch.normal(0.0, self.noise_stddev, x.shape)\n...             x_new = torch.add(x, noise)\n...         else:\n...             x_new = x\n...         return torch.add(torch.mm(x_new, self.w), self.b) \n, was to be generated and added to the input during training only and not used for inference or evaluation.\n```", "```py\n    >>> torch.manual_seed(1)\n    >>> noisy_layer = NoisyLinear(4, 2)\n    >>> x = torch.zeros((1, 4))\n    >>> print(noisy_layer(x, training=True))\n    tensor([[ 0.1154, -0.0598]], grad_fn=<AddBackward0>)\n    >>> print(noisy_layer(x, training=True))\n    tensor([[ 0.0432, -0.0375]], grad_fn=<AddBackward0>)\n    >>> print(noisy_layer(x, training=False))\n    tensor([[0., 0.]], grad_fn=<AddBackward0>) \n    ```", "```py\n    >>> class MyNoisyModule(nn.Module):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.l1 = NoisyLinear(2, 4, 0.07)\n    ...         self.a1 = nn.ReLU()\n    ...         self.l2 = nn.Linear(4, 4)\n    ...         self.a2 = nn.ReLU()\n    ...         self.l3 = nn.Linear(4, 1)\n    ...         self.a3 = nn.Sigmoid()\n    ...\n    ...     def forward(self, x, training=False):\n    ...         x = self.l1(x, training)\n    ...         x = self.a1(x)\n    ...         x = self.l2(x)\n    ...         x = self.a2(x)\n    ...         x = self.l3(x)\n    ...         x = self.a3(x)\n    ...         return x\n    ...\n    ...     def predict(self, x):\n    ...         x = torch.tensor(x, dtype=torch.float32)\n    ...         pred = self.forward(x)[:, 0]\n    ...         return (pred>=0.5).float()\n    ...\n    >>> torch.manual_seed(1)\n    >>> model = MyNoisyModule()\n    >>> model\n    MyNoisyModule(\n      (l1): NoisyLinear()\n      (a1): ReLU()\n      (l2): Linear(in_features=4, out_features=4, bias=True)\n      (a2): ReLU()\n      (l3): Linear(in_features=4, out_features=1, bias=True)\n      (a3): Sigmoid()\n    ) \n    ```", "```py\n    >>> loss_fn = nn.BCELoss()\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.015)\n    >>> torch.manual_seed(1)\n    >>> loss_hist_train = [0] * num_epochs\n    >>> accuracy_hist_train = [0] * num_epochs\n    >>> loss_hist_valid = [0] * num_epochs\n    >>> accuracy_hist_valid = [0] * num_epochs\n    >>> for epoch in range(num_epochs):\n    ...     for x_batch, y_batch in train_dl:\n    ...         pred = model(x_batch, True)[:, 0]\n    ...         loss = loss_fn(pred, y_batch)\n    ...         loss.backward()\n    ...         optimizer.step()\n    ...         optimizer.zero_grad()\n    ...         loss_hist_train[epoch] += loss.item()\n    ...         is_correct = (\n    ...             (pred>=0.5).float() == y_batch\n    ...         ).float()\n    ...         accuracy_hist_train[epoch] += is_correct.mean()\n    ...     loss_hist_train[epoch] /= 100/batch_size\n    ...     accuracy_hist_train[epoch] /= 100/batch_size\n    ...     pred = model(x_valid)[:, 0]\n    ...     loss = loss_fn(pred, y_valid)\n    ...     loss_hist_valid[epoch] = loss.item()\n    ...     is_correct = ((pred>=0.5).float() == y_valid).float()\n    ...     accuracy_hist_valid[epoch] += is_correct.mean() \n    ```", "```py\n    >>> fig = plt.figure(figsize=(16, 4))\n    >>> ax = fig.add_subplot(1, 3, 1)\n    >>> plt.plot(loss_hist_train, lw=4)\n    >>> plt.plot(loss_hist_valid, lw=4)\n    >>> plt.legend(['Train loss', 'Validation loss'], fontsize=15)\n    >>> ax.set_xlabel('Epochs', size=15)\n    >>> ax = fig.add_subplot(1, 3, 2)\n    >>> plt.plot(accuracy_hist_train, lw=4)\n    >>> plt.plot(accuracy_hist_valid, lw=4)\n    >>> plt.legend(['Train acc.', 'Validation acc.'], fontsize=15)\n    >>> ax.set_xlabel('Epochs', size=15)\n    >>> ax = fig.add_subplot(1, 3, 3)\n    >>> plot_decision_regions(\n    ...     X=x_valid.numpy(),\n    ...     y=y_valid.numpy().astype(np.integer),\n    ...     clf=model\n    ... )\n    >>> ax.set_xlabel(r'$x_1$', size=15)\n    >>> ax.xaxis.set_label_coords(1, -0.025)\n    >>> ax.set_ylabel(r'$x_2$', size=15)\n    >>> ax.yaxis.set_label_coords(-0.025, 1)\n    >>> plt.show() \n    ```", "```py\n>>> import pandas as pd\n>>> url = 'http://archive.ics.uci.edu/ml/' \\\n...       'machine-learning-databases/auto-mpg/auto-mpg.data'\n>>> column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower',\n...                 'Weight', 'Acceleration', 'Model Year', 'Origin']\n>>> df = pd.read_csv(url, names=column_names,\n...                  na_values = \"?\", comment='\\t',\n...                  sep=\" \", skipinitialspace=True)\n>>>\n>>> ## drop the NA rows\n>>> df = df.dropna()\n>>> df = df.reset_index(drop=True)\n>>>\n>>> ## train/test splits:\n>>> import sklearn\n>>> import sklearn.model_selection\n>>> df_train, df_test = sklearn.model_selection.train_test_split(\n...     df, train_size=0.8, random_state=1\n... )\n>>> train_stats = df_train.describe().transpose()\n>>>\n>>> numeric_column_names = [\n...     'Cylinders', 'Displacement',\n...     'Horsepower', 'Weight',\n...     'Acceleration'\n... ]\n>>> df_train_norm, df_test_norm = df_train.copy(), df_test.copy()\n>>> for col_name in numeric_column_names:\n...     mean = train_stats.loc[col_name, 'mean']\n...     std  = train_stats.loc[col_name, 'std']\n...     df_train_norm.loc[:, col_name] = \\\n...         (df_train_norm.loc[:, col_name] - mean)/std\n...     df_test_norm.loc[:, col_name] = \\\n...         (df_test_norm.loc[:, col_name] - mean)/std\n>>> df_train_norm.tail() \n```", "```py\nfloat. These columns will constitute the continuous features.\n```", "```py\n>>> boundaries = torch.tensor([73, 76, 79])\n>>> v = torch.tensor(df_train_norm['Model Year'].values)\n>>> df_train_norm['Model Year Bucketed'] = torch.bucketize(\n...     v, boundaries, right=True\n... )\n>>> v = torch.tensor(df_test_norm['Model Year'].values)\n>>> df_test_norm['Model Year Bucketed'] = torch.bucketize(\n...     v, boundaries, right=True\n... )\n>>> numeric_column_names.append('Model Year Bucketed') \n```", "```py\n>>> from torch.nn.functional import one_hot\n>>> total_origin = len(set(df_train_norm['Origin']))\n>>> origin_encoded = one_hot(torch.from_numpy(\n...     df_train_norm['Origin'].values) % total_origin)\n>>> x_train_numeric = torch.tensor(\n...     df_train_norm[numeric_column_names].values)\n>>> x_train = torch.cat([x_train_numeric, origin_encoded], 1).float()\n>>> origin_encoded = one_hot(torch.from_numpy(\n...     df_test_norm['Origin'].values) % total_origin)\n>>> x_test_numeric = torch.tensor(\n...     df_test_norm[numeric_column_names].values)\n>>> x_test = torch.cat([x_test_numeric, origin_encoded], 1).float() \n```", "```py\n>>> y_train = torch.tensor(df_train_norm['MPG'].values).float()\n>>> y_test = torch.tensor(df_test_norm['MPG'].values).float() \n```", "```py\n>>> train_ds = TensorDataset(x_train, y_train)\n>>> batch_size = 8\n>>> torch.manual_seed(1)\n>>> train_dl = DataLoader(train_ds, batch_size, shuffle=True) \n```", "```py\n>>> hidden_units = [8, 4]\n>>> input_size = x_train.shape[1]\n>>> all_layers = []\n>>> for hidden_unit in hidden_units:\n...     layer = nn.Linear(input_size, hidden_unit)\n...     all_layers.append(layer)\n...     all_layers.append(nn.ReLU())\n...     input_size = hidden_unit\n>>> all_layers.append(nn.Linear(hidden_units[-1], 1))\n>>> model = nn.Sequential(*all_layers)\n>>> model\nSequential(\n  (0): Linear(in_features=9, out_features=8, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=8, out_features=4, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=4, out_features=1, bias=True)\n) \n```", "```py\n>>> loss_fn = nn.MSELoss()\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.001) \n```", "```py\n>>> torch.manual_seed(1)\n>>> num_epochs = 200\n>>> log_epochs = 20\n>>> for epoch in range(num_epochs):\n...     loss_hist_train = 0\n...     for x_batch, y_batch in train_dl:\n...         pred = model(x_batch)[:, 0]\n...         loss = loss_fn(pred, y_batch)\n...         loss.backward()\n...         optimizer.step()\n...         optimizer.zero_grad()\n...         loss_hist_train += loss.item()\n...     if epoch % log_epochs==0:\n...         print(f'Epoch {epoch}  Loss '\n...               f'{loss_hist_train/len(train_dl):.4f}')\nEpoch 0  Loss 536.1047\nEpoch 20  Loss 8.4361\nEpoch 40  Loss 7.8695\nEpoch 60  Loss 7.1891\nEpoch 80  Loss 6.7062\nEpoch 100  Loss 6.7599\nEpoch 120  Loss 6.3124\nEpoch 140  Loss 6.6864\nEpoch 160  Loss 6.7648\nEpoch 180  Loss 6.2156 \n```", "```py\n>>> with torch.no_grad():\n...     pred = model(x_test.float())[:, 0]\n...     loss = loss_fn(pred, y_test)\n...     print(f'Test MSE: {loss.item():.4f}')\n...     print(f'Test MAE: {nn.L1Loss()(pred, y_test).item():.4f}')\nTest MSE: 9.6130\nTest MAE: 2.1211 \n```", "```py\n    >>> import torchvision\n    >>> from torchvision import transforms\n    >>> image_path = './'\n    >>> transform = transforms.Compose([\n    ...     transforms.ToTensor()\n    ... ])\n    >>> mnist_train_dataset = torchvision.datasets.MNIST(\n    ...     root=image_path, train=True,\n    ...     transform=transform, download=False\n    ... )\n    >>> mnist_test_dataset = torchvision.datasets.MNIST(\n    ...     root=image_path, train=False,\n    ...     transform=transform, download=False\n    ... )\n    >>> batch_size = 64\n    >>> torch.manual_seed(1)\n    >>> train_dl = DataLoader(mnist_train_dataset,\n    ...                       batch_size, shuffle=True) \n    ```", "```py\n    >>> hidden_units = [32, 16]\n    >>> image_size = mnist_train_dataset[0][0].shape\n    >>> input_size = image_size[0] * image_size[1] * image_size[2]\n    >>> all_layers = [nn.Flatten()]\n    >>> for hidden_unit in hidden_units:\n    ...     layer = nn.Linear(input_size, hidden_unit)\n    ...     all_layers.append(layer)\n    ...     all_layers.append(nn.ReLU())\n    ...     input_size = hidden_unit\n    >>> all_layers.append(nn.Linear(hidden_units[-1], 10))\n    >>> model = nn.Sequential(*all_layers)\n    >>> model\n    Sequential(\n      (0): Flatten(start_dim=1, end_dim=-1)\n      (1): Linear(in_features=784, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=16, bias=True)\n      (4): ReLU()\n      (5): Linear(in_features=16, out_features=10, bias=True)\n    ) \n    ```", "```py\n    >>> loss_fn = nn.CrossEntropyLoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    >>> torch.manual_seed(1)\n    >>> num_epochs = 20\n    >>> for epoch in range(num_epochs):\n    ...     accuracy_hist_train = 0\n    ...     for x_batch, y_batch in train_dl:\n    ...         pred = model(x_batch)\n    ...         loss = loss_fn(pred, y_batch)\n    ...         loss.backward()\n    ...         optimizer.step()\n    ...         optimizer.zero_grad()\n    ...         is_correct = (\n    ...             torch.argmax(pred, dim=1) == y_batch\n    ...         ).float()\n    ...         accuracy_hist_train += is_correct.sum()\n    ...     accuracy_hist_train /= len(train_dl.dataset)\n    ...     print(f'Epoch {epoch}  Accuracy '\n    ...           f'{accuracy_hist_train:.4f}')\n    Epoch 0  Accuracy 0.8531\n    ...\n    Epoch 9  Accuracy 0.9691\n    ...\n    Epoch 19  Accuracy 0.9813 \n    ```", "```py\n    >>> pred = model(mnist_test_dataset.data / 255.)\n    >>> is_correct = (\n    ...     torch.argmax(pred, dim=1) ==\n    ...     mnist_test_dataset.targets\n    ... ).float()\n    >>> print(f'Test accuracy: {is_correct.mean():.4f}')\n    Test accuracy: 0.9645 \n    ```", "```py\npip install pytorch-lightning \n```", "```py\nconda install pytorch-lightning -c conda-forge \n```", "```py\nimport pytorch_lightning as pl\nimport torch \nimport torch.nn as nn \nfrom torchmetrics import Accuracy\nclass MultiLayerPerceptron(pl.LightningModule):\n    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n        super().__init__()\n\n        # new PL attributes:\n        self.train_acc = Accuracy()\n        self.valid_acc = Accuracy()\n        self.test_acc = Accuracy()\n\n        # Model similar to previous section:\n        input_size = image_shape[0] * image_shape[1] * image_shape[2]\n        all_layers = [nn.Flatten()]\n        for hidden_unit in hidden_units: \n            layer = nn.Linear(input_size, hidden_unit) \n            all_layers.append(layer) \n            all_layers.append(nn.ReLU()) \n            input_size = hidden_unit \n\n        all_layers.append(nn.Linear(hidden_units[-1], 10))  \n        self.model = nn.Sequential(*all_layers)\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = nn.functional.cross_entropy(self(x), y)\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def training_epoch_end(self, outs):\n        self.log(\"train_acc\", self.train_acc.compute())\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = nn.functional.cross_entropy(self(x), y)\n        preds = torch.argmax(logits, dim=1)\n        self.valid_acc.update(preds, y)\n        self.log(\"valid_loss\", loss, prog_bar=True)\n        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = nn.functional.cross_entropy(self(x), y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optimizer \n```", "```py\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nclass MnistDataModule(pl.LightningDataModule):\n    def __init__(self, data_path='./'):\n        super().__init__()\n        self.data_path = data_path\n        self.transform = transforms.Compose([transforms.ToTensor()])\n\n    def prepare_data(self):\n        MNIST(root=self.data_path, download=True) \n    def setup(self, stage=None):\n        # stage is either 'fit', 'validate', 'test', or 'predict'\n        # here note relevant\n        mnist_all = MNIST( \n            root=self.data_path,\n            train=True,\n            transform=self.transform,  \n            download=False\n        ) \n        self.train, self.val = random_split(\n            mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n        )\n        self.test = MNIST( \n            root=self.data_path,\n            train=False,\n            transform=self.transform,  \n            download=False\n        ) \n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=64, num_workers=4)\n    def val_dataloader(self):\n        return DataLoader(self.val, batch_size=64, num_workers=4)\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=64, num_workers=4) \n```", "```py\ntorch.manual_seed(1) \nmnist_dm = MnistDataModule() \n```", "```py\nmnistclassifier = MultiLayerPerceptron()\nif torch.cuda.is_available(): # if you have GPUs\n    trainer = pl.Trainer(max_epochs=10, gpus=1)\nelse:\n    trainer = pl.Trainer(max_epochs=10)\ntrainer.fit(model=mnistclassifier, datamodule=mnist_dm) \n```", "```py\nEpoch 9: 100% 939/939 [00:07<00:00, 130.42it/s, loss=0.1, v_num=0, train_loss=0.260, valid_loss=0.166, valid_acc=0.949] \n```", "```py\npip install tensorboard \n```", "```py\nconda install tensorboard -c conda-forge \n```", "```py\ntensorboard --logdir lightning_logs/ \n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir lightning_logs/ \n```", "```py\nif torch.cuda.is_available(): # if you have GPUs\n    trainer = pl.Trainer(max_epochs=15, resume_from_checkpoint='./lightning_logs/version_0/checkpoints/epoch=8-step=7739.ckpt', gpus=1)\nelse:\n    trainer = pl.Trainer(max_epochs=15, resume_from_checkpoint='./lightning_logs/version_0/checkpoints/epoch=8-step=7739.ckpt')\ntrainer.fit(model=mnistclassifier, datamodule=mnist_dm) \n```", "```py\ntrainer.test(model=mnistclassifier, datamodule=mnist_dm) \n```", "```py\n[{'test_loss': 0.14912301301956177, 'test_acc': 0.9499600529670715}] \n```", "```py\nmodel = MultiLayerPerceptron.load_from_checkpoint(\"path/to/checkpoint.ckpt\") \n```"]