["```py\n    def loadGlove(path):\n        file = open(path,'r')\n        model = {}\n        for l in file:\n            line = l.split()\n            word = line[0]\n            value = np.array([float(val) for val in                           line[1:]])\n            model[word] = value\n        return model\n    glove = loadGlove('glove.6B.50d.txt')\n    ```", "```py\n    glove['python']\n    ```", "```py\n    cosine_similarity(glove['cat'].reshape(1, -1), glove['dog'].reshape(1, -1))\n    ```", "```py\n    cosine_similarity(glove['cat'].reshape(1, -1), glove['piano'].reshape(1, -1))\n    ```", "```py\npredicted_king_embedding = glove['queen'] - glove['woman'] + glove['man']\ncosine_similarity(predicted_king_embedding.reshape(1, -1), glove['king'].reshape(1, -1))\n```", "```py\n    text = text.replace(',','').replace('.','').lower().                            split()\n    ```", "```py\n    corpus = set(text)\n    corpus_length = len(corpus)\n    ```", "```py\n    word_dict = {}\n    inverse_word_dict = {}\n    for i, word in enumerate(corpus):\n        word_dict[word] = i\n        inverse_word_dict[i] = word\n    ```", "```py\n    data = []\n    for i in range(2, len(text) - 2):\n        sentence = [text[i-2], text[i-1],\n                    text[i+1], text[i+2]]\n        target = text[i]\n        data.append((sentence, target))\n\n    print(data[3])\n    ```", "```py\n    embedding_length = 20\n    ```", "```py\n    class CBOW(torch.nn.Module):\n        def __init__(self, corpus_length, embedding_dim):\n            super(CBOW, self).__init__()\n\n            self.embeddings = nn.Embedding(corpus_length,                             embedding_dim)\n            self.linear1 = nn.Linear(embedding_dim, 64)\n            self.linear2 = nn.Linear(64, corpus_length)\n\n            self.activation_function1 = nn.ReLU()\n            self.activation_function2 = nn.LogSoftmax                                        (dim = -1)\n        def forward(self, inputs):\n            embeds = sum(self.embeddings(inputs)).view(1,-1)\n            out = self.linear1(embeds)\n            out = self.activation_function1(out)\n            out = self.linear2(out)\n            out = self.activation_function2(out)\n            return out\n    ```", "```py\n    def get_word_emdedding(self, word):\n    word = torch.LongTensor([word_dict[word]])\n    return self.embeddings(word).view(1,-1)\n    ```", "```py\n    model = CBOW(corpus_length, embedding_length)\n    loss_function = nn.NLLLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    ```", "```py\n    def make_sentence_vector(sentence, word_dict):\n        idxs = [word_dict[w] for w in sentence]\n        return torch.tensor(idxs, dtype=torch.long)\n    print(make_sentence_vector(['stormy','nights','when','the'], word_dict))\n    ```", "```py\n    for epoch in range(100):\n        epoch_loss = 0\n        for sentence, target in data:\n            model.zero_grad()\n            sentence_vector = make_sentence_vector                               (sentence, word_dict)  \n            log_probs = model(sentence_vector)\n            loss = loss_function(log_probs, torch.tensor(\n            [word_dict[target]], dtype=torch.long))\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.data\n        print('Epoch: '+str(epoch)+', Loss: ' + str(epoch_loss.item()))\n    ```", "```py\n    def get_predicted_result(input, inverse_word_dict):\n        index = np.argmax(input)\n        return inverse_word_dict[index]\n    def predict_sentence(sentence):\n        sentence_split = sentence.replace('.','').lower().                              split()\n        sentence_vector = make_sentence_vector(sentence_                      split, word_dict)\n        prediction_array = model(sentence_vector).data.                             numpy()\n        print('Preceding Words: {}\\n'.format(sentence_           split[:2]))\n        print('Predicted Word: {}\\n'.format(get_predicted_            result(prediction_array[0], inverse_            word_dict)))\n        print('Following Words: {}\\n'.format(sentence_           split[2:]))\n    predict_sentence('to see leap and')\n    ```", "```py\n    print(model.get_word_emdedding('leap'))\n    ```", "```py\n    text = 'This is a single sentence.'\n    tokens = word_tokenize(text)\n    print(tokens)\n    ```", "```py\n    no_punctuation = [word.lower() for word in tokens if word.isalpha()]\n    print(no_punctuation)\n    ```", "```py\n    text = \"This is the first sentence. This is the second sentence. A document contains many sentences.\"\n    print(sent_tokenize(text))\n    ```", "```py\n    print([word_tokenize(sentence) for sentence in sent_tokenize(text)])\n    ```", "```py\n    stop_words = stopwords.words('english')\n    print(stop_words[:20])\n    ```", "```py\n    text = 'This is a single sentence.'\n    tokens = [token for token in word_tokenize(text) if token not in stop_words]\n    print(tokens)\n    ```", "```py\nsentence = \"The big dog is sleeping on the bed\"\ntoken = nltk.word_tokenize(sentence)\nnltk.pos_tag(token)\n```", "```py\nnltk.help.upenn_tagset(\"VBG\")\n```", "```py\nexpression = ('NP: {<DT>?<JJ>*<NN>}')\n```", "```py\ntagged = nltk.pos_tag(token)\nREchunkParser = nltk.RegexpParser(expression)\ntree = REchunkParser.parse(tagged)\nprint(tree)\n```", "```py\n    emma = nltk.corpus.gutenberg.sents('austen-emma.txt')\n    emma_sentences = []\n    emma_word_set = []\n    for sentence in emma:\n        emma_sentences.append([word.lower() for word in          sentence if word.isalpha()])\n        for word in sentence:\n            if word.isalpha():\n                emma_word_set.append(word.lower())\n    emma_word_set = set(emma_word_set)\n    ```", "```py\n    def TermFreq(document, word):\n        doc_length = len(document)\n        occurances = len([w for w in document if w == word])\n        return occurances / doc_length\n    TermFreq(emma_sentences[5], 'ago')\n    ```", "```py\n    def build_DF_dict():\n        output = {}\n        for word in emma_word_set:\n            output[word] = 0\n            for doc in emma_sentences:\n                if word in doc:\n                    output[word] += 1\n        return output\n\n    df_dict = build_DF_dict()\n    df_dict['ago']\n    ```", "```py\n    def InverseDocumentFrequency(word):\n        N = len(emma_sentences)\n        try:\n            df = df_dict[word] + 1\n        except:\n            df = 1\n        return np.log(N/df)\n    InverseDocumentFrequency('ago')\n    ```", "```py\n    def TFIDF(doc, word):\n        tf = TF(doc, word)\n        idf = InverseDocumentFrequency(word)\n        return tf*idf\n    print('ago - ' + str(TFIDF(emma_sentences[5],'ago')))\n    print('indistinct - ' + str(TFIDF(emma_sentences[5],'indistinct')))\n    ```", "```py\n    def loadGlove(path):\n        file = open(path,'r')\n        model = {}\n        for l in file:\n            line = l.split()\n            word = line[0]\n            value = np.array([float(val) for val in                           line[1:]])\n            model[word] = value\n        return model\n    glove = loadGlove('glove.6B.50d.txt')\n    ```", "```py\n    embeddings = []\n    for word in emma_sentences[5]:\n        embeddings.append(glove[word])\n    mean_embedding = np.mean(embeddings, axis = 0).reshape      (1, -1)\n    print(mean_embedding)\n    ```", "```py\n    embeddings = []\n    for word in emma_sentences[5]:\n        tfidf = TFIDF(emma_sentences[5], word)\n        embeddings.append(glove[word]* tfidf) \n\n    tfidf_weighted_embedding = np.mean(embeddings, axis =                               0).reshape(1, -1)\n    print(tfidf_weighted_embedding)\n    ```", "```py\n    cosine_similarity(mean_embedding, tfidf_weighted_embedding)\n    ```"]