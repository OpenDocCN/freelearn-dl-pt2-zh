["```py\n    >>> import tarfile\n    >>> with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n    ...     tar.extractall() \n    ```", "```py\n>>> import pyprind\n>>> import pandas as pd\n>>> import os\n>>> import sys\n>>> # change the 'basepath' to the directory of the\n>>> # unzipped movie dataset\n>>> basepath = 'aclImdb'\n>>>\n>>> labels = {'pos': 1, 'neg': 0}\n>>> pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n>>> df = pd.DataFrame()\n>>> for s in ('test', 'train'):\n...     for l in ('pos', 'neg'):\n...         path = os.path.join(basepath, s, l)\n...         for file in sorted(os.listdir(path)):\n...             with open(os.path.join(path, file),\n...                       'r', encoding='utf-8') as infile:\n...                 txt = infile.read()\n...             df = df.append([[txt, labels[l]]],\n...                            ignore_index=True)\n...             pbar.update()\n>>> df.columns = ['review', 'sentiment']\n0%                          100%\n[##############################] | ETA: 00:00:00\nTotal time elapsed: 00:00:25 \n```", "```py\n>>> import numpy as np\n>>> np.random.seed(0)\n>>> df = df.reindex(np.random.permutation(df.index))\n>>> df.to_csv('movie_data.csv', index=False, encoding='utf-8') \n```", "```py\n>>> df = pd.read_csv('movie_data.csv', encoding='utf-8')\n>>> # the following column renaming is necessary on some computers:\n>>> df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n>>> df.head(3) \n```", "```py\n>>> df.shape\n(50000, 2) \n```", "```py\n>>> import numpy as np\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> count = CountVectorizer()\n>>> docs = np.array(['The sun is shining',\n...                  'The weather is sweet',\n...                  'The sun is shining, the weather is sweet,'\n...                  'and one and one is two'])\n>>> bag = count.fit_transform(docs) \n```", "```py\n>>> print(count.vocabulary_)\n{'and': 0,\n'two': 7,\n'shining': 3,\n'one': 2,\n'sun': 4,\n'weather': 8,\n'the': 6,\n'sweet': 5,\n'is': 1} \n```", "```py\n>>> print(bag.toarray())\n[[0 1 0 1 1 0 1 0 0]\n [0 1 0 0 0 1 1 0 1]\n [2 3 2 1 1 1 2 1 1]] \n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfTransformer\n>>> tfidf = TfidfTransformer(use_idf=True,\n...                          norm='l2',\n...                          smooth_idf=True)\n>>> np.set_printoptions(precision=2)\n>>> print(tfidf.fit_transform(count.fit_transform(docs))\n...       .toarray())\n[[ 0\\.    0.43  0\\.    0.56  0.56  0\\.    0.43  0\\.    0\\.  ]\n [ 0\\.    0.43  0\\.    0\\.    0\\.    0.56  0.43  0\\.    0.56]\n [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]] \n```", "```py\n>>> df.loc[0, 'review'][-50:]\n'is seven.<br /><br />Title (Brazil): Not Available' \n```", "```py\n>>> import re\n>>> def preprocessor(text):\n...     text = re.sub('<[^>]*>', '', text)\n...     emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n...                            text)\n...     text = (re.sub('[\\W]+', ' ', text.lower()) +\n...             ' '.join(emoticons).replace('-', ''))\n...     return text \n```", "```py\n>>> preprocessor(df.loc[0, 'review'][-50:])\n'is seven title brazil not available'\n>>> preprocessor(\"</a>This :) is :( a test :-)!\")\n'this is a test :) :( :)' \n```", "```py\n>>> df['review'] = df['review'].apply(preprocessor) \n```", "```py\n>>> def tokenizer(text):\n...     return text.split()\n>>> tokenizer('runners like running and thus they run')\n['runners', 'like', 'running', 'and', 'thus', 'they', 'run'] \n```", "```py\n>>> from nltk.stem.porter import PorterStemmer\n>>> porter = PorterStemmer()\n>>> def tokenizer_porter(text):\n...     return [porter.stem(word) for word in text.split()]\n>>> tokenizer_porter('runners like running and thus they run')\n['runner', 'like', 'run', 'and', 'thu', 'they', 'run'] \n```", "```py\n>>> import nltk\n>>> nltk.download('stopwords') \n```", "```py\n>>> from nltk.corpus import stopwords\n>>> stop = stopwords.words('english')\n>>> [w for w in tokenizer_porter('a runner likes'\n...  ' running and runs a lot')\n...  if w not in stop]\n['runner', 'like', 'run', 'run', 'lot'] \n```", "```py\n>>> X_train = df.loc[:25000, 'review'].values\n>>> y_train = df.loc[:25000, 'sentiment'].values\n>>> X_test = df.loc[25000:, 'review'].values\n>>> y_test = df.loc[25000:, 'sentiment'].values \n```", "```py\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> tfidf = TfidfVectorizer(strip_accents=None,\n...                         lowercase=False,\n...                         preprocessor=None)\n>>> small_param_grid = [\n...     {\n...         'vect__ngram_range': [(1, 1)],\n...         'vect__stop_words': [None],\n...         'vect__tokenizer': [tokenizer, tokenizer_porter],\n...         'clf__penalty': ['l2'],\n...         'clf__C': [1.0, 10.0]\n...     },\n...     {\n...         'vect__ngram_range': [(1, 1)],\n...         'vect__stop_words': [stop, None],\n...         'vect__tokenizer': [tokenizer],\n...         'vect__use_idf':[False],\n...         'vect__norm':[None],\n...         'clf__penalty': ['l2'],\n...         'clf__C': [1.0, 10.0]\n...     },\n... ]\n>>> lr_tfidf = Pipeline([\n...     ('vect', tfidf),\n...     ('clf', LogisticRegression(solver='liblinear'))\n... ])\n>>> gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n...                            scoring='accuracy', cv=5,\n...                            verbose=2, n_jobs=1)\n>>> gs_lr_tfidf.fit(X_train, y_train) \n```", "```py\n>>> print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\nBest parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x169932dc0>} \n```", "```py\n>>> print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')\nCV Accuracy: 0.897\n>>> clf = gs_lr_tfidf.best_estimator_\n>>> print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')\nTest Accuracy: 0.899 \n```", "```py\n>>> import numpy as np\n>>> import re\n>>> from nltk.corpus import stopwords\n>>> stop = stopwords.words('english')\n>>> def tokenizer(text):\n...     text = re.sub('<[^>]*>', '', text)\n...     emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n...                            text.lower())\n...     text = re.sub('[\\W]+', ' ', text.lower()) \\\n...                   + ' '.join(emoticons).replace('-', '')\n...     tokenized = [w for w in text.split() if w not in stop]\n...     return tokenized \n```", "```py\n>>> def stream_docs(path):\n...     with open(path, 'r', encoding='utf-8') as csv:\n...         next(csv) # skip header\n...         for line in csv:\n...             text, label = line[:-3], int(line[-2])\n...             yield text, label \n```", "```py\n>>> next(stream_docs(path='movie_data.csv'))\n('\"In 1974, the teenager Martha Moxley ... ',1) \n```", "```py\n>>> def get_minibatch(doc_stream, size):\n...     docs, y = [], []\n...     try:\n...         for _ in range(size):\n...             text, label = next(doc_stream)\n...             docs.append(text)\n...             y.append(label)\n...     except StopIteration:\n...         return None, None\n...     return docs, y \n```", "```py\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> from sklearn.linear_model import SGDClassifier\n>>> vect = HashingVectorizer(decode_error='ignore',\n...                          n_features=2**21,\n...                          preprocessor=None,\n...                          tokenizer=tokenizer)\n>>> clf = SGDClassifier(loss='log', random_state=1)\n>>> doc_stream = stream_docs(path='movie_data.csv') \n```", "```py\n>>> import pyprind\n>>> pbar = pyprind.ProgBar(45)\n>>> classes = np.array([0, 1])\n>>> for _ in range(45):\n...     X_train, y_train = get_minibatch(doc_stream, size=1000)\n...     if not X_train:\n...         break\n...     X_train = vect.transform(X_train)\n...     clf.partial_fit(X_train, y_train, classes=classes)\n...     pbar.update()\n0%                          100%\n[##############################] | ETA: 00:00:00\nTotal time elapsed: 00:00:21 \n```", "```py\n>>> X_test, y_test = get_minibatch(doc_stream, size=5000)\n>>> X_test = vect.transform(X_test)\n>>> print(f'Accuracy: {clf.score(X_test, y_test):.3f}')\nAccuracy: 0.868 \n```", "```py\n>>> X_test, y_test = get_minibatch(doc_stream, size=5000) \n```", "```py\n>>> clf = clf.partial_fit(X_test, y_test) \n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv('movie_data.csv', encoding='utf-8')\n>>> # the following is necessary on some computers:\n>>> df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"}) \n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> count = CountVectorizer(stop_words='english',\n...                         max_df=.1,\n...                         max_features=5000)\n>>> X = count.fit_transform(df['review'].values) \n```", "```py\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> lda = LatentDirichletAllocation(n_components=10,\n...                                 random_state=123,\n...                                 learning_method='batch')\n>>> X_topics = lda.fit_transform(X) \n```", "```py\n>>> lda.components_.shape\n(10, 5000) \n```", "```py\n>>> n_top_words = 5\n>>> feature_names = count.get_feature_names_out()\n>>> for topic_idx, topic in enumerate(lda.components_):\n...     print(f'Topic {(topic_idx + 1)}:')\n...     print(' '.join([feature_names[i]\n...                     for i in topic.argsort()\\\n...                     [:-n_top_words - 1:-1]]))\nTopic 1:\nworst minutes awful script stupid\nTopic 2:\nfamily mother father children girl\nTopic 3:\namerican war dvd music tv\nTopic 4:\nhuman audience cinema art sense\nTopic 5:\npolice guy car dead murder\nTopic 6:\nhorror house sex girl woman\nTopic 7:\nrole performance comedy actor performances\nTopic 8:\nseries episode war episodes tv\nTopic 9:\nbook version original read novel\nTopic 10:\naction fight guy guys cool \n```", "```py\n>>> horror = X_topics[:, 5].argsort()[::-1]\n>>> for iter_idx, movie_idx in enumerate(horror[:3]):\n...     print(f'\\nHorror movie #{(iter_idx + 1)}:')\n...     print(df['review'][movie_idx][:300], '...')\nHorror movie #1:\nHouse of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\nHorror movie #2:\nOkay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there ...\nHorror movie #3:\n<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ... \n```"]