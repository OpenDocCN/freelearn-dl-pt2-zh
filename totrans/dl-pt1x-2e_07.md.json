["```py\nJust perfect. Script, character, animation....this manages to break free of the yoke of 'children's movie' to simply be one of the best movies of the 90's, full-stop.\n```", "```py\ntoy_story_review = \"Just perfect. Script, character, animation....this manages to break free of the yoke of 'children's movie' to simply be one of the best movies of the 90's, full-stop.\"\n\nprint(list(toy_story_review))\n```", "```py\n['J', 'u', 's', 't', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', '.', ' ', 'S', 'c', 'r', 'i', 'p', 't', ',', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ',', ' ', 'a', 'n', 'i', 'm', 'a', 't', 'i', 'o', 'n', '.', '.', '.', '.', 't', 'h', 'i', 's', ' ', 'm', 'a', 'n', 'a', 'g', 'e', 's', ' ', 't', 'o', ' ', 'b', 'r', 'e', 'a', 'k', ' ', 'f', 'r', 'e', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'y', 'o', 'k', 'e', ' ', 'o', 'f', ' ', \"'\", 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', \"'\", 's', ' ', 'm', 'o', 'v', 'i', 'e', \"'\", ' ', 't', 'o', ' ', 's', 'i', 'm', 'p', 'l', 'y', ' ', 'b', 'e', ' ', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'b', 'e', 's', 't', ' ', 'm', 'o', 'v', 'i', 'e', 's', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', '9', '0', \"'\", 's', ',', ' ', 'f', 'u', 'l', 'l', '-', 's', 't', 'o', 'p', '.']\n```", "```py\nprint(list(toy_story_review.split()))\n```", "```py\n['Just', 'perfect.', 'Script,', 'character,', 'animation....this', 'manages', 'to', 'break', 'free', 'of', 'the', 'yoke', 'of', \"'children's\", \"movie'\", 'to', 'simply', 'be', 'one', 'of', 'the', 'best', 'movies', 'of', 'the', \"90's,\", 'full-stop.']\n```", "```py\nfrom nltk import ngrams \nprint(list(ngrams(toy_story_review.split(),2)))\n```", "```py\n[('Just', 'perfect.'), ('perfect.', 'Script,'), ('Script,', 'character,'), ('character,', 'animation....this'), ('animation....this', 'manages'), ('manages', 'to'), ('to', 'break'), ('break', 'free'), ('free', 'of'), ('of', 'the'), ('the', 'yoke'), ('yoke', 'of'), ('of', \"'children's\"), (\"'children's\", \"movie'\"), (\"movie'\", 'to'), ('to', 'simply'), ('simply', 'be'), ('be', 'one'), ('one', 'of'), ('of', 'the'), ('the', 'best'), ('best', 'movies'), ('movies', 'of'), ('of', 'the'), ('the', \"90's,\"), (\"90's,\", 'full-stop.')]\n```", "```py\nprint(list(ngrams(toy_story_review.split(),3)))\n```", "```py\n[('Just', 'perfect.', 'Script,'), ('perfect.', 'Script,', 'character,'), ('Script,', 'character,', 'animation....this'), ('character,', 'animation....this', 'manages'), ('animation....this', 'manages', 'to'), ('manages', 'to', 'break'), ('to', 'break', 'free'), ('break', 'free', 'of'), ('free', 'of', 'the'), ('of', 'the', 'yoke'), ('the', 'yoke', 'of'), ('yoke', 'of', \"'children's\"), ('of', \"'children's\", \"movie'\"), (\"'children's\", \"movie'\", 'to'), (\"movie'\", 'to', 'simply'), ('to', 'simply', 'be'), ('simply', 'be', 'one'), ('be', 'one', 'of'), ('one', 'of', 'the'), ('of', 'the', 'best'), ('the', 'best', 'movies'), ('best', 'movies', 'of'), ('movies', 'of', 'the'), ('of', 'the', \"90's,\"), ('the', \"90's,\", 'full-stop.')]\n```", "```py\nclass Dictionary(object): \n    def init (self):\n        self.word2index = {} \n        self.index2word = [] \n        self.length = 0\n\n    def add_word(self,word):\n        if word not in self.index2word: \n            self.indexword.append(word) \n            self.word2index[word] = self.length + 1 \n            self.length += 1\n        return self.word2index[word] \n\n    def len (self):\n        return len(self.index2word) \n\n    def onehot_encoded(self,word):\n        vec = np.zeros(self.length)\n        vec[self.word2index[word]] = 1 \n        return vec\n```", "```py\ndic = Dictionary()\n\nfor tok in toy_story_review.split(): dic.add_word(tok)\nprint(dic.word2index)\n```", "```py\npip install torchtext\n```", "```py\nfrom torchtext import data\ntext = data.Field(lower=True, batch_first=True,fix_length=20) \nlabel = data.Field(sequential=False)\n```", "```py\ntrain, test = datasets.IMDB.splits(text, label)\n```", "```py\nprint('train.fields', train.fields)\n```", "```py\n#Results\ntrain.fields {'text': <torchtext.data.field.Field object at 0x1129db160>, 'label': <torchtext.data.field.Field object at 0x1129db1d0>}\n```", "```py\nprint(vars(train[0]))\n```", "```py\n#Results\nvars(train[0]) {'text': ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream.', 'watch', 'for', 'alan', '\"the', 'skipper\"', 'hale', 'jr.', 'as', 'a', 'police', 'sgt.'], 'label': 'pos'}\n```", "```py\ntext.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\nlabel.build_vocab(train)\n```", "```py\nprint(text.vocab.freqs)\n```", "```py\n# A sample result \nCounter({\"i'm\": 4174,\n         'not': 28597,\n         'tired': 328,\n         'to': 133967,\n         'say': 4392,\n         'this': 69714,\n         'is': 104171,\n         'one': 22480,\n         'of': 144462,\n         'the': 322198,\n```", "```py\nprint(text.vocab.vectors)\n```", "```py\n#Results displaying the 300 dimension vector for each word.\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0466 0.2132 -0.0074 ... 0.0091 -0.2099 0.0539\n ... ... ... \n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.7724 -0.1800 0.2072 ... 0.6736 0.2263 -0.2919\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n[torch.FloatTensor of size 10002x300]\n```", "```py\nprint(TEXT.vocab.stoi)\n```", "```py\n# Sample results\ndefaultdict(<function torchtext.vocab._default_unk_index>,\n {'<unk>': 0,\n '<pad>': 1,\n 'the': 2,\n 'a': 3,\n 'and': 4,\n 'of': 5,\n 'to': 6,\n 'is': 7,\n 'in': 8,\n 'i': 9,\n 'this': 10,\n 'that': 11,\n 'it': 12,\n```", "```py\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=128, device=-1,shuffle=True)\n#device = -1 represents cpu , if you want gpu leave it to None.\n```", "```py\nbatch = next(iter(train_iter)) batch.text\n```", "```py\n#Results\nVariable containing:\n 5128 427 19 ... 1688 0 542\n 58 2 0 ... 2 0 1352\n 0 9 14 ... 2676 96 9\n ... ... ... \n 129 1181 648 ... 45 0 2\n 6484 0 627 ... 381 5 2\n 748 0 5052 ... 18 6660 9827\n[torch.LongTensor of size 128x20]\n```", "```py\nbatch.label\n```", "```py\n#Results\nVariable containing:\n 2\n 1\n 2\n 1\n 2\n 1\n 1\n 1\n[torch.LongTensor of size 128]\n```", "```py\nclass EmbeddingNetwork(nn.Module):\n    def init(self,emb_size,hidden_size1,hidden_size2=400): \n        super().  init ()\n        self.embedding = nn.Embedding(emb_size,hidden_size1) \n        self.fc = nn.Linear(hidden_size2,3)\n    def forward(self,x):\n        embeds = self.embedding(x).view(x.size(0),-1) \n        out = self.fc(embeds)\n        return F.log_softmax(out,dim=-1)\n```", "```py\ndef fit(epoch,model,data_loader,phase='training',volatile=False): \n    if phase == 'training':\n        model.train()\n    if phase == 'validation': \n        model.evaluation() \nvolatile=True\nrunning_loss = 0.0\nrunning_correct = 0\n```", "```py\nfor batch_idx , batch in enumerate(data_loader):\n    text, target = batch.text , batch.label \n    if is_cuda:\n        text,target = text.cuda(),target.cuda() \n    if phase == 'training':\n        optimizer.zero_grad() \n        output = model(text)\n    loss = F.nll_loss(output,target) \n    running_loss += F.nll_loss(output,target,size_average=False).data[0] \n    predictions = output.data.max(dim=1,keepdim=True)[1]\n    running_correct += predictions.eq(target.data.view_as(predictions)).cpu().sum() \n    if phase == 'training':\n        loss.backward() \n        optimizer.step()\n        loss = running_loss/len(data_loader.dataset)\n        accuracy = 100\\. * running_correct/len(data_loader.dataset) \n        print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}').format(loss,accuracy)\n```", "```py\ntrain_losses , train_accuracy = [],[] \nvalidation_losses , validation_accuracy = [],[]\n\ntrain_iter.repeat = False\ntest_iter.repeat = False\nfor epoch in range(1,10): \n    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n    validation_epoch_loss, validation_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss) \n    train_accuracy.append(epoch_accuracy) \n    validation_losses.append(validation_epoch_loss) \n    validation_accuracy.append(validation_epoch_accuracy)\n```", "```py\nfrom torchtext.vocab import GloVe \nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10) \nLABEL.build_vocab(train,)\n```", "```py\nTEXT.vocab.vectors\n```", "```py\n#Output\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0466 0.2132 -0.0074 ... 0.0091 -0.2099 0.0539\n... ...  ...\n\n[torch.FloatTensor of size 10002x300]\n```", "```py\nmodel.embedding.weight.data = TEXT.vocab.vectors\n```", "```py\nclass EmbeddingNetwork(nn.Module):\ndef   init (self,embedding_size,hidden_size1,hidden_size2=400): super().  init ()\nself.embedding = nn.Embedding(embedding_size,hidden_size1) self.fc1 = nn.Linear(hidden_size2,3)\n\ndef forward(self,x):\nembeds = self.embedding(x).view(x.size(0),-1) out = self.fc1(embeddings)\nreturn F.log_softmax(out,dim=-1)\n\nmodel = EmbeddingNetwork(len(TEXT.vocab.stoi),300,12000)\n```", "```py\nmodel.embedding.weight.requires_grad = False\noptimizer = optim.SGD([ param for param in model.parameters() if param.requires_grad == True],lr=0.001)\n```", "```py\nrnn = RNN(input_size, hidden_size,output_size) \nfor i in range(len(toy_story_review):\n        output, hidden = rnn(toy_story_review[i], hidden)\n```", "```py\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def   init (self, input_size, hidden_size, output_size): \n        super(RNN, self). init ()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) \n        self.i2o = nn.Linear(input_size + hidden_size, output_size) \n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1) \n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output) \n        return output, hidden\n\n    def initHidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n```", "```py\nTEXT = data.Field(lower=True,fix_length=200,batch_first=False) \nLABEL = data.Field(sequential=False,)\ntrain, test = IMDB.splits(TEXT, LABEL) \nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10) \nLABEL.build_vocab(train,)\n```", "```py\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device=-1) \ntrain_iter.repeat = False \ntest_iter.repeat = False\n```", "```py\nclass IMDBRnn(nn.Module):\n\n    def   init (self,vocab,hidden_size,n_cat,bs=1,nl=2): \n        super().  init ()\n        self.hidden_size = hidden_size \n        self.bs = bs\n        self.nl = nl\n        self.e = nn.Embedding(n_vocab,hidden_size) \n        self.rnn = nn.LSTM(hidden_size,hidden_size,nl) \n       self.fc2 = nn.Linear(hidden_size,n_cat) \n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self,inp): \n        bs = inp.size()[1] \n        if bs != self.bs:\n            self.bs = bs \n        e_out = self.e(inp) \n        h0 = c0 = Variable(e_out.data.new(*(self.nl,self.bs,self.hidden_size)).zero_()) \n        rnn_o,_ = self.rnn(e_out,(h0,c0))\n        rnn_o = rnn_o[-1]\n        fc = F.dropout(self.fc2(rnn_o),p=0.8) \n        return self.softmax(fc)\n```", "```py\nmodel = IMDBRnn(n_vocab,n_hidden,3,bs=32) \nmodel = model.cuda()\n\noptimizer = optim.Adam(model.parameters(),lr=1e-3)\n\ndef fit(epoch,model,data_loader,phase='training',volatile=False): \n    if phase == 'training':\n        model.train()\n    if phase == 'validation': \n        model.eval() \n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , batch in enumerate(data_loader): \n        text , target = batch.text , batch.label\n        if is_cuda:\n            text,target = text.cuda(),target.cuda() \n\n        if phase == 'training':\n            optimizer.zero_grad() \n        output = model(text)\n        loss = F.nll_loss(output,target) \n\n        running_loss += F.nll_loss(output,target,size_average=False).data[0] \n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum() \n        if phase == 'training':\n            loss.backward() \n            optimizer.step()\n\n    loss = running_loss/len(data_loader.dataset)\n    accuracy = 100\\. * running_correct/len(data_loader.dataset) \n\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')         return loss,accuracy\n\ntrain_losses , train_accuracy = [],[]\nvalidation_losses , validation_accuracy = [],[]\n\nfor epoch in range(1,5): \n\n    epoch_loss, epoch_accuracy =\nfit(epoch,model,train_iter,phase='training')\n    validation_epoch_loss , validation_epoch_accuracy =\nfit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    validation_losses.append(validation_epoch_loss)\n    validation_accuracy.append(validation_epoch_accuracy)\n```", "```py\ntraining loss is 0.7 and training accuracy is 12564/25000 50.26\nvalidation loss is 0.7 and validation accuracy is 12500/25000 50.0\ntraining loss is 0.66 and training accuracy is 14931/25000 59.72\nvalidation loss is 0.57 and validation accuracy is 17766/25000 71.06\ntraining loss is 0.43 and training accuracy is 20229/25000 80.92\nvalidation loss is 0.4 and validation accuracy is 20446/25000 81.78\ntraining loss is 0.3 and training accuracy is 22026/25000 88.1\nvalidation loss is 0.37 and validation accuracy is 21009/25000 84.04\n```", "```py\nclass IMDBCnn(nn.Module): \n\n    def\n__init__(self,vocab,hidden_size,n_cat,bs=1,kernel_size=3,max_len=200):         super().__init__()\n        self.hidden_size = hidden_size \n        self.bs = bs\n    self.e = nn.Embedding(n_vocab,hidden_size)\n    self.cnn = nn.Conv1d(max_len,hidden_size,kernel_size) \n    self.avg = nn.AdaptiveAvgPool1d(10)\n        self.fc = nn.Linear(1000,n_cat)\n        self.softmax = nn.LogSoftmax(dim=-1) \n\n    def forward(self,inp):\n        bs = inp.size()[0] \n        if bs != self.bs:\n            self.bs = bs \n        e_out = self.e(inp)\n        cnn_o = self.cnn(e_out) \n        cnn_avg = self.avg(cnn_o)\n        cnn_avg = cnn_avg.view(self.bs,-1)\n        fc = F.dropout(self.fc(cnn_avg),p=0.5) \n        return self.softmax(fc)\n```", "```py\ntrain_losses , train_accuracy = [],[] \nvalidation_losses , validation_accuracy = [],[]\n\nfor epoch in range(1,5): \n\n    epoch_loss, epoch_accuracy =\nfit(epoch,model,train_iter,phase='training')\n    validation_epoch_loss , validation_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    validation_losses.append(validation_epoch_loss)\n    validation_accuracy.append(validation_epoch_accuracy)\n```", "```py\ntraining loss is 0.59 and training accuracy is 16724/25000 66.9\nvalidation loss is 0.45 and validation accuracy is 19687/25000 78.75\ntraining loss is 0.38 and training accuracy is 20876/25000 83.5\nvalidation loss is 0.4 and validation accuracy is 20618/25000 82.47\ntraining loss is 0.28 and training accuracy is 22109/25000 88.44\nvalidation loss is 0.41 and validation accuracy is 20713/25000 82.85\ntraining loss is 0.22 and training accuracy is 22820/25000 91.28\nvalidation loss is 0.44 and validation accuracy is 20641/25000 82.56\n```", "```py\nimport torch\nfrom torch.nn import functional as F\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ngtp2model = GPT2LMHeadModel.from_pretrained('gpt2')\n```", "```py\ninput_text = tokenizer.encode('We like unicorns because they')\ninput, past = torch.tensor([input_text]), None\nfor _ in range(25):\n    logits, past = gtp2model(input, past=past)\n    input = torch.multinomial(F.softmax(logits[:, -1]), 1)\n    input_text.append(input.item())\n```"]