- en: Chapter 2\. Getting Started with OpenAI API
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章。开始使用OpenAI API
- en: Even though GPT-3 is the most sophisticated and complex language model in the
    world, its capabilities are abstracted as a simple "text-in-text-out" interface
    to end users. This chapter will get you started with using that interface, Playground,
    and cover the technical nuances of the OpenAI API because it is always the details
    that reveal the true gems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-3是世界上最复杂和复杂的语言模型，但其功能被抽象为简单的“文本输入-文本输出”接口提供给最终用户。本章将帮助您开始使用该接口、Playground，并深入涵盖OpenAI
    API的技术细微差别，因为细节总是揭示真正的宝石。
- en: To work through this chapter, you must sign up for an OpenAI account at [https://beta.openai.com/signup](https://beta.openai.com/signup).
    If you haven’t done that, please do so now.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的学习，您必须在[https：//beta.openai.com/signup](https://beta.openai.com/signup)注册OpenAI帐户。如果您还没有这样做，请现在注册。
- en: OpenAI Playground
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Playground
- en: 'Your OpenAI developer account provides access to the API and infinite possibilities.
    We’ll start with Playground, a private web-based sandbox environment that allows
    you to experiment with the API, learn how its components work, and access developer
    documentation and the OpenAI community. We will then show you how to build robust
    prompts that generate favorable responses for your application. We’ll finish the
    chapter with examples of GPT-3 performing four NLP tasks: classification, named
    entity recognition (NER), summarization, and text generation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您的OpenAI开发人员帐户提供对API和无限可能性的访问。我们将从Playground开始，这是一个私人基于Web的沙箱环境，让您可以尝试API，了解其组件的工作原理，并访问开发人员文档和OpenAI社区。然后，我们将向您展示如何构建强大的提示，以生成应用程序的良好回应。最后，本章将演示GPT-3执行四项自然语言处理任务的示例：分类、命名实体识别（NER）、摘要和文本生成。
- en: In an interview with Peter Welinder, VP of Product at OpenAI, we asked about
    the key advice on navigating the Playground for first-time users. He told us his
    advice depends on the persona of the user. If the user has a machine learning
    background, Peter encourages them to “start by forgetting the things that they
    already know, and just go to the playground and try to get GPT-3 to do what you
    wanted it to by just asking it”. He suggests users “imagine GPT-3 as a friend
    or a colleague that you're asking to do something. How would you describe the
    task that you want them to do? And then see how GPT-3 responds. And if it doesn't
    respond in the way you want, iterate on your instructions."
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在与OpenAI产品副总裁彼得·韦琳德进行的一次采访中，我们问及了关于首次使用Playground的关键建议。他告诉我们，他的建议取决于用户的角色。如果用户具有机器学习背景，彼得鼓励他们“首先忘记他们已经知道的事情，然后只需前往Playground并尝试让GPT-3做到你要求的事情”。他建议用户“想象GPT-3是你要求做某事的朋友或同事。你会如何描述你想让他们做的任务？然后看看GPT-3如何回应。如果它的回应不符合你的期望，就调整你的指示。”
- en: 'As YouTuber and NLP influencer [Bakz Awan](https://www.youtube.com/user/bakztfuture)
    puts it, “The non-technical people ask: Do I need a degree to use this? Do I need
    to know how to code to use it? Absolutely not. You can use the Playground. You
    don''t need to write a single line of code. You''ll get results instantly. Anybody
    can do this.”'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如YouTube博主和NLP影响者[Bakz Awan](https://www.youtube.com/user/bakztfuture)所说，“非技术人员会问：我需要学位才能使用吗？我需要懂编程才能使用吗？绝对不需要。您可以使用Playground。您无需编写一行代码。您将立即获得结果。任何人都可以做到这一点。”
- en: Before you start using the Playground, we recommend reading OpenAI’s “[Getting
    Started](https://beta.openai.com/docs/developer-quickstart)” guide and the developer
    documentation. You’ll be able to access it with your OpenAI account.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始使用Playground之前，我们建议阅读OpenAI的“[入门指南](https://beta.openai.com/docs/developer-quickstart)”和开发人员文档。您可以使用您的OpenAI帐户访问它。
- en: 'Here are the steps to get started with the Playground:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是开始使用Playground的步骤：
- en: Login at https://openai.com. After you’ve authenticated, navigate to Playground
    from the main menu.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录https://openai.com。身份验证后，从主菜单中转到Playground。
- en: Look at the Playground screen (Figure 2-1).
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看Playground屏幕（图2-1）。
- en: ●        The big text box marked 1 is where you provide text input (prompts).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为1的大文本框是您提供文本输入（提示）的位置。
- en: ●        The box marked 2 on the right is the parameter-setting pane, which
    allows you to tweak the parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为2的右侧框是参数设置窗格，允许您调整参数。
- en: '●        The box marked 3 allows you to load an existing preset: an example
    prompt and Playground settings. You can provide your training prompt or load an
    existing preset.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为3的框允许您加载现有的预设：示例提示和Playground设置。您可以提供您的培训提示或加载现有的预设。
- en: '![](img/image-0-2.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. The Playground Interface, screenshot taken on 10th January 2022
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Select an existing QA preset (marked 3). This will automatically load the training
    prompt along with the associated parameter settings. Click the Generate button
    (marked 4 in Figure 2-1).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API processes your input and provides a response (called a completion) in
    the same text box. It also shows you the number of tokens utilized. A token is
    a numerical representation of words used to determine the pricing of API calls;
    we’ll discuss them later in this chapter.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the bottom of the screen, you have a token count on the right, and on the
    left you have a Generate button (see Figure 2-2).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image-0-3.jpg)Figure 2-2\. Q/A prompt completion along with token count'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Every time you click the Generate button, GPT-3 takes the prompt and completions
    within the text input field (marked as 1) into account and treats them as part
    of your training prompt for the next completion.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the prompt you can see in figure 2-2:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery, or has no clear answer, I will respond with "Unknown."
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Human life expectancy in the United States is 78 years.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Who was the president of the United States in 1955?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Dwight D. Eisenhower was president of the United States in 1955\.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Which party did he belong to?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'A: He belonged to the Republican Party.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is the square root of a banana?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Unknown'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: How does a telescope work?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Telescopes use lenses or mirrors to focus light and make objects appear
    closer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Where were the 1992 Olympics held?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the completion:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The 1992 Olympics were held in Barcelona, Spain.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the basic outline of the Playground, let's get into
    the nitty gritty of prompt engineering and design.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering and Design
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI API radically changed the way we interact with an AI model, stripping
    out layers and layers of complicated programming languages and frameworks. Andrej
    Karpathy, director of AI at Tesla, said jokingly, as soon as GPT-3 was released,
    that programming 3.0 is all about prompt design (the meme he tweeted is in Figure
    2-3). There is a direct relation between the training prompt you provide and the
    completion quality you get. The structure and arrangement of your words heavily
    influence the output. Understanding prompt design is the key to unlocking GPT-3’s
    true potential.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-4.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Meme source unknown; [tweeted by Andrej Karpathy](https://twitter.com/karpathy/status/1273788774422441984/photo/1)
    on June 18, 2020
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'While designing the training prompt, aim for a zero-shot response from the
    model: see if you can get the kind of response you want without priming the model
    with external training examples. If not, move forward by showing it a few examples
    rather than an entire dataset. The standard flow for designing a training prompt
    is to try for zero-shot first, then a few shots, and go for corpus-based fine-tuning
    (described below).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计训练提示时，要以零追问的方式获取模型的响应：看看你是否能在无需向模型提供外部训练例子的情况下获得你想要的响应。如果不能，那么请向模型展示一些例子，而不是整个数据集。设计训练提示的标准流程是首先尝试零追问，然后尝试一些追问，并进行基于语料库的精细调整（如下所述）。
- en: GPT-3 is the first step towards general-purpose artificial intelligence and
    thus has its limitations. It doesn’t know everything and can’t reason on a human
    level, but it’s competent when you know how to talk to it. That’s where the art
    of prompt engineering comes in.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是通往通用人工智能的第一步，因此它有其局限性。它不知道所有事情，也无法像人类一样思考，但当你知道如何与它交流时，它很有能力。这就是提示工程的艺术所在。
- en: 'GPT-3 isn’t a truth-teller but an exceptional storyteller. It takes in the
    text input and attempts to respond with the text it thinks best completes it.
    If you give it a few lines from your favorite fiction novel, it will try to continue
    in the same style. It works by navigating through the context; without proper
    context, it can generate inconsistent responses. Let’s look at an example to understand
    how GPT-3 processes the input prompt and generates the output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3不是一个讲真话的人，而是一个出色的讲故事者。它接受文本输入并尝试以它认为最好的方式作出回应。如果你给它你最喜欢的小说中的几行文字，它会试图以相同的风格继续故事。它通过理解上下文来进行工作；没有合适的上下文，它可能产生不一致的回应。让我们通过一个例子来了解GPT-3如何处理输入提示并生成输出：
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 美国的人类预期寿命是多少？'
- en: 'A:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'A:'
- en: If you provide a prompt like this to GPT-3 without any context, you ask it to
    look for general answers from its universe of training data. It will result in
    generalized and inconsistent responses as the model doesn’t know which part of
    the training data to answer these questions. [[6]](xhtml-0-12.xhtml#aid_54)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向GPT-3提供这样的提示而没有任何上下文，你要求它从其训练数据的范围中寻找一般性的答案。这将导致一般化和不一致的回应，因为模型不知道如何从训练数据的哪一部分中回答这些问题。[[6]](xhtml-0-12.xhtml#aid_54)
- en: On the other hand, providing the proper context will exponentially improve the
    quality of responses. It simply limits the universe of training data that the
    model has to examine to answer a question, resulting in more specific and to-the-point
    responses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，提供正确的上下文将指数级地提高回应的质量。它简单地限制了模型必须检查来回答问题的训练数据的范围，从而产生更具体和点到为止的回应。
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery or has no clear answer, I will respond with "Unknown."
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一个非常聪明的答疑机器人。如果你问我一个根植于事实的问题，我会给你答案。如果你问我一个毫无意义、诡计多端或者没有明确答案的问题，我会回答“未知”。
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 美国的人类预期寿命是多少？'
- en: 'A:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'A:'
- en: You can think of GPT-3 processing the input similar to the human brain. We tend
    to give random responses when somebody asks us questions without proper context.
    It happens because it’s difficult to get to a precise response without proper
    direction or context. The same is the case with GPT-3\. Its universe of training
    data is so big that it makes it difficult to navigate to a correct response without
    any external context or direction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把GPT-3处理输入的方式想象成类似于人脑。在没有合适上下文的情况下，当有人问我们问题时，我们往往会给出随机的回应。这是因为没有合适的指引或上下文，很难给出精确的回应。GPT-3也是一样的情况。它的训练数据范围非常大，这使得它在没有外部上下文或指引的情况下很难导航到正确的响应。
- en: 'LLMs like GPT-3 can creatively write and answer factual questions in the right
    context. Here is our five-step formula for creating efficient and effective training
    prompts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3这样的语言模型能够在正确的上下文中创造性地写作和回答事实性问题。以下是我们创建高效有效的训练提示的五步公式：
- en: 1\. Define the problem you are trying to solve and what kind of NLP task it
    is, such as classification, Q&A, text generation, or creative writing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 确定你要解决的问题以及这是什么类型的自然语言处理任务，比如分类、问答、文本生成或创意写作。
- en: 2\. Ask yourself if there is a way to get a zero-shot solution. If you need
    external examples to prime the model for your use case, think hard.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 问问自己是否有零追问解决方案。如果你需要外部示例来为你的使用案例引导模型，就要认真思考。
- en: '3\. Now think of how you might encounter the problem in a textual fashion given
    the “text-in, text-out” interface of GPT-3\. Think about all the possible scenarios
    to represent your problem in textual form. For example, you want to build an ad
    copy assistant that can generate creative copy by looking at product name and
    description. To frame this goal in the “text-in, text-out” format, you can define
    the input as the product name and description and the output as the ad copy:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Betty’s Bikes, for price-sensitive shoppers'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: Low prices and huge selection. Free and fast delivery. Order online
    today!'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 4\. If you do end up using external examples, use as few as possible and try
    to incorporate diversity, capturing all the representations to avoid essentially
    overfitting the model or skewing the predictions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: These steps will act as a standard framework whenever you create a training
    prompt from scratch. Before you can build an end-to-end solution for your data
    problems, you need to understand more about how the API works. Let's dig deeper
    by looking at its components.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Down OpenAI API
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-5.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Components of API
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1 shows an overview of the components in the OpenAI API.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Component
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Function
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Execution engine
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Determines the language model used for execution
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Response length
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Sets a limit on how much text the API includes in its completion
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Top P
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Temperature controls the randomness of the response, represented as a range
    from 0 to 1.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Top P controls how many random results the model should consider for completion,
    as suggested by
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: the temperature; it determines the scope of randomness.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty and
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty decreases the likelihood that the model will repeat the same
    line verbatim by
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: “punishing” it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty increases the likelihood that it will talk about new topics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Best of
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Lets you specify the number of completions (n) to generate on the server side
    and returns the best of “n” completions
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Stop sequence
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Specifies a set of characters that signals the API to stop generating completions
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Inject start and restart
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: text
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Inject start text allows you to insert text at the beginning of the completion.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Inject restart text allows you to insert text at the end of the completion.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Show probabilities
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Lets you debug the text prompt by showing the probability of tokens that the
    model can generate for a given input.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Components in the OpenAI API
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Here is an overview of the components of GPT-3 API. We will discuss all of these
    components in more detail throughout the chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Execution Engine
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution engine determines the language model used for execution. Choosing
    the right engine is the key to determining your model’s capabilities and getting
    the right output. GPT-3 has four execution engines of varying sizes and capabilities:
    Davinci, Ada, Babbage, and Curie. Davinci is the most powerful and the Playground’s
    default.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎决定了用于执行的语言模型。选择正确的引擎是确定模型能力并获得正确输出的关键。GPT-3 具有四种不同尺寸和能力的执行引擎：达芬奇、艾达、巴贝奇和居里。达芬奇是最强大的，也是
    Playground 的默认引擎。
- en: Response Length
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 响应长度
- en: 'The response length limits how much text the API includes in its completion.
    Because OpenAI charges by the length of text generated per API call (as noted,
    this is translated into tokens or numeric representations of words), response
    length is a crucial parameter for anyone on a budget. A higher response length
    will use more tokens and cost more. For example, suppose you do a classification
    task. In that case, it is not a good idea to set the response text dial to 100:
    the API could generate irrelevant texts and use extra tokens that will incur charges
    on your account. The API supports a maximum of 2048 tokens combined in the prompt
    and completion. So, while using the API you need to be careful that prompt and
    expected completion don’t exceed the maximum response length to avoid abrupt completions.
    If your use case involves large text prompts and completions, the workaround is
    to think of creative ways to solve problems within token limits, like condensing
    your prompt, breaking the text into smaller pieces, chaining multiple requests.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 响应长度限制了 API 在其完成中包含的文本量。因为 OpenAI 根据每次 API 调用生成的文本长度收费（值得注意的是，这被转换为标记或单词的数值表示），响应长度对于预算有限的任何人来说都是至关重要的参数。较高的响应长度将使用更多的标记并且成本更高。例如，假设您做一个分类任务。在这种情况下，将响应文本调节器设置为
    100 不是一个好主意：API 可能会生成无关的文本并使用额外的标记，这将导致您的账户产生费用。 API 支持在提示和完成中最多使用 2048 个标记。因此，在使用
    API 时，您需要注意提示和预期完成不要超过最大响应长度，以避免突然的完成。如果您的使用案例涉及大量文本提示和完成，解决方法是想出创造性的方式来在标记限制内解决问题，例如简化您的提示，将文本拆分为较小的部分，链式发送多个请求。
- en: Temperature and Top P
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和 Top P
- en: The temperature dial controls the creativity of the response, represented as
    a range from 0 to 1\. A lower temperature value means the API will predict the
    first thing the model sees, resulting in the correct text, but rather dull, with
    minor variation. On the other hand a higher temperature value means the model
    evaluates possible responses that could fit into the context before predicting
    the result. The generated text will be more diverse, but there is a higher possibility
    of grammar mistakes and the generation of nonsense.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 温度调节器控制响应的创造力，表示为从 0 到 1 的范围。较低的温度值意味着 API 将预测模型所见的第一件事情，导致正确文本，但相当乏味，变化微小。另一方面，较高的温度值意味着模型在预测结果之前评估可能符合上下文的响应。生成的文本将更加多样化，但有更高的语法错误和无意义生成的可能性。
- en: 'Top P controls how many random results the model should consider for completion,
    as suggested by the temperature dial; it determines the scope of randomness. Top
    P’s range is from 0 to 1\. A value close to zero means the random responses will
    be limited to a certain fraction: for example, if the value is 0.1, then only
    10% of the random responses will be considered for completion. This makes the
    engine deterministic, which means that it will always generate the same output
    for a given input text. If the value is set to 1, the API will consider all responses
    for completion, taking risks and coming up with creative responses. A lower value
    limits creativity; a higher value expands horizons.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 控制模型在完成时应考虑多少个随机结果，如温度调节器所示；它确定了随机性的范围。 Top P 的范围从 0 到 1。接近零的值意味着随机响应将受限于某个比例：例如，如果值为
    0.1，则只有 10% 的随机响应将被视为完成。这使得引擎具有确定性，这意味着对于给定的输入文本，它将始终生成相同的输出。如果值设置为 1，则 API 将考虑所有响应以进行完成，承担风险并提出创新性的响应。较低的值限制了创造力；较高的值扩展了视野。
- en: 'Temperature and Top P have a very significant effect on output. It can sometimes
    be confusing to get your head around when and how to use them to get the correct
    output. The two are correlated: changing the value of one will affect the other.
    So by setting Top P to 1, you can allow the model to unleash its creativity by
    exploring the entire spectrum of responses and control the randomness by using
    the temperature dial.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和Top P对输出具有非常显著的影响。有时，要弄清楚何时以及如何使用它们以获得正确的输出会令人困惑。两者是相关的：更改一个值将影响另一个值。因此，通过将Top
    P设置为1，您可以允许模型通过探索完整的响应谱来释放其创造力，并通过使用温度旋钮来控制随机性。
- en: 'TIP: We always advise changing either Top P or temperature and keeping the
    other dial set at 1\.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：我们建议始终更改Top P或温度，并将另一个旋钮保持在1的位置。
- en: Large language models rely on probabilistic approaches rather than conventional
    logic. Depending on how you set the model's parameters, they can generate various
    responses for the same input. The model tries to find the best probabilistic match
    within the universe of data it has been trained on instead of looking for a perfect
    solution every time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型依赖概率方法而不是传统逻辑。根据您设置模型的参数方式，它们可以为相同的输入生成各种响应。模型试图在其被训练的数据宇宙中找到最佳的概率匹配，而不是每次寻求完美的解决方案。
- en: 'As we mentioned in Chapter 1, GPT-3’s universe of training data is vast, consisting
    of a variety of publicly available books, internet forums, and Wikipedia articles
    specially curated by OpenAI, allowing it to generate a wide array of completions
    for a given prompt. That’s where temperature and Top P, sometimes called the “creativity
    dials,” come in: you can tune them to generate more natural or abstract responses
    with an element of playful creativity.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章中提到的，GPT-3的大量训练数据宇宙包括各种公开可用的书籍，互联网论坛和由OpenAI专门策划的维基百科文章，使其能够根据提示生成各种完成。这就是温度和Top
    P，有时被称为“创造力旋钮”的地方：您可以调整它们以产生更自然或抽象的响应，并带有一些俏皮的创造力。
- en: 'Let’s say you will use GPT-3 to create names for your startup. You can set
    the temperature dial higher to get the most creative response. When we spent days
    and nights trying to come up with the perfect name for our startup, we dialed
    the temperature. GPT-3 came to the rescue and helped us to arrive at a name we
    love: Kairos Data Labs.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你要使用GPT-3为你的创业公司创建名字。你可以将温度设置更高以获得最有创意的回应。当我们日日夜夜地努力想出我们创业公司的完美名称时，我们拨动了温度。GPT-3挺身而出，帮助我们找到了一款我们喜欢的名字：Kairos
    Data Labs。
- en: 'On other occasions, your task might require little to no creativity: classification
    and question-answering tasks, for example. For these, keep the temperature lower.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，你的任务可能需要很少或没有创造力：例如分类和问答任务。对于这些任务，要将温度设置较低。
- en: Let’s look at a simple classification example that categorizes companies into
    general buckets or categories based on their names.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的分类例子，根据他们的名字将公司分为常规的类别或类别。
- en: '![](img/image-0-6.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-6.jpg)'
- en: Figure 2-5\. Temperature component
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-5. 温度组件
- en: As you can see in Figure 2-5, we have again used temperature to control the
    degree of randomness. You can also do this by changing Top P while keeping the
    temperature dial set to 1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图2-5中所看到的，我们再次使用温度来控制随机程度。您也可以通过更改Top P来实现此目的，同时将温度旋钮设置为1。
- en: Frequency and Presence Penalties
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 频率和存在惩罚
- en: Like the temperature and Top P dials, the frequency penalty, and presence penalty
    dials consider text prompts (the previous completion plus the new input) instead
    of internal model parameters when deciding on output. The existing text thus influences
    the new completions. The frequency penalty decreases the likelihood of the model
    repeating the same line verbatim by “punishing” it. The presence penalty increases
    the likelihood that it will talk about new topics.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就像温度和Top P旋钮一样，频率惩罚和存在惩罚旋钮考虑文本提示（先前完成加上新的输入）而不是内部模型参数来决定输出。现有文本因此会影响新的完成。频率惩罚通过“惩罚”来减少模型重复相同行的可能性。存在惩罚增加了它会谈论新主题的可能性。
- en: These come in handy when preventing the exact completion text from being repeated
    across multiple completions. Although these dials are similar, there is one crucial
    distinction. The frequency penalty is applied if the suggested text output is
    repeated (for example, the model used the exact token in previous completions
    or during the same session) and the model chooses an old output over a new one.
    The presence penalty is applied if a token is present in a given text at all.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这在防止完全完成文本在多个完成中重复出现时非常有用。虽然这些刻度类似，但有一个关键区别。如果建议的文本输出重复（例如，模型在以前的完成中使用了相同的标记或在同一会话期间选择旧的输出），则应用频率惩罚。如果一个标记在给定文本中存在，则应用存在惩罚。
- en: Best of
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳
- en: GPT-3 uses the best of feature to generate multiple completions on the server
    side, evaluate them behind the scenes, and then provide you with the best probabilistic
    result. Using the “best of ” parameter, you can specify the number of completions
    (n) to generate on the server side. The model will return the best of n completions
    (the one with the lowest log probability per token).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 使用最佳功能在服务器端生成多个完成，并在幕后评估它们，然后为您提供最佳的概率结果。使用“最佳”参数，您可以指定在服务器端生成的完成次数（n）。模型将返回
    n 次完成中的最佳完成（每个标记的对数概率最低的那个）。
- en: 'This enables you to evaluate multiple prompt completions in a single API call
    rather than calling the API repeatedly to check the quality of different completions
    for the same input. However, using “best of ” is expensive: it costs n times the
    tokens in the prompt. For example, if you set the “best of ” value to 2, then
    you will be charged double the tokens present in the input prompt because on the
    backend the API will generate two completions and show you the best one.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您可以在单个 API 调用中评估多个提示完成，而不是重复调用 API 来检查相同输入的不同完成的质量。但是，“最佳”使用是昂贵的：它的成本是输入提示中的标记的
    n 倍。例如，如果您将“最佳”值设置为 2，那么您将被收取输入提示中标记数量的两倍，因为在后台，API 将生成两个完成，并显示给您最佳的一个。
- en: “Best of ” can range from 1 to 20 depending on your use case. If your use case
    serves clients for whom the quality of output needs to be consistent, then you
    can set the “best of ” value to a higher number. On the other hand, if your use
    case involves too many API invocations, then it makes sense to have a lower “best
    of ” value to avoid unnecessary latency and costs. We advise keeping response
    length minimal while generating multiple prompts using the “best of ” parameter
    to avoid additional charges.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳”值可以从 1 到 20 不等，具体取决于您的用例。如果您的用例为客户提供需要一致质量的输出，那么可以将“最佳”值设为更高的数字。另一方面，如果您的用例涉及太多的
    API 调用，那么将“最佳”值设为更低的数字可以避免不必要的延迟和成本。我们建议在使用“最佳”参数生成多个提示时，尽量保持响应长度最小，以避免额外收费。
- en: Stop Sequence
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列
- en: A stop sequence is a set of characters that signal the API to stop generating
    completions. It helps avoid unnecessary tokens, an essential cost-saving feature
    for regular users.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列是一组字符，用于通知 API 停止生成完成。这有助于避免不必要的标记，是常规用户不可或缺的节省成本的功能。
- en: You can provide up to 4 sequences for the API to stop generating further tokens.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为 API 提供最多 4 个序列，以停止生成进一步的标记。
- en: 'Let''s look at the example language translation task in Figure 2-6 to understand
    how the stop sequence works. In this example, English phrases are translated into
    French. We use the restart sequence “English:” as a stop sequence: whenever the
    API encounters that phrase, it will stop generating new tokens.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看图 2-6 中的示例语言翻译任务，以了解停止序列的工作原理。在这个示例中，英文短语被翻译成法语。我们使用重新启动序列“English:”作为停止序列：每当
    API 遇到该短语时，它将停止生成新的标记。
- en: '![](img/image-0-7.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-7.jpg)'
- en: Figure 2-6\. Stop Sequence component
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-6\. 停止序列组件
- en: Inject Start Text and Inject Restart Text
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注入开始文本和注入重新启动文本
- en: The inject start text and inject restart text parameters allow you to insert
    text at the beginning or end of the completion, respectively. You can use them
    to keep a desired pattern going. Often, these settings work in tandem with the
    stop sequence, as in our example. The prompt has a pattern where an English sentence
    is provided with the prefix “English:” (the restart text), and the translated
    output is generated with the prefix “French:” (the start text). As a result, anyone
    can easily distinguish between the two and create a training prompt that both
    the model and the user can clearly comprehend.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we run the model for such prompts, the model automatically injects
    a start text “French:” before the output and a restart text “English:” before
    the next input, so that this pattern can be sustained.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Show Probabilities
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The show probabilities parameter is at the bottom of the Playground settings
    pane. In conventional software engineering, developers use a debugger to troubleshoot
    (debug) a piece of code. You can use the show probabilities parameter to debug
    your text prompt. Whenever you select this parameter, you will see highlighted
    text. Hovering over it with the cursor will show a list of tokens that the model
    can generate for the particular input specified, with their respective probabilities.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this parameter to examine your options. In addition, it can make
    it easier to see alternatives that might be more effective. The “show probabilities”
    parameter has three settings:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Most Likely
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens most likely to be considered for completion in decreasing order
    of probability.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Least Likely
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens least likely to be considered for completion in decreasing
    order of probability.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Full Spectrum
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Shows the entire universe of tokens that could be selected for completion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at this parameter in the context of a simple prompt. We want to
    start the output sentence with a simple, well-known phrase: “Once upon a time”.
    We provide the API with the prompt “Once upon a” and then check the Most Likely
    option in the show probabilities tab.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: As Figure 2-7 shows, it generates “time” as the response. Because we have set
    the “show probabilities” parameter to “Most Likely,” the API indicates the response
    and a list of possible options along with their probabilities.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve had an overview, let’s look at these components in more detail.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-8.jpg)Figure 2-7\. Show Probabilities component showing the
    most likely tokens'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Execution Engines
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: As noted in Figure 2-7, the OpenAI API offers four different execution engines,
    differentiated by parameters and performance capabilities. Execution engines power
    OpenAI API. They serve as “autoML” solutions, providing automated ML methods and
    processes to make machine learning available for non-experts. They are easy to
    configure and adapt to a given dataset and task.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The four primary execution engines were named after famous scientists in alphabetical
    order: Ada (named after Ada Lovelace), Babbage (named after Charles Babbage),
    Curie (named after Madame Marie Curie), and Davinci (named after Leonardo da Vinci).
    Let’s deep-dive into each of these execution engines to understand when to use
    which execution engine when working with GPT-3:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: DaVinci
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Da Vinci is the largest execution engine and the default when you first open
    the Playground. It can do anything the other engines can, often with fewer instructions
    and better outcomes. However, the trade-off is that it costs more per API call
    and is slower than other engines. You might want to use other engines to optimize
    cost and runtimes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: We recommend starting with Davinci because of its superior capabilities
    when testing new ideas and prompts. Experimenting with Davinci is a great way
    to determine what the API can do. You can slowly move down the ladder to optimize
    budgets and runtimes as you become comfortable with your problem statement. Once
    you have an idea of what you want to accomplish, you can either stay with Davinci
    (if cost and speed are not a concern) or move on to Curie or other less costly
    engines and try to optimize the output around its capabilities. You can use [OpenAI’s
    Comparison Tool](https://gpttools.com/comparisontool) to generate an Excel spreadsheet
    that compares engines’ outputs, settings, and response times.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Davinci should be your first choice for tasks that require understanding the
    content, like summarizing meeting notes or generating creative ad copy. It’s great
    at solving logic problems and explaining the motives of fictional characters.
    It can write a story. Davinci has also been able to solve some of the most challenging
    AI problems involving cause and effect.[[7]](xhtml-0-12.xhtml#aid_85)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Curie
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Curie aims to find an optimal balance between power and speed that is very important
    for performing high-frequency tasks like classification on an immense scale or
    putting a model into production.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Curie is also quite good at answering questions, performing Q&As, and serving
    as a general-purpose chatbot. For instance, if you build a customer-support chatbot,
    you might choose Curie to serve high-volume requests faster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: While Davinci is stronger at analyzing complicated texts, Curie can perform
    with low latency and lightning-fast speed. It is always sensible to figure out
    your use case and do a cost-benefit analysis before choosing Davinci over Curie
    in production.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Babbage
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Babbage is faster than Cuire but not capable of performing tasks that involve
    understanding complex intent. However, it is quite capable and is preferable when
    it comes to semantic search rankings and analyzing how well documents match up
    with search queries. It’s less expensive than Curie and Davinci and is preferred
    for simple problems involving frequent API calls.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Ada
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Ada is the fastest and least expensive of all the engines available. It can
    perform simple tasks that do not require a nuanced understanding of context, like
    parsing text, correcting grammar, or simple classification. It is often possible
    to improve Ada's performance by providing more context with the input. Ada can
    be the preferred model of choice for use cases involving frequent API invocations,
    as it can achieve results similar to bigger models with suitable settings. The
    more you experiment with the API parameters, the more understanding you will gain
    of what settings work for your use case.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Instruct Series
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Based on the four primary models, OpenAI has launched a series of [InstructGPT](https://openai.com/blog/instruction-following/)
    models that are better at understanding the instructions and following them while
    being less toxic and more truthful than the original GPT-3\. They have been developed
    using techniques coming from OpenAI’s alignment research. These models are trained
    with humans in the loop and are now deployed as the default language models on
    OpenAI API.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-9.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. Output generated by InstructGPT Davinci
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Output generated by GPT Davinci
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Our input:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: “Write a story about AI for a 5-year-old.”
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'InstructGPT output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: “Robot was very happy. He had just been born, and he was already the smartest
    robot in the world. He could do anything that any other robot could do, but he
    could also think for himself.”
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: “This project is an attempt to make an AI that can produce fictional stories.
    It is a work in progress. Fictionality The aim of this project is to create an
    AI that can create fictional stories,”
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: To make the process of building prompts efficient, OpenAI decided to publicly
    launch text-davinci-001, text-curie-001, text-babbage-001, and text-ada-001\.
    With clear instructions, these models can produce better results than their base
    counterparts and are now the [default models of the API](https://help.openai.com/en/articles/5832130-what-s-changed-with-engine-names-and-best-practices).
    This series is important in bridging the gap between how humans think and models
    operate.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: We recommend using this model series as your default for all text-related
    tasks. The base version of GPT-3 models are available as davinci, curie, babbage,
    and ada and are meant to be used with the fine-tuning, search, classification,
    and answers endpoints.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The Playground is a graphical web interface that calls the OpenAI API behind
    the scenes, but there are several other ways to call the API. To do this, you
    will need to get familiar with its endpoints: the remote APIs that communicate
    back and forth when they are called. This section will familiarize you with the
    functionality and usage of six API endpoints.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: List Engines
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The list engine endpoint, also known as the “metadata API,” provides a list
    of available engines and specific metadata associated with each engine, such as
    owner and availability. To access it, you can hit the following URI with the HTTP
    GET method without passing any request parameters:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: GET https://api.openai.com/v1/engines
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve Engines
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'When you provide an engine name to the retrieve engine endpoint, it returns
    detailed metadata information about that engine. To access, hit the following
    URI with the HTTP GET method without passing any request parameters:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: GET [https://api.openai.com/v1/engines/{engine_id](https://api.openai.com/v1/engines/%7Bengine_id)}
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Completions
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Completions is GPT-3’s most famous and widely used endpoint. It simply takes
    in the text prompt as input and returns the completed response as output. It uses
    the HTTP POST method and requires an engine ID as part of the URI path. As part
    of the HTTP Body, the Completions endpoint accepts several additional parameters
    discussed in the previous section. Its signature is:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: POST https://api.openai.com/v1/engines/{engine_id}/completions
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Search
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The semantic search endpoint enables you to provide a query in natural language
    to search a set of documents, which can be words, sentences, paragraphs, or even
    longer texts. It will score and rank the documents based on how semantically related
    they are to the input query. For example, if you provide the documents ["school",
    "hospital", "park"] and query "the doctor", you’ll get a different similarity
    score for each document.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The similarity score is a positive score that usually ranges from 0 to 300 (but
    can sometimes go higher), where a score above 200 usually indicates that the document
    is semantically similar to the query. The higher the similarity score, the more
    semantically similar the document is to the query (in this example, "hospital"
    will be most similar to "the doctor"). You can provide up to 200 documents as
    part of your request to the API.[[8]](xhtml-0-12.xhtml#aid_48)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the signature for the semantic search endpoint:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: POST [https://api.openai.com/v1/engines/{engine_id}/search](https://api.openai.com/v1/engines/%7Bengine_id%7D/search)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Files
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The files endpoint can be used across different endpoints like answers, classification,
    and semantic search. It is used to upload documents or files to the OpenAI storage,
    which is accessible through the API features. The same endpoint can be used with
    different signatures to perform the following tasks:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: List Files
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: It simply returns the list of files that belong to the user's organization or
    that are linked to a particular user’s account. It's an HTTP GET call that doesn’t
    require any parameters to be passed with the request.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: GET https://api.openai.com/v1/files
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Upload Files
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: It is used to upload a file that contains documents to be used across various
    endpoints. It uploads the documents to the already allocated internal space by
    OpenAI for the users' organization. It is an HTTP POST call that requires the
    file path to be added to the API request.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: ​POST https://api.openai.com/v1/files
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve File
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'It returns the information about a specific file by just providing the file
    id as the request parameter. Following is the signature for the Retrieve endpoint:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: ​GET https://api.openai.com/v1/files/{file_id}
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Delete File
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'It deletes the specific file by providing the file as the request parameter.
    Following is the signature for the Delete endpoint:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: ​DELETE https://api.openai.com/v1/files/{file_id}
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Another experimental endpoint of the API is Embeddings. Embeddings are the core
    of any machine learning model and allow to capture of semantics from the text
    by converting it into high-dimensional vectors. Currently, developers tend to
    use open-source models like the BERT series to create embeddings for their data
    that can be used for a variety of tasks like recommendation, topic modeling, semantic
    search, etc.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI realized that GPT-3 holds great potential to power embedding-driven
    use cases and come up with state-of-the-art results. Generating embeddings for
    the input data is very straightforward and wrapped in the form of an API call.
    To create an embedding vector representing the input text, you can use the following
    API signature:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: POST [https://api.openai.com/v1/engines/{engine_id}/embeddings](https://api.openai.com/v1/engines/%7Bengine_id%7D/embeddings)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: To invoke the embeddings endpoint, you can choose the type of engine depending
    on your use case by referring to the [embeddings document](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings).
    Each engine has its specific dimensions of embedding, with Davinci being the biggest
    and Ada being the smallest. All the embedding engines are derived from the four
    base models and classified based on the use cases to allow efficient and cost-friendly
    usage.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Customizing GPT-3
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s research paper “[Process for Adapting Language Models to Society (PALMS)
    with Values-Targeted Datasets](https://cdn.openai.com/palms.pdf)” by Irene Solaiman
    and Christy Dennison (June 2021) led the company to launch a first-of-its-kind
    fine-tuning endpoint that allows you to get more out of GPT-3 than was previously
    possible by customizing the model for your particular use case. Customizing GPT-3
    improves the performance of any natural language task GPT-3 is capable of performing
    for your specific use case.[[9]](xhtml-0-12.xhtml#aid_36)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Let us explain how that works first.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI pre-trained GPT-3 on a [specially prepared dataset](https://arxiv.org/pdf/2005.14165.pdf)
    in a semi-supervised fashion. When given a prompt with just a few examples, it
    can often intuit what task you are trying to perform and generate a plausible
    completion. This is called "few-shot learning," as you learned in chapter 1.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI以半监督方式在一个 [特别准备的数据集](https://arxiv.org/pdf/2005.14165.pdf) 上对GPT-3进行了预训练。当只提供了几个示例的提示时，它通常能直觉到你试图执行的任务，并生成一个合理的完成。这被称为"少样本学习"，正如你在第1章中学到的。
- en: By fine-tuning GPT-3 on their own data, users can create a custom version of
    the model that is tailored to their specific project needs. This customization
    allows GPT-3 to be more reliable and efficient in a variety of use cases. Fine-tuning
    the model involves adjusting it so that it consistently performs in the desired
    way. This can be done using an existing dataset of any size or by incrementally
    adding data based on user feedback.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在自己的数据上微调GPT-3，用户可以创建一个根据其特定项目需求定制的模型版本。这种定制化使GPT-3在各种用例中更可靠和高效。微调模型涉及调整它以便始终以期望的方式执行。这可以使用任何大小的现有数据集完成，也可以根据用户反馈逐步添加数据完成。
- en: The process of fine-tuning will focus the knowledge and capabilities of the
    model on the contents and semantics of the data used for training, which will
    in turn limit the range of topics and creativity it can generate. This can be
    useful for downstream tasks that require specialized knowledge, such as classifying
    internal documents or dealing with internal jargon. Fine-tuning the model also
    focuses its attention on the specific data used for training, limiting its overall
    knowledge base.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型的过程将专注于使模型的知识和能力集中于用于训练的数据的内容和语义，这将限制它可以生成的主题范围和创造力。对于需要专业知识的后续任务，如分类内部文件或处理内部行话，这可能很有用。对模型进行微调还将注意力集中在用于训练的具体数据上，限制其整体知识库。
- en: Once a model has been fine-tuned, it no longer requires examples in the prompt,
    which saves costs and improves the speed and quality of the output. Customizing
    GPT-3 in this way appears to be more effective than using prompt design alone,
    as it allows for the use of more training examples.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，就不再需要提示中的示例，这可以节省成本，并提高输出的速度和质量。以这种方式定制化GPT-3似乎比仅使用提示设计更有效，因为它可以使用更多的训练示例。
- en: With less than 100 examples you can start seeing the benefits of fine-tuning
    the model. Its performance continues to improve as you add more data. In the PALMS
    research paper, OpenAI showed how fine-tuning with less than 100 examples can
    improve the model’s performance on a number of tasks. They’ve also found that
    doubling the number of examples tends to improve the quality of the output linearly.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 少于100个示例，你就可以开始看到微调模型的好处。随着添加更多数据，其性能将继续提高。在PALMS研究论文中，OpenAI展示了如何使用少于100个示例进行微调可以提高模型在许多任务上的性能。他们还发现，将示例数量翻倍往往会线性提高输出的质量。
- en: Customizing GPT-3 improves the reliability of its outputs and gives more consistent
    results that you can take to production use cases. Existing OpenAI API customers
    found that customizing GPT-3 can dramatically reduce the frequency of unreliable
    outputs - there is a growing group of customers that can vouch for it with their
    performance numbers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定制化GPT-3可以提高其输出的可靠性，并提供更一致的结果，可应用于生产用例。现有的OpenAI API客户发现，定制化GPT-3可以大大减少不可靠输出的频率
    - 有越来越多的客户可以凭借他们的性能数据为此作证。
- en: Apps Powered by Customized GPT-3 Models
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序由定制化GPT-3模型提供支持
- en: Keeper Tax helps independent contractors and freelancers with their taxes. It
    uses various models to extract text and classify transactions and then identifies
    easy-to-miss tax write-offs to help customers file their taxes directly from the
    app. By customizing GPT-3, Keeper Tax experienced an increasing accuracy from
    85% to 93%. And it continuously improves thanks to adding 500 new training examples
    to their model once a week, which is leading to about a 1% accuracy improvement
    a week.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Keeper Tax帮助独立承包商和自由职业者处理他们的税务。它使用各种模型来提取文本并分类交易，然后识别易被忽视的税务减免，从而帮助客户直接从应用程序中申报税款。通过定制化GPT-3，Keeper
    Tax的准确度由85%提高到93%。并且由于每周向他们的模型添加500个新的训练示例，所以它不断改进，导致每周约1%的准确度提高。
- en: Viable helps companies get insights from their customer feedback. By customizing
    GPT-3, Viable is able to transform massive amounts of unstructured data into readable
    natural language reports. Customizing GPT-3 has increased the reliability of Viable’s
    reports. By using a customized version of GPT-3, accuracy in summarizing customer
    feedback has improved from 66% to 90%. For an in-depth insight into Viable’s journey,
    refer to our interview with Viable’s CEO in chapter 4\.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Sana Labs is a global leader in the development and application of AI to learning.
    Their platform powers personalized learning experiences for businesses by leveraging
    the latest ML breakthroughs to personalize content. By customizing GPT-3 with
    their data, Sana’s question and content generation went from grammatically correct
    but general responses to highly accurate ones. This yielded a 60% improvement,
    enabling more personalized experiences for their users.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Elicit is an AI research assistant that helps directly answer research questions
    using findings from academic papers. The assistant finds the most relevant abstracts
    from a large corpus of research papers, then applies GPT-3 to generate the claim
    that the paper makes about the question. A custom version of GPT-3 outperformed
    prompt design and led to improvement in three areas: results were 24% easier to
    understand, 17% more accurate, and 33% better overall.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: How to customize GPT-3 for your application
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: To begin, simply use the OpenAI command line tool with a file of your choosing.
    Your personalized version will begin training and will be accessible through our
    API right away.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'At a very high level, customizing GPT-3 for your application involves the following
    three steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ●        Prepare new training data and upload it to the OpenAI server
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ●        Fine-tune the existing models with the new training data
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ●        Use the fine-tuned model
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Prepare and upload training data
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Training data is what the model takes in as input for fine-tuning. Your training
    data must be a JSONL document, where each line is a prompt-completion pair corresponding
    to a training example. For model fine-tuning, you can provide an arbitrary number
    of examples where it is highly recommended to create a value-targeted dataset
    to provide the model with quality data and wide representation. Fine-tuning improves
    performance with more examples, so the more examples you provide, the better the
    outcome.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Your JSONL document should look something like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: …
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Where the prompt text should include the exact prompt text you want to complete,
    and the ideal generated text should include an example of desired completion text
    that you want GPT-3 to generate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use OpenAI''s CLI data preparation tool to easily convert your data
    into this file format. CLI data preparation tool accepts files in different formats,
    with the only requirement that they contain a prompt and a completion column/key.
    You can pass a CSV, TSV, XLSX, JSON, or JSONL file, and it will save the output
    into a JSONL file ready for fine-tuning. To do that, you can use the following
    command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: openai tools fine_tunes.prepare_data -f <LOCAL_FILE>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Where LOCAL_FILE is the file you prepared for conversion.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Train a new fine-tuned model
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you prepare your training data as described above, you can move on to
    the fine-tuning job with the help of the OpenAI CLI. For that, you need the following
    command:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Where BASE_MODEL is the name of the base model you''re starting from (ada,
    babbage, or curie). Running this command does several things:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: ●        Uploads the file using the files endpoint (as discussed earlier in
    this chapter);
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: ●        Fine-tunes the model using the request configuration from the command;
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: ●        Streams the event logs until the fine-tuning job is completed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Log streaming is helpful to understand what’s happening in real-time and to
    respond to any incidents/failures as and when they happen. The streaming may take
    from minutes to hours depending on the number of jobs in the queue and the size
    of your dataset.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Using the fine-tuned model
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is successfully fine-tuned, you can start using it! You can now
    specify this model as a parameter to the Completion Endpoint, and make requests
    to it using the Playground.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: After the fine-tuning job completes, it may take several minutes for your
    model to become ready to handle requests. If completion requests to your model
    time out, it is likely because your model is still being loaded. If this happens,
    try again in a few minutes.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start making requests by passing the model name as the model parameter
    of a completion request using the following command:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Where FINE_TUNED_MODEL is the name of your model and YOUR_PROMPT is the prompt
    you want to complete in this request.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to use all the Completion Endpoint parameters that were discussed
    in this chapter, like temperature, frequency_penalty, presence_penalty, etc.,
    on these requests to the newly fine-tuned model as well.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: NOTE:No engine is specified on these requests. This is the intended design and
    something that OpenAI plans on standardizing across other API endpoints in the
    future.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: For more information, please refer to OpenAI’s [fine-tuning documentation](https://beta.openai.com/docs/guides/fine-tuning).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into how different prompts consume tokens, let's look more
    closely at what is a token.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: We’ve told you that tokens are numerical representations of words or characters.
    Using tokens as a standard measure, GPT-3 can handle training prompts from a few
    words to entire documents.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: For regular English text, 1 token consists of approximately 4 characters. It
    translates to roughly ¾ of a word, so for 100 tokens there will be approximately
    75 words. As a point of reference, the collected works of Shakespeare consist
    of about 900,000 words, roughly translating to 1.2M tokens.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the latency of API calls, OpenAI imposes a limit of 2,048 tokens
    (approximately ~1,500 words) for prompts and completions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: To further understand how the tokens are calculated/consumed in the context
    of GPT-3 and to stay within the limits set by the API let us walk you through
    the following ways you can measure the token count.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: In the Playground, as you enter text into the interface, you can see the token
    count update in real-time in the footer at the bottom right. It displays the number
    of tokens that will be consumed by the text prompt after hitting the submit button.
    You can use it to monitor your token consumption every time you interact with
    the Playground (see Figure 2-10).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-11.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Token Count in the Playground
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: The other way to measure the consumption of tokens is by using the explicit
    GPT-3 Tokenizer tool (see Figure 2-11) that lets you visualize the formation of
    tokens from the word characters. You can interact with the Tokenizer tool via
    a simple text box where you write the prompt text and Tokenizer will show you
    the token and character counts along with the detailed visualization.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-12.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. Tokenizer by OpenAI
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: For integrating the token count metric in your API calls to different endpoints,
    you can patch the logprobs and echo attribute along with the API request to get
    the full list of tokens consumed.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how tokens are priced based on the different
    execution engines.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: In the last section we talked about tokens, which are the smallest fungible
    unit used by OpenAI to determine the pricing for API calls. Tokens allow greater
    flexibility than measuring the number of words or sentences used in the training
    prompt and due to the token’s sheer granularity tokens can be easily processed
    and used to measure the pricing for a wide range of training prompts.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Every time you call the API from either the Playground or programmatically,
    behind the scenes the API calculates the number of tokens used in the training
    prompt along with the generated completion and charges each call on the basis
    of the total number of tokens used.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI generally charges a flat fee per 1,000 tokens, with the fee depending
    on the execution engine used in the API call. Davinci is the most powerful and
    expensive, while Curie, Babbage, and Ada are cheaper and faster.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2 shows the pricing for the various API engines at the time this chapter
    was written (December 2022).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Model
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Price Per 1k Tokens
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Davinci (most powerful)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: $0.0200
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Curie
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: $0.0020
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Babbage
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: $0.0005
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Ada (fastest)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: $0.0004
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Model Pricing
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: The company works on the cloud pricing model of “pay as you go”. For updated
    pricing please check the [online pricing schedule](https://beta.openai.com/pricing).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Instead of monitoring the tokens for each of the API calls, OpenAI provides
    a [reporting dashboard](https://beta.openai.com/account/usage) to monitor daily
    cumulative token usage. Depending on your usage, it may look something like Figure
    2-12.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-13.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. API Usage Dashboard
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 2-12 you can see a bar graph showing the daily token consumption from
    the API usage. The dashboard helps you to monitor the token usage and pricing
    for your organization. This helps you to regulate the API usage and stay within
    your budget. There is also an option to monitor the cumulative usage and breakdown
    of token count per API call. This should give you enough flexibility to create
    policies around token consumption and pricing for your organization. Now that
    you understand the ins and outs of the Playground and the API, we will take a
    look at GPT-3’s performance on typical language modeling tasks.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: For beginners who have just started with GPT-3 and find it hard to wrap
    their heads around token consumption. Many users enter too long prompt texts,
    which leads to the overuse of credits, followed by unplanned fees. To avoid this,
    during your initial days try to use the API dashboard to observe the number of
    tokens consumed and see how the length of prompts and completions affect token
    usage. It can help you to prevent the uncontrolled use of credits and keep everything
    within the budget.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3’s Performance on Standard NLP Tasks
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 is a highly advanced and sophisticated successor to the NLP field, built
    and trained using the core NLP approaches and deep neural networks. For any AI-based
    modeling approach, the model performance is evaluated in the following way: First,
    you train the model for a specific task (like classification, Q/A, text generation,
    etc) on training data; then you verify the model performance using the test data
    (unseen data).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, there is a standard set of NLP benchmarks for evaluating the
    performance of NLP models and coming up with a relative model ranking or comparison.
    This comparison, or relative ranking, allows you to pick and choose the best model
    for a specific NLP task (business problem).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will discuss the performance of GPT-3 on some standard NLP
    tasks as seen in Figure 2-13 and compare it with the performance of similar models
    on respective NLP tasks.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-14.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Conventional NLP Tasks
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Text Classification
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: NLP-powered text classification involves using algorithms to automatically analyze
    text and assign it to predefined categories or tags based on its context. This
    process helps to organize and categorize text into relevant groups.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Text classification involves analyzing the text provided as input and assigning
    it a label, score, or another attribute that characterizes the text. Some common
    examples of text classification are sentiment analysis, topic labeling, intent
    detection, etc. You can use a number of approaches to get GTP-3 to classify text,
    again ranging from zero-shot classification (where you don't give any examples
    to the model) to single-shot and few-shot classification (where you show some
    examples to the model).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Classification
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Modern artificial intelligence has long aimed to develop models that can perform
    predictive functions on data it has never seen before. This important research
    area is called zero-shot learning. Similarly, a zero-shot classification is a
    classification task where no prior training and fine-tuning on labeled data is
    required for the model to classify a piece of text.GPT-3 currently produces results
    for unseen data that are either better or at par with state-of-the-art AI models
    fine-tuned for that specific purpose. In order to perform zero-shot classification
    with GPT-3, we must provide it with a compatible prompt. In chapter 2, we will
    discuss prompt engineering.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of a zero-shot classification where the goal is to perform
    a fact-checking analysis to determine if the information included in the tweet
    is correct or incorrect. Figure 2-14 shows a pretty impressive information correctness
    classification result based on a zero-shot example.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-15.jpg)Figure 2-14\. Zero-shot classification example'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is our prompt:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Analyse the tweet in terms of information correctness.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: “More than 50%of global scientists don’t believe in climate change.”'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The tweet is incorrect.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Single-shot and Few Shot Text Classification
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: The other approach to text classification is via fine-tuning an AI model on
    a single or few training examples also known as a single shot or few shots text
    classification. When you provide examples of how to classify text, the model can
    learn information about the object categories based on the samples you provide.
    This is a superset of zero-shot classification that allows you to classify text
    by providing the model with three to four diversified examples. This can be useful
    specifically for downstream use cases, which require some level of context setting.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following example of a few-shot classification. We are asking
    the model to perform a tweet sentiment analysis classification and giving it three
    tweet examples to illustrate each of the possible labels: positive, neutral, and
    negative. As you can see in Figure 2-15, the model equipped with such a detailed
    context based on a few examples, is able to very easily perform the sentiment
    analysis of the next tweet.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: when you recreate prompt examples from the book or create your own, make
    sure to have adequate line spacing in your prompt. An additional line after a
    paragraph can result in a very different outcome, so you want to play with that
    and see what works best for you.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-16.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Few-shot classification example
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Analyse the tweet in terms of its sentiment. Depending on the sentiment, classify
    it as positive, neutral, or negative.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: positive
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Batch Classification
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the few-shot classification with GPT-3, let's dive deeper
    into the batch classification which allows you to classify input samples in batches
    in a single API call instead of just classifying one example per API call. It
    is suitable for applications where you want to classify multiple examples in a
    single go, just like the tweet sentiment analysis task we examined, but analyzing
    a few tweets in a row.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: As with a few shots classification, you want to provide enough context for the
    model to achieve the desired result but in a batch configuration format. Here,
    we define the different categories of tweet sentiment classification using various
    examples in the batch configuration format. Then we ask the model to analyze the
    next batch of tweets.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-17.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Batch classification example (Part-1)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-18.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Batch-classification example (Part-2)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Analyse tweets in terms of their sentiment. Depending on their sentiment, classify
    them as positive, neutral, or negative.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 1\. "I'm seriously worried that super intelligent AI will be disappointed in
    humans."
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 2\. "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 3\. "I think it is neither super likely nor super unlikely that the super intelligent
    AI will emerge one day."
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 4\. "Super intelligent AI is going to be the most exciting discovery in human
    history."
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 5\. "This is the latest report on the state of the AI"
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet sentiment:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 1\. negative
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 2\. positive
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 3\. neutral
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 4\. positive
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 5\. neutral
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 1\. "I can't stand bad techno music"
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 2\. "This is a tweet"
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 3\. "I can't wait for going to the Moon!!!"
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 4\. "AI is super cute ❤️"
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 5\. "Got VERY ANGRY now!!! ��"
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet sentiment:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 1\.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 1\. negative
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 2\. neutral
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 3\. positive
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 4\. positive
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 5\. negative
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model recreated the batch sentiment analysis format and
    classified the tweets successfully. Now let’s move on to see how it performs at
    the Named Entity Recognition tasks.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition (NER) is an information extraction task that involves
    identifying and categorizing named entities mentioned in unstructured text. These
    entities may include people, organizations, locations, dates, quantities, monetary
    values, and percentages. This task is useful for extracting important information
    from text.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: NER helps to make the responses more personalized and relevant but the current
    state-of-the-art approaches require massive amounts of data for training before
    you even start with the prediction. GPT-,3 on the other hand, can work out of
    the box for recognizing general entities like people, places, and organizations
    without humans providing even a single training example.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example we used a davinci-instruct-series version of the model
    that was in beta at the time of writing this book and the model gathers prompts
    to train and improve the future OpenAI API models. We gave it a simple task: to
    extract contact information from an example email. It successfully completed the
    task on the first attempt (Figure 2-18).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-19.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. NER example
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our input:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the name and mailing address from this email:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Shubham,
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: It was great to talk to you the other day!
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: I'm SO looking forward to start working on our book.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Here's my address 1307 Roosevelt Street, San Francisco CA​94107
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Best,
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Sandra Kublik
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Name and mailing address:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Sandra Kublik
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 1307 Roosevelt Street, San Francisco CA 94107
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Text Summarization
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: The goal of text summarization is to create a shortened version of a lengthy
    text while still accurately representing the original content and maintaining
    its overall meaning. This is done by identifying and highlighting the most important
    information in the text.GPT-3 based text summarization aims to transform lengthy
    pieces of texts into their condensed tl;dr[[10]](xhtml-0-12.xhtml#aid_41) versions.
    Such tasks are generally difficult and costly to accomplish manually. With GPT-3,
    it is a matter of a single input and a few seconds!
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: NLP models can be trained to comprehend documents and identify the sections
    that convey important facts and information before producing the required summarized
    texts. However, such models need a large amount of training samples before they
    can learn the context and start summarizing the unseen input.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3's abstractive summarization is the key to solving the problem of information
    extraction. By producing summaries instead of merely extracting key information,
    GPT-3 can provide a more comprehensive and accurate understanding of the text.
    It uses zero shot or few-shot approach towards text summarization making it useful
    for a variety of use cases. With GPT-3 there are multiple ways you can go about
    summarizing the text involving basic summaries, one-line summaries, and grade-level
    summaries depending on your use case. Let’s have a quick walkthrough of these
    approaches.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time the model is able to generate decent results in the form of
    a review summary, but sometimes it can output irrelevant results depending on
    the prior context. To avoid the problem of getting unwanted results, you can set
    the Best of parameter to 3 which will always give you the best of three results
    generated by the API. In the example shown in Figure 2-18, after a few tries and
    minor parameter tweaking, we got decent results.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that when it works, it works, but when it doesn’t, it really
    doesn’t. Luckily, the game’s low points like the horrible binary stealth system
    and the bugs and lack of QoL are either fixable or vastly outweighed by the good
    bits, and the overall tally still ends up being much higher than the average of
    many other games out there. That’s helped a lot by the moment-to-moment gameplay,
    which pays off just as much as the story beats; the movement system lets you climb
    almost any short object, vehicles are weighty to drive, and have fantastic interior
    and exterior designs, and guns feel properly loud and heavy. Its story and narrative
    kept me glued to the screen, and everything – from sex scenes and romance options
    to Keanu Reeves trying to kill you or help you throughout the game – are remarkably
    well done. Cyberpunk 2077 is a game crafted with love, and it shows.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'tl;dr:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Cyberpunk 2077 is a game crafted with love, and it shows. The story is great,
    the gameplay is great, the worldbuilding is great. It's not perfect by any means,
    but it's still one of the best games I've played in years.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'The Good:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '- Great story and characters'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '- Amazing worldbuilding'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '- Excellent gunplay and movement system'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bad:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '- Binary stealth system (sometimes you''re seen no matter what) - Bugs'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-20.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Text summarization example
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Since many people spend hours per day reading and writing emails, summarizing
    them is a widely desired use case for GPT-3\. Let’s see how GPT-3 does with summarizing
    a three-paragraph email into one crisp line (Figure 2-19).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-21.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. Email summarization example
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the desired result, we pasted the full email and then simply added
    “one-sentence summary:'' at the end. We also included a “.” stop sequence to tell
    the model that it should stop its summary generation after a single sentence.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Hey Greg,
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: We are Sandra and Shubham. We are the co-authors of a GPT-3 based book that
    aims to be a go-to manual for API users. It will provide the readers with the
    knowledge to get started with GPT-3 as well as the context of the growing ecosystem
    surrounding it. The book will be released by O’Reilly publishing house by early
    2022\.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to connect and explore the possibility of collaboration with OpenAI
    to evangelize access to API for people from all backgrounds and enable them to
    build exciting new solutions using GPT-3\. We would appreciate your input to ensure
    we accurately represent OpenAI's stance on GPT-3 and inform our readers on the
    upcoming advancements/features of the API.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: It will be great if we can catch up sometime this week to discuss it further?
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Best,
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: S&S
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'one-sentence summary:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: We’re writing a book on GPT-3 and would like to connect with OpenAI to get a
    better understanding of the API and the future developments.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Before the introduction of GPT-3, there was a general understanding that AI
    models are capable of carrying a short conversation with humans that answer specific
    questions or handle specific tasks. However the models were not sophisticated
    enough to handle complicated text generation tasks and started to lose track whenever
    they encountered something complex or abstract.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: In the complicated world of natural language generation, GPT-3 has shaken the
    notion of language models being limited to trivial tasks. Text generation is the
    greatest strength of GPT-3\. It is capable of generating textual content that
    is almost indistinguishable from human-written text. GPT-3 is trained on billions
    of words from the training dataset to generate text in response to a variety of
    prompts. It generates an average of 4.5 billion words per day, [according to OpenAI](https://openai.com/blog/gpt-3-apps/).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: In the next two examples, we experiment with using GPT-3 to create content for
    a personal productivity app start-up and social media posts. We give the model
    only minimal context, and it generates many of the responses in Figures 2-21 and
    2-22 on the first take.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Article Generation
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-22.jpg)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. Article generation example
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ​​Here are the top 5 tips for personal productivity.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Develop a routine
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Work can be overwhelming. You don’t need to remember every little detail. By
    having a routine, you will be able to work more efficiently.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Avoid multitasking
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: If you are multitasking, you are bound to miss something. It is better to work
    on one task at a time.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Take breaks
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: It is true that if you do not take breaks, you will be more productive. However,
    if you take breaks too often you will get tired and stressed. If you work more
    effectively, you will shorten the time needed to complete a task.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Learn to delegate
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: If you have a lot of work to do, you should not do everything on your own. Delegate
    some of the work to others.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Prioritize
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize what you have to do and do the most important tasks first. Don’t
    worry about the small things.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Social Media Post Generation
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-23.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
- en: Figure 2-22\. Social media post generation example
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: Write a social media post for Facebook, Twitter, and Instagram about launching
    a new beta version of the personal productivity app.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'Facebook post:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: “After extensive user testing, we are happy to announce that our latest update
    is available for download! Our team has worked hard to create the best version
    of our app yet. Download it now for free at www.app.com.”
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'Twitter post:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '“We''re rolling out an updated version of your favorite productivity app! Now
    available on iOS and Android. #productivity #beta”'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'Instagram post:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '“We just released a new beta version of your favorite productivity app! Download
    it today for free on iOS and Android. #productivity #beta”'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have covered the OpenAI playground, prompt engineering and
    the different components of the OpenAI API followed by playground examples covering
    the major NLP tasks. By now, you have an understanding of how the API works in
    tandem with different components and how to use Playground as the base to design
    and experiment with different training prompts.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will walk you through how to use GPT-3 with different
    programming languages to integrate the API within your product or build a completely
    new application from scratch.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
