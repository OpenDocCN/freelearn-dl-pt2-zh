- en: Chapter 2\. Getting Started with OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: Even though GPT-3 is the most sophisticated and complex language model in the
    world, its capabilities are abstracted as a simple "text-in-text-out" interface
    to end users. This chapter will get you started with using that interface, Playground,
    and cover the technical nuances of the OpenAI API because it is always the details
    that reveal the true gems.
  prefs: []
  type: TYPE_NORMAL
- en: To work through this chapter, you must sign up for an OpenAI account at [https://beta.openai.com/signup](https://beta.openai.com/signup).
    If you haven’t done that, please do so now.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Playground
  prefs: []
  type: TYPE_NORMAL
- en: 'Your OpenAI developer account provides access to the API and infinite possibilities.
    We’ll start with Playground, a private web-based sandbox environment that allows
    you to experiment with the API, learn how its components work, and access developer
    documentation and the OpenAI community. We will then show you how to build robust
    prompts that generate favorable responses for your application. We’ll finish the
    chapter with examples of GPT-3 performing four NLP tasks: classification, named
    entity recognition (NER), summarization, and text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: In an interview with Peter Welinder, VP of Product at OpenAI, we asked about
    the key advice on navigating the Playground for first-time users. He told us his
    advice depends on the persona of the user. If the user has a machine learning
    background, Peter encourages them to “start by forgetting the things that they
    already know, and just go to the playground and try to get GPT-3 to do what you
    wanted it to by just asking it”. He suggests users “imagine GPT-3 as a friend
    or a colleague that you're asking to do something. How would you describe the
    task that you want them to do? And then see how GPT-3 responds. And if it doesn't
    respond in the way you want, iterate on your instructions."
  prefs: []
  type: TYPE_NORMAL
- en: 'As YouTuber and NLP influencer [Bakz Awan](https://www.youtube.com/user/bakztfuture)
    puts it, “The non-technical people ask: Do I need a degree to use this? Do I need
    to know how to code to use it? Absolutely not. You can use the Playground. You
    don''t need to write a single line of code. You''ll get results instantly. Anybody
    can do this.”'
  prefs: []
  type: TYPE_NORMAL
- en: Before you start using the Playground, we recommend reading OpenAI’s “[Getting
    Started](https://beta.openai.com/docs/developer-quickstart)” guide and the developer
    documentation. You’ll be able to access it with your OpenAI account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to get started with the Playground:'
  prefs: []
  type: TYPE_NORMAL
- en: Login at https://openai.com. After you’ve authenticated, navigate to Playground
    from the main menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the Playground screen (Figure 2-1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ●        The big text box marked 1 is where you provide text input (prompts).
  prefs: []
  type: TYPE_NORMAL
- en: ●        The box marked 2 on the right is the parameter-setting pane, which
    allows you to tweak the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '●        The box marked 3 allows you to load an existing preset: an example
    prompt and Playground settings. You can provide your training prompt or load an
    existing preset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. The Playground Interface, screenshot taken on 10th January 2022
  prefs: []
  type: TYPE_NORMAL
- en: Select an existing QA preset (marked 3). This will automatically load the training
    prompt along with the associated parameter settings. Click the Generate button
    (marked 4 in Figure 2-1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API processes your input and provides a response (called a completion) in
    the same text box. It also shows you the number of tokens utilized. A token is
    a numerical representation of words used to determine the pricing of API calls;
    we’ll discuss them later in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the bottom of the screen, you have a token count on the right, and on the
    left you have a Generate button (see Figure 2-2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image-0-3.jpg)Figure 2-2\. Q/A prompt completion along with token count'
  prefs: []
  type: TYPE_NORMAL
- en: Every time you click the Generate button, GPT-3 takes the prompt and completions
    within the text input field (marked as 1) into account and treats them as part
    of your training prompt for the next completion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the prompt you can see in figure 2-2:'
  prefs: []
  type: TYPE_NORMAL
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery, or has no clear answer, I will respond with "Unknown."
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is human life expectancy in the United States?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Human life expectancy in the United States is 78 years.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Who was the president of the United States in 1955?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Dwight D. Eisenhower was president of the United States in 1955\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Which party did he belong to?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: He belonged to the Republican Party.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is the square root of a banana?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Unknown'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: How does a telescope work?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Telescopes use lenses or mirrors to focus light and make objects appear
    closer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Where were the 1992 Olympics held?'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the completion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The 1992 Olympics were held in Barcelona, Spain.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the basic outline of the Playground, let's get into
    the nitty gritty of prompt engineering and design.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering and Design
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI API radically changed the way we interact with an AI model, stripping
    out layers and layers of complicated programming languages and frameworks. Andrej
    Karpathy, director of AI at Tesla, said jokingly, as soon as GPT-3 was released,
    that programming 3.0 is all about prompt design (the meme he tweeted is in Figure
    2-3). There is a direct relation between the training prompt you provide and the
    completion quality you get. The structure and arrangement of your words heavily
    influence the output. Understanding prompt design is the key to unlocking GPT-3’s
    true potential.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Meme source unknown; [tweeted by Andrej Karpathy](https://twitter.com/karpathy/status/1273788774422441984/photo/1)
    on June 18, 2020
  prefs: []
  type: TYPE_NORMAL
- en: 'While designing the training prompt, aim for a zero-shot response from the
    model: see if you can get the kind of response you want without priming the model
    with external training examples. If not, move forward by showing it a few examples
    rather than an entire dataset. The standard flow for designing a training prompt
    is to try for zero-shot first, then a few shots, and go for corpus-based fine-tuning
    (described below).'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is the first step towards general-purpose artificial intelligence and
    thus has its limitations. It doesn’t know everything and can’t reason on a human
    level, but it’s competent when you know how to talk to it. That’s where the art
    of prompt engineering comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 isn’t a truth-teller but an exceptional storyteller. It takes in the
    text input and attempts to respond with the text it thinks best completes it.
    If you give it a few lines from your favorite fiction novel, it will try to continue
    in the same style. It works by navigating through the context; without proper
    context, it can generate inconsistent responses. Let’s look at an example to understand
    how GPT-3 processes the input prompt and generates the output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is human life expectancy in the United States?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A:'
  prefs: []
  type: TYPE_NORMAL
- en: If you provide a prompt like this to GPT-3 without any context, you ask it to
    look for general answers from its universe of training data. It will result in
    generalized and inconsistent responses as the model doesn’t know which part of
    the training data to answer these questions. [[6]](xhtml-0-12.xhtml#aid_54)
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, providing the proper context will exponentially improve the
    quality of responses. It simply limits the universe of training data that the
    model has to examine to answer a question, resulting in more specific and to-the-point
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery or has no clear answer, I will respond with "Unknown."
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What is human life expectancy in the United States?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A:'
  prefs: []
  type: TYPE_NORMAL
- en: You can think of GPT-3 processing the input similar to the human brain. We tend
    to give random responses when somebody asks us questions without proper context.
    It happens because it’s difficult to get to a precise response without proper
    direction or context. The same is the case with GPT-3\. Its universe of training
    data is so big that it makes it difficult to navigate to a correct response without
    any external context or direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs like GPT-3 can creatively write and answer factual questions in the right
    context. Here is our five-step formula for creating efficient and effective training
    prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Define the problem you are trying to solve and what kind of NLP task it
    is, such as classification, Q&A, text generation, or creative writing.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Ask yourself if there is a way to get a zero-shot solution. If you need
    external examples to prime the model for your use case, think hard.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Now think of how you might encounter the problem in a textual fashion given
    the “text-in, text-out” interface of GPT-3\. Think about all the possible scenarios
    to represent your problem in textual form. For example, you want to build an ad
    copy assistant that can generate creative copy by looking at product name and
    description. To frame this goal in the “text-in, text-out” format, you can define
    the input as the product name and description and the output as the ad copy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Betty’s Bikes, for price-sensitive shoppers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: Low prices and huge selection. Free and fast delivery. Order online
    today!'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. If you do end up using external examples, use as few as possible and try
    to incorporate diversity, capturing all the representations to avoid essentially
    overfitting the model or skewing the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: These steps will act as a standard framework whenever you create a training
    prompt from scratch. Before you can build an end-to-end solution for your data
    problems, you need to understand more about how the API works. Let's dig deeper
    by looking at its components.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Down OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Components of API
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1 shows an overview of the components in the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Component
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Function
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Execution engine
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Determines the language model used for execution
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Response length
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Sets a limit on how much text the API includes in its completion
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Top P
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature controls the randomness of the response, represented as a range
    from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Top P controls how many random results the model should consider for completion,
    as suggested by
  prefs: []
  type: TYPE_NORMAL
- en: the temperature; it determines the scope of randomness.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty and
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty decreases the likelihood that the model will repeat the same
    line verbatim by
  prefs: []
  type: TYPE_NORMAL
- en: “punishing” it.
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty increases the likelihood that it will talk about new topics.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Best of
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Lets you specify the number of completions (n) to generate on the server side
    and returns the best of “n” completions
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Stop sequence
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies a set of characters that signals the API to stop generating completions
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Inject start and restart
  prefs: []
  type: TYPE_NORMAL
- en: text
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Inject start text allows you to insert text at the beginning of the completion.
  prefs: []
  type: TYPE_NORMAL
- en: Inject restart text allows you to insert text at the end of the completion.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Show probabilities
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Lets you debug the text prompt by showing the probability of tokens that the
    model can generate for a given input.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Components in the OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: Here is an overview of the components of GPT-3 API. We will discuss all of these
    components in more detail throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Execution Engine
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution engine determines the language model used for execution. Choosing
    the right engine is the key to determining your model’s capabilities and getting
    the right output. GPT-3 has four execution engines of varying sizes and capabilities:
    Davinci, Ada, Babbage, and Curie. Davinci is the most powerful and the Playground’s
    default.'
  prefs: []
  type: TYPE_NORMAL
- en: Response Length
  prefs: []
  type: TYPE_NORMAL
- en: 'The response length limits how much text the API includes in its completion.
    Because OpenAI charges by the length of text generated per API call (as noted,
    this is translated into tokens or numeric representations of words), response
    length is a crucial parameter for anyone on a budget. A higher response length
    will use more tokens and cost more. For example, suppose you do a classification
    task. In that case, it is not a good idea to set the response text dial to 100:
    the API could generate irrelevant texts and use extra tokens that will incur charges
    on your account. The API supports a maximum of 2048 tokens combined in the prompt
    and completion. So, while using the API you need to be careful that prompt and
    expected completion don’t exceed the maximum response length to avoid abrupt completions.
    If your use case involves large text prompts and completions, the workaround is
    to think of creative ways to solve problems within token limits, like condensing
    your prompt, breaking the text into smaller pieces, chaining multiple requests.'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature and Top P
  prefs: []
  type: TYPE_NORMAL
- en: The temperature dial controls the creativity of the response, represented as
    a range from 0 to 1\. A lower temperature value means the API will predict the
    first thing the model sees, resulting in the correct text, but rather dull, with
    minor variation. On the other hand a higher temperature value means the model
    evaluates possible responses that could fit into the context before predicting
    the result. The generated text will be more diverse, but there is a higher possibility
    of grammar mistakes and the generation of nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Top P controls how many random results the model should consider for completion,
    as suggested by the temperature dial; it determines the scope of randomness. Top
    P’s range is from 0 to 1\. A value close to zero means the random responses will
    be limited to a certain fraction: for example, if the value is 0.1, then only
    10% of the random responses will be considered for completion. This makes the
    engine deterministic, which means that it will always generate the same output
    for a given input text. If the value is set to 1, the API will consider all responses
    for completion, taking risks and coming up with creative responses. A lower value
    limits creativity; a higher value expands horizons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temperature and Top P have a very significant effect on output. It can sometimes
    be confusing to get your head around when and how to use them to get the correct
    output. The two are correlated: changing the value of one will affect the other.
    So by setting Top P to 1, you can allow the model to unleash its creativity by
    exploring the entire spectrum of responses and control the randomness by using
    the temperature dial.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: We always advise changing either Top P or temperature and keeping the
    other dial set at 1\.'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models rely on probabilistic approaches rather than conventional
    logic. Depending on how you set the model's parameters, they can generate various
    responses for the same input. The model tries to find the best probabilistic match
    within the universe of data it has been trained on instead of looking for a perfect
    solution every time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in Chapter 1, GPT-3’s universe of training data is vast, consisting
    of a variety of publicly available books, internet forums, and Wikipedia articles
    specially curated by OpenAI, allowing it to generate a wide array of completions
    for a given prompt. That’s where temperature and Top P, sometimes called the “creativity
    dials,” come in: you can tune them to generate more natural or abstract responses
    with an element of playful creativity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you will use GPT-3 to create names for your startup. You can set
    the temperature dial higher to get the most creative response. When we spent days
    and nights trying to come up with the perfect name for our startup, we dialed
    the temperature. GPT-3 came to the rescue and helped us to arrive at a name we
    love: Kairos Data Labs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On other occasions, your task might require little to no creativity: classification
    and question-answering tasks, for example. For these, keep the temperature lower.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple classification example that categorizes companies into
    general buckets or categories based on their names.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Temperature component
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in Figure 2-5, we have again used temperature to control the
    degree of randomness. You can also do this by changing Top P while keeping the
    temperature dial set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency and Presence Penalties
  prefs: []
  type: TYPE_NORMAL
- en: Like the temperature and Top P dials, the frequency penalty, and presence penalty
    dials consider text prompts (the previous completion plus the new input) instead
    of internal model parameters when deciding on output. The existing text thus influences
    the new completions. The frequency penalty decreases the likelihood of the model
    repeating the same line verbatim by “punishing” it. The presence penalty increases
    the likelihood that it will talk about new topics.
  prefs: []
  type: TYPE_NORMAL
- en: These come in handy when preventing the exact completion text from being repeated
    across multiple completions. Although these dials are similar, there is one crucial
    distinction. The frequency penalty is applied if the suggested text output is
    repeated (for example, the model used the exact token in previous completions
    or during the same session) and the model chooses an old output over a new one.
    The presence penalty is applied if a token is present in a given text at all.
  prefs: []
  type: TYPE_NORMAL
- en: Best of
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 uses the best of feature to generate multiple completions on the server
    side, evaluate them behind the scenes, and then provide you with the best probabilistic
    result. Using the “best of ” parameter, you can specify the number of completions
    (n) to generate on the server side. The model will return the best of n completions
    (the one with the lowest log probability per token).
  prefs: []
  type: TYPE_NORMAL
- en: 'This enables you to evaluate multiple prompt completions in a single API call
    rather than calling the API repeatedly to check the quality of different completions
    for the same input. However, using “best of ” is expensive: it costs n times the
    tokens in the prompt. For example, if you set the “best of ” value to 2, then
    you will be charged double the tokens present in the input prompt because on the
    backend the API will generate two completions and show you the best one.'
  prefs: []
  type: TYPE_NORMAL
- en: “Best of ” can range from 1 to 20 depending on your use case. If your use case
    serves clients for whom the quality of output needs to be consistent, then you
    can set the “best of ” value to a higher number. On the other hand, if your use
    case involves too many API invocations, then it makes sense to have a lower “best
    of ” value to avoid unnecessary latency and costs. We advise keeping response
    length minimal while generating multiple prompts using the “best of ” parameter
    to avoid additional charges.
  prefs: []
  type: TYPE_NORMAL
- en: Stop Sequence
  prefs: []
  type: TYPE_NORMAL
- en: A stop sequence is a set of characters that signal the API to stop generating
    completions. It helps avoid unnecessary tokens, an essential cost-saving feature
    for regular users.
  prefs: []
  type: TYPE_NORMAL
- en: You can provide up to 4 sequences for the API to stop generating further tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the example language translation task in Figure 2-6 to understand
    how the stop sequence works. In this example, English phrases are translated into
    French. We use the restart sequence “English:” as a stop sequence: whenever the
    API encounters that phrase, it will stop generating new tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Stop Sequence component
  prefs: []
  type: TYPE_NORMAL
- en: Inject Start Text and Inject Restart Text
  prefs: []
  type: TYPE_NORMAL
- en: The inject start text and inject restart text parameters allow you to insert
    text at the beginning or end of the completion, respectively. You can use them
    to keep a desired pattern going. Often, these settings work in tandem with the
    stop sequence, as in our example. The prompt has a pattern where an English sentence
    is provided with the prefix “English:” (the restart text), and the translated
    output is generated with the prefix “French:” (the start text). As a result, anyone
    can easily distinguish between the two and create a training prompt that both
    the model and the user can clearly comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we run the model for such prompts, the model automatically injects
    a start text “French:” before the output and a restart text “English:” before
    the next input, so that this pattern can be sustained.
  prefs: []
  type: TYPE_NORMAL
- en: Show Probabilities
  prefs: []
  type: TYPE_NORMAL
- en: The show probabilities parameter is at the bottom of the Playground settings
    pane. In conventional software engineering, developers use a debugger to troubleshoot
    (debug) a piece of code. You can use the show probabilities parameter to debug
    your text prompt. Whenever you select this parameter, you will see highlighted
    text. Hovering over it with the cursor will show a list of tokens that the model
    can generate for the particular input specified, with their respective probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this parameter to examine your options. In addition, it can make
    it easier to see alternatives that might be more effective. The “show probabilities”
    parameter has three settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Most Likely
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens most likely to be considered for completion in decreasing order
    of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Least Likely
  prefs: []
  type: TYPE_NORMAL
- en: Lists the tokens least likely to be considered for completion in decreasing
    order of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Full Spectrum
  prefs: []
  type: TYPE_NORMAL
- en: Shows the entire universe of tokens that could be selected for completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at this parameter in the context of a simple prompt. We want to
    start the output sentence with a simple, well-known phrase: “Once upon a time”.
    We provide the API with the prompt “Once upon a” and then check the Most Likely
    option in the show probabilities tab.'
  prefs: []
  type: TYPE_NORMAL
- en: As Figure 2-7 shows, it generates “time” as the response. Because we have set
    the “show probabilities” parameter to “Most Likely,” the API indicates the response
    and a list of possible options along with their probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve had an overview, let’s look at these components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-8.jpg)Figure 2-7\. Show Probabilities component showing the
    most likely tokens'
  prefs: []
  type: TYPE_NORMAL
- en: Execution Engines
  prefs: []
  type: TYPE_NORMAL
- en: As noted in Figure 2-7, the OpenAI API offers four different execution engines,
    differentiated by parameters and performance capabilities. Execution engines power
    OpenAI API. They serve as “autoML” solutions, providing automated ML methods and
    processes to make machine learning available for non-experts. They are easy to
    configure and adapt to a given dataset and task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four primary execution engines were named after famous scientists in alphabetical
    order: Ada (named after Ada Lovelace), Babbage (named after Charles Babbage),
    Curie (named after Madame Marie Curie), and Davinci (named after Leonardo da Vinci).
    Let’s deep-dive into each of these execution engines to understand when to use
    which execution engine when working with GPT-3:'
  prefs: []
  type: TYPE_NORMAL
- en: DaVinci
  prefs: []
  type: TYPE_NORMAL
- en: Da Vinci is the largest execution engine and the default when you first open
    the Playground. It can do anything the other engines can, often with fewer instructions
    and better outcomes. However, the trade-off is that it costs more per API call
    and is slower than other engines. You might want to use other engines to optimize
    cost and runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: We recommend starting with Davinci because of its superior capabilities
    when testing new ideas and prompts. Experimenting with Davinci is a great way
    to determine what the API can do. You can slowly move down the ladder to optimize
    budgets and runtimes as you become comfortable with your problem statement. Once
    you have an idea of what you want to accomplish, you can either stay with Davinci
    (if cost and speed are not a concern) or move on to Curie or other less costly
    engines and try to optimize the output around its capabilities. You can use [OpenAI’s
    Comparison Tool](https://gpttools.com/comparisontool) to generate an Excel spreadsheet
    that compares engines’ outputs, settings, and response times.'
  prefs: []
  type: TYPE_NORMAL
- en: Davinci should be your first choice for tasks that require understanding the
    content, like summarizing meeting notes or generating creative ad copy. It’s great
    at solving logic problems and explaining the motives of fictional characters.
    It can write a story. Davinci has also been able to solve some of the most challenging
    AI problems involving cause and effect.[[7]](xhtml-0-12.xhtml#aid_85)
  prefs: []
  type: TYPE_NORMAL
- en: Curie
  prefs: []
  type: TYPE_NORMAL
- en: Curie aims to find an optimal balance between power and speed that is very important
    for performing high-frequency tasks like classification on an immense scale or
    putting a model into production.
  prefs: []
  type: TYPE_NORMAL
- en: Curie is also quite good at answering questions, performing Q&As, and serving
    as a general-purpose chatbot. For instance, if you build a customer-support chatbot,
    you might choose Curie to serve high-volume requests faster.
  prefs: []
  type: TYPE_NORMAL
- en: While Davinci is stronger at analyzing complicated texts, Curie can perform
    with low latency and lightning-fast speed. It is always sensible to figure out
    your use case and do a cost-benefit analysis before choosing Davinci over Curie
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Babbage
  prefs: []
  type: TYPE_NORMAL
- en: Babbage is faster than Cuire but not capable of performing tasks that involve
    understanding complex intent. However, it is quite capable and is preferable when
    it comes to semantic search rankings and analyzing how well documents match up
    with search queries. It’s less expensive than Curie and Davinci and is preferred
    for simple problems involving frequent API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Ada
  prefs: []
  type: TYPE_NORMAL
- en: Ada is the fastest and least expensive of all the engines available. It can
    perform simple tasks that do not require a nuanced understanding of context, like
    parsing text, correcting grammar, or simple classification. It is often possible
    to improve Ada's performance by providing more context with the input. Ada can
    be the preferred model of choice for use cases involving frequent API invocations,
    as it can achieve results similar to bigger models with suitable settings. The
    more you experiment with the API parameters, the more understanding you will gain
    of what settings work for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct Series
  prefs: []
  type: TYPE_NORMAL
- en: Based on the four primary models, OpenAI has launched a series of [InstructGPT](https://openai.com/blog/instruction-following/)
    models that are better at understanding the instructions and following them while
    being less toxic and more truthful than the original GPT-3\. They have been developed
    using techniques coming from OpenAI’s alignment research. These models are trained
    with humans in the loop and are now deployed as the default language models on
    OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. Output generated by InstructGPT Davinci
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Output generated by GPT Davinci
  prefs: []
  type: TYPE_NORMAL
- en: 'Our input:'
  prefs: []
  type: TYPE_NORMAL
- en: “Write a story about AI for a 5-year-old.”
  prefs: []
  type: TYPE_NORMAL
- en: 'InstructGPT output:'
  prefs: []
  type: TYPE_NORMAL
- en: “Robot was very happy. He had just been born, and he was already the smartest
    robot in the world. He could do anything that any other robot could do, but he
    could also think for himself.”
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT output:'
  prefs: []
  type: TYPE_NORMAL
- en: “This project is an attempt to make an AI that can produce fictional stories.
    It is a work in progress. Fictionality The aim of this project is to create an
    AI that can create fictional stories,”
  prefs: []
  type: TYPE_NORMAL
- en: To make the process of building prompts efficient, OpenAI decided to publicly
    launch text-davinci-001, text-curie-001, text-babbage-001, and text-ada-001\.
    With clear instructions, these models can produce better results than their base
    counterparts and are now the [default models of the API](https://help.openai.com/en/articles/5832130-what-s-changed-with-engine-names-and-best-practices).
    This series is important in bridging the gap between how humans think and models
    operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: We recommend using this model series as your default for all text-related
    tasks. The base version of GPT-3 models are available as davinci, curie, babbage,
    and ada and are meant to be used with the fine-tuning, search, classification,
    and answers endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  prefs: []
  type: TYPE_NORMAL
- en: 'The Playground is a graphical web interface that calls the OpenAI API behind
    the scenes, but there are several other ways to call the API. To do this, you
    will need to get familiar with its endpoints: the remote APIs that communicate
    back and forth when they are called. This section will familiarize you with the
    functionality and usage of six API endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: List Engines
  prefs: []
  type: TYPE_NORMAL
- en: 'The list engine endpoint, also known as the “metadata API,” provides a list
    of available engines and specific metadata associated with each engine, such as
    owner and availability. To access it, you can hit the following URI with the HTTP
    GET method without passing any request parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: GET https://api.openai.com/v1/engines
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve Engines
  prefs: []
  type: TYPE_NORMAL
- en: 'When you provide an engine name to the retrieve engine endpoint, it returns
    detailed metadata information about that engine. To access, hit the following
    URI with the HTTP GET method without passing any request parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: GET [https://api.openai.com/v1/engines/{engine_id](https://api.openai.com/v1/engines/%7Bengine_id)}
  prefs: []
  type: TYPE_NORMAL
- en: Completions
  prefs: []
  type: TYPE_NORMAL
- en: 'Completions is GPT-3’s most famous and widely used endpoint. It simply takes
    in the text prompt as input and returns the completed response as output. It uses
    the HTTP POST method and requires an engine ID as part of the URI path. As part
    of the HTTP Body, the Completions endpoint accepts several additional parameters
    discussed in the previous section. Its signature is:'
  prefs: []
  type: TYPE_NORMAL
- en: POST https://api.openai.com/v1/engines/{engine_id}/completions
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Search
  prefs: []
  type: TYPE_NORMAL
- en: The semantic search endpoint enables you to provide a query in natural language
    to search a set of documents, which can be words, sentences, paragraphs, or even
    longer texts. It will score and rank the documents based on how semantically related
    they are to the input query. For example, if you provide the documents ["school",
    "hospital", "park"] and query "the doctor", you’ll get a different similarity
    score for each document.
  prefs: []
  type: TYPE_NORMAL
- en: The similarity score is a positive score that usually ranges from 0 to 300 (but
    can sometimes go higher), where a score above 200 usually indicates that the document
    is semantically similar to the query. The higher the similarity score, the more
    semantically similar the document is to the query (in this example, "hospital"
    will be most similar to "the doctor"). You can provide up to 200 documents as
    part of your request to the API.[[8]](xhtml-0-12.xhtml#aid_48)
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the signature for the semantic search endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: POST [https://api.openai.com/v1/engines/{engine_id}/search](https://api.openai.com/v1/engines/%7Bengine_id%7D/search)
  prefs: []
  type: TYPE_NORMAL
- en: Files
  prefs: []
  type: TYPE_NORMAL
- en: 'The files endpoint can be used across different endpoints like answers, classification,
    and semantic search. It is used to upload documents or files to the OpenAI storage,
    which is accessible through the API features. The same endpoint can be used with
    different signatures to perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: List Files
  prefs: []
  type: TYPE_NORMAL
- en: It simply returns the list of files that belong to the user's organization or
    that are linked to a particular user’s account. It's an HTTP GET call that doesn’t
    require any parameters to be passed with the request.
  prefs: []
  type: TYPE_NORMAL
- en: GET https://api.openai.com/v1/files
  prefs: []
  type: TYPE_NORMAL
- en: Upload Files
  prefs: []
  type: TYPE_NORMAL
- en: It is used to upload a file that contains documents to be used across various
    endpoints. It uploads the documents to the already allocated internal space by
    OpenAI for the users' organization. It is an HTTP POST call that requires the
    file path to be added to the API request.
  prefs: []
  type: TYPE_NORMAL
- en: ​POST https://api.openai.com/v1/files
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve File
  prefs: []
  type: TYPE_NORMAL
- en: 'It returns the information about a specific file by just providing the file
    id as the request parameter. Following is the signature for the Retrieve endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: ​GET https://api.openai.com/v1/files/{file_id}
  prefs: []
  type: TYPE_NORMAL
- en: Delete File
  prefs: []
  type: TYPE_NORMAL
- en: 'It deletes the specific file by providing the file as the request parameter.
    Following is the signature for the Delete endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: ​DELETE https://api.openai.com/v1/files/{file_id}
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs: []
  type: TYPE_NORMAL
- en: Another experimental endpoint of the API is Embeddings. Embeddings are the core
    of any machine learning model and allow to capture of semantics from the text
    by converting it into high-dimensional vectors. Currently, developers tend to
    use open-source models like the BERT series to create embeddings for their data
    that can be used for a variety of tasks like recommendation, topic modeling, semantic
    search, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI realized that GPT-3 holds great potential to power embedding-driven
    use cases and come up with state-of-the-art results. Generating embeddings for
    the input data is very straightforward and wrapped in the form of an API call.
    To create an embedding vector representing the input text, you can use the following
    API signature:'
  prefs: []
  type: TYPE_NORMAL
- en: POST [https://api.openai.com/v1/engines/{engine_id}/embeddings](https://api.openai.com/v1/engines/%7Bengine_id%7D/embeddings)
  prefs: []
  type: TYPE_NORMAL
- en: To invoke the embeddings endpoint, you can choose the type of engine depending
    on your use case by referring to the [embeddings document](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings).
    Each engine has its specific dimensions of embedding, with Davinci being the biggest
    and Ada being the smallest. All the embedding engines are derived from the four
    base models and classified based on the use cases to allow efficient and cost-friendly
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing GPT-3
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s research paper “[Process for Adapting Language Models to Society (PALMS)
    with Values-Targeted Datasets](https://cdn.openai.com/palms.pdf)” by Irene Solaiman
    and Christy Dennison (June 2021) led the company to launch a first-of-its-kind
    fine-tuning endpoint that allows you to get more out of GPT-3 than was previously
    possible by customizing the model for your particular use case. Customizing GPT-3
    improves the performance of any natural language task GPT-3 is capable of performing
    for your specific use case.[[9]](xhtml-0-12.xhtml#aid_36)
  prefs: []
  type: TYPE_NORMAL
- en: Let us explain how that works first.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI pre-trained GPT-3 on a [specially prepared dataset](https://arxiv.org/pdf/2005.14165.pdf)
    in a semi-supervised fashion. When given a prompt with just a few examples, it
    can often intuit what task you are trying to perform and generate a plausible
    completion. This is called "few-shot learning," as you learned in chapter 1.
  prefs: []
  type: TYPE_NORMAL
- en: By fine-tuning GPT-3 on their own data, users can create a custom version of
    the model that is tailored to their specific project needs. This customization
    allows GPT-3 to be more reliable and efficient in a variety of use cases. Fine-tuning
    the model involves adjusting it so that it consistently performs in the desired
    way. This can be done using an existing dataset of any size or by incrementally
    adding data based on user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: The process of fine-tuning will focus the knowledge and capabilities of the
    model on the contents and semantics of the data used for training, which will
    in turn limit the range of topics and creativity it can generate. This can be
    useful for downstream tasks that require specialized knowledge, such as classifying
    internal documents or dealing with internal jargon. Fine-tuning the model also
    focuses its attention on the specific data used for training, limiting its overall
    knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model has been fine-tuned, it no longer requires examples in the prompt,
    which saves costs and improves the speed and quality of the output. Customizing
    GPT-3 in this way appears to be more effective than using prompt design alone,
    as it allows for the use of more training examples.
  prefs: []
  type: TYPE_NORMAL
- en: With less than 100 examples you can start seeing the benefits of fine-tuning
    the model. Its performance continues to improve as you add more data. In the PALMS
    research paper, OpenAI showed how fine-tuning with less than 100 examples can
    improve the model’s performance on a number of tasks. They’ve also found that
    doubling the number of examples tends to improve the quality of the output linearly.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing GPT-3 improves the reliability of its outputs and gives more consistent
    results that you can take to production use cases. Existing OpenAI API customers
    found that customizing GPT-3 can dramatically reduce the frequency of unreliable
    outputs - there is a growing group of customers that can vouch for it with their
    performance numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Apps Powered by Customized GPT-3 Models
  prefs: []
  type: TYPE_NORMAL
- en: Keeper Tax helps independent contractors and freelancers with their taxes. It
    uses various models to extract text and classify transactions and then identifies
    easy-to-miss tax write-offs to help customers file their taxes directly from the
    app. By customizing GPT-3, Keeper Tax experienced an increasing accuracy from
    85% to 93%. And it continuously improves thanks to adding 500 new training examples
    to their model once a week, which is leading to about a 1% accuracy improvement
    a week.
  prefs: []
  type: TYPE_NORMAL
- en: Viable helps companies get insights from their customer feedback. By customizing
    GPT-3, Viable is able to transform massive amounts of unstructured data into readable
    natural language reports. Customizing GPT-3 has increased the reliability of Viable’s
    reports. By using a customized version of GPT-3, accuracy in summarizing customer
    feedback has improved from 66% to 90%. For an in-depth insight into Viable’s journey,
    refer to our interview with Viable’s CEO in chapter 4\.
  prefs: []
  type: TYPE_NORMAL
- en: Sana Labs is a global leader in the development and application of AI to learning.
    Their platform powers personalized learning experiences for businesses by leveraging
    the latest ML breakthroughs to personalize content. By customizing GPT-3 with
    their data, Sana’s question and content generation went from grammatically correct
    but general responses to highly accurate ones. This yielded a 60% improvement,
    enabling more personalized experiences for their users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elicit is an AI research assistant that helps directly answer research questions
    using findings from academic papers. The assistant finds the most relevant abstracts
    from a large corpus of research papers, then applies GPT-3 to generate the claim
    that the paper makes about the question. A custom version of GPT-3 outperformed
    prompt design and led to improvement in three areas: results were 24% easier to
    understand, 17% more accurate, and 33% better overall.'
  prefs: []
  type: TYPE_NORMAL
- en: How to customize GPT-3 for your application
  prefs: []
  type: TYPE_NORMAL
- en: To begin, simply use the OpenAI command line tool with a file of your choosing.
    Your personalized version will begin training and will be accessible through our
    API right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a very high level, customizing GPT-3 for your application involves the following
    three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Prepare new training data and upload it to the OpenAI server
  prefs: []
  type: TYPE_NORMAL
- en: ●        Fine-tune the existing models with the new training data
  prefs: []
  type: TYPE_NORMAL
- en: ●        Use the fine-tuned model
  prefs: []
  type: TYPE_NORMAL
- en: Prepare and upload training data
  prefs: []
  type: TYPE_NORMAL
- en: Training data is what the model takes in as input for fine-tuning. Your training
    data must be a JSONL document, where each line is a prompt-completion pair corresponding
    to a training example. For model fine-tuning, you can provide an arbitrary number
    of examples where it is highly recommended to create a value-targeted dataset
    to provide the model with quality data and wide representation. Fine-tuning improves
    performance with more examples, so the more examples you provide, the better the
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your JSONL document should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: Where the prompt text should include the exact prompt text you want to complete,
    and the ideal generated text should include an example of desired completion text
    that you want GPT-3 to generate.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use OpenAI''s CLI data preparation tool to easily convert your data
    into this file format. CLI data preparation tool accepts files in different formats,
    with the only requirement that they contain a prompt and a completion column/key.
    You can pass a CSV, TSV, XLSX, JSON, or JSONL file, and it will save the output
    into a JSONL file ready for fine-tuning. To do that, you can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: openai tools fine_tunes.prepare_data -f <LOCAL_FILE>
  prefs: []
  type: TYPE_NORMAL
- en: Where LOCAL_FILE is the file you prepared for conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Train a new fine-tuned model
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you prepare your training data as described above, you can move on to
    the fine-tuning job with the help of the OpenAI CLI. For that, you need the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>
  prefs: []
  type: TYPE_NORMAL
- en: 'Where BASE_MODEL is the name of the base model you''re starting from (ada,
    babbage, or curie). Running this command does several things:'
  prefs: []
  type: TYPE_NORMAL
- en: ●        Uploads the file using the files endpoint (as discussed earlier in
    this chapter);
  prefs: []
  type: TYPE_NORMAL
- en: ●        Fine-tunes the model using the request configuration from the command;
  prefs: []
  type: TYPE_NORMAL
- en: ●        Streams the event logs until the fine-tuning job is completed.
  prefs: []
  type: TYPE_NORMAL
- en: Log streaming is helpful to understand what’s happening in real-time and to
    respond to any incidents/failures as and when they happen. The streaming may take
    from minutes to hours depending on the number of jobs in the queue and the size
    of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using the fine-tuned model
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is successfully fine-tuned, you can start using it! You can now
    specify this model as a parameter to the Completion Endpoint, and make requests
    to it using the Playground.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: After the fine-tuning job completes, it may take several minutes for your
    model to become ready to handle requests. If completion requests to your model
    time out, it is likely because your model is still being loaded. If this happens,
    try again in a few minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start making requests by passing the model name as the model parameter
    of a completion request using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>
  prefs: []
  type: TYPE_NORMAL
- en: Where FINE_TUNED_MODEL is the name of your model and YOUR_PROMPT is the prompt
    you want to complete in this request.
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to use all the Completion Endpoint parameters that were discussed
    in this chapter, like temperature, frequency_penalty, presence_penalty, etc.,
    on these requests to the newly fine-tuned model as well.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE:No engine is specified on these requests. This is the intended design and
    something that OpenAI plans on standardizing across other API endpoints in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, please refer to OpenAI’s [fine-tuning documentation](https://beta.openai.com/docs/guides/fine-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into how different prompts consume tokens, let's look more
    closely at what is a token.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve told you that tokens are numerical representations of words or characters.
    Using tokens as a standard measure, GPT-3 can handle training prompts from a few
    words to entire documents.
  prefs: []
  type: TYPE_NORMAL
- en: For regular English text, 1 token consists of approximately 4 characters. It
    translates to roughly ¾ of a word, so for 100 tokens there will be approximately
    75 words. As a point of reference, the collected works of Shakespeare consist
    of about 900,000 words, roughly translating to 1.2M tokens.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the latency of API calls, OpenAI imposes a limit of 2,048 tokens
    (approximately ~1,500 words) for prompts and completions.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand how the tokens are calculated/consumed in the context
    of GPT-3 and to stay within the limits set by the API let us walk you through
    the following ways you can measure the token count.
  prefs: []
  type: TYPE_NORMAL
- en: In the Playground, as you enter text into the interface, you can see the token
    count update in real-time in the footer at the bottom right. It displays the number
    of tokens that will be consumed by the text prompt after hitting the submit button.
    You can use it to monitor your token consumption every time you interact with
    the Playground (see Figure 2-10).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Token Count in the Playground
  prefs: []
  type: TYPE_NORMAL
- en: The other way to measure the consumption of tokens is by using the explicit
    GPT-3 Tokenizer tool (see Figure 2-11) that lets you visualize the formation of
    tokens from the word characters. You can interact with the Tokenizer tool via
    a simple text box where you write the prompt text and Tokenizer will show you
    the token and character counts along with the detailed visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. Tokenizer by OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: For integrating the token count metric in your API calls to different endpoints,
    you can patch the logprobs and echo attribute along with the API request to get
    the full list of tokens consumed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how tokens are priced based on the different
    execution engines.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs: []
  type: TYPE_NORMAL
- en: In the last section we talked about tokens, which are the smallest fungible
    unit used by OpenAI to determine the pricing for API calls. Tokens allow greater
    flexibility than measuring the number of words or sentences used in the training
    prompt and due to the token’s sheer granularity tokens can be easily processed
    and used to measure the pricing for a wide range of training prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you call the API from either the Playground or programmatically,
    behind the scenes the API calculates the number of tokens used in the training
    prompt along with the generated completion and charges each call on the basis
    of the total number of tokens used.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI generally charges a flat fee per 1,000 tokens, with the fee depending
    on the execution engine used in the API call. Davinci is the most powerful and
    expensive, while Curie, Babbage, and Ada are cheaper and faster.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2 shows the pricing for the various API engines at the time this chapter
    was written (December 2022).
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Price Per 1k Tokens
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Davinci (most powerful)
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: $0.0200
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Curie
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: $0.0020
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Babbage
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: $0.0005
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Ada (fastest)
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: $0.0004
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Model Pricing
  prefs: []
  type: TYPE_NORMAL
- en: The company works on the cloud pricing model of “pay as you go”. For updated
    pricing please check the [online pricing schedule](https://beta.openai.com/pricing).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of monitoring the tokens for each of the API calls, OpenAI provides
    a [reporting dashboard](https://beta.openai.com/account/usage) to monitor daily
    cumulative token usage. Depending on your usage, it may look something like Figure
    2-12.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. API Usage Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 2-12 you can see a bar graph showing the daily token consumption from
    the API usage. The dashboard helps you to monitor the token usage and pricing
    for your organization. This helps you to regulate the API usage and stay within
    your budget. There is also an option to monitor the cumulative usage and breakdown
    of token count per API call. This should give you enough flexibility to create
    policies around token consumption and pricing for your organization. Now that
    you understand the ins and outs of the Playground and the API, we will take a
    look at GPT-3’s performance on typical language modeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIP: For beginners who have just started with GPT-3 and find it hard to wrap
    their heads around token consumption. Many users enter too long prompt texts,
    which leads to the overuse of credits, followed by unplanned fees. To avoid this,
    during your initial days try to use the API dashboard to observe the number of
    tokens consumed and see how the length of prompts and completions affect token
    usage. It can help you to prevent the uncontrolled use of credits and keep everything
    within the budget.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3’s Performance on Standard NLP Tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 is a highly advanced and sophisticated successor to the NLP field, built
    and trained using the core NLP approaches and deep neural networks. For any AI-based
    modeling approach, the model performance is evaluated in the following way: First,
    you train the model for a specific task (like classification, Q/A, text generation,
    etc) on training data; then you verify the model performance using the test data
    (unseen data).'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, there is a standard set of NLP benchmarks for evaluating the
    performance of NLP models and coming up with a relative model ranking or comparison.
    This comparison, or relative ranking, allows you to pick and choose the best model
    for a specific NLP task (business problem).
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will discuss the performance of GPT-3 on some standard NLP
    tasks as seen in Figure 2-13 and compare it with the performance of similar models
    on respective NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Conventional NLP Tasks
  prefs: []
  type: TYPE_NORMAL
- en: Text Classification
  prefs: []
  type: TYPE_NORMAL
- en: NLP-powered text classification involves using algorithms to automatically analyze
    text and assign it to predefined categories or tags based on its context. This
    process helps to organize and categorize text into relevant groups.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification involves analyzing the text provided as input and assigning
    it a label, score, or another attribute that characterizes the text. Some common
    examples of text classification are sentiment analysis, topic labeling, intent
    detection, etc. You can use a number of approaches to get GTP-3 to classify text,
    again ranging from zero-shot classification (where you don't give any examples
    to the model) to single-shot and few-shot classification (where you show some
    examples to the model).
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Classification
  prefs: []
  type: TYPE_NORMAL
- en: Modern artificial intelligence has long aimed to develop models that can perform
    predictive functions on data it has never seen before. This important research
    area is called zero-shot learning. Similarly, a zero-shot classification is a
    classification task where no prior training and fine-tuning on labeled data is
    required for the model to classify a piece of text.GPT-3 currently produces results
    for unseen data that are either better or at par with state-of-the-art AI models
    fine-tuned for that specific purpose. In order to perform zero-shot classification
    with GPT-3, we must provide it with a compatible prompt. In chapter 2, we will
    discuss prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of a zero-shot classification where the goal is to perform
    a fact-checking analysis to determine if the information included in the tweet
    is correct or incorrect. Figure 2-14 shows a pretty impressive information correctness
    classification result based on a zero-shot example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-15.jpg)Figure 2-14\. Zero-shot classification example'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyse the tweet in terms of information correctness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: “More than 50%of global scientists don’t believe in climate change.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The tweet is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Single-shot and Few Shot Text Classification
  prefs: []
  type: TYPE_NORMAL
- en: The other approach to text classification is via fine-tuning an AI model on
    a single or few training examples also known as a single shot or few shots text
    classification. When you provide examples of how to classify text, the model can
    learn information about the object categories based on the samples you provide.
    This is a superset of zero-shot classification that allows you to classify text
    by providing the model with three to four diversified examples. This can be useful
    specifically for downstream use cases, which require some level of context setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following example of a few-shot classification. We are asking
    the model to perform a tweet sentiment analysis classification and giving it three
    tweet examples to illustrate each of the possible labels: positive, neutral, and
    negative. As you can see in Figure 2-15, the model equipped with such a detailed
    context based on a few examples, is able to very easily perform the sentiment
    analysis of the next tweet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: when you recreate prompt examples from the book or create your own, make
    sure to have adequate line spacing in your prompt. An additional line after a
    paragraph can result in a very different outcome, so you want to play with that
    and see what works best for you.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Few-shot classification example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyse the tweet in terms of its sentiment. Depending on the sentiment, classify
    it as positive, neutral, or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative):'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: positive
  prefs: []
  type: TYPE_NORMAL
- en: Batch Classification
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the few-shot classification with GPT-3, let's dive deeper
    into the batch classification which allows you to classify input samples in batches
    in a single API call instead of just classifying one example per API call. It
    is suitable for applications where you want to classify multiple examples in a
    single go, just like the tweet sentiment analysis task we examined, but analyzing
    a few tweets in a row.
  prefs: []
  type: TYPE_NORMAL
- en: As with a few shots classification, you want to provide enough context for the
    model to achieve the desired result but in a batch configuration format. Here,
    we define the different categories of tweet sentiment classification using various
    examples in the batch configuration format. Then we ask the model to analyze the
    next batch of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Batch classification example (Part-1)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Batch-classification example (Part-2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyse tweets in terms of their sentiment. Depending on their sentiment, classify
    them as positive, neutral, or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  prefs: []
  type: TYPE_NORMAL
- en: '###'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. "I'm seriously worried that super intelligent AI will be disappointed in
    humans."
  prefs: []
  type: TYPE_NORMAL
- en: 2\. "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."
  prefs: []
  type: TYPE_NORMAL
- en: 3\. "I think it is neither super likely nor super unlikely that the super intelligent
    AI will emerge one day."
  prefs: []
  type: TYPE_NORMAL
- en: 4\. "Super intelligent AI is going to be the most exciting discovery in human
    history."
  prefs: []
  type: TYPE_NORMAL
- en: 5\. "This is the latest report on the state of the AI"
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. negative
  prefs: []
  type: TYPE_NORMAL
- en: 2\. positive
  prefs: []
  type: TYPE_NORMAL
- en: 3\. neutral
  prefs: []
  type: TYPE_NORMAL
- en: 4\. positive
  prefs: []
  type: TYPE_NORMAL
- en: 5\. neutral
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. "I can't stand bad techno music"
  prefs: []
  type: TYPE_NORMAL
- en: 2\. "This is a tweet"
  prefs: []
  type: TYPE_NORMAL
- en: 3\. "I can't wait for going to the Moon!!!"
  prefs: []
  type: TYPE_NORMAL
- en: 4\. "AI is super cute ❤️"
  prefs: []
  type: TYPE_NORMAL
- en: 5\. "Got VERY ANGRY now!!! ��"
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweet sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. negative
  prefs: []
  type: TYPE_NORMAL
- en: 2\. neutral
  prefs: []
  type: TYPE_NORMAL
- en: 3\. positive
  prefs: []
  type: TYPE_NORMAL
- en: 4\. positive
  prefs: []
  type: TYPE_NORMAL
- en: 5\. negative
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model recreated the batch sentiment analysis format and
    classified the tweets successfully. Now let’s move on to see how it performs at
    the Named Entity Recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition (NER) is an information extraction task that involves
    identifying and categorizing named entities mentioned in unstructured text. These
    entities may include people, organizations, locations, dates, quantities, monetary
    values, and percentages. This task is useful for extracting important information
    from text.
  prefs: []
  type: TYPE_NORMAL
- en: NER helps to make the responses more personalized and relevant but the current
    state-of-the-art approaches require massive amounts of data for training before
    you even start with the prediction. GPT-,3 on the other hand, can work out of
    the box for recognizing general entities like people, places, and organizations
    without humans providing even a single training example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example we used a davinci-instruct-series version of the model
    that was in beta at the time of writing this book and the model gathers prompts
    to train and improve the future OpenAI API models. We gave it a simple task: to
    extract contact information from an example email. It successfully completed the
    task on the first attempt (Figure 2-18).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. NER example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the name and mailing address from this email:'
  prefs: []
  type: TYPE_NORMAL
- en: Shubham,
  prefs: []
  type: TYPE_NORMAL
- en: It was great to talk to you the other day!
  prefs: []
  type: TYPE_NORMAL
- en: I'm SO looking forward to start working on our book.
  prefs: []
  type: TYPE_NORMAL
- en: Here's my address 1307 Roosevelt Street, San Francisco CA​94107
  prefs: []
  type: TYPE_NORMAL
- en: Best,
  prefs: []
  type: TYPE_NORMAL
- en: Sandra Kublik
  prefs: []
  type: TYPE_NORMAL
- en: 'Name and mailing address:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: Sandra Kublik
  prefs: []
  type: TYPE_NORMAL
- en: 1307 Roosevelt Street, San Francisco CA 94107
  prefs: []
  type: TYPE_NORMAL
- en: Text Summarization
  prefs: []
  type: TYPE_NORMAL
- en: The goal of text summarization is to create a shortened version of a lengthy
    text while still accurately representing the original content and maintaining
    its overall meaning. This is done by identifying and highlighting the most important
    information in the text.GPT-3 based text summarization aims to transform lengthy
    pieces of texts into their condensed tl;dr[[10]](xhtml-0-12.xhtml#aid_41) versions.
    Such tasks are generally difficult and costly to accomplish manually. With GPT-3,
    it is a matter of a single input and a few seconds!
  prefs: []
  type: TYPE_NORMAL
- en: NLP models can be trained to comprehend documents and identify the sections
    that convey important facts and information before producing the required summarized
    texts. However, such models need a large amount of training samples before they
    can learn the context and start summarizing the unseen input.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3's abstractive summarization is the key to solving the problem of information
    extraction. By producing summaries instead of merely extracting key information,
    GPT-3 can provide a more comprehensive and accurate understanding of the text.
    It uses zero shot or few-shot approach towards text summarization making it useful
    for a variety of use cases. With GPT-3 there are multiple ways you can go about
    summarizing the text involving basic summaries, one-line summaries, and grade-level
    summaries depending on your use case. Let’s have a quick walkthrough of these
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time the model is able to generate decent results in the form of
    a review summary, but sometimes it can output irrelevant results depending on
    the prior context. To avoid the problem of getting unwanted results, you can set
    the Best of parameter to 3 which will always give you the best of three results
    generated by the API. In the example shown in Figure 2-18, after a few tries and
    minor parameter tweaking, we got decent results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that when it works, it works, but when it doesn’t, it really
    doesn’t. Luckily, the game’s low points like the horrible binary stealth system
    and the bugs and lack of QoL are either fixable or vastly outweighed by the good
    bits, and the overall tally still ends up being much higher than the average of
    many other games out there. That’s helped a lot by the moment-to-moment gameplay,
    which pays off just as much as the story beats; the movement system lets you climb
    almost any short object, vehicles are weighty to drive, and have fantastic interior
    and exterior designs, and guns feel properly loud and heavy. Its story and narrative
    kept me glued to the screen, and everything – from sex scenes and romance options
    to Keanu Reeves trying to kill you or help you throughout the game – are remarkably
    well done. Cyberpunk 2077 is a game crafted with love, and it shows.
  prefs: []
  type: TYPE_NORMAL
- en: 'tl;dr:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: Cyberpunk 2077 is a game crafted with love, and it shows. The story is great,
    the gameplay is great, the worldbuilding is great. It's not perfect by any means,
    but it's still one of the best games I've played in years.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Good:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Great story and characters'
  prefs: []
  type: TYPE_NORMAL
- en: '- Amazing worldbuilding'
  prefs: []
  type: TYPE_NORMAL
- en: '- Excellent gunplay and movement system'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bad:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Binary stealth system (sometimes you''re seen no matter what) - Bugs'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Text summarization example
  prefs: []
  type: TYPE_NORMAL
- en: Since many people spend hours per day reading and writing emails, summarizing
    them is a widely desired use case for GPT-3\. Let’s see how GPT-3 does with summarizing
    a three-paragraph email into one crisp line (Figure 2-19).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. Email summarization example
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the desired result, we pasted the full email and then simply added
    “one-sentence summary:'' at the end. We also included a “.” stop sequence to tell
    the model that it should stop its summary generation after a single sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Hey Greg,
  prefs: []
  type: TYPE_NORMAL
- en: We are Sandra and Shubham. We are the co-authors of a GPT-3 based book that
    aims to be a go-to manual for API users. It will provide the readers with the
    knowledge to get started with GPT-3 as well as the context of the growing ecosystem
    surrounding it. The book will be released by O’Reilly publishing house by early
    2022\.
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to connect and explore the possibility of collaboration with OpenAI
    to evangelize access to API for people from all backgrounds and enable them to
    build exciting new solutions using GPT-3\. We would appreciate your input to ensure
    we accurately represent OpenAI's stance on GPT-3 and inform our readers on the
    upcoming advancements/features of the API.
  prefs: []
  type: TYPE_NORMAL
- en: It will be great if we can catch up sometime this week to discuss it further?
  prefs: []
  type: TYPE_NORMAL
- en: Best,
  prefs: []
  type: TYPE_NORMAL
- en: S&S
  prefs: []
  type: TYPE_NORMAL
- en: 'one-sentence summary:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: We’re writing a book on GPT-3 and would like to connect with OpenAI to get a
    better understanding of the API and the future developments.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  prefs: []
  type: TYPE_NORMAL
- en: Before the introduction of GPT-3, there was a general understanding that AI
    models are capable of carrying a short conversation with humans that answer specific
    questions or handle specific tasks. However the models were not sophisticated
    enough to handle complicated text generation tasks and started to lose track whenever
    they encountered something complex or abstract.
  prefs: []
  type: TYPE_NORMAL
- en: In the complicated world of natural language generation, GPT-3 has shaken the
    notion of language models being limited to trivial tasks. Text generation is the
    greatest strength of GPT-3\. It is capable of generating textual content that
    is almost indistinguishable from human-written text. GPT-3 is trained on billions
    of words from the training dataset to generate text in response to a variety of
    prompts. It generates an average of 4.5 billion words per day, [according to OpenAI](https://openai.com/blog/gpt-3-apps/).
  prefs: []
  type: TYPE_NORMAL
- en: In the next two examples, we experiment with using GPT-3 to create content for
    a personal productivity app start-up and social media posts. We give the model
    only minimal context, and it generates many of the responses in Figures 2-21 and
    2-22 on the first take.
  prefs: []
  type: TYPE_NORMAL
- en: Article Generation
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. Article generation example
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: ​​Here are the top 5 tips for personal productivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Develop a routine
  prefs: []
  type: TYPE_NORMAL
- en: Work can be overwhelming. You don’t need to remember every little detail. By
    having a routine, you will be able to work more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Avoid multitasking
  prefs: []
  type: TYPE_NORMAL
- en: If you are multitasking, you are bound to miss something. It is better to work
    on one task at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Take breaks
  prefs: []
  type: TYPE_NORMAL
- en: It is true that if you do not take breaks, you will be more productive. However,
    if you take breaks too often you will get tired and stressed. If you work more
    effectively, you will shorten the time needed to complete a task.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Learn to delegate
  prefs: []
  type: TYPE_NORMAL
- en: If you have a lot of work to do, you should not do everything on your own. Delegate
    some of the work to others.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Prioritize
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize what you have to do and do the most important tasks first. Don’t
    worry about the small things.
  prefs: []
  type: TYPE_NORMAL
- en: Social Media Post Generation
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image-0-23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-22\. Social media post generation example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a social media post for Facebook, Twitter, and Instagram about launching
    a new beta version of the personal productivity app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Facebook post:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: “After extensive user testing, we are happy to announce that our latest update
    is available for download! Our team has worked hard to create the best version
    of our app yet. Download it now for free at www.app.com.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Twitter post:'
  prefs: []
  type: TYPE_NORMAL
- en: '“We''re rolling out an updated version of your favorite productivity app! Now
    available on iOS and Android. #productivity #beta”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instagram post:'
  prefs: []
  type: TYPE_NORMAL
- en: '“We just released a new beta version of your favorite productivity app! Download
    it today for free on iOS and Android. #productivity #beta”'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have covered the OpenAI playground, prompt engineering and
    the different components of the OpenAI API followed by playground examples covering
    the major NLP tasks. By now, you have an understanding of how the API works in
    tandem with different components and how to use Playground as the base to design
    and experiment with different training prompts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will walk you through how to use GPT-3 with different
    programming languages to integrate the API within your product or build a completely
    new application from scratch.
  prefs: []
  type: TYPE_NORMAL
