- en: Building Blocks of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the basic building blocks of a neural network, such as tensors,
    tensor operations, and gradient descent, is important for building complex neural
    networks. In this chapter, we will provide a general overview of neural networks
    and at the same time dive deep into the foundations of the PyTorch API. The original
    idea of a neural network was inspired by biological neurons that are found in
    the human brain, but, at the time of writing, the similarities between them are
    only superficial and any comparisons between the two systems may lead to incorrect
    assumptions about either of these systems. So, we won't ponder the similarities
    between the two systems and, instead, directly dive into the anatomy of the neural
    networks that are used in AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a neural network?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding PyTorch tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding tensor operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a neural network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In short, a neural network is an algorithm that learns about the relationship
    between the input variables and their associated target variables. For example,
    if you have a dataset consisting of students'' GPAs, GRE scores, the rank of the
    university, and the students'' admit status into a college, we can use a neural
    network to predict the admit status (target variable) of a student given their
    GPA, GRE scores, and the rank of the university (input variables):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b50745c-b26d-4788-82dc-2d9621b5afc1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, each of the arrows represents a weight. These weights
    are learned from the instances of training data, { ( (x1, y1), (x2, y2),..., (xm,
    ym) ) }, so that the composite feature that was created from the operations can
    predict the admit status of the student.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the network may learn the importance of GPA/GRE in terms of the
    rank of the institution, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b310c2d-99ad-4d4d-ae67-cc115371c46e.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the structure of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operations in a neural network are built from two underlying computations. One
    is a dot product between the weight vector and their corresponding input variable
    vector, while the other is a function that transforms the product into a non-linear
    space. We will learn about several types of these functions in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break this down further: The first dot product learns of a mixed concept
    since it creates a mixture of input variables that depend on the importance of
    each input variable. Passing these important features into a non-linear function
    allows us to build an output that is more powerful than just using a traditional
    linear combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/867a40d7-5261-460f-9c56-4c9f019a182e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By using these operations as building blocks, we can build robust neural networks.
    Let''s break down the neural network example from earlier; the neural network
    learns about features that, in turn, are the best predictors of the target variable.
    So, each layer of a neural network learns features that can help the neural network
    predict the target variable better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/403f4aa0-5ac4-45cd-96b0-8f1a3263f030.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see how the price of a house can be predicted
    using a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start off by building a neural network in PyTorch that will help us predict
    the admit status of a college student. There are two ways to build a neural network
    in PyTorch. First, we can use the simpler `torch.nn.Sequential` class, which allows
    us to pass the sequence of operations in our desired neural network as the argument,
    while instantiating our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other way, which is a more complex and powerful yet elegant approach, is
    to define our neural network as a class that inherits from the `torch.nn.Module`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/210dec47-aa4a-47d3-ac15-0bf205b43c13.png)'
  prefs: []
  type: TYPE_IMG
- en: We will build out our neural network using these two patterns, both of which
    are defined by the PyTorch API.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch sequential neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the commonly used operations in a neural network are available in the `torch.nn`
    module. Therefore, we need to start by importing the required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at how we build a neural network using the `torch.nn.Sequential`
    class. We use operations defined in the `torch.nn` module and pass them as arguments
    in a sequential fashion to the `torch.nn.Sequential` class to instantiate our
    neural network. After we import our operations, our neural network code should
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The most commonly used operation while building a neural network is the `nn.Linear()`
    operation. It takes two arguments: `in_features` and `out_features`. The `in_features`
    argument is the size of the input. In our case, we have three input features:
    GPA, GRE, and the rank of the university. The `out_features` argument is the size
    of the output, which, in our case, is two as we want to learn about two features
    from the input that can help us predict the admit status of a student. Essentially,
    the `nn.Linear(in_features, out_features)` operation takes the input and creates
    weight vectors to perform the dot product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, `nn.Linear(in_features` `= 3,` `out_features` `= 2)` would create
    two vectors: [w11, w12, w13] and [w21, w22, w23]. When the input, [xGRE, xGPA,
    xrank], is passed to the neural network, we would create a vector of two outputs,
    [h1, h2], which would be the result of [w11 . xGRE +  w12\.  xGPA + w13 .  xrank 
    ,  w21 . xGRE + w22 . xGPA  + w23 . xrank ].'
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern continues downstream when you want to keep adding more layers
    to your neural network. The following diagram shows the neural network''s structure
    after it''s been translated into the `nn.Linear()` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c7055d6-5e0d-4a87-a361-0aac3762905f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Great! But adding more linear operations does not exploit the power of neural
    networks. We also have to transform these outputs into a non-linear space using
    one of several non-linear functions. The types of these functions and the benefits
    and pitfalls of each of them will be described in more detail in the next chapter.
    For now, let''s use one of the most commonly used non-linear functions, that is,
    the **Rectified Linear Unit**, also known as **ReLU**. PyTorch provides us with
    an inbuilt ReLU operator that can be accessed by calling `nn.ReLU()`. The following diagram shows
    how non-linear functions can classify or solve learning problems where linear
    functions fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52e8414b-b952-4ac8-9a77-660005093bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, to get our predictions, we need to squash the output between 0 and
    1\. These states refer to a non-admit and an admit, respectively. The sigmoid
    function, as shown in the following diagram, is the most commonly used function
    to transform a continuous quantity between negative and positive infinity into
    a value between 0 and 1\. In PyTorch, we simply need to call the `nn.Sigmoid()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/911de9c9-979b-4f14-80df-9d889d683c91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s put our neural network code together in PyTorch so that we get
    a network that looks like what''s shown in the following diagram in terms of its
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aec6633-8406-45d1-acde-3959e9c14888.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for doing this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That's it! It's that simple to put a neural network together in PyTorch. The
    `my_neuralnet` Python object contains our neural network. We will look at how
    to use it a little later. For now, let's look at how to build neural networks
    using the more advanced API, which is based on defining a class that inherits
    from the `nn.Module` class.
  prefs: []
  type: TYPE_NORMAL
- en: Building a PyTorch neural network using nn.Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Defining a neural network using the `nn.Module` class is also simple and elegant.
    It starts by defining a class that will inherit from the `nn.Module` class and
    overrides two methods: the `__init__()` and `forward()` methods. The `__init__()`
    method should include the operations that are a part of the layers in our desired
    neural network. On the other hand, the `forward()` method should describe the
    flow of data through these desired layer operations. So, the structure of the
    code should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's understand this pattern in more detail. The `class` keyword helps define
    a Python class, followed by any arbitrary name you want your class to be. In this
    case, it is `MyNeuralNet`. Then, the argument that's passed in the parenthesis
    is the class that our currently defined class would inherit from. So, we always
    start off with the `MyNeuralNet(nn.Module)` class.
  prefs: []
  type: TYPE_NORMAL
- en: '`self` is the arbitrary first argument that is passed to every method defined
    in the class. It represents the instance of the class and can be used to access
    the attributes and methods defined in the class.'
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__()` method is a reserved method in Python classes. It is also known
    as a constructor. Whenever an object of the class is instantiated, the code wrapped
    inside the `__init__()` method is run. This helps us set up all our neural network
    operations once an object of our neural network class has been instantiated.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note is that, once we define the `__init__()` method inside our
    neural network class, we lose access to all of the code defined inside the `__init__()`
    method of the `nn.Module` class. Luckily, the Python `super()` function can help
    us run the code in the `__init__()` method of the `nn.Module` class. All we need
    to do is use the `super()` function in the first line inside our new `__init__()`
    method. Using the `super()` function to gain access to the `__init__()` method
    is pretty straightforward; we just use `super(NameOfClass, self).__init__()`.
    In our case, this would be `super(MyNeuralNet, self).__init__()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what goes into writing the first line of code for our `__init__()`
    method, let''s look at what other code we need to include in the definition of
    the `__init__()` method. We have to store the operations defined in PyTorch as
    attributes of `self`. In our case, we have two `nn.Linear` operations: one from
    the input variables to the two nodes in the neural network layer and another from
    these nodes to the output node. So, our `__init__()` method would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we store the operations for our desired neural network
    as attributes inside `self`. You should be comfortable with storing operations
    from PyTorch as attributes in `self`. The pattern that we use to do this is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there is one glaring mistake in the preceding code: the inputs to
    `nn.Linear` are hardcoded, so if the input size changes, we have to rewrite our
    neural network class again. Therefore, it''s good practice to use variable names
    instead and pass them as arguments when instantiating the object. The code would
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s dive into the implementation of the `forward()` method. This method
    takes two arguments: the `self` argument and the arbitrary `x` argument, which
    is a placeholder for our real data.'
  prefs: []
  type: TYPE_NORMAL
- en: We have already looked at the `nn.ReLU` operation, but there is also the more
    convenient functional interface defined in PyTorch that allows us to better describe
    the flow of data. It is important to note that these functional equivalents cannot
    be used inside the Sequential API. Our first job is to pass the data, which is
    represented by the `x` argument, to the first operation in our neural network.
    In PyTorch, passing the data to the first operation in our network is as simple
    as using `self.operationOne(x)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, using the PyTorch functional interface, we can pass the output of this
    operation to the nonlinear ReLU function by using `torch.nn.functional.relu.self.operationOne(x)`.
    Let''s put everything together and define the `forward()` method. It''s important
    to remember that the final output must be accompanied with the `return` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s polish and compile everything so that we can define our neural
    network in PyTorch using the class-based API. The following code is how you would
    find most PyTorch code in open source communities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to get access to our neural network, we have to instantiate an object
    of the `MyNeuralNet` class. We can do that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have access to our desired neural network with the `my_network` Python
    variable. We have built our neural network, so what's next? Can it predict the
    admit status of a student now? No. But we will get there. Before that, we need
    to understand how data should be represented in PyTorch so that our neural network
    understands it. That's where PyTorch Tensors come into play.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding PyTorch Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch Tensors is the engine that drives computation in PyTorch. If you have
    prior experience with Numpy, it'll be a breeze understanding PyTorch Tensors.
    Most of the patterns that you learned about with Numpy Arrays can be transformed
    into PyTorch Tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensors are data containers and are a generalized representation of vectors
    and matrices. A vector is a first-order tensor since it only has one axis and
    would look like [x1, x2, x3,..]. A matrix is a second-order tensor that has two
    axes and looks like [ [x11, x12, x13..] , [x21, x22, x23..] ]. On the other hand,
    a scalar is a zero-order tensor that only contains a single element, such as x1\.
    This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6aa9a85-08c1-4327-b7e8-fd7bdd989524.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can immediately observe that our dataset, which has columns of GPA, GRE,
    Rank, and Admit Status and rows of various observations, can be represented as
    a tensor of the second order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af640e04-447c-4b73-b733-1d9c6ea82470.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s quickly look at how to create PyTorch Tensors from Python lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Accessing elements from this container is also straightforward and the indexing
    starts at 0 and ends at n - 1, where n is the number of elements in the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`tensor(1)`, which we printed previously, is a zero-order tensor. Accessing
    multiple elements is similar to how we''d do this in NumPy and Python, where 0:2
    extracts elements starting at the element from index 0, up to, but not including,
    the element at index 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to access all the elements of the tensor starting at a particular
    index, you could use k:, where k is the index of the first element you want to
    extract:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s understand how second-order tensors work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Accessing elements from a second-order tensor is a little more complex. Now,
    let''s access element 12 from the tensor we created previously. It is important
    to view a second-order tensor as a tensor that''s being built from two first-order
    tensors, for example, [ [ tensor of order one ], [ tensor of order one ] ]. Element
    12 is inside the first tensor of order one and, within that tensor, it is at the
    second position, or index 1\. Therefore, we can access element 22 by using [0,
    1], where 0 describes the index of the tensor of order one and 1 describes the
    index of the element within the tensor of order one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s do a small mental exercise: how would you access element 23 from
    the tensor we created? Yes! You are right! We can access it using [1, 2].'
  prefs: []
  type: TYPE_NORMAL
- en: This same pattern holds for tensors of higher dimensions as well. It is important
    to note that the number of index positions you need to use to access the desired
    scalar element is equal to the order of the tensor. Let's carry out an exercise
    with a tensor of order 4!
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin, let's visualize a tensor of order 4; it must be composed of
    tensors of order 3\. Therefore, it must look similar to [ [tensor of order 3]
    , [tensor of order 3], [tensor of order 3]... ]. Each of these tensors of order
    3 must, in turn, be composed of tensors of order 2, which would look like [ [tensor
    of order 2], [tensor of order 2], [tensor of order 2], …], and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you will find a fourth-order tensor. It has been graciously spaced for
    the convenience of visualization. In this exercise, we need to access the elements
    1112, 1221, 2122, and 2221:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, the tensor is composed of two third-order tensors, each of which has two
    tensors of order 2, which in turn contain two tensors of order one. Let's look
    at how to access element 2122; the others are left for you to do in your spare
    time. Element 2122 is contained in the second tensor of order three in our original
    tensor [ [tensor of order 3], [*tensor of order 3] ]. So, the first indexing position
    is 1\. Next in the tensor of order 3, our desired element is in the first tensor
    of order 2 [ [*tensor of order 2], [tensor of order 2]]. So, the second indexing
    position is 0\. Inside the tensor of order 2, our desired element is in the second
    tensor of order one [ [tensor of order 1], [* tensor of order 1], so the indexing
    position is 1\. Finally, in the tensor of order one, our desired element is the
    second element [2121, 2122], for which the index position is 1\. When we put this
    all together, we can index element 2122 by using `fourth_order_tensor[1, 0, 1,
    1]`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Tensor shapes and reshaping Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to access elements from a Tensor, it is easy to understand
    Tensor shapes. All PyTorch Tensors have a `size()` method that describes the size
    of the Tensor across each of its axes. A PyTorch Tensor of order zero, or a scalar,
    does not have any axes, and so it doesn''t have a quantifiable size. Let''s look
    at the sizes of a few tensors in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Since there are five elements in the tensor along the first axis, the size
    of the Tensor is [5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Since the tensor of order two consists of two tensors of order one, the size
    of the first axis is 2 and each of the two tensors of order one consists of 3
    scalar elements, where the size along the second axis is 3\. Therefore, the size
    of the tensor is [2, 3].
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern scales to tensors of higher orders. Let''s complete a quick exercise
    with the `fourth_order_tensor` that we created in the preceding subsection. There
    are two tensors of order three, each of which has two tensors of order one, which
    in turn have two tensors of order one containing two scalar elements. Therefore,
    the size of the tensor is [2, 2, 2, 2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we understand tensor sizes, we can quickly generate tensors with random
    elements of our desired shape using `torch.rand()` and pass the desired tensor
    size as an argument to it. There are also other ways to generate tensors in PyTorch,
    which we''ll look at later in this book. The elements that are created in your
    tensor may vary from what you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'There will also be times when you want to reshape the tensor, that is, you
    would want to shift elements in the tensor to a different axis. We use the `.view()`
    method to reshape tensors. Let''s delve into a quick example of how we can do
    this in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to note that this is not an in-place operation and that the
    original `random_tensor` is still of the size [4, 2]. You will need to assign
    the returned value to actually store the result. Sometimes, when you have a lot
    of axes, you can use -1 so that PyTorch computes the size of one particular axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Understanding tensor operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've looked at the basic tensor properties, but what makes them so
    special is their ability to perform vectorized operations, which are extremely
    important to performant neural networks. Let's take a quick look at a few tensor
    operations that are available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The addition, subtraction, multiplication, and division operations are performed
    elementwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd185c74-2cad-4315-8799-e37da228ea2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a quick look at these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the +, -, *, and / operators to perform these operations on
    torch Tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a quick look at matrix multiplication in torch Tensors, which can
    be performed either using `torch.matmul()` or the `@` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a specific reason why we haven''t performed the division operation
    on the two tensors yet. Let''s do this now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: What? How was that possible? 5 / 3 should be approximately 1.667 and 3 / 2 should
    be 1.5\. But why did we get `tensor([1, 1])` as the result? If you guessed that
    it was because of the data type of the elements stored in the tensor, then you
    are absolutely right!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Tensor types in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch Tensors can only store elements of a single data type in a Tensor.
    There are also methods defined in PyTorch that require specific data types. Therefore,
    it is important to understand the data types a PyTorch Tensor can store. According
    to the PyTorch documentation, here are the datatypes that a PyTorch Tensor can
    store:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d43eb43a-47a1-4e7a-8eb1-316d84a3e3d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every PyTorch Tensor has a `dtype` attribute. Let''s look at the `dtype` of
    the tensors we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the data type of the elements stored in the tensors that
    we created was an int64\. Hence, the division that was performed between the elements
    was an integer division!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recreate the PyTorch tensors with 32-bit floating-point elements by
    passing the `dtype` argument to `torch.tensor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use `torch.FloatTensor()`, or the other names under the `tensor`
    column in the preceding screenshot, to directly create tensors of your desired
    type. You can also cast tensors to other data types using the `.type()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Importing our dataset as a PyTorch Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s import our `admit_status.csv` dataset as a PyTorch Tensor so that
    we can feed it to our neural network. To import our dataset, we will be using
    the NumPy library in Python. The dataset that we will work with can be seen in
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c11a11b3-1f2a-43dc-8b96-5c520abd4cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we import the dataset, we don''t want to import the first row, which are
    the column names. We will be using `np.genfromtext()` from the NumPy library to
    read our data as a numpy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/176e606b-204b-405b-87f8-8aee431430a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can directly import a numpy array as a PyTorch Tensor using `torch.from_numpy()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06ce88c0-349b-4be3-859b-340c30611f2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Training neural networks in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have our data as a PyTorch Tensor and we have our PyTorch neural network.
    Can we predict the admit status of a student now? No, not yet. First, we need
    to learn the specific weights that can help us predict the admit status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db55a7de-3df9-41b3-ad8f-d64ba0a3745b.png)'
  prefs: []
  type: TYPE_IMG
- en: The neural network we defined previously randomly generates weights at first.
    So, if we pass our data to the neural network directly, we would get meaningless
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The two important components in a neural network that help during the training
    process are the **Criterion** and the **Optimizer**. The Criterion generates a
    loss score that is proportional to how far away the predictions from the neural
    network are compared to the real ground truth, that is, the target variable, which
    in our case is the admit status.
  prefs: []
  type: TYPE_NORMAL
- en: The Optimizer takes this score to adjust the weights in the neural network so
    that the predictions from the network are as close to the ground truth as possible.
  prefs: []
  type: TYPE_NORMAL
- en: This iterative process of the Optimizer using the loss score from the Criterion
    to update the weights of the neural network is known as the training phase of
    a neural network. Now, we can train our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed and train our neural network, we have to split our dataset
    into inputs, x, and targets, y:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create an instance of the criterion and the optimizer so that we
    can train our neural network. There are multiple criteria built into PyTorch that
    are accessible from the `torch.nn` module. In this case, we will be using `BCELoss()`,
    also known as **binary cross-entropy loss**, which is used for binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several optimizers built into the `torch.optim` module in PyTorch.
    Here, we will be using the **SGD optimizer**, also known as the **stochastic gradient
    descent optimizer**. The optimizer takes the parameters, or the weights, of the
    neural network as an argument that can be accessed by using the `parameters()`
    method on the instance of the neural network we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to write a for loop that iterates over the process of updating the
    weights. First, we need to pass our data to get predictions from the neural network.
    This is very simple: we just need to pass the input data as an argument to the
    instance of the neural network with `y_pred = my_neuralnet(x_train)`. Then, we
    need to compute the loss score, which we arrive at by passing the predictions
    from the neural network and the ground truth to the criterion. We can do this
    with `loss_score = criterion(y_pred, y_train)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed to update the weights in our neural network, it is important
    to clear any gradients that have been accumulated from when we used the `zero_grad()`
    method on the optimizer. Then, to perform a backpropagation step, we use the `backward()`
    method on the computed `loss_score`. Finally, we update the parameters, or the
    weights, using the `step()` method on the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the previous logic must be in a loop, where we iterate over the training
    process until our network learns the best parameters. So, let''s put everything
    together into workable code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Voila! We have trained our neural network and it is ready to perform predictions.
    In the next chapter, we will delve into the various non-linear functions that
    are used in neural networks, the idea of validating what the neural network has
    learned, and dive deeper into the philosophy of building robust neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various data structures and operations provided
    by PyTorch. We implemented several components using the fundamental blocks of
    PyTorch. For the data preparation stage, we created the tensors that will be used
    by our algorithm. Our network architecture was a model that would learn to predict
    the average hours spent by users on our Wondermovies platform. We used the loss
    function to check the standard of our model and used the `optimize` function to
    adjust the learnable parameters of our model to make it perform better.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how PyTorch makes it easier for us to create data pipelines
    by abstracting away several complexities that would require us to parallelize
    and augment data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a deep dive into how neural networks and deep
    learning algorithms work. We will explore various built-in PyTorch modules for
    building network architectures, loss functions, and optimizations. We will also
    learn how to use them on real-world datasets.
  prefs: []
  type: TYPE_NORMAL
