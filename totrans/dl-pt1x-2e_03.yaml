- en: Building Blocks of Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的基本构建模块
- en: Understanding the basic building blocks of a neural network, such as tensors,
    tensor operations, and gradient descent, is important for building complex neural
    networks. In this chapter, we will provide a general overview of neural networks
    and at the same time dive deep into the foundations of the PyTorch API. The original
    idea of a neural network was inspired by biological neurons that are found in
    the human brain, but, at the time of writing, the similarities between them are
    only superficial and any comparisons between the two systems may lead to incorrect
    assumptions about either of these systems. So, we won't ponder the similarities
    between the two systems and, instead, directly dive into the anatomy of the neural
    networks that are used in AI.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 理解神经网络的基本构建模块，如张量、张量操作和梯度下降，对于构建复杂的神经网络至关重要。在本章中，我们将对神经网络进行一般性概述，同时深入探讨PyTorch
    API的基础。神经网络的原始想法受到人脑中的生物神经元的启发，但在撰写本文时，二者之间的相似性仅仅是表面的，对这两个系统的任何比较可能导致对其中任何一个系统的错误假设。因此，我们不会深究这两个系统之间的相似之处，而是直接深入探讨用于AI中的神经网络的解剖学。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is a neural network?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: Building a neural network in PyTorch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中构建神经网络
- en: Understanding PyTorch tensors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PyTorch张量
- en: Understanding tensor operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解张量操作
- en: What is a neural network?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: 'In short, a neural network is an algorithm that learns about the relationship
    between the input variables and their associated target variables. For example,
    if you have a dataset consisting of students'' GPAs, GRE scores, the rank of the
    university, and the students'' admit status into a college, we can use a neural
    network to predict the admit status (target variable) of a student given their
    GPA, GRE scores, and the rank of the university (input variables):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，神经网络是一种学习输入变量与其关联目标变量之间关系的算法。例如，如果您有一个数据集，其中包含学生的GPA、GRE分数、大学排名以及学生的录取状态，我们可以使用神经网络来预测学生在给定其GPA、GRE分数和大学排名的情况下的录取状态（目标变量）：
- en: '![](img/4b50745c-b26d-4788-82dc-2d9621b5afc1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b50745c-b26d-4788-82dc-2d9621b5afc1.png)'
- en: In the preceding diagram, each of the arrows represents a weight. These weights
    are learned from the instances of training data, { ( (x1, y1), (x2, y2),..., (xm,
    ym) ) }, so that the composite feature that was created from the operations can
    predict the admit status of the student.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图中，每个箭头代表一个权重。这些权重是从训练数据的实例中学习到的，{ ( (x1, y1), (x2, y2),..., (xm, ym) ) }，以便从操作中创建的复合特征能够预测学生的录取状态。
- en: 'For example, the network may learn the importance of GPA/GRE in terms of the
    rank of the institution, as shown in the following diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，网络可以学习GPA/GRE在院校排名中的重要性，如下图所示：
- en: '![](img/8b310c2d-99ad-4d4d-ae67-cc115371c46e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b310c2d-99ad-4d4d-ae67-cc115371c46e.png)'
- en: Understanding the structure of neural networks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的结构
- en: Operations in a neural network are built from two underlying computations. One
    is a dot product between the weight vector and their corresponding input variable
    vector, while the other is a function that transforms the product into a non-linear
    space. We will learn about several types of these functions in the next chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的操作由两个基础计算构建。一个是权重向量与其对应的输入变量向量之间的点积，另一个是将产品转换为非线性空间的函数。我们将在下一章节学习几种这些函数的类型。
- en: 'Let''s break this down further: The first dot product learns of a mixed concept
    since it creates a mixture of input variables that depend on the importance of
    each input variable. Passing these important features into a non-linear function
    allows us to build an output that is more powerful than just using a traditional
    linear combination:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分解：第一个点积学习到一个混合概念，因为它创建了依赖于每个输入变量重要性的输入变量的混合。将这些重要特征传递到非线性函数中允许我们构建比仅使用传统线性组合更强大的输出：
- en: '![](img/867a40d7-5261-460f-9c56-4c9f019a182e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/867a40d7-5261-460f-9c56-4c9f019a182e.png)'
- en: 'By using these operations as building blocks, we can build robust neural networks.
    Let''s break down the neural network example from earlier; the neural network
    learns about features that, in turn, are the best predictors of the target variable.
    So, each layer of a neural network learns features that can help the neural network
    predict the target variable better:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些操作作为构建模块，我们可以构建健壮的神经网络。让我们来详细分析之前的神经网络示例；神经网络学习关于特征的信息，这些特征反过来是目标变量的最佳预测因子。因此，神经网络的每一层都学习到可以帮助神经网络更好地预测目标变量的特征：
- en: '![](img/403f4aa0-5ac4-45cd-96b0-8f1a3263f030.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/403f4aa0-5ac4-45cd-96b0-8f1a3263f030.png)'
- en: In the preceding diagram, we can see how the price of a house can be predicted
    using a neural network.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到如何使用神经网络来预测房屋的价格。
- en: Building a neural network in PyTorch
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建神经网络
- en: Let's start off by building a neural network in PyTorch that will help us predict
    the admit status of a college student. There are two ways to build a neural network
    in PyTorch. First, we can use the simpler `torch.nn.Sequential` class, which allows
    us to pass the sequence of operations in our desired neural network as the argument,
    while instantiating our network.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在PyTorch中构建一个神经网络开始，它将帮助我们预测大学生的录取状态。在PyTorch中有两种构建神经网络的方式。首先，我们可以使用更简单的`torch.nn.Sequential`类，它允许我们将我们期望的神经网络操作序列作为参数传递给实例化我们的网络。
- en: 'The other way, which is a more complex and powerful yet elegant approach, is
    to define our neural network as a class that inherits from the `torch.nn.Module`
    class:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方式，这是一种更复杂、更强大但优雅的方法，是将我们的神经网络定义为从`torch.nn.Module`类继承的类：
- en: '![](img/210dec47-aa4a-47d3-ac15-0bf205b43c13.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/210dec47-aa4a-47d3-ac15-0bf205b43c13.png)'
- en: We will build out our neural network using these two patterns, both of which
    are defined by the PyTorch API.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用这两种模式来构建我们的神经网络，这两种模式都是由PyTorch API定义的。
- en: PyTorch sequential neural network
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch顺序神经网络
- en: 'All the commonly used operations in a neural network are available in the `torch.nn`
    module. Therefore, we need to start by importing the required modules:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中所有常用的操作都在`torch.nn`模块中可用。因此，我们需要从引入所需模块开始：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s look at how we build a neural network using the `torch.nn.Sequential`
    class. We use operations defined in the `torch.nn` module and pass them as arguments
    in a sequential fashion to the `torch.nn.Sequential` class to instantiate our
    neural network. After we import our operations, our neural network code should
    look as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用`torch.nn.Sequential`类构建神经网络。我们使用在`torch.nn`模块中定义的操作，并按顺序将它们作为参数传递给`torch.nn.Sequential`类，以实例化我们的神经网络。在我们导入操作之后，我们的神经网络代码应该如下所示：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The most commonly used operation while building a neural network is the `nn.Linear()`
    operation. It takes two arguments: `in_features` and `out_features`. The `in_features`
    argument is the size of the input. In our case, we have three input features:
    GPA, GRE, and the rank of the university. The `out_features` argument is the size
    of the output, which, in our case, is two as we want to learn about two features
    from the input that can help us predict the admit status of a student. Essentially,
    the `nn.Linear(in_features, out_features)` operation takes the input and creates
    weight vectors to perform the dot product.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建神经网络时最常用的操作是`nn.Linear()`操作。它接受两个参数：`in_features`和`out_features`。`in_features`参数是输入的大小。在我们的情况下，我们有三个输入特征：GPA、GRE和大学排名。`out_features`参数是输出的大小，对于我们来说是两个，因为我们想要从输入中学习两个特征，以帮助我们预测学生的录取状态。本质上，`nn.Linear(in_features,
    out_features)`操作接受输入并创建权重向量以执行点积。
- en: 'In our case, `nn.Linear(in_features` `= 3,` `out_features` `= 2)` would create
    two vectors: [w11, w12, w13] and [w21, w22, w23]. When the input, [xGRE, xGPA,
    xrank], is passed to the neural network, we would create a vector of two outputs,
    [h1, h2], which would be the result of [w11 . xGRE +  w12\.  xGPA + w13 .  xrank 
    ,  w21 . xGRE + w22 . xGPA  + w23 . xrank ].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，`nn.Linear(in_features = 3, out_features = 2)`会创建两个向量：[w11, w12, w13]
    和 [w21, w22, w23]。当输入 [xGRE, xGPA, xrank] 被传递到神经网络时，我们将创建一个包含两个输出的向量 [h1, h2]，其结果为
    [w11 . xGRE +  w12 . xGPA + w13 . xrank ,  w21 . xGRE + w22 . xGPA + w23 . xrank]。
- en: 'This pattern continues downstream when you want to keep adding more layers
    to your neural network. The following diagram shows the neural network''s structure
    after it''s been translated into the `nn.Linear()` operations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要继续向你的神经网络中添加更多层时，这种模式会继续下游。下图显示了被转换为`nn.Linear()`操作后的神经网络结构：
- en: '![](img/3c7055d6-5e0d-4a87-a361-0aac3762905f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c7055d6-5e0d-4a87-a361-0aac3762905f.png)'
- en: 'Great! But adding more linear operations does not exploit the power of neural
    networks. We also have to transform these outputs into a non-linear space using
    one of several non-linear functions. The types of these functions and the benefits
    and pitfalls of each of them will be described in more detail in the next chapter.
    For now, let''s use one of the most commonly used non-linear functions, that is,
    the **Rectified Linear Unit**, also known as **ReLU**. PyTorch provides us with
    an inbuilt ReLU operator that can be accessed by calling `nn.ReLU()`. The following diagram shows
    how non-linear functions can classify or solve learning problems where linear
    functions fail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！但是添加更多的线性操作并不能充分利用神经网络的能力。我们还必须使用几种非线性函数之一将这些输出转换为非线性空间。这些函数的类型以及每个函数的优点和缺点将在下一章节中更详细地描述。现在，让我们使用其中一种最常用的非线性函数之一，即
    **修正线性单元**，也称为 **ReLU**。PyTorch 通过调用 `nn.ReLU()` 提供了一个内置的 ReLU 操作符。以下图展示了非线性函数如何分类或解决线性函数失败的学习问题：
- en: '![](img/52e8414b-b952-4ac8-9a77-660005093bcb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52e8414b-b952-4ac8-9a77-660005093bcb.png)'
- en: 'Finally, to get our predictions, we need to squash the output between 0 and
    1\. These states refer to a non-admit and an admit, respectively. The sigmoid
    function, as shown in the following diagram, is the most commonly used function
    to transform a continuous quantity between negative and positive infinity into
    a value between 0 and 1\. In PyTorch, we simply need to call the `nn.Sigmoid()` operation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了获得我们的预测结果，我们需要将输出压缩到 0 到 1 之间。这些状态分别指非录取和录取。Sigmoid 函数，如下图所示，是将连续量转换为介于
    0 和 1 之间的值最常用的函数。在 PyTorch 中，我们只需调用 `nn.Sigmoid()` 操作：
- en: '![](img/911de9c9-979b-4f14-80df-9d889d683c91.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/911de9c9-979b-4f14-80df-9d889d683c91.png)'
- en: 'Now, let''s put our neural network code together in PyTorch so that we get
    a network that looks like what''s shown in the following diagram in terms of its
    structure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 PyTorch 中将我们的神经网络代码整合起来，以便获得一个结构如下图所示的网络：
- en: '![](img/9aec6633-8406-45d1-acde-3959e9c14888.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9aec6633-8406-45d1-acde-3959e9c14888.png)'
- en: 'The code for doing this is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个操作的代码如下：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That's it! It's that simple to put a neural network together in PyTorch. The
    `my_neuralnet` Python object contains our neural network. We will look at how
    to use it a little later. For now, let's look at how to build neural networks
    using the more advanced API, which is based on defining a class that inherits
    from the `nn.Module` class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在 PyTorch 中组合一个神经网络就是这么简单。`my_neuralnet` Python 对象包含了我们的神经网络。稍后我们将看看如何使用它。现在，让我们看看如何使用基于定义从
    `nn.Module` 类继承的类的更高级 API 来构建神经网络。
- en: Building a PyTorch neural network using nn.Module
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `nn.Module` 构建 PyTorch 神经网络
- en: 'Defining a neural network using the `nn.Module` class is also simple and elegant.
    It starts by defining a class that will inherit from the `nn.Module` class and
    overrides two methods: the `__init__()` and `forward()` methods. The `__init__()`
    method should include the operations that are a part of the layers in our desired
    neural network. On the other hand, the `forward()` method should describe the
    flow of data through these desired layer operations. So, the structure of the
    code should look similar to the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `nn.Module` 类定义神经网络也是简单而优雅的。它通过定义一个将继承自 `nn.Module` 类并重写两个方法的类开始：`__init__()`
    和 `forward()` 方法。`__init__()` 方法应包含我们期望的神经网络层中的操作。另一方面，`forward()` 方法应描述数据通过这些期望的层操作的流动。因此，代码的结构应类似于以下内容：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's understand this pattern in more detail. The `class` keyword helps define
    a Python class, followed by any arbitrary name you want your class to be. In this
    case, it is `MyNeuralNet`. Then, the argument that's passed in the parenthesis
    is the class that our currently defined class would inherit from. So, we always
    start off with the `MyNeuralNet(nn.Module)` class.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这种模式。`class` 关键字帮助定义一个 Python 类，后面跟着你想要为你的类使用的任意名称。在这种情况下，它是 `MyNeuralNet`。然后，括号中传递的参数是我们当前定义的类将继承的类。因此，我们始终从
    `MyNeuralNet(nn.Module)` 类开始。
- en: '`self` is the arbitrary first argument that is passed to every method defined
    in the class. It represents the instance of the class and can be used to access
    the attributes and methods defined in the class.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`self` 是传递给类中定义的每个方法的任意第一个参数。它表示类的实例，并可用于访问类中定义的属性和方法。'
- en: The `__init__()` method is a reserved method in Python classes. It is also known
    as a constructor. Whenever an object of the class is instantiated, the code wrapped
    inside the `__init__()` method is run. This helps us set up all our neural network
    operations once an object of our neural network class has been instantiated.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__()` 方法是 Python 类中的一个保留方法。它也被称为构造函数。每当实例化类的对象时，`__init__()` 方法中包装的代码将被运行。这帮助我们一旦实例化了我们的神经网络类的对象，就设置好所有的神经网络操作。'
- en: One thing to note is that, once we define the `__init__()` method inside our
    neural network class, we lose access to all of the code defined inside the `__init__()`
    method of the `nn.Module` class. Luckily, the Python `super()` function can help
    us run the code in the `__init__()` method of the `nn.Module` class. All we need
    to do is use the `super()` function in the first line inside our new `__init__()`
    method. Using the `super()` function to gain access to the `__init__()` method
    is pretty straightforward; we just use `super(NameOfClass, self).__init__()`.
    In our case, this would be `super(MyNeuralNet, self).__init__()`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，一旦我们在神经网络类内部定义了 `__init__()` 方法，我们就无法访问 `nn.Module` 类的 `__init__()`
    方法中定义的所有代码了。幸运的是，Python 的 `super()` 函数可以帮助我们运行 `nn.Module` 类中的 `__init__()` 方法中的代码。我们只需要在新的
    `__init__()` 方法的第一行中使用 `super()` 函数。使用 `super()` 函数来访问 `__init__()` 方法非常简单；我们只需使用
    `super(NameOfClass, self).__init__()`。在我们的情况下，这将是 `super(MyNeuralNet, self).__init__()`。
- en: 'Now that we know what goes into writing the first line of code for our `__init__()`
    method, let''s look at what other code we need to include in the definition of
    the `__init__()` method. We have to store the operations defined in PyTorch as
    attributes of `self`. In our case, we have two `nn.Linear` operations: one from
    the input variables to the two nodes in the neural network layer and another from
    these nodes to the output node. So, our `__init__()` method would look as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何编写我们的 `__init__()` 方法的第一行代码，让我们看看我们需要在 `__init__()` 方法的定义中包含哪些其他代码。我们必须将
    PyTorch 中定义的操作存储为 `self` 的属性。在我们的情况下，我们有两个 `nn.Linear` 操作：一个从输入变量到神经网络层中的两个节点，另一个从这些节点到输出节点。因此，我们的
    `__init__()` 方法如下所示：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code, we store the operations for our desired neural network
    as attributes inside `self`. You should be comfortable with storing operations
    from PyTorch as attributes in `self`. The pattern that we use to do this is as
    follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们将所需神经网络的操作存储为 `self` 的属性。您应该习惯将 PyTorch 中的操作存储为 `self` 中的属性。我们用来执行此操作的模式如下：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, there is one glaring mistake in the preceding code: the inputs to
    `nn.Linear` are hardcoded, so if the input size changes, we have to rewrite our
    neural network class again. Therefore, it''s good practice to use variable names
    instead and pass them as arguments when instantiating the object. The code would
    look as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在上述代码中存在一个明显的错误：`nn.Linear` 的输入是硬编码的，因此如果输入大小发生变化，我们就必须重新编写我们的神经网络类。因此，在实例化对象时，使用变量名而不是硬编码是一个很好的做法，并将它们作为参数传递。代码如下所示：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s dive into the implementation of the `forward()` method. This method
    takes two arguments: the `self` argument and the arbitrary `x` argument, which
    is a placeholder for our real data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解 `forward()` 方法的实现。此方法接受两个参数：`self` 参数和任意的 `x` 参数，这是我们实际数据的占位符。
- en: We have already looked at the `nn.ReLU` operation, but there is also the more
    convenient functional interface defined in PyTorch that allows us to better describe
    the flow of data. It is important to note that these functional equivalents cannot
    be used inside the Sequential API. Our first job is to pass the data, which is
    represented by the `x` argument, to the first operation in our neural network.
    In PyTorch, passing the data to the first operation in our network is as simple
    as using `self.operationOne(x)`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过 `nn.ReLU` 操作，但 PyTorch 中还有更方便的函数接口，允许我们更好地描述数据流。需要注意的是，这些函数等效物不能在 Sequential
    API 中使用。我们的第一项工作是将数据传递给由 `x` 参数表示的神经网络中的第一个操作。在 PyTorch 中，将数据传递给我们网络中的第一个操作就像简单地使用
    `self.operationOne(x)` 一样。
- en: 'Then, using the PyTorch functional interface, we can pass the output of this
    operation to the nonlinear ReLU function by using `torch.nn.functional.relu.self.operationOne(x)`.
    Let''s put everything together and define the `forward()` method. It''s important
    to remember that the final output must be accompanied with the `return` keyword:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用PyTorch的功能接口，我们可以通过`torch.nn.functional.relu.self.operationOne(x)`将此操作的输出传递给非线性ReLU函数。让我们把一切都放在一起，并定义`forward()`方法。重要的是要记住最终输出必须伴随着`return`关键字：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s polish and compile everything so that we can define our neural
    network in PyTorch using the class-based API. The following code is how you would
    find most PyTorch code in open source communities:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行精加工和编译，以便使用基于类的API在PyTorch中定义我们的神经网络。以下代码展示了您在开源社区中找到的大部分PyTorch代码：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, to get access to our neural network, we have to instantiate an object
    of the `MyNeuralNet` class. We can do that as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了访问我们的神经网络，我们必须实例化`MyNeuralNet`类的对象。我们可以这样做：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we have access to our desired neural network with the `my_network` Python
    variable. We have built our neural network, so what's next? Can it predict the
    admit status of a student now? No. But we will get there. Before that, we need
    to understand how data should be represented in PyTorch so that our neural network
    understands it. That's where PyTorch Tensors come into play.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过`my_network` Python变量访问我们想要的神经网络。我们已经构建了我们的神经网络，那么接下来呢？它现在能预测学生的录取状态吗？不行。但我们会到达那里。在此之前，我们需要了解如何在PyTorch中表示数据，以便我们的神经网络能够理解。这就是PyTorch张量发挥作用的地方。
- en: Understanding PyTorch Tensors
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解PyTorch张量
- en: PyTorch Tensors is the engine that drives computation in PyTorch. If you have
    prior experience with Numpy, it'll be a breeze understanding PyTorch Tensors.
    Most of the patterns that you learned about with Numpy Arrays can be transformed
    into PyTorch Tensors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量是驱动PyTorch计算的引擎。如果您之前有使用Numpy的经验，理解PyTorch张量将会轻而易举。大多数您在Numpy数组中学到的模式可以转换为PyTorch张量。
- en: 'Tensors are data containers and are a generalized representation of vectors
    and matrices. A vector is a first-order tensor since it only has one axis and
    would look like [x1, x2, x3,..]. A matrix is a second-order tensor that has two
    axes and looks like [ [x11, x12, x13..] , [x21, x22, x23..] ]. On the other hand,
    a scalar is a zero-order tensor that only contains a single element, such as x1\.
    This is shown in the following diagram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是数据容器，是向量和矩阵的广义表示。向量是一阶张量，因为它只有一个轴，看起来像[x1, x2, x3..]。矩阵是二阶张量，它有两个轴，看起来像[[x11,
    x12, x13..], [x21, x22, x23..]]。另一方面，标量是零阶张量，只包含单个元素，如x1。这在下图中显示：
- en: '![](img/c6aa9a85-08c1-4327-b7e8-fd7bdd989524.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6aa9a85-08c1-4327-b7e8-fd7bdd989524.png)'
- en: 'We can immediately observe that our dataset, which has columns of GPA, GRE,
    Rank, and Admit Status and rows of various observations, can be represented as
    a tensor of the second order:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即观察到，我们的数据集，其中包含GPA、GRE、排名和录取状态列，以及各种观察行，可以表示为二阶张量：
- en: '![](img/af640e04-447c-4b73-b733-1d9c6ea82470.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af640e04-447c-4b73-b733-1d9c6ea82470.png)'
- en: 'Let''s quickly look at how to create PyTorch Tensors from Python lists:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下如何从Python列表创建PyTorch张量：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Accessing elements from this container is also straightforward and the indexing
    starts at 0 and ends at n - 1, where n is the number of elements in the container:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 访问该容器中的元素也很简单，索引从0开始，以n - 1结束，其中n是容器中的元素数目：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`tensor(1)`, which we printed previously, is a zero-order tensor. Accessing
    multiple elements is similar to how we''d do this in NumPy and Python, where 0:2
    extracts elements starting at the element from index 0, up to, but not including,
    the element at index 2:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensor(1)`，我们之前打印过的，是一个零阶张量。访问多个元素类似于在NumPy和Python中的方式，其中0:2提取从索引0开始的元素，但不包括索引2处的元素：'
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you want to access all the elements of the tensor starting at a particular
    index, you could use k:, where k is the index of the first element you want to
    extract:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想访问从特定索引开始的张量的所有元素，你可以使用k:，其中k是你想提取的第一个元素的索引：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s understand how second-order tensors work:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解一下二阶张量的工作原理：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Accessing elements from a second-order tensor is a little more complex. Now,
    let''s access element 12 from the tensor we created previously. It is important
    to view a second-order tensor as a tensor that''s being built from two first-order
    tensors, for example, [ [ tensor of order one ], [ tensor of order one ] ]. Element
    12 is inside the first tensor of order one and, within that tensor, it is at the
    second position, or index 1\. Therefore, we can access element 22 by using [0,
    1], where 0 describes the index of the tensor of order one and 1 describes the
    index of the element within the tensor of order one:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个二阶张量中访问元素稍微复杂一些。现在，让我们从之前创建的张量中访问元素12。重要的是将二阶张量视为由两个一阶张量构成的张量，例如，[[一阶张量],
    [一阶张量]]。元素12位于第一个一阶张量内部，并且在该张量内部位于第二个位置，即索引1。因此，我们可以使用[0, 1]来访问元素22，其中0描述了一阶张量的索引，1描述了一阶张量内部元素的索引：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let''s do a small mental exercise: how would you access element 23 from
    the tensor we created? Yes! You are right! We can access it using [1, 2].'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们做一个小的思维练习：如何从我们创建的张量中访问第23个元素？是的！你是对的！我们可以使用[1, 2]来访问它。
- en: This same pattern holds for tensors of higher dimensions as well. It is important
    to note that the number of index positions you need to use to access the desired
    scalar element is equal to the order of the tensor. Let's carry out an exercise
    with a tensor of order 4!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高维度的张量，这个模式同样适用。需要注意的是，你需要使用的索引位置数目等于张量的阶数。让我们来做一个四阶张量的练习！
- en: Before we begin, let's visualize a tensor of order 4; it must be composed of
    tensors of order 3\. Therefore, it must look similar to [ [tensor of order 3]
    , [tensor of order 3], [tensor of order 3]... ]. Each of these tensors of order
    3 must, in turn, be composed of tensors of order 2, which would look like [ [tensor
    of order 2], [tensor of order 2], [tensor of order 2], …], and so on.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们想象一个四阶张量；它必须由三阶张量组成。因此，它看起来应该类似于[[张量的三阶]，[张量的三阶]，[张量的三阶]…]。每个这些三阶张量必须依次由二阶张量组成，看起来像[[张量的二阶]，[张量的二阶]，[张量的二阶]，…]，依此类推。
- en: 'Here, you will find a fourth-order tensor. It has been graciously spaced for
    the convenience of visualization. In this exercise, we need to access the elements
    1112, 1221, 2122, and 2221:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你会找到一个四阶张量。为了便于可视化，它已经得到了合理的间隔。在这个练习中，我们需要访问元素1112, 1221, 2122 和 2221：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, the tensor is composed of two third-order tensors, each of which has two
    tensors of order 2, which in turn contain two tensors of order one. Let's look
    at how to access element 2122; the others are left for you to do in your spare
    time. Element 2122 is contained in the second tensor of order three in our original
    tensor [ [tensor of order 3], [*tensor of order 3] ]. So, the first indexing position
    is 1\. Next in the tensor of order 3, our desired element is in the first tensor
    of order 2 [ [*tensor of order 2], [tensor of order 2]]. So, the second indexing
    position is 0\. Inside the tensor of order 2, our desired element is in the second
    tensor of order one [ [tensor of order 1], [* tensor of order 1], so the indexing
    position is 1\. Finally, in the tensor of order one, our desired element is the
    second element [2121, 2122], for which the index position is 1\. When we put this
    all together, we can index element 2122 by using `fourth_order_tensor[1, 0, 1,
    1]`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，张量由两个三阶张量组成，每个张量都有两个二阶张量，而每个二阶张量又包含两个一阶张量。让我们看看如何访问元素2122；其余的留给你在空闲时间里完成。元素2122包含在我们原始张量的第二个三阶张量中[[张量的三阶],
    [*张量的三阶]]。所以，第一个索引位置是1。接下来在三阶张量中，我们想要的元素在第一个二阶张量内[[*二阶张量], [二阶张量]]。因此，第二个索引位置是0。在二阶张量内部，我们想要的元素在第二个一阶张量中[[张量的一阶],
    [*张量的一阶]]，所以索引位置是1。最后，在一阶张量中，我们想要的元素是第二个元素[2121, 2122]，索引位置是1。当我们把这些放在一起时，我们可以使用`fourth_order_tensor[1,
    0, 1, 1]`来索引元素2122。
- en: Understanding Tensor shapes and reshaping Tensors
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解张量的形状和重塑张量
- en: 'Now that we know how to access elements from a Tensor, it is easy to understand
    Tensor shapes. All PyTorch Tensors have a `size()` method that describes the size
    of the Tensor across each of its axes. A PyTorch Tensor of order zero, or a scalar,
    does not have any axes, and so it doesn''t have a quantifiable size. Let''s look
    at the sizes of a few tensors in PyTorch:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从张量中访问元素，理解张量形状就很容易了。所有 PyTorch 张量都有一个 `size()` 方法，描述了张量在每个轴上的尺寸。零阶张量，即标量，没有任何轴，因此没有可量化的尺寸。让我们看一下
    PyTorch 中几个张量的尺寸：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since there are five elements in the tensor along the first axis, the size
    of the Tensor is [5]:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于张量沿着第一个轴有五个元素，张量的尺寸是 [5]：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since the tensor of order two consists of two tensors of order one, the size
    of the first axis is 2 and each of the two tensors of order one consists of 3
    scalar elements, where the size along the second axis is 3\. Therefore, the size
    of the tensor is [2, 3].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于二阶张量包含两个一阶张量，第一个轴的尺寸是 2，每个一阶张量包含 3 个标量元素，第二个轴的尺寸是 3。因此，张量的尺寸是 [2, 3]。
- en: 'This pattern scales to tensors of higher orders. Let''s complete a quick exercise
    with the `fourth_order_tensor` that we created in the preceding subsection. There
    are two tensors of order three, each of which has two tensors of order one, which
    in turn have two tensors of order one containing two scalar elements. Therefore,
    the size of the tensor is [2, 2, 2, 2]:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式可以扩展到更高阶的张量。让我们完成一个关于在前一小节中创建的 `fourth_order_tensor` 的快速练习。有两个三阶张量，每个三阶张量有两个一阶张量，这些一阶张量又包含两个一阶张量，每个一阶张量包含两个标量元素。因此，张量的尺寸是
    [2, 2, 2, 2]：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we understand tensor sizes, we can quickly generate tensors with random
    elements of our desired shape using `torch.rand()` and pass the desired tensor
    size as an argument to it. There are also other ways to generate tensors in PyTorch,
    which we''ll look at later in this book. The elements that are created in your
    tensor may vary from what you can see here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了张量的尺寸，我们可以使用 `torch.rand()` 快速生成具有所需形状的随机元素张量。在本书的后续部分中，我们还会看到其他生成张量的方法。在你的张量中创建的元素可能与这里看到的不同：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There will also be times when you want to reshape the tensor, that is, you
    would want to shift elements in the tensor to a different axis. We use the `.view()`
    method to reshape tensors. Let''s delve into a quick example of how we can do
    this in PyTorch:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你可能希望重塑张量，即将张量中的元素移动到不同的轴上。我们使用 `.view()` 方法来重塑张量。让我们深入一个快速的例子，展示如何在 PyTorch
    中完成这个操作：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It is important to note that this is not an in-place operation and that the
    original `random_tensor` is still of the size [4, 2]. You will need to assign
    the returned value to actually store the result. Sometimes, when you have a lot
    of axes, you can use -1 so that PyTorch computes the size of one particular axis:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这不是一个原地操作，并且原始的 `random_tensor` 仍然是尺寸为 [4, 2] 的。你需要将返回的值赋值给变量以存储结果。有时，当你有很多轴时，可以使用
    -1 让 PyTorch 计算特定轴的尺寸：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Understanding tensor operations
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解张量操作
- en: So far, we've looked at the basic tensor properties, but what makes them so
    special is their ability to perform vectorized operations, which are extremely
    important to performant neural networks. Let's take a quick look at a few tensor
    operations that are available in PyTorch.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过了基本的张量属性，但是使它们如此特殊的是它们执行向量化操作的能力，这对于高效的神经网络非常重要。让我们快速看一下 PyTorch
    中可用的一些张量操作。
- en: 'The addition, subtraction, multiplication, and division operations are performed
    elementwise:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 加法、减法、乘法和除法操作是按元素执行的：
- en: '![](img/cd185c74-2cad-4315-8799-e37da228ea2c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd185c74-2cad-4315-8799-e37da228ea2c.png)'
- en: 'Let''s take a quick look at these operations:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下这些操作：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can also use the +, -, *, and / operators to perform these operations on
    torch Tensors:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 +、-、* 和 / 运算符在 torch 张量上执行这些操作：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s take a quick look at matrix multiplication in torch Tensors, which can
    be performed either using `torch.matmul()` or the `@` operator:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下 torch 张量中的矩阵乘法，可以使用 `torch.matmul()` 或 `@` 运算符来执行：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'There is a specific reason why we haven''t performed the division operation
    on the two tensors yet. Let''s do this now:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个特定的原因，为什么我们还没有对两个张量执行除法操作。现在让我们来做这个操作：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What? How was that possible? 5 / 3 should be approximately 1.667 and 3 / 2 should
    be 1.5\. But why did we get `tensor([1, 1])` as the result? If you guessed that
    it was because of the data type of the elements stored in the tensor, then you
    are absolutely right!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 什么？那怎么可能？5 / 3 应该约为 1.667，而 3 / 2 应该是 1.5。但为什么我们得到`tensor([1, 1])`作为结果？如果你猜到这是因为张量中存储的元素的数据类型，那你绝对是对的！
- en: Understanding Tensor types in PyTorch
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 PyTorch 中的张量类型
- en: 'PyTorch Tensors can only store elements of a single data type in a Tensor.
    There are also methods defined in PyTorch that require specific data types. Therefore,
    it is important to understand the data types a PyTorch Tensor can store. According
    to the PyTorch documentation, here are the datatypes that a PyTorch Tensor can
    store:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量只能存储单一数据类型的元素。PyTorch 中还定义了需要特定数据类型的方法。因此，了解 PyTorch 张量可以存储的数据类型非常重要。根据
    PyTorch 文档，以下是 PyTorch 张量可以存储的数据类型：
- en: '![](img/d43eb43a-47a1-4e7a-8eb1-316d84a3e3d4.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d43eb43a-47a1-4e7a-8eb1-316d84a3e3d4.png)'
- en: 'Every PyTorch Tensor has a `dtype` attribute. Let''s look at the `dtype` of
    the tensors we created previously:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 PyTorch 张量都有一个`dtype`属性。让我们来看看之前创建的张量的`dtype`：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we can see that the data type of the elements stored in the tensors that
    we created was an int64\. Hence, the division that was performed between the elements
    was an integer division!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们创建的张量中存储的元素的数据类型是 int64。因此，元素之间执行的除法是整数除法！
- en: 'Let''s recreate the PyTorch tensors with 32-bit floating-point elements by
    passing the `dtype` argument to `torch.tensor()`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`torch.tensor()`中传递`dtype`参数，让我们重新创建具有 32 位浮点元素的 PyTorch 张量：
- en: '[PRE28]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also use `torch.FloatTensor()`, or the other names under the `tensor`
    column in the preceding screenshot, to directly create tensors of your desired
    type. You can also cast tensors to other data types using the `.type()` method:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`torch.FloatTensor()`或前述截图中`tensor`列下的其他名称，直接创建所需类型的张量。你也可以使用`.type()`方法将张量转换为其他数据类型：
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Importing our dataset as a PyTorch Tensor
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将我们的数据集作为 PyTorch 张量导入
- en: 'Now, let''s import our `admit_status.csv` dataset as a PyTorch Tensor so that
    we can feed it to our neural network. To import our dataset, we will be using
    the NumPy library in Python. The dataset that we will work with can be seen in
    the following image:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将`admit_status.csv`数据集作为 PyTorch 张量导入，以便我们可以将其馈送到我们的神经网络中。为了导入我们的数据集，我们将使用
    Python 中的 NumPy 库。我们将要处理的数据集如下图所示：
- en: '![](img/c11a11b3-1f2a-43dc-8b96-5c520abd4cb0.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c11a11b3-1f2a-43dc-8b96-5c520abd4cb0.png)'
- en: 'When we import the dataset, we don''t want to import the first row, which are
    the column names. We will be using `np.genfromtext()` from the NumPy library to
    read our data as a numpy array:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们导入数据集时，我们不想导入第一行，即列名。我们将使用 NumPy 库中的`np.genfromtext()`来将数据读取为一个 numpy 数组：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will give us the following output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![](img/176e606b-204b-405b-87f8-8aee431430a9.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/176e606b-204b-405b-87f8-8aee431430a9.png)'
- en: 'We can directly import a numpy array as a PyTorch Tensor using `torch.from_numpy()`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torch.from_numpy()`直接将 numpy 数组导入为 PyTorch 张量：
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will give us the following output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![](img/06ce88c0-349b-4be3-859b-340c30611f2e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06ce88c0-349b-4be3-859b-340c30611f2e.png)'
- en: Training neural networks in PyTorch
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中训练神经网络
- en: 'We have our data as a PyTorch Tensor and we have our PyTorch neural network.
    Can we predict the admit status of a student now? No, not yet. First, we need
    to learn the specific weights that can help us predict the admit status:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将数据作为 PyTorch 张量，也有了 PyTorch 神经网络。我们现在可以预测学生的录取状态了吗？不，还不行。首先，我们需要学习可以帮助我们预测录取状态的具体权重：
- en: '![](img/db55a7de-3df9-41b3-ad8f-d64ba0a3745b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db55a7de-3df9-41b3-ad8f-d64ba0a3745b.png)'
- en: The neural network we defined previously randomly generates weights at first.
    So, if we pass our data to the neural network directly, we would get meaningless
    predictions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义的神经网络首先随机生成权重。因此，如果我们直接将数据传递给神经网络，我们将得到毫无意义的预测结果。
- en: The two important components in a neural network that help during the training
    process are the **Criterion** and the **Optimizer**. The Criterion generates a
    loss score that is proportional to how far away the predictions from the neural
    network are compared to the real ground truth, that is, the target variable, which
    in our case is the admit status.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中两个在训练过程中起作用的重要组件是**Criterion**和**Optimizer**。Criterion 生成一个损失分数，该分数与神经网络的预测与真实目标值之间的差距成正比，即我们的情况下是录取状态。
- en: The Optimizer takes this score to adjust the weights in the neural network so
    that the predictions from the network are as close to the ground truth as possible.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器使用这个分数来调整神经网络中的权重，使网络的预测尽可能接近真实值。
- en: This iterative process of the Optimizer using the loss score from the Criterion
    to update the weights of the neural network is known as the training phase of
    a neural network. Now, we can train our neural network.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器使用 Criterion 的损失分数来更新神经网络的权重的迭代过程被称为神经网络的训练阶段。现在，我们可以训练我们的神经网络。
- en: 'Before we proceed and train our neural network, we have to split our dataset
    into inputs, x, and targets, y:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续训练我们的神经网络之前，我们必须将数据集分割为输入 `x` 和目标 `y`：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We need to create an instance of the criterion and the optimizer so that we
    can train our neural network. There are multiple criteria built into PyTorch that
    are accessible from the `torch.nn` module. In this case, we will be using `BCELoss()`,
    also known as **binary cross-entropy loss**, which is used for binary classification:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建 Criterion 和 Optimizer 的实例，以便训练我们的神经网络。PyTorch 中内置了多个 Criterion，可以从 `torch.nn`
    模块中访问。在这种情况下，我们将使用 `BCELoss()`，也被称为**二进制交叉熵损失**，用于二元分类：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'There are several optimizers built into the `torch.optim` module in PyTorch.
    Here, we will be using the **SGD optimizer**, also known as the **stochastic gradient
    descent optimizer**. The optimizer takes the parameters, or the weights, of the
    neural network as an argument that can be accessed by using the `parameters()`
    method on the instance of the neural network we created previously:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，`torch.optim` 模块内置了几种优化器。在这里，我们将使用**SGD 优化器**，也被称为**随机梯度下降优化器**。该优化器接受神经网络的参数或权重作为参数，并可以通过在之前创建的神经网络实例上使用
    `parameters()` 方法来访问：
- en: '[PRE34]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have to write a for loop that iterates over the process of updating the
    weights. First, we need to pass our data to get predictions from the neural network.
    This is very simple: we just need to pass the input data as an argument to the
    instance of the neural network with `y_pred = my_neuralnet(x_train)`. Then, we
    need to compute the loss score, which we arrive at by passing the predictions
    from the neural network and the ground truth to the criterion. We can do this
    with `loss_score = criterion(y_pred, y_train)`.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须编写一个循环，迭代更新权重的过程。首先，我们需要传递数据以从神经网络中获取预测结果。这非常简单：我们只需将输入数据作为参数传递给神经网络实例，使用
    `y_pred = my_neuralnet(x_train)`。然后，我们需要计算损失分数，通过将神经网络的预测结果和真实值传递给 Criterion 来得到
    `loss_score = criterion(y_pred, y_train)`。
- en: Before we proceed to update the weights in our neural network, it is important
    to clear any gradients that have been accumulated from when we used the `zero_grad()`
    method on the optimizer. Then, to perform a backpropagation step, we use the `backward()`
    method on the computed `loss_score`. Finally, we update the parameters, or the
    weights, using the `step()` method on the optimizer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续更新神经网络的权重之前，清除累积的梯度非常重要，可以通过在优化器上使用 `zero_grad()` 方法来实现。然后，我们使用计算的 `loss_score`
    上的 `backward()` 方法执行反向传播步骤。最后，使用优化器上的 `step()` 方法更新参数或权重。
- en: 'All of the previous logic must be in a loop, where we iterate over the training
    process until our network learns the best parameters. So, let''s put everything
    together into workable code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有之前的逻辑必须放在一个循环中，我们在训练过程中迭代，直到我们的网络学习到最佳参数。因此，让我们将所有内容整合成可运行的代码：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Voila! We have trained our neural network and it is ready to perform predictions.
    In the next chapter, we will delve into the various non-linear functions that
    are used in neural networks, the idea of validating what the neural network has
    learned, and dive deeper into the philosophy of building robust neural networks.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大功告成！我们已经训练好了我们的神经网络，它已准备好进行预测。在下一章中，我们将深入探讨神经网络中使用的各种非线性函数，验证神经网络学到的内容，并深入探讨构建强大神经网络的理念。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various data structures and operations provided
    by PyTorch. We implemented several components using the fundamental blocks of
    PyTorch. For the data preparation stage, we created the tensors that will be used
    by our algorithm. Our network architecture was a model that would learn to predict
    the average hours spent by users on our Wondermovies platform. We used the loss
    function to check the standard of our model and used the `optimize` function to
    adjust the learnable parameters of our model to make it perform better.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了PyTorch提供的各种数据结构和操作。我们使用PyTorch的基本模块实现了几个组件。在数据准备阶段，我们创建了张量，这些张量将被我们的算法使用。我们的网络架构是一个模型，它将学习预测用户在我们的Wondermovies平台上平均花费的时间。我们使用损失函数来检查我们模型的标准，并使用`optimize`函数来调整模型的可学习参数，使其表现更好。
- en: We also looked at how PyTorch makes it easier for us to create data pipelines
    by abstracting away several complexities that would require us to parallelize
    and augment data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了PyTorch如何通过抽象化几个复杂性，使我们能够更轻松地创建数据管道，而不需要我们并行化和增强数据。
- en: In the next chapter, we will take a deep dive into how neural networks and deep
    learning algorithms work. We will explore various built-in PyTorch modules for
    building network architectures, loss functions, and optimizations. We will also
    learn how to use them on real-world datasets.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨神经网络和深度学习算法的工作原理。我们将探索各种内置的PyTorch模块，用于构建网络架构、损失函数和优化。我们还将学习如何在真实世界的数据集上使用它们。
