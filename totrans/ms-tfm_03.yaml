- en: '*Chapter 2*: A Hands-On Introduction to the Subject'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：主题的实践介绍'
- en: So far, we have had an overall look at the evolution of **Natural Language Processing**
    (**NLP**) using **Deep Learning** (**DL**)-based methods. We have learned some
    basic information about Transformer and their respective architecture. In this
    chapter, we are going to have a deeper look into how a transformer model can be
    used. Tokenizers and models, such as **Bidirectional Encoder Representations from
    Transformer** (**BERT**), will be described in more technical detail in this chapter
    with hands-on examples, including how to load a tokenizer/model and use community-provided
    pretrained models. But before using any specific model, we will understand the
    installation steps required to provide the necessary environment by using Anaconda.
    In the installation steps, installing libraries and programs on various operating
    systems such as Linux, Windows, and macOS will be covered. The installation of
    **PyTorch** and **TensorFlow**, in two versions of a **Central Processing Unit**
    (**CPU**) and a **Graphics Processing Unit** (**GPU**), is also shown. A quick
    jump into a **Google Colaboratory** (**Google Colab**) installation of the Transformer
    library is provided. There is also a section dedicated to using models in the
    PyTorch and TensorFlow frameworks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经总体了解了基于**深度学习**（**DL**）方法的**自然语言处理**（**NLP**）的演变。我们也学习了一些有关Transformer及其各自架构的基本信息。在本章中，我们将深入研究如何使用transformer模型。在本章中，像**双向编码器表示的Transformer**（**BERT**）这样的分词器和模型将以更加技术性的细节进行描述，并提供实际示例，包括如何加载分词器/模型和使用社区提供的预训练模型。但在使用任何特定模型之前，我们需要了解使用安装Anaconda所需的安装步骤以提供必要的环境。在安装步骤中，将涵盖在各种操作系统如Linux、Windows和macOS上安装库和程序的内容。还会展示**PyTorch**和**TensorFlow**的安装，在**CPU**和**GPU**的两个版本上。还提供了一个**Google
    Colaboratory**（**Google Colab**）安装Transformer库的快速指南。还有一个专门讨论在PyTorch和TensorFlow框架中使用模型的部分。
- en: The HuggingFace models repository is also another important part of this chapter,
    in which finding different models and steps to use various pipelines are discussed—for
    example, models such as **Bidirectional and Auto-Regressive Transformer** (**BART**),
    BERT, and **TAble PArSing** (**TAPAS**) are detailed, with a glance at **Generative
    Pre-trained Transformer 2** (**GPT-2**) text generation. However, this is purely
    an overview, and this part of the chapter relates to getting the environment ready
    and using pretrained models. No model training is discussed here as this is given
    greater significance in upcoming chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace模型存储库也是本章的另一个重要部分，讨论了查找不同模型和使用各种管道的步骤，例如，详细介绍了像**双向自回归Transformer**（**BART**）、BERT和**表格解析**（**TAPAS**）这样的模型，以及一瞥**生成式预训练Transformer
    2**（**GPT-2**）文本生成。然而，这只是一个概述，在本章中涉及到的部分是准备环境和使用预训练模型，这里不讨论模型训练，因为这在接下来的章节中被赋予了更重要的意义。
- en: After everything is ready and we have understood how to use the `Transformer`
    library for inference by community-provided models, the `datasets` library is
    described. Here, we look at loading various datasets, benchmarks, and using metrics.
    Loading a specific dataset and getting data back from it is one of the main areas
    we look at here. Cross-lingual datasets and how to use local files with the `datasets`
    library are also considered here. The `map` and `filter` functions are important
    functions of the `datasets` library in terms of model training and are also examined
    in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪，我们已经了解了如何通过社区提供的模型使用`Transformer`库进行推理，接下来介绍`datasets`库。在这里，我们将介绍加载各种数据集、基准测试和使用指标的方法。加载特定数据集并从中获取数据是我们在这里主要关注的领域之一。这里还会考虑跨语言数据集以及如何使用`datasets`库中的本地文件。`datasets`库中的`map`和`filter`函数是模型训练的重要函数，并且在本章中也会被研究。
- en: This chapter is an essential part of the book because the `datasets` library
    is described in more detail here. It's also very important for you to understand
    how to use community-provided models and get the system ready for the rest of
    the book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是书中的重要部分，因为这里更详细地介绍了`datasets`库。了解如何使用社区提供的模型并准备好系统以供接下来的内容是非常重要的。
- en: 'To sum all this up, we will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，本章中我们将涵盖以下主题：
- en: Installing Transformer with Anaconda
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Anaconda安装Transformer
- en: Working with language models and tokenizers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型和分词器进行工作
- en: Working with community-provided models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用社区提供的模型进行工作
- en: Working with benchmarks and datasets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基准测试和数据集
- en: Benchmarking for speed and memory
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度和内存的基准测试
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You will need to install the libraries and software listed next. Although having
    the latest version is a plus, it is mandatory to install versions that are compatible
    with each other. For more information about the latest version installation for
    HuggingFace Transformer, take a look at their official web page at [https://huggingface.co/Transformer/installation.html](https://huggingface.co/transformers/installation.html):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装接下来列出的库和软件。虽然拥有最新版本是一个优点，但是强制安装与彼此兼容版本是必须的。有关HuggingFace Transformer的最新版本安装的更多信息，请查看它们在[https://huggingface.co/transformers/installation.html](https://huggingface.co/transformers/installation.html)的官方网页：
- en: Anaconda
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Transformer 4.0.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 4.0.0
- en: PyTorch 1.1.0
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 1.1.0
- en: TensorFlow 2.4.0
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.4.0
- en: Datasets 1.4.1
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集1.4.1
- en: Finally, all the code shown in this chapter is available in this book's GitHub
    repository at https://github.com/PacktPublishing/Mastering-Transformer/tree/main/CH02.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章中显示的所有代码都可以在本书的GitHub存储库https://github.com/PacktPublishing/Mastering-Transformer/tree/main/CH02
    中找到。
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/372ek48](https://bit.ly/372ek48)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 点击以下链接查看动态代码演示视频：[https://bit.ly/372ek48](https://bit.ly/372ek48)
- en: Installing Transformer with Anaconda
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Anaconda安装Transformer
- en: '`Transformer` library. However, it is also possible to install this library
    without the aid of Anaconda. The main motivation to use Anaconda is to explain
    the process more easily and moderate the packages used.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`库。但是，也可以在没有Anaconda的帮助下安装此库。使用Anaconda的主要动机是更容易解释过程并调节使用的软件包。'
- en: To start installing the related libraries, the installation of Anaconda is a
    mandatory step. Official guidelines provided by the Anaconda documentation offer
    simple steps to install it for common operating systems (macOS, Windows, and Linux).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始安装相关库，安装Anaconda是必须的一步。Anaconda文档提供了官方指南，简单介绍了为常见操作系统（macOS，Windows和Linux）安装Anaconda的步骤。
- en: Installation on Linux
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux安装
- en: 'Many distributions of Linux are available for users to enjoy, but among them,
    **Ubuntu** is one of the preferred ones. In this section, the steps to install
    Anaconda are covered for Linux. Proceed as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以享受许多Linux发行版，但其中**Ubuntu**是偏爱的之一。在本节中，将介绍安装Anaconda的步骤。请按以下步骤进行：
- en: Download the Anaconda installer for Linux from https://www.anaconda.com/products/individual#Downloads
    and go to the Linux section, as illustrated in the following screenshot:![Figure
    2.1 – Anaconda download link for Linux ](img/B17123_02_01.jpg)
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从https://www.anaconda.com/products/individual#Downloads下载Linux版本的Anaconda安装程序并转到Linux部分，如下截图所示：![图2.1
    - Linux的Anaconda下载链接](img/B17123_02_01.jpg)
- en: Figure 2.1 – Anaconda download link for Linux
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.1 - Linux的Anaconda下载链接
- en: 'Run a `bash` command to install it and complete the following steps:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`bash`命令来安装它并完成以下步骤：
- en: 'Open the Terminal and run the following command:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并运行以下命令：
- en: '[PRE0]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Press *Enter* to see the license agreement and press *Q* if you do not want
    to read it all, and then do the following:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下*Enter*键查看许可协议，如果你不想全部阅读，按*Q*键，然后执行以下操作：
- en: Click **Yes** to agree.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**是**同意。
- en: Click `conda` root environment.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`conda`根环境。
- en: After running a `python` command from the Terminal, you should see an Anaconda
    prompt after the Python version information.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行`python`命令后，您应该看到Python版本信息后出现Anaconda提示符。
- en: 'You can access Anaconda Navigator by running an `anaconda-navigator` command
    from the Terminal. As a result, you will see the Anaconda **Graphical User Interface**
    (**GUI**) start loading the related modules, as shown in the following screenshot:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过从终端运行`anaconda-navigator`命令来访问Anaconda Navigator。结果，您将看到Anaconda **图形用户界面**（**GUI**）开始加载相关模块，如下面的截图所示：
- en: '![Figure 2.2 – Anaconda Navigator ](img/B17123_02_002.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 - Anaconda Navigator](img/B17123_02_002.jpg)'
- en: Figure 2.2 – Anaconda Navigator
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 - Anaconda Navigator
- en: Let's move on to the next section!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一节！
- en: Installation on Windows
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Windows安装
- en: 'The following steps describe how you can install Anaconda on Windows operating
    systems:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的步骤描述了如何在Windows操作系统上安装Anaconda：
- en: Download the installer from https://www.anaconda.com/products/individual#Downloads
    and go to the Windows section, as illustrated in the following screenshot:![Figure
    2.3 – Anaconda download link for Windows ](img/B17123_02_03.jpg)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从https://www.anaconda.com/products/individual#Downloads下载安装程序并转到Windows部分，如下截图所示：![图
    2.3 – Windows 上的Anaconda下载链接](img/B17123_02_03.jpg)
- en: Figure 2.3 – Anaconda download link for Windows
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.3 – Windows 上的Anaconda下载链接
- en: Open the installer and follow the guide by clicking the **I Agree** button.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开安装程序并通过点击**我同意**按钮按照指南进行操作。
- en: Select the location for installation, as illustrated in the following screenshot:![Figure
    2.4 – Anaconda installer for Windows ](img/B17123_02_04.jpg)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择安装位置，如下截图所示：![图 2.4 – Windows 上的Anaconda安装程序](img/B17123_02_04.jpg)
- en: Figure 2.4 – Anaconda installer for Windows
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.4 – Windows 上的Anaconda安装程序
- en: Don't forget to check the `python` command from the Windows shell or the Windows
    command line:![Figure 2.5 – Anaconda installer advanced options ](img/B17123_02_05.jpg)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记从Windows shell或Windows命令行检查`python`命令：![图 2.5 – Windows 上的Anaconda安装程序高级选项](img/B17123_02_05.jpg)
- en: Figure 2.5 – Anaconda installer advanced options
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.5 – Windows 上的Anaconda安装程序高级选项
- en: Follow the rest of the installation instructions and finish the installation.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照其余的安装说明进行操作并完成安装。
- en: You should now be able to start Anaconda Navigator from the **Start** menu.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该能够从**开始**菜单启动Anaconda Navigator。
- en: Installation on macOS
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: macOS上的安装
- en: 'The following steps must be followed to install Anaconda on macOS:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Anaconda在macOS上必须遵循以下步骤：
- en: Download the installer from https://www.anaconda.com/products/individual#Downloads
    and go to the macOS section, as illustrated in the following screenshot:![Figure
    2.6 – Anaconda download link for macOS ](img/B17123_02_06.jpg)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从https://www.anaconda.com/products/individual#Downloads下载安装程序并转到macOS部分，如下截图所示：![图
    2.6 – macOS 上的Anaconda下载链接](img/B17123_02_06.jpg)
- en: Figure 2.6 – Anaconda download link for macOS
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.6 – macOS 上的Anaconda下载链接
- en: Open the installer.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开安装程序。
- en: 'Follow the instructions and click the **Install** button to install macOS in
    a predefined location, as illustrated in the following screenshot. You can change
    the default directory, but this is not recommended:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照说明并点击**安装**按钮在预定义位置安装macOS，如下截图所示。你可以更改默认目录，但不建议这样做：
- en: '![Figure 2.7 – Anaconda installer for macOS ](img/B17123_02_07.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – macOS 上的Anaconda安装程序](img/B17123_02_07.jpg)'
- en: Figure 2.7 – Anaconda installer for macOS
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – macOS 上的Anaconda安装程序
- en: Once you finish the installation, you should be able to access Anaconda Navigator.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 完成安装后，您应该能够访问Anaconda Navigator。
- en: Installing TensorFlow, PyTorch, and Transformer
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装TensorFlow、PyTorch和Transformer
- en: The installation of TensorFlow and PyTorch as two major libraries that are used
    for DL can be made through `pip` or `conda` itself. `conda` provides a **Command-Line
    Interface** (**CLI**) for easier installation of these libraries.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 安装TensorFlow和PyTorch作为用于DL的两个主要库可以通过`pip`或`conda`本身进行。`conda`提供了一个用于更轻松安装这些库的**命令行界面**（**CLI**）。
- en: 'For a clean installation and to avoid interrupting other environments, it is
    better to create a `conda` environment for the `huggingface` library. You can
    do this by running the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行干净的安装并避免中断其他环境，最好为`huggingface`库创建一个`conda`环境。您可以通过运行以下代码来实现：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This command will create an empty environment for installing other libraries.
    Once created, we will need to activate it, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将创建一个空的环境以安装其他库。一旦创建，我们需要激活它，如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Installation of the `Transformer` library is easily done by running the following
    commands:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`Transformer`库非常简单，只需运行以下命令：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `-c` argument in the `conda install` command lets Anaconda use additional
    channels to search for libraries.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install`命令中的`-c`参数让Anaconda使用额外的渠道来搜索库。'
- en: 'Note that it is a requirement to have TensorFlow and PyTorch installed because
    the `Transformer` library uses both of these libraries. An additional note is
    the easy handling of CPU and GPU versions of TensorFlow by Conda. If you simply
    put `–gpu` after `tensorflow`, it will install the GPU version automatically.
    For installation of PyTorch through the `cuda` library (GPU version), you are
    required to have related libraries such as `cuda`, but `conda` handles this automatically
    and no further manual setup or installation is required. The following screenshot
    shows how `conda` automatically takes care of installing the PyTorch GPU version
    by installing the related `cudatoolkit` and `cudnn` libraries:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，必须安装TensorFlow和PyTorch，因为`Transformer`库同时使用这两个库。另一个注意事项是，通过Conda轻松处理TensorFlow的CPU和GPU版本。如果在`tensorflow`之后简单地放置`–gpu`，它将自动安装GPU版本。通过`cuda`库（GPU版本）安装PyTorch时，需要相关库，如`cuda`，但`conda`会自动处理这个，不需要进一步的手动设置或安装。以下屏幕截图显示了`conda`如何自动处理安装PyTorch
    GPU版本，安装相关的`cudatoolkit`和`cudnn`库：
- en: '![Figure 2.8 – Conda installing PyTorch and related cuda libraries ](img/B17123_02_008.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 使用Conda安装PyTorch和相关cuda库](img/B17123_02_008.jpg)'
- en: Figure 2.8 – Conda installing PyTorch and related cuda libraries
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 使用Conda安装PyTorch和相关cuda库
- en: Note that all of these installations can also be done without `conda`, but the
    reason behind using Anaconda is its ease of use. In terms of using environments
    or installing GPU versions of TensorFlow or PyTorch, Anaconda works like magic
    and is a good time saver.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有这些安装也可以在没有`conda`的情况下进行，但使用Anaconda的原因是它的易用性。在使用环境或安装TensorFlow或PyTorch的GPU版本方面，Anaconda像魔术一样工作，是一个很好的时间节省工具。
- en: Installing using Google Colab
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Colab进行安装
- en: 'Even if the utilization of Anaconda saves time and is useful, in most cases,
    not everyone has such a good and reasonable computation resource available. Google
    Colab is a good alternative in such cases. Installation of the `Transformer` library
    in Colab is carried out with the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用Anaconda节省时间且有用，但在大多数情况下，不是每个人都有这样好的和合理的计算资源可用。在这种情况下，Google Colab是一个很好的替代方案。在Colab中安装`Transformer`库的命令如下进行：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: An exclamation mark before the statement makes the code run in a Colab shell,
    which is equivalent to running the code in the Terminal instead of running it
    using a Python interpreter. This will automatically install the `Transformer`
    library.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在语句前加上感叹号会使代码在Colab shell中运行，这相当于在终端中而不是使用Python解释器运行代码。这将自动安装`Transformer`库。
- en: Working with language models and tokenizers
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用语言模型和分词器
- en: 'In this section, we will look at using the `Transformer` library with language
    models, along with their related **tokenizers**. In order to use any specified
    language model, we first need to import it. We will start with the BERT model
    provided by Google and use its pretrained version, as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何使用`Transformer`库与语言模型及其相关的**分词器**。为了使用任何指定的语言模型，我们首先需要导入它。我们将从谷歌提供的BERT模型开始，并使用其预训练版本，如下所示：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first line of the preceding code snippet imports the BERT tokenizer, and
    the second line downloads a pretrained tokenizer for the BERT base version. Note
    that the uncased version is trained with uncased letters, so it does not matter
    whether the letters appear in upper- or lowercase. To test and see the output,
    you must run the following line of code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的第一行导入了BERT分词器，第二行下载了BERT基础版本的预训练分词器。请注意，无大小写版本是使用无大小写字母训练的，因此字母出现在大写或小写中都没有关系。要测试并查看输出，必须运行以下代码行：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will be the output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是输出：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`input_ids` shows the token ID for each token, and `token_type_ids` shows the
    type of each token that separates the first and second sequence, as shown in the
    following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_ids` 显示每个标记的标记ID，`token_type_ids` 显示每个标记的类型，它们分隔了第一个和第二个序列，如下面的屏幕截图所示：'
- en: '![Figure 2.9 – Sequence separation for BERT ](img/B17123_02_09.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – BERT 的序列分隔](img/B17123_02_09.jpg)'
- en: Figure 2.9 – Sequence separation for BERT
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – BERT的序列分隔
- en: '`attention_mask` is a mask of 0s and 1s that is used to show the start and
    end of a sequence for the transformer model in order to prevent unnecessary computations.
    Each tokenizer has its own way of adding special tokens to the original sequence.
    In the case of the BERT tokenizer, it adds a `[CLS]` token to the beginning and
    an `[SEP]` token to the end of the sequence, which can be seen by 101 and 102\.
    These numbers come from the token IDs of the pretrained tokenizer.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`attention_mask` 是用于显示变压器模型序列的起始和结束的 0 和 1 的掩码，以防止不必要的计算。每个分词器都有将特殊标记添加到原始序列的自己方式。对于
    BERT 分词器，它在序列的开头添加了一个 `[CLS]` 标记，在序列的结尾添加了一个 `[SEP]` 标记，可以通过 101 和 102 观察到。这些数字来自预训练分词器的标记
    ID。'
- en: 'A tokenizer can be used for both PyTorch- and TensorFlow-based `Transformer`
    models. In order to have output for each one, `pt` and `tf` keywords must be used
    in `return_tensors`. For example, you can use a tokenizer by simply running the
    following command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器可用于基于 PyTorch 和 TensorFlow 的 `Transformer` 模型。为了每个模型都有输出，必须在 `return_tensors`
    中使用 `pt` 和 `tf` 关键字。例如，您可以通过简单运行以下命令来使用分词器：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`encoded_input` has the tokenized text to be used by the PyTorch model. In
    order to run the model—for example, the BERT base model—the following code can
    be used to download the model from the `huggingface` model repository:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoded_input` 具有将被 PyTorch 模型使用的标记化文本。为了运行模型，例如 BERT 基础模型，可以使用以下代码从 `huggingface`
    模型存储库下载模型：'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the tokenizer can be passed to the downloaded model with the
    following line of code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的输出可以通过以下代码行传递给已下载的模型：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will give you the output of the model in the form of embeddings and cross-attention
    outputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这将以嵌入和交叉注意输出的形式给您模型的输出。
- en: 'When loading and importing models, you can specify which version of a model
    you are trying to use. If you simply put `TF` before the name of a model, the
    `Transformer` library will load the TensorFlow version of it. The following code
    shows how to load and use the TensorFlow version of BERT base:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载和导入模型时，您可以指定要使用的模型的版本。如果您只需在模型名称前加上 `TF`，`Transformer` 库将加载其 TensorFlow 版本。以下代码显示了如何加载和使用
    TensorFlow 版本的 BERT base：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For specific tasks such as filling masks using language models, there are pipelines
    designed by `huggingface` that are ready to use. For example, a task of filling
    a mask can be seen in the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定任务，如使用语言模型填充掩码，`huggingface` 设计了准备就绪的管道。例如，可以在以下代码片段中看到填充掩码的任务：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code will produce the following output, which shows the scores and possible
    tokens to be placed in the `[MASK]` token:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将生成以下输出，显示分数和可能放置在 `[MASK]` 标记中的标记：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To get a neat view with pandas, run the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 pandas 获取清晰的视图，请运行以下代码：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result can be seen in the following screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在以下截图中看到：
- en: '![Figure 2.10 – Output of the BERT mask filling ](img/B17123_02_10.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10 – BERT 填充掩码的输出](img/B17123_02_10.jpg)'
- en: Figure 2.10 – Output of the BERT mask filling
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – BERT 填充掩码的输出
- en: So far, we have learned how to load and use a pretrained BERT model and have
    understood the basics of tokenizers, as well as the difference between PyTorch
    and TensorFlow versions of the models. In the next section, we will learn to work
    with community-provided models by loading different models, reading the related
    information provided by the model authors and using different pipelines such as
    text-generation or **Question Answering** (**QA**) pipelines.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何加载和使用预训练的 BERT 模型，并了解了分词器的基础知识，以及模型的 PyTorch 和 TensorFlow 版本之间的区别。在下一节中，我们将学习如何使用社区提供的模型，通过加载不同的模型，阅读模型作者提供的相关信息，并使用不同的管道，如文本生成或**问答**（**QA**）管道。
- en: Working with community-provided models
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用社区提供的模型
- en: '**Hugging Face** has tons of community models provided by collaborators from
    large **Artificial Intelligence** (**AI**) and **Information Technology** (**IT**)
    companies such as Google and Facebook. There are also many interesting models
    that individuals and universities provide. Accessing and using them is also very
    easy. To start, you should visit the Transformer models directory available at
    their website (https://huggingface.co/models), as shown in the following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face** 拥有大量由来自大型**人工智能**（**AI**）和**信息技术**（**IT**）公司（如谷歌和Facebook）的合作者提供的社区模型。还有许多个人和大学提供的有趣模型。访问和使用它们也非常容易。要开始，请访问他们网站上提供的
    Transformer 模型目录（https://huggingface.co/models），如下截图所示：'
- en: '![Figure 2.11 – Hugging Face models repository ](img/B17123_02_011.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – Hugging Face模型库 ](img/B17123_02_011.jpg)'
- en: Figure 2.11 – Hugging Face models repository
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – Hugging Face模型库
- en: Apart from these models, there are also many good and useful datasets available
    for NLP tasks. To start using some of these models, you can explore them by keyword
    searches, or just specify your major NLP task and pipeline.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些模型，还有很多好用的数据集可供NLP任务使用。要开始使用其中一些模型，你可以通过关键字搜索或只需指定你的主要NLP任务和流水线来探索它们。
- en: 'For example, we are looking for a table QA model. After finding a model that
    we are interested in, a page such as the following one will be available from
    the Hugging Face website ([https://huggingface.co/google/tapas-base-finetuned-wtq](https://huggingface.co/google/tapas-base-finetuned-wtq)):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们正在寻找一个表格QA模型。在找到我们感兴趣的模型之后，从Hugging Face网站（[https://huggingface.co/google/tapas-base-finetuned-wtq](https://huggingface.co/google/tapas-base-finetuned-wtq)）会提供类似下面这个页面：
- en: '![Figure 2.12 – TAPAS model page ](img/B17123_02_012.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – TAPAS模型页面 ](img/B17123_02_012.jpg)'
- en: Figure 2.12 – TAPAS model page
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – TAPAS模型页面
- en: 'On the right side, there is a panel where you can test this model. Note that
    this is a table QA model that can answer questions about a table provided to the
    model. If you ask a question, it will reply by highlighting the answer. The following
    screenshot shows how it gets input and provides an answer for a specific table:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧有一个面板，你可以在这里测试这个模型。请注意，这是一个可以回答关于提供给模型的表格的问题的表格QA模型。如果你问一个问题，它会用高亮显示的方式来回复你。以下截图展示了它如何获取输入并为特定表格提供答案：
- en: '![Figure 2.13 – Table QA using TAPAS ](img/B17123_02_13.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 使用TAPAS进行表格QA ](img/B17123_02_13.jpg)'
- en: Figure 2.13 – Table QA using TAPAS
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 使用TAPAS进行表格QA
- en: 'Each model has a page provided by the model authors that is also known as a
    `huggingface` repository page and take a look at the example provided by the authors
    ([https://huggingface.co/gpt2](https://huggingface.co/gpt2)), as shown in the
    following screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有一个由模型作者提供的页面，也被称为`huggingface`库页面，并查看作者提供的示例（[https://huggingface.co/gpt2](https://huggingface.co/gpt2)），如下截图所示：
- en: '![Figure 2.14 – Text-generation code example from Hugging Face GPT-2 page ](img/B17123_02_014.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 来自Hugging Face GPT-2页面的文本生成代码示例 ](img/B17123_02_014.jpg)'
- en: Figure 2.14 – Text-generation code example from the Hugging Face GPT-2 page
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 来自Hugging Face GPT-2页面的文本生成代码示例
- en: 'Using pipelines is recommended because all the dirty work is taken care of
    by the `Transformer` library. As another example, let''s assume you need an out-of-the-box
    zero-shot classifier. The following code snippet shows how easy it is to implement
    and use such a pretrained model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流水线是推荐的，因为所有繁琐的工作都由`Transformer`库处理好了。举个例子，假设你需要一个开箱即用的零-shot分类器。下面的代码片段展示了实现和使用这样一个预训练模型是多么容易：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code will provide the following result:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将提供以下结果：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are done with the installation and the `hello-world` application part. So
    far, we have introduced the installation process, completed the environment settings,
    and experienced the first transformer pipeline. In the next part, we will introduce
    the `datasets` library, which will be our essential utility in the experimental
    chapters to come.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了安装和`hello-world`应用程序部分。到目前为止，我们介绍了安装过程，完成了环境设置，并体验了第一个transformer流水线。接下来，我们将介绍`datasets`库，这将是我们接下来的实验章节中的必备工具。
- en: Working with benchmarks and datasets
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与基准和数据集一起工作
- en: Before introducing the `datasets` library, we'd better talk about important
    benchmarks such as `Transformer` library, we are able to transfer what we have
    learned from a particular task to a related task, which is called **Transfer Learning**
    (**TL**). By transferring representations between related problems, we are able
    to train general-purpose models that share common linguistic knowledge across
    tasks, also known as **Multi-Task Learning** (**MTL**). Another aspect of TL is
    to transfer knowledge across natural languages (multilingual models).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍`datasets`库之前，我们最好先谈谈重要的基准，比如`Transformer`库，我们可以将从一个特定任务学到的内容转移到一个相关任务中，这被称为**迁移学习**（**TL**）。通过在相关问题之间迁移表示，我们能够训练出共享通用语言知识的通用模型，也就是**多任务学习**（**MTL**）。TL的另一个方面是跨语言传递知识（多语言模型）。
- en: Important benchmarks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要的基准指标
- en: 'In this part, we will introduce the important benchmarks that are widely used
    by transformer-based architectures. These benchmarks exclusively contribute a
    lot to MTL and to multilingual and zero-shot learning, including many challenging
    tasks. We will look at the following benchmarks:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '**GLUE**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SuperGLUE**'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XTREME**'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGLUE**'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQuAD**'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of using fewer pages, we give details of the tasks for only the
    GLUE benchmark, so let's look at this benchmark first.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: GLUE benchmark
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent studies addressed the fact that multitask training approaches can achieve
    better results than single-task learning as a particular model for a task. In
    this direction, the **GLUE** benchmark has been introduced for MTL, which is a
    collection of tools and datasets for evaluating the performance of MTL models
    across a list of tasks. It offers a public leaderboard for monitoring submission
    performance on the benchmark, along with a single-number metric summarizing 11
    tasks. This benchmark includes many sentence-understanding tasks that are based
    on existing tasks covering various datasets of differing size, text type, and
    difficulty levels. The tasks are categorized under three types, outlined as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-sentence tasks**'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoLA**: The **Corpus of Linguistic Acceptability** dataset. This task consists
    of English acceptability judgments drawn from articles on linguistic theory.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pos`/`neg` labels.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity and paraphrase tasks**'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MRPC**: The **Microsoft Research Paraphrase Corpus** dataset. This task looks
    at whether the sentences in a pair are semantically equivalent.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QQP**: The **Quora Question Pairs** dataset. This task decides whether a
    pair of questions is semantically equivalent.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**STS-B**: The **Semantic Textual Similarity Benchmark** dataset. This task
    is a collection of sentence pairs drawn from news headlines, with a similarity
    score between 1 and 5.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference tasks**'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MNLI**: The **Multi-Genre Natural Language Inference** corpus. This is a
    collection of sentence pairs with textual entailment. The task is to predict whether
    the text entails a hypothesis (entailment), contradicts the hypothesis (contradiction),
    or neither (neutral).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QNLI**: **Question Natural Language Inference** dataset. This is a converted
    version of SquAD. The task is to check whether a sentence contains the answer
    to a question.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RTE**: The **Recognizing Textual Entailment** dataset. This is a task of
    textual entailment challenges to combine data from various sources. This dataset
    is similar to the previous QNLI dataset, where the task is to check whether a
    first text entails a second one.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WNLI**: The **Winograd Natural Language Inference** schema challenge. This
    is originally a pronoun resolution task linking a pronoun and a phrase in a sentence.
    GLUE converted the problem into sentence-pair classification, as detailed next.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuperGLUE benchmark
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like Glue, **SuperGLUE** is a new benchmark styled with a new set of more difficult
    language-understanding tasks and offers a public leaderboard of around currently
    eight language tasks, drawing on existing data, associated with a single-number
    performance metric like that of GLUE. The motivation behind it is that as of writing
    this book, the current state-of-the-art GLUE Score (90.8) surpasses human performance
    (87.1). Thus, SuperGLUE provides a more challenging and diverse task toward general-purpose,
    language-understanding technologies.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与Glue类似，**SuperGLUE**是一个新的基准测试，具有一组更难的语言理解任务，并提供大约八项语言任务的公共排行榜，借鉴了现有数据，与GLUE的类似，都使用单一数字性能指标。其背后的动机是，截至撰写本书，在写作时间，当前的GLUE得分（90.8）超过人类表现（87.1）。因此，SuperGLUE提供了一个更具挑战性和多样化的任务，以实现通用目的的语言理解技术。
- en: You can access both GLUE and SuperGLUE benchmarks at [gluebenchmark.com](http://gluebenchmark.com).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[gluebenchmark.com](http://gluebenchmark.com)上访问GLUE和SuperGLUE基准测试。
- en: XTREME benchmark
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XTREME基准测试
- en: In recent years, NLP researchers have increasingly focused on learning general-purpose
    representations rather than a single task that can be applied to many related
    tasks. Another way of building a general-purpose language model is by using multilingual
    tasks. It has been observed that recent multilingual models such as **Multilingual
    BERT** (**mBERT**) and XLM-R pretrained on massive amounts of multilingual corpora
    have performed better when transferring them to other languages. Thus, the main
    advantage here is that cross-lingual generalization enables us to build successful
    NLP applications in resource-poor languages through zero-shot cross-lingual transfer.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自然语言处理研究人员越来越注重学习通用表示而不是单一任务，可以应用于许多相关任务。构建通用语言模型的另一种方法是使用多语言任务。已经观察到最近的多语言模型，如**多语言BERT**（**mBERT**）和XLM-R，预训练了大量的多语言语料库，当将它们转移到其他语言时表现更好。因此，这里的主要优势是跨语言泛化使我们能够通过零样本跨语言传输在资源匮乏的语言中构建成功的自然语言处理应用程序。
- en: 'In this direction, the **XTREME** benchmark has been designed. It currently
    includes around 40 different languages belonging to 12 language families and includes
    9 different tasks that require reasoning for various levels of syntax or semantics.
    However, it is still challenging to scale up a model to cover over 7,000 world
    languages and there exists a trade-off between language coverage and model capability.
    Please check out the following link for more details on this: [https://sites.research.google/xtreme](https://sites.research.google/xtreme).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方向上，**XTREME**基准测试已经设计好。它目前包括约40种属于12个语言家族的不同语言，并包括需要在各种句法或语义水平上进行推理的9个不同任务。然而，将模型扩展到覆盖超过7000种世界语言仍然具有挑战性，并存在语言覆盖和模型能力之间的权衡。有关此事的更多详细信息，请查看以下链接：[https://sites.research.google/xtreme](https://sites.research.google/xtreme)。
- en: XGLUE benchmark
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGLUE基准测试
- en: '**XGLUE** is another cross-lingual benchmark to evaluate and improve the performance
    of cross-lingual pretrained models for **Natural Language Understanding** (**NLU**)
    and **Natural Language Generation** (**NLG**). It originally consisted of 11 tasks
    over 19 languages. The main difference from XTREME is that the training data is
    only available in English for each task. This forces the language models to learn
    only from the textual data in English and transfer this knowledge to other languages,
    which is called zero-shot cross-lingual transfer capability. The second difference
    is that it has tasks for NLU and NLG at the same time. Please check out the following
    link for more details on this: [https://microsoft.github.io/XGLUE/](https://microsoft.github.io/XGLUE/).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGLUE**是另一个跨语言基准测试，用于评估和改进**自然语言理解**（**NLU**）和**自然语言生成**（**NLG**）的跨语言预训练模型的性能。它最初由19种语言的11项任务组成。与XTREME的主要区别在于，每项任务的训练数据仅在英语中可用。这迫使语言模型仅从英语文本数据中学习，并将这些知识传递给其他语言，这被称为零样本跨语言传输能力。第二个区别是它同时具有NLU和NLG任务。有关此事的更多详细信息，请查看以下链接：[https://microsoft.github.io/XGLUE/](https://microsoft.github.io/XGLUE/)。'
- en: SQuAD benchmark
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQuAD基准测试
- en: '**SQuAD** is a widely used QA dataset in the NLP field. It provides a set of
    QA pairs to benchmark the reading comprehension capabilities of NLP models. It
    consists of a list of questions, a reading passage, and an answer annotated by
    crowdworkers on a set of Wikipedia articles. The answer to the question is a span
    of text from the reading passage. The initial version, SQuAD1.1, doesn''t have
    an unanswerable option where the datasets are collected, so each question has
    an answer to be found somewhere in the reading passage. The NLP model is forced
    to answer the question even if this appears impossible. SQuAD2.0 is an improved
    version, whereby the NLP models must not only answer questions when possible,
    but should also abstain from answering when it is impossible to answer. SQuAD2.0
    contains 50,000 unanswerable questions written adversarially by crowdworkers to
    look similar to answerable ones. Additionally, it also has 100,000 questions taken
    from SQuAD1.1.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the datasets with an Application Programming Interface
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `datasets` library provides a very efficient utility to load, process, and
    share datasets with the community through the Hugging Face hub. As with TensorFlow
    datasets, it makes it easier to download, cache, and dynamically load the sets
    directly from the original dataset host upon request. The library also provides
    evaluation metrics along with the data. Indeed, the hub does not hold or distribute
    the datasets. Instead, it keeps all information about the dataset, including the
    owner, preprocessing script, description, and download link. We need to check
    whether we have permission to use the datasets under their corresponding license.
    To see other features, please check the `dataset_infos.json` and `DataSet-Name.py`
    files of the corresponding dataset under the GitHub repository, at [https://github.com/huggingface/datasets/tree/master/datasets](https://github.com/huggingface/datasets/tree/master/datasets)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by installing the `dataset` library, as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following code automatically loads the `cola` dataset using the Hugging
    Face hub. The `datasets.load_dataset()` function downloads the loading script
    from the actual path if the data is not cached already:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusability of the datasets: As you rerun the code a couple of times, the `datasets`
    library starts caching your loading and manipulation request. It first stores
    the dataset and starts caching your operations on the dataset, such as splitting,
    selection, and sorting. You will see a warning message such as **reusing dataset
    xtreme (/home/savas/.cache/huggingface/dataset...)** or **loading cached sorted...**.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we downloaded the `cola` dataset from the GLUE benchmark
    and selected a few examples from the `train` split of it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are 661 NLP datasets and 21 metrics for diverse tasks, as
    the following code snippet shows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: A dataset might have several configurations. For instance, GLUE, as an aggregated
    benchmark, has many subsets, such as CoLA, SST-2, and MRPC, as we mentioned before.
    To access each GLUE benchmark dataset, we pass two arguments, where the first
    is `glue` and the second is a particular dataset of its example dataset (`cola`
    or `sst2`) that can be chosen. Likewise, the Wikipedia dataset has several configurations
    provided for several languages.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset comes with the `DatasetDict` object, including several `Dataset`
    instances. When the split selection `(split=''...'')` is used, we get `Dataset`
    instances. For example, the `CoLA` dataset comes with `DatasetDict`, where we
    have three splits: *train*, *validation*, and *test*. While train and validation
    datasets include two labels (`1` for acceptable, `0` for unacceptable), the label
    value of test split is `-1`, which means no-label.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the structure of the `CoLA` dataset object, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The dataset object has some additional metadata information that might be helpful
    for us: `split`, `description`, `citation`, `homepage`, `license`, and `info`.
    Let''s run the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The GLUE benchmark provides many datasets, as mentioned previously. Let''s
    download the MRPC dataset, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Likewise, to access other GLUE tasks, we will change the second parameter,
    as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In order to apply a sanity check of data availability, run the following piece
    of code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: XTREME (working with a cross-lingual dataset) is another popular cross-lingual
    dataset that we already discussed. Let's pick the `MLQA` example from the XTREME
    set. MLQA is a subset of the XTREME benchmark, which is designed for assessing
    the performance of cross-lingual QA models. It includes about 5,000 extractive
    QA instances in the SQuAD format across seven languages, which are English, German,
    Arabic, Hindi, Vietnamese, Spanish, and Simplified Chinese.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `MLQA.en.de` is an English-German QA example dataset and can be
    loaded as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It could be more convenient to view it within a pandas DataFrame, as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the output of the preceding code:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – English-German cross-lingual QA dataset ](img/B17123_02_015.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – English-German cross-lingual QA dataset
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation with the datasets library
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets come with many dictionaries of subsets, where the `split` parameter
    is used to decide which subset(s) or portion of the subset is to be loaded. If
    this is `none` by default, it will return a dataset dictionary of all subsets
    (`train`, `test`, `validation`, or any other combination). If the `split` parameter
    is specified, it will return a single dataset rather than a dictionary. For the
    following example, we retrieve a `train` split of the `cola` dataset only:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can get a mixture of `train` and `validation` subsets, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `split` expression means that the first 300 examples of `train` and the
    last 30 examples of `validation` are obtained as `cola_sel`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply different combinations, as shown in the following split examples:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用不同的组合，如下所示的拆分示例：
- en: 'The first 100 examples from `train` and `validation`, as shown here:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如下所示是来自 `train` 和 `validation` 的前100个示例：
- en: '[PRE30]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '50% of `train` and the last 30% of `validation`, as shown here:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train` 的50%和 `validation` 的最后30%，如下所示：'
- en: '[PRE31]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first 20% of `train` and the examples in the slice [30:50] from `validation`,
    as shown here:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train` 的前20%和从 `validation` 的切片 [30:50] 中的示例，如下所示：'
- en: '[PRE32]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Sorting, indexing, and shuffling
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排序、索引和洗牌
- en: 'The following execution calls the `sort()` function of the `cola_sel` object.
    We see the first 15 and the last 15 labels:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下执行调用 `cola_sel` 对象的 `sort()` 函数。我们看到前15个和最后15个标签：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We are already familiar with Python slicing notation. Likewise, we can also
    access several rows using similar slice notation or with a list of indices, as
    follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉 Python 的切片表示法。同样，我们也可以使用类似的切片表示法或索引列表来访问多行，如下所示：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We shuffle the dataset as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式洗牌数据集：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Important note
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Seed value: When shuffling, we need to pass a seed value to control the randomness
    and achieve a consistent output between the author and the reader.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 种子值：在洗牌时，我们需要传递一个种子值来控制随机性，实现作者和读者之间的一致输出。
- en: Caching and reusability
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存和可重用性
- en: 'Using cached files allows us to load large datasets by means of memory mapping
    (if datasets fit on the drive) by using a fast backend. Such smart caching helps
    in saving and reusing the results of operations executed on the drive. To see
    cache logs with regard to the dataset, run the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用缓存文件可以通过内存映射（如果数据集适合驱动器）使用快速后端加载大型数据集。这种智能缓存有助于保存和重用在驱动器上执行的操作结果。要查看关于数据集的缓存日志，运行以下代码：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Dataset filter and map function
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集过滤和映射函数
- en: 'We might want to work with a specific selection of a dataset. For instance,
    we can retrieve sentences only, including the term `kick` in the `cola` dataset,
    as shown in the following execution. The `datasets.Dataset.filter()` function
    returns sentences including `kick` where an anonymous function and a `lambda`
    keyword are applied:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要处理数据集的特定选择。例如，我们可以仅检索具有 `cola` 数据集中包含术语 `kick` 的句子，如下面的执行所示。`datasets.Dataset.filter()`
    函数返回包含 `kick` 的句子，其中应用了匿名函数和 `lambda` 关键字：
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following filtering is used to get positive (acceptable) examples from
    the set:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下过滤用于从集中获取正面（可接受的）示例：
- en: '[PRE38]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In some cases, we might not know the integer code of a class label. Suppose
    we have many classes, and the code of the `culture` class is hard to remember
    out of 10 classes. Instead of giving integer code `1` in our preceding example,
    which is the code for `acceptable`, we can pass an `acceptable` label to the `str2int()`
    function, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能不知道类标签的整数代码。假设我们有许多类，而 `culture` 类的代码难以记住在10个类中。我们可以在我们之前的示例中传递一个
    `acceptable` 标签给 `str2int()` 函数，代替在我们之前的示例中给出整数代码 `1`，即 `acceptable` 的代码，如下所示：
- en: '[PRE39]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This produces the same output as with the previous execution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生与之前执行相同的输出。
- en: Processing data with the map function
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用映射函数处理数据
- en: 'The `datasets.Dataset.map()` function iterates over the dataset, applying a
    processing function to each example in the set, and modifies the content of the
    examples. The following execution shows a new `''len''` feature being added that
    denotes the length of a sentence:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`datasets.Dataset.map()` 函数在数据集上迭代，对集合中的每个示例应用处理函数，并修改示例的内容。以下执行显示添加一个新的 `''len''`
    特征，表示句子的长度：'
- en: '[PRE40]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This is the output of the preceding code snippet:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码片段的输出：
- en: '![Figure 2.16 – Cola dataset an with additional column ](img/B17123_02_16.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16 – 带有附加列的 Cola 数据集](img/B17123_02_16.jpg)'
- en: Figure 2.16 – Cola dataset an with additional column
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 带有附加列的 Cola 数据集
- en: 'As another example, the following piece of code cut the sentence after 20 characters.
    We do not create a new feature, but instead update the content of the sentence
    feature, as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个示例，以下代码片段在20个字符后剪切句子。我们不创建新特征，而是更新句子特性的内容，如下所示：
- en: '[PRE41]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is shown here:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 2.17 – Cola dataset with an update ](img/B17123_02_17.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17 – 带有更新的 Cola 数据集](img/B17123_02_17.jpg)'
- en: Figure 2.17 – Cola dataset with an update
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 带有更新的 Cola 数据集
- en: Working with local files
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用本地文件工作
- en: 'To load a dataset from local files in a `csv`, `text`, or `json`) to the generic
    `load_dataset()` loading script, as shown in the following code snippet. Under
    the `../data/` folder, there are three CSV files (`a.csv`, `b.csv`, and `c.csv`),
    which are randomly selected toy examples from the SST-2 dataset. We can load a
    single file, as shown in the `data1` object, merge many files, as in the `data2`
    object, or make dataset splits, as in `data3`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要从本地文件加载数据集（在`csv`、`text`或`json`中），以及加载脚本`load_dataset()`到通用加载脚本。如下代码片段所示，在`../data/`文件夹中，有三个CSV文件（`a.csv`、`b.csv`和`c.csv`），这些文件是从SST-2数据集中随机选择的玩具示例。我们可以加载单个文件，如`data1`对象所示，合并多个文件，如`data2`对象所示，或进行数据集分割，如`data3`所示：
- en: '[PRE42]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In order to get the files in other formats, we pass `json` or `text` instead
    of `csv`, as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以其他格式获取文件，我们传递`json`或`text`而不是`csv`，如下所示：
- en: '[PRE43]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: So far, we have discussed how to load, handle, and manipulate datasets that
    are either already hosted in the hub or are on our local drive. Now, we will study
    how to prepare datasets for transformer model training.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何加载、处理和操作数据集，这些数据集要么已经托管在Hub上，要么在我们的本地驱动器上。现在，我们将研究如何为Transformer模型训练准备数据集。
- en: Preparing a dataset for model training
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备数据集以进行模型训练
- en: 'Let''s start with the tokenization process. Each model has its own tokenization
    model that is trained before the actual language model. We will discuss this in
    detail in the next chapter. To use a tokenizer, we should have installed the `Transformer`
    library. The following example loads the tokenizer model from the pretrained `distilBERT-base-uncased`
    model. We use `map` and an anonymous function with `lambda` to apply a tokenizer
    to each split in `data3`. If `batched` is selected `True` in the `map` function,
    it provides a batch of examples to the `tokenizer` function. The `batch_size`
    value is `1000` by default, which is the number of examples per batch passed to
    the function. If not selected, the whole dataset is passed as a single batch.
    The code can be seen here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从标记化过程开始吧。每个模型都有自己的标记化模型，在实际的语言模型之前进行了训练。我们将在下一章节详细讨论这个问题。为了使用标记器，我们应该已经安装了`Transformer`库。下面的示例从预训练的`distilBERT-base-uncased`模型加载了标记器模型。我们使用`map`和带有`lambda`的匿名函数将标记器应用于`data3`中的每个拆分。如果在`map`函数中选择了`batched`为`True`，它会将一批例子传递给`tokenizer`函数。`batch_size`值默认为`1000`，这是传递给函数的每批例子的数量。如果没有选择，则整个数据集作为单个批次传递。代码可以在这里看到：
- en: '[PRE44]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As shown in the following output, we see the difference between `data3` and
    `encoded_data3`, where two additional features—`attention_mask` and `input_ids`—are
    added to the datasets accordingly. We already introduced these two features in
    the previous part in this chapter. Put simply, `input_ids` are the indices corresponding
    to each token in the sentence. They are expected features needed by the `Trainer`
    class of Transformer, which we will discuss in the next fine-tuning chapters.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如下输出所示，我们看到了`data3`和`encoded_data3`之间的区别，这里添加了两个额外特征——`attention_mask`和`input_ids`——并相应地添加到了数据集中。我们已经在本章的前面部分介绍了这两个特征。简而言之，`input_ids`是与句子中每个标记对应的索引。这些特征是Transformer的`Trainer`类需要的特征，我们将在接下来的微调章节中讨论。
- en: 'We mostly pass several sentences at once (called a `max_length` parameter—`12`
    in this toy example. We also truncate longer sentences to fit that maximum length.
    The code can be seen in the following snippet:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常一次传递多个句子（称为`max_length`参数，在这个玩具示例中为`12`）。我们还截断较长的句子以符合最大长度。代码可以在下面的代码片段中看到：
- en: '[PRE45]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We are done with the `datasets` library. Up to this point, we have evaluated
    all aspects of datasets. We have covered GLUE-like benchmarking, where classification
    metrics are taken into consideration. In the next section, we will focus on how
    to benchmark computational performance for speed and memory rather than classification.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了`datasets`库的使用。到目前为止，我们已经评估了数据集的所有方面。我们涵盖了类似于GLUE的基准测试，在这里，我们考虑了分类指标。在接下来的部分中，我们将专注于如何对速度和内存的计算性能进行基准测试，而不是分类。
- en: Benchmarking for speed and memory
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度和内存基准测试
- en: Just comparing the classification performance of large models on a specific
    task or a benchmark turns out to be no longer sufficient. We must now take care
    of the computational cost of a particular model for a given environment (`Transformer`
    library, `PyTorchBenchmark` and `TensorFlowBenchmark`, make it possible to benchmark
    models for both TensorFlow and PyTorch.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅比较大型模型在特定任务或基准上的分类性能已经不再足够。现在，我们必须关注特定环境对于特定模型的计算成本（`Transformer`库，`PyTorchBenchmark`和`TensorFlowBenchmark`使得可以为TensorFlow和PyTorch的模型进行基准测试。
- en: 'Before we start our experiment, we need to check our GPU capabilities with
    the following execution:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实验之前，我们需要检查我们的GPU能力，采用以下执行：
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The output is obtained from NVIDIA GeForce GTX 1050 (3 `Transformer` library
    currently only supports single-device benchmarking. When we conduct benchmarking
    on a GPU, we are expected to indicate on which GPU device the Python code will
    run, which is done by setting the `CUDA_VISIBLE_DEVICES` environment variable.
    For example, `export CUDA_VISIBLE_DEVICES=0`. `O` indicates that the first `cuda`
    device will be used.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是从NVIDIA GeForce GTX 1050 (3 `Transformer`库当前仅支持单设备基准测试。当我们在GPU上进行基准测试时，我们需要指示Python代码将在哪个GPU设备上运行，这是通过设置`CUDA_VISIBLE_DEVICES`环境变量来完成的。例如，`export
    CUDA_VISIBLE_DEVICES=0`。 `0`表示将使用第一个`cuda`设备。
- en: 'In the code example that follows, two grids are explored. We compare four randomly
    selected pretrained BERT models, as listed in the `models` array. The second parameter
    to be observed is `sequence_lengths`. We keep the batch size as `4`. If you have
    a better GPU capacity, you can extend the parameter search space with batch values
    in the range 4-64 and other parameters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码示例中，我们探索了两个网格。我们比较了四个随机选择的预训练BERT模型，如`models`数组中所列。要观察的第二个参数是`sequence_lengths`。我们将批量大小保持为`4`。如果您有更好的GPU容量，可以将批量值扩展到4-64的范围以及其他参数的搜索空间：
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Important note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: 'Benchmarking for TensorFlow: The code examples are for PyTorch benchmarking
    in this part. For TensorFlow benchmarking, we simply use the `TensorFlowBenchmarkArguments`
    and `TensorFlowBenchmark` counterpart classes instead.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的基准测试：本部分的代码示例是用于PyTorch基准测试的。对于TensorFlow的基准测试，我们简单地使用`TensorFlowBenchmarkArguments`和`TensorFlowBenchmark`相应的类。
- en: 'We are ready to conduct the benchmarking experiment by running the following
    code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已准备通过运行以下代码进行基准测试实验：
- en: '[PRE48]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This may take some time, depending on your CPU/GPU capacity and argument selection.
    If you face an out-of-memory problem for it, you should take the following actions
    to overcome this:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会花费一些时间，这取决于您的CPU/GPU容量和参数选择。如果遇到内存不足的问题，您应该采取以下措施来解决问题：
- en: Restart your kernel or your operating system.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新启动您的内核或操作系统。
- en: Delete all unnecessary objects in the memory before starting.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始之前删除内存中所有不必要的对象。
- en: Set a lower batch size, such as 2, or even 1.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置较低的批处理大小，例如2，甚至1。
- en: 'The following output indicates the inference speed performance. Since our search
    space has four different models and five different sequence lengths, we see 20
    rows in the results:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了推理速度性能。由于我们的搜索空间有四种不同的模型和五种不同的序列长度，我们在结果中看到了20行：
- en: '![Figure 2.18 – Inference speed performance ](img/B17123_02_018.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图2.18 – 推理速度性能](img/B17123_02_018.jpg)'
- en: Figure 2.18 – Inference speed performance
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – 推理速度性能
- en: 'Likewise, we see the inference memory usage for 20 different scenarios, as
    follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将看到20种不同情景的推理内存使用情况，如下所示：
- en: '![Figure 2.19 – Inference memory usage ](img/B17123_02_019.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图2.19 – 推理内存使用情况](img/B17123_02_019.jpg)'
- en: Figure 2.19 – Inference memory usage
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 – 推理内存使用情况
- en: 'To observe the memory usage across the parameters, we will plot them by using
    the `results` object that stores the statistics. The following execution will
    plot the time inference performance across models and sequence lengths:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察跨参数的内存使用情况，我们将使用存储统计数据的`results`对象进行绘制。以下执行将绘制模型和序列长度的推理时间性能：
- en: '[PRE49]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As shown in the following screenshot, two DistillBERT models showed close results
    and performed better than other two models. The `BERT-based-uncased` model performs
    poorly compared to the others, especially as the sequence length increases:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示，两个DistillBERT模型表现出近似的结果，并且比其他两个模型表现更好。`BERT-based-uncased`模型在特别是序列长度增加时表现不佳：
- en: '![Figure 2.20 – Inference speed result ](img/B17123_02_20.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图2.20 – 推理速度结果](img/B17123_02_20.jpg)'
- en: Figure 2.20 – Inference speed result
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 – 推理速度结果
- en: To plot the memory performance, you need to use the `memory_inference_result`
    result of the `results` object instead of `time_inference_result`, shown in the
    preceding code.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'For more interesting benchmarking examples, please check out the following
    links:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/benchmarks.html](https://huggingface.co/transformers/benchmarks.html)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are done with this section, we successfully completed this chapter.
    Congratulations on achieving the installation, running your first `hello-world`
    transformer program, working with the `datasets` library, and benchmarking!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a variety of introductory topics and also got
    our hands dirty with the `hello-world` transformer application. On the other hand,
    this chapter plays a crucial role in terms of applying what has been learned so
    far to the upcoming chapters. So, what has been learned so far? We took a first
    small step by setting the environment and system installation. In this context,
    the `anaconda` package manager helped us to install the necessary modules for
    the main operating systems. We also went through language models, community-provided
    models, and tokenization processes. Additionally, we introduced multitask (GLUE)
    and cross-lingual benchmarking (XTREME), which enables these language models to
    become stronger and more accurate. The `datasets` library was introduced, which
    facilitates efficient access to NLP datasets provided by the community. Finally,
    we learned how to evaluate the computational cost of a particular model in terms
    of memory usage and speed. Transformer frameworks make it possible to benchmark
    models for both TensorFlow and PyTorch.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The models that have been used in this section were already trained and shared
    with us by the community. Now, it is our turn to train a language model and disseminate
    it to the community.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train a BERT language model as well
    as a tokenizer, and look at how to share them with the community.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
