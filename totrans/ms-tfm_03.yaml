- en: '*Chapter 2*: A Hands-On Introduction to the Subject'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have had an overall look at the evolution of **Natural Language Processing**
    (**NLP**) using **Deep Learning** (**DL**)-based methods. We have learned some
    basic information about Transformer and their respective architecture. In this
    chapter, we are going to have a deeper look into how a transformer model can be
    used. Tokenizers and models, such as **Bidirectional Encoder Representations from
    Transformer** (**BERT**), will be described in more technical detail in this chapter
    with hands-on examples, including how to load a tokenizer/model and use community-provided
    pretrained models. But before using any specific model, we will understand the
    installation steps required to provide the necessary environment by using Anaconda.
    In the installation steps, installing libraries and programs on various operating
    systems such as Linux, Windows, and macOS will be covered. The installation of
    **PyTorch** and **TensorFlow**, in two versions of a **Central Processing Unit**
    (**CPU**) and a **Graphics Processing Unit** (**GPU**), is also shown. A quick
    jump into a **Google Colaboratory** (**Google Colab**) installation of the Transformer
    library is provided. There is also a section dedicated to using models in the
    PyTorch and TensorFlow frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The HuggingFace models repository is also another important part of this chapter,
    in which finding different models and steps to use various pipelines are discussed—for
    example, models such as **Bidirectional and Auto-Regressive Transformer** (**BART**),
    BERT, and **TAble PArSing** (**TAPAS**) are detailed, with a glance at **Generative
    Pre-trained Transformer 2** (**GPT-2**) text generation. However, this is purely
    an overview, and this part of the chapter relates to getting the environment ready
    and using pretrained models. No model training is discussed here as this is given
    greater significance in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: After everything is ready and we have understood how to use the `Transformer`
    library for inference by community-provided models, the `datasets` library is
    described. Here, we look at loading various datasets, benchmarks, and using metrics.
    Loading a specific dataset and getting data back from it is one of the main areas
    we look at here. Cross-lingual datasets and how to use local files with the `datasets`
    library are also considered here. The `map` and `filter` functions are important
    functions of the `datasets` library in terms of model training and are also examined
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is an essential part of the book because the `datasets` library
    is described in more detail here. It's also very important for you to understand
    how to use community-provided models and get the system ready for the rest of
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum all this up, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Transformer with Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with language models and tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with community-provided models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with benchmarks and datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking for speed and memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to install the libraries and software listed next. Although having
    the latest version is a plus, it is mandatory to install versions that are compatible
    with each other. For more information about the latest version installation for
    HuggingFace Transformer, take a look at their official web page at [https://huggingface.co/Transformer/installation.html](https://huggingface.co/transformers/installation.html):'
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer 4.0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch 1.1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow 2.4.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets 1.4.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, all the code shown in this chapter is available in this book's GitHub
    repository at https://github.com/PacktPublishing/Mastering-Transformer/tree/main/CH02.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/372ek48](https://bit.ly/372ek48)'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Transformer with Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Transformer` library. However, it is also possible to install this library
    without the aid of Anaconda. The main motivation to use Anaconda is to explain
    the process more easily and moderate the packages used.'
  prefs: []
  type: TYPE_NORMAL
- en: To start installing the related libraries, the installation of Anaconda is a
    mandatory step. Official guidelines provided by the Anaconda documentation offer
    simple steps to install it for common operating systems (macOS, Windows, and Linux).
  prefs: []
  type: TYPE_NORMAL
- en: Installation on Linux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many distributions of Linux are available for users to enjoy, but among them,
    **Ubuntu** is one of the preferred ones. In this section, the steps to install
    Anaconda are covered for Linux. Proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Anaconda installer for Linux from https://www.anaconda.com/products/individual#Downloads
    and go to the Linux section, as illustrated in the following screenshot:![Figure
    2.1 – Anaconda download link for Linux ](img/B17123_02_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.1 – Anaconda download link for Linux
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run a `bash` command to install it and complete the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the Terminal and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Press *Enter* to see the license agreement and press *Q* if you do not want
    to read it all, and then do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Yes** to agree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `conda` root environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After running a `python` command from the Terminal, you should see an Anaconda
    prompt after the Python version information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can access Anaconda Navigator by running an `anaconda-navigator` command
    from the Terminal. As a result, you will see the Anaconda **Graphical User Interface**
    (**GUI**) start loading the related modules, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Anaconda Navigator ](img/B17123_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Anaconda Navigator
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Installation on Windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps describe how you can install Anaconda on Windows operating
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the installer from https://www.anaconda.com/products/individual#Downloads
    and go to the Windows section, as illustrated in the following screenshot:![Figure
    2.3 – Anaconda download link for Windows ](img/B17123_02_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.3 – Anaconda download link for Windows
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the installer and follow the guide by clicking the **I Agree** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the location for installation, as illustrated in the following screenshot:![Figure
    2.4 – Anaconda installer for Windows ](img/B17123_02_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.4 – Anaconda installer for Windows
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Don't forget to check the `python` command from the Windows shell or the Windows
    command line:![Figure 2.5 – Anaconda installer advanced options ](img/B17123_02_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.5 – Anaconda installer advanced options
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Follow the rest of the installation instructions and finish the installation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should now be able to start Anaconda Navigator from the **Start** menu.
  prefs: []
  type: TYPE_NORMAL
- en: Installation on macOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps must be followed to install Anaconda on macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the installer from https://www.anaconda.com/products/individual#Downloads
    and go to the macOS section, as illustrated in the following screenshot:![Figure
    2.6 – Anaconda download link for macOS ](img/B17123_02_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.6 – Anaconda download link for macOS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the installer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Follow the instructions and click the **Install** button to install macOS in
    a predefined location, as illustrated in the following screenshot. You can change
    the default directory, but this is not recommended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Anaconda installer for macOS ](img/B17123_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Anaconda installer for macOS
  prefs: []
  type: TYPE_NORMAL
- en: Once you finish the installation, you should be able to access Anaconda Navigator.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow, PyTorch, and Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The installation of TensorFlow and PyTorch as two major libraries that are used
    for DL can be made through `pip` or `conda` itself. `conda` provides a **Command-Line
    Interface** (**CLI**) for easier installation of these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a clean installation and to avoid interrupting other environments, it is
    better to create a `conda` environment for the `huggingface` library. You can
    do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will create an empty environment for installing other libraries.
    Once created, we will need to activate it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Installation of the `Transformer` library is easily done by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `-c` argument in the `conda install` command lets Anaconda use additional
    channels to search for libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that it is a requirement to have TensorFlow and PyTorch installed because
    the `Transformer` library uses both of these libraries. An additional note is
    the easy handling of CPU and GPU versions of TensorFlow by Conda. If you simply
    put `–gpu` after `tensorflow`, it will install the GPU version automatically.
    For installation of PyTorch through the `cuda` library (GPU version), you are
    required to have related libraries such as `cuda`, but `conda` handles this automatically
    and no further manual setup or installation is required. The following screenshot
    shows how `conda` automatically takes care of installing the PyTorch GPU version
    by installing the related `cudatoolkit` and `cudnn` libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Conda installing PyTorch and related cuda libraries ](img/B17123_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Conda installing PyTorch and related cuda libraries
  prefs: []
  type: TYPE_NORMAL
- en: Note that all of these installations can also be done without `conda`, but the
    reason behind using Anaconda is its ease of use. In terms of using environments
    or installing GPU versions of TensorFlow or PyTorch, Anaconda works like magic
    and is a good time saver.
  prefs: []
  type: TYPE_NORMAL
- en: Installing using Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if the utilization of Anaconda saves time and is useful, in most cases,
    not everyone has such a good and reasonable computation resource available. Google
    Colab is a good alternative in such cases. Installation of the `Transformer` library
    in Colab is carried out with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: An exclamation mark before the statement makes the code run in a Colab shell,
    which is equivalent to running the code in the Terminal instead of running it
    using a Python interpreter. This will automatically install the `Transformer`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Working with language models and tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at using the `Transformer` library with language
    models, along with their related **tokenizers**. In order to use any specified
    language model, we first need to import it. We will start with the BERT model
    provided by Google and use its pretrained version, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line of the preceding code snippet imports the BERT tokenizer, and
    the second line downloads a pretrained tokenizer for the BERT base version. Note
    that the uncased version is trained with uncased letters, so it does not matter
    whether the letters appear in upper- or lowercase. To test and see the output,
    you must run the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`input_ids` shows the token ID for each token, and `token_type_ids` shows the
    type of each token that separates the first and second sequence, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Sequence separation for BERT ](img/B17123_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Sequence separation for BERT
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` is a mask of 0s and 1s that is used to show the start and
    end of a sequence for the transformer model in order to prevent unnecessary computations.
    Each tokenizer has its own way of adding special tokens to the original sequence.
    In the case of the BERT tokenizer, it adds a `[CLS]` token to the beginning and
    an `[SEP]` token to the end of the sequence, which can be seen by 101 and 102\.
    These numbers come from the token IDs of the pretrained tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tokenizer can be used for both PyTorch- and TensorFlow-based `Transformer`
    models. In order to have output for each one, `pt` and `tf` keywords must be used
    in `return_tensors`. For example, you can use a tokenizer by simply running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`encoded_input` has the tokenized text to be used by the PyTorch model. In
    order to run the model—for example, the BERT base model—the following code can
    be used to download the model from the `huggingface` model repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the tokenizer can be passed to the downloaded model with the
    following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will give you the output of the model in the form of embeddings and cross-attention
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'When loading and importing models, you can specify which version of a model
    you are trying to use. If you simply put `TF` before the name of a model, the
    `Transformer` library will load the TensorFlow version of it. The following code
    shows how to load and use the TensorFlow version of BERT base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For specific tasks such as filling masks using language models, there are pipelines
    designed by `huggingface` that are ready to use. For example, a task of filling
    a mask can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will produce the following output, which shows the scores and possible
    tokens to be placed in the `[MASK]` token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a neat view with pandas, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Output of the BERT mask filling ](img/B17123_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Output of the BERT mask filling
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to load and use a pretrained BERT model and have
    understood the basics of tokenizers, as well as the difference between PyTorch
    and TensorFlow versions of the models. In the next section, we will learn to work
    with community-provided models by loading different models, reading the related
    information provided by the model authors and using different pipelines such as
    text-generation or **Question Answering** (**QA**) pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Working with community-provided models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hugging Face** has tons of community models provided by collaborators from
    large **Artificial Intelligence** (**AI**) and **Information Technology** (**IT**)
    companies such as Google and Facebook. There are also many interesting models
    that individuals and universities provide. Accessing and using them is also very
    easy. To start, you should visit the Transformer models directory available at
    their website (https://huggingface.co/models), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Hugging Face models repository ](img/B17123_02_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Hugging Face models repository
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these models, there are also many good and useful datasets available
    for NLP tasks. To start using some of these models, you can explore them by keyword
    searches, or just specify your major NLP task and pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we are looking for a table QA model. After finding a model that
    we are interested in, a page such as the following one will be available from
    the Hugging Face website ([https://huggingface.co/google/tapas-base-finetuned-wtq](https://huggingface.co/google/tapas-base-finetuned-wtq)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – TAPAS model page ](img/B17123_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – TAPAS model page
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right side, there is a panel where you can test this model. Note that
    this is a table QA model that can answer questions about a table provided to the
    model. If you ask a question, it will reply by highlighting the answer. The following
    screenshot shows how it gets input and provides an answer for a specific table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Table QA using TAPAS ](img/B17123_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Table QA using TAPAS
  prefs: []
  type: TYPE_NORMAL
- en: 'Each model has a page provided by the model authors that is also known as a
    `huggingface` repository page and take a look at the example provided by the authors
    ([https://huggingface.co/gpt2](https://huggingface.co/gpt2)), as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Text-generation code example from Hugging Face GPT-2 page ](img/B17123_02_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Text-generation code example from the Hugging Face GPT-2 page
  prefs: []
  type: TYPE_NORMAL
- en: 'Using pipelines is recommended because all the dirty work is taken care of
    by the `Transformer` library. As another example, let''s assume you need an out-of-the-box
    zero-shot classifier. The following code snippet shows how easy it is to implement
    and use such a pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will provide the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are done with the installation and the `hello-world` application part. So
    far, we have introduced the installation process, completed the environment settings,
    and experienced the first transformer pipeline. In the next part, we will introduce
    the `datasets` library, which will be our essential utility in the experimental
    chapters to come.
  prefs: []
  type: TYPE_NORMAL
- en: Working with benchmarks and datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before introducing the `datasets` library, we'd better talk about important
    benchmarks such as `Transformer` library, we are able to transfer what we have
    learned from a particular task to a related task, which is called **Transfer Learning**
    (**TL**). By transferring representations between related problems, we are able
    to train general-purpose models that share common linguistic knowledge across
    tasks, also known as **Multi-Task Learning** (**MTL**). Another aspect of TL is
    to transfer knowledge across natural languages (multilingual models).
  prefs: []
  type: TYPE_NORMAL
- en: Important benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part, we will introduce the important benchmarks that are widely used
    by transformer-based architectures. These benchmarks exclusively contribute a
    lot to MTL and to multilingual and zero-shot learning, including many challenging
    tasks. We will look at the following benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GLUE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SuperGLUE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XTREME**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGLUE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQuAD**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sake of using fewer pages, we give details of the tasks for only the
    GLUE benchmark, so let's look at this benchmark first.
  prefs: []
  type: TYPE_NORMAL
- en: GLUE benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent studies addressed the fact that multitask training approaches can achieve
    better results than single-task learning as a particular model for a task. In
    this direction, the **GLUE** benchmark has been introduced for MTL, which is a
    collection of tools and datasets for evaluating the performance of MTL models
    across a list of tasks. It offers a public leaderboard for monitoring submission
    performance on the benchmark, along with a single-number metric summarizing 11
    tasks. This benchmark includes many sentence-understanding tasks that are based
    on existing tasks covering various datasets of differing size, text type, and
    difficulty levels. The tasks are categorized under three types, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-sentence tasks**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoLA**: The **Corpus of Linguistic Acceptability** dataset. This task consists
    of English acceptability judgments drawn from articles on linguistic theory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pos`/`neg` labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity and paraphrase tasks**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MRPC**: The **Microsoft Research Paraphrase Corpus** dataset. This task looks
    at whether the sentences in a pair are semantically equivalent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QQP**: The **Quora Question Pairs** dataset. This task decides whether a
    pair of questions is semantically equivalent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**STS-B**: The **Semantic Textual Similarity Benchmark** dataset. This task
    is a collection of sentence pairs drawn from news headlines, with a similarity
    score between 1 and 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference tasks**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MNLI**: The **Multi-Genre Natural Language Inference** corpus. This is a
    collection of sentence pairs with textual entailment. The task is to predict whether
    the text entails a hypothesis (entailment), contradicts the hypothesis (contradiction),
    or neither (neutral).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QNLI**: **Question Natural Language Inference** dataset. This is a converted
    version of SquAD. The task is to check whether a sentence contains the answer
    to a question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RTE**: The **Recognizing Textual Entailment** dataset. This is a task of
    textual entailment challenges to combine data from various sources. This dataset
    is similar to the previous QNLI dataset, where the task is to check whether a
    first text entails a second one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WNLI**: The **Winograd Natural Language Inference** schema challenge. This
    is originally a pronoun resolution task linking a pronoun and a phrase in a sentence.
    GLUE converted the problem into sentence-pair classification, as detailed next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuperGLUE benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like Glue, **SuperGLUE** is a new benchmark styled with a new set of more difficult
    language-understanding tasks and offers a public leaderboard of around currently
    eight language tasks, drawing on existing data, associated with a single-number
    performance metric like that of GLUE. The motivation behind it is that as of writing
    this book, the current state-of-the-art GLUE Score (90.8) surpasses human performance
    (87.1). Thus, SuperGLUE provides a more challenging and diverse task toward general-purpose,
    language-understanding technologies.
  prefs: []
  type: TYPE_NORMAL
- en: You can access both GLUE and SuperGLUE benchmarks at [gluebenchmark.com](http://gluebenchmark.com).
  prefs: []
  type: TYPE_NORMAL
- en: XTREME benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, NLP researchers have increasingly focused on learning general-purpose
    representations rather than a single task that can be applied to many related
    tasks. Another way of building a general-purpose language model is by using multilingual
    tasks. It has been observed that recent multilingual models such as **Multilingual
    BERT** (**mBERT**) and XLM-R pretrained on massive amounts of multilingual corpora
    have performed better when transferring them to other languages. Thus, the main
    advantage here is that cross-lingual generalization enables us to build successful
    NLP applications in resource-poor languages through zero-shot cross-lingual transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this direction, the **XTREME** benchmark has been designed. It currently
    includes around 40 different languages belonging to 12 language families and includes
    9 different tasks that require reasoning for various levels of syntax or semantics.
    However, it is still challenging to scale up a model to cover over 7,000 world
    languages and there exists a trade-off between language coverage and model capability.
    Please check out the following link for more details on this: [https://sites.research.google/xtreme](https://sites.research.google/xtreme).'
  prefs: []
  type: TYPE_NORMAL
- en: XGLUE benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**XGLUE** is another cross-lingual benchmark to evaluate and improve the performance
    of cross-lingual pretrained models for **Natural Language Understanding** (**NLU**)
    and **Natural Language Generation** (**NLG**). It originally consisted of 11 tasks
    over 19 languages. The main difference from XTREME is that the training data is
    only available in English for each task. This forces the language models to learn
    only from the textual data in English and transfer this knowledge to other languages,
    which is called zero-shot cross-lingual transfer capability. The second difference
    is that it has tasks for NLU and NLG at the same time. Please check out the following
    link for more details on this: [https://microsoft.github.io/XGLUE/](https://microsoft.github.io/XGLUE/).'
  prefs: []
  type: TYPE_NORMAL
- en: SQuAD benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**SQuAD** is a widely used QA dataset in the NLP field. It provides a set of
    QA pairs to benchmark the reading comprehension capabilities of NLP models. It
    consists of a list of questions, a reading passage, and an answer annotated by
    crowdworkers on a set of Wikipedia articles. The answer to the question is a span
    of text from the reading passage. The initial version, SQuAD1.1, doesn''t have
    an unanswerable option where the datasets are collected, so each question has
    an answer to be found somewhere in the reading passage. The NLP model is forced
    to answer the question even if this appears impossible. SQuAD2.0 is an improved
    version, whereby the NLP models must not only answer questions when possible,
    but should also abstain from answering when it is impossible to answer. SQuAD2.0
    contains 50,000 unanswerable questions written adversarially by crowdworkers to
    look similar to answerable ones. Additionally, it also has 100,000 questions taken
    from SQuAD1.1.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the datasets with an Application Programming Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `datasets` library provides a very efficient utility to load, process, and
    share datasets with the community through the Hugging Face hub. As with TensorFlow
    datasets, it makes it easier to download, cache, and dynamically load the sets
    directly from the original dataset host upon request. The library also provides
    evaluation metrics along with the data. Indeed, the hub does not hold or distribute
    the datasets. Instead, it keeps all information about the dataset, including the
    owner, preprocessing script, description, and download link. We need to check
    whether we have permission to use the datasets under their corresponding license.
    To see other features, please check the `dataset_infos.json` and `DataSet-Name.py`
    files of the corresponding dataset under the GitHub repository, at [https://github.com/huggingface/datasets/tree/master/datasets](https://github.com/huggingface/datasets/tree/master/datasets)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by installing the `dataset` library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code automatically loads the `cola` dataset using the Hugging
    Face hub. The `datasets.load_dataset()` function downloads the loading script
    from the actual path if the data is not cached already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusability of the datasets: As you rerun the code a couple of times, the `datasets`
    library starts caching your loading and manipulation request. It first stores
    the dataset and starts caching your operations on the dataset, such as splitting,
    selection, and sorting. You will see a warning message such as **reusing dataset
    xtreme (/home/savas/.cache/huggingface/dataset...)** or **loading cached sorted...**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we downloaded the `cola` dataset from the GLUE benchmark
    and selected a few examples from the `train` split of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are 661 NLP datasets and 21 metrics for diverse tasks, as
    the following code snippet shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: A dataset might have several configurations. For instance, GLUE, as an aggregated
    benchmark, has many subsets, such as CoLA, SST-2, and MRPC, as we mentioned before.
    To access each GLUE benchmark dataset, we pass two arguments, where the first
    is `glue` and the second is a particular dataset of its example dataset (`cola`
    or `sst2`) that can be chosen. Likewise, the Wikipedia dataset has several configurations
    provided for several languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset comes with the `DatasetDict` object, including several `Dataset`
    instances. When the split selection `(split=''...'')` is used, we get `Dataset`
    instances. For example, the `CoLA` dataset comes with `DatasetDict`, where we
    have three splits: *train*, *validation*, and *test*. While train and validation
    datasets include two labels (`1` for acceptable, `0` for unacceptable), the label
    value of test split is `-1`, which means no-label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the structure of the `CoLA` dataset object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset object has some additional metadata information that might be helpful
    for us: `split`, `description`, `citation`, `homepage`, `license`, and `info`.
    Let''s run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The GLUE benchmark provides many datasets, as mentioned previously. Let''s
    download the MRPC dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, to access other GLUE tasks, we will change the second parameter,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to apply a sanity check of data availability, run the following piece
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: XTREME (working with a cross-lingual dataset) is another popular cross-lingual
    dataset that we already discussed. Let's pick the `MLQA` example from the XTREME
    set. MLQA is a subset of the XTREME benchmark, which is designed for assessing
    the performance of cross-lingual QA models. It includes about 5,000 extractive
    QA instances in the SQuAD format across seven languages, which are English, German,
    Arabic, Hindi, Vietnamese, Spanish, and Simplified Chinese.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `MLQA.en.de` is an English-German QA example dataset and can be
    loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It could be more convenient to view it within a pandas DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – English-German cross-lingual QA dataset ](img/B17123_02_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – English-German cross-lingual QA dataset
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation with the datasets library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets come with many dictionaries of subsets, where the `split` parameter
    is used to decide which subset(s) or portion of the subset is to be loaded. If
    this is `none` by default, it will return a dataset dictionary of all subsets
    (`train`, `test`, `validation`, or any other combination). If the `split` parameter
    is specified, it will return a single dataset rather than a dictionary. For the
    following example, we retrieve a `train` split of the `cola` dataset only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get a mixture of `train` and `validation` subsets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `split` expression means that the first 300 examples of `train` and the
    last 30 examples of `validation` are obtained as `cola_sel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply different combinations, as shown in the following split examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first 100 examples from `train` and `validation`, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '50% of `train` and the last 30% of `validation`, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first 20% of `train` and the examples in the slice [30:50] from `validation`,
    as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Sorting, indexing, and shuffling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following execution calls the `sort()` function of the `cola_sel` object.
    We see the first 15 and the last 15 labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We are already familiar with Python slicing notation. Likewise, we can also
    access several rows using similar slice notation or with a list of indices, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We shuffle the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Seed value: When shuffling, we need to pass a seed value to control the randomness
    and achieve a consistent output between the author and the reader.'
  prefs: []
  type: TYPE_NORMAL
- en: Caching and reusability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using cached files allows us to load large datasets by means of memory mapping
    (if datasets fit on the drive) by using a fast backend. Such smart caching helps
    in saving and reusing the results of operations executed on the drive. To see
    cache logs with regard to the dataset, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Dataset filter and map function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We might want to work with a specific selection of a dataset. For instance,
    we can retrieve sentences only, including the term `kick` in the `cola` dataset,
    as shown in the following execution. The `datasets.Dataset.filter()` function
    returns sentences including `kick` where an anonymous function and a `lambda`
    keyword are applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following filtering is used to get positive (acceptable) examples from
    the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, we might not know the integer code of a class label. Suppose
    we have many classes, and the code of the `culture` class is hard to remember
    out of 10 classes. Instead of giving integer code `1` in our preceding example,
    which is the code for `acceptable`, we can pass an `acceptable` label to the `str2int()`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This produces the same output as with the previous execution.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data with the map function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `datasets.Dataset.map()` function iterates over the dataset, applying a
    processing function to each example in the set, and modifies the content of the
    examples. The following execution shows a new `''len''` feature being added that
    denotes the length of a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output of the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Cola dataset an with additional column ](img/B17123_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Cola dataset an with additional column
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, the following piece of code cut the sentence after 20 characters.
    We do not create a new feature, but instead update the content of the sentence
    feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Cola dataset with an update ](img/B17123_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Cola dataset with an update
  prefs: []
  type: TYPE_NORMAL
- en: Working with local files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To load a dataset from local files in a `csv`, `text`, or `json`) to the generic
    `load_dataset()` loading script, as shown in the following code snippet. Under
    the `../data/` folder, there are three CSV files (`a.csv`, `b.csv`, and `c.csv`),
    which are randomly selected toy examples from the SST-2 dataset. We can load a
    single file, as shown in the `data1` object, merge many files, as in the `data2`
    object, or make dataset splits, as in `data3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get the files in other formats, we pass `json` or `text` instead
    of `csv`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have discussed how to load, handle, and manipulate datasets that
    are either already hosted in the hub or are on our local drive. Now, we will study
    how to prepare datasets for transformer model training.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a dataset for model training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s start with the tokenization process. Each model has its own tokenization
    model that is trained before the actual language model. We will discuss this in
    detail in the next chapter. To use a tokenizer, we should have installed the `Transformer`
    library. The following example loads the tokenizer model from the pretrained `distilBERT-base-uncased`
    model. We use `map` and an anonymous function with `lambda` to apply a tokenizer
    to each split in `data3`. If `batched` is selected `True` in the `map` function,
    it provides a batch of examples to the `tokenizer` function. The `batch_size`
    value is `1000` by default, which is the number of examples per batch passed to
    the function. If not selected, the whole dataset is passed as a single batch.
    The code can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the following output, we see the difference between `data3` and
    `encoded_data3`, where two additional features—`attention_mask` and `input_ids`—are
    added to the datasets accordingly. We already introduced these two features in
    the previous part in this chapter. Put simply, `input_ids` are the indices corresponding
    to each token in the sentence. They are expected features needed by the `Trainer`
    class of Transformer, which we will discuss in the next fine-tuning chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mostly pass several sentences at once (called a `max_length` parameter—`12`
    in this toy example. We also truncate longer sentences to fit that maximum length.
    The code can be seen in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We are done with the `datasets` library. Up to this point, we have evaluated
    all aspects of datasets. We have covered GLUE-like benchmarking, where classification
    metrics are taken into consideration. In the next section, we will focus on how
    to benchmark computational performance for speed and memory rather than classification.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking for speed and memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just comparing the classification performance of large models on a specific
    task or a benchmark turns out to be no longer sufficient. We must now take care
    of the computational cost of a particular model for a given environment (`Transformer`
    library, `PyTorchBenchmark` and `TensorFlowBenchmark`, make it possible to benchmark
    models for both TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start our experiment, we need to check our GPU capabilities with
    the following execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The output is obtained from NVIDIA GeForce GTX 1050 (3 `Transformer` library
    currently only supports single-device benchmarking. When we conduct benchmarking
    on a GPU, we are expected to indicate on which GPU device the Python code will
    run, which is done by setting the `CUDA_VISIBLE_DEVICES` environment variable.
    For example, `export CUDA_VISIBLE_DEVICES=0`. `O` indicates that the first `cuda`
    device will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code example that follows, two grids are explored. We compare four randomly
    selected pretrained BERT models, as listed in the `models` array. The second parameter
    to be observed is `sequence_lengths`. We keep the batch size as `4`. If you have
    a better GPU capacity, you can extend the parameter search space with batch values
    in the range 4-64 and other parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking for TensorFlow: The code examples are for PyTorch benchmarking
    in this part. For TensorFlow benchmarking, we simply use the `TensorFlowBenchmarkArguments`
    and `TensorFlowBenchmark` counterpart classes instead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are ready to conduct the benchmarking experiment by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This may take some time, depending on your CPU/GPU capacity and argument selection.
    If you face an out-of-memory problem for it, you should take the following actions
    to overcome this:'
  prefs: []
  type: TYPE_NORMAL
- en: Restart your kernel or your operating system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete all unnecessary objects in the memory before starting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set a lower batch size, such as 2, or even 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following output indicates the inference speed performance. Since our search
    space has four different models and five different sequence lengths, we see 20
    rows in the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Inference speed performance ](img/B17123_02_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Inference speed performance
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, we see the inference memory usage for 20 different scenarios, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Inference memory usage ](img/B17123_02_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Inference memory usage
  prefs: []
  type: TYPE_NORMAL
- en: 'To observe the memory usage across the parameters, we will plot them by using
    the `results` object that stores the statistics. The following execution will
    plot the time inference performance across models and sequence lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following screenshot, two DistillBERT models showed close results
    and performed better than other two models. The `BERT-based-uncased` model performs
    poorly compared to the others, especially as the sequence length increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Inference speed result ](img/B17123_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Inference speed result
  prefs: []
  type: TYPE_NORMAL
- en: To plot the memory performance, you need to use the `memory_inference_result`
    result of the `results` object instead of `time_inference_result`, shown in the
    preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more interesting benchmarking examples, please check out the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/benchmarks.html](https://huggingface.co/transformers/benchmarks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are done with this section, we successfully completed this chapter.
    Congratulations on achieving the installation, running your first `hello-world`
    transformer program, working with the `datasets` library, and benchmarking!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a variety of introductory topics and also got
    our hands dirty with the `hello-world` transformer application. On the other hand,
    this chapter plays a crucial role in terms of applying what has been learned so
    far to the upcoming chapters. So, what has been learned so far? We took a first
    small step by setting the environment and system installation. In this context,
    the `anaconda` package manager helped us to install the necessary modules for
    the main operating systems. We also went through language models, community-provided
    models, and tokenization processes. Additionally, we introduced multitask (GLUE)
    and cross-lingual benchmarking (XTREME), which enables these language models to
    become stronger and more accurate. The `datasets` library was introduced, which
    facilitates efficient access to NLP datasets provided by the community. Finally,
    we learned how to evaluate the computational cost of a particular model in terms
    of memory usage and speed. Transformer frameworks make it possible to benchmark
    models for both TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The models that have been used in this section were already trained and shared
    with us by the community. Now, it is our turn to train a language model and disseminate
    it to the community.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train a BERT language model as well
    as a tokenizer, and look at how to share them with the community.
  prefs: []
  type: TYPE_NORMAL
