- en: Getting Started with Deep Learning Using PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**) has revolutionized industry after industry. It was
    once famously described by Andrew Ng on Twitter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Artificial intelligence is the new electricity!"*'
  prefs: []
  type: TYPE_NORMAL
- en: Electricity transformed countless industries; now, **artificial intelligence**
    (**AI**) will do the same.
  prefs: []
  type: TYPE_NORMAL
- en: AI and DL are used as synonyms, but there are substantial differences between
    the two. Let's demystify the terminology that's used in the industry so that you,
    as a practitioner, will be able to differentiate between signal and noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following different parts of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring artificial intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning in the real world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up PyTorch 1.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Countless articles discussing AI are published every day. The trend has increased
    in the last 2 years. There are several definitions of AI floating around the web,
    with my favorite being *the* *automation of intellectual tasks normally* *performed
    by humans*.
  prefs: []
  type: TYPE_NORMAL
- en: The history of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you've picked up this book, you may be well aware of the recent hype in
    AI. But it all started when John McCarthy, then a young assistant professor at
    Dartmouth, coined the term *artificial intelligence* in 1995, which he defined
    as a field pertaining to the science and engineering of intelligent machines.
    This kick-started the first wave of AI, which was primarily driven by symbolic
    reasoning; its outcomes were astonishing, to say the least. AI that was developed
    during this time was capable of reading and solving high-school Algebra problems
    [STUDENT], proving theorems in Geometry [SAINT], and learning the English language
    [SHRDLU]. Symbolic reasoning is the use of complex rules nested in if-then statements.
  prefs: []
  type: TYPE_NORMAL
- en: The most promising work in this era, though, was the perceptron, which was introduced
    in 1958 by Frank Rosenblatt. The perceptron, when combined with intelligent optimization techniques
    that were discovered later, laid the foundations for deep learning as we know
    it today.
  prefs: []
  type: TYPE_NORMAL
- en: It wasn't plain sailing for AI, though, since the funding in the field significantly
    reduced during lean periods, mostly due to overpromising initial discoveries and,
    as we were yet to discover, a lack of data and compute power. The rise in prominence
    of **machine learning** (**ML**) in the early nineties bucked the trend and created
    significant interest in the field. First, we need to understand the paradigm of
    ML and its relationship with DL.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in the real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is a subfield of AI that uses algorithms and statistical techniques to perform
    a task without the use of any explicit instructions. Instead, it relies on underlying
    statistical patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: To build successful machine learning models, we need to provide ML algorithms
    with labeled data. The success of this approach was heavily dependent on the available
    data and compute power so that large amounts of data could be used.
  prefs: []
  type: TYPE_NORMAL
- en: So, why DL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most ML algorithms perform well on structured data, such as sales predictions,
    recommendation systems, and marketing personalization. An important factor for
    any ML algorithm is feature engineering and data scientists need to spend a lot
    of time exploring possible features with high predictive power for ML algorithms.
    In certain domains, such as computer vision and **natural language processing**
    (**NLP**), feature engineering is challenging as features that are important for
    one task may not hold up well for other tasks. This is where DL excels—the algorithm
    itself engineers features in a non-linear space so that they are important for
    a particular task.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional ML algorithms still outperform DL methods when there is a paucity
    of data, but as data increases, the performance of traditional machine learning
    algorithms tends to plateau and deep learning algorithms tend to significantly
    outperform other learning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the relationship DL has with ML and AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5af5f990-efc1-438a-adad-f17373307f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize this, DL is a subfield of machine learning; feature engineering
    is where the algorithm non-linearly explores its space.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL is at the center of the most important innovations of the 21^(st) century,
    from detecting tumors with a lower error rate than radiologists to self-driving
    cars. Let's quickly look at a few DL applications.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic translation of text from images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A 2015 blog from Google details how the team at Google can translate text from
    images. The following image shows the steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c92634a-07ee-4595-b502-5d9ebfc32e10.png)'
  prefs: []
  type: TYPE_IMG
- en: First, a DL algorithm is used to perform **optical character recognition** (**OCR**)
    and recognize the text from the image. Later, another DL algorithm is used to
    translate the text from the source language to the language of choice. The improvements
    we see today in machine translation are attributed to the switch to DL from traditional
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection in self-driving cars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tesla did a deep dive into their autonomous driving system for investors in
    2019, where they mentioned how they use deep neural networks to detect objects
    from cameras in the car. The output of this algorithm is used by the proprietary
    self-driving policy developed by Tesla:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec4ad571-9360-44e3-8326-912fd5c7a0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image is the output of an object detection deep learning network.
    The semantic information it has captured from the visual image is crucial for
    self-driving tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It used to be extremely hard to write code for deep learning algorithms since
    writing code for the learning step, which involved chaining complex derivatives,
    was extremely error-prone and lengthy. DL frameworks used ingenious heuristics
    to automate the computation of these complex derivatives. The choice of such heuristics
    significantly changes the way these frameworks work. The following diagram shows
    the current ecosystem of DL frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9e22904-cd5b-43c5-959c-056577097188.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorFlow is the most popular deep learning framework but the simplicity and
    usefulness of PyTorch has made DL research accessible to a lot of people. Let's
    look at why using PyTorch can speed up our DL research and development time significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Why PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To compute complex chained derivatives, TensorFlow uses a **Define and Run**
    paradigm, whereas PyTorch uses a more ingenuous **Define by Run** paradigm. Let''s
    delve deeper into this by looking at the following image, where we will be computing
    the sum of the series *1 + 1 / 2 + 1 / 4 + 1 / 8 ...*, which should add up to
    2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fdbadfc-20f4-4a8a-a02b-57dc759cf1b5.png)'
  prefs: []
  type: TYPE_IMG
- en: We can immediately see how succinct and simple it is to write code to perform
    operations in PyTorch. This difference is more widely noticeable in more complex
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: As the head of AI at Tesla and one of the biggest thought leaders in computer
    vision at the moment, Andrej Karpathy tweeted—*I've been using PyTorch for a few
    months now and I've never felt better. I have more energy. My skin is clearer.
    My eyesight has improved.* PyTorch definitely makes the lives of people writing
    DL code better.
  prefs: []
  type: TYPE_NORMAL
- en: This **Define by Run** paradigm also has many advantages other than just creating
    cleaner and simpler code. Debugging also becomes extremely easy and all of the
    tools that you currently use to debug Python code can be used with PyTorch as
    well. This is a significant advantage because, as networks get more and more complex,
    debugging your networks with ease will be a lifesaver.
  prefs: []
  type: TYPE_NORMAL
- en: What's new in PyTorch v1.x?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch 1.x expands on its flexibility and tries to unify research and production
    capabilities into a single framework. Caffe2, a production-grade deep learning
    framework, is integrated into PyTorch, allowing us to deploy PyTorch models to
    mobile operating systems and high-performance C++ services. PyTorch v1.0 also
    natively supports exporting models into the ONNX format, which allows PyTorch
    models to be imported into other DL frameworks. It truly is an exciting time to
    be a PyTorch developer!
  prefs: []
  type: TYPE_NORMAL
- en: CPU versus GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CPUs have fewer but more powerful compute cores, whereas GPUs have a large
    number of lower-performant cores. CPUs are more suited to sequential tasks, whereas
    GPUs are suitable for tasks with significant parallelization. In summary, a CPU
    can execute large, sequential instructions but can only execute a small number
    of instructions in parallel in contrast to a GPU, which can execute hundreds of
    small instructions in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd4c49c5-0ffa-4eaf-a81e-69e4735ca792.png)'
  prefs: []
  type: TYPE_IMG
- en: While using DL, we will be performing a large number of linear algebraic operations
    that are more suited to a GPU and can provide a significant boost in terms of
    the time it takes to train a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: What is CUDA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA is a framework developed by NVIDIA that allows us to use **General Purpose
    Computing on Graphics Processing Units** (**GPGPU**). It is a widely used framework
    written in C++ that allows us to write general-purpose programs that run on GPUs.
    Almost all deep learning frameworks leverage CUDA to execute instructions on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Which GPUs should we use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since most deep learning frameworks, including PyTorch, use NVIDIA''s CUDA
    framework, it is highly recommended that you buy and use a NVIDIA GPU for deep
    learning. Let''s do a quick comparison of a few NVIDIA GPU models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecafa3fd-4629-4ba1-8cff-76d2ad6949a6.png)'
  prefs: []
  type: TYPE_IMG
- en: What should you do if you don't have a GPU?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a lot of Cloud services such as Azure, AWS, and GCP that provide instances
    that have GPUs and all the required deep learning software preinstalled. FloydHub
    is a great tool for running deep learning models in the cloud. However, the single
    most important tool you should definitely check out is Google's Colaboratory,
    which provides high-performance GPUs for free so that you can run deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up PyTorch v1.x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we will be using the Anaconda Distribution for Python
    and PyTorch 1.x. You can follow along with the code by executing the relevant
    command based on your current configuration by going to the official PyTorch website
    ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)).
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is available as a Python package and you can either use `pip` or `conda` to
    build it. Alternatively, you can build it from the source. The recommended approach
    for this book is to use the Anaconda Python 3 distribution. To install Anaconda,
    please refer to the Anaconda official documentation at [https://conda.io/docs/user-guide/install/index.html](https://conda.io/docs/user-guide/install/index.html).
    All the examples will be available as Jupyter Notebooks in this book's GitHub
    repository. I would strongly recommend that you use Jupyter Notebook since it
    allows you to experiment interactively. If you already have Anaconda Python installed,
    then you can proceed with the following instructions for PyTorch installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPU-based installation with Cuda 8, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For GPU-based installation with Cuda 7.5, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For non-GPU-based installation, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: At the time of writing, PyTorch does not work on Windows machines, so you can
    try a **virtual machine** (**VM**) or Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned about the history of AI, why we use deep learning,
    multiple frameworks in the deep learning ecosystem, why PyTorch is an important
    tool, why we use GPUs for deep learning, and setting up PyTorch v1.0.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the building blocks of neural networks
    and learn how to write PyTorch code to train them.
  prefs: []
  type: TYPE_NORMAL
