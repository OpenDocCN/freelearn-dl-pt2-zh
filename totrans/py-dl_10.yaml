- en: Chapter 10. Building a Production-Ready Intrusion Detection System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第十章。构建生产就绪的入侵检测系统
- en: In the previous chapter, we explained in detail what an anomaly detection is
    and how it can be implemented using auto-encoders. We proposed a semi-supervised
    approach for novelty detection. We introduced H2O and showed a couple of examples
    (MNIST digit recognition and ECG pulse signals) implemented on top of the framework
    and running in local mode. Those examples used a small dataset already cleaned
    and prepared to be used as proof-of-concept.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们详细解释了异常检测是什么以及如何使用自动编码器来实现它。我们提出了一种半监督方法来进行新颖性检测。我们介绍了 H2O，并展示了一些在该框架之上实现的例子（MNIST
    数字识别和 ECG 脉冲信号），这些例子在本地模式下运行。这些示例使用了一个已经清理和准备好用作概念验证的小型数据集。
- en: Real-world data and enterprise environments work very differently. In this chapter,
    we will leverage H2O and general common practices to build a scalable distributed
    system ready for deployment in production.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据和企业环境工作方式大不相同。在本章中，我们将利用 H2O 和一般的常见做法来构建一个可扩展的分布式系统，准备好在生产环境中部署。
- en: We will use as an example an intrusion detection system with the goal of detecting
    intrusions and attacks in a network environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以入侵检测系统为例，旨在检测网络环境中的入侵和攻击。
- en: We will raise a few practical and technical issues that you would probably face
    in building a data product for intrusion detection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提出一些实际和技术问题，这些问题在构建用于入侵检测的数据产品时可能会遇到。
- en: 'In particular, you will learn:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，你将学到：
- en: What a data product is
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据产品是什么
- en: How to better initialize the weights of a deep network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何更好地初始化深度网络的权重
- en: How to parallelize in multi-threading the Stochastic Gradient Descent algorithm
    with HOGWILD!
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 HOGWILD! 并行化多线程随机梯度下降算法
- en: How to distribute computation using Map/Reduce on top of Apache Spark using
    Sparkling Water
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何利用 Apache Spark 和 Sparkling Water 在 Map/Reduce 上分布计算
- en: A few rules of thumb for tweaking scalability and implementation parameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整可扩展性和实现参数的一些经验法则
- en: A comprehensive list of techniques for adaptive learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应学习的全面技术列表
- en: How to validate both in presence and absence of ground truth
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在有和无地面真实情况下进行验证
- en: How to pick the right trade-off between precision and reduced false alarms
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在精度和减少误报之间选择正确的权衡
- en: An example of an exhaustive evaluation framework considering both technical
    and business aspects
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑技术和业务方面的详尽评估框架的一个例子
- en: A summary of model hyper parameters and tuning techniques
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型超参数和调整技术的摘要
- en: How to export your trained model as a POJO and deploy it in an anomaly detection
    API
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将训练好的模型导出为 POJO 并部署在异常检测 API 中
- en: What is a data product?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是数据产品？
- en: The final goal in data science is to solve problems by adopting data-intensive
    solutions. The focus is not only on answering questions but also on satisfying
    business requirements.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的最终目标是通过采用数据密集型解决方案来解决问题。重点不仅在于回答问题，而且在于满足业务需求。
- en: Just building data-driven solutions is not enough. Nowadays, any app or website
    is powered by data. Building a web platform for listing items on sale does consume
    data but is not necessarily a data product.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 仅构建数据驱动的解决方案是不够的。如今，任何应用程序或网站都由数据驱动。构建一个用于列出待售物品的 Web 平台确实会使用数据，但不一定是一个数据产品。
- en: 'Mike Loukides gives an excellent definition:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Mike Loukides 给出了一个很好的定义：
- en: '*A data application acquires its value from the data itself, and creates more
    data as a result; it''s not just an application with data; it''s a data product.
    Data science enables the creation of data products.*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据应用程序从数据本身获取其价值，并因此生成更多数据；它不仅仅是一个带有数据的应用程序；它是一个数据产品。数据科学使得能够创建数据产品。*'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*From "What is Data Science" ([https://www.oreilly.com/ideas/what-is-data-science](https://www.oreilly.com/ideas/what-is-data-science))*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*来源于《什么是数据科学》([https://www.oreilly.com/ideas/what-is-data-science](https://www.oreilly.com/ideas/what-is-data-science))*'
- en: The fundamental requirement is that the system is able to derive value from
    data—not just consuming it as it is—and generate knowledge (in the form of data
    or insights) as output. A data product is the automation that let you extract
    information from raw data, build knowledge, and consume it effectively to solve
    a specific problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基本要求是系统能够从数据中提取价值——而不仅仅是消耗它——并生成知识（以数据或见解的形式）作为输出。数据产品是能够从原始数据中提取信息、建立知识并有效地消耗它以解决特定问题的自动化。
- en: The two examples in the anomaly detection chapter are the definition of what
    a data product is not. We opened a notebook, loaded a snapshot of data, started
    analyzing and experimenting with deep learning, and ultimately produced some plots
    that prove we could apply auto-encoders for detecting anomalies. Although the
    whole analysis is reproducible, in the best case, we could have built a proof-of-concept
    or a toy model. Will this be suitable for solving a real-world problem? Is this
    a Minimum Viable Product (MVP) for your business? Probably not.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在异常检测章节中的两个示例定义了数据产品的概念。我们打开了一个笔记本，加载了一份数据快照，开始分析和尝试深度学习，并最终产生了一些证明我们可以应用自编码器来检测异常的图表。尽管整个分析是可重复的，在最好的情况下，我们可能已经建立了一个概念验证或玩具模型。这对解决现实世界的问题合适吗？这对你的业务来说是一个最小可行产品（MVP）吗？可能不是。
- en: Machine learning, statistics, and data analysis techniques are not new. The
    origin of mathematical statistics dates back to the 17th century; Machine Learning
    is a subset of **Artificial Intelligence** (**AI**), which was proven by Alan
    Turing with his *Turing Test* in 1950\. You might argue that the data revolution
    started with the increase of data collection and advances in technology. I would
    say this is what enabled the data revolution to happen smoothly. The real shift
    probably happened when companies started realizing they can create new products,
    offer better services, and significantly improve their decision-making by trusting
    their data. Nevertheless, the innovation is not in manually looking for answers
    in data; it is in integrating streams of information generated from data-driven
    systems that can extract and provide insights able to drive human actions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、统计学和数据分析技术并不是新事物。数学统计学的起源可以追溯到17世纪；机器学习是**人工智能**（**AI**）的一个子集，这是由艾伦·图灵在1950年通过他的*Turing
    Test*证明的。你可能会认为数据革命始于数据收集的增加和技术的进步。我认为这正是使数据革命能够顺利进行的原因。真正的转变可能发生在公司开始意识到他们可以通过信任他们的数据来创建新产品、提供更好的服务，并显著改进他们的决策。然而，创新不在于手动地在数据中寻找答案；而是在于整合从数据驱动系统中生成的信息流，这些信息流可以提取并提供能够推动人类行动的见解。
- en: A **data product** is the result of the intersection between science and technology
    in order to generate artificial intelligence, able to scale and take unbiased
    decisions on our behalf.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据产品**是科学和技术交汇的结果，旨在生成人工智能，能够在我们的代表进行规模化和不偏颇的决策。'
- en: Because a data product grows and get better by consuming more data, and because
    it generates data itself, the generative effect could theoretically establish
    an infinite stream of information. For this reason, a data product must also be
    self-adapting and able to incrementally incorporate new knowledge as new observations
    are collected. A statistical model is just one component of the final data product.
    For instance, an intrusion detection system after the anomaly inspection would
    feed back a bunch of labeled data that can be re-used for training the model in
    the following generations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据产品通过消耗更多的数据而变得更好，而且它本身也会生成数据，所以生成效应理论上可以建立一个无限的信息流。因此，数据产品必须也是自适应的，并能在收集到新观测数据时逐步融合新知识。统计模型只是最终数据产品的一个组成部分。例如，在异常检测后的入侵检测系统会反馈一堆可用于后续模型训练的标记数据。
- en: Nevertheless, data analytics is also extremely important in every organization.
    It is quite common to find hybrid teams of Data Scientists and Analysts within
    organizations. The manual supervision, inspection, and visualization of intermediate
    results is a must requirement for building successful solutions. What we aim to
    remove is the manual intervention in the finite product. In other words, the development
    stage involves a lot of exploratory analysis and manual checkpoints but the final
    deliverable is generally an end-to-end pipeline (or a bunch of independent micro-services)
    that receives data as input and produces data as output. The whole workflow should
    preferably be automated, tested, and scalable. Ideally we would like to have real-time
    predictions integrated within the enterprise system that can react upon each detection.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据分析在每个组织中也非常重要。在组织中经常会找到数据科学家和分析师混合团队。手动监督、检查和可视化中间结果对于构建成功的解决方案是必不可少的要求。我们的目标是消除有限产品的人工干预。换句话说，开发阶段涉及大量的探索性分析和手动检查点，但最终的交付通常是端到端的管道（或一堆独立的微服务），它以数据作为输入并产生数据作为输出。整个工作流最好是自动化、经过测试且可扩展的。理想情况下，我们希望在企业系统中集成实时预测，以便对每次检测做出反应。
- en: An example could be a large screen in a factory showing a live dashboard with
    real-time measurements coming from the active machines and firing alerts whenever
    something goes wrong. This data product would not fix the machine for you but
    would be a support tool for human intervention.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，工厂中的一个大屏幕显示实时测量数据，来自活动机器，可以在出现问题时发出警报。这些数据产品不会替你修复机器，但会成为人类干预的支持工具。
- en: 'Human interaction should generally happen as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 人类互动通常应该是：
- en: Domain expertise by setting priors coming from their experience
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域专业知识通过从经验中设置先验来
- en: Developing and testing
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发与测试
- en: Final consumption of the product
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品的最终消费
- en: In our intrusion detection system, we will use the data to recommend actions
    for a team of security analysts so that they can prioritize and take better decisions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的入侵检测系统中，我们将利用数据为安全分析团队推荐行动，以便他们能够优先考虑并做出更好的决策。
- en: Training
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: Training a network means having already designed its topology. For that purpose
    we recommend the corresponding Auto-Encoder section in [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning* for
    design guidelines according to the type of input data and expected use cases.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络意味着已经设计好了网络的拓扑结构。为此，我们建议参考[第四章](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第四章. 无监督特征学习")中的相应自编码器部分，对输入数据的类型和预期用例进行设计指南。
- en: Once we have defined the topology of the neural network, we are just at the
    starting point. The model now needs to be fitted during the training phase. We
    will see a few techniques for scaling and accelerating the learning of our training
    algorithm that are very suitable for production environments with large datasets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了神经网络的拓扑结构，我们就处于起点了。模型现在需要在训练阶段进行拟合。我们将介绍一些适合于具有大型数据集的生产环境的训练算法的学习加速和扩展技术。
- en: Weights initialization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重初始化
- en: The final convergence of neural networks can be strongly influenced by the initial
    weights. Depending on which activation function we have selected, we would like
    to have a gradient with a steep slope in the first iterations so that the gradient
    descent algorithm can quickly jump into the optimum area.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的最终收敛性可以受到初始权重的强烈影响。根据我们选择的激活函数，我们希望在最初的迭代中具有陡峭的斜率，以便梯度下降算法可以快速跳入最佳区域。
- en: 'For a hidden unit *j* in the first layer (directly connected to the input layer),
    the sum of values in the first iteration for the training sample *x* of dimensionality
    *d* would be:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一层（直接连接到输入层）的隐藏单元*j*，维度为*d*的训练样本*x*在第一次迭代的值之和为：
- en: '![Weights initialization](img/00327.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00327.jpeg)'
- en: Here, *w*[*0,i*] is the initial weight of the *i*^(th) dimension.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w*[*0,i*]是第*i*维的初始权重。
- en: 'Since we choose the weights to be independent and identically distributed *(i.i.d.)*
    and also independent from the inputs, the mean of unit *j* is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择的权重是独立同分布的（*i.i.d.*），并且也独立于输入，单元*j*的均值为：
- en: '![Weights initialization](img/00328.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00328.jpeg)'
- en: 'If the input values *x*[*i*] are normalized to have *µ*[*x*]*=0* and standard
    deviation *s*[*x*]*=1*, the mean will be *E(h*[*j*]*)* and the variance will be:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入值*x*[*i*]被归一化为*µ*[*x*]*=0*和标准差*s*[*x*]*=1*，则均值为*E(h*[*j*]*)*，方差为：
- en: '![Weights initialization](img/00329.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00329.jpeg)'
- en: 'The output of the hidden unit j will be transformed through its activation
    function as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏单元*j*的输出将通过其激活函数转换为：
- en: '![Weights initialization](img/00330.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00330.jpeg)'
- en: Here *b* is the bias term that can be simply initialized to 0 or some value
    very close to 0, such as 0.01 in the case of ReLU activation functions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*b*是偏置项，可以简单地初始化为0或非常接近0的值，例如在ReLU激活函数的情况下为0.01。
- en: In the case of a sigmoid function, we have very flat curve for large values
    (both positives and negatives). In order to have a large gradient we would like
    to be in the range between *[-4, +4]*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在sigmoid函数的情况下，对于大值（正负），我们得到非常平坦的曲线。为了获得较大的梯度，我们希望处于*[-4，+4]*范围内。
- en: 'If we draw the initial weights from a uniform distribution ![Weights initialization](img/00331.jpeg),
    the variance of unit *j* becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从均匀分布![权重初始化](img/00331.jpeg)中抽取初始权重，则单元*j*的方差变为：
- en: '![Weights initialization](img/00332.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00332.jpeg)'
- en: The probability that *h*[*j*] will fall outside [-4, +4] is very small. We are
    effectively reducing the probability of early saturation regardless of the size
    of *d*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[*j*]落在[-4，+4]之外的概率非常小。我们有效地减少了过早饱和的概率，无论*d*的大小如何。'
- en: This technique of assigning the initial weights as function of the number of
    nodes in the input layer *d* is called uniform adaptive initialization. H2O by
    default applies the uniform adaptive option which is generally a better choice
    than a fixed uniform or normal distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将初始权重分配为输入层节点数*d*的函数的技术称为均匀自适应初始化。H2O默认应用均匀自适应选项，通常比固定均匀或正态分布更好。
- en: If we have only one hidden layer, it is sufficient to just initialize the weights
    of the first layer. In case of deep auto-encoders we can pre-train a stack of
    single layer auto-encoders. That is, we create a bunch of shallow auto-encoders
    where the first one reconstructs the input layer, the second one reconstructs
    the latent states of the first hidden layer, and so on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有一个隐藏层，只需初始化第一层的权重即可。在深度自动编码器的情况下，我们可以预先训练一堆单层自动编码器。也就是说，我们创建一堆浅自动编码器，其中第一个重建输入层，第二个重建第一个隐藏层的潜在状态，依此类推。
- en: Let's use the label *L*[*i*] to identify the *i*^(th) layer with *L*[*0*] to
    be the input layer, the last one to be the final output, and all the others in
    the middle to be the hidden layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用标签*L*[*i*]来标识第*i*层，其中*L*[*0*]是输入层，最后一个是最终输出，其他所有层都是隐藏层。
- en: For example, a 5-layer network ![Weights initialization](img/00333.jpeg) could
    be broken down into 2 networks ![Weights initialization](img/00334.jpeg) and ![Weights
    initialization](img/00335.jpeg).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个5层网络![权重初始化](img/00333.jpeg)可以拆分为2个网络![权重初始化](img/00334.jpeg)和![权重初始化](img/00335.jpeg)。
- en: The first auto-encoder, after training, will initialize the weights of *L*[*1*]
    and will turn the input data into the latent states of *L*[*1*]. These states
    are used to train the second auto-encoder, which will be used to initialize the
    weights of *L*[*2*].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个自动编码器，在训练后，将初始化*L*[*1*]的权重，并将输入数据转换为*L*[*1*]的潜在状态。这些状态用于训练第二个自动编码器，后者将用于初始化*L*[*2*]的权重。
- en: The decoding layers share the same initial weights and bias of the encoding
    counterpart. Thus, we only need to pre-train the left-half of the network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解码层共享编码对应的初始权重和偏置。因此，我们只需要预训练网络的左半部分。
- en: Likely, a 7-layer network ![Weights initialization](img/00336.jpeg) can be broken
    down into ![Weights initialization](img/00337.jpeg), ![Weights initialization](img/00338.jpeg)
    and ![Weights initialization](img/00339.jpeg).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，一个7层网络![权重初始化](img/00336.jpeg)可以拆分为![权重初始化](img/00337.jpeg)、![权重初始化](img/00338.jpeg)和![权重初始化](img/00339.jpeg)。
- en: 'In general, if the deep auto-encoder has N layers we can treat it as a stack
    of ![Weights initialization](img/00340.jpeg)stacked single-layer auto-encoders:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果深度自动编码器有N层，我们可以将其视为一堆![权重初始化](img/00340.jpeg)堆叠的单层自动编码器：
- en: '![Weights initialization](img/00341.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![权重初始化](img/00341.jpeg)'
- en: After pre-training, we can train the entire network with the specified weights
    all together.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练后，我们可以一起训练整个网络，使用指定的权重。
- en: Parallel SGD using HOGWILD!
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HOGWILD!的并行SGD
- en: As we have seen in previous chapters, deep neural networks are trained via backpropagation
    of a given error generated from a loss function. Backpropagation provides the
    gradient of the model parameters (weights *W* and biases *B* of each layer). Once
    we have calculated the gradient, we could use it to follow the direction that
    minimizes the error. One of the most popular technique is **Stochastic Gradient
    Descent** (**SGD**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中所看到的，深度神经网络是通过反向传播给定损失函数产生的错误来进行训练的。反向传播提供了模型参数（每一层的权重 *W* 和偏差 *B*）的梯度。一旦我们计算出梯度，我们可以使用它来沿着最小化错误的方向移动。其中最流行的技术之一是**随机梯度下降**（**SGD**）。
- en: SGD can be summarized as following.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SGD可以总结如下。
- en: Initialize *W*, *B*.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *W*, *B*。
- en: 'While convergence is not reached:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在收敛前：
- en: Get the training example *i*
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取训练样本*i*
- en: '![Parallel SGD using HOGWILD!](img/00342.jpeg) for any ![Parallel SGD using
    HOGWILD!](img/00343.jpeg) in *W*'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![使用HOGWILD!的并行SGD](img/00342.jpeg) 对于任何![使用HOGWILD!的并行SGD](img/00343.jpeg)的*W*'
- en: '![Parallel SGD using HOGWILD!](img/00344.jpeg) for any ![Parallel SGD using
    HOGWILD!](img/00345.jpeg) in *B*'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![使用HOGWILD!的并行SGD](img/00344.jpeg) 对于任何![使用HOGWILD!的并行SGD](img/00345.jpeg)的*B*'
- en: Here *W* is the weights matrix, *B* is bias vector, ![Parallel SGD using HOGWILD!](img/00346.jpeg)
    the is gradient computed via backpropagation and *a* is the learning rate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*W*是权重矩阵，*B*是偏置向量，![使用HOGWILD!的并行SGD](img/00346.jpeg)是通过反向传播计算的梯度，*a*是学习率。
- en: While SGD is the de-facto most popular training algorithm for many machine learning
    models, it is not efficiently parallelizable. Many parallelized versions have
    been proposed in the literature, but most of them are bottlenecked by the synchronization
    and memory locks amongst processors, without taking advantage of the sparsity
    of the parameters updates, a common property for neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SGD是许多机器学习模型最流行的训练算法，但它并不是高效的可并行化的。文献中提出了许多并行化版本，但大多数都受到处理器之间同步和内存锁限制的困扰，没有利用参数更新的稀疏性，这是神经网络的常见特性。
- en: In most neural networks problems, the update step is generally sparse. For every
    training input, only a few weights associated with the neurons that are wrongly
    reacting are updated. Generally, a neural network is built so that each neuron
    only activates when a specific characteristic of input is present. As a matter
    of fact, a neuron that activates for every input would not be very useful.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数神经网络问题中，更新步骤通常是稀疏的。对于每个训练输入，只有少数与错误反应的神经元相关的权重被更新。一般来说，神经网络被构建成每个神经元只有在输入中存在特定特征时才激活。事实上，每次输入都激活的神经元并不是很有用。
- en: '**HOGWILD!** is an alternative algorithm that allows each thread to overwrite
    each other''s work and provide better performances. Using HOGWILD!, multiple cores
    can asynchronously handle separate subsets of the training data and make independent
    contributions to the updates of the gradient ![Parallel SGD using HOGWILD!](img/00347.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**HOGWILD!** 是一种替代算法，允许每个线程覆盖其他线程的工作，并提供更好的性能。使用 HOGWILD!，多个核心可以异步处理训练数据的不同子集，并独立地对梯度更新做出贡献![使用HOGWILD!的并行SGD](img/00347.jpeg)'
- en: 'If we divide the data dimensionality *d* into small subsets *E* of *{1,…,d}*
    and ![Parallel SGD using HOGWILD!](img/00348.jpeg) the portion of the vector *x*
    on the coordinates indexed by *e*, we can separate the whole cost function *L*
    as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把数据的维度 *d* 分成小的子集 *E*，然后![使用HOGWILD!的并行SGD](img/00348.jpeg)是由*E*的坐标索引的向量*x*的部分，我们可以把整个成本函数
    *L* 分解为：
- en: '![Parallel SGD using HOGWILD!](img/00349.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用HOGWILD!的并行SGD](img/00349.jpeg)'
- en: The key property that we exploit is that cost functions are sparse in the sense
    that ![Parallel SGD using HOGWILD!](img/00350.jpeg) and *d* can be large but *L*[*e*]
    is calculated only on a much smaller components of the input vector (![Parallel
    SGD using HOGWILD!](img/00351.jpeg)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用的关键属性是成本函数在某种意义上是稀疏的，即![使用HOGWILD!的并行SGD](img/00350.jpeg)，而*d*可能很大，但是*L*[*e*]只在输入向量（![使用HOGWILD!的并行SGD](img/00351.jpeg)）的较小部分上计算。
- en: 'If we have *p* processors, all sharing the same memory and all able to access
    the vector *x*, the component-wise update would be atomic due to the additive
    property:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有*p*个处理器，共享相同的内存，且都能访问向量*x*，则组件更新是原子的，因为具有加法性质：
- en: '![Parallel SGD using HOGWILD!](img/00352.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![使用HOGWILD!的并行SGD](img/00352.jpeg)'
- en: 'That means we can update the state of the single unit without a separate locking
    structure. A different story is the case of updating multiple components at once
    where each processor would repeat asynchronously the following loop:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以更新单个单元的状态而无需单独的锁定结构。更新多个组件的情况则不同，在这种情况下，每个处理器都会异步重复以下循环：
- en: Sample *e* uniformly at random from *E*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*E*中均匀随机采样*e*。
- en: Read current state ![Parallel SGD using HOGWILD!](img/00348.jpeg) and evaluate
    ![Parallel SGD using HOGWILD!](img/00353.jpeg).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 读取当前状态 ![使用HOGWILD的并行SGD！](img/00348.jpeg) 并评估 ![使用HOGWILD的并行SGD！](img/00353.jpeg)。
- en: For ![Parallel SGD using HOGWILD!](img/00354.jpeg) do ![Parallel SGD using HOGWILD!](img/00355.jpeg).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ![使用HOGWILD的并行SGD！](img/00354.jpeg) 执行 ![使用HOGWILD的并行SGD！](img/00355.jpeg)。
- en: Here ![Parallel SGD using HOGWILD!](img/00356.jpeg) is the gradient ![Parallel
    SGD using HOGWILD!](img/00346.jpeg) multiplied by ![Parallel SGD using HOGWILD!](img/00357.jpeg).
    *b*[*v*] is a bitmask vector where 1 corresponds to a selected index of *e*, and
    *?* is the step size, which is diminished by a factor ß at the end of each epoch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![使用HOGWILD的并行SGD！](img/00356.jpeg) 是梯度 ![使用HOGWILD的并行SGD！](img/00346.jpeg)
    乘以 ![使用HOGWILD的并行SGD！](img/00357.jpeg)。*b*[*v*]是一个位掩码向量，其中1对应于*e*的选定索引，*?*是步长，每个时期末会缩小一个因子ß。
- en: Because computing the gradient is not instantaneous and any processor may have
    modified *x* at any time, we might update *x* with a gradient computed with an
    old value read many clock cycles earlier. The novelty of HOGWILD! is in providing
    conditions under which this asynchronous, incremental gradient algorithm converges.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因为梯度计算不是瞬时的，任何处理器可能随时修改*x*，我们可能会使用在许多时钟周期之前读取的旧值计算梯度来更新*x*。HOGWILD的新颖之处在于提供了一种异步的、增量的梯度算法在其中收敛的条件。
- en: In particular, it has been proven that the lag between when the gradient is
    computed and when it is used is always less than or equal to a maximum value,
    t. The upper bound value of t depends on the number of processors and it converges
    to 0 as we approach the standard serial version of the algorithm. If the number
    of processors is less than *d*^(*1/4*), then we get nearly the same number of
    gradient steps of the serial version, which means we get a linear speedup in terms
    of the number of processors. Moreover, the sparser the input data, the less the
    probability of memory contention between processors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，已经证明梯度计算和使用之间的延迟始终小于或等于最大值$t$。$t$的上界值取决于处理器的数量，并且当我们接近算法的标准串行版本时，$t$收敛于0。如果处理器的数量小于*d*^(*1/4*)，那么我们获得的梯度步数几乎与串行版本相同，这意味着我们在处理器数量方面实现了线性加速。此外，输入数据越稀疏，处理器之间的内存争用可能性就越小。
- en: In the worst case, the algorithm can always provide some speed improvement even
    when the gradients are computationally intensive.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，即使梯度计算具有计算密集性，该算法也始终可以提供一些速度改进。
- en: 'You can find more details in the original paper: [https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf](https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在原始论文中找到更多细节：[https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf](https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf)。
- en: In conclusion, there are many techniques for optimizing learning in terms of
    speed, stability, and the probability of getting stuck into a local optimum. Non-adaptive
    learning rates associated with Momentum would probably give the best results,
    but it will require more parameters to be tuned. Adadelta is a trade-off between
    complexity and performance since it only requires two parameters (ρ and ϵ) and
    is able to adapt to different scenarios.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，有许多优化学习速度、稳定性和陷入局部最优的概率的技术。非自适应学习率与动量结合可能会产生最好的结果，但这将需要调整更多的参数。Adadelta是复杂性和性能之间的权衡，因为它只需要两个参数（ρ和ϵ），并且能够适应不同的场景。
- en: Adaptive learning
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应学习
- en: In the previous paragraphs, we have seen the importance of the weights initialization
    and an overview of the SGD algorithm, which in its base version uses a fixed value
    of the learning rate a. Both of them are important requirements in order to guarantee
    a fast and accurate convergence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的段落中，我们已经看到了权重初始化的重要性和SGD算法的概述，其基本版本使用固定值的学习率a。它们都是保证快速和准确收敛的重要条件。
- en: 'A few advanced techniques can be adopted to dynamically optimize the learning
    algorithm. In particular, we can divide into two types of techniques: the ones
    that attempt to speed up the learning wherever is convenient and the ones that
    slows down when near local minima.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用一些先进的技术来动态优化学习算法。特别是，我们可以划分为两种类型的技术：一种旨在在任何方便的地方加快学习，另一种在接近局部最小值时减慢学习。
- en: 'If θ[t] represents the quantity we are updating at iteration *t* (the weights
    and biases parameters), the general SGD algorithm will update as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果θ[t]表示我们在迭代*t*（权重和偏差参数）更新的数量，则一般SGD算法的更新如下：
- en: '![Adaptive learning](img/00358.jpeg)![Adaptive learning](img/00359.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![自适应学习](img/00358.jpeg)![自适应学习](img/00359.jpeg)'
- en: Rate annealing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率退火
- en: We need to choose α. Low values of the learning rate will require a lot of iterations
    in order to converge with the risk of getting stuck into a local minimum. Having
    a high learning rate will cause instability. If the algorithm contains too much
    kinetic energy, the step to minimize θ would cause it to bounce around chaotically.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择α。学习率较低将需要很多迭代才能收敛，并且有搁置在局部最小值的风险。具有较高学习率会导致不稳定性。如果算法包含太多动能，那么最小化θ的步骤会导致其在周围跳来跳去。
- en: 'Rate Annealing slowly reduces the α[*t*] as we consume data points during training.
    One technique is to update ![Rate annealing](img/00360.jpeg) every *k* samples:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率退火在训练期间消耗数据点时，会将α[*t*]缓慢降低。一种技术是在每*k*个样本更新一次![学习率退火](img/00360.jpeg)：
- en: '![Rate annealing](img/00361.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![学习率退火](img/00361.jpeg)'
- en: Thus, the decay rate would correspond to the inverse of the number of training
    samples required to divide the learning rate in half.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，衰减率将对应于需要将学习率减半所需的训练样本数的倒数。
- en: Momentum
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: 'Momentum takes into account the results of previous iterations to influence
    the learning of a current iteration. A new velocity vector *v* is introduced and
    defined as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 动量考虑了前几次迭代的结果来影响当前迭代的学习。引入并定义一个新的速度向量*v*，如下所示：
- en: '![Momentum](img/00362.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/00362.jpeg)'
- en: Here µ is the momentum decay coefficient. Instead of using the gradient to change
    position, we use the gradient to change velocity. The momentum term is in charge
    of speeding up the learning over dimensions where the gradient continues pointing
    at the same direction and slowing down those dimensions where the sign of the
    gradient is alternating, that is, those areas corresponding to a region with a
    local optimum.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里µ是动量衰减系数。我们不再使用梯度来改变位置，而是使用梯度来改变速度。动量项负责加快学习，在梯度继续指向同一方向的维度上，减慢那些梯度符号交替的维度，也就是那些对应于局部最优解区域的区域。
- en: 'This additional momentum term will help reach convergence faster. Too much
    momentum could lead to divergence though. Suppose we are running SGD with momentum
    for enough epochs, the final velocity would eventually be:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个额外的动量项将有助于更快地收敛。不过过多的动量可能会导致发散。假设我们运行带动量的SGD足够的epochs，最终速度最终将是：
- en: '![Momentum](img/00363.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/00363.jpeg)'
- en: 'It is a geometric series if *µ* is less than 1; then the limit will converge
    to something proportional to:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*µ*小于1，则这是一个几何级数；那么极限将收敛到与以下成比例的某物：
- en: '![Momentum](img/00364.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![动量](img/00364.jpeg)'
- en: In this formula, when *µ* is close to 1, the system would be moving too fast.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，当*µ*接近1时，系统会移动得太快。
- en: Moreover, at the beginning of the learning, there may already be large gradients
    (the effect of weights initialization). Thus, we would like to start with a small
    momentum (for example, 0.5); once the large gradients have disappeared, we can
    increase the momentum until it reaches a final stable value (for example, 0.9),
    and keep it constant.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在学习初期，可能已经存在大梯度（权重初始化的影响）。因此，我们希望以一个小的动量开始（例如0.5）; 一旦大梯度消失，我们可以增加动量，直到它达到最终稳定值（例如0.9），并保持恒定。
- en: Nesterov's acceleration
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Nesterov的加速
- en: The standard momentum computes the gradient at the current location and amplifies
    the steps in the direction of the accumulated gradient. It is like pushing a ball
    down a hill and blindly following the hill slope. Since we can approximate where
    the ball will land, we would like to take this information into account when computing
    the gradient.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 标准动量计算当前位置的梯度，并放大累积梯度方向的步骤。就像把球推下山并盲目地跟随山坡斜率一样。由于我们可以近似地预测球会落在哪里，所以我们希望在计算梯度时考虑这个信息。
- en: 'Let''s remember the value of our parameters *θ* at time *t* is given by:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住时间*t*处参数*θ*的值是：
- en: '![Nesterov''s acceleration](img/00365.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Nesterov的加速法](img/00365.jpeg)'
- en: 'The gradient of ?t, if we omit the second derivative, can be approximated as:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们省略二阶导数，?t的梯度可以近似为：
- en: '![Nesterov''s acceleration](img/00366.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Nesterov的加速法](img/00366.jpeg)'
- en: 'The update step will be calculated using the gradient at time *t* instead of
    *t – 1*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 更新步骤将使用时间*t*处的梯度而不是*t – 1*处的梯度计算：
- en: '![Nesterov''s acceleration](img/00367.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Nesterov的加速法](img/00367.jpeg)'
- en: The Nesterov variation would first make a big step in the direction of the previously
    accumulated gradient and then correct it with the gradient calculated after the
    jump. This correction prevents it from going too fast and improves stability.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov变化首先会朝着先前累积梯度的方向迈出一大步，然后再根据跳跃后计算的梯度进行校正。这种校正防止了它过快地行进并提高了稳定性。
- en: In the *ball down the hill* analogy, the Nesterov correction adapts the velocity
    according to the hill slope and speeds up only where possible.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在*球滚下山*的类比中，Nesterov校正根据山坡调整速度，并且仅在可能的情况下加速。
- en: Newton's method
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 牛顿方法
- en: 'Whereas single-order methods only use gradient and function evaluations to
    minimize *L*, second-order methods can use the curvature as well. In Newton''s
    method, we compute the Hessian matrix *HL(θ)* which is the square matrix of second-order
    partial derivatives of the loss function *L(θ)* . The inverse Hessian will define
    the value of a and final step equation is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 而单阶方法只使用梯度和函数评估来最小化*L*，二阶方法也可以使用曲率。在牛顿方法中，我们计算损失函数*L(θ)*的二阶偏导数的Hessian矩阵*HL(θ)*
    。逆Hessian将定义a的值，最终步骤方程为：
- en: '![Newton''s method](img/00368.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![牛顿方法](img/00368.jpeg)'
- en: Here the absolute value of the diagonal is used to ensure the negative gradient
    direction to minimize *L*. The parameter ? is used for smoothing regions with
    a small curvature.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用对角线的绝对值来确保负梯度方向最小化*L*。参数?用于平滑具有小曲率的区域。
- en: By using second-order derivatives, we can perform updates in more efficient
    directions. In particular, we will have more aggressive updates over shallow (flat)
    curvatures and smaller steps over steep curvatures.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用二阶导数，我们可以在更有效的方向上执行更新。特别是，在平缓（平坦）曲率上我们会有更激进的更新，而在陡峭的曲率上会有更小的步长。
- en: The best property of this method is that it has no hyper-parameters, except
    the smoothing parameter which is fixed to a small value; thus it is one dimension
    less to tune. The major issue is in the computational and memory costs. The size
    of *H* is the square of the size of the neural network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的最佳属性是它没有超参数，除了平滑参数被固定为一个小值；因此它是一个维度较少的调整。主要问题在于计算和内存成本。*H*的大小是神经网络大小的平方。
- en: A number of quasi-Newton methods have been developed to approximate the inverse
    Hessian. For instance, **L-BFGS (Limited Memory Broyden-Fletcher-Goldfarb-Shanno**)
    stores only a few vectors that implicitly represent the approximation and the
    history of the last updates of all of the previous vectors. Since the Hessian
    is constructed approximately from the previous gradient evaluation, it is important
    that the objective function is not changed during the optimization process. Moreover,
    the Naïve implementation requires the full dataset to be computed in a single
    step and is not very suitable for mini-batch training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了许多拟牛顿方法来近似逆Hessian。例如，**L-BFGS（Limited Memory Broyden-Fletcher-Goldfarb-Shanno）**只存储几个向量，这些向量隐含地表示近似和所有先前向量的最后更新的历史。由于Hessian是从以前的梯度评估中近似构建的，因此在优化过程中不改变目标函数非常重要。此外，朴素实现需要在单个步骤中计算完整数据集，并且不太适合小批量训练。
- en: Adagrad
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adagrad
- en: '**Adagrad** is another optimization of SGD that adapts the learning rate of
    each parameter based on the L2 norm of all previous computed gradients on a per-dimension
    basis.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adagrad**是SGD的另一种优化，根据先前所有计算梯度的L2范数每个维度进行学习率的调整。'
- en: 'The value of alpha will depend on the time *t* and the *i*^(th) parameter θ[t,i]:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: α的值取决于时间*t*和第*i*个参数θ[*t,i*]：
- en: '![Adagrad](img/00369.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Adagrad](img/00369.jpeg)'
- en: 'Here *G*[*t*] is a diagonal matrix of size *d* x *d* and the element *i, i*
    is the sum of squares of the gradients of *θ*[*k,i*] up to the iteration *t –1*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*G*[*t*]是一个*d* x *d*大小的对角矩阵，元素*i, i*是*θ*[*k,i*]的梯度平方和直到迭代*t – 1*：
- en: '![Adagrad](img/00370.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Adagrad](img/00370.jpeg)'
- en: Each dimension will have a learning rate inversely proportioned to the gradient.
    That is, larger gradients will have smaller learning rates and vice versa.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个维度的学习率与梯度成反比。也就是说，较大的梯度将具有较小的学习率，反之亦然。
- en: The parameter ϵ is a smoothing term helpful for avoiding divisions by zero.
    It generally ranges between 1e-4 and 1e-10.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数ϵ是一个平滑项，有助于避免除以零。它通常在1e-4和1e-10之间波动。
- en: 'The vectorized update step is given by the element-wise matrix-vector multiplication
    ![Adagrad](img/00371.jpeg):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化更新步骤由按元素矩阵-向量乘法给出：
- en: '![Adagrad](img/00372.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Adagrad](img/00372.jpeg)'
- en: The global learning rate *a* at the nominator can be set to a default value
    (for example, 0.01) since the algorithm will automatically adapt it after a few
    iterations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 全局学习率*a*在分子上可以设置为默认值（例如0.01），因为算法会在几次迭代后自动适应它。
- en: We have now obtained the same decaying effect of rate annealing but with the
    nice property that progress along each dimension evens out over time, just like
    second-order optimization methods.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了速率退火的相同衰减效果，但具有良好的性质，即每个维度随着时间的推移均匀化，就像二阶优化方法一样。
- en: Adadelta
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adadelta
- en: One problem with Adagrad is that is very sensitive to the initial state. If
    the initial gradients are large, and we want them to be large as described in
    the weights initialization, the corresponding learning rates will be very small
    from the beginning of the training. Hence, we have to counter-balance this effect
    by setting high values of *a*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad的一个问题是非常敏感于初始状态。如果初始梯度很大，并且我们希望它们像权重初始化中描述的那样很大，那么相应的学习率将从训练开始就非常小。因此，我们必须通过设置*a*的高值来抵消这种效应。
- en: Another problem with Adagrad is that the denominator keeps accumulating gradients
    and growing at each iteration. This makes the learning rate eventually become
    infinitesimally small such that the algorithm cannot longer learn anything new
    from the remaining training data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad的另一个问题是分母一直在积累梯度，并在每次迭代中增长。这使得学习率最终变得无限小，以至于算法不能再从剩余的训练数据中学到任何新东西。
- en: 'Adadelta aims to solve the latter problem by fixing the number of accumulated
    past gradients to some value *w* instead of *t – 1*. Instead of storing the *w*
    previous values, it recursively performs an incremental decaying with the running
    average at time *t*. We can replace the diagonal matrix *G*[*t*] with the decaying
    average of past gradients:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta旨在通过将累积的过去梯度数量固定为某个*W*值，而不是*t-1*来解决后一个问题。它不是存储*w*个先前的值，而是在时间*t*上以递减的方式执行正在运行的平均值。我们可以用过去梯度的递减平均值替换对角矩阵*G*[*t*]：
- en: '![Adadelta](img/00373.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00373.jpeg)'
- en: Here *ρ* is the decay constant typically ranging between 0.9 and 0.999.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*ρ*是衰减常数，通常在0.9和0.999之间波动。
- en: 'What we really need is the square root of ![Adadelta](img/00374.jpeg) which
    approximates the **root mean square** (**RMS**) of ![Adadelta](img/00375.jpeg)
    at time *t*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正需要的是![Adadelta](img/00374.jpeg)的平方根，它近似了时间*t*下![Adadelta](img/00375.jpeg)的**均方根**(**RMS**)：
- en: '![Adadelta](img/00376.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00376.jpeg)'
- en: 'The update step would be:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 更新步骤将是：
- en: '![Adadelta](img/00377.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00377.jpeg)'
- en: 'We have defined Δ, the update step, to add to the parameters vector at each
    iteration. In order to make those equations correct, we shall ensure that the
    units are matching. If we imagine the parameters to have some hypothetical unit,
    Δ should be of the same unit. All of the first-order methods considered so far
    relate the units of Δ to the gradient of the parameters and assume the cost function
    *L* to be unitless:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了Δ，即每次迭代时要添加到参数向量中的更新步骤。为了使这些方程正确，我们必须确保单位匹配。如果我们想象参数有一些假设的单位，Δ应具有相同的单位。到目前为止考虑的所有一阶方法都将Δ的单位与参数的梯度相关联，并假设成本函数*L*是无量纲的：
- en: '![Adadelta](img/00378.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00378.jpeg)'
- en: 'In contrast, second-order methods such as Newton''s method use the Hessian
    information, or an approximation of it, to get the correct units for the update
    step ?:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，牛顿法等二阶方法使用Hessian信息，或其近似值，来获取正确的更新步骤单位？：
- en: '![Adadelta](img/00379.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00379.jpeg)'
- en: For the ![Adadelta](img/00380.jpeg) equation, we need to replace the term *a*
    with a quantity proportional to the RMS of ?(*t*).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于![Adadelta](img/00380.jpeg)方程，我们需要用某个与*t*的RMS成比例的量替换项*a*。
- en: 'Since we don''t know ?(*t*) yet, we can only compute the RMS over the same
    window of size *w* of ?(*t* – 1):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们目前不知道?(*t*)，所以我们只能计算相同大小的窗口*w*上*t* – 1的均方根值：
- en: '![Adadelta](img/00381.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00381.jpeg)'
- en: Where the same constant ? is used and has the purpose of both starting the first
    iteration when ?(0) = 0 and ensuring progress even if previous updates are small
    due to the saturating effect of the accumulating gradients at the denominator.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中使用相同的常数 ?，其目的是在 ?(0) = 0 时启动第一次迭代，并确保即使由于累积梯度在分母上饱和效应导致之前的更新很小，也能保持进展。
- en: 'If the curvature is smooth enough, we can approximate ![Adadelta](img/00382.jpeg),
    which changes the equation of Adadelta to:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果曲率足够平滑，我们可以近似 ![Adadelta](img/00382.jpeg)，这将改变Adadelta的方程为：
- en: '![Adadelta](img/00383.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![Adadelta](img/00383.jpeg)'
- en: 'The final Adadelta equation covers many of the properties discussed in previous
    methods:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的Adadelta方程覆盖了讨论的许多方法的特性：
- en: It is an approximation of the diagonal Hessian but uses only the RMS measures
    of ?L and ? and only one gradient computation per iteration.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是对对角Hessian的近似，但只使用 ?L 和 ? 的RMS度量，并且每次迭代只进行一次梯度计算。
- en: It always follows the negative gradient as in plain SGD.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它始终遵循负梯度，就像普通的SGD一样。
- en: The numerator lags behind by 1 the denominator. This makes the learning more
    robust for sudden large gradients, which would increase the denominator and reduce
    the learning rate before the numerator can react.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分子滞后于分母1次。这使得学习对突然出现的大梯度更加稳健，在分子能够反应之前，它会增加分母并降低学习率。
- en: The numerator acts as an accelerator term, just like Momentum.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分子起到了加速项的作用，就像动量法一样。
- en: The denominator acts like the per-dimension decay seen in Adagrad, but the fixed
    window ensures that progress is always made in every dimension at any step.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分母的作用类似于Adagrad中的每个维度衰减，但是通过固定的窗口保证了在任何步骤中，每个维度总是取得进展。
- en: Distributed learning via Map/Reduce
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Map/Reduce进行分布式学习
- en: Parallelizing the training in multiple concurrent threads is a great improvement
    but it is constrained by the quantity of cores and memory available in the single
    machine. In other words, we can only scale vertically by buying more resourceful
    and expensive machines.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练并行化在多个并发线程中是一个很大的改进，但它受到单台机器中可用核心和内存的数量的约束。换句话说，我们只能通过购买更多资源丰富和更昂贵的机器来实现垂直扩展。
- en: Combining the parallel and distributed computation enables the desired horizontal
    scalability, which is theoretically unbounded as long as we have the capability
    of adding additional nodes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 结合并行和分布式计算可以实现所需的水平可扩展性，只要我们有增加额外节点的能力，理论上是无限的。
- en: Two of the reasons why we chose H2O as the framework for anomaly detection are
    that it provides an easy-to-use built-in implementation of auto-encoders, and
    it provides an abstraction layer between the functionality (what we want to achieve)
    and the implementation (how we do it). This abstraction layer provides transparent
    and scalable implementations that allows to obtain the distribution of computation
    and data processing in a map/reduce fashion.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择H2O作为异常检测框架的两个原因是它提供了一个易于使用的内置自动编码器实现，以及它在功能（我们想要实现的内容）和实现（我们如何实现它）之间提供了一个抽象层。这个抽象层提供了透明和可扩展的实现，允许以map/reduce的方式进行计算和数据处理的分布。
- en: 'If our data is partitioned uniformly in smaller shards in each node, we can
    describe the high-level distributed algorithm as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据在每个节点上均匀分区在较小的分片中，我们可以将高级分布式算法描述如下：
- en: '**Initialize**: An initial model is provided with weights and biases.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：提供具有权重和偏置的初始模型。'
- en: '**Shuffling**: Data can be either entirely available in each node or bootstrapped.
    We will cover this data replication problem at the end of the paragraph.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**洗牌**：数据可以完全在每个节点上可用，也可以进行引导采样。我们将在段落末尾解决这个数据复制问题。'
- en: '**Map**: Each node will train a model based on the local data via asynchronous
    threads using HOGWILD!.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**映射**：每个节点将通过使用HOGWILD!中的异步线程基于本地数据进行模型训练。'
- en: '**Reduce**: The weights and biases of each trained model are averaged into
    the final one. This is a monoidal and commutative operation; averaging is associative
    and commutative.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**减少**：每个训练模型的权重和偏置被平均到最终模型中。这是一个蒙德运算和可交换操作；平均是可结合和可交换的。'
- en: '**Validate** (optional): The current averaged model can be scored against a
    validation set for monitoring, model selection, and/or early stopping criteria.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证**（可选）：当前的平均模型可以针对验证集进行评分，以进行监控、模型选择和/或提前停止准则。'
- en: '**Iterate**: Repeat the whole workflow several times until a convergence criterion
    is met.![Distributed learning via Map/Reduce](img/00384.jpeg)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代**：在满足收敛标准之前多次重复整个工作流程。![通过Map/Reduce进行分布式学习](img/00384.jpeg)'
- en: H2O Deep Learning Architecture
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: H2O深度学习架构
- en: The complexity time will be o(n/p + log(p)) per iteration, where n is number
    of data points in each node and p the number of processors (the nodes). The linear
    term is the map computation and the logarithmic term the reduce.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂度时间将会是每次迭代 o(n/p + log(p))，其中 n 是每个节点中数据点的数量，p 是处理器的数量（节点）。线性项是映射计算，对数项是减少计算。
- en: In the preceding formula, we are not considering the memory occupation and the
    expensiveness of the data shuffling. We can ignore the complexity of the model
    averaging in the reduce step since we assume the model parameters to be small
    enough compared to the data size. In particular, the size of a model is the number
    of parameters that corresponds to the number of neurons of the network plus the
    number of hidden layers (the bias terms). Assuming you have one million neurons,
    the total size of the model would be less than 8 MB.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述公式中，我们没有考虑内存占用和数据洗牌的昂贵性。我们可以忽略减少步骤中模型平均的复杂性，因为我们假设模型参数相对于数据大小足够小。特别是，模型的大小是网络的神经元数量加上隐藏层的数量（偏置项）对应的参数数量。假设你有一百万个神经元，模型的总大小将小于8MB。
- en: 'The final scalability will depend on:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的可扩展性将取决于：
- en: Computation parallelism
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算并行性
- en: Memory buffering
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存缓冲
- en: Network traffic and I/O
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络流量和I/O
- en: Our goal is to find the right trade-off between model accuracy and training
    speed.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在模型精度和训练速度之间找到合适的权衡。
- en: We will use the term iteration to represent the single Map/Reduce step trained
    only on the specified number of `train_samples_per_iteration`. The parameter `epochs`
    will define the necessary number of passes over the data to complete the training.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用术语迭代来表示仅在指定数量的`train_samples_per_iteration`上训练的单个Map/Reduce步骤。参数`epochs`将定义完成训练所需的数据通行证数量。
- en: The `train_samples_per_iteration` parameter could correspond to the whole dataset,
    be smaller (stochastic sampling without replacement), or even be larger (stochastic
    sampling with replacement).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_samples_per_iteration`参数可以对应整个数据集，也可以更小（无替换的随机采样），甚至更大（有替换的随机采样）。'
- en: The value of `train_samples_per_iteration` will affect both the memory occupation
    and the time between models averaging, that is, the training speed.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_samples_per_iteration`的值将影响内存占用和模型平均时间，也就是训练速度。'
- en: Another important parameter is the Boolean flag `replicate_training_data`. If
    it is enabled, a copy of the whole data will be made available in each node. This
    option will allow each model to be trained faster.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的参数是布尔标志`replicate_training_data`。如果启用，整个数据的副本将在每个节点上可用。这个选项将允许每个模型训练得更快。
- en: Another linked parameter is `shuffle_trainingd_data`, which determines whether
    the data can or cannot be shuffled among nodes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关联参数是`shuffle_trainingd_data`，它决定数据是否可以在节点之间进行洗牌。
- en: 'If N is the number of available nodes and n is the size of the training dataset,
    we can identify a few operating modes characterized by the special values of `train_samples_per_iteration`
    and by the activation of `replicate_training_data`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果N是可用节点的数量，n是训练数据集的大小，我们可以通过`train_samples_per_iteration`的特殊值和`replicate_training_data`的激活来识别一些特定的操作模式：
- en: '| `train_samples_per_iteration` | `replicate_training_data` | Description |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `train_samples_per_iteration` | `replicate_training_data` | 描述 |'
- en: '| 0 | False | Only one epoch, averaging over N models built with local data.
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 0 | False | 只进行一个epoch，在本地数据平均构建N个模型。 |'
- en: '| -1 | True | Each node processes the whole dataset per iteration. This results
    in training N epochs per iteration in parallel in N nodes. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| -1 | True | 每个节点每次迭代处理整个数据集。这导致N个节点中的每个并行训练N个epoch。 |'
- en: '| -1 | False | All nodes process only the locally stored data. One epoch corresponds
    to one iteration. You can have many epochs. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| -1 | False | 所有节点只处理本地存储的数据。一个epoch对应一个迭代。你可以有很多epochs。 |'
- en: '| -2 | True | Auto-tuning of the number of sample per iteration based on both
    computation time and network overhead. Full dataset is replicated, with sampling
    without replacement. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| -2 | True | 根据计算时间和网络开销的自动调整迭代次数。完整数据集被复制，进行无替换采样。 |'
- en: '| -2 | False | Auto-tuning of the number of samples per iteration based on
    both computation time and network overhead. Only local data is available; it might
    require sampling with replacement. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| -2 | False | 基于计算时间和网络开销自动调整每次迭代的样本数。只有本地数据可用；可能需要有放回地进行采样。 |'
- en: '| > 0 | true | Fixed number of samples per iteration sampled from the full
    dataset. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| > 0 | true | 从完整数据集中抽样的每次迭代的固定样本数量。 |'
- en: '| > 0 | false | Fixed number of samples per iteration sampled from only the
    local available data. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| > 0 | false | 从只有本地可用数据中抽样的每次迭代的固定样本数量。 |'
- en: If *n = 1M* and *N = 4*, each node on an average will store 25K locally. If
    we set *samples_per_iteration=200K*, the single Map/Reduce iteration will process
    200 K records. That is, each node will process 50K rows. In order to complete
    one epoch, we will need 5 Map/Reduce iterations corresponding to 20 local training
    steps.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*n=1M*且*N=4*，每个节点平均将存储25K个本地数据。如果我们设置*samples_per_iteration=200K*，单个Map/Reduce迭代将处理20万条记录。也就是说，每个节点将处理5万行。为了完成一个epoch，我们需要5个Map/Reduce迭代对应20个本地训练步骤。
- en: In the preceding example, each node will have those 50K samples from the local
    available data with or without sampling depending on whether the local data is
    greater or smaller than the requested one. Sampling with replacement may negatively
    affect the accuracy of the model since we would train on a repeated and limited
    subset of the data. If we enable the replication, we would always have the most
    data locally in every node, assuming it can fit in memory.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，每个节点都将从本地可用数据中获取这50K个样本，根据本地数据量与请求的数据量的大小，可以有或没有对数据进行采样。采用有放回抽样可能会对模型的准确性产生负面影响，因为我们将在数据的重复和有限的子集上进行训练。如果我们启用复制，我们在每个节点上始终具有最多的本地数据，假设可以放入内存。
- en: A special case is when we want to process exactly the amount of local data without
    sampling (*train_samples_per_iteration = -1*). In that case, we would iterate
    over the same dataset again and again at every iteration, which is redundant for
    multiple epochs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要精确处理本地数据量而不对数据进行采样（*train_samples_per_iteration = -1*）时，也是一个特殊情况。在这种情况下，在每次迭代中，我们将反复迭代相同的数据集，这对于多次迭代来说是多余的。
- en: Another special case is when `samples_per_iteration` is close to or greater
    than N * n with replication enabled. In this case, every node would train with
    almost the whole data or more at each iteration. Similarly, it would re-use almost
    the same data at every iteration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个特殊情况是当`samples_per_iteration`接近或大于启用复制的N * n。在这种情况下，每个节点在每次迭代中将几乎使用整个数据集或更多进行训练。同样，在每次迭代中几乎使用相同的数据。
- en: For those two special cases, the `shuffle_training_data` is automatically turned
    on. That is, local data will be randomly shuffled before each training.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种特殊情况，`shuffle_training_data`会自动开启。也就是说，在每次训练之前本地数据将被随机混洗。
- en: To conclude, depending on the size of data we could or could not replicate in
    every node. H2O offers a smart way to automatically tune and adapt the size of
    each iteration by balancing the CPU cost and the network overhead. Unless you
    have some requirement for fine-tuning your system, you probably want to use the
    self-tuning option.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，根据数据大小的不同，我们可能会在每个节点上复制或不复制数据。H2O提供了一种智能的方式，通过平衡CPU成本和网络开销自动调整和适应每次迭代的大小。除非你对系统进行微调有特殊要求，你可能会想使用自动调整选项。
- en: The distributed algorithm for deep learning will benefit your final model in
    both accuracy and training speed. Even though you might not have a very large
    dataset, this distributed approach is something you want to consider for a production
    system.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的分布式算法将在准确性和训练速度上使你的最终模型受益。即使你可能没有一个非常大的数据集，这种分布式方法也是你考虑用于生产系统的东西。
- en: Sparkling Water
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sparkling Water
- en: Although H2O can run on its own standalone cluster, an enterprise environment
    would probably already have a distributed data processing cluster. Managing two
    separate clusters, even if physically on the same machines, can be expensive and
    conflicting.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管H2O可以在自己的独立集群上运行，但企业环境可能已经有一个分布式数据处理集群。即使物理上在相同的机器上，管理两个单独的集群也可能会很昂贵和冲突。
- en: '**Apache Spark** is nowadays the de-facto computation framework for large datasets
    and for building scalable data products. H2O includes Sparkling Water, an abstraction
    layer that lets you model your data and algorithms together with all of the features
    and functionalities of the native framework but with the capabilities of Spark.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**如今是处理大型数据集和构建可扩展数据产品的事实计算框架。H2O包括Sparkling Water，这是一个抽象层，让你可以将数据和算法模型与本机框架的所有功能和功能结合起来，同时还具有Spark的能力。'
- en: Sparkling Water is an alternative to the ML and MLlib frameworks for doing machine
    learning and one of the few alternatives for deep learning on top of Spark.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water是用于进行机器学习的ML和MLlib框架的替代品，也是在Spark之上进行深度学习的少数替代品之一。
- en: Spark is designed and implemented in Scala. In order to understand the inter-operability
    of H2O and Spark, we need to refer to the native Scala APIs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala设计和实施的。为了理解H2O和Spark的互操作性，我们需要参考本地Scala API。
- en: In the Sparkling Water architecture, the H2O context co-exists with the Spark
    context in the driver node. Also, we now have SparkSession as main entry point
    in Spark 2\. Likely, the H2O and Spark executors co-exist in the worker nodes.
    As such, they share the same **Java Virtual Machine** (**JVM**) and memory. The
    resource allocation and setup could happen via YARN, a Hadoop component used for
    resource management and job scheduling.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在Sparkling Water架构中，H2O上下文与Spark上下文共存于驱动节点。此外，现在Spark 2有SparkSession作为主要入口点。很可能，H2O和Spark执行器共存于工作节点。因此，它们共享相同的**Java虚拟机**(**JVM**)和内存。资源分配和设置可以通过YARN来完成，YARN是用于资源管理和作业调度的Hadoop组件。
- en: You could build end-to-end pipelines combining both the strengths of Spark and
    MLlib with the features of H2O.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以构建端到端的管道，结合了Spark和MLlib的优势以及H2O的特点。
- en: For example, you might use Spark and H2O together for data munging and alternate
    different transformation functions. Then do the deep learning modeling in H2O.
    Ultimately you can return the trained model to be integrated within a greater
    application.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会一起使用Spark和H2O进行数据整理并交替应用不同的转换函数。然后在H2O中进行深度学习建模。最终，你可以将训练好的模型返回，以在更大的应用程序中进行集成。
- en: Spark offers three APIs for storing, modeling, and manipulating data. The typed
    **RDD** (**Resilient Distributed Data**), the DataFrame and the recent unified
    DataSet API. **DataFrame** is an RDD of objects of type `sql.Row`; thus in this
    integration, they are considered similarly.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了三种API用于存储、建模和操纵数据。类型化的**RDD**(**弹性分布式数据**)、DataFrame和最近统一的DataSet API。**DataFrame**是`sql.Row`类型的RDD；因此在这种集成中，它们被认为是类似的。
- en: Sparkling Water currently offers the conversion between `H2OFrame` and both
    RDD and DataFrame, in both directions. When converting an `H2OFrame` to an RDD,
    a wrapper is created, mapping the column names to the corresponding elements of
    a specified class type bound in the `Product` trait. That is, you will typically
    have to declare a Scala case class that acts as container for the data you are
    converting from the `H2OFrame`. This has the limitation that case classes can
    only store at most 21 flat fields. For larger tables, you can use nested structures
    or dictionaries.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water目前提供了在`H2OFrame`和RDD以及DataFrame之间的双向转换。将`H2OFrame`转换为RDD时，会创建一个包装器，将列名映射到在`Product`
    trait中指定的类类型的相应元素。也就是说，你通常需要声明一个Scala case类，作为你从`H2OFrame`转换数据的容器。这种方法的局限性在于case类只能存储最多21个平面字段。对于更大的表，可以使用嵌套结构或字典。
- en: Converting an `H2OFrame` into a Spark DataFrame does not require any type of
    parameter. The schema is dynamically derived from the column names and types of
    the `H2OFrame`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 将`H2OFrame`转换为Spark DataFrame不需要任何类型的参数。模式会动态地从`H2OFrame`的列名和类型中派生出来。
- en: Vice versa, the conversion from an existing RDD or DataFrame into an `H2OFrame`
    requires data to be duplicated and reloaded. Since the `H2OFrame` is registered
    in a Key/Value store, we can optionally specify the frame name. No explicit type
    is required to be specified in the case of RDDs since the Scala compiler can infer
    it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 相反地，将现有的RDD或DataFrame转换成`H2OFrame`需要数据被复制和重新加载。由于`H2OFrame`被注册在键/值存储中，我们可以选择性地指定框架名称。在RDD的情况下，不需要指定明确的类型，因为Scala编译器可以推断出来。
- en: 'The column primitive types will have to match according to the following table:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列的基本类型必须与以下表格相匹配：
- en: '| Scala/Java type | SQL type | H2O type |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Scala/Java类型 | SQL类型 | H2O类型 |'
- en: '| --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| NA | BinaryType | Numeric |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 二进制类型 | 数值 |'
- en: '| Byte | ByteType | Numeric |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Byte | ByteType | Numeric |'
- en: '| Short | ShortType | Numeric |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Short | ShortType | Numeric |'
- en: '| Integer | IntegerType | Numeric |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Integer | IntegerType | Numeric |'
- en: '| Long | LongType | Numeric |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Long | LongType | Numeric |'
- en: '| Float | FloatType | Numeric |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Float | FloatType | Numeric |'
- en: '| Double | DoubleType | Numeric |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Double | DoubleType | Numeric |'
- en: '| String | StringType | String |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| String | StringType | String |'
- en: '| Boolean | BooleanType | Numeric |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Boolean | BooleanType | Numeric |'
- en: '| java.sql.TimeStamp | TimestampType | Time |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| java.sql.TimeStamp | TimestampType | Time |'
- en: Both RDDs and `H2OFrame` share the same memory space in the executor JVMs; it
    is convenient to un-persist them after the conversion and duplication.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 和 `H2OFrame` 在执行器 JVM 中共享相同的内存空间；在转换和复制后取消持久化它们是方便的。
- en: Now that we have understood how the native Scala integration with Spark works,
    we can consider the Python wrapper.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了与 Spark 的本地 Scala 集成的工作原理，我们可以考虑 Python 包装器。
- en: In the driver program, the Python `SparkContext` will use `Py4J` to start the
    driver JVM and the Java-corresponding `SparkContext`. The latter will create the
    `H2OContext` which will then start the H2O cloud in the Spark cluster. After this
    setup stage, the Python APIs of both H2O and `PySpark` can be used to interact
    with data and algorithms.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在驱动程序中，Python `SparkContext` 将使用 `Py4J` 启动驱动程序 JVM 和相应的 Java `SparkContext`。后者将创建
    `H2OContext`，然后在 Spark 集群中启动 H2O 云。在此设置阶段之后，可以使用 H2O 和 `PySpark` 的 Python API
    与数据和算法进行交互。
- en: Although `PySpark` and `PySparkling` are good options for developing on top
    of Spark and H2O in Python, please bear in mind that the Python APIs are wrappers
    around the JVM executors. Maintaining and debugging complex projects in a distributed
    environment could be more tedious than sticking with the native APIs could help.
    Nevertheless, in most cases, the Python API will work just fine and you will not
    have to switch between Python and the native language.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `PySpark` 和 `PySparkling` 是在 Python 中开发 Spark 和 H2O 的良好选择，但请记住 Python API
    是 JVM 执行器的包装器。在分布式环境中维护和调试复杂项目可能比坚持使用本地 API 更加繁琐。尽管如此，在大多数情况下，Python API 都能正常工作，您不必在
    Python 和本地语言之间切换。
- en: Testing
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: Before we discuss what testing means in data science, let's summarize a few
    concepts.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论数据科学中测试的含义之前，让我们总结一些概念。
- en: 'Firstly and in general, what is a model in science? We can cite the following
    definitions:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，总的来说，在科学中什么是模型？我们可以引用以下定义：
- en: '*In science, a model is a representation of an idea, an object or even a process
    or a system that is used to describe and explain phenomena that cannot be experienced
    directly.*'
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在科学中，模型是用于描述和解释无法直接体验的现象的想法、对象、甚至过程或系统的表示。*'
- en: ''
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Scientific Modelling, Science Learning Hub, http://sciencelearn.org.nz/Contexts/The-Noisy-Reef/Science-Ideas-and-Concepts/Scientific-modelling*'
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*科学建模, 科学学习中心, http://sciencelearn.org.nz/Contexts/The-Noisy-Reef/Science-Ideas-and-Concepts/Scientific-modelling*'
- en: 'And this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 还有这个：
- en: '*A scientific model is a conceptual, mathematical or physical representation
    of a real-world phenomenon. A model is generally constructed for an object or
    process when it is at least partially understood, but difficult to observe directly.
    Examples include sticks and balls representing molecules, mathematical models
    of planetary movements or conceptual principles like the ideal gas law. Because
    of the infinite variations actually found in nature, all but the simplest and
    most vague models are imperfect representations of real-world phenomena.*'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*科学模型是对现实世界现象的概念、数学或物理表示。当对象或过程至少部分理解但难以直接观察时，通常会构建模型。例如，用棍子和球表示分子，数学模型表示行星运动或概念原理如理想气体定律。由于实际自然界中的无限变化，除了最简单和最模糊的模型外，其他模型都是对真实世界现象的不完美表示。*'
- en: ''
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What is a model in science?, Reference: https://www.reference.com/science/model-science-727cde390380e207'
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '在科学中，什么是模型？参考: https://www.reference.com/science/model-science-727cde390380e207'
- en: We need a model in order to simplify the complexity of a system in the form
    of a hypothesis. We proved that deep neural networks can describe complex non-linear
    relationships. Even though we are just approximating a real system with something
    more complex than shallow models, in the end this is just another approximation.
    I doubt any real system actually works as a neural network. Neural networks were
    inspired by the way our brain processes information, but they are a huge simplification
    of it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个模型来简化系统的复杂性，以一种假设的形式。我们证明了深度神经网络可以描述复杂的非线性关系。尽管我们只是用比浅层模型更复杂的东西来逼近一个真实系统，但最终这只是另一个近似。我怀疑任何真实系统实际上都像神经网络一样工作。神经网络受到我们的大脑处理信息的方式的启发，但它们只是对它的巨大简化。
- en: A model is defined according to some parameters (parametric model). On one hand,
    we have a definition of a model as function mapping an input space to an output.
    On the other hand, we have a bunch of parameters that the function needs in order
    to apply the mapping. For instance, the weights matrix and biases.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是根据一些参数（参数模型）来定义的。一方面，我们有一个将输入空间映射到输出的函数模型的定义。另一方面，我们需要一堆参数，函数需要这些参数来应用映射。例如，权重矩阵和偏差。
- en: Model fitting and training are two terms referring to the process of estimating
    the parameters of that model so that it best describes the underlying data. Model
    fitting happens via a learning algorithms that defines a loss function depending
    on both the model parameters and the data, and it tries to minimize this function
    by estimating the best set of values for the model parameters. One of the most
    common algorithm is Gradient Descent, with all its variants. See the previous
    Training section. For auto-encoder, you would minimize the reconstruction error
    plus the regularization penalty, if any.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 模型拟合和训练是指估计模型参数以使其最佳描述基础数据的过程。模型拟合通过定义依赖于模型参数和数据的损失函数的学习算法进行，然后尝试通过估计模型参数的最佳值集合来最小化这个函数。其中最常见的算法之一是梯度下降，以及它的所有变体。请参见之前的训练部分。对于自动编码器，你将最小化重构误差以及正则化惩罚（如果有的话）。
- en: '**Validation** is sometimes confused with testing and evaluation. Validation
    and testing often use the same techniques and/or methodology but they serve two
    different purposes.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**验证**有时被与测试和评估混淆。验证和测试通常使用相同的技术和/或方法，但它们有两个不同的目的。'
- en: Model validation corresponds to a type of hypothesis validation. We consider
    our data to be well described by a model. The hypothesis is that, if that model
    is correct, after having been trained (parameters estimation), it will describe
    unseen data the same way it describes the training set. We hypothesize that the
    model generalizes enough given the limits of the scenario in which we will use
    it. Model validation aims to find a measure (often referred to as a metric) that
    quantifies how well the model fits the validation data. For labeled data, we might
    derive a few metrics from either the **Receiver Operating Characteristic** (**ROC**)
    or Precision-Recall (**PR**) curve computed from the anomaly scores on the validation
    data. For unlabeled data, you could for instance use the **Excess-Mass** (**EM**)
    or **Mass-Volume** (**MV**) curve.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 模型验证对应于一种假设验证。我们认为我们的数据可以被模型很好地描述。假设是，如果该模型是正确的，在经过训练（参数估计）后，它将以与训练集相同的方式描述未见过的数据。我们假设模型在我们将要使用的场景的限制下足够泛化。模型验证旨在找到一个量化模型如何拟合验证数据的度量（通常称为指标）。对于有标签数据，我们可以从验证数据上的异常分数计算的**受试者工作特征**（**ROC**）或精确率-召回率（**PR**）曲线中推导出一些指标。对于无标签数据，例如可以使用**异常质量**（**EM**）或**质量-体积**（**MV**）曲线。
- en: Although model validation can be a way to evaluate performances, it is widely
    used for model selection and tuning.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型验证可以作为评估性能的一种方式，但它被广泛用于模型选择和调整。
- en: '**Model selection** is the process of selecting among a set of candidates,
    the model that scores highest in the validation. The set of candidates could be
    different configurations of the same model, many different models, a selection
    of different features, different normalization, and/or transformation techniques,
    and so on.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型选择**是在一组候选模型中选择得分最高的模型的过程。候选模型可以是相同模型的不同配置，许多不同模型，选择不同特征、不同归一化和/或转换技术等。'
- en: In deep neural networks, feature selection could be omitted because we delegate
    to the network itself the role of figuring out and generating relevant features.
    Moreover, features are also discarded via regularization during learning.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，特征选择可能被省略，因为我们委托网络本身来扮演找出和生成相关特征的角色。此外，特征也通过学习过程中的正则化而被丢弃。
- en: The hypothesis space (the model parameters) depends on the choice of topology,
    the activation functions, size and depth, pre-processing (for example, whitening
    of an image or data cleansing), and post-processing (for example, use an auto-encoder
    to reduce the dimensionality and then run a clustering algorithm). We might see
    the whole pipeline (the set of components on a given configuration) as the model,
    even though the fitting could happen independently for each piece.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 假设空间（模型参数）取决于拓扑选择、激活函数、大小和深度、预处理（例如图像白化或数据清洗）和后处理（例如，使用自动编码器减少维度，然后运行聚类算法）。我们可以将整个流程（给定配置上的组件集）看作模型，即使每个部分的拟合可能是独立进行的。
- en: Analogously, the learning algorithm will introduce a few parameters (for example,
    learning rate or decay rate). In particular, since we want to maximize the generalization
    of the model, we generally introduce a regularization technique during the learning
    function, and that will introduce additional parameters (for example, sparsity
    coefficient, noise ratio, or regularization weight).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，学习算法将引入一些参数（例如，学习率或衰减率）。特别是，因为我们希望最大化模型的泛化能力，通常在学习函数中引入正则化技术，这将引入额外的参数（例如，稀疏系数，噪声比或正则化权重）。
- en: Moreover, the particular implementation of the algorithm also has a few parameters
    (for example, epochs, number of samples per iteration).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，算法的特定实施还具有一些参数（例如，周期，迭代次数）
- en: We can use the same validation technique to quantify the performance of the
    model and learning algorithm together. We can imagine to have a single big vector
    of parameters that include the model parameters plus the hyper-parameters. We
    can tune everything in order to minimize the validation metric.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的验证技术来量化模型和学习算法的性能。我们可以想象存在一个包括模型参数和超参数的单个大向量。我们可以调整所有内容以最小化验证度量标准。
- en: 'At the end of the model selection and tuning via validation, we have obtained
    a system that:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过验证的模型选择和调整结束时，我们得到了一个系统，该系统：
- en: Takes some of the available data
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取可用数据的一部分
- en: Divides into training and validation, making sure to not introduce biases or
    unbalancing
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分为训练和验证，确保不引入偏见或不平衡
- en: Creates a search space made up of the set of different models, or different
    configurations, learning parameters, and implementation parameters
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建由不同模型或不同配置、学习参数和实现参数构成的搜索空间
- en: Fits each model on the training set by using the training data and learning
    algorithm with a given loss function, including regularization, according to the
    specified parameters
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用给定的损失函数（包括正则化）根据指定参数在训练数据上使用训练数据和学习算法对每个模型进行拟合
- en: Computes the validation metric by applying the fitted model on the validation
    data
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在验证数据上应用拟合模型来计算验证度量标准
- en: Selects the one point in the search space that minimizes the validation metric
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择使验证度量标准最小化的搜索空间中的一个点
- en: The selected point will formalize our final theory. The theory says that our
    observations are generated from a model that is the outcome of the pipeline corresponding
    to the selected point.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的点将明确定义我们的最终理论。该理论表明我们的观察结果是从所选点对应的流程生成的模型生成的。
- en: Evaluation is the process of verifying that the final theory is acceptable and
    quantifying its quality from both technical and business perspectives.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是验证最终理论的可接受性并从技术和业务角度量化其质量的过程。
- en: Scientific literature shows how, during the course of history, one theory has
    succeeded another. Choosing the right theory without introducing a cognitive bias
    requires rationality, accurate judgment, and logical interpretation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 科学文献显示了在历史进程中一个理论是如何取代另一个的。在不引入认知偏见的情况下选择正确的理论需要理性、准确的判断和逻辑解释。
- en: Confirmation theory, the study that guides scientific reasoning other than reasoning
    of the deductive kind, can help us defining a few principles.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 确认理论，即指导科学推理而非演绎推理的研究，可以帮助我们定义一些原则。
- en: In our context, we want to quantify the quality of our theory and verify it
    is good enough and that it an evident advantage with respect to a much simpler
    theory (the baseline). A baseline could be a Naïve implementation of our system.
    In the case of an anomaly detector, it could simply be a rule-based threshold
    model where anomalies are flagged for each observation whose feature values are
    above a static set of thresholds. Such a baseline is probably the simplest theory
    we can implement and maintain over time. It will probably not satisfy the full
    acceptance criteria, but it will help us to justify why we need another theory,
    that is, a more advanced model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们想量化我们的理论的质量，并验证它是否足够好，并且与一个简单得多的理论（基线）相比具有显而易见的优势。基线可以是我们系统的一个天真的实现。在异常检测器的情况下，它可以简单地是一个基于规则的阈值模型，其中对于每个特征值超过静态阈值集的观察结果都标记为异常。这样一个基线可能是我们可以在一段时间内实现和维护的最简单的理论。它可能不会满足所有的接受标准，但它将帮助我们证明为什么我们需要另一个理论，即更高级的模型。
- en: 'Colyvan, in his book *The Indispensability of Mathematics*, summarized the
    criteria for accepting a good theory as a replacement for another based on four
    major criteria:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Colyvan，在他的书 *数学的不可或缺性* 中，总结了接受一个好的理论作为另一个理论替代品的四个主要标准：
- en: '**Simplicity**/**Parsimony**: Simple is better than complex if the empirical
    results are comparable. Complexity is only required when you need to overcome
    some limitation. Otherwise, simplicity should be preferred in both its mathematical
    form and its ontological commitments.'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**简洁性**/**简约性**：如果实证结果可以比较的话，简单比复杂更好。只有在需要克服某些限制时才需要复杂性。否则，无论是数学形式还是本体论承诺，都应该更喜欢简单。'
- en: '**Unification**/**Explanatory Power**: The capacity of consistently explaining
    both existing and future observations. Moreover, unification means minimizing
    the number of *theoretical devices* needed for the explanation. A good theory
    offers an intuitive way of explaining why a given prediction is expected.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**统一性**/**解释力**：能够一致解释现有和未来观察结果的能力。此外，统一性意味着尽量减少解释所需的*理论设备*数量。一个好的理论提供了一个直观的方法来解释为什么期望某个给定的预测。'
- en: '**Boldness**/**Fruitfulness**: A bold theory is an idea that, if it was true,
    would be able to predict and/or explain a lot more about the system we are modeling.
    Boldness helps us refuse theories that would contribute very little to what we
    know already. It is allowed to formulate something new and innovative and then
    try to contradict it with known evidence. If we can''t prove a theory is correct
    we can demonstrate that the evidence does not prove the contrary. Another aspect
    is heuristic potential. A good theory can enable more theories. Between two theories
    we want to favor the more fruitful: the one that has more potential for being
    reused or extended in future.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大胆性**/**富有成效性**：一个大胆的理论是一个想法，如果它是真实的，就能够预测和/或解释我们正在建模的系统的更多内容。大胆性有助于我们拒绝那些对我们已知的知识贡献很少的理论。可以制定一些新颖而创新的内容，然后尝试用已知证据来反驳它。如果我们无法证明一个理论是正确的，我们可以证明证据并不证明相反。另一个方面是启发式潜力。一个好的理论可以促使更多的理论。在两个理论之间，我们希望更偏向于更富有成效的那一个：具有更多被重新使用或扩展的潜力的那一个。'
- en: '**Formal elegance**: A theory must have an aesthetic appeal and should be robust
    enough for ad-hoc modifications to a failing theory. Elegance is the quality of
    explaining something in a clear, economical, and concise way. Elegance also enables
    better scrutiny and maintainability.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**形式优雅**：一个理论必须具有美学吸引力，并且应该足够强大，以便对一个失败的理论进行临时修改。优雅是以一种清晰、经济、简洁的方式解释某事的质量。优雅也能够更好地进行审查和维护。'
- en: 'These criteria, in the case of neural networks, are translated into the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的情况下，这些标准被转化为以下内容：
- en: Shallow models with a few layers and small capacity are preferred. As we discussed
    in the Network design section, we start with something simpler and incrementally
    increase complexity if we need so. Eventually the complexity will converge and
    any further increase will not give any benefit.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们更喜欢具有少量层和小容量的浅层模型。正如我们在网络设计部分讨论的那样，我们从简单的东西开始，如果需要的话逐渐增加复杂性。最终，复杂性将收敛，并且任何进一步的增加都不会带来任何好处。
- en: 'We will distinguish between explanatory power and unificatory power:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将区分解释力和统一力：
- en: '**Explanatory power** is evaluated similarly to model validation but with a
    different dataset. We mentioned earlier that we broke the data into three groups:
    training, validation, and testing. We will use the training and validation to
    formulate the theory (the model and hyper-parameters) that the model is retrained
    on the union of both training and validation set becoming the new training set;
    and ultimately the final, already validated, model is evaluated against the test
    set. It is important at this stage to consider the validation metrics on the training
    set and test set. We would expect the model to perform better on the training
    set, but having too wide a gap between the two means the model does not explain
    unseen observations very well.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释力** 与模型验证类似，但使用不同的数据集进行评估。我们之前提到我们将数据分成三组：训练、验证和测试。我们将使用训练和验证来制定理论（模型和超参数），然后模型会重新训练在两者的联合上，成为新的训练集；最终，已经验证过的最终模型将与测试集进行评估。在这个阶段，考虑在训练集和测试集上的验证指标非常重要。我们期望模型在训练集上表现更好，但两者之间有太大差距意味着模型无法很好地解释未见观察。  '
- en: '**Unificatory power** can be represented by the model sparsity. Explaining
    means mapping input to output. Unifying means reducing the number of elements
    required to apply the mapping. By adding a regularization penalty, we make the
    features sparser, which means we can explain an observation and its prediction
    using fewer regressors (*theoretical devices*).'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一力** 可以通过模型的稀疏性来表示。解释意味着将输入映射到输出。统一意味着减少应用映射所需的元素数量。通过添加正则化惩罚，我们使特征更稀疏，这意味着我们可以使用更少的回归器（*理论设备*）来解释观察和其预测。  '
- en: 'Boldness and fruitfulness can also be split into two aspects:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**果实性和果敢性** 也可以分为两个方面：  '
- en: '**Boldness** is represented by our test-driven approach. In addition to point
    2, where we try to make clear what a model does and why, in the test-driven approach,
    we treat the system as a black box and check the responses under different conditions.
    For an anomaly detection, we can systematically create some failing scenarios
    with different degrees of anomalousness and measure at which level the system
    is able to detect and react. Or for time-responsive detectors, we could measure
    how long it takes to detect a drift in the data. If the tests pass, then we have
    achieved confidence that it works no matter how. This is probably one of the most
    common approaches in machine learning. We try everything that we believe can work;
    we carefully evaluate and tentatively accept when our critical efforts are unsuccessful
    (that is,. the tests pass).'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**果敢性** 由我们的测试驱动方法来代表。除了第2点，我们试图明确模型的功能和原因，并在测试驱动方法中，我们把系统视为黑盒，并检查在不同条件下的回应。对于异常检测，我们可以系统地创建一些不同程度异常性的失败场景，并测量系统在何种程度上能够检测和反应。或者对于时间反应探测器，我们可以测量检测数据漂移需要多长时间。如果测试通过，那么我们就可以确定它无论如何都能正常工作。这可能是机器学习中最常见的方法之一。我们尝试一切我们认为可能奏效的方法；当我们的关键努力未能成功时，我们会谨慎评估并暂时接受（即，测试通过）。'
- en: '**Fruitfulness** comes from the reusability of a given model and system. Is
    it too strongly coupled to the specific use case? Auto-encoders work independently
    of what the underlying data represent, they use very little domain knowledge.
    Thus, if the theory is that a given auto-encoders can be used for explaining a
    system in its working conditions, then we could extend it and re-use it for detecting
    in any kind of system. If we introduce a pre-processing step (such as image whitening),
    then we are assuming the input data are pixels of an image, thus even if this
    theory superbly fit our use case it has a smaller contribution to the greater
    usability. Nevertheless, if the domain-specific pre-processing improves the final
    result noticeably, then we will consider it as an important part of the theory.
    But if the contribution is negligible, it is recommended to refuse it in favor
    of something more reusable.'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**果实性** 来自于给定模型和系统的可重复性。它是否与特定用例过于紧密耦合？自编码器独立于底层数据表示的内容，它们使用非常少的领域知识。因此，如果理论是特定自编码器可用于解释系统在其工作条件下的情况，那么我们可以扩展它并在任何类型的系统中重复使用。如果我们引入一个预处理步骤（如图像白化），那么我们就假设输入数据是图像的像素，因此即使这个理论非常适合我们的用例，它对更大范围的可用性贡献度较小。然而，如果领域特定的预处理显著改善最终结果，那么我们将把它视为理论的重要部分。但如果贡献可以忽略不计，建议拒绝以换取更可重复的东西。  '
- en: One aspect of elegance in deep neural networks could implicitly be represented
    as the capacity of learning features from the data rather than hand-crafting them.
    If that is the case, we can measure how the same model is able to self-adapt to
    different scenarios by learning relevant features. For example, we could test
    that given any dataset we consider normal, we can always construct an auto-encoder
    that learns the normal distribution. We can either add or remove features from
    the same dataset or partition according to some external criteria generating dataset
    with different distributions. Then we can inspect the learned representations
    and measure the reconstruction ability of the model. Instead of describing the
    model as a function of the specific input features and weights, we describe it
    in terms of neurons—entities with learning capabilities. Arguably, this is a good
    example of elegance.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度神经网络中优雅的一个方面可以被隐式地表示为从数据中学习特征而不是手动构建特征的能力。如果是这样，我们可以通过学习相关特征来衡量同一模型在不同场景下的自适应能力。例如，我们可以测试，在给定任何我们认为正常的数据集的情况下，我们是否可以构建一个始终学习正态分布的自动编码器。我们可以向同一数据集中添加或删除特征，或根据某些外部标准进行分组，从而生成具有不同分布的数据集。然后，我们可以检查学习到的表示，并测量模型的重构能力。与描述模型的具体输入特征和权重的函数相比，我们将其描述为具有学习能力的神经元实体。可以说，这是一个很好的优雅示例。
- en: From a business perspective, we really need to think carefully about what the
    acceptance criteria are.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从商业角度来看，我们真的需要仔细考虑接受标准是什么。
- en: 'We would like to answer at least the following questions:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至少要回答以下问题：
- en: What problem are we trying to solve?
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们试图解决什么问题？
- en: How is the business going to benefit from it?
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司将如何从中受益？
- en: In which way will the model be integrated within an existing system from a practical
    and technical point of view?
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实际和技术层面上，模型将以何种方式集成到现有系统中？
- en: What is the final deliverable so that it is consumable and actionable?
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的交付物如何才能具有可消化性和可执行性？
- en: We will try to use as example an intrusion detection system and try to respond
    to these questions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试以入侵检测系统为例，并尝试回答这些问题。
- en: We would like to monitor a network traffic in real time, taking individual network
    connections and marking them as normal or suspicious. This will allow the business
    to have enhanced protection against intruders. The flagged connections will be
    stopped and will go into a queue for manual inspection. A team of security experts
    will look into those connections and determine whether it is a false alarm and,
    in the case of a confirmed attack, will mark the connection under one of the available
    labels. Thus, the model has to provide a real-time list of connections sorted
    by their anomaly score. The list cannot contain more elements than the capability
    of the security team. Moreover, we need to balance the cost of permitting an attack,
    the cost of damages in the case of an attack, and the cost required for inspection.
    A minimum requirement consisting of precision and recall is a must in order to
    probabilistically limit the worst-case scenario.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要实时监控网络流量，对每个单独的网络连接进行标记，标记为正常或可疑。这将使业务能够更好地防范入侵者。被标记的连接将被停止，并进入手动检查队列。安全专家团队将查看这些连接，并确定是否为误报，如果确认是攻击，则将该连接标记为其中一个可用的标签。因此，模型必须提供按异常分数排序的连接实时列表。列表中的元素数量不能超过安全团队的能力。此外，我们需要在允许攻击的成本、在发生攻击时的损害成本以及检查所需的成本之间取得平衡。为了以概率化方式限制最坏情况，最低要求是精确度和召回率。
- en: All of these evaluation strategies have been mainly defined qualitatively rather
    quantitatively. It would be quite hard to compare and report something that is
    not numerically measurable.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些评估策略都主要是定性而非定量定义的。很难比较和报告那些无法用数字衡量的内容。
- en: 'Bryan Hudson, a Data Science practitioner, said:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学从业者Bryan Hudson说：
- en: '*If you can''t define it, you can''t measure it. If it can''t be measured,
    it shouldn''t be reported. Define, then measure, then report.*'
  id: totrans-294
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你无法定义它，那就无法衡量它。如果无法测量，就不应该报告。首先定义，然后测量，再报告。*'
- en: Define, then measure, then report. But be careful. We might think of defining
    a new evaluation metric that takes into account every possible aspect and scenario
    discussed so far.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义，然后测量，再报告。但要小心。我们可以考虑定义一个新的评估指标，考虑到迄今讨论的每个可能的方面和场景。
- en: 'Whilst many data scientists may attempt to quantify the evaluation of a model
    using a single utility function, as you do during validation, for a real production
    system, this is not advised. As also expressed in the Professional Data Science
    Manifesto:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多数据科学家可能会尝试使用单一的实用函数来量化模型的评估，就像您在验证过程中所做的那样，但对于真正的生产系统，这是不被建议的。正如专业数据科学宣言中所表达的那样：
- en: '*A product needs a pool of measures to evaluate its quality. A single number
    cannot capture the complexity of reality.*'
  id: totrans-297
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*产品需要一系列措施来评估其质量。一个单一数字无法捕捉现实的复杂性。*'
- en: ''
  id: totrans-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The Professional Data Science Manifesto, www.datasciencemanifesto.org*'
  id: totrans-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*专业数据科学宣言，www.datasciencemanifesto.org*'
- en: And even after we have defined our **Key Performance Indicators** (**KPIs**),
    their real meaning is relative when compared to a baseline. We must ponder over
    why we need this solution with respect to a much simpler or existing one.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们定义了**关键绩效指标** (**KPIs**)之后，与基准相比，它们的实际含义是相对的。我们必须考虑为什么我们需要对比更简单或现有的解决方案。
- en: The evaluation strategy requires defining test cases and KPIs so that we can
    cover the most scientific aspects and business needs. Some of them are aggregated
    numbers, others can be represented in charts. We aim to summarize and efficiently
    present all of them in a single evaluation dashboard.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 评估策略需要定义测试用例和KPI，以便我们可以涵盖最科学的方面和业务需求。其中一些是聚合数字，其他可以用图表表示。我们的目标是在单个评估仪表板中总结所有这些内容并有效地呈现它们。
- en: In the following sections, we will see a few techniques used for model validation
    using both labeled and unlabelled data.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将看到一些使用标记和未标记数据进行模型验证的技术。
- en: Then we will see how to tune the space for parameters using some parallel search
    space techniques.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何使用一些并行搜索空间技术来调整参数空间。
- en: Lastly we will give an example of a final evaluation for a network intrusion
    use case using A/B testing techniques.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将给出使用A/B测试技术进行网络入侵使用情况的最终评估的示例。
- en: Model validation
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型验证
- en: The goal of model validation is to evaluate whether the numerical results quantifying
    the hypothesized estimations/predictions of the trained model are acceptable descriptions
    of an independent dataset. The main reason is that any measure on the training
    set would be biased and optimistic since the model has already seen those observations.
    If we don't have a different dataset for validation, we can hold one fold of the
    data out from training and use it as benchmark. Another common technique is the
    cross-fold validation, and its stratified version, where the whole historical
    dataset is split into multiple folds. For simplicity, we will discuss the hold-one-out
    method; the same criteria apply also to the cross-fold validation.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 模型验证的目标是评估所训练模型的假设估计/预测的数值结果是否是对独立数据集的可接受描述。主要原因是由于训练集上的任何测量都会存在偏见和乐观主义，因为模型已经看到了这些观察结果。如果我们没有不同的验证数据集，我们可以从训练数据中留出一部分并将其用作基准。另一个常见的技术是交叉折叠验证，及其分层版本，其中整个历史数据集被分成多个折叠。为简单起见，我们将讨论留一法;
    同样的标准也适用于交叉折叠验证。
- en: The splitting into training and validation set cannot be purely random. The
    validation set should represent the future hypothetical scenario in which we will
    use the model for scoring. It is important not to contaminate the validation set
    with information that is highly correlated with the training set (leakage).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集的划分不能完全随机。验证集应代表我们将用模型进行评分的未来假设场景。重要的是不要用与训练集高度相关的信息（泄露）污染验证集。
- en: A bunch of criteria can be considered. The easiest is the time. If your data
    is chronological, then you'll want to select the validation set to always be after
    the training set.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 可以考虑一系列标准。最简单的是时间。如果您的数据是按时间顺序排列的，那么您将希望选择验证集总是在训练集之后。
- en: If your deployment plan is to retrain once a day and score all the observations
    of the next 24 hours, then your validation set should be exactly 24 hours. All
    observations after 24 hours would never be scored with the last trained model
    but with a model trained with the additional past 24 hours' observations.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的部署计划是每天重新训练一次，并对接下来24小时的所有观察结果进行评分，那么您的验证集应恰好为24小时。24小时后的所有观察结果将永远不会使用最后训练的模型进行评分，而是使用包括额外过去24小时观察结果的模型进行评分。
- en: Of course, only using 24 hours of observations for validation is too restrictive.
    We will have to perform a few validations, where we select a number of time split
    points; for each split point, we train the model up to that point and validate
    on the data in the following validation window.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，仅使用24小时观察来进行验证太过严格了。我们需要进行几次验证，在每个分割点，我们将在该点之前训练模型，并在随后的验证窗口中验证数据。
- en: The choice of number of split points depends on the amount of available resources.
    Ideally, we would like to map the exact frequency at which the model will be trained,
    that is, one split point a day for the last year or so.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 分割点的选择取决于可用资源的数量。理想情况下，我们希望能够映射模型训练的确切频率，也就是说，过去一年左右每天一个分割点。
- en: 'There are a bunch of operational things to consider when splitting in train
    and validation set:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割训练和验证集时需要考虑一些操作事项：
- en: Regardless of whether the data has a timestamp or not, the chronological time
    should be set by what would have been available at that time. In other words,
    let's suppose that you have 6 hours of delay between the data generation and the
    time when it is turned into a feature space for training; you should consider
    the latter time in order to filter what was before or after the given split point.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论数据是否具有时间戳，时间顺序应该根据当时可用的时间来设定。换句话说，假设数据生成和将其转换为训练特征空间之间有6小时的延迟；你应该考虑后者的时间，以便过滤掉分割点之前或之后的数据。
- en: How long does the training procedure take? Suppose our model requires 1 hour
    to be retrained; we would schedule its training one hour before the expiration
    of the previous model. The scores during its training interval will be covered
    by the previous model. That means we cannot predict any observation that happens
    in the following hour of the last data collected for training. This introduces
    a gap between the training set and validation set.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程需要多长时间？假设我们的模型需要1小时进行重新训练；我们将在之前模型过期的前一小时安排重新训练。在其训练间隔期间得分将由以前的模型覆盖。这意味着我们无法对在最后一次收集训练数据的后续一小时内发生的任何观察进行预测。这在训练集和验证集之间引入了一个间隙。
- en: How does the model perform for day-0 malware (the cold start problem)? During
    validation, we want to project the model in the worst-case scenario instead of
    being over-optimistic. If we can find a partitioning attribute, such as device
    ID or network card MAC address, we can then divide users into buckets representing
    different validation folds and perform a cross-fold validation where iteratively
    you select one fold of users to validate the model trained with the remaining
    users folds. By doing so, we always validate predictions for users whose history
    we have never seen before. That helps with truly measuring the generalization
    for those cases where the training set already contains a strong signal of anomaly
    for the same device over past connections. In that case, it would be very easy
    for the model to spot anomalies but they would not necessary match a real use
    case.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在 day-0 恶意软件（冷启动问题）上表现如何？在验证过程中，我们希望以最坏的情况来评估模型，而不是过于乐观。如果我们可以找到一个分区属性，例如设备ID或网络卡MAC地址，那么我们可以将用户分成代表不同验证fold的桶，并进行交叉fold验证，依次选择一个用户fold来验证使用其他用户fold训练的模型。通过这样做，我们总是验证我们以前从未见过历史的用户的预测结果。这有助于真正衡量对于那些训练集已经包含同一设备在过去连接中的异常信号的情况的泛化能力。在这种情况下，模型很容易发现异常，但他们不一定与实际用例相匹配。
- en: The choice of attribute (primary key) on which to apply the partitioning is
    not simple. We want to reduce the correlation among folds as much as possible.
    If we ingenuously partition on the device ID, how will we cope with the same user
    or the same machine with multiple devices, all registered with a different identifier?
    The choice of partitioning key is an entity resolution problem. The correct way
    of solving this issue would be to firstly cluster the data belonging to the same
    entity and then partition such that data of the same entity is never split among
    different folds. The definition of the entity depends on the particular use case
    context.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用分区的属性(主键)的选择并不简单。我们希望尽可能减少fold之间的相关性。如果我们简单地根据设备ID进行分区，我们将如何处理同一用户或同一台机器具有多个设备，都使用不同的标识符注册的情况？选择分区键是一个实体解析问题。解决这个问题的正确方法是首先对属于同一实体的数据进行聚类，然后分区使得属于同一实体的数据绝不会分隔在不同的fold中。实体的定义取决于特定的用例背景。
- en: When performing cross-fold validation, we still need to ensure the time constraint.
    That is, for each validation fold, we need to find a time split point in the intersection
    with the other training folds. Filter the training set both on the entity id and
    timestamp; then filter the data in the validation fold according to the validation
    window and gap.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行交叉折叠验证时，我们仍然需要确保时间约束。也就是说，对于每个验证折叠，我们需要在与其他训练折叠的交集中找到一个时间分割点。在实体ID和时间戳上过滤训练集；然后根据验证窗口和间隔来过滤验证折叠中的数据。
- en: Cross-fold validation introduces a problem with class unbalancing. By definition;
    anomalies are rare; thus our dataset is highly skewed. If we randomly sample entities,
    then we would probably end up with a few folds without anomalies and a few with
    too many. Thus, we need to apply a stratified cross-fold validation where we want
    to preserve the same distribution of anomalies uniformly in each fold. This is
    a tricky problem in the case of unlabeled data. But we can still run some statistics
    on the whole feature space and partition in such a way as to minimize the distribution
    differences among folds.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉折叠验证引入了一个类别不平衡的问题。按定义；异常是罕见的；因此我们的数据集是高度倾斜的。如果我们随机抽样实体，那么我们可能会得到一些没有异常的折叠和一些有太多异常的折叠。因此，我们需要应用分层交叉折叠验证，我们希望在每个折叠中均匀保留相同的异常分布。这在未标记数据的情况下是一个棘手的问题。但是我们仍然可以对整个特征空间运行一些统计，并以最小化折叠之间的分布差异的方式进行分区。
- en: We have just listed a few of the common pitfalls to consider when defining the
    splitting strategy. Now we need to compute some metrics. The choice of the validation
    metric should be significant with the real operational use case.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚列举了在定义分割策略时需要考虑的一些常见陷阱。现在我们需要计算一些度量标准。验证度量标准的选择应与真实操作用例显著相关。
- en: We will see in the following sections a few possible metrics defined for both
    labeled and unlabeled data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中看到为标记和未标记数据定义的几个可能的度量。
- en: Labeled Data
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记数据
- en: Anomaly detection on labeled data can be seen just as a standard binary classifier.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 标记数据的异常检测可以被视为标准的二元分类器。
- en: Let ![Labeled Data](img/00385.jpeg) be our anomaly scoring function where the
    higher the score, the higher the probability of being an anomaly. For auto-encoders,
    it could simply be the MSE computed on the reconstruction error and rescaled to
    be in the range[0,1]. We are mainly interested in relative ordering rather than
    absolute values.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让![标记数据](img/00385.jpeg)成为我们的异常评分函数，其中分数越高，成为异常的概率就越高。对于自编码器来说，它可以简单地是重构误差上计算的MSE，并重新缩放为[0,1]范围内。我们主要关心的是相对排序而不是绝对值。
- en: We can now validate using either the ROC or PR curve.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用ROC或PR曲线进行验证。
- en: In order to do so, we need to set a threshold *a* that corresponds to the scoring
    function *s* and consider all of the points *x* with score *s(x) = a* to be classified
    as anomalies.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要设置一个与评分函数*s*对应的阈值*a*，并将具有评分*s(x) = a*的所有点*x*视为异常。
- en: 'For each value of *a*, we can calculate the confusion matrix as:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个*a*值，我们可以计算混淆矩阵如下：
- en: '| Number of observations n | Predicted anomaly *s(x) = a* | Predicted non-anomaly
    *(s < a)* |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 观察数量n | 预测的异常 *s(x) = a* | 预测的非异常 *(s < a)* |'
- en: '| True anomaly | True Positive (TP) | False Negative (FN) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 真异常 | 真正例（TP） | 假负例（FN） |'
- en: '| True non-anomaly | False Positive (FP) | True Negative (TN) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 真负例 | 假正例（FP） | 真负例（TN） |'
- en: 'From each confusion matrix corresponding to a value of a, we can derive the
    measures of **True Positive Rate** (**TPR**) and **False Positive Rate** (**FPR**)
    as:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从与a值对应的每个混淆矩阵中，我们可以推导出**真正例率**（**TPR**）和**假正例率**（**FPR**）的度量标准：
- en: '![Labeled Data](img/00386.jpeg)![Labeled Data](img/00387.jpeg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![标记数据](img/00386.jpeg)![标记数据](img/00387.jpeg)'
- en: We can draw each value of *a* in a two-dimensional space that generates the
    ROC curve consisting of ![Labeled Data](img/00388.jpeg).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在二维空间中绘制每个*a*值，生成包含![标记数据](img/00388.jpeg)的ROC曲线。
- en: 'The way we interpret the plot is as follows: each cut-off point tells us on
    the y-axis the fraction of anomalies that we have spotted among the full set of
    anomalies in the validation data (Recall). The x-axis is the false alarm ratio,
    the fraction of observations marked as anomalies among the full set of normal
    observations.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释图的方式如下：每个切断点告诉我们在y轴上我们在验证数据中发现的异常的比例（召回率）。x轴是误报比率，标记为异常的观察值在所有正常观察值中的比例。
- en: If we set the threshold close to 0, it means we are flagging everything as anomaly
    but all the normal observations will produce false alarms. If we set it close
    to 1, we will never fire any anomaly.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将阈值设定为接近0，意味着我们将一切标记为异常，但所有正常的观察将产生虚警。如果我们将其设定为接近1，我们将永远不会触发任何异常。
- en: Let's suppose for a given value of a the corresponding TPR = 0.9 and FPR = 0.5;
    this means that we detected 90% of anomalies but the anomaly queue contained half
    of the normal observations as well.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于给定的a值，相应的TPR = 0.9和FPR = 0.5；这意味着我们检测到了90%的异常，但异常队列中也包含了一半的正常观察。
- en: The best threshold point would be the one located at coordinates (0,1), which
    corresponds to 0 false positive and 0 false negatives. This never happens, so
    we need to find a trade-off between the Recall and false alarm ratio.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳阈值点将位于坐标（0,1）处，对应于0假阳性和0假阴性。这种情况从来不会发生，因此我们需要在召回率和虚警率之间找到一个折衷。
- en: One of the issues with the ROC curve is that does not show very well what happens
    for a highly skewed dataset. If anomalies represent only 1% of the data, the *x*
    axis is very likely to be small and we might be tempted to relax the threshold
    in order to increase the Recall without any major effect on the *x* axis.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线的一个问题是它不能很好地展现高度偏斜的数据集的情况。如果异常只占数据的1%，那么* x *轴很可能会很小，我们可能会放松阈值以增加召回率，而对*
    x *轴没有太大的影响。
- en: 'The **Precision-Recall** (**PR**) plot swaps the axis and replaces the FPR
    with the Precision defined as:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度-召回率**（**PR**）图交换轴，并用精确度替换FPR定义为：'
- en: '![Labeled Data](img/00389.jpeg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![有标签的数据](img/00389.jpeg)'
- en: Precision is a more meaningful metric and represents the fraction of anomalies
    among the list of detected ones.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是一个更有意义的指标，它代表了检测到的异常中的异常部分。
- en: The idea now is to maximize both the axes. On the *y* axis, we can observe the
    expected results of the portion that will be inspected, and the *x* axis tells
    how many anomalies we will miss, both of them on a scale that depends only on
    the anomaly probability.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的想法是最大化两个轴。在* y *轴上，我们可以观察到将要被检查的部分的预期结果，* x *轴告诉我们有多少异常将会遗漏，它们都取决于异常概率。
- en: Having a two-dimensional plot can help us understand how the detector would
    behave in different scenarios, but in order to apply model selection, we need
    to minimize a single utility function.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个二维图可以帮助我们理解检测器在不同场景下的行为，但为了应用模型选择，我们需要最小化一个单一的效用函数。
- en: A bunch of measures can be used to synthesize this. The most common one is the
    **area under the curve** (**AUC**), which is an indicator of the average performance
    of the detector under any threshold. For the ROC curve, the AUC can be interpreted
    as the probability that a uniformly drawn random anomaly observation is ranked
    higher than a uniformly drawn random normal observation. It is not very useful
    for anomaly detection.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 有一系列措施可以用来综合这一点。最常见的是**曲线下面积**（**AUC**），它是检测器在任何阈值下的平均性能指标。对于ROC曲线，AUC可以解释为均匀抽取的随机异常观察排在均匀抽取的随机正常观察之前的概率。这对于异常检测并不是非常有用。
- en: 'The absolute values of Precision and Recall being defined on the same scale
    can be aggregated using the harmonic mean, also known as **F-score**:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率的绝对值在同一尺度上被定义，可以使用调和平均值（也称为**F-score**）进行汇总：
- en: '![Labeled Data](img/00390.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![有标签的数据](img/00390.jpeg)'
- en: Here, *ß* is a coefficient that weights to what extent Recall is more important
    than Precision.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*ß*是一个系数，它权衡了召回率比精确度更重要的程度。
- en: The term ![Labeled Data](img/00391.jpeg) is added in order to scale the score
    between 0 and 1.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将评分缩放在0和1之间，添加了术语![有标签的数据](img/00391.jpeg)。
- en: 'In case of symmetry we obtain the F1-score:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对称的情况下，我们得到了F1分数：
- en: '![Labeled Data](img/00392.jpeg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![有标签的数据](img/00392.jpeg)'
- en: 'Security analysts can also set preferences based on minimum requirements for
    the values of Precision and Recall. In that situation, we can define the Preference-Centric
    score as:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 安全分析员也可以根据精确度和召回率的最小要求设定偏好。在这种情况下，我们可以将偏好中心得分定义为：
- en: '![Labeled Data](img/00393.jpeg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![有标签的数据](img/00393.jpeg)'
- en: The PC-score allows us to select a range of acceptable thresholds and optimize
    the points in the middle based on the F1-score. The unit term in the first case
    is added so that it will always outperform the second one.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: PC分数使我们能够选择一系列可接受的阈值，并根据F1分数优化中间点。第一个情况中的单位术语是添加的，因此它将始终优于第二个情况。
- en: Unlabeled Data
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无标签的数据
- en: Unfortunately, most of the times data comes without a label and it would require
    too much human effort to categorize each observation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，大多数情况下数据都没有标签，而且需要太多的人力去对每个观察结果进行分类。
- en: 'We propose two alternatives to the ROC and PR curves that do not require labels:
    the **Mass-Volume** (**MV**) and the **Excess-Mass** (**EM**) curves.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了两种不需要标签的 ROC 和 PR 曲线的替代品：**质量体积**（**MV**）和 **超额质量**（**EM**）曲线。
- en: 'Let ![Unlabeled Data](img/00385.jpeg) be our inverse anomaly scoring function
    this time, where the smaller the score, the higher the probability of it being
    an anomaly. In the case of an auto-encoder, we can use the inverse of the reconstruction
    error:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这次让 ![未标记的数据](img/00385.jpeg) 成为我们的逆异常评分函数，其中分数越小，异常的概率越高。在自动编码器的情况下，我们可以使用重构误差的倒数：
- en: '![Unlabeled Data](img/00394.jpeg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![未标记的数据](img/00394.jpeg)'
- en: Here ϵ is a small term to stabilize in the case of a near zero reconstruction
    error.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ϵ 是一个小项，用于在接近零的重构误差情况下稳定。
- en: The scoring function will give an ordering of each observation.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 评分函数将对每个观察结果进行排序。
- en: Let ![Unlabeled Data](img/00395.jpeg) be the probability density function of
    the normal distribution of a set of i.i.d. observations X[1],…,X[n] and *F* its
    cumulative density function.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![未标记的数据](img/00395.jpeg) 成为一组 i.i.d. 观测值 X[1],…,X[n] 的正态分布的概率密度函数，*F* 是其累积密度函数。
- en: The function *f* would return a score very close to 0 for any observation that
    does not belong to the normal distribution. We want to find a measure of how close
    the scoring function *s* is to *f*. The ideal scoring function would just coincide
    with *f*. We will call such a performance criterion *C(s)*.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f* 对于任何不属于正态分布的观察结果都会返回一个非常接近 0 的分数。我们想找到评分函数 *s* 与 *f* 的接近程度的度量。理想的评分函数将与
    *f* 完全一致。我们将称这样的性能准则为 *C(s)*。
- en: Given a set *S* of scoring functions integrable with respect to the Lebesgue
    measure.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组与勒贝格测度可积的评分函数 *S*。
- en: 'The MV-curve of *s* is the plot of the mapping:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*s* 的 MV-曲线是映射的绘图：'
- en: '![Unlabeled Data](img/00396.jpeg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![未标记的数据](img/00396.jpeg)'
- en: Here ![Unlabeled Data](img/00397.jpeg).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![未标记的数据](img/00397.jpeg)。
- en: The Lebesgue measure of a set *X* is obtained by dividing the set into buckets
    (sequence of open intervals) and summing the n-volume of each bucket. The n-volume
    is the multiplication of the lengths of each dimension defined as the difference
    between max and min values. If *X*[*i*] is a subset of a bunch of d-dimensional
    points, their projection on each axis will give the lengths and the multiplication
    of the lengths will give the d-dimensional volume.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 *X* 的勒贝格测度通过将集合分成桶（开区间序列）并求和每个桶的 n-体积得到。n-体积是每个维度的长度的乘积，定义为最大值和最小值之间的差异。如果
    *X*[i] 是一堆 d 维点的子集，则它们在每个轴上的投影将给出长度，长度的乘积将给出 d 维体积。
- en: The MV measure at *a* corresponds to the n-volume corresponding to the infimum
    subset of *X* defined by the threshold *t* such that the c.d.f. of *s(X)* at *t*
    is higher than or equal to *a*.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*a* 处的 MV 测度对应于 *X* 的由阈值 *t* 定义的下确界子集的 n-体积，使得 *s(X)* 在 *t* 处的 c.d.f. 高于或等于
    *a*。'
- en: '![Unlabeled Data](img/00398.jpeg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![未标记的数据](img/00398.jpeg)'
- en: Volume-Mass curve from "Mass Volume Curves and Anomaly Ranking", S. Clemencon,
    UMR LTCI No. 5141, Telecom ParisTech/CNRS
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 来自“质量体积曲线与异常排名”的体积质量曲线，S. Clemencon，UMR LTCI No. 5141，Telecom ParisTech/CNRS
- en: The optimal MV curve would be the one calculated on *f*. We would like to find
    the scoring function *s* which minimizes the L1 norm of the point-wise difference
    with MVf on an interested interval I*MV* representing the large density level-sets
    (for example, [0.9, 1]).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的 MV 曲线将是在 *f* 上计算的曲线。我们希望找到最小化在感兴趣的区间 I*MV* 上点与 MVf 之间的逐点差异的得分函数 *s*，该区间表示大密度级集合（例如，[0.9,
    1]）。
- en: It is proven that ![Unlabeled Data](img/00399.jpeg). Since *MV* *s* is always
    below *MV* *f*, the ![Unlabeled Data](img/00400.jpeg) will correspond to the ![Unlabeled
    Data](img/00401.jpeg). Our performance criterion for MV is ![Unlabeled Data](img/00402.jpeg).
    The smaller the value of C*MV* the better is the scoring function.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明 ![未标记的数据](img/00399.jpeg)。由于 *MV* *s* 总是在 *MV* *f* 下方，因此 ![未标记的数据](img/00400.jpeg)
    将对应于 ![未标记的数据](img/00401.jpeg)。我们的 MV 的性能准则为 ![未标记的数据](img/00402.jpeg)。C*MV* 的值越小，评分函数的性能越好。
- en: One problem with the MV-curve is that the area under the curve (AUC) diverges
    for *a* = 1 if the support of the distribution is infinite (the set of possible
    values is not bounded).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: MV 曲线的一个问题是，如果分布的支持是无限的（可能值的集合没有界限），则曲线下的面积（AUC）在 *a* = 1 时会发散。
- en: One workaround is to choose the interval ![Unlabeled Data](img/00403.jpeg).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方法是选择区间 ![Unlabeled Data](img/00403.jpeg)。
- en: 'A better variant is the Excess-Mass (EM) curve defined as the plot of the mapping:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的变体是过剩质量（EM）曲线，定义为映射的绘制：
- en: '![Unlabeled Data](img/00404.jpeg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![Unlabeled Data](img/00404.jpeg)'
- en: The performance criterion will be ![Unlabeled Data](img/00405.jpeg) and ![Unlabeled
    Data](img/00406.jpeg), where ![Unlabeled Data](img/00407.jpeg). *EM*[*s*] is now
    always finite.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标将是 ![Unlabeled Data](img/00405.jpeg) 和 ![Unlabeled Data](img/00406.jpeg)，其中
    ![Unlabeled Data](img/00407.jpeg)。*EM*[*s*] 现在总是有限的。
- en: '![Unlabeled Data](img/00408.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![Unlabeled Data](img/00408.jpeg)'
- en: Excess-Mass curve from "On anomaly Ranking and Excess-Mass curves", N. Goix,
    A. Sabourin, S. Clemencon, UMR LTCI No. 5141, Telecom ParisTech/CNRS
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 从《异常排名和过剩质量曲线》中的过剩质量曲线，N. Goix，A. Sabourin，S. Clemencon，UMR LTCI No. 5141，Telecom
    ParisTech/CNRS。
- en: One problem of EM is that the interval of large level sets is of the same order
    of magnitude as the inverse of the total support volume. This is a problem for
    datasets with large dimensions. Moreover, for both EM and MV, the distribution
    *f* of the normal data is not known and must be estimated. For practicality, the
    Lebesgue volume can be estimated via the Monte Carlo approximation, which applies
    only to small dimensions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: EM 的一个问题是，大级别集的区间与总支持体积的倒数数量级相同。对于具有大尺寸的数据集来说，这是一个问题。此外，对于 EM 和 MV，正常数据的分布 *f*
    是未知的，必须进行估计。为了实用性，可以通过蒙特卡洛逼近来估计勒贝格体积，这仅适用于小尺寸。
- en: In order to scale to large-dimensional data, we can sub-sample training and
    validation data with replacement iteratively along a randomly fixed number of
    features *d'* in order to compute the EM or MV performance criterion score. Replacement
    is done only after we have drawn the samples for each subset of features.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应大维数据，我们可以迭代地用替换子集的方式在随机固定数量的特征 *d'* 中进行训练和验证数据的子采样，以计算 EM 或 MV 性能指标分数。仅在我们为每个特征子集绘制样本后才进行替换。
- en: The final performance criterion is obtained by averaging these partial criteria
    along the different features draw. The drawback is that we cannot validate combinations
    of more than *d'* features. On the other hand, this feature sampling allows us
    to estimate EM or MV for large dimensions and allows us to compare models produced
    from space of different dimensions, supposing we want to select over models that
    consume different views of the input data.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的性能指标是通过对不同特征绘制的这些部分指标进行平均得到的。缺点是我们不能验证超过 *d'* 个特征的组合。另一方面，这种特征抽样使我们能够估计大维度下的
    EM 或 MV，并且使我们能够比较从不同维度的数据输入空间产生的模型，假设我们想要在消耗不同视图的模型之间进行选择。
- en: Summary of validation
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证摘要
- en: We have seen how we can plot curve diagrams and compute aggregated measures
    in the case of both labeled and unlabeled data.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何在有标签和无标签数据的情况下绘制曲线图并计算聚合度量。
- en: We have shown how to select sub ranges of the threshold value of the scoring
    function in order to make the aggregated metric more significant for anomaly detections.
    For the PR-curve, we can set the minimum requirements of Precision and Recall;
    for EM or MV we arbitrarily select the interval corresponding to large level-sets
    even if they don't have a directly corresponding meaning.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何选择得分函数的阈值子范围，以使聚合度量在异常检测中更具意义。对于 PR 曲线，我们可以设置精确度和召回率的最小要求；对于 EM 或 MV，即使它们没有直接对应的含义，我们也可以任意选择相应于大级别集的区间。
- en: In our example of network intrusion, we score anomalous points and store them
    into a queue for further human inspection. In that scenario, we need to consider
    also the throughput of the security team. Let's suppose they can inspect only
    50 connections per day; our performance metrics should be computed only on the
    top 50 elements of the queue. Even if the model is able to reach a recall of 100%
    on the first 1,000 elements, those 1,000 elements are not feasible to inspect
    in a real scenario.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络入侵示例中，我们对异常点进行评分并将其存储到队列中供进一步人工检查。在这种情况下，我们还需要考虑安全团队的吞吐量。假设他们每天只能检查 50
    个连接；我们的性能指标应仅计算队列中的前 50 个元素。即使模型能够在前 1,000 个元素上达到 100% 的召回率，这些 1,000 个元素在实际情况下也不可检查。
- en: This situation kind of simplifies the problem because we will automatically
    select the threshold that gives us the expected number of predicted anomalies
    independently of true positive or false positives. This is the best the model
    can do given the top N observations most likely to be anomalies.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况有点简化了问题，因为我们将自动选择给出预期数量的预测异常的阈值，而与真正阳性或假阳性无关。这是模型可以做的最好的，鉴于最有可能是异常的前 N 个观察值。
- en: 'There is also another issue in this kind of threshold-based validation metrics
    in the case of cross-fold validation, that is, the aggregation technique. There
    are two major ways of aggregating: micro and macro.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉折叠验证中，基于阈值的验证指标存在另一个问题，那就是聚合技术。聚合有两种主要方式：微观和宏观。
- en: Macro aggregation is the most common one; we compute thresholds and metrics
    in each validation fold and then we average them. Micro aggregation consists of
    storing the results of each validation fold, concatenating them together and computing
    one single threshold and metric at the end.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观聚合是最常见的一种；我们在每个验证折叠中计算阈值和指标，然后对它们求平均。微观聚合包括存储每个验证折叠的结果，将它们串联在一起，并在最后计算一个单一的阈值和指标。
- en: The macro aggregation technique also gives a measure of stability, and of how
    much the performance of our system changes if we perturb by using different samples.
    On the other hand, macro aggregation introduces more bias into the model estimates,
    especially in rare classes like anomaly detection. Thus, micro aggregation is
    generally preferred.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观聚合技术还提供了稳定性的度量，以及如果我们通过使用不同样本进行扰动时系统性能的变化程度。另一方面，宏观聚合会给模型估计引入更多偏差，特别是在罕见类别（如异常检测）中。因此，一般偏向于微观聚合。
- en: Hyper-parameters tuning
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Following the design of our deep neural network according to the previous sections,
    we would end up with a bunch of parameters to tune. Some of them have default
    or recommended values and do not require expensive fine-tuning. Others strongly
    depends on the underlying data, specific application domain, and a set of other
    components. Thus, the only way to find best values is to perform a model selection
    by validating based on the desired metric computed on the validation data fold.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面章节的深度神经网络设计，我们将得到一堆需要调整的参数。其中一些具有默认值或推荐值，并且不需要昂贵的微调。其他参数则严重依赖于底层数据、特定应用领域和一系列其他组件。因此，找到最佳值的唯一方法是执行模型选择，根据在验证数据折叠上计算的所需指标进行验证。
- en: 'Now we will list a table of parameters that we might want to consider tuning.
    Please consider that each library or framework may have additional parameters
    and a custom way of setting them. This table is derived from the available tuning
    options in H2O. It summarizes the common parameters, but not all of them, when
    building a deep auto-encoder network in production:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将列出一个表格，其中包含我们可能想要考虑调整的参数。请注意，每个库或框架可能有额外的参数和自定义设置方式。此表格源自于 H2O 中可用的调整选项。它总结了在生产中构建深度自动编码器网络时的常见参数，但不是全部：
- en: '| Parameter | Description | Recommended value(s) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 推荐值 |'
- en: '| --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `activation` | The differentiable activation function. | Depends on the data
    nature. Popular functions are: `Sigmoid`, `Tanh`, `Rectifier` and `Maxout`.Each
    function can then be mapped into the corresponding drop-out version. Refer to
    the network design section. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| `activation` | 可微激活函数。 | 取决于数据的特性。流行函数包括：`Sigmoid`、`Tanh`、`Rectifier` 和 `Maxout`。每个函数都可以映射到相应的丢弃版本。请参考网络设计部分。'
- en: '| hidden | Size and number of layers. | Number of layers is always odd and
    symmetric between encoding and decoding when the network is an autoencoder.The
    size depends on both the network design and the regularization technique.Without
    regularization the encoding layers should be consecutively smaller than the previous
    layer.With regularization we can have higher capacity than the input size. |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| hidden | 尺寸和层数。 | 当网络是自编码器时，层数始终是奇数，并且在编码和解码之间对称。尺寸取决于网络设计和正则化技术。没有正则化时，编码层应连续小于前一层。有了正则化，我们可以拥有比输入尺寸更高的容量。'
- en: '| epochs | Number of iterations over the training set. | Generally, between
    10 and a few hundreds. Depending on the algorithm, it may require extra epochs
    to converge.If using early stopping we don''t need to worry about having too many
    epochs.For model selection using grid search, it is better to keep it small enough
    (less than 100). |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| epochs | 对训练集进行的迭代次数。 | 一般来说，介于10和几百之间。根据算法的不同，可能需要额外的迭代来收敛。如果使用了早停法，就不需要担心迭代次数太多。对于使用网格搜索进行模型选择，最好将其保持足够小（小于100）。'
- en: '| `train_samples_per_iteration` | Number of training examples for Map/Reduce
    iteration. | This parameter applies only in the case of distributed learning.This
    strongly depends on the implementation.H2O offers an auto-tuning option.Please
    refer to the *Distributed learning via Map/Reduce* section. |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| `train_samples_per_iteration` | Map/Reduce迭代中的训练样例数。 | 此参数仅适用于分布式学习的情况。这在很大程度上取决于实现方式。H2O提供了自动调优选项。请参考*Distributed
    learning via Map/Reduce*部分。'
- en: '| `adaptive_rate` | Enable the adaptive learning rate. | Each library may have
    different strategies. H2O implements as default `ADADELTA`.In case of `ADADELTA`,
    additional parameters rho (between 0.9 and 0.999) and epsilon (between 1e-10 and
    1e-4) must be specified.Please refer to the Adaptive Learning section. |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| `adaptive_rate` | 启用自适应学习率。 | 每个库可能有不同的策略。H2O的默认实现是`ADADELTA`。对于`ADADELTA`，还必须指定额外的参数rho（介于0.9和0.999之间）和epsilon（介于1e-10和1e-4之间）。请参考自适应学习部分。'
- en: '| `rate`, `rate_decay` | Learning rate values and decay factor (if not adaptive
    learning). | High values of the rate may lead to unstable models, lower values
    will slow down the convergence. A reasonable value is 0.005.The decay factory
    represents the Rate at which the learning rate decays across layers. |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| `rate`，`rate_decay` | 学习率的值和衰减系数（如果不是自适应学习）。 | 较高的学习率可能导致不稳定的模型，较低的值会减缓收敛速度。一个合理的值是0.005。衰减系数表示学习率在各个层级上衰减的速率。'
- en: '| `momentum_start`, `momentum_ramp`, `momentum_stable` | Parameters of the
    momentum technique (if not adaptive learning). | When exists a gap between the
    momentum start and the stable value, the momentum ramp is measured in number of
    training samples. The default is typically a large value, for example, 1e6. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| `momentum_start`，`momentum_ramp`，`momentum_stable` | 动量技术的参数（如果不是自适应学习）。
    | 当动量开始和稳定值之间存在间隔时，动量斜坡是以训练样例数量为单位衡量的。默认值通常较大，例如1e6。'
- en: '| `Input_dropout_ratio`, `hidden_dropout_ratio` | Fraction of input nodes for
    each layer to omit during training. | Default values are 0 for input (all features)
    and a value around 0.5 for hidden layers. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| `Input_dropout_ratio`，`hidden_dropout_ratio` | 每个层级中要在训练过程中省略的输入节点的比例。 |
    输入（所有特征）的默认值为0，隐藏层的值约为0.5。'
- en: '| `l1`, `l2` | L1 and L2 regularization parameters. | High values of L1 will
    cause many weights to go to 0 while high values of L2 will reduce but keep most
    of the weights. |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| `l1`，`l2` | L1和L2正则化参数。 | 较大的L1值会导致许多权重变为0，较大的L2值会减小但保留大部分权重。'
- en: '| `max_w2` | Maximum value of sum of squared weights incoming for a node. |
    A useful parameter for unbounded activation functions such as ReLU or Maxout.
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| `max_w2` | 一个节点上所有权重的平方和的最大值。 | 对于无界激活函数（如ReLU或Maxout）很有用的一个参数。'
- en: '| `initial_weight_distribution` | The distribution of initial weights. | Typical
    values are Uniform, Normal, or UniformAdaptive. The latter is generally preferred.
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| `initial_weight_distribution` | 初始权重的分布。 | 典型的值有均匀分布（Uniform）、正态分布（Normal）或自适应均匀分布（UniformAdaptive）。通常更倾向于后者。'
- en: '| `loss` | The loss function to use during back-propagation. | It depends on
    the problem and nature of data.Typical functions are CrossEntropy, Quadratic,
    Absolute, Huber. Please refer to the Network design section. |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| `loss` | 后向传播过程中要使用的损失函数。 | 这取决于问题和数据的性质。常见的函数有交叉熵（CrossEntropy）、平方差（Quadratic）、绝对值（Absolute）、Huber。请参考网络设计部分。'
- en: '| `rho_sparsity`, `beta_sparsity` | Parameters of the sparse auto-encoders.
    | Rho is the average activation frequency and beta is the weight associated to
    the sparsity penalty. |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| `rho_sparsity`，`beta_sparsity` | 稀疏自动编码器的参数。 | Rho是平均激活频率，beta是与稀疏惩罚相关的权重。'
- en: These parameters can be tuned using search space optimization techniques. Two
    of the most basic, popular and supported by H2O techniques, are grid search and
    random search.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数可以使用搜索空间优化技术来进行调优。H2O支持的两个基本和流行的技术是网格搜索和随机搜索。
- en: Grid search is an exhaustive approach. Each dimension specifies a limited number
    of possible values and the Cartesian product generates the search space. Each
    point will be evaluated in a parallel fashion and the point that scores the lowest
    will be selected. The scoring function is defined by the validation metric.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一种穷举的方法。每个维度指定了一系列可能的值，笛卡尔积生成了搜索空间。每个点将以并行方式进行评估，并选择得分最低的点。评分函数由验证指标定义。
- en: On the one hand, we have a computational cost equals to the power of the dimensionality
    (the curse of dimensionality). On the other hand, it is embarrassingly parallel.
    That is, each point is perfectly parallelizable and its run is independent from
    the others.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们的计算成本与维度的幂等于(维度的诅咒）。另一方面，它是尴尬地并行的。也就是说，每个点都是完全可以并行化的，它的运行与其他点是独立的。
- en: Alternatively, randomly choosing points in a dense search space could be more
    efficient and can lead to similar results with much less computation. The number
    of wasted grid search trials is exponential in the number of search dimensions
    that turned out to be irrelevant for a particular dataset. Not every parameter
    has the same importance during tuning. Random search is not affected by those
    low-importance dimensions.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在密集搜索空间中随机选择点可能更有效，并且可以在需要更少的计算的情况下产生类似的结果。在一个特定数据集中，浪费的网格搜索尝试的数量与被证明对于某一个数据集是无关紧要的搜索维度的数量是指数级的。不是每个参数在调整过程中都具有相同的重要性。随机搜索不受这些低重要性维度的影响。
- en: In random search, each parameter must provide a distribution, continuous or
    discrete depending on the values of the parameter. The trials are points sampled
    independently from those distributions.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索中，每个参数必须提供一个分布，取决于参数的值是连续的还是离散的。试验点是从这些分布中独立抽样的点。
- en: 'The main advantages of random search are:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索的主要优势包括：
- en: You can fix the budget (maximum number of points to explore or maximum allowed
    time)
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以固定预算（最大探索点数或最大允许时间）。
- en: You can set a convergence criterion
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以设置收敛标准。
- en: Adding parameters that do not influence the validation performance does not
    affect the efficiency
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加不影响验证性能的参数不影响效率。
- en: During tuning, you could add extra parameters dynamically without have to adjust
    the grid and increase the number of trials
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调整过程中，您可以动态地添加额外的参数，而无需调整网格并增加尝试次数。
- en: If one trial run fails for any reason, it could either be abandoned or restarted
    without jeopardizing the entire tuning algorithm
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某次试验运行失败，由于任何原因，可以放弃或重新启动，而不会危及整个调整算法。
- en: Common applications of random search are associated with early stopping. Especially
    in high-dimensional spaces with many different models, the number of trials before
    to converge to a global optimum can be a lot. Early stopping will stop the search
    when the learning curve (training) or the validation curve (tuning) flattens out.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索的常见应用与早期停止有关。特别是在高维空间中有许多不同模型的情况下，收敛到全局最优解之前的尝试次数可能会很多。当学习曲线（训练）或验证曲线（调整）趋于平缓时，早期停止将停止搜索。
- en: 'Because we can also constrain the computation budget we could set criteria
    like: *stop when RMSE has improved over the moving average of the best 5 models
    by less than 0.0001, but take no more than 1 hours*.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们也可以限制计算预算，所以我们可以设置诸如：*当 RMSE 比最佳 5 个模型的移动平均改善少于 0.0001 时停止，但最多不超过 1 小时*
    的标准。
- en: Metric-based early stopping combined with max runtime generally gives the best
    tradeoff.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的早期停止结合最大运行时一般给出最佳的权衡。
- en: It also common to have multi-stage tuning where, for example, you run a random
    search to identify the sub-space where the best configuration might exist and
    then have further tuning stages only in the selected subspace.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 通常也会有多阶段的调整，例如，您可以运行随机搜索来识别可能存在最佳配置的子空间，然后仅在所选子空间中进行进一步的调整阶段。
- en: More advanced techniques also exploit sequential, adaptive search/optimization
    algorithms, where the result of one trial affects the choice of next trials and/or
    the hyper-parameters are optimized jointly. There is ongoing research trying to
    predetermine the *variable importance* of hyper-parameters. Also, domain knowledge
    and manual fine-tuning can be valuable for those systems where automated techniques
    struggle to converge.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的技术还利用了顺序，自适应的搜索/优化算法，其中一个试验的结果影响下一个试验的选择和/或超参数是联合优化的。目前正在进行研究，试图预先确定超参数的*变量重要性*。此外，领域知识和手动微调对于那些自动技术难以收敛的系统可能是有价值的。
- en: End-to-end evaluation
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**端到端评估**'
- en: From a business point of view what really matters is the final end-to-end performance.
    None of your stakeholders will be interested in your training error, parameters
    tuning, model selection, and so on. What matters is the KPIs to compute on top
    of the final model. Evaluation can be seen as the ultimate verdict.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 从商业角度来看，真正重要的是最终的端到端性能。你的利益相关者都不会对你的训练误差、参数调整、模型选择等感兴趣。重要的是基于最终模型计算的关键绩效指标。评估可以被看作是最终的裁决。
- en: Also, as we anticipated, evaluating a product cannot be done with a single metric.
    Generally, it is a good and effective practice to build an internal dashboard
    that can report, or measure in real-time, a bunch of performance indicators of
    our product in the form of aggregated numbers or easy-to-interpret visualization
    charts. Within a single glance, we would like to understand the whole picture
    and translate it in the value we are generating within the business.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们预期的那样，评估产品不能仅仅依靠单一指标。通常，构建一个内部仪表板是一个好的有效的做法，它可以以汇总数字或易于解释的可视化图表的形式实时报告或测量我们产品的一系列绩效指标。通过一瞥，我们希望理解整个图片并将其转化为我们在业务中产生的价值。
- en: The evaluation phase can, and generally does, include the same methodology as
    the model validation. We have seen in previous sections a few techniques for validating
    in case of labeled and unlabeled data. Those can be the starting points.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 评估阶段通常包括与模型验证相同的方法。我们在前面的章节中看到了一些在有标签和无标签数据情况下验证的技术。这些可以作为起点。
- en: 'In addition to those, we ought to include a few specific test scenarios. For
    instance:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那些，我们还应该包括一些具体的测试场景。例如：
- en: '**Known versus unknown detection performance**: This means measuring the performance
    of the detector for both known and unknown attacks. We can use the labels to create
    different training sets, some of them with no attacks at all and some of them
    with a small percentage; remember that having too many anomalies in the training
    set would be against the definition of anomalies. We could measure the precision
    on the top N elements in the function of the percentage of anomalies in the training
    set. This will give us an indicator of how general the detector is with respect
    to past anomalies and hypothetical novel ones. Depending on what we are trying
    to build, we might be interested more on novel anomalies or more on known ones.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已知与未知检测性能**：这意味着衡量检测器对已知和未知攻击的性能。我们可以使用标签创建不同的训练集，其中一些根本没有攻击，而另一些则有小部分攻击；请记住，在训练集中有太多异常将违反异常的定义。我们可以根据训练集中异常百分比的函数来测量前N个元素的精度。这将为我们提供检测器相对于过去异常和假设的新异常的一般性的指示。取决于我们试图构建的内容，我们可能更感兴趣于新异常还是已知异常。'
- en: '**Relevance performance**: Just scoring enough to hit the threshold or being
    select in the top priority queue is important but the ranking also matters. We
    would like the most relevant anomalies to always score at the top of the queue.
    Here we could either define the priorities of the different labels and compute
    a ranking coefficient (for example, Spearman) or use some evaluation technique
    used for Recommender Systems. One example of the latter is mean average precision
    at k (MAP@k) used in Information Retrieval to score a query engine with regards
    to the relevance of the returned documents.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性能**：只有得分达到阈值或者在优先级队列中被选择是重要的，但排名也很重要。我们希望最相关的异常总是排在队列的前面。在这里，我们可以定义不同标签的优先级，并计算排名系数（例如，Spearman系数），或者使用一些用于推荐系统的评估技术。后者的一个例子是信息检索中使用的k个均值平均精度（MAP@k），用于评分查询引擎返回文档的相关性。'
- en: '**Model stability**: We select the best model during validation. If we sample
    the training data differently or use slightly different validation dataset (containing
    different types of anomalies) we would like the best model to always be the same
    or at least among the top selected models. We can create histogram charts showing
    the frequency of a given model of being selected. If there is no obvious winner
    or a subset of frequent candidates, then the model selection is a bit unstable.
    Every day, we might select a different model that is good for reacting to new
    attacks but at the price of instability.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型稳定性**：我们在验证过程中选择最佳模型。如果我们以不同的方式抽样训练数据，或者使用略有不同的验证数据集（包含不同类型的异常），我们希望最佳模型始终保持相同，或者至少是在顶部选出的模型之一。我们可以创建直方图，显示给定模型被选择的频率。如果没有明显的获胜者或一组频繁候选模型，那么模型选择就有些不稳定。每天，我们可能会选择一个不同的模型，该模型可以很好地对新攻击做出反应，但代价是稳定性不好。'
- en: '**Attack outcome**: If the model detects an attack with a very high score and
    this attack is confirmed by the analysts, is the model also able to detect whether
    the system has been compromised or returned to normalcy? One way of testing this
    is to measure the distribution of the anomaly score right after an alert is raised.
    Comparing the new distribution with the older one and measuring any gap. A good
    anomaly detector should be able to tell you about the state of the system. The
    evaluation dashboard could have this information visualized for the last or recently
    detected anomalies.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**攻击结果**：如果模型检测到一次得分非常高的攻击，并且分析师们确认了这次攻击，那么模型是否能够检测出系统是否已被入侵或恢复正常？一种测试方法是在发出警报后测量异常得分的分布。将新的分布与旧的分布进行比较，并测量其中的差距。一个好的异常检测器应该能够告诉你系统的状态。评估仪表板可以将最近检测到的异常可视化显示出来。'
- en: '**Failure case simulations**: Security analysts can define some scenarios and
    generate some synthetic data. One business target could be "being able to protect
    from those future types of attacks". Dedicated performance indicators can be derived
    from this artificial dataset. For example, an increasing ramp of network connections
    to the same host and port could be a sign of **Denial of Service** (**DOS**) attack.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障案例模拟**：安全分析师可以定义一些场景并生成一些合成数据。其中一个业务目标可以是“能够保护免受未来这些类型的攻击”。可以从这个人工数据集中提取专用的性能指标。例如，对同一主机和端口的网络连接进行递增的斜坡可能是**拒绝服务**
    (**DOS**) 攻击的迹象。'
- en: '**Time to detect**: The detector generally scores each point independently.
    For contextual and time-based anomalies, the same entities might generate many
    points. For example, if we open a new network connection, we can start scoring
    it against the detector while it is still open and every few seconds generate
    a new point with the features collected over a different time interval. Likely,
    you will collect multiple sequential connections together into a single point
    to score. We would like to measure how long it takes to react. If the first connection
    is not considered anomalous, maybe after 10 consecutive attempts, the detector
    will react. We can pick a known anomaly, break it down into sequentially growing
    data points, and then report after how many of those the contextual anomaly is
    raised.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测时间**：检测器通常独立地对每个数据点进行评分。对于上下文和基于时间的异常，同一实体可能会生成许多数据点。例如，如果我们打开一个新的网络连接，我们可以在连接仍然打开时对其进行评分，并且每隔几秒生成一个特征收集在不同时间间隔内的新数据点。通常，您会将多个连续的连接整合到一个数据点中进行评分。我们希望能够测量反应所需的时间。如果第一个连接不被视为异常，也许在连续尝试了10次之后，检测器将会有反应。我们可以将已知的异常拆分成连续增长的数据点，然后报告在经过多少个数据点后发现了上下文异常。'
- en: '**Damage cost**: If somehow we are able to quantify the impact of attack damages
    or savings due to the detection, we should incorporate this in the final evaluation.
    We could use as benchmark the last past month or year and estimate the savings;
    hopefully this balance will be positive, in case we have deployed the current
    solution since then or the real savings if the current solution was deployed in
    this last period.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损害成本**：如果我们能够以某种方式量化攻击造成的损害或由于检测而产生的节省，我们应该将其纳入最终评估中。我们可以以过去的一个月或一年作为基准，并估计节省的金额；希望这个平衡是正向的，如果我们自那时起部署了当前解决方案，或者如果当前解决方案是在最近这段时间部署的，这样可以获得真正的节省。'
- en: 'We would like to summarize all of this information within a single dashboard
    from where we can make statements such as: *Our anomaly detector is able to detect
    previously seen anomalies with a precision of 76% (+- 5%) and average reacting
    time of 10 seconds and novel anomalies with precision of 68% (+- 15%) and reaction
    time of 14 seconds. We observed an average of 10 anomalies per day. Considering
    the capability of 1,000 inspections per day, we can fill the 80% of the most relevant
    detections corresponding to 6 anomalies within just 120 top elements of the queue.
    Of these, only the 2 out of 10 that compromise the system are included in this
    list. We can then divide the inspections in 2 tiers; the first tier will respond
    immediately of the top 120 elements and the second tier will take care of the
    tail. Standing to the current simulated failing scenarios, we are protected in
    90% of them. Total saving since last year corresponds to 1.2 million dollars*.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够在单个仪表板中总结所有这些信息，以便我们可以发表如下的声明：*我们的异常检测器能够以76%（+- 5%）的精度和平均反应时间为10秒来检测先前发现的异常，以及以68%（+-
    15%）的精度和14秒的反应时间来检测新异常。我们每天观察到平均10个异常。考虑到每天可以进行1,000次检查的能力，我们可以在队列的前120个元素中填充80%的最相关检测，对应于仅将6个异常纳入其中。这些中仅有2个是危及系统的。然后我们可以将检查分为两个级别；第一级将立即响应前120个元素，第二级将处理剩下的。按照当前模拟的故障场景，我们在其中受到了90%的保护。自去年以来的总节省额相当于120万美元*。
- en: A/B Testing
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B测试
- en: So far, we have only considered evaluation based on past historical data (retrospective
    analysis) and/or based on simulations with synthetic dataset. The second one is
    based on the assumption of a particular failure scenario to happen in the future.
    Evaluating only based on historical data assumes that the system will always behave
    under those conditions and that the current data distribution also describes the
    stream of future data. Moreover, any KPI or performance metric should be evaluated
    relative to a baseline. The product owner wants to justify the investment for
    that project. What if the same problem could have been solved in a much cheaper
    way?
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑过基于过去历史数据（事后分析）和/或基于合成数据集模拟的评估。第二种方法是基于假设未来会发生特定故障场景的。仅基于历史数据进行评估假定了系统将始终在这些条件下运行，并且当前的数据分布也描述了未来数据流。此外，任何关键绩效指标都应相对于基线进行评估。产品负责人希望为该项目的投资提供理由。如果相同的问题可以以更便宜的方式解决呢？
- en: For this reason, the only truth for evaluating any machine learning system is
    A/B testing. A/B testing is a statistical hypothesis testing with two variants
    (the control and variation) in a controlled experiment. The goal of A/B testing
    is to identify performance differences between the two groups. It is a technique
    widely used in user experience design for websites or for advertising and/or marketing
    campaigns. In the case of anomaly detection, we can use a baseline (the simplest
    rule-based detector) as the control version and the currently selected model as
    variation candidate.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，评估任何机器学习系统的唯一方法是A/B测试。A/B测试是一种统计假设检验，有两种变体（控制组和变体组）的受控实验。A/B测试的目标是确定两组之间的性能差异。在网站用户体验设计或广告/营销活动中广泛使用这种技术。在异常检测的情况下，我们可以将基线（最简单的基于规则的检测器）作为控制版本，当前选择的模型作为变体候选者。
- en: The next step is to find a meaningful evaluation that quantifies the return
    of investment.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是找到一个有意义的评估，量化投资回报。
- en: '*"We have to find a way of making the important measurable, instead of making
    the measurable important."*'
  id: totrans-441
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"我们必须找到一种让重要的事物可度量，而不是让可度量的事物变得重要的方法。"*'
- en: ''
  id: totrans-442
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Robert McNamara, former US Secretary of Defense*'
  id: totrans-443
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*罗伯特·麦克纳马拉，前美国国防部部长*'
- en: 'The return of investment will be represented by the uplift defined as:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 投资回报将由提升定义为：
- en: '![A/B Testing](img/00409.jpeg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![A/B测试](img/00409.jpeg)'
- en: It is the difference between the two KPIs that quantifies the effectiveness
    of the treatment.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 两个KPI之间的差异量化了治疗效果。
- en: In order to make the comparison fair we must ensure that the two groups share
    the same distribution of the population. We want to remove any bias given by the
    choice of individuals (data samples). In the case of the anomaly detector, we
    could, in principle, apply the same stream of data to both the two models. This
    is not recommended though. By applying one model you can influence the behavior
    of a given process. A typical example is an intruder who is first detected by
    a model, and as such, the system would react by dropping his open connections.
    A smart intruder would realize that he has been discovered and would not attempt
    to connect again. In that case, the second model may never observe a given expected
    pattern because of the influence of the first model.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，我们必须确保这两组共享相同的人口分布。我们希望消除数据样本选择所带来的任何偏见。在异常检测器的情况下，我们原则上可以将相同的数据流应用到这两个模型。虽然这样做并不推荐。通过应用一个模型，您可以影响给定过程的行为。一个典型的例子是一个入侵者首先被模型检测到，因此系统会通过中断他的开放连接来做出反应。一个聪明的入侵者会意识到他已被发现，不会再尝试连接。在这种情况下，由于第一个模型的影响，第二个模型可能永远不会观察到某个预期模式。
- en: By separating the two models over two disjoint subsets of data, we make sure
    the two models cannot influence each other. Moreover, if our use case requires
    the anomalies to be further investigated by our analysts, then they cannot be
    duplicated.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将两个模型分离到数据的两个不相交子集中，我们确保这两个模型不能相互影响。此外，如果我们的用例要求分析师进一步调查异常，那么它们不能被复制。
- en: 'Here, we must split according to the same criteria as we have seen in the data
    validation: no data leakage and entity sub-sampling. The final test that can confirm
    whether the two groups are actually identically distributed is A/A testing.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须根据与数据验证中相同的标准进行分割：没有数据泄露和实体子采样。能够确认这两组实际上是相同分布的最终测试是A/A测试。
- en: As the name suggests, A/A testing consists on re-using the control version on
    both the two groups. We expect that the performance should be very similar equivalent
    to an uplift close to 0\. It is also an indicator of the performance variance.
    If the A/A uplift is non-zero, then we have to redesign the controlled experiment
    to be more stable.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，A/A测试就是在两组上重新使用控制版。我们期望性能应该非常相似，相当于接近0的增益。这也是性能方差的指标。如果A/A增益不为零，则我们必须重新设计受控实验，使其更加稳定。
- en: A/B testing is great for measuring the difference in performance between the
    two models but just the model is not the only factor that influence the final
    performance. If we take into account the damage cost model, which is the business
    core, the model must be accurate on generating a prioritized list of anomalies
    to investigate but also the analysts must be good on identifying, confirming and
    reacting upon.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: A/B测试非常适合衡量两个模型之间性能的差异，但模型不是唯一影响最终性能的因素。如果我们考虑损耗成本模型，即业务核心，模型必须准确地生成一个优先级列表，以便调查异常，同时分析师必须擅长识别、确认和采取行动。
- en: 'Hence, we have two factors: the model accuracy and the security team effectiveness.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个因素：模型准确性和安全团队的有效性。
- en: 'We can divide the controlled experiment into an A/B/C/D test where four independent
    groups are created, as follows:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将受控实验分成A/B/C/D测试，创建四个独立的组，如下所示：
- en: '|   | Base model | Advanced model |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|   | 基础模型 | 先进模型 |'
- en: '| --- | --- | --- |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **No action from security team** | Group A | Group B |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| **来自安全团队的无操作** | A组 | B组 |'
- en: '| **Intervention from security team** | Group C | Group D |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| **来自安全团队的干预** | C组 | D组 |'
- en: 'We can compute a bunch of uplift measures that quantify both the model accuracy
    and security team effectiveness. In particular:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算一系列增益度量，量化模型准确性和安全团队的有效性。特别是：
- en: '`uplift(A,B)`: The effectiveness of the advanced model alone'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uplift(A,B)`: 先进模型单独的有效性'
- en: '`uplift(D,C)`: The effectiveness of the advanced model in case of security
    intervention'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uplift(D,C)`: 发生安全干预时先进模型的有效性'
- en: '`uplift(D,A)`: The effectiveness of both advanced model and security intervention
    joint together'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uplift(D,A)`: 先进模型和安全干预一起的有效性'
- en: '`uplift(C,A)`: The effectiveness of the security intervention on the low-accuracy
    queue'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uplift(C,A)`: 低准确性队列上的安全干预的有效性'
- en: '`uplift(D,B)`: The effectiveness of the security intervention on the high-accuracy
    queue'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uplift(D,B)`: 安全干预对高准确性队列的有效性'
- en: This is just an example of meaningful experiment and evaluations you want to
    carry out in order to quantify in numbers what the business really cares about.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个有意义的实验和评估的示例，您想进行这些评估以便以数字形式量化业务真正关心的内容。
- en: Furthermore, there are a bunch of advanced techniques for A/B testing. Just
    to name a popular one, the multi-armed bandit algorithm allows you to dynamically
    adjust the size of the different testing groups in order to adapt to the performance
    of those and minimize the loss due to low performing groups.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一堆用于A/B测试的高级技术。只是举一个例子，多臂老虎机算法允许您动态调整不同测试组的大小，以适应它们的性能并最小化由于性能低的组造成的损失。
- en: A summary of testing
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试摘要
- en: 'To summarize, for an anomaly detection system using neural networks and labeled
    data, we can define the following:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，对于使用神经网络和标记数据的异常检测系统，我们可以定义以下内容：
- en: Model as the definition of the network topology (number and size of hidden layers),
    activation functions, pre-processing and post-processing transformations.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型作为网络拓扑的定义（隐藏层的数量和大小），激活函数，预处理和后处理转换。
- en: Model parameters as the weights of hidden units and biases of hidden layers.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数作为隐藏单元的权重和隐藏层的偏置。
- en: Fitted model as the model with an estimated value of parameters and able to
    map samples from the input layer to the output.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合模型作为具有参数估计值的模型，能够将样本从输入层映射到输出层。
- en: Learning algorithm (also training algorithm) as SGD or its variants (HOGWILD!,
    adaptive learning) + the loss function + regularization.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习算法（也称为训练算法）作为SGD或其变体（HOGWILD！，自适应学习）+损失函数+正则化。
- en: Training set, validation set and test set are three disjoint and possibly independent
    subsets of the available data where we preserve the same distribution.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集、验证集和测试集是可用数据的三个不相交且可能独立的子集，其中我们保留相同的分布。
- en: Model validation as the maximum F-measure score from the ROC curve computed
    on the validation set using model fitted on the training set.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证作为在训练集上拟合的模型计算的ROC曲线上的最大F-度量分数。
- en: Model selection as the best validated model among a set of possible configurations
    (1 hidden layer Vs. 3 hidden layers, 50 neurons Vs. 1000 neurons, Tanh Vs. Sigmoid,
    Z-scaling Vs. Min/Max normalization and so on…).
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择作为一组可能配置中的最佳验证模型（1隐藏层Vs. 3隐藏层，50个神经元Vs. 1000个神经元，Tanh Vs. Sigmoid，Z-scaling
    Vs. Min/Max归一化等等...）。
- en: Hyper-parameters tuning as the extension of model selection with algorithm and
    implementation parameters such as learning parameters (epochs, batch size, learning
    rate, decay factor, momentum…), distributed implementation parameters (samples
    per iteration), regularization parameters (lambda in L1 and L2, noise factor,
    sparsity constraint…), initialization parameters (weights distribution) and so
    on.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整作为模型选择的延伸，使用算法和实现参数，如学习参数（epochs，批量大小，学习速率，衰减因子，动量...），分布式实现参数（每次迭代的样本），正则化参数（L1和L2中的lambda，噪声因子，稀疏性约束...），初始化参数（权重分布）等等。
- en: Model evaluation, or testing, as the final business metrics and acceptance criteria
    computed on the test set using model fitted on both training and validation set
    merged together. Some examples are the precision and recall for just top N test
    samples, time to detection, and so on.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估，或测试，作为在测试集上计算的最终业务指标和验收标准，使用在训练集和验证集上拟合的模型合并在一起。一些示例是仅针对前N个测试样本的精确度和召回率，检测时间等等。
- en: A/B testing as the uplift of evaluation performances of a model with respect
    to a baseline computed on two different, but homogeneous, subsets of the live
    data population (the control and variation groups).
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A/B测试作为模型与基线之间的评估性能提升，基线是根据现场数据人群的两个不同但同质的子集计算的（对照组和变化组）。
- en: We hope that we've clarified the essential and most important steps to consider
    when testing a production-ready deep learning intrusion detection system. These
    techniques, metrics, or tuning parameters may not be the same for your use case,
    but we hope that the thoughtful methodology can serve as a guideline for any data
    product.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们已经澄清了在测试生产就绪的深度学习入侵检测系统时需要考虑的基本和最重要的步骤。这些技术、指标或调整参数可能对您的用例不同，但我们希望深思熟虑的方法论可以作为任何数据产品的指南。
- en: 'A great resource of guidelines and best practices for building Data Science
    systems that are both scientifically correct and valuable for the business is
    the Professional Data Science Manifesto: [www.datasciencemanifesto.org](http://www.datasciencemanifesto.org).
    It is recommended the reading and reasoning around the listed principles.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于构建既科学正确又对业务有价值的数据科学系统的指导方针和最佳实践的重要资源是专业数据科学宣言：[www.datasciencemanifesto.org](http://www.datasciencemanifesto.org)。推荐阅读并围绕列出的原则进行思考。
- en: Deployment
  id: totrans-480
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: At this stage, we should have done almost all of the analysis and development
    needed for building an anomaly detector, or in general a data product using deep
    learning.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们应该已经完成了几乎所有构建异常检测器或通用深度学习数据产品所需的分析和开发工作。
- en: 'We are only left with final, but not less important, step: the deployment.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只剩下最后但同样重要的一步：部署。
- en: Deployment is generally very specific of the use case and enterprise infrastructure.
    In this section, we will cover some common approaches used in general data science
    production systems.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 部署通常非常特定于用例和企业基础架构。在本节中，我们将介绍一些在通用数据科学生产系统中使用的常见方法。
- en: POJO model export
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: POJO 模型导出
- en: In the Testing section, we summarized all the different entities in a machine
    learning pipeline. In particular, we have seen the definition and differences
    of a model, a fitted model and the learning algorithm. After we have trained,
    validated, and selected the final model, we have a final fitted version of it
    ready to be used. During the testing phase (except in A/B testing), we have scored
    only historical data that was generally already available in the machines where
    we trained the model.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试部分，我们总结了机器学习管道中的所有不同实体。特别是，我们已经看到了模型、适配模型和学习算法的定义和区别。在我们训练、验证和选择了最终模型之后，我们得到了一个准备好使用的最终适配版本。在测试阶段（除了
    A/B 测试），我们只对通常已经可用于训练模型的历史数据进行了评分。
- en: In enterprise architectures, it is common to have a Data Science cluster wherein
    you build a model and the production environment where you deploy and use the
    fitted model.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业架构中，常见的是有一个数据科学集群，您在其中构建一个模型，以及用于部署和使用适配模型的生产环境。
- en: One common way is to export a fitted model is **Plain Old Java Object** (**POJO**).
    The main advantage of POJO is that it can be easily integrated within a Java app
    and scheduled to run on a specific dataset or deployed to score in real-time.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的导出适配模型的方法是**纯旧的 Java 对象**（**POJO**）。POJO 的主要优点是它可以很容易地集成到 Java 应用程序中，并安排在特定数据集上运行或部署以实时进行评分。
- en: H2O allows you to extract a fitted model programmatically or from the Flow Web
    UI, which we have not covered in this book.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 允许您通过编程方式或从 Flow Web UI 中提取适配模型，这在本书中没有涵盖。
- en: 'If `model` is your fitted model, you can save it as `POJO jar` in the specified
    `path` by running:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `model` 是您的适配模型，您可以通过运行以下命令将其保存为指定路径中的 `POJO jar`：
- en: '[PRE0]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The POJO jar contains a standalone Java class of the base class `hex.genmodel.easy.EasyPredictModelWrapper`,
    with no dependencies on the training data or the entire H2O framework but only
    the `h2o-genmodel.jar` file, which defines the POJO interfaces. It can be read
    and used from anything that runs in a JVM.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: POJO jar 包含了一个独立的 Java 类 `hex.genmodel.easy.EasyPredictModelWrapper`，不依赖于训练数据或整个
    H2O 框架，而只依赖于 `h2o-genmodel.jar` 文件，该文件定义了 POJO 接口。它可以从任何在 JVM 中运行的东西中读取和使用。
- en: The POJO object will contain the model class name corresponding to the model
    id used in H2O (`model.id`) and the model category for anomaly detection will
    be `hex.ModelCategory.AutoEncoder`.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: POJO 对象将包含与在 H2O 中使用的模型 id（`model.id`）对应的模型类名称，以及用于异常检测的模型类别将是 `hex.ModelCategory.AutoEncoder`。
- en: 'Unfortunately, at the time of writing this chapter, there is still an open
    issue over implementing the Easy API for AutoEncoder: [https://0xdata.atlassian.net/browse/PUBDEV-2232](https://0xdata.atlassian.net/browse/PUBDEV-2232).'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在撰写本章时，关于实现 AutoEncoder 的 Easy API 仍然存在一个未解决的问题：[https://0xdata.atlassian.net/browse/PUBDEV-2232](https://0xdata.atlassian.net/browse/PUBDEV-2232)。
- en: 'Roberto Rösler, from the h2ostream mailing list, solved this problem by implementing
    its own version of the `AutoEncoderModelPrediction` class as:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 h2ostream 邮件列表的 Roberto Rösler 通过实现自己的 `AutoEncoderModelPrediction` 类解决了这个问题，如下所示：
- en: '[PRE1]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And modified the method `predictAutoEncoder` in the `EasyPredictModelWrapper`
    as:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 并修改了 `EasyPredictModelWrapper` 中的 `predictAutoEncoder` 方法，如下所示：
- en: '[PRE2]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The custom modified API will expose a method for retrieving the reconstruction
    error on each predicted row.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义修改的 API 将公开一种检索每个预测行的重构错误的方法。
- en: In order to make the POJO model to work, we must specify the same data format
    used during training. The data should be loaded into `hex.genmodel.easy.RowData`
    objects that are simply instances of `java.util.Hashmap<String, Object>.`
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 为使POJO模型工作，我们必须指定与训练期间使用的相同的数据格式。数据应加载到`hex.genmodel.easy.RowData`对象中，这只是`java.util.Hashmap<String,
    Object>`的实例。
- en: 'When you create a `RowData` object you must ensure these things:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`RowData`对象时，必须确保以下事项：
- en: The same column names and types of the `H2OFrame` are used. For categorical
    columns, you must use String. For numerical columns, you can either use Double
    or String. Different column types are not supported.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相同的列名和`H2OFrame`的类型。对于分类列，必须使用String。对于数值列，可以使用Double或String。不支持不同的列类型。
- en: In case of categorical features, the values must belong to the same set used
    for training unless you explicitly set `convertUnknownCategoricalLevelsToNa` to
    true in the model wrapper.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类特征，除非您将`convertUnknownCategoricalLevelsToNa`显式设置为模型包装器中的true，否则值必须属于训练时使用的相同集合。
- en: Additional columns can be specified but will be ignored.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以指定其他列，但将被忽略。
- en: Any missing column will be treated as NA.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何缺少的列都将被视为NA。
- en: The same pre-processing transformation should be applied to the data as well.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据也应该应用相同的预处理转换。
- en: This last requirement is probably the trickiest one. If our machine learning
    pipeline is made of a bunch of transformers, those must be exactly replicated
    in the deployment. Thus, the `POJO` class is not enough and should also be accompanied
    by all of the remaining steps in the pipeline in addition to the H2O neural network.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一个要求可能是最棘手的。如果我们的机器学习流水线由一堆转换器组成，那么这些转换器必须在部署中完全复制。因此，`POJO`类是不够的，还应该与H2O神经网络以及流水线中的所有其他步骤一起使用。
- en: 'Here is an example of a Java main method that reads some data and scores it
    against an exported `POJO` class:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个Java主函数的示例，它读取一些数据，并针对导出的`POJO`类进行评分：
- en: '[PRE3]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have seen an example of how to instantiate the POJO model as a Java class
    and use it for scoring a mock data point. We can re-adapt this code to be integrated
    within an existing enterprise JVM-based system. If you are integrating it in Spark,
    you can simply wrap the logic we have implemented in the example main class within
    a function and call it from a map method on a Spark data collection. All you need
    is the model POJO jar to be loaded into the JVM where you want to make the predictions.
    Alternatively, if your enterprise stack is JVM-based, there are a few util entry
    points, such as `hex.genmodel.PredictCsv`. It allows you to specify a csv input
    file and a path where the output will be stored. Since `AutoEncoder` is not yet
    supported in the Easy API, you will have to modify the `PredictCsv` main class
    according to the custom patch we have seen before. Another architecture could
    be like this: you use Python to build the model and a JVM-based application for
    the production deployment.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何将POJO模型实例化为Java类并将其用于评分模拟数据点的示例。我们可以重新调整此代码，以便将其集成到现有的基于JVM的企业系统中。如果您正在集成它到Spark中，您只需将我们在示例主类中实现的逻辑包装在一个函数中，并从Spark数据集的map方法中调用它。您所需要的只是将模型POJO
    jar加载到您想要进行预测的JVM中。或者，如果您的企业栈是基于JVM的，还有一些实用的入口点，例如`hex.genmodel.PredictCsv`。它允许您指定一个csv输入文件和一个用于存储输出的路径。由于Easy
    API尚不支持`AutoEncoder`，您将不得不根据我们之前看到的自定义补丁修改`PredictCsv`主类。另一种架构可能是使用Python构建模型并在生产部署中使用基于JVM的应用程序。
- en: Anomaly score APIs
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常分数API
- en: Exporting the model as a POJO class is one way to programmatically include it
    in an existing JVM system, pretty much like the way you import an external library.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型导出为POJO类是以程序方式将其包含在现有JVM系统中的一种方法，就像导入外部库一样。
- en: There are a bunch of other situations where the integration works better using
    a self-containing API, such as in a micro-services architecture or non-JVM-based
    systems.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些其他情况下，使用自包含的API进行集成会更好，比如在微服务架构或非JVM-based系统中。
- en: H2O offers the capability of wrapping the trained model in a REST API to call
    specifying the row data to score via a JSON object attached to an HTTP request.
    The backend implementation behind the REST API is capable of performing everything
    you would do with the Python H2O API, included the pre and post-processing steps.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: H2O可以将训练好的模型封装为一个REST API，通过附加在HTTP请求中的JSON对象指定要评分的行数据来调用。 REST API后端的实现可以执行您在Python
    H2O API中执行的所有操作，包括预处理和后处理步骤。
- en: 'The REST API is accessible from:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: REST API可从以下位置访问：
- en: Any browser using simple add-ons, such as Postman in Chrome
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何使用简单插件的浏览器，例如Chrome中的Postman
- en: curl, one of the most popular tools for client-side URL transfers
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: curl，用于客户端URL传输的最流行工具之一
- en: Any language of your choice; REST APIs are completely language agnostic
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何您选择的语言；REST API完全与语言无关
- en: In spite of the POJO class, the REST API offered by H2O depends on a running
    instance of the H2O cluster. You can access the REST API at `http://hostname:54321`
    followed by the API version (latest is 3); and the resource path, for example,
    `http://hostname:54321/3/Frames` will return the list of all Frames.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在POJO类，但H2O提供的REST API依赖于运行中的H2O集群实例。您可以在`http://hostname:54321`后面加上API版本（最新为3）和资源路径，例如，`http://hostname:54321/3/Frames`将返回所有Frames的列表。
- en: '`REST` APIs supports five verbs or methods: `GET`, `POST`, `PUT`, `PATCH`,
    and `DELETE`.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '`REST`API支持五个动词或方法：`GET`、`POST`、`PUT`、`PATCH`和`DELETE`。'
- en: '`GET` is used to read a resource with no side-effects, `POST` to create a new
    resource, PUT to update and replace entirely an existing resource, `PATCH` to
    modify a part of an existing resource, and `DELETE` to delete a resource. The
    `H2O REST` API does not support the `PATCH` method and adds a new method called
    `HEAD`. It is like a `GET` request but returns only the `HTTP` status, useful
    to check whether a resource exists or not without loading it.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET`用于读取没有副作用的资源，`POST`用于创建新资源，`PUT`用于更新和完全替换现有资源，`PATCH`用于修改现有资源的一部分，`DELETE`用于删除资源。`H2O
    REST`API不支持`PATCH`方法，并添加了一个称为`HEAD`的新方法。它类似于`GET`请求，但仅返回`HTTP`状态，可用于检查资源是否存在而无需加载它。'
- en: Endpoints in H2O could be Frames, Models, or Clouds, which are pieces of information
    related to the status of nodes in the H2O cluster.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: H2O中的端点可以是Frames、Models或Clouds，这些是与H2O集群中节点状态相关的信息片段。
- en: Each endpoint will specify its own payload and schema, and the documentation
    can be found on [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/rest-api-reference.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/rest-api-reference.html).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 每个端点将指定其自己的有效载荷和模式，并且文档可以在[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/rest-api-reference.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/rest-api-reference.html)上找到。
- en: 'H2O provides in the Python module a connection handler for all the REST requests:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: H2O在Python模块中提供了一个连接处理程序，用于所有REST请求：
- en: '[PRE4]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `hc` object has a method called `request` that can be used to send `REST`
    requests:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '`hc`对象有一个名为`request`的方法，可以用来发送`REST`请求：'
- en: '[PRE5]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Data payloads for `POST` requests can be added using either the argument `data`
    (x-www format) or `json` (json format) and specifying a dictionary of key-value
    pairs. Uploading a file happens by specifying the `filename` argument mapping
    to a local file path.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`POST`请求的数据载荷可以使用`data`参数（x-www格式）或`json`参数（json格式），并指定一个键值对字典。通过指定`filename`参数映射到本地文件路径来上传文件。
- en: 'At this stage, whether we use the Python module or any REST client, we must
    do the following steps in order to upload some data and get the model scores back:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，无论我们使用Python模块还是任何REST客户端，我们必须按照以下步骤上传一些数据并获取模型得分：
- en: 'Import the data you want to score using the `POST /3/ImportFiles` by using
    an `ImporFilesV3` schema, including a remote path from where to load data (via
    http, s3, or other protocols). The corresponding destination frame name will be
    the file path:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`POST /3/ImportFiles`导入要评分的数据，使用`ImporFilesV3`模式，包括从哪里加载数据的远程路径（通过http、s3或其他协议）。相应的目标帧名称将是文件路径：
- en: '[PRE6]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Guess the parameters for parsing; it will return a bunch of parameters inferred
    from the data for the final parsing (you can skip and manually specify those):'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猜测解析参数；它将返回从数据中推断出的一堆参数，用于最终解析（您可以跳过并手动指定这些参数）：
- en: '[PRE7]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Parse according to the parsing parameters:'
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据解析参数解析：
- en: '[PRE8]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Get the job name from the response and poll for import completion:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从响应中获取作业名称，并轮询导入完成状态：
- en: '[PRE9]'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When the returned status is DONE, you can run the model scoring as:'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当返回状态为DONE时，您可以运行模型评分如下：
- en: '[PRE10]'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After parsing the results, you can delete both the input and prediction frames:'
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析结果后，您可以删除输入和预测框架：
- en: '[PRE11]'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s analyze the input and output of the Predictions API. `reconstruction_error`,
    `reconstruction_error_per_feature`, and `deep_features_hidden_layer` are specific
    parameters for AutoEncoder models and determine what will be included in the output.
    The output is an array of `model_metrics` where for AutoEncoder will contain:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析 Predictions API 的输入和输出。 `reconstruction_error`、`reconstruction_error_per_feature`
    和 `deep_features_hidden_layer` 是 AutoEncoder 模型的特定参数，并确定输出中将包含什么。输出是一个 `model_metrics`
    数组，对于 AutoEncoder 将包含：
- en: '**MSE**: Mean Squared Error of the predictions'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MSE**：预测的均方误差'
- en: '**RMSE**: Root Mean Squared Error of the predictions'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE**：预测的均方根误差'
- en: '**scoring_time**: Time in mS since the epoch for the start of this scoring
    run'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**得分时间**：自这次评分运行开始以来的毫秒数'
- en: '**predictions**: The frame with the all the prediction rows'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：包含所有预测行的框架'
- en: A summary of deployment
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署的总结
- en: 'We have seen two options for exporting and deploying a trained model: exporting
    it as a `POJO` and incorporating it into a JVM-based application or using the
    REST API to call a model which is already loaded into a running H2O instance.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到两种导出和部署训练模型的选项：将其导出为 `POJO` 并将其合并到基于 JVM 的应用程序中，或者使用 REST API 调用已加载到运行中
    H2O 实例中的模型。
- en: Generally, using `POJO` is a better choice because it does not depend on a running
    H2O cluster. Thus, you can use H2O for building the model and then deploy it on
    any other system.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，使用 `POJO` 是一个更好的选择，因为它不依赖于运行中的 H2O 集群。因此，您可以使用 H2O 构建模型，然后在任何其他系统上部署它。
- en: The REST API will be useful if you want to achieve more flexibility and being
    able to generate predictions from any client at any time as long as the H2O cluster
    is running. The procedure, though, requires multiple steps compared to the `POJO`
    deployment.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要实现更大的灵活性，并且能够在任何客户端随时生成预测，只要 H2O 集群正在运行，那么 REST API 就会很有用。然而，这个过程需要比 `POJO`
    部署更多的步骤。
- en: Another recommended architecture is to use the exported `POJO` and wrap it within
    a JVM REST API using frameworks such as Jersey for Java and Play or `akka-http`
    if you prefer Scala. Building your own API means you can define programmatically
    the way you want to accept input data and what you want to return as output in
    a single request, as opposed to the multiple steps in H2O. Moreover, your `REST`
    API could be stateless. That is, you don't need to import data into frames and
    delete them afterwards.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个推荐的架构是使用导出的 `POJO` 并将其包装在使用 Jersey 进行 Java 或者 Play 或 `akka-http` 进行 Scala
    的 JVM REST API 中。构建自己的 API 意味着您可以以编程方式定义接受输入数据的方式以及单个请求中要返回的内容，而不是 H2O 中的多个步骤。此外，您的
    `REST` API 可以是无状态的。也就是说，您不需要将数据导入帧并在之后删除它们。
- en: 'Ultimately, if you want your POJO-based REST API to be easily ported and deployed
    everywhere, it is recommended to wrap it in a virtual container using Docker.
    Docker is an open source framework that allows you to wrap a piece of software
    in a complete filesystem that contains everything you need to run: code, runtime,
    system tools, libraries and everything you need to have installed. In such a way,
    you have a single lightweight container that can always run the same service in
    every environment.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，如果您希望基于 POJO 的 REST API 能够轻松地移植和部署到任何地方，建议您使用 Docker 将其包装在虚拟容器中。Docker 是一个开源框架，允许您将软件包装在一个完整的文件系统中，其中包含运行所需的所有内容：代码、运行时、系统工具、库以及您需要安装的所有内容。这样，您就有了一个单一的轻量级容器，在任何环境中始终可以运行相同的服务。
- en: A Dockerized API can easily be shipped and deployed to any of your production
    servers.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 化的 API 可以轻松地部署到您的任何生产服务器上。
- en: Summary
  id: totrans-553
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署摘要
- en: In this chapter, we went through a long journey of optimizations, tweaks, testing
    strategies, and engineering practices to turn our neural network into an intrusion
    detection data product.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们经历了一段漫长的优化、调整、测试策略和工程实践之旅，将我们的神经网络转化为入侵检测数据产品。
- en: In particular, we defined a data product as a system that extracts value from
    raw data and returns actionable knowledge as output.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将数据产品定义为从原始数据中提取价值并将可操作的知识作为输出返回的系统。
- en: We saw a few optimizations for training a deep neural network to be faster,
    scalable, and more robust. We addressed the problem of early saturation via weights
    initialization. Scalability using both a parallel multi-threading version of SGD
    and a distributed implementation in Map/Reduce. We saw how the H2O framework can
    leverage Apache Spark as the backend for computation via Sparkling Water.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一些训练深度神经网络以获得更快、可扩展和更健壮性的优化方法。我们通过权重初始化解决了早期饱和问题。利用并行多线程版本的SGD和Map/Reduce中的分布式实现来提高可扩展性。我们看到了H2O框架如何通过Sparkling
    Water将Apache Spark作为计算后端来实现。
- en: We remarked the importance of testing and the difference between model validation
    and full end-to-end evaluation. Model validation is used to reject or accept a
    given model, or to select the best performing one. Likely, model validation metrics
    can be used for hyper-parameter tuning. On the other hand, end-to-end evaluation
    is what quantifies more comprehensibly how the full solution is solving real business
    problems.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了测试的重要性，以及模型验证和完整端到端评估之间的区别。模型验证用于拒绝或接受给定模型，或选择性能最佳的模型。同样，模型验证指标可用于超参数调整。另一方面，端到端评估更全面地量化了完整解决方案如何解决实际业务问题。
- en: Ultimately, we did the last step—to deploy the tested model straight into production,
    by either exporting it as a POJO object or turning it into a service via a `REST`
    API.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们进行了最后一步——通过将测试过的模型直接部署到生产环境中，要么将其导出为POJO对象，要么通过`REST` API将其转换为服务。
- en: We summarized a few lessons learnt in the experience of building robust machine
    learning systems and deeper architectures. We expect the reader to use those as
    a basis for further development and customized solutions according to each use
    case.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了在构建强大的机器学习系统和更深层次的架构方面所学到的一些经验教训。我们期望读者将这些作为进一步发展和根据每个使用案例定制解决方案的基础。
