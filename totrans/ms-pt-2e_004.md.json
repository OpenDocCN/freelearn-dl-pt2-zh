["```py\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n```", "```py\nclass PosEnc(nn.Module):\n    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n        # d_m is same as the dimension of the embeddings\n        pos = torch.arange(     size_limit, dtype=torch.float).unsqueeze(1)\n        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n        # divider is the list of radians, multiplied by position indices of words, and fed to the sinusoidal and cosinusoidal function.  \n        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n    def forward(self, x):\n        return self.dropout(x + self.p_enc[:x.size(0)     ])\n```", "```py\nclass Transformer(nn.Module):\n    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n        self.position_enc = PosEnc(num_inputs, dropout)\n        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n        self.enc = nn.Embedding(num_token, num_inputs)\n        self.num_inputs = num_inputs\n        self.dec = nn.Linear(num_inputs, num_token)\n```", "```py\n def forward(self, source):\n        source = self.enc(source) * math.sqrt(self.num_inputs)\n        source = self.position_enc(source)\n        op = self.enc_transformer(source, self.mask_source)\n        op = self.dec(op)\n        return op\n```", "```py\ntr_iter = WikiText2(split='train')\ntkzer = get_tokenizer('basic_english')\nvocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\nvocabulary.set_default_index(vocabulary['<unk>'])\n```", "```py\ndef process_data(raw_text):\n    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\ntr_iter, val_iter, te_iter = WikiText2()\ntraining_text = process_data(tr_iter)\nvalidation_text = process_data(val_iter)\ntesting_text = process_data(te_iter) \n```", "```py\ndef gen_batches(text_dataset, batch_size):\n    num_batches = text_dataset.size(0) // batch_size\n    text_dataset = text_dataset[:num_batches * batch_size]\n    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n    return text_dataset.to(device)\ntraining_batch_size = 32\nevaluation_batch_size = 16\ntraining_data = gen_batches(training_text, training_batch_size) \n```", "```py\nmax_seq_len = 64\ndef return_batch(src, k):\n    sequence_length = min(max_seq_len, len(src) - 1 - k)\n    sequence_data = src[k:k+sequence_length]\n    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n    return sequence_data, sequence_label \n```", "```py\nnum_tokens = len(vocabulary) # vocabulary size\nembedding_size = 256 # dimension of embedding layer\nnum_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\nnum_layers = 2 # num of transformer encoder layers within transformer encoder\nnum_heads = 2 # num of heads in (multi head) attention models\ndropout = 0.25 # value (fraction) of dropout\nloss_func = nn.CrossEntropyLoss()\nlrate = 4.0 # learning rate\noptim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\nsched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)\ntransformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, dropout).to(device) \n```", "```py\ndef train_model():     )\n    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n        train_data_batch, train_label_batch = return_batch(training_data, i)\n        sequence_length = train_data_batch.size(0)\n        if sequence_length != max_seq_len:  # only on last batch\n            mask_source = mask_source[:sequence_length, :sequence_length]\n\n        op = transformer_model(train_data_batch, mask_source)\n        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n       optim_module.zero_grad()\n        loss_curr.backward()\ntorch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n        optim_module.step()\n        loss_total += loss_curr.item()\ndef eval_model(eval_model_obj, eval_data_source):\n...\n```", "```py\nmin_validation_loss = float(\"inf\")\neps = 5\nbest_model_so_far = None\nfor ep in range(1, eps + 1):\n    ep_time_start = time.time()\n    train_model()\n    validation_loss = eval_model(transformer_model, validation_data)\n    if validation_loss < min_validation_loss:\n        min_validation_loss = validation_loss\n        best_model_so_far = transformer_model\n```", "```py\ntesting_loss = eval_model(best_model_so_far, testing_data)\nprint(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")\n```", "```py\nimport torch\nfrom transformers import BertForMaskedLM, BertTokenizer\nbert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\ntoken_gen = BertTokenizer.from_pretrained('bert-base-uncased')\nip_sequence = token_gen(\"I love PyTorch !\", return_tensors=\"pt\")[\"input_ids\"]\nop = bert_model(ip_sequence, labels=ip_sequence)\ntotal_loss, raw_preds = op[:2]\n```", "```py\nfrom torchviz import make_dot\nimport networkx as nx\n```", "```py\ndef train(model, train_dataloader, optim, loss_func, epoch_num, lrate):\n    for training_data, training_label in train_dataloader:\n        pred_raw = model(training_data)\n        curr_loss = loss_func(pred_raw, training_label)\n        training_loss += curr_loss.data\n    return training_loss / data_size, training_accuracy / data_size\n```", "```py\ndef load_dataset(batch_size):\n    train_dataloader = torch.utils.data.DataLoader(\n        datasets.CIFAR10('dataset', transform=transform_train_dataset, train=True, download=True),\n        batch_size=batch_size,  shuffle=True)\n    return train_dataloader, test_dataloader\ntrain_dataloader, test_dataloader = load_dataset(batch_size)\n```", "```py\nclass RndGraph(object):\n    def __init__(self, num_nodes, graph_probability, nearest_neighbour_k=4, num_edges_attach=5):\n    def make_graph_obj(self):\n        graph_obj = nx.random_graphs.connected_watts_strogatz_graph(self.num_nodes, self.nearest_neighbour_k,self.graph_probability)\n        return graph_obj\n```", "```py\n def get_graph_config(self, graph_obj):\n        return node_list, incoming_edges\n    def save_graph(self, graph_obj, path_to_write):\n        nx.write_yaml(graph_obj, \"./cached_graph_obj/\" + path_to_write)\n    def load_graph(self, path_to_read):\n        return nx.read_yaml(\"./cached_graph_obj/\" + path_to_read)\n```", "```py\nclass SepConv2d(nn.Module):\n    def __init__(self, input_ch, output_ch, kernel_length=3, dilation_size=1, padding_size=1, stride_length=1, bias_flag=True):\n        super(SepConv2d, self).__init__()\n        self.conv_layer = nn.Conv2d(input_ch, input_ch, kernel_length, stride_length, padding_size, dilation_size, bias=bias_flag, groups=input_ch)\n        self.pointwise_layer = nn.Conv2d(input_ch, output_ch, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=bias_flag)\n    def forward(self, x):\n        return self.pointwise_layer(self.conv_layer(x))\n```", "```py\nclass UnitLayer(nn.Module):\n    def __init__(self, input_ch, output_ch, stride_length=1):\n        self.unit_layer = nn.Sequential(\n            nn.ReLU(),\n            SepConv2d(input_ch, output_ch, stride_length=stride_length),nn.BatchNorm2d(output_ch),nn.Dropout(self.dropout)\n        )\n    def forward(self, x):\n        return self.unit_layer(x)\n```", "```py\nclass GraphNode(nn.Module):\n    def __init__(self, input_degree, input_ch, output_ch, stride_length=1):\n        self.unit_layer = UnitLayer(input_ch, output_ch, stride_length=stride_length)\n    def forward(self, *ip):\n        if len(self.input_degree) > 1:\n            op = (ip[0] * torch.sigmoid(self.params[0]))\n            for idx in range(1, len(ip)):\n                op += (ip[idx] * torch.sigmoid(self.params[idx]))\n            return self.unit_layer(op)\n        else:\n            return self.unit_layer(ip[0])\n```", "```py\nclass RandWireGraph(nn.Module):\n    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode, graph_name):\n        # get graph nodes and in edges\n        rnd_graph_node = RndGraph(self.num_nodes, self.graph_prob)\n        if self.train_mode is True:\n            rnd_graph = rnd_graph_node.make_graph_obj()\n            self.node_list, self.incoming_edge_list = rnd_graph_node.get_graph_config(rnd_graph)\n        else:\n        # define source Node\n        self.list_of_modules = nn.ModuleList([GraphNode(self.incoming_edge_list[0], self.input_ch, self.output_ch,\nstride_length=2)])\n        # define the sink Node\nself.list_of_modules.extend([GraphNode(self.incoming_edge_list[n], self.output_ch, self.output_ch)\n                                     for n in self.node_list if n > 0])\n```", "```py\n def forward(self, x):\n        # source vertex\n        op = self.list_of_modules[0].forward(x)\n        mem_dict[0] = op\n        # the rest of the vertices\n        for n in range(1, len(self.node_list) - 1):\n            if len(self.incoming_edge_list[n]) > 1:\n                op = self.list_of_modules[n].forward(*[mem_dict[incoming_vtx]\n                                                       for incoming_vtx in self.incoming_edge_list[n]])\n            mem_dict[n] = op\n        for incoming_vtx in range(1, len(self.incoming_edge_list[self.num_nodes + 1])):\n            op += mem_dict[self.incoming_edge_list[self.num_nodes + 1][incoming_vtx]]\n        return op / len(self.incoming_edge_list[self.num_nodes + 1])\n```", "```py\nclass RandWireNNModel(nn.Module):\n    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode):\n        self.conv_layer_1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=self.output_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(self.output_ch) )\n        self.conv_layer_2 = …\n        self.conv_layer_3 = …\n        self.conv_layer_4 = …\n        self.classifier_layer = nn.Sequential(\n            nn.Conv2d(in_channels=self.input_ch*8, out_channels=1280, kernel_size=1), nn.BatchNorm2d(1280))\n        self.output_layer = nn.Sequential(nn.Dropout(self.dropout), nn.Linear(1280, self.class_num))\n```", "```py\n def forward(self, x):\n        x = self.conv_layer_1(x)\n        x = self.conv_layer_2(x)\n        x = self.conv_layer_3(x)\n        x = self.conv_layer_4(x)\n        x = self.classifier_layer(x)\n        # global average pooling\n        _, _, h, w = x.size()\n        x = F.avg_pool2d(x, kernel_size=[h, w])\n        x = torch.squeeze(x)\n        x = self.output_layer(x)\n        return x\n```", "```py\nnum_epochs = 5\ngraph_probability = 0.7\nnode_channel_count = 64\nnum_nodes = 16\nlrate = 0.1\nbatch_size = 64\ntrain_mode = True\n```", "```py\nrand_wire_model = RandWireNNModel(num_nodes, graph_probability, node_channel_count, node_channel_count, train_mode).to(device)\noptim_module = optim.SGD(rand_wire_model.parameters(), lr=lrate, weight_decay=1e-4, momentum=0.8)\nloss_func = nn.CrossEntropyLoss().to(device)\n```", "```py\nfor ep in range(1, num_epochs + 1):\n    epochs.append(ep)\n    training_loss, training_accuracy = train(rand_wire_model, train_dataloader, optim_module, loss_func, ep, lrate)\n    test_accuracy = accuracy(rand_wire_model, test_dataloader)\n    test_accuracies.append(test_accuracy)\n    training_losses.append(training_loss)\n    training_accuracies.append(training_accuracy)\n    if best_test_accuracy < test_accuracy:\n        torch.save(model_state, './model_checkpoint/' + model_filename + 'ckpt.t7')\n    print(\"model train time: \", time.time() - start_time)\n```", "```py\nrand_wire_nn_model.load_state_dict(model_checkpoint['model'])\nfor test_data, test_label in test_dataloader:\n    success += pred.eq(test_label.data).sum()\n    print(f\"test accuracy: {float(success) * 100\\. / len(test_dataloader.dataset)} %\")\n```"]