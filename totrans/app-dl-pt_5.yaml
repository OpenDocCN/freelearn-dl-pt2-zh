- en: '*Chaper 5*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Style Transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Load pretrained models from PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the style of an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain contents of an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new image using the style of one image and contents from another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you'll learn how to transfer the artistic style from one picture
    to another. This way, you'll be able to convert your everyday pictures into masterpieces.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous chapter explained different building blocks of traditional convolutional
    neural networks (CNNs), as well as some techniques to be able to improve performance
    and reduce training time. The architecture explained there, although typical,
    is not set in stone, and, on the contrary, a proliferation of CNNs architectures
    has emerged to solve different data problems, more commonly in the field of computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: These architectures vary in configuration as well as learning tasks. A very
    popular one nowadays is the VGG architecture created by Oxford's Visual Geometry
    Group. It was developed for object recognition, achieving state-of-the-art performance
    thanks to the massive number of parameters that the network relies on. One of
    the main reasons for its popularity popularity among data scientists is due to
    the availability of the parameters (weights and biases) of the trained model,
    which allows researchers to use it without training, as well as the outstanding
    performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use this pretrained model to solve a computer vision
    problem that is particularly famous due to the popularity of social media channels
    specializing in sharing images. It consists of performing style transfer in order
    to improve the appearance of an image with the style (colors and textures) of
    another one.
  prefs: []
  type: TYPE_NORMAL
- en: The previous task is performed millions of times every day when applying filters
    over regular images to improve their quality and appeal while posting on social
    media profiles. Although it seems like a simple task when in use, this chapter
    will explain the magic that occurs behind the scenes of these image editing apps.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, the GitHub repository containing all code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  prefs: []
  type: TYPE_NORMAL
- en: Style Transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In simple words, style transfer consists of modifying the style of an image,
    while still preserving its content. For instance, taking an image of an animal
    and transforming the style into a Van Gogh-like painting, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Style transfer inputs and output.Result from the final exercise
    of this chapter.](img/C11865_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Style transfer inputs and output.Result from the final exercise
    of this chapter.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'According to the preceding figure, there are two inputs to a pretrained model:
    a content image and a style image. The content refers to the objects, while the
    style refers to the colors and textures. As a result, the output from the model
    should be an image containing the objects from the content image and the artistic
    appearance of the style image.'
  prefs: []
  type: TYPE_NORMAL
- en: How Does It Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different to solving a traditional computer vision problem, which was explained
    in the previous chapter, style transfer requires to follow a different set of
    steps to effectively take two images as an input and create a new image as the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a brief explanation of the steps followed when solving a style
    transfer problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feeding the inputs**: Both the content and the style image are to be fed
    to the model, and they need to be the same shape. A common practice here is to
    resize the style image to be the same shape of the content image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loading the model**: The Oxford''s Visual Geometry Group created a model''s
    architecture that performs outstandingly well over style transfer problems, which
    is known as the VGG network. Moreover, they also made the model''s parameters
    available to anyone so that the training process of the model could be shortened
    or skipped.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: There are different versions of the VGG network, which use different number
    of layers. To differentiate the different versions, the nomenclature on the matter
    will add a dash and a number at the end of the acronym, which represents the number
    of layers of that particular architecture. For the present chapter, we will use
    the network's 19-layer version, which is known as the VGG-19.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Because of this, and using PyTorch's pretrained models' subpackage, it is possible
    to load the pretrained model in order to perform the style transfer task without
    the need to train the network with a large number of images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Determining the layers'' functions**: Given that there are two main tasks
    at hand (recognizing the content of an image and distinguishing the style of another
    one), different layers will have different functions to extract the different
    features; for the style image, the focus should be on colors and textures; while
    for the content image, the focus should be on edges and forms. In this step, the
    different layers are separated into different tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defining the optimization problem**: Like any other supervised problem, it
    is necessary to define a loss function, which will have the responsibility of
    measuring the difference between the output and inputs. Unlike other supervised
    problems, it is required to minimize three different loss functions for style
    transfer problems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Content loss: This measures the distance between the content image and the
    output, only considering features related to content.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Style loss: This measures the distance between the style image and the output,
    only considering features related to style.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Total loss: This combines both the content and style loss. Both the content
    and style loss have a weight associated to them, which is used to determine their
    participation in the calculation of the total loss.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parameters update**: This step uses gradients to update the different parameters
    of the network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation of Style Transfer Using the VGG-19 Network Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The VGG-19 is a CNN consisting of 19 layers. It was trained using millions of
    images from the ImageNet database. The network is capable of classifying images
    into 1000 different class labels, including a vast number of animals and different
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To explore the ImageNet database, use the following URL: [http://www.image-net.org/](http://www.image-net.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Considering its depth, the network is able to identify complex features from
    a wide variety of images, which makes it particularly good for style transfer
    problems, where feature extraction is crucial at different stages and for different
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The following section will focus on explaining the process of using a pretrained
    VGG-19 model to perform style transfer. The end purpose of this chapter will be
    to take an image of an animal or a landscape (as the content image) and one of
    a painting from a well-known artist (as the style image) to create a new image
    of a regular object with an artistic style.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before diving into the process, the following is an explanation of
    the imports with a brief explanation of their use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**: This will be used to transform images to be displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**torch, torch.nn, and torch.optim**: These will implement the neural network,
    as well as define the optimization algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PIL.Image**: This will load images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**matplotlib.pyplot**: This will display images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**torchvision.transforms and torchvision.models**: These will convert the images
    into tensors and load the pretrained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inputs: Loading and Displaying'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step of performing a style transfer consists of loading both the content
    and style images. During this step, the basic pre-processing is handled, where
    images must be equally sized (preferably the size of the images used to train
    the pretrained model), which will be the size of the output image as well. Additionally,
    images are converted into PyTorch tensors and can be normalized if desired.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is always a good practice to display images that have been loaded
    in order to make sure that they are as desired. Considering images have already
    been converted to tensors and normalized at this point, the tensor should be cloned,
    and a new set of transformations need to be performed in order to be able to display
    them using Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: Defining functions to both load and display images can help save up time, and
    can also make sure that the same process is done over both the content and style
    image. This process will be expanded in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All exercises of this chapter are to be coded in the same notebook, as together,
    they will perform the style transfer task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Loading and Displaying Images'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the first of four steps to perform a style transfer. The objective of
    this chapter is to load and display images (both content and style) that will
    be used in further exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inside the GitHub repository (link shared at the beginning of this chapter),
    you will be able to find different images that will be used throughout this chapter
    in the different exercises and activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all packages required to perform style transfer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the image size to be used for both images. Also, set the transformations
    to be performed over images, which should include resizing images, converting
    them to tensors, and normalizing them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The VGG network was trained using normalized images, where each channel has
    a mean of 0.485, 0.456, and 0.406, respectively, and a standard deviation of 0.229,
    0.224, and 0.225.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function that will receive the image path as input and use PIL to
    open the image. Next, it should apply the transformations over the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the function to load both content and style images. Use the dog image
    as content and the Matisse image as style, both of which are available in the
    GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To display the images, convert them back to PIL images and revert the normalization
    process. Define these transformations in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To revert the normalization, it is necessary to use as mean the negative value
    of the mean used for normalizing the data, divided by the standard deviation previously
    used for normalizing the data. Moreover, the new standard deviation should be
    equal to one divided by the standard deviation used to normalize the data before.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a function that clones the tensor, squeezes it, and finally applies
    transformations to the tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the function for both images and plot the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting images should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Content image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.2: Content image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 5.3: Style image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.3: Style image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have successfully loaded and displayed the content and
    style images to be used for style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As in many other frameworks, PyTorch has a subpackage that contains different
    models that have been previously trained and made available for public use. This
    is important considering that training a neural network from scratch is time consuming,
    while starting off with a pretrained model can help reduce this training times.
    This means that pretrained models can be loaded in order to use their final parameters
    (which should be those that minimize the loss function) without the need to go
    through an iterative process.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned eariler, the architecture to be used to perform the style transfer
    task is that of the VGG network of 19 layers, also known as VGG-19\. The pretrained
    model is available under the model''s subpackage of `torchvision`. The saved model
    in PyTorch is split into two portions, as mentioned and explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**vgg19.features**: This consists of all the convolutional and pooling layers
    of the network along with the parameters. These layers are in charge of extracting
    the features from the images, where some of the layers specialize in style features,
    such as colors, while others specialize in content features, such as edges.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**vgg19.classifier**: This refers to the linear layers (also known as fully
    connected layers) that are located at the end of the network, including their
    parameters. These layers are the ones that perform the classification of the image
    into one of the label classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: To explore other pretrained models available in PyTorch, visit [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: According to the preceding information, only the features portion of the model
    should be loaded in order to extract the necessary features of both the content
    and style images. Loading a model consists of calling the models' subpackage followed
    by the name of the model, making sure that the pretrained argument is set to `True`
    and that only the feature layers are being loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the parameters in each layer should be kept unchanged, considering
    that those are the ones that will help detect the desired features. This can be
    achieved by defining that the model does not need to calculate gradients for any
    of these layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Loading a Pretrained Model in PyTorch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the same notebook as in the previous exercise, this exercise aims to
    load the pretrained model that will be used in subsequent exercises to perform
    the style transfer task using the images previously loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the notebook from the previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the VGG-19 pretrained model from PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Select the features portion of the model, as explained previously. This will
    give access to all the convolutional and pooling layers of the model, which are
    to be used to perform the extraction of features in subsequent exercises of this
    chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a `for` loop through the parameters of the previously loaded model.
    Set each parameter to not require gradients calculations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By setting the calculation of gradients to `False`, we ensure that this to not
    require gradients calculations unchanged during the process of creating the target
    image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have successfully loaded a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The VGG-19 network, as mentioned before, contains 19 different layers, including
    convolutional, pooling, and fully connected layers. The convolutional layers come
    in stacks before every pooling layer, five being the number of stacks in the entire
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of style transfer, there have been different papers that have identified
    those layers that are crucial at recognizing relevant features over the content
    and style images. According to this, it is conventionally accepted that the first
    convolutional layer of every stack is capable of extracting style features, while
    only the second convolutional layer of the fourth stack should be used to extract
    content features. From now on, we will refer to the layers that extract the style
    features as `conv1_1`, `conv2_1`, `conv3_1`, `conv4_1`, and `conv5_1`, while the
    layer in charge of extracting the content features will be known as `conv4_2`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The paper used as guide for this chapter can be accessed in the following URL:
    [https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the style image should be passed through five different layers,
    while the content image only needs to go through one layer. The output from each
    of these layers is used to compare the output image to the input images, where
    the objective would be to modify the parameters of the target image to resemble
    the content of the content image and the style of the style image, which can be
    achieved through the optimization of three different loss functions (which will
    be further explained in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: To determine whether the target image contains the same content as the content
    image, we need to check whether certain features are present in both images. However,
    to check the style representation of the target image and the style image, it
    is necessary to check for correlations and not the strict presence of the features
    on both images. This is because the style features of both images will not be
    exact, but rather an approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, the gram matrix is introduced. It consists of the creation
    of a matrix that looks at the correlations of different style features in a given
    layer. This is done by multiplying the vectorized output from the convolutional
    layer by the same transposed vectorized output, as can be seen in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Calculation of the gram matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.4: Calculation of the gram matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, A refers to the input style image with four-by-four
    dimensions (height and width), B represents the output after passing the image
    through a convolutional layer with five filters. Finally, C refers to the calculation
    of the gram matrix, where the image to the left represents the vectorized version
    of B, and the image to the right is its transposed version. From the multiplication
    of the vectorized outputs, a five-by-five gram matrix is created, whose values
    indicate the similarities (correlations) in terms of style features along the
    different channels (filters).
  prefs: []
  type: TYPE_NORMAL
- en: These correlations can be used to determine those features that are relevant
    for the style representation of the image, which can then be used to alter the
    target image. Considering that the style features are obtained in five different
    layers, it is safe to assume that the network is capable of detecting small and
    large features from the style image, considering that a gram matrix has to be
    created for each of the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Setting up the Feature Extraction Process'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the network architecture from the previous exercise and the image from
    the first exercise of this chapter, we will create a couple of functions capable
    of extracting features from the input images and creating the gram matrix for
    the style features:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the notebook from the previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print the architecture of the model loaded in the previous exercise. This will
    help identify relevant layers to perform the style transfer task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dictionary mapping the index of the relevant layers (keys) to a name
    (values). This will facilitate the process of calling relevant layers in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To create the dictionary, we use the output from the previous step that displays
    each of the layers in the network. There, it is possible to observe that the first
    layer of the first stack is labelled as `0`, while the first layer of the second
    stack is labelled as `5` and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a function that will extract the relevant features (features extracted
    from the relevant layers only) from an input image. Name it `features_extractor`
    and make sure it takes as inputs the image, the model, and the dictionary previously
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`model._modules` contains a dictionary holding each layer of the network. By
    performing a `for` loop through the different layers, we pass the image through
    the layers of interest (the ones inside the `layers` dictionary previously created)
    and save the output into the `features` dictionary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output dictionary consists of keys containing the name of the layer and
    values containing the output features from that layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `features_extractor` function over the content and style images loaded
    in the first exercise of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform the gram matrix calculation over style features. Consider that the
    style features were obtained from different layers, which is why different gram
    matrices should be created, one for each layer''s output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an initial target image. This image will be later compared against the
    content and style images and be changed until the desired similarity is achieved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is a good practice to create the initial target image as a copy of the content
    image. Moreover, it is essential to set it to require the calculation of gradients,
    considering that we want to be able to modify it in an iterative process until
    the content is similar to that of the content image and the style to that of the
    style image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the `tensor2image` function created during the first exercise of this
    chapter, plot the target image, which should look the same as the content image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output image is the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5: The target Image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.5: The target Image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have successfully performed feature extraction and the
    calculation of the gram matrix to perform the style transfer task.
  prefs: []
  type: TYPE_NORMAL
- en: The Optimization Algorithm, Losses, and Parameter Updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although style transfer is performed using a pretrained network where the parameters
    are left unchanged, creating the target image consists of an iterative process
    where three different loss functions are calculated and minimized by updating
    only the parameters related to the target image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, two different loss functions are calculated (the content and
    style losses), which are then put together to calculate a total loss function
    that is to be optimized to arrive at an appropriate target image. However, considering
    that measuring accuracy in terms of content and style is achieved very differently,
    the following is an explanation of the calculation of both the content and style
    loss functions, as well as a description on how the total loss is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content loss**'
  prefs: []
  type: TYPE_NORMAL
- en: This consists of a function that, based on the feature map obtained by a given
    layer, calculates the distance between the content image and the target image.
    In the case of the VGG-19 network, the content loss is only calculated based on
    the output from the `conv4_2` layer.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind the content loss function is to minimize the distance between
    the content image and the target image so that the latter highly resembles the
    former one in terms of content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content loss can be calculated as the mean squared difference between the
    feature maps of the content and target images at the relevant layer (`conv4_2`),
    which can be achieved using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: The content loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.6: The content loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Style loss**'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the content loss, the style loss is a function that measures the
    distance between the style and the target image in terms of style features (for
    instance, color and texture) by calculating the mean squared difference.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to the content loss, instead of comparing the feature maps derived
    from the different layers, it compares the gram matrices calculated based on the
    feature maps of both the style and the target image.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to mention that the style loss has to be calculated for all
    relevant layers (in this case, five layers) using a `for` loop. This will result
    in a loss function that considers simple and complex style representations from
    both images.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is a good practice to weigh the style representation of each
    of these layers between zero to one in order to give more emphasis to the layers
    that extract larger and simpler features over layers that extract very complex
    features. This is achieved by giving higher weights to earlier layers (`conv1_1`
    and `conv2_1`) that extract more generic features from the style image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this, the calculation of the style loss can be performed using
    the following equation for each of the relevant layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7: Style loss calculation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.7: Style loss calculation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Total loss**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the total loss function consists of a combination of both the content
    loss and the style loss. Its value is minimized during the iterative process of
    creating the target image by updating parameters of the target image.
  prefs: []
  type: TYPE_NORMAL
- en: Again, it is recommended to assign weights to the content and the style losses
    in order to determine their participation in the final output. This helps determine
    the degree at which the target image will be stylized, while making the content
    still visible. Considering this, it is a good practice to set the weight of the
    content loss as equal to one, whereas the one for the style loss must be much
    higher to achieve the ratio of your preference.
  prefs: []
  type: TYPE_NORMAL
- en: The weight assigned to the content loss is conventionally known as alpha, while
    the one given to the style loss is known as beta.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final equation to calculate the total loss can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: Total loss calculation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.8: Total loss calculation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the weights of the losses are defined, it is time to set the number of
    iteration steps, as well as the optimization algorithm which should only affect
    the target image. This means that, in every iteration step, all three losses will
    be calculated to then use the gradients to optimize the parameters associated
    to the target image, until the loss functions are minimized and a target function
    with the desired look is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the optimization of previous neural networks, the following are the steps
    followed in each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the features, both in terms of content and style, from the target image.
    In the initial iteration, this image will be an exact copy of the content image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the content loss. This is done comparing the content features map
    of the content and the target images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the average style loss of all relevant layers. This is achieved by
    comparing the gram matrices for all layers of both the style and target image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the total loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the partial derivatives of the total loss function in respect to the
    parameters (weights and biases) of the target image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until the desired number of iterations has been reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final output will be an image with content similar to the content image
    and a style similar to the style image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Creating the Target Image'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the final exercise of this chapter, the tasks of style transfer will be
    achieved. This exercise consists of coding the section in charge of performing
    the different iterations while optimizing the loss functions in order to arrive
    at an ideal target image. To do so, it is crucial to make use of the code bits
    programmed in the previous exercises of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the notebook from the previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a dictionary containing the weights for each of the layers in charge
    of extracting style features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Be sure to use the same name that you gave your layers in the previous exercise
    as keys.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the weights associated with the content and style losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the number of iteration steps, as well as the optimization algorithm.
    We can also set the number of iterations after we want to see a plot of the image
    that has been created to that point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The parameters to be updated by this optimization algorithm should be the parameters
    of the target image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Running 2,000 iterations, as per the example in this exercise, will take quite
    some time, depending on your resources. However, to reach an outstanding result
    in style transfer even more iterations are typically required (around 6,000, perhaps).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To appreciate the changes that occur to the target image from iteration to iteration,
    a couple of iterations will suffice, but you are encouraged to try training longer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `for` loop where all three loss functions will be calculated, and
    the optimization will be performed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot both the content and the target image to compare the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final image should look similar to the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.9: Comparison between content and target image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C11865_05_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.9: Comparison between content and target image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have successfully performed the style transfer task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Performing Style Transfer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will perform the task of style transfer. To do so, we
    will code all the concepts learned throughout this chapter. Let''s look at the
    following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are a globe trotter and you have decided to create a blog that documents
    your travels. However, you are also passionate about art and would like all your
    images to have an artistic look to it, that of a Monet painting. To be able to
    achieve that, you have decided to create a code that uses a pretrained neural
    network to perform a style transfer task:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the transformations to be performed over the input images. Be sure to
    resize them to the same size, convert them to tensor, and normalize them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an image loader function. It should open the image and load it. Call
    the image loader function to load both input images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to display the images, set the transformations to revert the normalization
    of the images and to convert the tensors into PIL images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function capable of performing the previous transformation over the
    tensors. Call the function for both images and plot the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the VGG-19 model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dictionary mapping the index of the relevant layers (keys) to a name
    (values). Then, create a function to extract the feature maps of the relevant
    layers. Use them to extract the features of both input images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gram matrix for the style features. Also, create the initial target
    image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the weights for the different style layers, as well as the weights for the
    content and style losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the model for 500 iterations. Define the Adam optimization algorithm before
    starting to train the model using 0.001 as the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Depending on your resources, the training process may take several hours, considering
    that, to achieve excellent results, it is recommended that you train for thousands
    of iterations. Adding print statements is a good practice to see the progress
    of the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: According to the previous information, the results of this chapter were achieved
    by running around 30,000 iterations, which will take a long time to run without
    a GPU (this configuration can be found on the GitHub's repository). However, to
    see some minor changes, it will suffice to run it for a couple of hundred iterations,
    as recommended in this activity (500).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot both the content and target images to compare the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 214.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter introduced style transfers, which is a popular task nowadays that
    can be solved using CNNs. It consists of taking both content and a style images
    as inputs and returning a newly created image as output that keeps the content
    of one of the images and the style of the other. It is typically used to give
    images an artistic look, by combining random regular images with those of the
    paintings of great artists.
  prefs: []
  type: TYPE_NORMAL
- en: Although style transfer is resolved using CNNs, the process of creating the
    new image is not achieved by training the network conventionally. In this chapter,
    it was explained how, by using pretrained networks, it is possible to consider
    the output of some relevant layers that are especially good at identifying certain
    features.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter explains each of the steps for developing a code capable of performing
    the task of style transfer, where the first step consisted of loading and displaying
    the inputs. As mentioned earlier, there are two inputs to the model (the content
    and style images). Each image is to go through a series of transformations that
    aims to resize the images to equal size, convert them into tensors, and normalize
    them in order for them to be properly processed by the network.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the pretrained model was loaded. As mentioned in this chapter, the VGG-19
    is one of the most typically used architectures to solve such tasks. It consists
    of 19 layers, including convolutional, pooling, and fully connected layers, where,
    for the task in question, only some of the convolutional layers are to be used.
    The process of loading the pretrained model is fairly simple, considering PyTorch
    provides a subpackage containing several pretrained network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, once the network is loaded, it was explained how certain layers
    of the network have been identified as overperformers at detecting certain features
    that are crucial for style transfer. While five different layers have the capability
    to extract features related to the style of an image, such as colors and textures,
    just one of the layers is exceptionally good at extracting content features, such
    as edges and shapes. According to this, it is crucial to define those relevant
    layers, which will be used to extract the information from the input images in
    order to create the desired target image.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it was time to code the iterative process capable of creating a target
    image with the desired features. To do so, three different losses were calculated.
    One for comparing the difference between the content image and the target image
    in terms of content (content loss), another one for comparing the difference in
    terms of style between the style image and the target image, which is achieved
    by the calculation of the gram matrix (the style loss). Lastly, one that combines
    both the losses (the total loss).
  prefs: []
  type: TYPE_NORMAL
- en: The target image was then achieved by minimizing the value of the total loss,
    which can be done by updating the parameters related to the target image. Although
    a pretrained network is used, the process of arriving at an ideal target image
    may take several thousand iterations and quite some time.
  prefs: []
  type: TYPE_NORMAL
