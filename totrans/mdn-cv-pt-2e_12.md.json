["```py\n    !pip install -qU openimages torch_snippets \n    ```", "```py\n    from torch_snippets import *\n    !wget -O train-annotations-object-segmentation.csv -q https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv\n    !wget -O classes.csv -q \\\n    https://raw.githubusercontent.com/openimages/dataset/master/dict.csv \n    ```", "```py\n    required_classes = 'person,dog,bird,car,elephant,football,\\\n    jug,laptop,mushroom,pizza,rocket,shirt,traffic sign,\\\n    Watermelon,Zebra'\n    required_classes = [c.lower() for c in \\\n                            required_classes.lower().split(',')]\n    classes = pd.read_csv('classes.csv', header=None)\n    classes.columns = ['class','class_name']\n    classes = classes[classes['class_name'].map(lambda x: x \\\n                               in required_classes)] \n    ```", "```py\n    from torch_snippets import *\n    df = pd.read_csv('train-annotations-object-segmentation.csv')\n    data = pd.merge(df, classes, left_on='LabelName', \n                    right_on='class')\n    subset_data = data.groupby('class_name').agg( \\\n                            {'ImageID': lambda x: list(x)[:500]})\n    subset_data = flatten(subset_data.ImageID.tolist())\n    subset_data = data[data['ImageID'].map(lambda x: x in subset_data)]\n    subset_masks = subset_data['MaskPath'].tolist() \n    ```", "```py\n    !mkdir -p masks\n    for c in Tqdm('0123456789abcdef'):\n        !wget -q \\\n         https://storage.googleapis.com/openimages/v5/train-masks/train-masks-{c}.zip\n        !unzip -q train-masks-{c}.zip -d tmp_masks\n        !rm train-masks-{c}.zip\n        tmp_masks = Glob('tmp_masks', silent=True)\n        items = [(m,fname(m)) for m in tmp_masks]\n        items = [(i,j) for (i,j) in items if j in subset_masks]\n        for i,j in items:\n            os.rename(i, f'masks/{j}')\n        !rm -rf tmp_masks \n    ```", "```py\n    masks = Glob('masks')\n    masks = [fname(mask) for mask in masks]\n    subset_data=subset_data[subset_data['MaskPath'].map(lambda \\\n                                   x: x in masks)]\n    subset_imageIds = subset_data['ImageID'].tolist()\n    from openimages.download import _download_images_by_id\n    !mkdir images\n    _download_images_by_id(subset_imageIds, 'train', './images/') \n    ```", "```py\n    import zipfile\n    files = Glob('images') + Glob('masks') + \\\n    ['train-annotations-object-segmentation.csv', 'classes.csv']\n    with zipfile.ZipFile('data.zip','w') as zipme:\n        for file in Tqdm(files):\n            zipme.write(file,compress_type=zipfile.ZIP_DEFLATED) \n    ```", "```py\n    !mkdir -p train/\n    !mv images train/myData2020\n    !mv masks train/annotations \n    ```", "```py\n    !pip install git+git://github.com/sizhky/pycococreator.git@0.2.0\n    import datetime\n    INFO = {\n        \"description\": \"MyData2020\",\n        \"url\": \"None\",\n        \"version\": \"1.0\",\n        \"year\": 2020,\n        \"contributor\": \"sizhky\",\n        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n    }\n    LICENSES = [\n        {\n            \"id\": 1,\n            \"name\": \"MIT\"\n        }\n    ]\n    CATEGORIES = [{'id': id+1, 'name': name.replace('/',''), \\\n                   'supercategory': 'none'} \\\n                  for id,(_,(name, clss_name)) in \\\n                  enumerate(classes.iterrows())] \n    ```", "```py\n    !pip install pycocotools\n    from pycococreatortools import pycococreatortools\n    from os import listdir\n    from os.path import isfile, join\n    from PIL import Image\n    coco_output = {\n        \"info\": INFO,\n        \"licenses\": LICENSES,\n        \"categories\": CATEGORIES,\n        \"images\": [],\n        \"annotations\": []\n    } \n    ```", "```py\n    ROOT_DIR = \"train\"\n    IMAGE_DIR, ANNOTATION_DIR = 'train/myData2020/', 'train/annotations/'\n    image_files = [f for f in listdir(IMAGE_DIR) if \\\n                   isfile(join(IMAGE_DIR, f))]\n    annotation_files = [f for f in listdir(ANNOTATION_DIR) if \\\n                        isfile(join(ANNOTATION_DIR, f))] \n    ```", "```py\n    image_id = 1\n    # go through each image\n    for image_filename in Tqdm(image_files):\n        image = Image.open(IMAGE_DIR + '/' + image_filename)\n        image_info = pycococreatortools.create_image_info(image_id, \\\n                    os.path.basename(image_filename), image.size)\n        coco_output[\"images\"].append(image_info)\n        image_id = image_id + 1 \n    ```", "```py\n    segmentation_id = 1\n    for annotation_filename in Tqdm(annotation_files):\n        image_id = [f for f in coco_output['images'] if \\\n                    stem(f['file_name']) == \\\n                    annotation_filename.split('_')[0]][0]['id']\n        class_id = [x['id'] for x in CATEGORIES \\\n                    if x['name'] in annotation_filename][0]\n        category_info = {'id': class_id, \\\n                        'is_crowd': 'crowd' in image_filename}\n        binary_mask = np.asarray(Image.open(f'{ANNOTATION_DIR}/\\\n    {annotation_filename}').convert('1')).astype(np.uint8)\n\n        annotation_info = pycococreatortools\\\n                        .create_annotation_info( \\\n                        segmentation_id, image_id, \n                        category_info, \n                        binary_mask, image.size, tolerance=2)\n        if annotation_info is not None:\n            coco_output[\"annotations\"].append(annotation_info)\n            segmentation_id = segmentation_id + 1 \n    ```", "```py\n    coco_output['categories'] = [{'id': id+1,'name':clss_name, \\\n                                  'supercategory': 'none'} for \\\n                                 id,(_,(name, clss_name)) in \\\n                                 enumerate(classes.iterrows())]\n    import json\n    with open('images.json', 'w') as output_json_file:\n        json.dump(coco_output, output_json_file) \n    ```", "```py\n    %cd /content/\n    # install detectron2:\n    !git clone https://github.com/facebookresearch/detectron2\n    %cd /content/detectron2\n    %pip install -r requirements.txt\n    !python setup.py install\n    %pip install git+https://github.com/facebookresearch/fvcore.git\n    %cd /content/ \n    ```", "```py\n    from detectron2 import model_zoo\n    from detectron2.engine import DefaultPredictor\n    from detectron2.config import get_cfg\n    from detectron2.utils.visualizer import Visualizer\n    from detectron2.data import MetadataCatalog, DatasetCatalog\n    from detectron2.engine import DefaultTrainer \n    ```", "```py\n    from torch_snippets import *\n    required_classes= 'person,dog,bird,car,elephant,football,jug,\\\n    laptop,Mushroom,Pizza,Rocket,Shirt,Traffic sign,\\\n    Watermelon,Zebra'\n    required_classes = [c.lower() for c in \\\n                        required_classes.lower().split(',')]\n    classes = pd.read_csv('classes.csv', header=None)\n    classes.columns = ['class','class_name']\n    classes = classes[classes['class_name'].map(lambda \\\n                                    x: x in required_classes)] \n    ```", "```py\n    from detectron2.data.datasets import register_coco_instances\n    register_coco_instances(\"dataset_train\", {}, \\\n                            \"images.json\", \"train/myData2020\") \n    ```", "```py\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-\\ InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n    cfg.DATASETS.TRAIN = (\"dataset_train\",)\n    cfg.DATASETS.TEST = ()\n    cfg.DATALOADER.NUM_WORKERS = 2\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-\\ InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") # pretrained \n    # weights\n    cfg.SOLVER.IMS_PER_BATCH = 2\n    cfg.SOLVER.BASE_LR = 0.00025 # pick a good LR\n    cfg.SOLVER.MAX_ITER = 5000 # instead of epochs, we train on \n    # 5000 batches\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes) \n    ```", "```py\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    trainer = DefaultTrainer(cfg) \n    trainer.resume_or_load(resume=False)\n    trainer.train() \n    ```", "```py\n    !cp output/model_final.pth output/trained_model.pth \n    ```", "```py\n    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"trained_model.pth\") \n    ```", "```py\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25 \n    ```", "```py\n    predictor = DefaultPredictor(cfg) \n    ```", "```py\n    from detectron2.utils.visualizer import ColorMode\n    files = Glob('train/myData2020')\n    for _ in range(30):\n        im = cv2.imread(choose(files))\n        outputs = predictor(im)\n        v = Visualizer(im[:, :, ::-1], scale=0.5, \n                        metadata=MetadataCatalog.get(\"dataset_train\"), \n                        instance_mode=ColorMode.IMAGE_BW \n    # remove the colors of unsegmented pixels. \n    # This option is only available for segmentation models\n        )\n        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n        show(out.get_image()) \n    ```", "```py\n    %cd /content/\n    # install detectron2:\n    !git clone https://github.com/facebookresearch/detectron2\n    %cd /content/detectron2\n    %pip install -r requirements.txt\n    !python setup.py install\n    %pip install git+https://github.com/facebookresearch/fvcore.git\n    %cd /content/\n    %pip install torch_snippets\n    %pip install pyyaml==5.1 pycocotools>=2.0.1\n    from torch_snippets import *\n    import detectron2\n    from detectron2.utils.logger import setup_logger\n    setup_logger()\n    from detectron2 import model_zoo\n    from detectron2.engine import DefaultPredictor\n    from detectron2.config import get_cfg\n    from detectron2.utils.visualizer import Visualizer\n    from detectron2.data import MetadataCatalog, DatasetCatalog \n    ```", "```py\n    cfg = get_cfg() # get a fresh new config\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")) \n    ```", "```py\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold \n    # for this model\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n    predictor = DefaultPredictor(cfg) \n    ```", "```py\n    from torch_snippets import read, resize\n    !wget -q https://i.imgur.com/ldzGSHk.jpg -O image.png\n    im = read('image.png',1)\n    im = resize(im, 0.5) # resize image to half its dimensions \n    ```", "```py\n    outputs = predictor(im)\n    v = Visualizer(im[:,:,::-1], \n                   MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), \n                   scale=1.2)\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    plt.imshow(out.get_image()) \n    ```", "```py\n    %%writefile kaggle.json\n    {\"username\":\"xx\",\"key\":\"xx\"}\n    !mkdir -p ~/.kaggle\n    !mv kaggle.json ~/.kaggle/\n    !ls ~/.kaggle\n    !chmod 600 /root/.kaggle/kaggle.json\n    %%time\n    %cd /content\n    import os\n    if not os.path.exists('shanghaitech-with-people-density-map'):\n        print('downloading data...')\n        !kaggle datasets download -d tthien/shanghaitech-with-people-density-map/\n        print('unzipping data...')\n        !unzip -qq shanghaitech-with-people-density-map.zip\n    if not os.path.exists('CSRNet-pytorch/'):\n        %pip install -U scipy torch_snippets torch_summary\n        !git clone https://github.com/sizhky/CSRNet-pytorch.git\n    %cd CSRNet-pytorch\n    !ln -s ../shanghaitech_with_people_density_map\n    from torch_snippets import *\n    import h5py\n    from scipy import io \n    ```", "```py\n    part_A = Glob('shanghaitech_with_people_density_map/\\\n    ShanghaiTech/part_A/train_data/');\n    image_folder = 'shanghaitech_with_people_density_map/\\\n    ShanghaiTech/part_A/train_data/images/'\n    heatmap_folder = 'shanghaitech_with_people_density_map/\\\n    ShanghaiTech/part_A/train_data/ground-truth-h5/'\n    gt_folder = 'shanghaitech_with_people_density_map/\\\n    ShanghaiTech/part_A/train_data/ground-truth/' \n    ```", "```py\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    tfm = T.Compose([\n        T.ToTensor()\n    ])\n    class Crowds(Dataset):\n        def __init__(self, stems):\n            self.stems = stems\n        def __len__(self):\n            return len(self.stems)\n        def __getitem__(self, ix):\n            _stem = self.stems[ix]\n            image_path = f'{image_folder}/{_stem}.jpg'\n            heatmap_path = f'{heatmap_folder}/{_stem}.h5'\n            gt_path = f'{gt_folder}/GT_{_stem}.mat'\n            pts = io.loadmat(gt_path)\n            pts = len(pts['image_info'][0,0][0,0][0])\n            image = read(image_path, 1)\n            with h5py.File(heatmap_path, 'r') as hf:\n                gt = hf['density'][:]\n            **gt = resize(gt,** **1****/****8****)*****64**\n            return image.copy(), gt.copy(), pts\n        def collate_fn(self, batch):\n            ims, gts, pts = list(zip(*batch))\n            ims = torch.cat([tfm(im)[None] for im in ims]).to(device)\n            gts = torch.cat([tfm(gt)[None] for gt in gts]).to(device)\n            return ims, gts, torch.tensor(pts).to(device)\n        def choose(self):\n            return self[randint(len(self))]\n    from sklearn.model_selection import train_test_split\n    trn_stems, val_stems = train_test_split(stems(Glob(image_folder)),\n                                                     random_state=10)\n    trn_ds = Crowds(trn_stems)\n    val_ds = Crowds(val_stems)\n    trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True, \n                        collate_fn=trn_ds.collate_fn)\n    val_dl = DataLoader(val_ds, batch_size=1, shuffle=True, \n                        collate_fn=val_ds.collate_fn) \n    ```", "```py\n    import torch.nn as nn\n    import torch\n    from torchvision import models\n    from utils import save_net,load_net\n    def make_layers(cfg, in_channels = 3, batch_norm=False, dilation = False):\n        if dilation:\n            d_rate = 2\n        else:\n            d_rate = 1\n        layers = []\n        for v in cfg:\n            if v == 'M': # M is for maxpooling\n                layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels,v,kernel_size=3,\n                                   padding=d_rate, dilation=d_rate)\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), \n                               nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers) \n    ```", "```py\n    class CSRNet(nn.Module):\n        def __init__(self, load_weights=False):\n            super(CSRNet, self).__init__()\n            self.seen = 0\n            self.frontend_feat = [64, 64, 'M', 128, 128, \n                                  'M',256, 256, 256, 'M', 512, 512, 512]\n            self.backend_feat = [512, 512, 512, 256, 128, 64]\n            self.frontend = make_layers(self.frontend_feat)\n            self.backend = make_layers(self.backend_feat, in_channels = 512,\n                                       dilation = True)\n            self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n            if not load_weights:\n                mod = models.vgg16(pretrained = True)\n                self._initialize_weights()\n                items = list(self.frontend.state_dict().items())\n                _items = list(mod.state_dict().items())\n                for i in range(len(self.frontend.state_dict().items())):\n                    items[i][1].data[:] = _items[i][1].data[:]\n        def forward(self,x):\n            x = self.frontend(x)\n            x = self.backend(x)\n            x = self.output_layer(x)\n            return x\n        def _initialize_weights(self):\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.normal_(m.weight, std=0.01)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0) \n    ```", "```py\n    def train_batch(model, data, optimizer, criterion):\n        model.train()\n        optimizer.zero_grad()\n        ims, gts, pts = data\n        _gts = model(ims)\n        loss = criterion(_gts, gts)\n        loss.backward()\n        optimizer.step()\n        pts_loss = nn.L1Loss()(_gts.sum(), gts.sum())\n        return loss.item(), pts_loss.item()\n    @torch.no_grad()\n    def validate_batch(model, data, criterion):\n        model.eval()\n        ims, gts, pts = data\n        _gts = model(ims)\n        loss = criterion(_gts, gts)\n        pts_loss = nn.L1Loss()(_gts.sum(), gts.sum())\n        return loss.item(), pts_loss.item() \n    ```", "```py\n    model = CSRNet().to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n    n_epochs = 20\n    log = Report(n_epochs)\n    for ex in range(n_epochs):\n        N = len(trn_dl)\n        for bx, data in enumerate(trn_dl):\n            loss,pts_loss=train_batch(model, data, optimizer, criterion)\n            log.record(ex+(bx+1)/N, trn_loss=loss, \n                               trn_pts_loss=pts_loss, end='\\r')\n        N = len(val_dl)\n        for bx, data in enumerate(val_dl):\n            loss, pts_loss = validate_batch(model, data, criterion)\n            log.record(ex+(bx+1)/N, val_loss=loss, \n                        val_pts_loss=pts_loss, end='\\r')\n        log.report_avgs(ex+1)\n        if ex == 10: optimizer = optim.Adam(model.parameters(), \n                                            lr=1e-7) \n    ```", "```py\n    from matplotlib import cm as c\n    from torchvision import datasets, transforms\n    from PIL import Image\n    transform=transforms.Compose([\n                     transforms.ToTensor(),transforms.Normalize(\\\n                              mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225]),\n                      ])\n    test_folder = 'shanghaitech_with_people_density_map/\\\n    ShanghaiTech/part_A/test_data/'\n    imgs = Glob(f'{test_folder}/images')\n    f = choose(imgs)\n    print(f)\n    img = transform(Image.open(f).convert('RGB')).to(device) \n    ```", "```py\n    output = model(img[None])\n    print(\"Predicted Count : \", int(output.detach().cpu().sum().numpy()))\n    temp = np.asarray(output.detach().cpu()\\\n                        .reshape(output.detach().cpu()\\\n                        .shape[2],output.detach()\\\n                        .cpu().shape[3]))\n    plt.imshow(temp,cmap = c.jet)\n    plt.show() \n    ```", "```py\n    !pip install torch_snippets\n    from torch_snippets import *\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    from torchvision import datasets\n    import torch\n    data_folder = '~/cifar10/cifar/' \n    datasets.CIFAR10(data_folder, download=True) \n    ```", "```py\n    class Colorize(torchvision.datasets.CIFAR10):\n        def __init__(self, root, train):\n            super().__init__(root, train)\n\n        def __getitem__(self, ix):\n            im, _ = super().__getitem__(ix)\n            bw = im.convert('L').convert('RGB')\n            bw, im = np.array(bw)/255., np.array(im)/255.\n            bw, im = [torch.tensor(i).permute(2,0,1)\\\n                      .to(device).float() for i in [bw,im]]\n            return bw, im\n    trn_ds = Colorize('~/cifar10/cifar/', train=True)\n    val_ds = Colorize('~/cifar10/cifar/', train=False)\n    trn_dl = DataLoader(trn_ds, batch_size=256, shuffle=True)\n    val_dl = DataLoader(val_ds, batch_size=256, shuffle=False) \n    ```", "```py\n    a,b = trn_ds[0]\n    subplots([a,b], nc=2) \n    ```", "```py\n    class Identity(nn.Module):\n        def __init__(self):\n            super().__init__()\n        def forward(self, x):\n            return x\n    class DownConv(nn.Module):\n        def __init__(self, ni, no, maxpool=True):\n            super().__init__()\n            self.model = nn.Sequential(\n                nn.MaxPool2d(2) if maxpool else Identity(),\n                nn.Conv2d(ni, no, 3, padding=1),\n                nn.BatchNorm2d(no),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(no, no, 3, padding=1),\n                nn.BatchNorm2d(no),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        def forward(self, x):\n            return self.model(x)\n    class UpConv(nn.Module):\n        def __init__(self, ni, no, maxpool=True):\n            super().__init__()\n            self.convtranspose = nn.ConvTranspose2d(ni, no, 2, stride=2)\n            self.convlayers = nn.Sequential(\n                nn.Conv2d(no+no, no, 3, padding=1),\n                nn.BatchNorm2d(no),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(no, no, 3, padding=1),\n                nn.BatchNorm2d(no),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n\n        def forward(self, x, y):\n            x = self.convtranspose(x)\n            x = torch.cat([x,y], axis=1)\n            x = self.convlayers(x)\n            return x\n    class UNet(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.d1 = DownConv( 3, 64, maxpool=False)\n            self.d2 = DownConv( 64, 128)\n            self.d3 = DownConv( 128, 256)\n            self.d4 = DownConv( 256, 512)\n            self.d5 = DownConv( 512, 1024)\n            self.u5 = UpConv (1024, 512)\n            self.u4 = UpConv ( 512, 256)\n            self.u3 = UpConv ( 256, 128)\n            self.u2 = UpConv ( 128, 64)\n            self.u1 = nn.Conv2d(64, 3, kernel_size=1, stride=1)\n        def forward(self, x):\n            x0 = self.d1( x) # 32\n            x1 = self.d2(x0) # 16\n            x2 = self.d3(x1) # 8\n            x3 = self.d4(x2) # 4\n            x4 = self.d5(x3) # 2\n            X4 = self.u5(x4, x3)# 4\n            X3 = self.u4(X4, x2)# 8\n            X2 = self.u3(X3, x1)# 16\n            X1 = self.u2(X2, x0)# 32\n            X0 = self.u1(X1) # 3\n            return X0 \n    ```", "```py\n    def get_model():\n        model = UNet().to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.MSELoss()\n        return model, optimizer, loss_fn \n    ```", "```py\n    def train_batch(model, data, optimizer, criterion):\n        model.train()\n        x, y = data\n        _y = model(x)\n        optimizer.zero_grad()\n        loss = criterion(_y, y)\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    @torch.no_grad()\n    def validate_batch(model, data, criterion):\n        model.eval()\n        x, y = data\n        _y = model(x)\n        loss = criterion(_y, y)\n        return loss.item() \n    ```", "```py\n    model, optimizer, criterion = get_model()\n    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n                                        step_size=10, gamma=0.1)\n    _val_dl = DataLoader(val_ds, batch_size=1, shuffle=True)\n    n_epochs = 100\n    log = Report(n_epochs)\n    for ex in range(n_epochs):\n        N = len(trn_dl)\n        for bx, data in enumerate(trn_dl):\n            loss = train_batch(model, data, optimizer,criterion)\n            log.record(ex+(bx+1)/N, trn_loss=loss, end='\\r')\n            if (bx+1)%50 == 0:\n                for _ in range(5):\n                    a,b = next(iter(_val_dl))\n                    _b = model(a)\n                    subplots([a[0], b[0], _b[0]], nc=3, figsize=(5,5))\n        N = len(val_dl)\n        for bx, data in enumerate(val_dl):\n            loss = validate_batch(model, data, criterion)\n            log.record(ex+(bx+1)/N, val_loss=loss, end='\\r')\n\n        exp_lr_scheduler.step()\n        if (ex+1) % 5 == 0: log.report_avgs(ex+1)\n        for _ in range(5):\n            a,b = next(iter(_val_dl))\n            _b = model(a)\n            subplots([a[0], b[0], _b[0]], nc=3, figsize=(5,5))\n    log.plot_epochs() \n    ```", "```py\nfiles = Glob('training/velodyne')\nF = choose(files)\npts = np.fromfile(F, dtype=np.float32).reshape(-1, 4)\npts \n```", "```py\n# take the points and remove faraway points\nx,y,z = np.clip(pts[:,0], 0, 50), \n        np.clip(pts[:,1], -25, 25), \n        np.clip(pts[:,2],-3, 1.27)\nfig = go.Figure(data=[go.Scatter3d(\\\n        x=x, y=y, z=z, mode='markers',\n        marker=dict(\n            size=2,\n            color=z, # set color to a list of desired values\n            colorscale='Viridis', # choose a colorscale\n            opacity=0.8\n        )\n    )])\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show() \n```", "```py\n$ git clone https://github.com/sizhky/Complex-YOLOv4-Pytorch \n```", "```py\n$ cd Complex-YOLOv4-Pytorch/src/data_process\n$ python kitti_dataloader.py --output-width 600 \n```", "```py\n$ cd Complex-YOLOv4-Pytorch/src\n$ python train.py --gpu_idx 0 --batch_size 2 --num_workers 4 \\\n                  --num_epochs 5 \n```", "```py\n$ cd Complex-YOLOv4-Pytorch/src\n$ python test.py --gpu_idx 0 --pretrained_path ../checkpoints/complexer_yolo/Model_complexer_yolo_epoch_5.pth --cfgfile ./config/cfg/complex_yolov4.cfg --show_image \n```", "```py\n    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n    %pip install -U \"openmim==0.3.9\"\n    !mim install -U \"mmengine==0.10.4\"\n    !mim install \"mmcv==2.2.0\"\n    !git clone https://github.com/sizhky/mmaction2.git -b main\n    %cd mmaction2\n    %pip install -e .\n    %pip install -r requirements/optional.txt\n    %pip install \"timm==0.9.16\"\n    %pip install \"torch-snippets==0.528\" lovely-tensors \n    ```", "```py\n    !mkdir checkpoints\n    !wget -c https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth -O       checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \n    ```", "```py\n    from mmaction.apis import inference_recognizer, init_recognizer\n    from mmengine import Config\n    from torch_snippets import *\n    from builtins import print \n    ```", "```py\n    # Choose to use a config and initialize the recognizer\n    config = 'configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py'\n    config = Config.fromfile(config) \n    ```", "```py\n    # Setup a checkpoint file to load\n    checkpoint = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n    # Initialize the recognizer\n    model = init_recognizer(config, checkpoint, device='cuda:0') \n    ```", "```py\n    from operator import itemgetter\n    video = 'demo/demo.mp4'\n    label = 'tools/data/kinetics/label_map_k400.txt'\n    results = inference_recognizer(model, video)\n    pred_scores = results.pred_score.cpu().numpy().tolist()\n    score_tuples = tuple(zip(range(len(pred_scores)), pred_scores))\n    score_sorted = sorted(score_tuples, key=itemgetter(1), reverse=True)\n    top5_label = score_sorted[:5]\n    labels = open(label).readlines()\n    labels = [x.strip() for x in labels]\n    results = [(labels[k[0]], k[1]) for k in top5_label] \n    ```", "```py\n    for result in results:\n        print(f'{result[0]}: ', result[1]) \n    ```", "```py\n    # download, decompress the data\n    !rm kinetics400_tiny.zip*\n    !rm -rf kinetics400_tiny\n    !wget https://download.openmmlab.com/mmaction/kinetics400_tiny.zip\n    !unzip kinetics400_tiny.zip > /dev/null \n    ```", "```py\n    !cat kinetics400_tiny/kinetics_tiny_train_video.txt \n    ```", "```py\n    cfg = Config.fromfile('./configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py') \n    ```", "```py\n    from mmengine.runner import set_random_seed\n    # Modify dataset type and path\n    cfg.data_root = 'kinetics400_tiny/train/'\n    cfg.data_root_val = 'kinetics400_tiny/val/'\n    cfg.ann_file_train = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n    cfg.ann_file_val = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n    cfg.test_dataloader.dataset.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n    cfg.test_dataloader.dataset.data_prefix.video = 'kinetics400_tiny/val/'\n    cfg.train_dataloader.dataset.ann_file = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n    cfg.train_dataloader.dataset.data_prefix.video = 'kinetics400_tiny/train/'\n    cfg.val_dataloader.dataset.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n    cfg.val_dataloader.dataset.data_prefix.video  = 'kinetics400_tiny/val/'\n    # Modify num classes of the model in cls_head\n    cfg.model.cls_head.num_classes = 2\n    # We can use the pre-trained TSN model\n    cfg.load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n    # Set up working dir to save files and logs.\n    cfg.work_dir = './output_dir'\n    cfg.train_dataloader.batch_size = cfg.train_dataloader.batch_size // 16\n    cfg.val_dataloader.batch_size = cfg.val_dataloader.batch_size // 16\n    cfg.optim_wrapper.optimizer.lr = cfg.optim_wrapper.optimizer.lr / 8 / 16\n    cfg.train_cfg.max_epochs = 10\n    cfg.train_dataloader.num_workers = 2\n    cfg.val_dataloader.num_workers = 2\n    cfg.test_dataloader.num_workers = 2 \n    ```", "```py\n    import os.path as osp\n    import mmengine\n    from mmengine.runner import Runner\n    # Create work_dir\n    mmengine.mkdir_or_exist(osp.abspath(cfg.work_dir))\n    # build the runner from config\n    runner = Runner.from_cfg(cfg)\n    # start training\n    runner.train() \n    ```", "```py\n    runner.test() \n    ```"]