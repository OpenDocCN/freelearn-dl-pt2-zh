- en: Implementing Policy Gradients and Policy Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on policy gradient methods as one of the most
    popular reinforcement learning techniques over recent years. We will start with
    implementing the fundamental REINFORCE algorithm and will proceed with an improvement
    algorithm baseline. We will also implement a more powerful algorithm, actor-critic,
    and its variations, and apply it to solve the CartPole and Cliff Walking problems.
    We will also experience an environment with continuous action space and resort
    to Gaussian distribution to solve it. By way of a fun section at the end, we will
    train an agent based on the cross-entropy method to play the CartPole game.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the REINFORCE algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the REINFORCE algorithm with baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the actor-critic algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving Cliff Walking with the actor-critic algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the continuous Mountain Car environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the continuous Mountain Car environment with the advantage actor-critic
    network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing CartPole through the cross-entropy method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the REINFORCE algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recent publication stipulated that policy gradient methods are becoming more
    and more popular. Their learning goal is to optimize the probability distribution
    of actions so that given a state, a more rewarding action will have a higher probability
    value. In the first recipe of the chapter, we will talk about the REINFORCE algorithm,
    which is foundational to advanced policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**The REINFORCE** algorithm is also known as the **Monte Carlo policy gradient**,
    as it optimizes the policy based on Monte Carlo methods. Specifically, it collects
    trajectory samples from one episode using its current policy and uses them to
    the policy parameters, θ . The learning objective function for policy gradients
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be1f51e8-dae6-47fc-887a-dab7ec8b8735.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Its gradient can be derived as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e464bcc7-6206-4b29-82e9-1d8e2399292a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/882979d6-0eff-4d04-aed6-2b874834e4bf.png)] is the return, which
    is the cumulative discounted reward until time, t, [![](img/cf46b0c7-4df9-4378-ad71-978797d59fa4.png)]and
    is the stochastic policy, which determines the probabilities of taking certain
    actions at a given state. Since a policy update is conducted after the entire
    episode finishes and all samples are collected, REINFORCE is an off-policy algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: After we compute the policy gradients, we use backpropagation to update the
    policy parameters. With the updated policy, we roll out an episode, collect a
    set of samples, and use them to repeatedly update the policy parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We will now develop the REINFORCE algorithm to solve the CartPole ([https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/))
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We develop the REINFORCE algorithm to solve the CartPole environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a CartPole instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the `__init__method` of the `PolicyNetwork` class, which
    approximates the policy using a neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the `predict` method, which computes the estimated policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now develop the training method, which updates the neural network with samples
    collected in an episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The final method for the `PolicyNetwork` class is `get_action`, which samples
    an action given a state based on the predicted policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It also returns the log probability of the selected action, which will be used
    as part of the training sample.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for the `PolicyNetwork` class!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can move on to developing the **REINFORCE** algorithm with a policy
    network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the discount factor as `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform learning with the REINFORCE algorithm using the policy network we
    just developed for 500 episodes, and we also keep track of the total rewards for
    each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now display the plot of episode reward over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 2*, we use a neural network with one hidden layer for simplicity. The
    input of the policy network is a state, followed by a hidden layer, while the
    output is the probability of taking possible individual actions. Therefore, we
    use the softmax function as the activation for the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 4* is for updating the network parameters: given all the data gathered
    in an episode, including the returns and the log probabilities of all steps, we
    compute the policy gradients, and then update the policy parameters accordingly
    via backpropagation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 6*, the REINFORCE algorithm does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It runs an episode: for each step in the episode, it samples an action based
    on the current estimated policy; it stores the reward and the log policy at each
    step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once an episode finishes, it calculates the discounted cumulative rewards at
    each step; it normalizes the resulting returns by subtracting their mean and then
    dividing them by their standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It computes policy gradients using the returns and log probabilities, and then
    updates the policy parameters. We also display the total reward for each episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 8* will generate the following training logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You will observe the following plot in *Step 9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a4cb660-d548-43f3-88f6-5a260d87dbff.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that most of the last 200 episodes have rewards with a maximum value
    of +200.
  prefs: []
  type: TYPE_NORMAL
- en: 'The REINFORCE algorithm is a family of policy gradient methods that updates
    the policy parameters directly through the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/854ef6cc-3c4a-4666-be4a-76a12b7337b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, α is the learning rate, [![](img/f6974151-1450-44d0-8d7c-a73695e2bade.png)],
    as the probability mappings of actions, and [![](img/478630e2-8421-4b5f-bf2a-7499715d4dfd.png)],
    as the cumulative discounted rewards, are experiences collected in an episode.
    Since the set of training samples is constructed only after the full episode is
    completed, learning in REINFORCE is in an off-policy manner. The learning process
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize the policy parameter, *θ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform an episode by selecting actions based on the current policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each step, store the log probability of the chosen action as well as the
    resulting reward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the returns for individual steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute policy gradients using log probabilities and returns, and update the
    policy parameter, θ, via backpropagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 2* to *5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, since the REINFORCE algorithm relies on a full trajectory generated by
    a stochastic policy, it constitutes a Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is quite tricky to derive the policy gradient equation. It utilizes the
    log-derivative trick. In case you are wondering, here is a detailed explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf](http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the REINFORCE algorithm with baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the REINFORCE algorithm, Monte Carlo plays out the whole trajectory in an
    episode that is used to update the policy afterward. However, the stochastic policy
    may take different actions at the same state in different episodes. This can confuse
    the training, since one sampled experience wants to increase the probability of
    choosing one action while another sampled experience may want to decrease it.
    To reduce this high variance problem in vanilla REINFORCE, we will develop a variation
    algorithm, REINFORCE with baseline, in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In REINFORCE with baseline, we subtract the baseline state-value from the return,
    G. As a result, we use an advantage function A in the gradient update, which is
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a840734-d2e4-437c-ae60-966118f045cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, V(s) is the value function that estimates the state-value given a state.
    Typically, we can use a linear function, or a neural network, to approximate the
    state-value. By introducing the baseline value, we can calibrate the rewards with
    respect to the average action given a state.
  prefs: []
  type: TYPE_NORMAL
- en: We develop the REINFORCE with baseline algorithm using two neural networks,
    one for policy, and another one for value estimation, in order to solve the CartPole
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the CartPole environment using the REINFORCE with baseline algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a CartPole instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For the policy network part, it is basically the same as the `PolicyNetwork`
    class we used in the *Implementing the REINFORCE algorithm* recipe. Keep in mind
    that the advantage values are used in the `update` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For the value network part, we use a regression neural network with one hidden
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Its learning goal is to approximate state-values; hence, we use the mean squared
    error as the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `update` method trains the value regression model with a set of input states
    and target outputs, via backpropagation of course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `predict` method estimates the state-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can move on to developing the REINFORCE with baseline algorithm with
    a policy and value network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the value network, we also set its size and create an instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the discount factor as `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform learning using the REINFORCE with baseline algorithm for 2,000 episodes,
    and we also keep track of the total rewards for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we display the plot of episode reward over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: REINFORCE relies heavily on Monte Carlo methods to generate a whole trajectory
    used to train the policy network. However, different actions may be taken in different
    episodes under the same stochastic policy. To reduce the variance for the sampled
    experience, we subtract the state-value from the return . The resulting advantage
    measures the reward relative to the average action, which will be used in the
    gradient update.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, REINFORCE with a baseline algorithm does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It runs an episode—es the state, reward, and the log policy at each step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once an episode finishes, it calculates the discounted cumulative reward at
    each step; it estimates the baseline values using the value network; it computes
    the advantage values by subtracting the baseline values from the returns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It computes policy gradients using the advantage values and log probabilities,
    and updates the policy and value networks. We also display the total reward for
    each episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Executing the code in *Step 7* will result in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f2957a0-aa31-4cce-9e87-a8e6d4ed9949.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the performance is very stable after around 1,200 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: With the additional value baseline, we are able to recalibrate the rewards and
    reduce variance on the gradient estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the REINFORCE with baseline algorithm, there are two separate components,
    the policy model and the value function. We can actually combine the learning
    of these two components, since the goal of learning the value function is to update
    the policy network. This is what the **actor-critic** algorithm does, and which
    we are going to develop in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network for the actor-critic algorithm consists of the following two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor**: This takes in the input state and outputs the action probabilities.
    Essentially, it learns the optimal policy by updating the model using information
    provided by the critic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Critic**: This evaluates how good it is to be at the input state by computing
    the value function. The value guides the actor on how it should adjust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two components share parameters of input and hidden layers in the network,
    as learning is more efficient in this way than learning them separately. Accordingly,
    the loss function is a summation of two parts, specifically, the negative log
    likelihood of action measuring the actor, and the mean squared error between the
    estimated and computed return measuring the critic.
  prefs: []
  type: TYPE_NORMAL
- en: A more popular version of the actor-critic algorithm is **Advantage Actor-Critic**
    (**A2C**). As its name implies, the critic part computes the advantage value,
    instead of the state-value, which is similar to REINFORCE with baseline. It evaluates
    how better an action is at a state compared to the other actions, and is known
    to reduce variance in policy networks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We develop the actor-critic algorithm in order to solve the CartPole environment
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a CartPole instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the actor-critic neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue with the `__init__` method of the `PolicyNetwork` class using the
    actor-critic neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use herein a learning rate reducer that allows a dynamic learning
    rate according to learning progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the `predict` method, which computes the estimated action probabilities
    and state-value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We now develop the `training` method, which updates the neural network with
    samples collected in an episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The final method for the `PolicyNetwork` class is get_action, which samples
    an action given a state based on the predicted policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It also returns the log probability of the selected action, as well as the estimated
    state-value.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for the `PolicyNetwork` class!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can move on to developing the main function, training an actor-critic
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the discount factor as `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform learning with the actor-critic algorithm using the policy network
    we just developed for 1,000 episodes, and we also keep track of the total rewards
    for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we display the plot of episode reward over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in *Step 2*, the actor and critic share parameters of the input
    and the hidden layers; the output of the actor consists of the probability of
    taking individual actions, and the output of the critic is the estimated value
    of the input state.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we compute the advantage value and its negative log likelihood.
    The loss function in actor-critic is the combination of the negative log likelihood
    of advantage and the mean squared error between the return and estimated state-value.
    Note that we use `smooth_l1_loss`, which is a squared term, if the absolute error
    falls below 1, and an absolute error otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 7*, the training function for the actor-critic model carries out the
    following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It runs an episode: for each step in the episode, it samples an action based
    on the current estimated policy; it stores the reward, log policy, and estimated
    state-value at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once an episode finishes, it calculates the discounted cumulative rewards at
    each step; it normalizes the resulting returns by subtracting their mean and then
    dividing them by their standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It updates policy parameters using the returns, log probabilities, and state-values.
    We also display the total reward for each episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the total reward for an episode is more than +195, we reduce the learning
    rate slightly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will see the following logs after executing the training in *Step 9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is the result of *Step 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f41226b6-6f40-4a12-899d-0d2507546eb7.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the rewards after around 400 episodes stay in the maximum value
    of +200.
  prefs: []
  type: TYPE_NORMAL
- en: In the advantage actor-critic algorithm, we decompose learning into two pieces
    – actor and critic. Critic in A2C evaluates how good an action is at a state,
    which guides the actor on how it should react. Again, the advantage value is computed
    as A(s,a) = Q(s,a) -V(s), which means subtracting state-values from Q values.
    Actor estimates the action probabilities based on critic's guidance. The introduction
    of advantage can reduce variance, and hence, A2C is considered a more stable model
    than the standard actor-critic. As we can see in the CartPole environment, the
    performance of A2C has been consistent after the training of several hundred episodes.
    It outperforms REINFORCE with baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Solving Cliff Walking with the actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's solve a more complicated Cliff Walking environment using
    the A2C algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Cliff Walking is a typical Gym environment with long episodes without a guarantee
    of termination. It is a grid problem with a 4 * 12 board. An agent makes a move
    of up, right, down and left at a step. The bottom-left tile is the starting point
    for the agent, and the bottom-right is the winning point where an episode will
    end if it is reached. The remaining tiles in the last row are cliffs where the
    agent will be reset to the starting position after stepping on any of them, but
    the episode continues. Each step the agent takes incurs a -1 reward, with the
    exception of stepping on the cliffs, where a -100 reward incurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state is an integer from 0 to 47, indicating where the agent is located,
    as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f3f091a-97ef-4fee-8d51-45257ee50773.png)'
  prefs: []
  type: TYPE_IMG
- en: Such value does not contain a numerical meaning. For example, being at state
    30 does not mean it is 3 times different from being in state 10\. Hence, we will
    first convert it into a one-hot encoded vector before feeding the state to the
    policy network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve Cliff Walking using the A2C algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a CartPole instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As the state becomes 48-dimension, we use a more complicated actor-critic neural
    network with two hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Again, the actor and critic share parameters of the input and hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: We continue with the `PolicyNetwork` class using the actor-critic neural network
    we just developed in *Step 2*. It is the same as the `PolicyNetwork` class in
    the *Implementing the actor-critic algorithm* recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we develop the main function, training an actor-critic model. It is almost
    the same as the one in the *Implementing the actor-critic algorithm* recipe, with
    the additional transformation of state into a one-hot encoded vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And we set the discount factor as `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform learning with the actor-critic algorithm using the policy network
    we just developed for 1,000 episodes, and we also keep track of the total rewards
    for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we display the plot of episode reward over time since the 100th episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may notice in *Step 4* that we reduce the learning rate slightly if the
    total reward for an episode is more than -14\. A reward of -13 is the maximum
    value we are able to achieve, by taking the path 36-24-25-26-27-28-29-30-31-32-33-34-35-47\.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following logs after executing the training in *Step 6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is the result of *Step 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d2e58d4-093b-42d1-9f0f-202d1a669b37.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe, after around the 180th episode, rewards in most episodes
    achieve the optimal value, -13.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we solved the Cliff Walking problem with the A2C algorithm.
    As the integer state from 0 to 47 represents the location of the agent in the
    4*12 board, which doesn't contain numerical meaning, we first converted it into
    a one-hot encoded vector of 48 dimensions. To deal with the input of 48 dimensions,
    we use a slightly more complicated neural network with two hidden layers. A2C
    has proved to be a stable policy method in our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the continuous Mountain Car environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the environments we have worked on have discrete action values, such
    as 0 or 1, representing up or down, left or right. In this recipe, we will experience
    a Mountain Car environment with continuous actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous Mountain Car ([https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0))
    is a Mountain Car environment with continuous actions whose value is from -1 to
    1\. As shown in the following screenshot, its goal is to get the car to the top
    of the hill on the right-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2805ac57-39fd-4fd6-a56b-18e052a2926b.png)'
  prefs: []
  type: TYPE_IMG
- en: In a one-dimensional track, the car is positioned between -1.2 (leftmost) and
    0.6 (rightmost), and the goal (yellow flag) is located at 0.5\. The engine of
    the car is not strong enough to drive it to the top in a single pass, so it has
    to drive back and forth to build up momentum. Hence, the action is a float that
    represents the force of pushing the car to the left if it is in a negative value
    from -1 to 0, or pushing the car to the right if it is in a positive value from
    0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two states of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Position of the car**: This is a continuous variable from -1.2 to 0.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity of the car**: This is a continuous variable from -0.07 to 0.07'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The starting state consists of a position between -0.6 to -0.4, and a velocity
    of 0.
  prefs: []
  type: TYPE_NORMAL
- en: The reward associated with each step is *-a²*, where a is the action. And there
    is an additional reward of + 100 for reaching the goal. So, it penalizes the force
    taken in each step until the car reaches the goal. An episode ends when the car
    reaches the goal position (obviously), or after 1,000 steps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s simulate the continuous Mountain Car environment by observing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the Gym library and create an instance of the continuous Mountain
    Car environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We then reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The car starts with state [-0.56756635, 0\. ], which means that the initial
    position is around -0.56 and the velocity is 0\. You may see a different initial
    position as it is randomly generated from -0.6 to -0.4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a naive approach: we just take a random action from -1 to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The state (position and velocity) keeps changing accordingly, and the reward
    is *-a²* for each step.
  prefs: []
  type: TYPE_NORMAL
- en: You will also see in the video that the car is repeatedly moving right and back
    to the left.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can imagine, the continuous Mountain Car problem is a challenging environment,
    even more so than the original discrete one with only three different possible
    actions. We need to drive the car back and forth to build up momentum with the
    right amount of force and direction. Furthermore, the action space is continuous,
    which means that the value lookup / update method (such as the TD method, DQN)
    will not work. In the next recipe, we will solve the continuous Mountain Car problem
    with a continuous control version of the A2C algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the continuous Mountain Car environment with the advantage actor-critic
    network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to solve the continuous Mountain Car problem using
    the advantage actor-critic algorithm, a continuous version this time of course.
    You will see how it differs from the discrete version.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in A2C for environments with discrete actions, we sample actions
    based on the estimated probabilities. How can we model a continuous control, since
    we can''t do such sampling for countless continuous actions? We can actually resort
    to Gaussian distribution. We can assume that the action values are under a Gaussian
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ceacbcf-3d76-4b84-80b6-2a9a8b84ab95.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the mean, [![](img/80063eb8-d3ac-4465-a2b3-bfbab4aa8e51.png)], and deviation,
    ![](img/2aed4bd3-11a4-42cc-8496-64b4c2a2fa2c.png), are computed from the policy
    network. With this tweak, we can sample actions from the constructed Gaussian
    distribution by current mean and deviation. The loss function in continuous A2C
    is similar to the one we used in discrete control, which is a combination of negative
    log likelihood computed with the action probabilities under the Gaussian distribution
    and the advantage values, and the regression error between the actual return values
    and estimated state-values.
  prefs: []
  type: TYPE_NORMAL
- en: Note that one Gaussian distribution is used to simulate action in one dimension,
    so, if the action space is in k dimensions, we need to use k Gaussian distributions.
    In the continuous Mountain Car environment, the action space is one-dimensional.
    The main difficulty of A2C as regards continuous control is how to construct the
    policy network, as it computes parameters for the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the continuous Mountain Car problem using continuous A2C as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a continuous Mountain Car instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the actor-critic neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue with the `__init__` method of the `PolicyNetwork` class using the
    actor-critic neural network we just developed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add the `predict` method, which computes the estimated action probabilities
    and state-value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We now develop the training method, which updates the policy network with samples
    collected in an episode. We will reuse the update method developed in the *Implementing
    the actor-critic algorithm* recipe and will not repeat it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final method for the `PolicyNetwork` class is `get_action`, which samples
    an action from the estimated Gaussian distribution given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: It also returns the log probability of the selected action, and the estimated
    state-value.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for the `PolicyNetwork` class for continuous control!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can move on to developing the main function, training an actor-critic
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `scale_state` function is used to normalize (standardize) the inputs for
    faster model convergence. We first randomly generate 10,000 observations and use
    them to train a scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the scaler is trained, we use it in the `scale_state` function to transform
    new input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the discount factor as `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform continuous control with the actor-critic algorithm using the policy
    network we just developed for 200 episodes, and we also keep track of the total
    rewards for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s display the plot of episode reward over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used Gaussian A2C to solve the continuous Mountain Car environment.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 2*, the network in our example has one hidden layer. There are three
    separate components in the output layer. They are the mean and deviation of the
    Gaussian distribution, and the state-value. The output of the distribution mean
    is scaled to the range of [-1, 1] (or [-2, 2] in this example), using a tanh activation
    function. As for the distribution deviation, we use softplus as the activation
    function to ensure a positive deviation. The network returns the current Gaussian
    distribution (actor) and estimated state-value (critic).
  prefs: []
  type: TYPE_NORMAL
- en: The training function for the actor-critic model in *Step 7* is quite similar
    to what we developed in the *Implementing the actor-critic algorithm* recipe.
    You may notice that we add a value clip to the sampled action in order to keep
    it within the [-1, 1] range . We will explain what the `scale_state` function
    does in an upcoming step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following logs after executing the training in *Step 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is the result of *Step 11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51521f2d-b160-4dc5-a9e3-ba36cae41947.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the resolved requirements in [https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0),
    getting a reward above +90 is regarded as the environment being solved. We have
    multiple episodes where we solve the environment.
  prefs: []
  type: TYPE_NORMAL
- en: In continuous A2C, we assume that each dimension of the action space is Gaussian
    distributed. The mean and deviation of a Gaussian distribution are parts of the
    output layer of the policy network. The remainder of the output layer is for the
    estimation of state-value. An action (or set of actions) is (are) sampled from
    the Gaussian distribution(s) parameterized by the current mean(s) and deviation(s).
    The loss function in continuous A2C is similar to its discrete version, which
    is the combination of negative log likelihood computed with the action probabilities
    under the Gaussian distributions and the advantage values, and the regression
    error between the actual return values and estimated state-values.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have always modeled policy in a stochastic manner where we sample
    actions from distributions or computed probabilities. As a bonus section, we will
    briefly talk about the **Deterministic Policy Gradient** (**DPG**), where we model
    the policy as a deterministic decision. We simply treat the deterministic policy
    as a special case of stochastic policy by directly mapping input states to actions
    instead of probabilities of actions. The DPG algorithm generally uses the following
    two sets of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor-critic network**: This is very similar to the A2C we have experienced,
    but in a deterministic manner. It predicts state-values and actions to be taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target actor-critic network**: This is a periodical copy of the actor-critic
    network with the purpose of stabilizing learning. Obviously, you don''t want to
    have targets that keep on changing. This network provides time-delayed targets
    for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there is not much new in DPG, but a nice combination of A2C
    and a time-delayed target mechanism. Feel free to implement the algorithm yourself
    and use it to solve the continuous Mountain Car environment.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are not familiar with softplus activation, or want to read more about
    DPG, please check out the following material:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Softplus: [https://en.wikipedia.org/wiki/Rectifier_(neural_networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Original paper of the DFP: [https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf](https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing CartPole through the cross-entropy method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last recipe, by way of a bonus (and fun) section, we will develop a
    simple, yet powerful, algorithm to solve CartPole. It is based on cross-entropy,
    and directly maps input states to an output action. In fact, it is more straightforward
    than all the other policy gradient algorithms in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have applied several policy gradient algorithms to solve the CartPole environment.
    They use complicated neural network architectures and a loss function, which may
    be overkill for simple environments such as CartPole. Why don''t we directly predict
    the actions for given states? The idea behind this is straightforward: we model
    the mapping from state to action, and train it ONLY with the most successful experiences
    from the past. We are only interested in what the correct actions should be. The
    objective function, in this case, is the cross-entropy between the actual and
    predicted actions. In CartPole, there are two possible actions: left and right.
    For simplicity, we can convert it into a binary classification problem with the
    following model diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a968090-e424-4f42-b82d-6d100b67aa41.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We solve the CartPole problem using cross-entropy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages and create a CartPole instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the action estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We now develop the main training function for the cross-entropy algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We then specify the input size of the action estimator and the learning rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create an `Estimator` instance accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We will generate 5,000 random episodes and cherry-pick the best 10,000 (state,
    action) pairs for training the estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model is trained, let''s test it out. We use it to play 100 episodes
    and record the total rewards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We then visualize the performance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in *Step 2*, the action estimator has two layers – input and
    output layers, followed by a sigmoid activation function, and the loss function
    is binary cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3* is for training the cross-entropy model. Specifically, for each training
    episode, we take random actions, accumulate rewards, and record states and actions.
    After experiencing `n_episode` episodes, we take the most successful episodes
    (with the highest total rewards) and extract `n_samples` of (state, action) pairs
    as training samples. We then train the estimator for 100 iterations on the training
    set we just constructed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the lines of code in *Step 7* will result in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de8cf5c8-e443-43a1-b654-5816869e7d01.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are +200 rewards for all testing episodes!
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy is very simple, yet useful, for simple environments. It directly
    models the relationship between input states and output actions. A control problem
    is framed into a classification problem where we try to predict the correct action
    among all the alternatives. The trick is that we only learn from the right experience,
    which guides the model in terms of what the most rewarding action should be, given
    a state.
  prefs: []
  type: TYPE_NORMAL
