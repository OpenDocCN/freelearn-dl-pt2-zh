- en: 3 Deep CNN Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/file7.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will first briefly review the evolution of CNNs (in terms
    of architectures), and then we will study the different CNN architectures in detail.
    We will implement these CNN architectures using PyTorch and in doing so, we aim
    to exhaustively explore the tools (modules and built-in functions) that PyTorch
    has to offer in the context of building **Deep CNNs**. Building strong CNN expertise
    in PyTorch will enable us to solve a number of deep learning problems involving
    CNNs. This will also help us in building more complex deep learning models or
    applications of which CNNs are a part.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Why are CNNs so powerful?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of CNN architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing LeNet from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the AlexNet model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a pre-trained VGG model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring GoogLeNet and Inception v3
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing ResNet and DenseNet architectures
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding EfficientNets and the future of CNN architectures
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are CNNs so powerful?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs are among the most powerful machine learning models at solving challenging
    problems such as image classification, object detection, object segmentation,
    video processing, natural language processing, and speech recognition. Their success
    is attributed to various factors, such as the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight sharing**: This makes CNNs parameter-efficient, that is, different
    features are extracted using the same set of weights or parameters. **Features**
    are the high-level representations of input data that the model generates with
    its parameters.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic feature extraction**: Multiple feature extraction stages help a
    CNN to automatically learn feature representations in a dataset.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical learning**: The multi-layered CNN structure helps CNNs to learn
    low-, mid-, and high-level features.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to explore both **spatial and temporal** correlations in the data,
    such as in video- processing tasks.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides these pre-existing fundamental characteristics, CNNs have advanced
    over the years with the help of improvements in the following areas:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The use of better **activation** and **loss functions**, such as using **ReLU**
    to overcome the **vanishing gradient problem**.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter optimization, such as using an optimizer based on Adaptive Momentum
    (Adam) instead of simple Stochastic Gradient Descent.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization: Applying dropouts and batch normalization besides L2 regularization.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FAQ - What is the vanishing gradient problem?
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Backpropagation in neural networks works on the basis of the chain rule of differentiation.
    According to the chain rule, the gradient of the loss function with respect to
    the input layer parameters can be written as a product of gradients at each layer.
    If these gradients are all less than 1 – and worse still, tending toward 0 – then
    the product of these gradients will be a vanishingly small value. The vanishing
    gradient problem can cause serious troubles in the optimization process by preventing
    the network parameters from changing their values, which is equivalent to stunted
    learning.
  id: totrans-26
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在神经网络中的反向传播基于微分的链式法则。根据链式法则，损失函数对输入层参数的梯度可以写成每层梯度的乘积。如果这些梯度都小于1，甚至趋近于0，那么这些梯度的乘积将会是一个接近于零的值。梯度消失问题可能会在优化过程中造成严重问题，因为它会阻止网络参数改变其值，这相当于限制了学习能力。
- en: 'But some of the most significant drivers of development in CNNs over the years
    have been the various *architectural innovations*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多年来推动CNN发展的一些最重要的因素之一是各种*架构创新*：
- en: '**Spatial exploration-based CNNs**: The idea behind **spatial exploration**
    is using different kernel sizes in order to explore different levels of visual
    features in input data. The following diagram shows a sample architecture for
    a spatial exploration-based CNN model:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于空间探索的CNNs**：**空间探索**的理念是使用不同的核尺寸来探索输入数据中不同级别的视觉特征。以下图表展示了一个基于空间探索的CNN模型的示例架构：'
- en: '![Figure 3.1 – Spatial exploration-based CNN](img/file8.jpg)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.1 – 基于空间探索的CNN](img/file8.jpg)'
- en: Figure 3.1 – Spatial exploration-based CNN
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.1 – 基于空间探索的CNN
- en: '**Depth-based CNNs**: The **depth** here refers to the depth of the neural
    network, that is, the number of layers. So, the idea here is to create a CNN model
    with multiple convolutional layers in order to extract highly complex visual features.
    The following diagram shows an example of such a model architecture:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于深度的CNNs**：这里的**深度**指的是神经网络的深度，也就是层数。因此，这里的理念是创建一个带有多个卷积层的CNN模型，以提取高度复杂的视觉特征。以下图表展示了这样一个模型架构的示例：'
- en: '![Figure 3.2 – Depth-based CNN](img/file9.jpg)'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.2 – 基于深度的CNN](img/file9.jpg)'
- en: Figure 3.2 – Depth-based CNN
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.2 – 基于深度的CNN
- en: '**Width-based CNNs**: **Width** refers to the number of channels or feature
    maps in the data or features extracted from the data. So, width-based CNNs are
    all about increasing the number of feature maps as we go from the input to the
    output layers, as demonstrated in the following diagram:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于宽度的CNNs**：**宽度**指的是数据中的通道数或特征图数量。因此，基于宽度的CNNs旨在从输入到输出层增加特征图的数量，如以下图表所示：'
- en: '![Figure 3.3 – Width-based CNN](img/file10.jpg)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.3 – 基于宽度的CNN](img/file10.jpg)'
- en: Figure 3.3 – Width-based CNN
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.3 – 基于宽度的CNN
- en: '**Multi-path-based CNNs**: So far, the preceding three types of architectures
    have monotonicity in connections between layers, that is, direct connections exist
    only between consecutive layers. **Multi-path CNNs** brought the idea of making
    shortcut connections or skip connections between non-consecutive layers. The following
    diagram shows an example of a multi-path CNN model architecture:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于多路径的CNNs**：到目前为止，前面提到的三种架构在层之间的连接上是单调的，即仅存在于连续层之间的直接连接。**多路径CNNs**引入了在非连续层之间建立快捷连接或跳跃连接的理念。以下图表展示了一个多路径CNN模型架构的示例：'
- en: '![Figure 3.4 – Multi-path CNN](img/file11.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 多路径CNN](img/file11.jpg)'
- en: Figure 3.4 – Multi-path CNN
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 多路径CNN
- en: A key advantage of multi-path architectures is a better flow of information
    across several layers, thanks to the skip connections. This, in turn, also lets
    the gradient flow back to the input layers without too much dissipation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多路径架构的一个关键优势是信息在多个层之间的更好流动，这要归功于跳跃连接。这反过来也使得梯度能够回流到输入层而不会有太多损耗。
- en: Having looked at the different architectural setups found in CNN models, we
    will now look at how CNNs have evolved over the years ever since they were first
    used.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过CNN模型中不同的架构设置，接下来我们将看看自从它们首次使用以来，CNN如何在这些年来发展。
- en: Evolution of CNN architectures
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN架构的演变
- en: 'CNNs have been in existence since 1989, when the first multilayered CNN, called
    **ConvNet**, was developed by Yann LeCun. This model could perform visual cognition
    tasks such as identifying handwritten digits. In 1998, LeCun developed an improved
    ConvNet model called **LeNet**. Due to its high accuracy in optical recognition
    tasks, LeNet was adopted for industrial use soon after its invention. Ever since,
    CNNs have been one of the most successful machine learning models, both in industry
    as well as academia. The following diagram shows a brief timeline of architectural
    developments in the lifetime of CNNs, starting from 1989 all the way to 2020:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 1989 年以来，CNN 一直存在，当时第一个多层次 CNN，称为**ConvNet**，是由 Yann LeCun 开发的。这个模型可以执行视觉认知任务，如识别手写数字。1998
    年，LeCun 开发了一个改进的 ConvNet 模型称为**LeNet**。由于其在光学识别任务中的高准确性，LeNet 很快就被工业界采用。从那时起，CNN
    一直是最成功的机器学习模型之一，无论在工业界还是学术界。以下图表显示了从 1989 年到 2020 年 CNN 架构发展的简要时间轴：
- en: '![Figure 3.5 – CNN architecture evolution – a broad picture](img/file12.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – CNN 架构演进 – 大局览](img/file12.jpg)'
- en: Figure 3.5 – CNN architecture evolution – a broad picture
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – CNN 架构演进 – 大局览
- en: As we can see, there is a significant gap between the years 1998 and 2012\.
    This was primarily because there wasn't a dataset big and suitable enough to demonstrate
    the capabilities of CNNs, especially deep CNNs. And on the existing small datasets
    of the time, such as MNIST, classical machine learning models such as SVMs were
    starting to beat CNN performance. During those years, a few CNN developments took
    place.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，1998 年和 2012 年之间存在显著差距。这主要是因为当时没有足够大且合适的数据集来展示 CNN，特别是深度 CNN 的能力。在当时现有的小数据集（如
    MNIST）上，传统的机器学习模型如 SVM 开始击败 CNN 的性能。在这些年里，进行了一些 CNN 的发展。
- en: The ReLU activation function was designed in order to deal with the gradient
    explosion and decay problem during backpropagation. Non-random initialization
    of network parameter values proved to be crucial. **Max-pooling** was invented
    as an effective method for subsampling. GPUs were getting popular for training
    neural networks, especially CNNs at scale. Finally, and most importantly, a large-scale
    dedicated dataset of annotated images called **ImageNet** [3.1] was created by
    a research group at Stanford. This dataset is still one of the primary benchmarking
    datasets for CNN models to date.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数的设计旨在处理反向传播过程中的梯度爆炸和衰减问题。网络参数值的非随机初始化被证明是至关重要的。**Max-pooling**被发明作为一种有效的子采样方法。GPU
    在训练神经网络，尤其是大规模 CNN 时变得流行。最后但也是最重要的是，由斯坦福研究团队创建的大规模带注释图像数据集**ImageNet** [3.1]，至今仍然是
    CNN 模型的主要基准数据集之一。
- en: With all of these developments compounding over the years, in 2012, a different
    architectural design brought about a massive improvement in CNN performance on
    the `ImageNet` dataset. This network was called **AlexNet** (named after the creator,
    Alex Krizhevsky). AlexNet, along with having various novel aspects such as random
    cropping and pre-training, established the trend of uniform and modular convolutional
    layer design. The uniform and modular layer structure was taken forward by repeatedly
    stacking such modules (of convolutional layers), resulting in very deep CNNs also
    known as **VGGs**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些发展多年来的叠加，2012 年，一种不同的架构设计在`ImageNet`数据集上显著改善了 CNN 性能。这个网络被称为**AlexNet**（以创建者
    Alex Krizhevsky 命名）。AlexNet 除了具有随机裁剪和预训练等各种新颖特点外，还确立了统一和模块化卷积层设计的趋势。这种统一和模块化的层结构通过反复堆叠这些模块（卷积层）被推广，导致了非常深的
    CNN，也被称为**VGGs**。
- en: Another approach of branching the blocks/modules of convolutional layers and
    stacking these branched blocks on top of each other proved extremely effective
    for tailored visual tasks. This network was called **GoogLeNet** (as it was developed
    at Google) or **Inception v1** (inception being the term for those branched blocks).
    Several variants of the **VGG** and **Inception** networks followed, such as **VGG16**,
    **VGG19**, **Inception v2**, **Inception v3**, and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分支卷积层块/模块并将这些分支块堆叠在一起的方法对定制视觉任务非常有效。这种网络被称为**GoogLeNet**（因为它是在 Google 开发的）或**Inception
    v1**（inception 是指那些分支块的术语）。随后出现了几个**VGG**和**Inception**网络的变体，如**VGG16**、**VGG19**、**Inception
    v2**、**Inception v3**等。
- en: The next phase of development began with **skip connections**. To tackle the
    problem of gradient decay while training CNNs, non-consecutive layers were connected
    via skip connections lest information dissipated between them due to small gradients.
    A popular type of network that emerged with this trick, among other novel characteristics
    such as batch normalization, was **ResNet**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 开发的下一阶段始于**跳跃连接**。为了解决训练 CNN 时梯度衰减的问题，非连续层通过跳跃连接连接，以免信息因小梯度而在它们之间消失。利用这一技巧出现了一种流行的网络类型，其中包括批量归一化等其他新特性，即**ResNet**。
- en: A logical extension of ResNet was **DenseNet**, where layers were densely connected
    to each other, that is, each layer gets the input from all the previous layers'
    output feature maps. Furthermore, hybrid architectures were then developed by
    mixing successful architectures from the past such as **Inception-ResNet** and
    **ResNeXt**, where the parallel branches within a block were increased in number.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 的一个逻辑扩展是**DenseNet**，其中各层之间密集连接，即每一层都从前面所有层的输出特征图中获取输入。此外，混合架构随后发展，结合了过去成功的架构，如**Inception-ResNet**和**ResNeXt**，其中块内的并行分支数量增加。
- en: Lately, the **channel boosting** technique has proven useful in improving CNN
    performance. The idea here is to learn novel features and exploit pre-learned
    features through transfer learning. Most recently, automatically designing new
    blocks and finding optimal CNN architectures has been a growing trend in CNN research.
    Examples of such CNNs are **MnasNets** and **EfficientNets**. The approach behind
    these models is to perform a neural architecture search to deduce an optimal CNN
    architecture with a uniform model scaling approach.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，**通道增强**技术在提高 CNN 性能方面证明了其实用性。其思想是通过迁移学习学习新特征并利用预先学习的特征。最近，自动设计新块并找到最优 CNN
    架构已成为 CNN 研究的一个趋势。这些 CNN 的例子包括**MnasNets**和**EfficientNets**。这些模型背后的方法是执行神经架构搜索，以推断具有统一模型缩放方法的最优
    CNN 架构。
- en: In the next section, we will go back to one of the earliest CNN models and take
    a closer look at the various CNN architectures developed since. We will build
    these architectures using PyTorch, training some of the models on real-world datasets.
    We will also explore PyTorch's pre-trained CNN models repository, popularly known
    as **model-zoo**. We will learn how to fine-tune these pre-trained models as well
    as running predictions on them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将回顾最早的一些 CNN 模型，并深入研究自那时以来发展的各种 CNN 架构。我们将使用 PyTorch 构建这些架构，训练其中一些模型并使用真实数据集。我们还将探索
    PyTorch 的预训练 CNN 模型库，通常称为**模型动物园**。我们将学习如何微调这些预训练模型以及在它们上运行预测。
- en: Developing LeNet from scratch
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始开发 LeNet
- en: LeNet, originally known as **LeNet-5**, is one of the earliest CNN models, developed
    in 1998\. The number *5* in LeNet-5 represents the *total number of layers* in
    this model, that is, two convolutional and three fully connected layers. With
    roughly 60,000 total parameters, this model gave state-of-the-art performance
    on image recognition tasks for handwritten digit images in the year 1998\. As
    expected from a CNN model, LeNet demonstrated rotation, position, and scale invariance
    as well as robustness against distortion in images. Contrary to the classical
    machine learning models of the time, such as SVMs, which treated each pixel of
    the image separately, LeNet exploited the correlation among neighboring pixels.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet，最初称为**LeNet-5**，是最早的 CNN 模型之一，于 1998 年开发。LeNet-5 中的数字*5*代表了该模型中的总层数，即两个卷积层和三个全连接层。这个模型总共大约有
    60,000 个参数，在 1998 年的手写数字图像识别任务中表现出色。与当时的经典机器学习模型（如 SVM）不同，后者将图像的每个像素分别处理，LeNet
    则利用了相邻像素之间的相关性，展示了旋转、位置和尺度不变性以及对图像扭曲的鲁棒性。
- en: 'Note that although LeNet was developed for handwritten digit recognition, it
    can certainly be extended for other image classification tasks, as we shall see
    in our next exercise. The following diagram shows the architecture of a LeNet
    model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管 LeNet 最初是为手写数字识别而开发的，但它当然可以扩展到其他图像分类任务，正如我们将在下一个练习中看到的那样。以下图显示了 LeNet
    模型的架构：
- en: '![Figure 3.6 – LeNet architecture](img/file13.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – LeNet 架构](img/file13.jpg)'
- en: Figure 3.6 – LeNet architecture
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – LeNet 架构
- en: As mentioned earlier, there are two convolutional layers followed by three fully
    connected layers (including the output layer). This approach of stacking convolutional
    layers followed by fully connected layers later became a trend in CNN research
    and is still applied to the latest CNN models. Besides these layers, there are
    pooling layers in between. These are basically subsampling layers that reduce
    the spatial size of image representation, thereby reducing the number of parameters
    and computations. The pooling layer used in LeNet was an average pooling layer
    that had trainable weights. Soon after, **max pooling** emerged as the most commonly
    used pooling function in CNNs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，图中有两个卷积层，接着是三个全连接层（包括输出层）。这种先堆叠卷积层，然后后续使用全连接层的方法后来成为CNN研究的趋势，并且仍然应用于最新的CNN模型。除了这些层外，中间还有池化层。这些基本上是减少图像表示的空间大小的子采样层，从而减少参数和计算量。LeNet中使用的池化层是一个具有可训练权重的平均池化层。不久之后，**最大池化**成为CNN中最常用的池化函数。
- en: The numbers in brackets in each layer in the figure demonstrate the dimensions
    (for input, output, and fully connected layers) or window size (for convolutional
    and pooling layers). The expected input for a grayscale image is 32x32 pixels
    in size. This image is then operated on by 5x5 convolutional kernels, followed
    by 2x2 pooling, and so on. The output layer size is 10, representing the 10 classes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图中每个层中括号中的数字显示了维度（对于输入、输出和全连接层）或窗口大小（对于卷积和池化层）。灰度图像的预期输入大小为32x32像素。然后该图像经过5x5的卷积核操作，接着是2x2的池化操作，依此类推。输出层大小为10，代表10个类别。
- en: In this section, we will use PyTorch to build LeNet from scratch and train and
    evaluate it on a dataset of images for the task of image classification. We will
    see how easy and intuitive it is to build the network architecture in PyTorch
    using the outline from *Figure 3.6*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用PyTorch从头开始构建LeNet，并在图像分类任务的图像数据集上对其进行训练和评估。我们将看到使用PyTorch根据*图3.6*中的概述构建网络架构是多么简单和直观。
- en: Furthermore, we will demonstrate how effective LeNet is, even on a dataset different
    from the ones it was originally developed on (that is, MNIST) and how PyTorch
    makes it easy to train and test the model in a few lines of code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将演示LeNet的有效性，即使在与其最初开发的数据集（即MNIST）不同的数据集上，并且PyTorch如何在几行代码中轻松训练和测试模型。
- en: Using PyTorch to build LeNet
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch构建LeNet
- en: 'Observe the following steps to build the model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以下步骤构建模型：
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    `import` statements:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于此练习，我们需要导入几个依赖项。执行以下`import`语句：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Besides the usual imports, we also invoke the use_deterministic_algorithms function
    to ensure the reproducibility of this exercise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通常的导入之外，我们还调用了`use_deterministic_algorithms`函数，以确保此练习的可重现性。
- en: 'Next, we will define the model architecture based on the outline given in *Figure
    3.6*:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将根据*图3.6*中的概述定义模型架构：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the last two lines, we instantiate the model and print the network architecture.
    The output will be as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两行，我们实例化模型并打印网络架构。输出将如下所示：
- en: '![Figure 3.7 – LeNet PyTorch model object](img/file14.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – LeNet PyTorch模型对象](img/file14.jpg)'
- en: Figure 3.7 – LeNet PyTorch model object
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – LeNet PyTorch模型对象
- en: There are the usual `__init__` and `forward` methods for architecture definition
    and running a forward pass, respectively. The additional `flattened_features`
    method is meant to calculate the total number of features in an image representation
    layer (usually an output of a convolutional layer or pooling layer). This method
    helps to flatten the spatial representation of features into a single vector of
    numbers, which is then used as input to fully connected layers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 架构定义和运行前向传播的通常`__init__`和`forward`方法。额外的`flattened_features`方法旨在计算图像表示层（通常是卷积层或池化层的输出）中的总特征数。此方法有助于将特征的空间表示展平为单个数字向量，然后作为全连接层的输入使用。
- en: Besides the details of the architecture mentioned earlier, ReLU is used throughout
    the network as the activation function. Also, contrary to the original LeNet network,
    which takes in single-channel images, the current model is modified to accept
    RGB images, that is, three channels as input. This is done in order to adapt to
    the dataset that is used for this exercise.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the training routine, that is, the actual backpropagation step:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For each epoch, this function iterates through the entire training dataset,
    runs a forward pass through the network and, using backpropagation, updates the
    parameters of the model based on the specified optimizer. After iterating through
    every 1,000 mini-batches of the training dataset, this method also logs the calculated
    loss.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the training routine, we will define the test routine that we will
    use to evaluate model performance:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function runs a forward pass through the model for each test-set image,
    calculates the correct number of predictions, and prints the percentage of correct
    predictions on the test set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Before we get on to training the model, we need to load the dataset. For this
    exercise, we will be using the `CIFAR-10` dataset.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset citation
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Learning Multiple Layers of Features from Tiny Images*, Alex Krizhevsky, 2009'
  id: totrans-84
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This dataset consists of 60,000 32x32 RGB images labeled across 10 classes,
    with 6,000 images per class. The 60,000 images are split into 50,000 training
    images and 10,000 test images. More details can be found at the dataset website
    [3.2] . Torch provides the `CIFAR` dataset under the `torchvision.datasets` module.
    We will be using the module to directly load the data and instantiate train and
    test dataloaders as demonstrated in the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous chapter, we manually downloaded the dataset and wrote a custom
    dataset class and a `dataloader` function. We will not need to write those here,
    thanks to the `torchvision.datasets` module.
  id: totrans-89
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Because we set the `download` flag to `True`, the dataset will be downloaded
    locally. Then, we shall see the following output :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – CIFAR-10 dataset download](img/file15.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – CIFAR-10 dataset download
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The transformations used for training and testing datasets are different because
    we apply some data augmentation to the training dataset, such as flipping and
    cropping, which are not applicable to the test dataset. Also, after defining `trainloader`
    and `testloader`, we declare the 10 classes in this dataset with a pre-defined
    ordering.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the datasets, let''s investigate how the data looks:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code shows us four sample images with their respective labels
    from the training dataset. The output will be as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – CIFAR-10 dataset samples](img/file16.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – CIFAR-10 dataset samples
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output shows us four color images, which are 32x32 pixels in size.
    These four images belong to four different labels, as displayed in the text following
    the images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出展示了四张颜色图像，每张图像大小为32x32像素。这四张图片属于四个不同的标签，如下文所示。
- en: We will now train the LeNet model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练 LeNet 模型。
- en: Training LeNet
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 LeNet
- en: 'Let us train the model with the help of the following steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤训练模型：
- en: 'We will define the `optimizer` and start the training loop as shown here:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义 `optimizer` 并开始如下的训练循环：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output will be as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 3.10 – Training LeNet](img/file17.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 训练 LeNet](img/file17.png)'
- en: Figure 3.10 – Training LeNet
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 训练 LeNet
- en: 'Once the training is finished, we can save the model file locally:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以将模型文件保存到本地：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Having trained the LeNet model, we will now test its performance on the test
    dataset in the next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完 LeNet 模型后，我们将在下一节中测试其在测试数据集上的表现。
- en: Testing LeNet
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试 LeNet
- en: 'The following steps need to be followed to test the LeNet model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 LeNet 模型需要遵循以下步骤：
- en: 'Let''s make predictions by loading the saved model and running it on the test
    dataset:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过加载保存的模型并在测试数据集上运行，让我们进行预测：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will be as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 3.11 – LeNet predictions](img/file18.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – LeNet 预测](img/file18.png)'
- en: Figure 3.11 – LeNet predictions
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – LeNet 预测
- en: Evidently, three out of four predictions are correct.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，四次预测中有三次是正确的。
- en: 'Finally, we will check the overall accuracy of this model on the test dataset
    as well as per- class accuracy:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将检查该模型在测试数据集上的总体准确度以及每类准确度：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 3.12 – LeNet overall accuracy](img/file19.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – LeNet 总体准确度](img/file19.png)'
- en: Figure 3.12 – LeNet overall accuracy
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – LeNet 总体准确度
- en: 'For per- class accuracy, the code is as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每类准确度，代码如下：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output will be as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 3.13 – LeNet per class accuracy](img/file20.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – LeNet 每类准确度](img/file20.png)'
- en: Figure 3.13 – LeNet per class accuracy
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – LeNet 每类准确度
- en: Some classes have better performance than others. Overall, the model is far
    from perfect (that is, 100% accuracy) but much better than a model making random
    predictions, which would have an accuracy of 10% (due to the 10 classes).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有些类别的表现比其他类别好。总体而言，该模型远非完美（即100% 准确率），但比随机预测的模型要好得多，后者的准确率为10%（由于有10个类别）。
- en: Having built a LeNet model from scratch and evaluated its performance using
    PyTorch, we will now move on to a successor of LeNet – **AlexNet**. For LeNet,
    we built the model from scratch, trained, and tested it. For AlexNet, we will
    use a pre-trained model, fine-tune it on a smaller dataset, and test it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在从头开始构建 LeNet 模型并评估其在 PyTorch 中的表现后，我们现在将转向 LeNet 的后继者 – **AlexNet**。对于 LeNet，我们从头开始构建了模型，进行了训练和测试。对于
    AlexNet，我们将使用预训练模型，对其在较小数据集上进行微调，并进行测试。
- en: Fine-tuning the AlexNet model
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 AlexNet 模型进行微调
- en: In this section, we will first take a quick look at the AlexNet architecture
    and how to build one using PyTorch. Then we will explore PyTorch's pre-trained
    CNN models repository, and finally, use a pre-trained AlexNet model for fine-tuning
    on an image classification task, as well as making predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先快速浏览 AlexNet 的架构以及如何使用 PyTorch 构建一个。然后我们将探索 PyTorch 的预训练 CNN 模型库，最后，使用预训练的
    AlexNet 模型进行微调，用于图像分类任务以及进行预测。
- en: 'AlexNet is a successor of LeNet with incremental changes in the architecture,
    such as 8 layers (5 convolutional and 3 fully connected) instead of 5, and 60
    million model parameters instead of 60,000, as well as using `MaxPool` instead
    of `AvgPool`. Moreover, AlexNet was trained and tested on a much bigger dataset
    – ImageNet, which is over 100 GB in size, as opposed to the MNIST dataset (on
    which LeNet was trained), which amounts to a few MBs. AlexNet truly revolutionized
    CNNs as it emerged as a significantly more powerful class of models on image-related
    tasks than the other classical machine learning models, such as SVMs. *Figure
    3.14* shows the AlexNet architecture:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet 是 LeNet 的后继者，其架构有所增强，例如由5层（5个卷积层和3个全连接层）增加到8层，并且模型参数从6万增加到6000万，同时使用
    `MaxPool` 而不是 `AvgPool`。此外，AlexNet 是在更大的数据集 ImageNet 上训练和测试的，ImageNet 数据集超过100
    GB，而 LeNet 是在 MNIST 数据集上训练的，后者只有几 MB 大小。AlexNet 在图像相关任务中显著领先于其他传统机器学习模型，如 SVM。*图
    3.14* 展示了 AlexNet 的架构：
- en: '![Figure 3.14 – AlexNet architecture](img/file21.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 – AlexNet 架构](img/file21.jpg)'
- en: Figure 3.14 – AlexNet architecture
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – AlexNet架构
- en: 'As we can see, the architecture follows the common theme from LeNet of having
    convolutional layers stacked sequentially, followed by a series of fully connected
    layers toward the output end. PyTorch makes it easy to translate such a model
    architecture into actual code. This can be seen in the following PyTorch code-
    equivalent of the architecture:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该架构遵循了LeNet的常见主题，即由卷积层顺序堆叠，然后是一系列全连接层朝向输出端。PyTorch使得将这样的模型架构转化为实际代码变得容易。可以在以下PyTorch代码中看到这一点-
    该架构的等效代码：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The code is quite self-explanatory, wherein the `__init__` function contains
    the initialization of the whole layered structure, consisting of convolutional,
    pooling, and fully connected layers, along with ReLU activations. The `forward`
    function simply runs a data point *x* through this initialized network. Please
    note that the second line of the `forward` method already performs the flattening
    operation so that we need not define that function separately as we did for LeNet.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 代码相当自解释，`__init__`函数包含了整个分层结构的初始化，包括卷积、池化和全连接层，以及ReLU激活函数。`forward`函数简单地通过初始化的网络运行数据点*x*。请注意，`forward`方法的第二行已经执行了扁平化操作，因此我们无需像为LeNet那样单独定义该函数。
- en: 'But besides the option of initializing the model architecture and training
    it ourselves, PyTorch, with its `torchvision` package, provides a `models` sub-package,
    which contains definitions of CNN models meant for solving different tasks, such
    as image classification, semantic segmentation, object detection, and so on. Following
    is a non-exhaustive list of available models for the task of image classification
    [3.3] :'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了初始化模型架构并自行训练的选项外，PyTorch还提供了一个`torchvision`包，其中包含用于解决不同任务（如图像分类、语义分割、目标检测等）的CNN模型的定义。以下是用于图像分类任务的可用模型的非详尽列表
    [3.3]：
- en: AlexNet
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet
- en: VGG
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG
- en: ResNet
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: SqueezeNet
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SqueezeNet
- en: DenseNet
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: Inception v3
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception v3
- en: GoogLeNet
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GoogLeNet
- en: ShuffleNet v2
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShuffleNet v2
- en: MobileNet v2
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileNet v2
- en: ResNeXt
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNeXt
- en: Wide ResNet
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wide ResNet
- en: MNASNet
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNASNet
- en: EfficientNet
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EfficientNet
- en: In the next section, we will use a pre-trained AlexNet model as an example and
    demonstrate how to fine-tune it using PyTorch in the form of an exercise.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用一个预训练的AlexNet模型作为示例，并展示如何使用PyTorch对其进行微调，形式上是一个练习。
- en: Using PyTorch to fine-tune AlexNet
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch对AlexNet进行微调
- en: 'In the following exercise, we will load a pre-trained AlexNet model and fine-tune
    it on an image classification dataset different from ImageNet (on which it was
    originally trained). Finally, we will test the fine-tuned model''s performance
    to see if it could transfer-learn from the new dataset. Some parts of the code
    in the exercise are trimmed for readability but you can find the full code in
    our github repo [3.4] :'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将加载一个预训练的AlexNet模型，并在一个与ImageNet不同的图像分类数据集上进行微调。最后，我们将测试微调后模型的性能，看它是否能够从新数据集中进行迁移学习。练习中的部分代码为了可读性而进行了修剪，但你可以在我们的github库[3.4]中找到完整的代码。
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    `import` statements:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们需要导入几个依赖项。执行以下`import`语句：
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we will download and transform the dataset. For this fine-tuning exercise,
    we will use a small image dataset of bees and ants. There are 240 training images
    and 150 validation images divided equally between the two classes (bees and ants).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将下载并转换数据集。对于这次的微调练习，我们将使用一个小型的昆虫图像数据集，包括蜜蜂和蚂蚁。共有240张训练图像和150张验证图像，均等分为两类（蜜蜂和蚂蚁）。
- en: We download the dataset from kaggel [3.5] and store it in the current working
    directory. More information about the dataset can be found at the dataset’s website[3.6]
    .
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从kaggel [3.5]下载数据集，并存储在当前工作目录中。有关数据集的更多信息可以在数据集的网站[3.6]找到。
- en: Dataset citation
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据集引用
- en: ''
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Elsik CG, Tayal A, Diesh CM, Unni DR, Emery ML, Nguyen HN, Hagen DE. Hymenoptera
    Genome Database: integrating genome annotations in HymenopteraMine. Nucleic Acids
    Research 2016 Jan 4;44(D1):D793-800\. doi: 10.1093/nar/gkv1208\. Epub 2015 Nov
    17\. PubMed PMID: 26578564.'
  id: totrans-162
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Elsik CG, Tayal A, Diesh CM, Unni DR, Emery ML, Nguyen HN, Hagen DE。Hymenoptera
    Genome Database：在HymenopteraMine中整合基因组注释。Nucleic Acids Research 2016年1月4日;44(D1):D793-800。doi:
    10.1093/nar/gkv1208。在线发表于2015年11月17日。PubMed PMID: 26578564。'
- en: 'In order to download the dataset, you will need to log into Kaggle. If you
    do not already have a Kaggle account, you will need to register:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载数据集，您需要登录Kaggle。如果您还没有Kaggle账户，您需要注册：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have completed the pre-requisites, let''s begin:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize some sample training dataset images:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Bees versus ants dataset](img/file22.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Bees versus ants dataset
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'We now define the fine-tuning routine, which is essentially a training routine
    performed on a pre-trained model:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this function, we require the pre-trained model (that is, the architecture
    as well as weights) as input along with the loss function, optimizer, and number
    of epochs. Basically, instead of starting from a random initialization of weights,
    we start with the pre-trained weights of AlexNet. The other parts of this function
    are pretty similar to our previous exercises.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting to fine-tune (train) the model, we will define a function to
    visualize the model predictions:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we get to the interesting part. Let''s use PyTorch''s `torchvision.models`
    sub-package to load the pre-trained AlexNet model:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This model object has the following two main components:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'i) `features`: The feature extraction component, which contains all the convolutional
    and pooling layers'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'ii) `classifier`: The classifier block, which contains all the fully connected
    layers leading to the output layer'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize these components as shown here:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This should output the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – AlexNet feature extractor](img/file23.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – AlexNet feature extractor
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Next , we inspect the `classifier` block as follows:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This should output the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – AlexNet classifier](img/file24.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – AlexNet classifier
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have noticed, the pre-trained model has the output layer of size
    `1000`, but we only have `2` classes in our fine-tuning dataset. So, we shall
    alter that, as shown here:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And now, we are all set to define the optimizer and loss function, and thereafter
    run the training routine as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will be as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – AlexNet fine-tuning loop](img/file25.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – AlexNet fine-tuning loop
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize some of the model predictions to see whether the model has
    indeed learned the relevant features from this small dataset:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This should output the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – AlexNet predictions](img/file26.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – AlexNet predictions
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the pretrained AlexNet model has been able to transfer-learn on this
    rather tiny image classification dataset. This both demonstrates the power of
    transfer learning as well as the speed and ease with which we can fine-tune well
    known models using PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss an even deeper and more complex successor
    of AlexNet – the VGG network. We have demonstrated the model definition, dataset
    loading, model training (or fine-tuning), and evaluation steps in detail for LeNet
    and AlexNet. In subsequent sections, we will focus mostly on model architecture
    definition, as the PyTorch code for other aspects (such as data loading and evaluation)
    will be similar.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Running a pre-trained VGG model
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行预训练的 VGG 模型
- en: We have already discussed LeNet and AlexNet, two of the foundational CNN architectures.
    As we progress in the chapter, we will explore increasingly complex CNN models.
    Although, the key principles in building these model architectures will be the
    same. We will see a modular model-building approach in putting together convolutional
    layers, pooling layers, and fully connected layers into blocks/modules and then
    stacking these blocks sequentially or in a branched manner. In this section, we
    look at the successor to AlexNet – VGGNet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 LeNet 和 AlexNet，这两个基础的 CNN 架构。随着章节的进展，我们将探索越来越复杂的 CNN 模型。虽然在构建这些模型架构时的关键原则是相同的。我们将看到在组合卷积层、池化层和全连接层到块/模块中以及顺序或分支堆叠这些块/模块时的模块化模型构建方法。在本节中，我们将看到
    AlexNet 的继任者 – VGGNet。
- en: 'The name VGG is derived from the **Visual Geometry Group of Oxford University**,
    where this model was invented. Compared to the 8 layers and 60 million parameters
    of AlexNet, VGG consists of 13 layers (10 convolutional layers and 3 fully connected
    layers) and 138 million parameters. VGG basically stacks more layers onto the
    AlexNet architecture with smaller size convolution kernels (2x2 or 3x3). Hence,
    VGG''s novelty lies in the unprecedented level of depth that it brings with its
    architecture. *Figure 3.20* shows the VGG architecture:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 VGG 源自于**牛津大学视觉几何组**，这个模型在那里被发明。相比于 AlexNet 的 8 层和 6000 万参数，VGG 由 13 层（10
    个卷积层和 3 个全连接层）和 1.38 亿参数组成。VGG 基本上在 AlexNet 架构上堆叠更多层，使用较小尺寸的卷积核（2x2 或 3x3）。因此，VGG
    的新颖之处在于它带来的前所未有的深度。*图3.20* 展示了 VGG 的架构：
- en: '![Figure 3.20 – VGG16 architecture](img/file27.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图3.20 – VGG16 架构](img/file27.jpg)'
- en: Figure 3.20 – VGG16 architecture
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 – VGG16 架构
- en: The preceding VGG architecture is called **VGG13**, because of the 13 layers.
    Other variants are VGG16 and VGG19, consisting of 16 and 19 layers, respectively.
    There is another set of variants – **VGG13_bn**, **VGG16_bn**, and **VGG19_bn**,
    where **bn** suggests that these models also consist of **batch-normalization
    layers**.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的 VGG 架构被称为**VGG13**，因为它有 13 层。其他变体包括 VGG16 和 VGG19，分别包含 16 层和 19 层。还有另一组变体
    – **VGG13_bn**、**VGG16_bn** 和 **VGG19_bn**，其中 **bn** 表示这些模型也包含**批处理归一化层**。
- en: 'PyTorch''s `torchvision.model` sub-package provides the pre-trained `VGG` model
    (with all of the six variants discussed earlier) trained on the ImageNet dataset.
    In the following exercise, we will use the pre-trained `VGG13` model to make predictions
    on a small dataset of bees and ants (used in the previous exercise). We will focus
    on the key pieces of code here, as most other parts of our code will overlap with
    that of the previous exercises. We can always refer to our notebooks to explore
    the full code [3.7]:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的`torchvision.model`子包提供了在 ImageNet 数据集上训练的预训练`VGG`模型（包括前述的六个变体）。在下面的练习中，我们将使用预训练的`VGG13`模型对一小组蚂蚁和蜜蜂（用于前面的练习）进行预测。我们将专注于这里的关键代码部分，因为我们的大部分代码将与之前的练习重叠。我们可以随时查阅我们的笔记本来探索完整的代码
    [3.7]：
- en: First, we need to import dependencies, including `torchvision.models`.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入依赖项，包括`torchvision.models`。
- en: Download the data and set up the ants and bees dataset and dataloader, along
    with the transformations.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据并设置蚂蚁和蜜蜂数据集以及数据加载器，同时进行转换。
- en: In order to make predictions on these images, we will need to download the 1,000
    labels of the ImageNet dataset [3.8] .
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了对这些图像进行预测，我们需要下载 ImageNet 数据集的 1,000 个标签 [3.8] 。
- en: 'Once downloaded, we need to create a mapping between the class indices 0 to
    999 and the corresponding class labels, as shown here:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载后，我们需要创建类索引 0 到 999 和相应类标签之间的映射，如下所示：
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This should output the first five class mappings, as shown in the following
    screenshot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出前五个类映射，如下截图所示：
- en: '![Figure 3.21 – ImageNet class mappings](img/file28.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图3.21 – ImageNet 类映射](img/file28.jpg)'
- en: Figure 3.21 – ImageNet class mappings
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 – ImageNet 类映射
- en: Define the model prediction visualization function that takes in the pre-trained
    model object and the number of images to run predictions on. This function should
    output the images with predictions.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型预测可视化函数，该函数接受预训练模型对象和要运行预测的图像数量。该函数应输出带有预测的图像。
- en: 'Load the pretrained `VGG13` model:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的`VGG13`模型：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This should output the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 3.22 – Loading the VGG13 model](img/file29.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图3.22 – 加载 VGG13 模型](img/file29.jpg)'
- en: Figure 3.22 – Loading the VGG13 model
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The `VGG13` model is downloaded in this step.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: FAQ - What is the disk size of VGG13 model?
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: VGG13 model will consume roughly 508 MB on your hard disk.
  id: totrans-229
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, we run predictions on our ants and bees dataset using this pre-trained
    model:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This should output the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – VGG13 predictions](img/file30.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – VGG13 predictions
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The `VGG13` model trained on an entirely different dataset seems to predict
    all the test samples correctly in the ants and bees dataset. Basically, the model
    grabs the two most similar animals from the dataset out of the 1,000 classes and
    finds them in the images. By doing this exercise, we see that the model is still
    able to extract relevant visual features out of the images and the exercise demonstrates
    the utility of PyTorch's out-of-the-box inference feature.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to study a different type of CNN architecture
    – one that involves modules that have multiple parallel convolutional layers.
    The modules are called **Inception modules** and the resulting network is called
    the **Inception network**. We will explore the various parts of this network and
    the reasoning behind its success. We will also build the inception modules and
    the Inception network architecture using PyTorch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Exploring GoogLeNet and Inception v3
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have discovered the progression of CNN models from LeNet to VGG so far,
    we have observed the sequential stacking of more convolutional and fully connected
    layers. This resulted in deep networks with a lot of parameters to train. *GoogLeNet*
    emerged as a radically different type of CNN architecture that is composed of
    a module of parallel convolutional layers called the inception module. Because
    of this, GoogLeNet is also called **Inception v1** (v1 marked the first version
    as more versions came along later). Some of the drastically new elements introduced
    in GoogLeNet were the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The **inception module** – a module of several parallel convolutional layers
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **1x1 convolutions** to reduce the number of model parameters
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global average pooling** instead of a fully connected layer – reduces overfitting'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **auxiliary classifiers** for training – for regularization and gradient
    stability
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet has 22 layers, which is more than the number of layers of any VGG
    model variant. Yet, due to some of the optimization tricks used, the number of
    parameters in GoogLeNet is 5 million, which is far less than the 138 million parameters
    of VGG. Let's expand on some of the key features of this model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Inception modules
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perhaps the single most important contribution of this model was the development
    of a convolutional module with several convolutional layers running in parallel,
    which are finally concatenated to produce a single output vector. These parallel
    convolutional layers operate with different kernel sizes ranging from 1x1 to 3x3
    to 5x5\. The idea is to extract all levels of visual information from the image.
    Besides these convolutions, a 3x3 max-pooling layer adds another level of feature
    extraction. *Figure 3.24* shows the inception block diagram along with the overall
    GoogLeNet architecture:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – GoogLeNet architecture](img/file31.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – GoogLeNet architecture
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'By using this architecture diagram, we can build the inception module in PyTorch
    as shown here:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, we will look at another important feature of GoogLeNet – 1x1 convolutions.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 1x1 convolutions
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the parallel convolutional layers in an inception module, each
    parallel layer has a preceding **1x1 convolutional layer**. The reason behind
    using these 1x1 convolutional layers is *dimensionality reduction*. 1x1 convolutions
    do not change the width and height of the image representation but can alter the
    depth of an image representation. This trick is used to reduce the depth of the
    input visual features before performing the 1x1, 3x3, and 5x5 convolutions parallelly.
    Reducing the number of parameters not only helps build a lighter model but also
    combats overfitting.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Global average pooling
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we look at the overall GoogLeNet architecture in *Figure 3.24*, the penultimate
    output layer of the model is preceded by a 7x7 average pooling layer. This layer
    again helps in reducing the number of parameters of the model, thereby reducing
    overfitting. Without this layer, the model would have millions of additional parameters
    due to the dense connections of a fully connected layer.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary classifiers
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Figure 3.24* also shows two extra or auxiliary output branches in the model.
    These auxiliary classifiers are supposed to tackle the vanishing gradient problem
    by adding to the gradients'' magnitude during backpropagation, especially for
    the layers towards the input end. Because these models have a large number of
    layers, vanishing gradients can become a bottleneck. Hence, using auxiliary classifiers
    has proven useful for this 22-layer deep model. Additionally, the auxiliary branches
    also help in regularization. Please note that these auxiliary branches are switched
    off/discarded while making predictions.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the inception module defined using PyTorch, we can easily instantiate
    the entire Inception v1 model as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Besides instantiating our own model, we can always load a pre-trained GoogLeNet
    with just two lines of code:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, as mentioned earlier, a number of versions of the Inception model were
    developed later. One of the eminent ones was Inception v3, which we will briefly
    discuss next.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Inception v3
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This successor of Inception v1 has a total of 24 million parameters as compared
    to 5 million in v1\. Besides the addition of several more layers, this model introduced
    different kinds of inception modules, which are stacked sequentially. *Figure
    3.25* shows the different inception modules and the full model architecture:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Inception v1 的后继者，总共有 2400 万个参数，而 v1 中仅有 500 万个参数。除了增加了几个更多的层外，该模型引入了不同种类的
    Inception 模块，这些模块按顺序堆叠。*图 3.25* 展示了不同的 Inception 模块和完整的模型架构：
- en: '![Fig 3.25 – Inception v3 architecture](img/file32.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25 – Inception v3 架构](img/file32.jpg)'
- en: Fig 3.25 – Inception v3 architecture
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – Inception v3 架构
- en: 'It can be seen from the architecture that this model is an architectural extension
    of the Inception v1 model. Once again, besides building the model manually, we
    can use the pre-trained model from PyTorch''s repository as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构中可以看出，该模型是 Inception v1 模型的架构扩展。除了手动构建模型外，我们还可以按如下方式使用 PyTorch 的预训练模型：
- en: '[PRE29]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the next section, we will go through the classes of CNN models that have
    effectively combatted the vanishing gradient problem in very deep CNNs – **ResNet**
    and **DenseNet**. We will learn about the novel techniques of skip connections
    and dense connections and use PyTorch to code the fundamental modules behind these
    advanced architectures.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细讨论在非常深的 CNNs 中有效对抗消失梯度问题的 CNN 模型的类别 – **ResNet** 和 **DenseNet**。我们将学习跳跃连接和密集连接的新技术，并使用
    PyTorch 编写这些先进架构背后的基础模块。
- en: Discussing ResNet and DenseNet architectures
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论 ResNet 和 DenseNet 架构
- en: In the previous section, we explored the Inception models, which had a reduced
    number of model parameters as the number of layers increased, thanks to the 1x1
    convolutions and global average pooling. Furthermore, auxiliary classifiers were
    used to combat the vanishing gradient problem.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们探讨了 Inception 模型，随着层数的增加，由于 1x1 卷积和全局平均池化的使用，模型参数数量减少。此外，还使用了辅助分类器来对抗消失梯度问题。
- en: ResNet introduced the concept of **skip connections**. This simple yet effective
    trick overcomes the problem of both parameter overflow and vanishing gradients.
    The idea, as shown in the following diagram, is quite simple. The input is first
    passed through a non-linear transformation (convolutions followed by non-linear
    activations) and then the output of this transformation (referred to as the residual)
    is added to the original input. Each block of such computation is called a **residual
    block**, hence the name of the model – **residual network** or **ResNet**.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 引入了 **跳跃连接** 的概念。这个简单而有效的技巧克服了参数溢出和消失梯度问题。如下图所示，其思想非常简单。首先，输入经过非线性变换（卷积后跟非线性激活），然后将这个变换的输出（称为残差）加到原始输入上。每个这样的计算块称为
    **残差块**，因此模型被称为 **残差网络** 或 **ResNet**。
- en: '![Figure 3.26 – Skip connections](img/file33.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.26 – 跳跃连接](img/file33.jpg)'
- en: Figure 3.26 – Skip connections
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – 跳跃连接
- en: 'Using these skip (or shortcut) connections, the number of parameters is limited
    to 26 million parameters for a total of 50 layers (ResNet-50). Due to the limited
    number of parameters, ResNet has been able to generalize well without overfitting
    even when the number of layers is increased to 152 (ResNet-152). The following
    diagram shows the ResNet-50 architecture:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些跳跃（或快捷）连接，参数数量仅限于 2600 万个参数，共计 50 层（ResNet-50）。由于参数数量有限，即使层数增至 152 层（ResNet-152），ResNet
    仍然能够很好地泛化，而不会过拟合。以下图表显示了 ResNet-50 的架构：
- en: '![Figure 3.27 – ResNet architecture](img/file34.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.27 – ResNet 架构](img/file34.jpg)'
- en: Figure 3.27 – ResNet architecture
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 – ResNet 架构
- en: 'There are two kinds of residual blocks – **convolutional** and **identity**,
    both having skip connections. For the convolutional block, there is an added 1x1
    convolutional layer, which further helps to reduce dimensionality. A residual
    block for ResNet can be implemented in PyTorch as shown here:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种残差块 – **卷积** 和 **恒等**，两者均具有跳跃连接。对于卷积块，还添加了一个额外的 1x1 卷积层，这进一步有助于降低维度。在 PyTorch
    中，可以如下实现 ResNet 的残差块：
- en: '[PRE30]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To get started quickly with ResNet, we can always use the pre-trained ResNet
    model from PyTorch''s repository:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速开始使用 ResNet，我们可以随时从 PyTorch 的仓库中使用预训练的 ResNet 模型：
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ResNet uses the identity function (by directly connecting input to output) to
    preserve the gradient during backpropagation (as the gradient will be 1). Yet,
    for extremely deep networks, this principle might not be sufficient to preserve
    strong gradients from the output layer back to the input layer.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The CNN model we will discuss next is designed to ensure a strong gradient flow,
    as well as a further reduction in the number of required parameters.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The skip connections of ResNet connected the input of a residual block directly
    to its output. However, the inter-residual-blocks connection is still sequential,
    that is, residual block number 3 has a direct connection with block 2 but no direct
    connection with block 1.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet, or dense networks, introduced the idea of connecting every convolutional
    layer with every other layer within what is called a **dense block**. And every
    dense block is connected to every other dense block in the overall DenseNet. A
    dense block is simply a module of two 3x3 densely connected convolutional layers.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: These dense connections ensure that every layer is receiving information from
    all of the preceding layers of the network. This ensures that there is a strong
    gradient flow from the last layer down to the very first layer. Counterintuitively,
    the number of parameters of such a network setting will also be low. As every
    layer is receiving the feature maps from all the previous layers, the required
    number of channels (depth) can be fewer. In the earlier models, the increasing
    depth represented the accumulation of information from earlier layers, but we
    don't need that anymore, thanks to the dense connections everywhere in the network.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: One key difference between ResNet and DenseNet is also that, in ResNet, the
    input was added to the output using skip connections. But in the case of DenseNet,
    the preceding layers' outputs are concatenated with the current layer's output.
    And the concatenation happens in the depth dimension.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'This might raise a question about the exploding size of outputs as we proceed
    further in the network. To combat this compounding effect, a special type of block
    called the **transition block** is devised for this network. Composed of a 1x1
    convolutional layer followed by a 2x2 pooling layer, this block standardizes or
    resets the size of the depth dimension so that the output of this block can then
    be fed to the subsequent dense block(s). The following diagram shows the DenseNet
    architecture:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – DenseNet architecture](img/file35.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – DenseNet architecture
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, there are two types of blocks involved – the **dense
    block** and the **transition block**. These blocks can be written as classes in
    PyTorch in a few lines of code, as shown here:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'These blocks are then stacked densely to form the overall DenseNet architecture.
    DenseNet, like ResNet, comes in variants such as **DenseNet121**, **DenseNet161**,
    **DenseNet169**, and **DenseNet201**, where the numbers represent the total number
    of layers. Such large numbers of layers are obtained by the repeated stacking
    of the dense and transition blocks plus a fixed 7x7 convolutional layer at the
    input end and a fixed fully connected layer at the output end. PyTorch provides
    pre-trained models for all of these variants:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些块被密集堆叠以形成整体的DenseNet架构。像ResNet一样，DenseNet有各种变体，如**DenseNet121**、**DenseNet161**、**DenseNet169**和**DenseNet201**，其中数字代表总层数。通过在输入端重复堆叠密集块和过渡块以及固定的7x7卷积层和输出端的固定全连接层，可以获得这些大量的层。PyTorch为所有这些变体提供了预训练模型：
- en: '[PRE33]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'DenseNet outperforms all the models discussed so far on the ImageNet dataset.
    Various hybrid models have been developed by mixing and matching the ideas presented
    in the previous sections. The Inception-ResNet and ResNeXt models are examples
    of such hybrid networks. The following diagram shows the ResNeXt architecture:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上，DenseNet优于迄今讨论的所有模型。通过混合和匹配前几节中提出的思想，开发了各种混合模型。Inception-ResNet和ResNeXt模型是这种混合网络的示例。下图显示了ResNeXt架构：
- en: '![Figure 3.29 – ResNeXt architecture](img/file36.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图3.29 – ResNeXt架构](img/file36.jpg)'
- en: Figure 3.29 – ResNeXt architecture
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.29 – ResNeXt架构
- en: As you can see, it looks like a wider variant of a *ResNet + Inception* hybrid
    because there is a large number of parallel convolutional branches in the residual
    blocks – and the idea of parallelism is derived from the inception network.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它看起来像是*ResNet + Inception*混合的更广泛变体，因为残差块中有大量并行的卷积分支——并行的概念源于Inception网络。
- en: In the next and last section of this chapter, we are going to look at the current
    best performing CNN architectures – EfficientNets. We will also discuss the future
    of CNN architectural development while touching upon the use of CNN architectures
    for tasks beyond image classification.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一部分，我们将探讨当前表现最佳的CNN架构——EfficientNets。我们还将讨论CNN架构发展的未来，同时涉及CNN架构在超越图像分类的任务中的应用。
- en: Understanding EfficientNets and the future of CNN architectures
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解EfficientNets和CNN架构的未来
- en: 'So far in our exploration from LeNet to DenseNet, we have noticed an underlying
    theme in the advancement of CNN architectures. That theme is the expansion or
    scaling of the CNN model through one of the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，从LeNet到DenseNet的探索中，我们已经注意到CNN架构进步的一个潜在主题。这一主题是通过以下一种方式扩展或缩放CNN模型：
- en: An increase in the number of layers
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数增加
- en: An increase in the number of feature maps or channels in a convolutional layer
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积层中特征映射或通道数的增加
- en: An increase in the spatial dimension going from 32x32 pixel images in LeNet
    to 224x224 pixel images in AlexNet and so on
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从LeNet的32x32像素图像到AlexNet的224x224像素图像等空间维度的增加
- en: These three different aspects on which scaling can be performed are identified
    as *depth*, *width*, and *resolution*, respectively. Instead of manually scaling
    these attributes, which often leads to suboptimal results, **EfficientNets** use
    neural architecture search to calculate the optimal scaling factors for each of
    them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 可进行缩放的三个不同方面分别被确定为*深度*、*宽度*和*分辨率*。EfficientNets不再手动调整这些属性，这通常会导致次优结果，而是使用神经架构搜索来计算每个属性的最优缩放因子。
- en: Scaling up depth is deemed important because the deeper the network, the more
    complex the model, and hence it can learn highly complex features. However, there
    is a trade-off because, with increasing depth, the vanishing gradient problem
    escalates along with the general problem of overfitting.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 增加深度被认为很重要，因为网络越深，模型越复杂，因此可以学习到更复杂的特征。然而，增加深度也存在一定的权衡，因为随着深度的增加，梯度消失问题以及过拟合问题普遍加剧。
- en: Similarly, scaling up width should theoretically help, as with a greater number
    of channels, the network should learn more fine-grained features. However, for
    extremely wide models, the accuracy tends to saturate quickly.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，理论上增加宽度应该有助于性能，因为通道数越多，网络应该能够学习更细粒度的特征。然而，对于极宽的模型，精度往往会迅速饱和。
- en: Finally, higher resolution images, in theory, should work better as they have
    more fine-grained information. Empirically, however, the increase in resolution
    does not yield a linearly equivalent increase in the model performance. All of
    this is to say that there are trade-offs to be made while deciding the scaling
    factors and hence, neural architecture search helps in finding the optimal scaling
    factors.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从理论上讲，更高分辨率的图像应该效果更好，因为它们包含更精细的信息。然而，经验上，分辨率的增加并不会线性等效地提高模型性能。总之，这些都表明在确定缩放因子时需要权衡，因此神经架构搜索有助于找到最优缩放因子。
- en: 'EfficientNet proposes finding the architecture that has the right balance between
    depth, width, and resolution, and all three of these aspects are scaled together
    using a global scaling factor. The EfficientNet architecture is built in two steps.
    First, a basic architecture (called the **base network**) is devised by fixing
    the scaling factor to `1`. At this stage, the relative importance of depth, width,
    and resolution is decided for the given task and dataset. The base network obtained
    is pretty similar to a well-known CNN architecture – **MnasNet**, short for **Mobile
    Neural Architecture Search Network**. PyTorch offers the pre-trained `MnasNet`
    model, which can be loaded as shown here:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientNet提出了找到具有正确深度、宽度和分辨率平衡的架构，这三个方面通过全局缩放因子一起进行缩放。EfficientNet架构分为两步。首先，通过将缩放因子固定为`1`来设计一个基本架构（称为**基础网络**）。在这个阶段，决定了给定任务和数据集的深度、宽度和分辨率的相对重要性。所得到的基础网络与一个著名的CNN架构非常相似，即**MnasNet**，全称**Mobile
    Neural Architecture Search Network**。PyTorch提供了预训练的`MnasNet`模型，可以像这样加载：
- en: '[PRE34]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once the base network is obtained in the first step, the optimal global scaling
    factor is then computed with the aim of maximizing the accuracy of the model and
    minimizing the number of computations (or flops). The base network is called **EfficientNet
    B0** and the subsequent networks derived for different optimal scaling factors
    are called **EfficientNet B1-B7**. PyTorch provides pre-trained models for all
    of these variants:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在第一步得到了基础网络，就会计算出最优的全局缩放因子，目标是最大化模型的准确性并尽量减少计算量（或浮点运算）。基础网络称为**EfficientNet
    B0**，而为不同最优缩放因子衍生的后续网络称为**EfficientNet B1-B7**。PyTorch为所有这些变体提供了预训练模型：
- en: '[PRE35]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we go forward, efficient scaling of CNN architecture is going to be a prominent
    direction of research along with the development of more sophisticated modules
    inspired by the inception, residual, and dense modules. Another aspect of CNN
    architecture development is minimizing the model size while retaining performance.
    **MobileNets** [3.9] are a prime example and there is a lot of ongoing research
    on this front.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的进展，CNN架构的高效扩展将成为研究的一个突出方向，同时还将开发受到启发的更复杂的模块，例如inception、残差和密集模块。CNN架构发展的另一个方面是在保持性能的同时最小化模型大小。**MobileNets**
    [3.9] 就是一个主要的例子，目前在这个领域有大量的研究。
- en: Besides the top-down approach of looking at architectural modifications of a
    pre-existing model, there will be continued efforts adopting the bottom-up view
    of fundamentally rethinking the units of CNNs such as the convolutional kernels,
    pooling mechanism, more effective ways of flattening, and so on. One concrete
    example of this could be **CapsuleNet** [3.10] , which revamped the convolutional
    units to cater to the third dimension (depth) in images.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述从架构上修改预先存在的模型的自上而下方法之外，还将继续采用从根本上重新思考CNN单元的自下而上方法，例如卷积核、池化机制、更有效的展平方式等。一个具体的例子是**胶囊网络**
    [3.10] ，它重新设计了卷积单元以适应图像中的第三维度（深度）。
- en: 'CNNs are a huge topic of study in themselves. In this chapter, we have touched
    upon the architectural development of CNNs, mostly in the context of image classification.
    However, these same architectures are used across a wide variety of applications.
    One well-known example is the use of ResNets for object detection and segmentation
    in the form of **RCNNs** [3.11] . Some of the improved variants of RCNNs are **Faster
    R-CNN**, **Mask-RCNN**, and **Keypoint-RCNN**. PyTorch provides pre-trained models
    for all three variants:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: CNN本身就是一个广泛研究的话题。在本章中，我们主要讨论了CNN在图像分类背景下的架构发展。然而，这些相同的架构被广泛应用于各种应用中。一个著名的例子是在对象检测和分割中使用ResNets的形式，如**RCNNs**
    [3.11] 。RCNNs的改进变体包括**Faster R-CNN**、**Mask-RCNN**和**Keypoint-RCNN**。PyTorch为这三个变体提供了预训练模型：
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'PyTorch also provides pre-trained models for ResNets that are applied to video-related
    tasks such as video classification. Two such ResNet-based models used for video
    classification are **ResNet3D** and **ResNet Mixed Convolution**:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了预训练模型用于ResNets，这些模型应用于视频相关任务，比如视频分类。用于视频分类的两个基于ResNet的模型分别是**ResNet3D**和**混合卷积ResNet**：
- en: '[PRE37]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: While we do not extensively cover these different applications and corresponding
    CNN models in this chapter, we encourage you to read more on them. PyTorch's website
    can be a good starting point [3.12] .
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本章没有详细涵盖这些不同的应用及相应的CNN模型，但我们鼓励您深入了解它们。PyTorch的网站可以作为一个很好的起点 [3.12]。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has been all about CNN architectures. In the next chapter, we will
    explore a similar journey but for another important type of neural network – recurrent
    neural networks. We will discuss the various recurrent net architectures and use
    PyTorch to effectively implement, train, and test them.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讲述了CNN的架构。在下一章中，我们将探索另一类重要的神经网络——递归神经网络。我们将讨论各种递归网络的架构，并使用PyTorch来有效地实现、训练和测试它们。
