- en: 3 Deep CNN Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/file7.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will first briefly review the evolution of CNNs (in terms
    of architectures), and then we will study the different CNN architectures in detail.
    We will implement these CNN architectures using PyTorch and in doing so, we aim
    to exhaustively explore the tools (modules and built-in functions) that PyTorch
    has to offer in the context of building **Deep CNNs**. Building strong CNN expertise
    in PyTorch will enable us to solve a number of deep learning problems involving
    CNNs. This will also help us in building more complex deep learning models or
    applications of which CNNs are a part.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why are CNNs so powerful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of CNN architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing LeNet from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the AlexNet model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a pre-trained VGG model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring GoogLeNet and Inception v3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing ResNet and DenseNet architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding EfficientNets and the future of CNN architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are CNNs so powerful?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs are among the most powerful machine learning models at solving challenging
    problems such as image classification, object detection, object segmentation,
    video processing, natural language processing, and speech recognition. Their success
    is attributed to various factors, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight sharing**: This makes CNNs parameter-efficient, that is, different
    features are extracted using the same set of weights or parameters. **Features**
    are the high-level representations of input data that the model generates with
    its parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic feature extraction**: Multiple feature extraction stages help a
    CNN to automatically learn feature representations in a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical learning**: The multi-layered CNN structure helps CNNs to learn
    low-, mid-, and high-level features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to explore both **spatial and temporal** correlations in the data,
    such as in video- processing tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides these pre-existing fundamental characteristics, CNNs have advanced
    over the years with the help of improvements in the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: The use of better **activation** and **loss functions**, such as using **ReLU**
    to overcome the **vanishing gradient problem**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter optimization, such as using an optimizer based on Adaptive Momentum
    (Adam) instead of simple Stochastic Gradient Descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization: Applying dropouts and batch normalization besides L2 regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FAQ - What is the vanishing gradient problem?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Backpropagation in neural networks works on the basis of the chain rule of differentiation.
    According to the chain rule, the gradient of the loss function with respect to
    the input layer parameters can be written as a product of gradients at each layer.
    If these gradients are all less than 1 – and worse still, tending toward 0 – then
    the product of these gradients will be a vanishingly small value. The vanishing
    gradient problem can cause serious troubles in the optimization process by preventing
    the network parameters from changing their values, which is equivalent to stunted
    learning.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But some of the most significant drivers of development in CNNs over the years
    have been the various *architectural innovations*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial exploration-based CNNs**: The idea behind **spatial exploration**
    is using different kernel sizes in order to explore different levels of visual
    features in input data. The following diagram shows a sample architecture for
    a spatial exploration-based CNN model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Spatial exploration-based CNN](img/file8.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.1 – Spatial exploration-based CNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Depth-based CNNs**: The **depth** here refers to the depth of the neural
    network, that is, the number of layers. So, the idea here is to create a CNN model
    with multiple convolutional layers in order to extract highly complex visual features.
    The following diagram shows an example of such a model architecture:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Depth-based CNN](img/file9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.2 – Depth-based CNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Width-based CNNs**: **Width** refers to the number of channels or feature
    maps in the data or features extracted from the data. So, width-based CNNs are
    all about increasing the number of feature maps as we go from the input to the
    output layers, as demonstrated in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Width-based CNN](img/file10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.3 – Width-based CNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multi-path-based CNNs**: So far, the preceding three types of architectures
    have monotonicity in connections between layers, that is, direct connections exist
    only between consecutive layers. **Multi-path CNNs** brought the idea of making
    shortcut connections or skip connections between non-consecutive layers. The following
    diagram shows an example of a multi-path CNN model architecture:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Multi-path CNN](img/file11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Multi-path CNN
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of multi-path architectures is a better flow of information
    across several layers, thanks to the skip connections. This, in turn, also lets
    the gradient flow back to the input layers without too much dissipation.
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at the different architectural setups found in CNN models, we
    will now look at how CNNs have evolved over the years ever since they were first
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of CNN architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs have been in existence since 1989, when the first multilayered CNN, called
    **ConvNet**, was developed by Yann LeCun. This model could perform visual cognition
    tasks such as identifying handwritten digits. In 1998, LeCun developed an improved
    ConvNet model called **LeNet**. Due to its high accuracy in optical recognition
    tasks, LeNet was adopted for industrial use soon after its invention. Ever since,
    CNNs have been one of the most successful machine learning models, both in industry
    as well as academia. The following diagram shows a brief timeline of architectural
    developments in the lifetime of CNNs, starting from 1989 all the way to 2020:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – CNN architecture evolution – a broad picture](img/file12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – CNN architecture evolution – a broad picture
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there is a significant gap between the years 1998 and 2012\.
    This was primarily because there wasn't a dataset big and suitable enough to demonstrate
    the capabilities of CNNs, especially deep CNNs. And on the existing small datasets
    of the time, such as MNIST, classical machine learning models such as SVMs were
    starting to beat CNN performance. During those years, a few CNN developments took
    place.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU activation function was designed in order to deal with the gradient
    explosion and decay problem during backpropagation. Non-random initialization
    of network parameter values proved to be crucial. **Max-pooling** was invented
    as an effective method for subsampling. GPUs were getting popular for training
    neural networks, especially CNNs at scale. Finally, and most importantly, a large-scale
    dedicated dataset of annotated images called **ImageNet** [3.1] was created by
    a research group at Stanford. This dataset is still one of the primary benchmarking
    datasets for CNN models to date.
  prefs: []
  type: TYPE_NORMAL
- en: With all of these developments compounding over the years, in 2012, a different
    architectural design brought about a massive improvement in CNN performance on
    the `ImageNet` dataset. This network was called **AlexNet** (named after the creator,
    Alex Krizhevsky). AlexNet, along with having various novel aspects such as random
    cropping and pre-training, established the trend of uniform and modular convolutional
    layer design. The uniform and modular layer structure was taken forward by repeatedly
    stacking such modules (of convolutional layers), resulting in very deep CNNs also
    known as **VGGs**.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach of branching the blocks/modules of convolutional layers and
    stacking these branched blocks on top of each other proved extremely effective
    for tailored visual tasks. This network was called **GoogLeNet** (as it was developed
    at Google) or **Inception v1** (inception being the term for those branched blocks).
    Several variants of the **VGG** and **Inception** networks followed, such as **VGG16**,
    **VGG19**, **Inception v2**, **Inception v3**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The next phase of development began with **skip connections**. To tackle the
    problem of gradient decay while training CNNs, non-consecutive layers were connected
    via skip connections lest information dissipated between them due to small gradients.
    A popular type of network that emerged with this trick, among other novel characteristics
    such as batch normalization, was **ResNet**.
  prefs: []
  type: TYPE_NORMAL
- en: A logical extension of ResNet was **DenseNet**, where layers were densely connected
    to each other, that is, each layer gets the input from all the previous layers'
    output feature maps. Furthermore, hybrid architectures were then developed by
    mixing successful architectures from the past such as **Inception-ResNet** and
    **ResNeXt**, where the parallel branches within a block were increased in number.
  prefs: []
  type: TYPE_NORMAL
- en: Lately, the **channel boosting** technique has proven useful in improving CNN
    performance. The idea here is to learn novel features and exploit pre-learned
    features through transfer learning. Most recently, automatically designing new
    blocks and finding optimal CNN architectures has been a growing trend in CNN research.
    Examples of such CNNs are **MnasNets** and **EfficientNets**. The approach behind
    these models is to perform a neural architecture search to deduce an optimal CNN
    architecture with a uniform model scaling approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go back to one of the earliest CNN models and take
    a closer look at the various CNN architectures developed since. We will build
    these architectures using PyTorch, training some of the models on real-world datasets.
    We will also explore PyTorch's pre-trained CNN models repository, popularly known
    as **model-zoo**. We will learn how to fine-tune these pre-trained models as well
    as running predictions on them.
  prefs: []
  type: TYPE_NORMAL
- en: Developing LeNet from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LeNet, originally known as **LeNet-5**, is one of the earliest CNN models, developed
    in 1998\. The number *5* in LeNet-5 represents the *total number of layers* in
    this model, that is, two convolutional and three fully connected layers. With
    roughly 60,000 total parameters, this model gave state-of-the-art performance
    on image recognition tasks for handwritten digit images in the year 1998\. As
    expected from a CNN model, LeNet demonstrated rotation, position, and scale invariance
    as well as robustness against distortion in images. Contrary to the classical
    machine learning models of the time, such as SVMs, which treated each pixel of
    the image separately, LeNet exploited the correlation among neighboring pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that although LeNet was developed for handwritten digit recognition, it
    can certainly be extended for other image classification tasks, as we shall see
    in our next exercise. The following diagram shows the architecture of a LeNet
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – LeNet architecture](img/file13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – LeNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, there are two convolutional layers followed by three fully
    connected layers (including the output layer). This approach of stacking convolutional
    layers followed by fully connected layers later became a trend in CNN research
    and is still applied to the latest CNN models. Besides these layers, there are
    pooling layers in between. These are basically subsampling layers that reduce
    the spatial size of image representation, thereby reducing the number of parameters
    and computations. The pooling layer used in LeNet was an average pooling layer
    that had trainable weights. Soon after, **max pooling** emerged as the most commonly
    used pooling function in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The numbers in brackets in each layer in the figure demonstrate the dimensions
    (for input, output, and fully connected layers) or window size (for convolutional
    and pooling layers). The expected input for a grayscale image is 32x32 pixels
    in size. This image is then operated on by 5x5 convolutional kernels, followed
    by 2x2 pooling, and so on. The output layer size is 10, representing the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use PyTorch to build LeNet from scratch and train and
    evaluate it on a dataset of images for the task of image classification. We will
    see how easy and intuitive it is to build the network architecture in PyTorch
    using the outline from *Figure 3.6*.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we will demonstrate how effective LeNet is, even on a dataset different
    from the ones it was originally developed on (that is, MNIST) and how PyTorch
    makes it easy to train and test the model in a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch to build LeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Observe the following steps to build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    `import` statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Besides the usual imports, we also invoke the use_deterministic_algorithms function
    to ensure the reproducibility of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define the model architecture based on the outline given in *Figure
    3.6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the last two lines, we instantiate the model and print the network architecture.
    The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – LeNet PyTorch model object](img/file14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – LeNet PyTorch model object
  prefs: []
  type: TYPE_NORMAL
- en: There are the usual `__init__` and `forward` methods for architecture definition
    and running a forward pass, respectively. The additional `flattened_features`
    method is meant to calculate the total number of features in an image representation
    layer (usually an output of a convolutional layer or pooling layer). This method
    helps to flatten the spatial representation of features into a single vector of
    numbers, which is then used as input to fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the details of the architecture mentioned earlier, ReLU is used throughout
    the network as the activation function. Also, contrary to the original LeNet network,
    which takes in single-channel images, the current model is modified to accept
    RGB images, that is, three channels as input. This is done in order to adapt to
    the dataset that is used for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the training routine, that is, the actual backpropagation step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For each epoch, this function iterates through the entire training dataset,
    runs a forward pass through the network and, using backpropagation, updates the
    parameters of the model based on the specified optimizer. After iterating through
    every 1,000 mini-batches of the training dataset, this method also logs the calculated
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the training routine, we will define the test routine that we will
    use to evaluate model performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function runs a forward pass through the model for each test-set image,
    calculates the correct number of predictions, and prints the percentage of correct
    predictions on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get on to training the model, we need to load the dataset. For this
    exercise, we will be using the `CIFAR-10` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset citation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Learning Multiple Layers of Features from Tiny Images*, Alex Krizhevsky, 2009'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This dataset consists of 60,000 32x32 RGB images labeled across 10 classes,
    with 6,000 images per class. The 60,000 images are split into 50,000 training
    images and 10,000 test images. More details can be found at the dataset website
    [3.2] . Torch provides the `CIFAR` dataset under the `torchvision.datasets` module.
    We will be using the module to directly load the data and instantiate train and
    test dataloaders as demonstrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous chapter, we manually downloaded the dataset and wrote a custom
    dataset class and a `dataloader` function. We will not need to write those here,
    thanks to the `torchvision.datasets` module.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Because we set the `download` flag to `True`, the dataset will be downloaded
    locally. Then, we shall see the following output :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – CIFAR-10 dataset download](img/file15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – CIFAR-10 dataset download
  prefs: []
  type: TYPE_NORMAL
- en: The transformations used for training and testing datasets are different because
    we apply some data augmentation to the training dataset, such as flipping and
    cropping, which are not applicable to the test dataset. Also, after defining `trainloader`
    and `testloader`, we declare the 10 classes in this dataset with a pre-defined
    ordering.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the datasets, let''s investigate how the data looks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows us four sample images with their respective labels
    from the training dataset. The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – CIFAR-10 dataset samples](img/file16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – CIFAR-10 dataset samples
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output shows us four color images, which are 32x32 pixels in size.
    These four images belong to four different labels, as displayed in the text following
    the images.
  prefs: []
  type: TYPE_NORMAL
- en: We will now train the LeNet model.
  prefs: []
  type: TYPE_NORMAL
- en: Training LeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us train the model with the help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define the `optimizer` and start the training loop as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Training LeNet](img/file17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Training LeNet
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is finished, we can save the model file locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Having trained the LeNet model, we will now test its performance on the test
    dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Testing LeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following steps need to be followed to test the LeNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make predictions by loading the saved model and running it on the test
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – LeNet predictions](img/file18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – LeNet predictions
  prefs: []
  type: TYPE_NORMAL
- en: Evidently, three out of four predictions are correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will check the overall accuracy of this model on the test dataset
    as well as per- class accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – LeNet overall accuracy](img/file19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – LeNet overall accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'For per- class accuracy, the code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – LeNet per class accuracy](img/file20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – LeNet per class accuracy
  prefs: []
  type: TYPE_NORMAL
- en: Some classes have better performance than others. Overall, the model is far
    from perfect (that is, 100% accuracy) but much better than a model making random
    predictions, which would have an accuracy of 10% (due to the 10 classes).
  prefs: []
  type: TYPE_NORMAL
- en: Having built a LeNet model from scratch and evaluated its performance using
    PyTorch, we will now move on to a successor of LeNet – **AlexNet**. For LeNet,
    we built the model from scratch, trained, and tested it. For AlexNet, we will
    use a pre-trained model, fine-tune it on a smaller dataset, and test it.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the AlexNet model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will first take a quick look at the AlexNet architecture
    and how to build one using PyTorch. Then we will explore PyTorch's pre-trained
    CNN models repository, and finally, use a pre-trained AlexNet model for fine-tuning
    on an image classification task, as well as making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'AlexNet is a successor of LeNet with incremental changes in the architecture,
    such as 8 layers (5 convolutional and 3 fully connected) instead of 5, and 60
    million model parameters instead of 60,000, as well as using `MaxPool` instead
    of `AvgPool`. Moreover, AlexNet was trained and tested on a much bigger dataset
    – ImageNet, which is over 100 GB in size, as opposed to the MNIST dataset (on
    which LeNet was trained), which amounts to a few MBs. AlexNet truly revolutionized
    CNNs as it emerged as a significantly more powerful class of models on image-related
    tasks than the other classical machine learning models, such as SVMs. *Figure
    3.14* shows the AlexNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – AlexNet architecture](img/file21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – AlexNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the architecture follows the common theme from LeNet of having
    convolutional layers stacked sequentially, followed by a series of fully connected
    layers toward the output end. PyTorch makes it easy to translate such a model
    architecture into actual code. This can be seen in the following PyTorch code-
    equivalent of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The code is quite self-explanatory, wherein the `__init__` function contains
    the initialization of the whole layered structure, consisting of convolutional,
    pooling, and fully connected layers, along with ReLU activations. The `forward`
    function simply runs a data point *x* through this initialized network. Please
    note that the second line of the `forward` method already performs the flattening
    operation so that we need not define that function separately as we did for LeNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'But besides the option of initializing the model architecture and training
    it ourselves, PyTorch, with its `torchvision` package, provides a `models` sub-package,
    which contains definitions of CNN models meant for solving different tasks, such
    as image classification, semantic segmentation, object detection, and so on. Following
    is a non-exhaustive list of available models for the task of image classification
    [3.3] :'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SqueezeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ShuffleNet v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNet v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNeXt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MNASNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will use a pre-trained AlexNet model as an example and
    demonstrate how to fine-tune it using PyTorch in the form of an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch to fine-tune AlexNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following exercise, we will load a pre-trained AlexNet model and fine-tune
    it on an image classification dataset different from ImageNet (on which it was
    originally trained). Finally, we will test the fine-tuned model''s performance
    to see if it could transfer-learn from the new dataset. Some parts of the code
    in the exercise are trimmed for readability but you can find the full code in
    our github repo [3.4] :'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will download and transform the dataset. For this fine-tuning exercise,
    we will use a small image dataset of bees and ants. There are 240 training images
    and 150 validation images divided equally between the two classes (bees and ants).
  prefs: []
  type: TYPE_NORMAL
- en: We download the dataset from kaggel [3.5] and store it in the current working
    directory. More information about the dataset can be found at the dataset’s website[3.6]
    .
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Elsik CG, Tayal A, Diesh CM, Unni DR, Emery ML, Nguyen HN, Hagen DE. Hymenoptera
    Genome Database: integrating genome annotations in HymenopteraMine. Nucleic Acids
    Research 2016 Jan 4;44(D1):D793-800\. doi: 10.1093/nar/gkv1208\. Epub 2015 Nov
    17\. PubMed PMID: 26578564.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In order to download the dataset, you will need to log into Kaggle. If you
    do not already have a Kaggle account, you will need to register:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have completed the pre-requisites, let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize some sample training dataset images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Bees versus ants dataset](img/file22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Bees versus ants dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We now define the fine-tuning routine, which is essentially a training routine
    performed on a pre-trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we require the pre-trained model (that is, the architecture
    as well as weights) as input along with the loss function, optimizer, and number
    of epochs. Basically, instead of starting from a random initialization of weights,
    we start with the pre-trained weights of AlexNet. The other parts of this function
    are pretty similar to our previous exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting to fine-tune (train) the model, we will define a function to
    visualize the model predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we get to the interesting part. Let''s use PyTorch''s `torchvision.models`
    sub-package to load the pre-trained AlexNet model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This model object has the following two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'i) `features`: The feature extraction component, which contains all the convolutional
    and pooling layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'ii) `classifier`: The classifier block, which contains all the fully connected
    layers leading to the output layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize these components as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – AlexNet feature extractor](img/file23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – AlexNet feature extractor
  prefs: []
  type: TYPE_NORMAL
- en: 'Next , we inspect the `classifier` block as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – AlexNet classifier](img/file24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – AlexNet classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have noticed, the pre-trained model has the output layer of size
    `1000`, but we only have `2` classes in our fine-tuning dataset. So, we shall
    alter that, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, we are all set to define the optimizer and loss function, and thereafter
    run the training routine as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – AlexNet fine-tuning loop](img/file25.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – AlexNet fine-tuning loop
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize some of the model predictions to see whether the model has
    indeed learned the relevant features from this small dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – AlexNet predictions](img/file26.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – AlexNet predictions
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the pretrained AlexNet model has been able to transfer-learn on this
    rather tiny image classification dataset. This both demonstrates the power of
    transfer learning as well as the speed and ease with which we can fine-tune well
    known models using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss an even deeper and more complex successor
    of AlexNet – the VGG network. We have demonstrated the model definition, dataset
    loading, model training (or fine-tuning), and evaluation steps in detail for LeNet
    and AlexNet. In subsequent sections, we will focus mostly on model architecture
    definition, as the PyTorch code for other aspects (such as data loading and evaluation)
    will be similar.
  prefs: []
  type: TYPE_NORMAL
- en: Running a pre-trained VGG model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already discussed LeNet and AlexNet, two of the foundational CNN architectures.
    As we progress in the chapter, we will explore increasingly complex CNN models.
    Although, the key principles in building these model architectures will be the
    same. We will see a modular model-building approach in putting together convolutional
    layers, pooling layers, and fully connected layers into blocks/modules and then
    stacking these blocks sequentially or in a branched manner. In this section, we
    look at the successor to AlexNet – VGGNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name VGG is derived from the **Visual Geometry Group of Oxford University**,
    where this model was invented. Compared to the 8 layers and 60 million parameters
    of AlexNet, VGG consists of 13 layers (10 convolutional layers and 3 fully connected
    layers) and 138 million parameters. VGG basically stacks more layers onto the
    AlexNet architecture with smaller size convolution kernels (2x2 or 3x3). Hence,
    VGG''s novelty lies in the unprecedented level of depth that it brings with its
    architecture. *Figure 3.20* shows the VGG architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – VGG16 architecture](img/file27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – VGG16 architecture
  prefs: []
  type: TYPE_NORMAL
- en: The preceding VGG architecture is called **VGG13**, because of the 13 layers.
    Other variants are VGG16 and VGG19, consisting of 16 and 19 layers, respectively.
    There is another set of variants – **VGG13_bn**, **VGG16_bn**, and **VGG19_bn**,
    where **bn** suggests that these models also consist of **batch-normalization
    layers**.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch''s `torchvision.model` sub-package provides the pre-trained `VGG` model
    (with all of the six variants discussed earlier) trained on the ImageNet dataset.
    In the following exercise, we will use the pre-trained `VGG13` model to make predictions
    on a small dataset of bees and ants (used in the previous exercise). We will focus
    on the key pieces of code here, as most other parts of our code will overlap with
    that of the previous exercises. We can always refer to our notebooks to explore
    the full code [3.7]:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to import dependencies, including `torchvision.models`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data and set up the ants and bees dataset and dataloader, along
    with the transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to make predictions on these images, we will need to download the 1,000
    labels of the ImageNet dataset [3.8] .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once downloaded, we need to create a mapping between the class indices 0 to
    999 and the corresponding class labels, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the first five class mappings, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – ImageNet class mappings](img/file28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – ImageNet class mappings
  prefs: []
  type: TYPE_NORMAL
- en: Define the model prediction visualization function that takes in the pre-trained
    model object and the number of images to run predictions on. This function should
    output the images with predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the pretrained `VGG13` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Loading the VGG13 model](img/file29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Loading the VGG13 model
  prefs: []
  type: TYPE_NORMAL
- en: The `VGG13` model is downloaded in this step.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ - What is the disk size of VGG13 model?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: VGG13 model will consume roughly 508 MB on your hard disk.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, we run predictions on our ants and bees dataset using this pre-trained
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – VGG13 predictions](img/file30.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – VGG13 predictions
  prefs: []
  type: TYPE_NORMAL
- en: The `VGG13` model trained on an entirely different dataset seems to predict
    all the test samples correctly in the ants and bees dataset. Basically, the model
    grabs the two most similar animals from the dataset out of the 1,000 classes and
    finds them in the images. By doing this exercise, we see that the model is still
    able to extract relevant visual features out of the images and the exercise demonstrates
    the utility of PyTorch's out-of-the-box inference feature.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to study a different type of CNN architecture
    – one that involves modules that have multiple parallel convolutional layers.
    The modules are called **Inception modules** and the resulting network is called
    the **Inception network**. We will explore the various parts of this network and
    the reasoning behind its success. We will also build the inception modules and
    the Inception network architecture using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring GoogLeNet and Inception v3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have discovered the progression of CNN models from LeNet to VGG so far,
    we have observed the sequential stacking of more convolutional and fully connected
    layers. This resulted in deep networks with a lot of parameters to train. *GoogLeNet*
    emerged as a radically different type of CNN architecture that is composed of
    a module of parallel convolutional layers called the inception module. Because
    of this, GoogLeNet is also called **Inception v1** (v1 marked the first version
    as more versions came along later). Some of the drastically new elements introduced
    in GoogLeNet were the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The **inception module** – a module of several parallel convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **1x1 convolutions** to reduce the number of model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global average pooling** instead of a fully connected layer – reduces overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **auxiliary classifiers** for training – for regularization and gradient
    stability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet has 22 layers, which is more than the number of layers of any VGG
    model variant. Yet, due to some of the optimization tricks used, the number of
    parameters in GoogLeNet is 5 million, which is far less than the 138 million parameters
    of VGG. Let's expand on some of the key features of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Inception modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perhaps the single most important contribution of this model was the development
    of a convolutional module with several convolutional layers running in parallel,
    which are finally concatenated to produce a single output vector. These parallel
    convolutional layers operate with different kernel sizes ranging from 1x1 to 3x3
    to 5x5\. The idea is to extract all levels of visual information from the image.
    Besides these convolutions, a 3x3 max-pooling layer adds another level of feature
    extraction. *Figure 3.24* shows the inception block diagram along with the overall
    GoogLeNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – GoogLeNet architecture](img/file31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – GoogLeNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'By using this architecture diagram, we can build the inception module in PyTorch
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will look at another important feature of GoogLeNet – 1x1 convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 1x1 convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the parallel convolutional layers in an inception module, each
    parallel layer has a preceding **1x1 convolutional layer**. The reason behind
    using these 1x1 convolutional layers is *dimensionality reduction*. 1x1 convolutions
    do not change the width and height of the image representation but can alter the
    depth of an image representation. This trick is used to reduce the depth of the
    input visual features before performing the 1x1, 3x3, and 5x5 convolutions parallelly.
    Reducing the number of parameters not only helps build a lighter model but also
    combats overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Global average pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we look at the overall GoogLeNet architecture in *Figure 3.24*, the penultimate
    output layer of the model is preceded by a 7x7 average pooling layer. This layer
    again helps in reducing the number of parameters of the model, thereby reducing
    overfitting. Without this layer, the model would have millions of additional parameters
    due to the dense connections of a fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary classifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Figure 3.24* also shows two extra or auxiliary output branches in the model.
    These auxiliary classifiers are supposed to tackle the vanishing gradient problem
    by adding to the gradients'' magnitude during backpropagation, especially for
    the layers towards the input end. Because these models have a large number of
    layers, vanishing gradients can become a bottleneck. Hence, using auxiliary classifiers
    has proven useful for this 22-layer deep model. Additionally, the auxiliary branches
    also help in regularization. Please note that these auxiliary branches are switched
    off/discarded while making predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the inception module defined using PyTorch, we can easily instantiate
    the entire Inception v1 model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides instantiating our own model, we can always load a pre-trained GoogLeNet
    with just two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Finally, as mentioned earlier, a number of versions of the Inception model were
    developed later. One of the eminent ones was Inception v3, which we will briefly
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Inception v3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This successor of Inception v1 has a total of 24 million parameters as compared
    to 5 million in v1\. Besides the addition of several more layers, this model introduced
    different kinds of inception modules, which are stacked sequentially. *Figure
    3.25* shows the different inception modules and the full model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.25 – Inception v3 architecture](img/file32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3.25 – Inception v3 architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be seen from the architecture that this model is an architectural extension
    of the Inception v1 model. Once again, besides building the model manually, we
    can use the pre-trained model from PyTorch''s repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will go through the classes of CNN models that have
    effectively combatted the vanishing gradient problem in very deep CNNs – **ResNet**
    and **DenseNet**. We will learn about the novel techniques of skip connections
    and dense connections and use PyTorch to code the fundamental modules behind these
    advanced architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing ResNet and DenseNet architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we explored the Inception models, which had a reduced
    number of model parameters as the number of layers increased, thanks to the 1x1
    convolutions and global average pooling. Furthermore, auxiliary classifiers were
    used to combat the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet introduced the concept of **skip connections**. This simple yet effective
    trick overcomes the problem of both parameter overflow and vanishing gradients.
    The idea, as shown in the following diagram, is quite simple. The input is first
    passed through a non-linear transformation (convolutions followed by non-linear
    activations) and then the output of this transformation (referred to as the residual)
    is added to the original input. Each block of such computation is called a **residual
    block**, hence the name of the model – **residual network** or **ResNet**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Skip connections](img/file33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Skip connections
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these skip (or shortcut) connections, the number of parameters is limited
    to 26 million parameters for a total of 50 layers (ResNet-50). Due to the limited
    number of parameters, ResNet has been able to generalize well without overfitting
    even when the number of layers is increased to 152 (ResNet-152). The following
    diagram shows the ResNet-50 architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – ResNet architecture](img/file34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – ResNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of residual blocks – **convolutional** and **identity**,
    both having skip connections. For the convolutional block, there is an added 1x1
    convolutional layer, which further helps to reduce dimensionality. A residual
    block for ResNet can be implemented in PyTorch as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To get started quickly with ResNet, we can always use the pre-trained ResNet
    model from PyTorch''s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ResNet uses the identity function (by directly connecting input to output) to
    preserve the gradient during backpropagation (as the gradient will be 1). Yet,
    for extremely deep networks, this principle might not be sufficient to preserve
    strong gradients from the output layer back to the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN model we will discuss next is designed to ensure a strong gradient flow,
    as well as a further reduction in the number of required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The skip connections of ResNet connected the input of a residual block directly
    to its output. However, the inter-residual-blocks connection is still sequential,
    that is, residual block number 3 has a direct connection with block 2 but no direct
    connection with block 1.
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet, or dense networks, introduced the idea of connecting every convolutional
    layer with every other layer within what is called a **dense block**. And every
    dense block is connected to every other dense block in the overall DenseNet. A
    dense block is simply a module of two 3x3 densely connected convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: These dense connections ensure that every layer is receiving information from
    all of the preceding layers of the network. This ensures that there is a strong
    gradient flow from the last layer down to the very first layer. Counterintuitively,
    the number of parameters of such a network setting will also be low. As every
    layer is receiving the feature maps from all the previous layers, the required
    number of channels (depth) can be fewer. In the earlier models, the increasing
    depth represented the accumulation of information from earlier layers, but we
    don't need that anymore, thanks to the dense connections everywhere in the network.
  prefs: []
  type: TYPE_NORMAL
- en: One key difference between ResNet and DenseNet is also that, in ResNet, the
    input was added to the output using skip connections. But in the case of DenseNet,
    the preceding layers' outputs are concatenated with the current layer's output.
    And the concatenation happens in the depth dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might raise a question about the exploding size of outputs as we proceed
    further in the network. To combat this compounding effect, a special type of block
    called the **transition block** is devised for this network. Composed of a 1x1
    convolutional layer followed by a 2x2 pooling layer, this block standardizes or
    resets the size of the depth dimension so that the output of this block can then
    be fed to the subsequent dense block(s). The following diagram shows the DenseNet
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – DenseNet architecture](img/file35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – DenseNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, there are two types of blocks involved – the **dense
    block** and the **transition block**. These blocks can be written as classes in
    PyTorch in a few lines of code, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'These blocks are then stacked densely to form the overall DenseNet architecture.
    DenseNet, like ResNet, comes in variants such as **DenseNet121**, **DenseNet161**,
    **DenseNet169**, and **DenseNet201**, where the numbers represent the total number
    of layers. Such large numbers of layers are obtained by the repeated stacking
    of the dense and transition blocks plus a fixed 7x7 convolutional layer at the
    input end and a fixed fully connected layer at the output end. PyTorch provides
    pre-trained models for all of these variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'DenseNet outperforms all the models discussed so far on the ImageNet dataset.
    Various hybrid models have been developed by mixing and matching the ideas presented
    in the previous sections. The Inception-ResNet and ResNeXt models are examples
    of such hybrid networks. The following diagram shows the ResNeXt architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – ResNeXt architecture](img/file36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – ResNeXt architecture
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it looks like a wider variant of a *ResNet + Inception* hybrid
    because there is a large number of parallel convolutional branches in the residual
    blocks – and the idea of parallelism is derived from the inception network.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and last section of this chapter, we are going to look at the current
    best performing CNN architectures – EfficientNets. We will also discuss the future
    of CNN architectural development while touching upon the use of CNN architectures
    for tasks beyond image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding EfficientNets and the future of CNN architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in our exploration from LeNet to DenseNet, we have noticed an underlying
    theme in the advancement of CNN architectures. That theme is the expansion or
    scaling of the CNN model through one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An increase in the number of layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An increase in the number of feature maps or channels in a convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An increase in the spatial dimension going from 32x32 pixel images in LeNet
    to 224x224 pixel images in AlexNet and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These three different aspects on which scaling can be performed are identified
    as *depth*, *width*, and *resolution*, respectively. Instead of manually scaling
    these attributes, which often leads to suboptimal results, **EfficientNets** use
    neural architecture search to calculate the optimal scaling factors for each of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up depth is deemed important because the deeper the network, the more
    complex the model, and hence it can learn highly complex features. However, there
    is a trade-off because, with increasing depth, the vanishing gradient problem
    escalates along with the general problem of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, scaling up width should theoretically help, as with a greater number
    of channels, the network should learn more fine-grained features. However, for
    extremely wide models, the accuracy tends to saturate quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, higher resolution images, in theory, should work better as they have
    more fine-grained information. Empirically, however, the increase in resolution
    does not yield a linearly equivalent increase in the model performance. All of
    this is to say that there are trade-offs to be made while deciding the scaling
    factors and hence, neural architecture search helps in finding the optimal scaling
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'EfficientNet proposes finding the architecture that has the right balance between
    depth, width, and resolution, and all three of these aspects are scaled together
    using a global scaling factor. The EfficientNet architecture is built in two steps.
    First, a basic architecture (called the **base network**) is devised by fixing
    the scaling factor to `1`. At this stage, the relative importance of depth, width,
    and resolution is decided for the given task and dataset. The base network obtained
    is pretty similar to a well-known CNN architecture – **MnasNet**, short for **Mobile
    Neural Architecture Search Network**. PyTorch offers the pre-trained `MnasNet`
    model, which can be loaded as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the base network is obtained in the first step, the optimal global scaling
    factor is then computed with the aim of maximizing the accuracy of the model and
    minimizing the number of computations (or flops). The base network is called **EfficientNet
    B0** and the subsequent networks derived for different optimal scaling factors
    are called **EfficientNet B1-B7**. PyTorch provides pre-trained models for all
    of these variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we go forward, efficient scaling of CNN architecture is going to be a prominent
    direction of research along with the development of more sophisticated modules
    inspired by the inception, residual, and dense modules. Another aspect of CNN
    architecture development is minimizing the model size while retaining performance.
    **MobileNets** [3.9] are a prime example and there is a lot of ongoing research
    on this front.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the top-down approach of looking at architectural modifications of a
    pre-existing model, there will be continued efforts adopting the bottom-up view
    of fundamentally rethinking the units of CNNs such as the convolutional kernels,
    pooling mechanism, more effective ways of flattening, and so on. One concrete
    example of this could be **CapsuleNet** [3.10] , which revamped the convolutional
    units to cater to the third dimension (depth) in images.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are a huge topic of study in themselves. In this chapter, we have touched
    upon the architectural development of CNNs, mostly in the context of image classification.
    However, these same architectures are used across a wide variety of applications.
    One well-known example is the use of ResNets for object detection and segmentation
    in the form of **RCNNs** [3.11] . Some of the improved variants of RCNNs are **Faster
    R-CNN**, **Mask-RCNN**, and **Keypoint-RCNN**. PyTorch provides pre-trained models
    for all three variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch also provides pre-trained models for ResNets that are applied to video-related
    tasks such as video classification. Two such ResNet-based models used for video
    classification are **ResNet3D** and **ResNet Mixed Convolution**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: While we do not extensively cover these different applications and corresponding
    CNN models in this chapter, we encourage you to read more on them. PyTorch's website
    can be a good starting point [3.12] .
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter has been all about CNN architectures. In the next chapter, we will
    explore a similar journey but for another important type of neural network – recurrent
    neural networks. We will discuss the various recurrent net architectures and use
    PyTorch to effectively implement, train, and test them.
  prefs: []
  type: TYPE_NORMAL
