["```py\nmdl_pth = './transformer.pth'\ntorch.save(best_model_so_far.state_dict(), mdl_pth)\n```", "```py\n# load the best trained model\ntransformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, dropout).to(device)\ntransformer_cached.load_state_dict(torch.load(mdl_pth))\n```", "```py\nln = 5     \nsntc = 'They are       _'\nsntc_split = sntc.split()\nmask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n```", "```py\ntorch.manual_seed(34     )\nwith torch.no_grad():\n    for i in range(ln):\n        sntc = ' '.join(sntc_split)\n        txt_ds = Tensor(vocabulary(sntc_split)).unsqueeze(0).to(torch.long)\n        num_b = txt_ds.size(0)\n        txt_ds = txt_ds.narrow(0, 0, num_b)\n        txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)\n        ev_X, _ = return_batch(txt_ds, i+1)\n        sequence_length = ev_X.size(0)\n        if sequence_length != max_seq_len:\n            mask_source = mask_source[:sequence_length, :sequence_length]\n        op = transformer_cached(ev_X, mask_source)\n        op_flat = op.view(-1, num_tokens)\n        res = vocabulary.get_itos()[op_flat.argmax(1)[0]]\n        sntc_split.insert(-1, res)\n\nprint(sntc[:-2])\n```", "```py\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n```", "```py\ntorch.manual_seed(799)\ntkz = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmdl = GPT2LMHeadModel.from_pretrained('gpt2')\nln = 10\ncue = \"They     \"\ngen = tkz(cue, return_tensors=\"pt\")     \nto_ret      = gen[\"input_ids\"][0] \n```", "```py\nprv=None\nfor i in range(ln):\n    outputs = mdl(**gen)\n    next_token_logits = torch.argmax(outputs.logits[-1, :])\n    to_ret = torch.cat([to_ret, next_token_logits.unsqueeze(0)])\n    gen = {\"input_ids\": to_ret}\nseq = tkz.decode(to_ret)\nprint(seq) \n```", "```py\nip_ids = tkz.encode(cue, return_tensors='pt')\nop_greedy = mdl.generate(ip_ids, max_length=ln, pad_token_id=tkz.eos_token_id)\nseq = tkz.decode(op_greedy[0], skip_special_tokens=True)\nprint(seq) \n```", "```py\nop_beam = mdl.generate(\n    ip_ids,\n    max_length=5,\n    num_beams=3,\n    num_return_sequences=3,\n    pad_token_id=tkz.eos_token_id\n)\nfor op_beam_cur in op_beam:\n    print(tkz.decode(op_beam_cur, skip_special_tokens=True))\n```", "```py\nfor i in range(3):\n    torch.manual_seed(i+10)\n    op = mdl.generate(\n        ip_ids,\n        do_sample=True,\n        max_length=5,\n        top_k=2,\n        pad_token_id=tkz.eos_token_id\n    )\n    seq = tkz.decode(op[0], skip_special_tokens=True)\n    print(seq)\n```", "```py\nfor i in range(3):\n    torch.manual_seed(i+10)\n    op_greedy = mdl.generate(ip_ids, max_length=5, pad_token_id=tkz.eos_token_id)\n    seq = tkz.decode(op_greedy[0], skip_special_tokens=True)\n    print(seq) \n```", "```py\nfor i in range(3):\n    torch.manual_seed(i+10)\n    op = mdl.generate(\n        ip_ids,\n        do_sample=True,\n        max_length=5,\n        top_p=0.75,\n        top_k=0,\n        pad_token_id=tkz.eos_token_id\n    )\n    seq = tkz.decode(op[0], skip_special_tokens=True)\n    print(seq)\n```", "```py\nimport skimage.io as io\nfrom struct import pack, unpack\nfrom io import StringIO, BytesIO\n```", "```py\nNOTE_MIDI_OFF = 0x80\nNOTE_MIDI_ON = 0x90\nCHNL_PRESS = 0xD0\nMIDI_PITCH_BND = 0xE0\n...\n```", "```py\nclass MOStrm:\n# MIDI Output Stream\n...\nclass MIFl:\n# MIDI Input File Reader\n...\nclass MOFl(MOStrm):\n# MIDI Output File Writer\n...\nclass RIStrFl:\n# Raw Input Stream File Reader\n...\nclass ROStrFl:\n# Raw Output Stream File Writer\n...\nclass MFlPrsr:\n# MIDI File Parser\n...\nclass EvtDspch:\n# Event Dispatcher\n...\nclass MidiDataRead(MOStrm):\n# MIDI Data Reader\n...\n```", "```py\ndef md_fl_to_pio_rl(md_fl):\n    md_d = MidiDataRead(md_fl, dtm=0.3)\n    pio_rl = md_d.pio_rl.transpose()\n    pio_rl[pio_rl > 0] = 1    \n    return pio_rl\ndef pd_pio_rl(pio_rl, mx_l=132333, pd_v=0):        \n    orig_rol_len = pio_rl.shape[1]    \n    pdd_rol = np.zeros((88, mx_l))\n    pdd_rol[:] = pd_v    \n    pdd_rol[:, - orig_rol_len:] = pio_rl\n    return pdd_rol\n```", "```py\nclass NtGenDataset(data.Dataset):    \n    def __init__(self, md_pth, mx_seq_ln=1491):        \n        ...    \n    def mx_len_upd(self):        \n        ...   \n    def __len__(self):        \n        return len(self.md_fnames_ful)    \n    def __getitem__(self, index):        \n        md_fname_ful = self.md_fnames_ful[index]        \n        pio_rl = md_fl_to_pio_rl(md_fname_ful)\n        seq_len = pio_rl.shape[1] - 1\n        ip_seq = pio_rl[:, :-1]\n        gt_seq = pio_rl[:, 1:]\n        ...\n        return (torch.FloatTensor(ip_seq_pad),\n                torch.LongTensor(gt_seq_pad), torch.LongTensor([seq_len]))\n```", "```py\ndef pos_proc_seq(btch):\n    ip_seqs, op_seqs, lens = btch    \n    ...\n    ord_tr_data_tups = sorted(tr_data_tups,\n                                         key=lambda c: int(c[2]),\n                                         reverse=True)\n    ip_seq_splt_btch, op_seq_splt_btch, btch_splt_lens = zip(*ord_tr_data_tups)\n    ...  \n    return tps_ip_seq_btch, ord_op_seq_btch, list(ord_btch_lens_l)\n```", "```py\ntraining_dataset = NtGenDataset('./mozart/train', mx_seq_ln=None)\ntraining_datasetloader = data.DataLoader(training_dataset, batch_size=5,shuffle=True, drop_last=True)\nvalidation_dataset = NtGenDataset('./mozart/valid/', mx_seq_ln=None)\nvalidation_datasetloader = data.DataLoader(validation_dataset, batch_size=3, shuffle=False, drop_last=False)\nX_validation = next(iter(validation_datasetloader))\nX_validation[0].shape\n```", "```py\nclass MusicLSTM(nn.Module):    \n    def __init__(self, ip_sz, hd_sz, n_cls, lyrs=2):        \n        ...       \n        self.nts_enc = nn.Linear(in_features=ip_sz, out_features=hd_sz)        \n        self.bn_layer = nn.BatchNorm1d(hd_sz)        \n        self.lstm_layer = nn.LSTM(hd_sz, hd_sz, lyrs)        \n        self.fc_layer = nn.Linear(hd_sz, n_cls)\n\n    def forward(self, ip_seqs, ip_seqs_len, hd=None):\n        ...\n        pkd = torch.nn.utils.rnn.pack_padded_sequence(nts_enc_ful, ip_seqs_len)\n        op, hd = self.lstm_layer(pkd, hd)\n        ...\n        lgts = self.fc_layer(op_nrm_drp.permute(2,0,1))\n        ...\n        zero_one_lgts = torch.stack((lgts, rev_lgts), dim=3).contiguous()\n        flt_lgts = zero_one_lgts.view(-1, 2)\n        return flt_lgts, hd\n```", "```py\ndef lstm_model_training(lstm_model, lr, ep=10, val_loss_best=float(\"inf\")):\n    ...\n    for curr_ep in range(ep):\n        ...\n        for batch in training_datasetloader:\n            ...\n            lgts, _ = lstm_model(ip_seq_b_v, seq_l)\n            loss = loss_func(lgts, op_seq_b_v)\n            ...\n        if vl_ep_cur < val_loss_best:\n            torch.save(lstm_model.state_dict(), 'best_model.pth')\n            val_loss_best = vl_ep_cur\n    return val_loss_best, lstm_model\n```", "```py\ndef evaluate_model(lstm_model):\n    ...\n    for batch in validation_datasetloader:\n        ...\n        lgts, _ = lstm_model(ip_seq_b_v, seq_l)\n        loss = loss_func(lgts, op_seq_b_v)\n        vl_loss_full += loss.item()\n        seq_len += sum(seq_l)\n    return vl_loss_full/(seq_len*88)\n```", "```py\nloss_func = nn.CrossEntropyLoss().cpu()\nlstm_model = MusicLSTM(ip_sz=88, hd_sz=512, n_cls=88).cpu()\nval_loss_best, lstm_model = lstm_model_training(lstm_model, lr=0.01, ep=10)\n```", "```py\ndef generate_music(lstm_model, ln=100, tmp=1, seq_st=None):\n    ...\n    for i in range(ln):\n        op, hd = lstm_model(seq_ip_cur, [1], hd)\n        probs = nn.functional.softmax(op.div(tmp), dim=1)\n        ...\n    gen_seq = torch.cat(op_seq, dim=0).cpu().numpy()\n    return gen_seq\n```", "```py\nseq = generate_music(lstm_model, ln=100, tmp=1, seq_st=None)\nmidiwrite('generated_music.mid', seq, dtm=0.2)\n```", "```py\nio.imshow(seq)\n```"]