- en: Deep Learning Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: Deep learning is generally considered a subset of machine learning, involving
    the training of **artificial neural networks** (**ANNs**). ANNs are at the forefront
    of machine learning. They have the ability to solve complex problems involving
    massive amounts of data. Many of the principles of machine learning generally
    are also important in deep learning specifically, so we will spend some time reviewing
    these here.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常被认为是机器学习的一个子集，涉及**人工神经网络**（**ANNs**）的训练。人工神经网络处于机器学习的前沿。它们有能力解决涉及海量数据的复杂问题。机器学习的许多原则在深度学习中也同样重要，因此我们将在这里花一些时间进行回顾。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Approaches to machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习方法
- en: Learning tasks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习任务
- en: Features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征
- en: Models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: Artificial neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Approaches to machine learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习方法
- en: 'Prior to general machine learning, if we wanted to, for example, build a spam
    filter, we could start by compiling a list of words that commonly appear in spam.
    The spam detector then scans each email and when the number of blacklisted words
    reaches a threshold, the email would be classified as spam. This is called a rules-based
    approach, and is illustrated in the following diagram:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的机器学习出现之前，如果我们想要构建一个垃圾邮件过滤器，我们可以先编制一个常出现在垃圾邮件中的单词列表。然后垃圾邮件检测器会扫描每封电子邮件，当黑名单单词的数量达到阈值时，该电子邮件将被分类为垃圾邮件。这被称为基于规则的方法，如下图所示：
- en: '![](img/fb64e897-aa70-4633-bed0-090d24381b10.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb64e897-aa70-4633-bed0-090d24381b10.png)'
- en: 'The problem with this approach is that once the writers of spam know the rules,
    they are able to craft emails that avoid this filter. The people with the unenviable
    task of maintaining this spam filter would have to continually update the list
    of rules. With machine learning, we can effectively automate this rule-updating
    process. Instead of writing a list of rules, we build and train a model. As a
    spam detector, it will be more accurate since it can analyze large volumes of
    data. It is able to detect patterns in data that would be impossible for a human
    to do in a meaningful timeframe. The following diagram illustrates this approach:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的问题在于，一旦垃圾邮件的撰写者知道了规则，他们就能够制作避开这一过滤器的邮件。负责维护这个垃圾邮件过滤器的人们将不得不不断更新规则列表。通过机器学习，我们可以有效地自动化这个规则更新过程。我们不再需要撰写一长串规则，而是构建和训练一个模型。作为一个垃圾邮件检测器，它将更加准确，因为它可以分析大量数据。它能够检测到数据中的模式，这在有限的时间内对人类来说是不可能做到的。下图说明了这种方法：
- en: '![](img/742c2b29-9dc9-4346-bd61-2dec414eabb2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/742c2b29-9dc9-4346-bd61-2dec414eabb2.png)'
- en: 'There are a large number of ways that we can approach machine learning and
    these approaches are broadly characterized by the following factors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多种方法可以用来处理机器学习问题，这些方法大致可以根据以下因素进行分类：
- en: Whether or not models are trained with labelled training data. There are several
    possibilities here, including entirely supervised, semi-supervised, based on reinforcement,
    or entirely unsupervised.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否是用标记的训练数据进行训练。这里有几种可能性，包括完全监督、半监督、基于强化的或完全无监督的方法。
- en: Whether they are **online** (that is, learning on the fly as new data is presented),
    or learn using pre-existing data. This is referred to as batch learning.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是**在线**的（即在提供新数据时即时学习），还是使用预先存在的数据进行学习。这被称为批量学习。
- en: Whether they are instance-based, simply comparing new data to known data, or
    model-based, involving the detection of patterns and building a predictive model.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是基于实例的，只是简单地比较新数据与已知数据，还是基于模型的，涉及检测模式并构建预测模型。
- en: These approaches are not mutually exclusive and most algorithms are a combination
    of the approaches. For example, a typical way to build a spam detector is using
    an online, model-based supervised learning algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法并不是互斥的，大多数算法都是这些方法的组合。例如，构建一个垃圾邮件检测器的典型方式是使用在线的、基于模型的监督学习算法。
- en: Learning tasks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习任务
- en: 'There are several distinct types of learning tasks that are partially defined
    by the type of data that they work on. Based on this, we can divide learning tasks
    into two broad categories:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种明显不同的学习任务类型，这些类型部分由它们处理的数据类型定义。基于这一点，我们可以将学习任务分为两大类：
- en: '**Unsupervised learning**: Data is unlabeled so the algorithm must infer a
    relationship between variables or by finding clusters of similar variables'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：数据没有标签，因此算法必须推断变量之间的关系或通过找到相似变量的群集来学习。'
- en: '**Supervised learning**: Uses a labeled dataset to build an inferred function
    that can be used to predict the label of an unlabeled sample'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：使用标记数据集构建推断函数，该函数可用于预测未标记样本的标签'
- en: Whether the data is labeled or not has a predetermining effect on the way a
    learning algorithm is built.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是否有标签对学习算法的构建方式有着预设的影响。
- en: Unsupervised learning
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: One of the main drawbacks to supervised learning is that it requires data that
    is accurately labeled. Most real-world data consists of unlabeled and unstructured
    data and this is the major challenge to machine learning and the broader endeavor of
    artificial intelligence. Unsupervised learning plays an important role in finding
    structure in unstructured data. The division between supervised and unsupervised learning
    is not absolute. Many unsupervised algorithms are used to together with supervised
    learning; for example, where data is only partially labeled or when we are trying
    to find the most important features of a deep learning model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的一个主要缺点是需要准确标记的数据。大多数现实世界的数据是未标记和非结构化的，这是机器学习和人工智能更广泛努力的主要挑战。无监督学习在发现非结构化数据中的结构方面起重要作用。监督学习和无监督学习之间的区分并不是绝对的。许多无监督算法与监督学习一起使用；例如，在数据只有部分标记或者我们试图找出深度学习模型最重要特征时。
- en: Clustering
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: This is the most straightforward unsupervised method. In many cases, it does
    not matter that the data is unlabeled; what we are interested in is the fact that
    the data clusters around certain points. Recommender systems that, say, recommend
    movies or books from an online store often use clustering techniques. An approach
    here is for an algorithm to analyze a customer's purchase history, comparing it
    to other customers, and making recommendations based on similarities. The algorithm *clusters* customers'
    usage patterns into groups. At no time does the algorithm know what the groups
    are; it is able to work this out for itself. One of the most used clustering algorithms
    is **k-means**. This algorithm works by establishing cluster centers based on
    the mean of the observed samples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最直接的无监督方法之一。在许多情况下，数据是否有标签并不重要；我们关心的是数据聚集在某些点附近的事实。例如，推荐系统可以利用聚类技术推荐在线商店的电影或书籍。在这里，算法分析客户的购买历史，将其与其他客户进行比较，并根据相似性进行推荐。该算法将客户的使用模式*聚类*成群组。在任何时候，算法都不知道这些群组是什么；它能够自行解决这个问题。最常用的聚类算法之一是**k-means**。该算法通过建立基于观察样本均值的聚类中心来运作。
- en: Principle component analysis
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Another unsupervised method, often used in conjunction with supervised learning,
    is **principle component analysis** (**PCA**). This is used when we have a large
    amount of features that may be correlated and we are unsure of the impact each
    feature has in determining a result. For example, in weather prediction, we could
    use each meteorological observation as a feature and feed them directly to a model.
    This means the model would have to analyze a large amount of data, much of it
    irrelevant. Further, the data may be correlated so that we need to consider not
    just individual features but how these features interact with each other. What
    we need is a tool that will reduce this large amount of possibly correlated and
    redundant features into a small number of principle components. PCA belongs to
    a type of algorithm called **dimensionality reduction** because this reduces the
    number of dimensions in the input dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无监督方法，通常与监督学习一起使用的是**主成分分析**（**PCA**）。当我们有大量可能相关的特征并且不确定每个特征对确定结果的影响时，可以使用此方法。例如，在天气预测中，我们可以将每个气象观测作为一个特征直接输入模型。这意味着模型需要分析大量数据，其中许多数据是无关的。此外，数据可能相关，因此我们不仅需要考虑单个特征，还需要考虑这些特征如何相互作用。我们需要的是一种工具，能够将这些可能相关和冗余的特征减少到少数几个主成分。PCA属于一种称为**降维**的算法，因为它减少了输入数据集的维数。
- en: Reinforcement learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is somewhat different to other methods and is often classified
    as an unsupervised method because the data it uses is not labeled in the supervised
    sense. Reinforcement learning probably comes closer to the way humans interact
    and learn from the world than other methods. In reinforcement learning, the learning
    system is called an **agent** and this agent interacts with an **environment** by
    observation and by performing **actions. **Each action results in either a **reward** or
    a **penalty**. The agent must develop a strategy or **policy** to maximize reward
    and minimize penalties over time. Reinforcement learning has applications in many
    domains, such as game theory and robotics where the algorithm must learn its environment
    without direct human prompting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与其他方法有些不同，并且通常被归类为无监督方法，因为它使用的数据并非以监督方式标记。强化学习可能更接近人类与世界互动和学习的方式。在强化学习中，学习系统被称为一个**代理**，这个代理通过观察和执行**动作**与**环境**互动。每个动作会导致**奖励**或**惩罚**。代理必须制定策略或**策略**，以在时间内最大化奖励并最小化惩罚。强化学习在许多领域有应用，如博弈论和机器人技术，算法必须学习其环境，而无需直接人类提示。
- en: Supervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'In supervised learning, a machine learning model is trained on a labeled dataset.
    Most successful deep learning models so far have been focused on supervised learning
    tasks. With supervised learning, each data instance (say, an image or an email),
    comes with two elements: a set of features, usually denoted as an uppercase *X*,
    and a label, denoted with a lower case, *y*. Sometimes, the label is called the
    target or answer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，机器学习模型在标记数据集上进行训练。到目前为止，大多数成功的深度学习模型都集中在监督学习任务上。通过监督学习，每个数据实例（如图像或电子邮件）都有两个元素：通常表示为大写*X*的一组特征，以及用小写*y*表示的标签。有时，标签也称为目标或答案。
- en: 'Supervised learning is usually conducted in two stages: a training phase when
    the model learns the characteristics of the data, and a testing phase, where predictions
    are made on unlabeled data. It is important that the model is trained and tested
    on separate datasets, since the goal is to generalize to new data and not precisely
    learn the characteristics of a single dataset. This can lead to the common problems
    of over **overfitting** the training set, and consequently underfitting a test
    set of data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常分为两个阶段进行：训练阶段，模型在此阶段学习数据的特征；测试阶段，对未标记数据进行预测。重要的是，模型在不同的数据集上进行训练和测试，因为目标是对新数据进行泛化，而不是精确学习单一数据集的特征。这可能导致常见的过拟合训练集的问题，从而导致在测试数据集上欠拟合的问题。
- en: Classification
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: 'Classification is probably the most common supervised machine learning task.
    There are several types of classification problems based the number of input and
    output labels. The task of a classification model is to find a pattern in the
    input features and associate this pattern with a label. A model should learn the
    distinguishing features of the data and then be able to predict the label of an
    unlabeled sample. The model essentially builds an inferred function from the training
    data. We will look at how this function is built shortly. We can distinguish three
    types of classification models:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可能是最常见的监督学习任务。基于输入和输出标签的数量，分类问题有几种类型。分类模型的任务是在输入特征中找到模式，并将此模式与一个标签关联起来。模型应该学习数据的区分特征，然后能够预测未标记样本的标签。模型本质上是从训练数据中构建一个推断函数。我们马上会看到这个函数是如何构建的。我们可以区分三种类型的分类模型：
- en: '**Binary classification**: As in our toy—no toy example, this involves distinguishing between
    two labels.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**：就像我们的玩具示例一样，这涉及区分两个标签。'
- en: '**Multi-label classification**: Involves distinguishing between more than two
    classes. For example, if the toy example was extended to distinguish between the
    types of toy in the image (car, truck, plane, and so on). A common way to solve
    multi-label classification problems is to divide the problem into multiple binary
    problems.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签分类**：涉及区分两个以上的类别。例如，如果将玩具示例扩展到区分图像中的玩具类型（汽车、卡车、飞机等）。解决多标签分类问题的常见方法是将问题分解为多个二进制问题。'
- en: '**Multiple output classification**: Each sample may have more than one output
    label. For example, perhaps the task is to analyze images of scenes and determine
    what type of toys are in them. Each image can have multiple types of toys and
    therefore has multiple labels.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Multiple output classification**: 每个样本可能有多个输出标签。例如，也许任务是分析场景图像并确定其中的玩具类型。每个图像可以有多种类型的玩具，因此具有多个标签。'
- en: Evaluating classifiers
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类器
- en: 'You may think that the best way to measure the performance of a classifier
    is to count the proportion of successful predictions compared with the total predictions
    made. However, consider a classification task on a dataset of handwritten digits,
    where the target is all the digits that are *not* 7\. Just guessing that every
    sample is not 7 will give a success rate, assuming the data is evenly distributed,
    of 90%. When evaluating classifiers, we must consider four variables:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为衡量分类器性能的最佳方法是计算成功预测的比例与总预测量之比。然而，考虑一个关于手写数字数据集的分类任务，目标是所有不是7的数字。仅猜测每个样本不是7将给出一个成功率，假设数据均匀分布，为90%。在评估分类器时，我们必须考虑四个变量：
- en: '**TP true positive**: The predictions that correctly identify a target'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TP true positive**: 正确识别目标的预测'
- en: '**TN true negative**: The predictions that correctly identify a non-target'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TN true negative**: 正确识别非目标的预测'
- en: '**FP false positive**: Predictions that incorrectly identify a target'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP false positive**: 错误识别目标的预测'
- en: '**FN false negative**: Predictions that incorrectly identify a non-target'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FN false negative**: 错误识别非目标的预测'
- en: 'Two metrics, *precision* and *recall*, are commonly used together to measure
    the performance of a classifier. *Precision* is defined by the following equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确率*和*召回率*是常一起用来衡量分类器性能的两个指标。*精确率*由以下方程定义：'
- en: '![](img/1e226135-1258-4d90-9aea-31b85dba8926.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e226135-1258-4d90-9aea-31b85dba8926.png)'
- en: '*Recall* is defined by the following equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*由以下方程定义：'
- en: '![](img/24bcaee3-cd40-422c-b114-089bff352a26.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24bcaee3-cd40-422c-b114-089bff352a26.png)'
- en: 'We can combine these ideas in what is known as a confusion matrix. It is called
    confusion matrix, not because it is confusing to understand, but because it tabulates
    instances where the classifier confuses targets. The following diagram should
    make this clearer:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些想法结合在所谓的混淆矩阵中。它被称为混淆矩阵，并不是因为它难以理解，而是因为它总结了分类器混淆目标的情况。下面的图表应该能更清晰地说明这一点：
- en: '![](img/e501a920-e58f-461c-b7df-f41a983284ac.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e501a920-e58f-461c-b7df-f41a983284ac.png)'
- en: Which measure we use, or give more weight in determining the success or not
    of a classifier, really depends on the application. There is a trade-off between
    precision and recall. Improving precision will often result in a reduction in
    recall. For example, increasing the number of true positives will often mean that
    the false positive rate is increased. The right balance of precision and recall
    depends on the requirements of the application. For example, in a medical test
    for cancer, we probably need higher precision, since a false negative means an
    instance of cancer remains undiagnosed, with potentially fatal consequences.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定分类器的成功与否时，我们使用哪种度量或赋予更多权重，实际上取决于应用程序。精确率和召回率之间存在权衡。提高精确率通常会导致召回率降低。例如，增加真阳性的数量通常意味着假阳性率增加。精确率和召回率的合理平衡取决于应用程序的要求。例如，在癌症医学检测中，我们可能需要更高的精确率，因为假阴性意味着癌症实例未被诊断出来，可能导致致命后果。
- en: Features
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征
- en: 'It is important to remember that an image detection model does not see an image
    but a set of pixel color values, or, in the case of a spam filter, a collection
    of characters in an email. These are raw features of the model. An important part
    of machine learning is feature transformation. A feature transformation we have
    already discussed is dimensionality reduction in regard to principle component
    analysis. The following is a list common feature transformations:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，图像检测模型不是看到图像，而是一组像素颜色值，或者在垃圾邮件过滤器中，是电子邮件中的字符集合。这些是模型的原始特征。机器学习的一个重要部分是特征转换。我们已经讨论过的一个特征转换是关于主成分分析的降维。以下是常见的特征转换列表：
- en: Dimensionality reduction to reduce the number of features using techniques such
    as PCA
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维以减少特征数量，使用主成分分析等技术
- en: Scaling or normalizing features to be within a particular numerical range
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放或归一化特征使其处于特定的数值范围内
- en: Transforming the feature data type (for example, assigning categories to numbers)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换特征数据类型（例如，将类别分配给数字）
- en: Adding random or generated data to augment features
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加随机或生成的数据以增加特征
- en: Each feature is encoded on to a dimension of our input tensor, *X*, so in order
    to make a learning model as efficient as possible, the number of features needs
    to be minimised. This is where principle component analysis and other dimensionality reduction techniques
    come in to play.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征被编码到我们输入张量 *X* 的一个维度上，为了使学习模型尽可能高效，需要尽量减少特征数量。这就是主成分分析和其他降维技术发挥作用的地方。
- en: 'Another important feature transformation is scaling. Most machine learning
    models do not perform well when features are of different scales. There are two
    common techniques used for feature scaling:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的特征转换是缩放。当特征的比例不同时，大多数机器学习模型表现不佳。用于特征缩放的两种常见技术是：
- en: '**Normalization or min-max scaling**: Values are shifted and re-scaled to be
    between zero and one. This is the most used scaling method for neural networks.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化或最小-最大缩放**：值被移动和重新缩放到0到1之间。这是神经网络中最常用的缩放方法。'
- en: '**Standardization**: Subtracts the mean and divides by the variance. This does
    not bound variables to a particular range, but the resultant distribution has
    unit variance.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：减去均值并除以方差。这并不将变量限制在特定范围内，但结果分布具有单位方差。'
- en: Handling text and categories
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本和类别
- en: 'What do we do when a feature is a set of categories rather than a number? Suppose
    we are building a model to predict house prices. A feature of this model could
    be the cladding material of the house, with possible values such as timber, iron,
    and cement. How can we encode this feature to be of use to a deep learning model?
    The obvious solution is to simply assign a real number to each category: say,
    1 for timber, 2 for iron, and 3 for cement. The problem with this representation
    is that it infers that the category values are ordered. That is, timber and iron
    are somehow closer than timber and cement.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个特征是一组类别而不是一个数字时，我们该怎么办？假设我们正在构建一个预测房价的模型。该模型的一个特征可能是房屋的外包材料，可能的值包括木材、铁和水泥。我们如何编码这个特征以供深度学习模型使用？显而易见的解决方案是简单地为每个类别分配一个实数：比如木材为1，铁为2，水泥为3。这种表示方法的问题在于它推断类别值是有序的。也就是说，木材和铁比木材和水泥更接近。
- en: 'A solution that avoids this is **one-hot encoding**. The feature values are
    encoded as binary vectors, as shown in the following table:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种情况的一种解决方案是**独热编码**。特征值被编码为二进制向量，如下表所示：
- en: '| **Timber** | 1 | 0 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **木材** | 1 | 0 | 0 |'
- en: '| **Iron** | 0 | 1 | 0 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **铁** | 0 | 1 | 0 |'
- en: '| **Cement** | 0 | 0 | 1 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **水泥** | 0 | 0 | 1 |'
- en: This solution works well when the number of category values is small. If, for
    example, the data is a corpus of text, and our task is natural language processing,
    using one-hot encoding is not practical. The number of category values, and therefore
    the length of the feature vector, is the number of words in the vocabulary. In
    this case, the feature vector becomes large and unmanageable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别值的数量较少时，这种解决方案效果很好。例如，如果数据是一个文本语料库，我们的任务是自然语言处理，使用独热编码是不实际的。类别值的数量，因此特征向量的长度，是词汇表中单词的数量。在这种情况下，特征向量变得庞大且难以管理。
- en: 'One-hot encoding uses what is called a sparse representation. Most of the values
    are 0\. As well as not scaling very well, one-hot encoding has another serious
    drawback for natural language processing. It does not encode a word''s meaning,
    or its relationship to other words. An approach we can use is called dense word
    embedding. Each word in a vocabulary is represented by a real numbered vector,
    representing a score for a particular attribute. The general idea is that this
    vector encodes semantic information relevant to the task at hand. For example,
    if the task is to analyze movie reviews and determine the genre of the movie based
    on a review, we could create word embedding, as shown in the following table:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码使用称为稀疏表示的方法。大多数值为0。除了不太适合缩放外，独热编码在自然语言处理中还有另一个严重的缺点。它不编码单词的含义或其与其他单词的关系。我们可以使用的一种方法称为密集词嵌入。词汇表中的每个单词由一个实数向量表示，表示特定属性的分数。总体思想是，这个向量编码与当前任务相关的语义信息。例如，如果任务是分析电影评论并根据评论确定电影的类型，我们可以创建如下表所示的词嵌入：
- en: '| **Word** | **Drama** | **Comedy** | **Documentary** |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **词语** | **戏剧** | **喜剧** | **纪录片** |'
- en: '| Funny | -4 | 4.5 | 0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 有趣 | -4 | 4.5 | 0 |'
- en: '| Action | 3.5 | 2.5 | 2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 3.5 | 2.5 | 2 |'
- en: '| Suspense | 4.5 | 1.5 | 3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 悬疑 | 4.5 | 1.5 | 3 |'
- en: Here, the leftmost column lists words that may be present in a movie review.
    Each word is given a score relative to how often it appears in a review of the
    respective genre. We could build such a table from a supervised learning task
    that analyzes movie reviews in conjunction with their labeled genre. This trained
    model could then be applied to non-labeled reviews to determine the most likely
    genre.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，最左边的列列出可能出现在电影评论中的单词。每个单词根据其在相应类型的评论中出现的频率得分。我们可以从分析带标签的电影评论的监督学习任务中构建这样一个表格。然后，可以将训练过的模型应用于非标记的评论，以确定最可能的类型。
- en: Models
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Choosing a model representation is an important task in machine learning. So
    far, we have been referring to models as black boxes. Some data is put in, and,
    based on training, the model makes a prediction. Before we look inside this black
    box, let's review some of the linear algebra that we will need to understand deep
    learning models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，选择模型表示是一项重要任务。到目前为止，我们一直把模型称为黑盒子。一些数据被输入，基于训练，模型进行预测。在我们打开这个黑盒子之前，让我们回顾一些我们需要理解深度学习模型所需的线性代数知识。
- en: Linear algebra review
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数复习
- en: Linear algebra is concerned with the representation of linear equations through
    the use of matrices. In the algebra taught in high school, we were concerned with
    scalar, that is, single number, values. We have equations, and rules for manipulating
    these equations, so that they can be evaluated. The same is true when, instead
    of scalar values, we use matrices. Let's review some of the concepts involved.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数涉及通过使用矩阵来表示线性方程。在高中教的代数中，我们关注的是标量，也就是单个数字值。我们有方程式，以及操作这些方程式的规则，以便它们可以被评估。当我们使用矩阵而不是标量值时，情况也是如此。让我们回顾一些涉及的概念。
- en: 'A matrix is simply a rectangular array of numbers. We saw that we added two
    matrices simply by adding each corresponding element. A matrix can be multiplied
    by a scalar by simply multiplying every element in the array by the scalar, as
    shown in the following example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵只是一个数字的矩形数组。我们看到，通过简单地添加每个对应的元素来添加两个矩阵。可以通过简单地将数组中的每个元素乘以标量来将矩阵乘以标量，如下例所示：
- en: '![](img/c6fc4406-537b-47ea-b2a9-d794189a35a8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6fc4406-537b-47ea-b2a9-d794189a35a8.png)'
- en: This is an example of matrix addition and, as you would expect, you can perform
    matrix subtraction in the same way, except of course, rather than add corresponding
    elements, you subtract them. Note that we can only add or subtract matrices of
    the same size.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是矩阵加法的一个例子，正如您预期的那样，您可以以相同的方式执行矩阵减法，除了当然，不是添加对应的元素，而是减去它们。请注意，我们只能添加或减去相同大小的矩阵。
- en: 'Another common matrix operation is multiplication by a scalar:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的矩阵操作是乘以一个标量：
- en: '![](img/07f16399-180b-42b3-9407-bab4e2276361.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07f16399-180b-42b3-9407-bab4e2276361.png)'
- en: Notice the indexing style we use: *X*[*ij*], where *i* refers to the row and
    *j* refers to the column. There are two conventions when it comes to indexing.
    Here, I am using zero indexing; that is, indexing starts at zero. This is to keep
    it consistent with the way we index tensors in PyTorch. Be aware that in some
    mathematical texts, and depending on what programming language you use, indexing
    may start at 1\. Also, we refer to the size, or the dimension of a matrix, as
    *m* by *n, *where *m* is the number of rows and *n* is the number of columns.
    For example, *A* and *B* are both 3 x 2 matrices.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们使用的索引风格：*X*[*ij*]，其中*i*表示行，*j*表示列。在索引方面有两种惯例。这里，我使用的是零索引；即索引从零开始。这是为了与我们在PyTorch中索引张量的方式保持一致。请注意，在一些数学文本中，以及取决于您使用的编程语言，索引可能从1开始。此外，我们将矩阵的大小或维度称为*m*乘以*n*，其中*m*是行数，*n*是列数。例如，*A*和*B*都是3
    x 2矩阵。
- en: 'There is a special case of a matrix called a vector. This is simply a *n* by
    1 matrix, so it has one column and any number of rows, as shown in the following
    example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个矩阵的特殊情况称为向量。这只是一个*n*乘以1的矩阵，所以它有一列和任意数量的行，如下例所示：
- en: '![](img/b994a6ca-e95c-48f8-940c-60f2fd32b490.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b994a6ca-e95c-48f8-940c-60f2fd32b490.png)'
- en: 'Let''s now look at how to multiply a vector with a matrix. In the following
    example, we multiply a 3 x 2 matrix with a vector of size 2:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将一个向量与一个矩阵相乘。在下面的例子中，我们将一个3 x 2矩阵与一个大小为2的向量相乘：
- en: '![](img/d221cdb5-43b0-4208-a954-0fa03513a687.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d221cdb5-43b0-4208-a954-0fa03513a687.png)'
- en: 'A concrete example may make this clearer:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的例子可能会使这一点更清楚：
- en: '![](img/838642ca-7a21-4e66-a288-caf75ed27121.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/838642ca-7a21-4e66-a288-caf75ed27121.png)'
- en: Note that here, the 3 x 2 matrix results in a 3 vector and, in general, an *m*
    row matrix multiplied by a vector will result in an *m-*sized vector.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里，3 x 2矩阵的结果是一个3向量，在一般情况下，乘以向量的*m*行矩阵将导致一个*m*大小的向量。
- en: 'We can also multiply matrices with other matrices by combining matrix vector
    multiplication, as shown in the following example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过结合矩阵向量乘法来将矩阵与其他矩阵相乘，如以下示例所示：
- en: '![](img/7320f5e4-cd95-4a40-bb1e-113494f9740f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7320f5e4-cd95-4a40-bb1e-113494f9740f.png)'
- en: 'Here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![](img/feea3afa-8e16-4b66-ae43-43fd16c99832.png)![](img/e0143737-6993-49d0-b83d-9cc5c3da7902.png)![](img/18b7f873-51af-4b74-9cfe-ced353327e1a.png)![](img/dffb20dd-011b-46ff-a040-2a87a2f06522.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/feea3afa-8e16-4b66-ae43-43fd16c99832.png)![](img/e0143737-6993-49d0-b83d-9cc5c3da7902.png)![](img/18b7f873-51af-4b74-9cfe-ced353327e1a.png)![](img/dffb20dd-011b-46ff-a040-2a87a2f06522.png)'
- en: Another way of understanding this is that we obtain the first column of matrix
    *C* by multiplying matrix *A* with a vector comprising of the first column of
    matrix *B*. We obtain the second column of matrix *C* by multiplying matrix *A*
    with a vector obtained from the second column of matrix *B*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解方式是，我们通过将矩阵*A*与矩阵*B*的第一列组成的向量相乘，得到矩阵*C*的第一列。我们通过将矩阵*A*与从矩阵*B*的第二列获得的向量相乘，得到矩阵*C*的第二列。
- en: 'Let''s look at a concrete example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子：
- en: '![](img/ff79a42b-72f4-48a9-9042-5accf01153db.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff79a42b-72f4-48a9-9042-5accf01153db.png)'
- en: 'It is important to understand that we can only multiply two matrices if the
    number of rows in *A* is equal to the number of columns in *B*. The resultant
    matrix will always have the same number of rows as *A* and the same number of
    columns as *B*. Note that matrix multiplication is not commutative; as in the
    following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，只有在矩阵*A*的行数等于矩阵*B*的列数时，我们才能相乘两个矩阵。结果矩阵的行数始终与矩阵*A*相同，并且列数与矩阵*B*相同。请注意，矩阵乘法不是交换的，如下所示：
- en: '![](img/9da42d43-ccbd-467c-8504-65fef2b733f5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9da42d43-ccbd-467c-8504-65fef2b733f5.png)'
- en: 'Matrix multiplication is, however, associative, as shown in the following example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，矩阵乘法是结合的，如下例所示：
- en: '![](img/f10ca8a5-8315-4c00-a0dc-8c44e48eea54.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f10ca8a5-8315-4c00-a0dc-8c44e48eea54.png)'
- en: 'Matrices are useful because we can represent a large number of operations with
    relatively simple equations. There are two matrix operations that are particularly
    important for machine learning:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵非常有用，因为我们可以用相对简单的方程表示大量操作。对于机器学习来说，有两个矩阵运算特别重要：
- en: Transpose
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置
- en: Inverse
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆矩阵
- en: 'To transpose a matrix, we simply swap the columns and rows, as shown in the
    following example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要转置矩阵，我们只需交换列和行，如以下示例所示：
- en: '![](img/aced5013-84eb-400b-bbbc-fb68b385eb48.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aced5013-84eb-400b-bbbc-fb68b385eb48.png)'
- en: 'Finding the inverse of a matrix is a little more complicated. In the set of
    real numbers, the numbers 1 plays the role of **identity**. That is to say, the
    number 1 multiplied by any other number that equals that number. Also, almost
    every number has an inverse; that is, a number that when it is multiplied by itself
    equals 1\. For example, the inverse of 2 is 0.5 because two times 0.5 equals 1\.
    It turns out an equivalent idea holds for matrices and tensors. The identity matrix
    consists of 1s along its primary diagonal and zeros everywhere else, as shown
    in the following 3 x 3 example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找矩阵的逆矩阵稍微复杂一些。在实数集中，数字1扮演**单位**的角色。也就是说，数字1乘以任何其他等于那个数字的数字。几乎每个数字都有一个逆元；也就是说，一个乘以自己等于1的数字。例如，2的逆元是0.5，因为2乘以0.5等于1。矩阵和张量也有类似的概念。单位矩阵沿其主对角线为1，其他位置为0，如以下3
    x 3示例所示：
- en: '![](img/ad6dd3f0-ace6-4ad6-9a4f-42b799f0d4bd.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad6dd3f0-ace6-4ad6-9a4f-42b799f0d4bd.png)'
- en: 'The identity matrix is the result when we multiply a matrix that is inverse.
    We write this in the following way:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们乘以一个逆矩阵时，单位矩阵是其结果。我们用以下方式表示：
- en: '![](img/8724e18d-d0b9-4b0d-8f9d-d33d188d5a39.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8724e18d-d0b9-4b0d-8f9d-d33d188d5a39.png)'
- en: Importantly, we can only find the inverse of a square matrix. It is not expected
    to calculate inverse matrices, or indeed any matrix operation, by hand. That is
    what computers are good at. Inverting matrices is a non-trivial operation and
    they are, even for a computer, computationally expensive.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们只能找到方阵的逆矩阵。不要期望手工计算逆矩阵，或者说实际上，任何矩阵操作。这是计算机擅长的。矩阵求逆是一个非平凡的操作，即使对于计算机来说，也是计算密集型的。
- en: Linear models
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: The simplest models we will encounter in machine learning are linear models.
    Solving linear models is important in many different settings, and they form the
    building blocks of many nonlinear techniques. With a linear model, we attempt
    to fit training data to a linear function, sometimes called the **hypothesis function**.
    This is done through a process called linear regression.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们将遇到的最简单的模型是线性模型。解决线性模型在许多不同的设置中都很重要，并且它们是许多非线性技术的基础。使用线性模型，我们试图将训练数据拟合成一个线性函数，有时称为**假设函数**。这是通过线性回归的过程完成的。
- en: 'The hypothesis function for single variable linear regression has the following
    form:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量线性回归的假设函数具有以下形式：
- en: '![](img/9c54f0a3-0a2f-41ad-a199-0ee91fab1854.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c54f0a3-0a2f-41ad-a199-0ee91fab1854.png)'
- en: Here, *θ[0]* and *θ[1]* are the model **parameters** and *x* is the single independent
    variable. For our house price example, *x* could represent the size of floor space
    and *h(x)* could represent the predicted house price.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*θ[0]* 和 *θ[1]* 是模型的**参数**，*x* 是单个独立变量。对于我们的房价示例，*x* 可以代表房屋的地板面积大小，*h(x)*
    可以代表预测的房价。
- en: For simplicity, we will begin by looking at just the single variable, or single
    feature case.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将首先看一下只有单变量或单特征的情况。
- en: 'In the following diagram, we show a number of points, representing training
    data, and an attempt to fit a straight line to these points:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了许多点，代表着训练数据，并尝试将一条直线拟合到这些点上：
- en: '![](img/e1a8bfdf-e61c-4ae5-a5cc-59c85d44da5e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1a8bfdf-e61c-4ae5-a5cc-59c85d44da5e.png)'
- en: Here, *x* is the single feature and **θ*[0]*** and **θ*[1]*** represent the
    intercept and the slope of the hypothesis function, respectively. Our aim is to
    find values for **θ*[0]*** and **θ*[1]***, the model parameters, which will give
    us our line of best fit in the preceding diagram. In this diagram, **θ***[**0** ]*is
    set to **1** and **θ***[**1** ]*is set to **0.5**. Therefore, its intercept is
    **1** and the line has a slope of **0.5**. We can see that most of the training
    points lie above the line and a few lower-valued points lie below the line. We
    could guess that **θ***[**1** ]*is probably slightly too low, since the training
    points appear to have a slightly steeper slope. Also, **θ*[0]*** is too high,
    since there are two data points below the line on the left and the intercept appears
    to be slightly lower than 1.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x* 是单个特征，**θ*[0]** 和 **θ*[1]** 分别表示假设函数的截距和斜率。我们的目标是找到**θ*[0]** 和 **θ*[1]**
    这两个模型参数的数值，这将使我们在上图中得到最佳拟合直线。在这张图中，**θ*[0]** 被设为 **1**，**θ*[1]** 被设为 **0.5**。因此，直线的截距是
    **1**，斜率为 **0.5**。我们可以看到大多数训练点位于直线上方，只有少数数值较低的点位于直线下方。我们可以猜测 **θ*[1]** 可能略低，因为训练点似乎具有稍陡的斜率。而且
    **θ*[0]** 太高，因为左侧有两个数据点位于直线下方，截距似乎略低于 **1**。
- en: 'It is clear that we need a formal approach to finding the error in the hypothesis
    function. This is done through what is known as a **cost function**. The cost
    function measures the total error between the values given by the hypotheses function
    and the actual values in the data. Essentially, the loss function sums each point''s
    distance from the hypothesis. The cost function is sometimes called the **mean
    squared error** (**MSE**). This is expressed by the following equation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们需要一种正式的方法来找出假设函数中的误差。这是通过所谓的**代价函数**完成的。代价函数衡量假设函数给出的值与数据实际值之间的总误差。本质上，损失函数对每个点的假设值与实际值之间的距离进行求和。代价函数有时也被称为**均方误差**（**MSE**）。其表达式如下：
- en: '![](img/c3956662-ef67-4e3d-9aa5-9e42c2c31af5.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3956662-ef67-4e3d-9aa5-9e42c2c31af5.png)'
- en: Here, *h[θ](x^i**)* is the value calculated by the hypothesis for the *i*th
    training sample, and *y^i* is its actual value. The difference is squared as statistical
    convenience, since it ensures the result is always positive. Squaring also adds
    more weight to larger differences; that is, it places greater importance on outliers.
    This sum is then divided by *m*, the number of training samples, to calculate
    the mean. Here, the sum is also divided by two to make subsequent math a little
    more straightforward.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h[θ](x^i)* 是假设函数对第*i*个训练样本计算出的值，*y^i* 是其实际值。差值被平方是为了统计上的便利性，因为它确保结果始终为正。平方还会增加较大差异的权重；也就是说，它对离群值更加重视。将这些平方误差之和除以训练样本数*m*，可以计算得到均值。此外，将和除以2可以使后续的数学运算更为简单。
- en: The final part is to adjust the parameter values so that the hypothesis function
    fits the training data as closely as possible. We need to find parameter values
    that minimize the error.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是调整参数值，使得假设函数尽可能地拟合训练数据。我们需要找到能够最小化误差的参数值。
- en: 'There are two ways we can do this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式实现这一点：
- en: Using gradient descent to iterate over the training set and adjust parameters
    to minimize a cost function
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降迭代训练集并调整参数以最小化成本函数。
- en: Directly computing model parameters, using a *closed-form* equation
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接计算模型参数，使用*闭合形式*方程式
- en: Gradient descent
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is a general-purpose optimization algorithm that has a wide
    variety of applications. Gradient descent minimizes the cost function by iteratively
    adjusting the model parameters. Gradient descent works by taking the partial derivative
    of the cost function. If we plot the cost function against a parameter value,
    it forms a convex function, as shown in the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种通用的优化算法，具有广泛的应用。梯度下降通过迭代调整模型参数来最小化成本函数。梯度下降通过对成本函数进行偏导数来实现。如果我们将成本函数绘制为参数值的函数，它形成一个凸函数，如下图所示：
- en: '![](img/b4f54c07-d3e6-4ae9-b04d-fcc3629b874e.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4f54c07-d3e6-4ae9-b04d-fcc3629b874e.png)'
- en: 'You can see that as we vary *θ*, from right to left in the preceding diagram,
    the cost, *J[θ]*, decreases to a minimum and then rises. The aim is that on each
    iteration of gradient descent, the cost moves closer to the minimum, and then
    stops once it reaches this minimum. This is achieved using the following update
    rule:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在图表中从右向左变化 *θ* 时，成本 *J[θ]* 减少至最小值，然后上升。目标是在梯度下降的每次迭代中，成本更接近最小值，一旦达到最小值就停止。这是通过以下更新规则实现的：
- en: '![](img/5a4faae8-3e9e-4791-bf23-3419d8c76146.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a4faae8-3e9e-4791-bf23-3419d8c76146.png)'
- en: Here, *α* is the **learning rate,** a settable **hyperparameter**. It is called
    a hyperparameter to distinguish it from the model parameters, theta. The partial
    derivative term is the slope of the cost function and this needs to be calculated
    for both theta *0* and theta *1*. You can see that when the derivative, and therefore
    the slope, is positive, a positive value is subtracted from the previous value
    of theta, moving from right to left in the preceding diagram. Alternatively, if
    the slope is negative, theta increases, moving from right to left. Also, at the
    minimum, the slope is zero, so gradient descent will stop. This is exactly what
    we want, since no matter where we start gradient descent, the update rule is guaranteed
    to move theta toward the minimum.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*α* 是**学习率**，一个可设置的**超参数**。它被称为超参数，以区别于模型参数 theta。偏导数项是成本函数的斜率，需要分别计算 theta
    *0* 和 theta *1* 的值。当导数和因此斜率为正数时，会从 theta 的先前值中减去正值，在图表中从右向左移动。相反，如果斜率为负数，则 theta
    增加，从右向左移动。此外，在最小值处，斜率为零，因此梯度下降会停止。这正是我们想要的，因为无论我们从何处开始梯度下降，更新规则都确保将 theta 向最小值移动。
- en: 'Substituting the cost function into the preceding equation and then taking
    the derivative for both values of theta results in the following two update rules,
    results in the following equations:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将成本函数代入上述方程，然后分别对 theta 的两个值进行导数运算，结果得到以下两个更新规则，即以下两个方程：
- en: '![](img/2b3128b9-8194-467d-92f2-62d2fa158d26.png)![](img/0b152aad-3357-429e-8de3-0a4358fa65a0.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b3128b9-8194-467d-92f2-62d2fa158d26.png)![](img/0b152aad-3357-429e-8de3-0a4358fa65a0.png)'
- en: On iteration and subsequent updates, theta will converge to values that minimize
    the cost function, resulting in the best fit straight line to the training data. There
    are two things that need to be considered. Firstly, the initialization values
    of theta; that is, where we start gradient descent. In most cases, random initialization works
    best. The other thing we need to consider is setting the learning rate, alpha
    (*α*). This is a number between zero and one. If the learning rate is set too
    high, then it will likely overshoot the minima. If it is set too low, then it
    will take too long to converge. It may take some experimentation with the particular
    model being used; in deep learning, an adaptive learning rate is often used for
    best results. This is where the learning rate changes, usually getting smaller,
    on each iteration of gradient descent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代和后续更新中，θ将收敛到能够最小化成本函数的值，从而得到最佳拟合的直线。有两件事需要考虑。首先是θ的初始化值；也就是说，我们从哪里开始梯度下降。在大多数情况下，随机初始化效果最好。另一件需要考虑的事情是设置学习率α。这是一个介于零和一之间的数字。如果学习率设置得太高，它可能会超过最小值。如果设置得太低，它将需要较长时间收敛。可能需要对使用的特定模型进行一些实验；在深度学习中，通常会使用自适应学习率以获得最佳结果。这是指在梯度下降的每次迭代中学习率通常会变小。
- en: The type of gradient descent we have discussed so far is called **batch gradient
    descent **(**BGD**). This refers to the fact that on each update, the entire training
    set is used. This means that as the training set gets large, batch gradient descent
    becomes increasingly slow. On the other hand, batch gradient descent scales much
    better when there are a large number of features, so it is most often used when
    there is a smaller training set with a large number of features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的梯度下降类型称为**批量梯度下降**（**BGD**）。这是因为每次更新时都使用整个训练集。这意味着随着训练集的增大，批量梯度下降变得越来越慢。另一方面，当特征数量很大时，批量梯度下降的性能更好，因此在训练集较小但特征较多时通常会使用它。
- en: 'An alternative to batch gradient descent is **stochastic gradient descent**
    (**SGD**). Instead of calculating the gradient using the entire training set,
    SGD calculates the gradient using a single sample chosen randomly on each iteration.
    The advantage of SGD is that the entire training set does not have to reside in
    memory, since on each iteration it works with one instance only. Because stochastic
    gradient descent chooses samples at random, its behavior is a little less regular
    than BGD. With batch gradient descent, every iteration smoothly moves the error
    (*J[θ]*) toward the minima. With SGD, every iteration does not necessarily move
    the cost closer to the minima. It tends to jump around a bit, moving toward the
    minima only on average over a number of iterations. This means that it may jump
    around close to the minima but never actually reach it by the time it completes
    its iterations. The random nature of SGD can be used to advantage when there is
    more than one minima, since it may be able to jump out of this local minima and
    find the global minima. For example, consider the following cost function:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代批量梯度下降的方法是**随机梯度下降**（**SGD**）。与使用整个训练集计算梯度不同，SGD在每次迭代中随机选择一个样本计算梯度。SGD的优势在于不需要整个训练集一次性存储在内存中，因为每次迭代只处理一个实例。因为随机梯度下降是随机选择样本，其行为比批量梯度下降稍显不规则。使用批量梯度下降时，每次迭代都会平滑地将误差（*J[θ]*）向最小值移动。而使用SGD时，并不是每次迭代都能使成本函数更接近最小值。它往往会有些许跳跃，平均而言才会向最小值移动。这意味着它可能会在最小值附近跳动，但在完成迭代时可能并未真正到达最小值。SGD的随机性在存在多个最小值时可以派上用场，因为它可能会跳出局部最小值找到全局最小值。例如，请考虑以下成本函数：
- en: '![](img/f6c75291-ee32-430b-9413-662bb4d21b56.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6c75291-ee32-430b-9413-662bb4d21b56.png)'
- en: If batch gradient descent were to begin to the right of the **Local Minimum**,
    it would not be able to find the **Global Minimum**. Fortunately, the cost function
    for linear regression is always going to be a convex function with a single minima.
    However, this is not always the case, particularly with neural networks, where
    the cost function can have a number of local minima.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果批量梯度下降从**局部最小值**的右侧开始，它将无法找到**全局最小值**。幸运的是，线性回归的成本函数始终是一个具有单一最小值的凸函数。然而，并非总是如此，特别是在神经网络中，成本函数可能具有多个局部最小值。
- en: Multiple features
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个特征
- en: 'In a realistic example, we would have more than one feature and each feature
    has an associated parameter value that requires fitting. We write the hypothesis function
    for multiple features as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个实际的例子中，我们会有多个特征，每个特征都有一个关联的参数值需要拟合。我们将多个特征的假设函数写成如下形式：
- en: '![](img/e037ff82-52fb-4849-89dc-81d5480d803f.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e037ff82-52fb-4849-89dc-81d5480d803f.png)'
- en: Here, *x[0]* is called the bias variable and is set to one, *x**[1]* to *x**[n]* are
    the feature values, and *n* is the total number of features. Notice that we can
    write a vectorized version of the hypothesis function. Here, *θ* is the **parameter
    vector** and *x* is the **feature vector.**
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x[0]*称为偏置变量并设为1，*x[1]*到*x[n]*是特征值，*n*是特征的总数。请注意，我们可以编写假设函数的向量化版本。在这里，*θ*是**参数向量**，*x*是**特征向量**。
- en: The cost function is still basically the same as the single feature case; we
    are just summing the error. We do, however, need to adjust the gradient descent
    rules and be clear about the required notation. In the update rules for gradient
    descent for a single feature, we used the notation for parameter values *θ[0]*
    and *θ[1]*. For the multiple feature version, we simply wrap these parameters'
    values and their associated features into vectors. The parameter vector is notated
    as *θ[j]*, where the subscript *j* refers to the feature and is an integer between
    *1* and *n*, where *n* is the number of features.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数与单特征情况基本相同；我们只是在求和误差时需要调整梯度下降规则，并明确所需的符号表示。在单特征梯度下降的更新规则中，我们使用了参数值*θ[0]*和*θ[1]*的符号表示。对于多特征版本，我们简单地将这些参数的值及其关联的特征包装成向量。参数向量表示为*θ[j]*，其中下标*j*表示特征，是介于1到*n*之间的整数，*n*是特征的数量。
- en: 'There needs to be a separate update rule for each parameter. We can generalize
    these rules as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数都需要有单独的更新规则。我们可以将这些规则概括如下：
- en: '![](img/b17f154a-f6b4-4a97-94f6-00b15af88d0e.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b17f154a-f6b4-4a97-94f6-00b15af88d0e.png)'
- en: 'There is an update rule for each parameter; so, for example, the update rule
    for the parameter for feature *j = 1* would be the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数都有一个更新规则；因此，例如特征*j = 1*的参数的更新规则如下：
- en: '![](img/2f946159-a81c-443f-ad8c-d070160ca57e.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f946159-a81c-443f-ad8c-d070160ca57e.png)'
- en: The variables *x^((i))* and *y^((i))* refer, as in the single feature example,
    to the predicted value and actual value of the *i^(th)* training sample, respectively.
    In the multiple feature case, however, instead of being single values, they are
    now vectors. The value *x[j]^((i))* refers to feature *j* of training sample *i*, and *m*
    is the total number of samples in the training set.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 变量*x^((i))*和*y^((i))*，如单特征示例中所述，分别指预测值和第*i*个训练样本的实际值。然而，在多特征情况下，它们不再是单个值，而是向量。值*x[j]^((i))*指训练样本*i*的特征*j*，*m*是训练集中样本的总数。
- en: The normal equation
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正规方程
- en: 'For some linear regression problems, the closed form solution, also known as
    the **normal equation**, is a better way to find optimum values of theta. If you
    know calculus, then to minimize the cost function, you can find the partial derivatives
    of the cost function, with respect to each value of theta, and set each derivative
    to zero and then solve for each value of theta. Don''t worry if you are not familiar
    with calculus; it turns out that we can derive the normal equation from these
    partial derivatives and this results in the following equation:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些线性回归问题，闭式解，也称为**正规方程**，是寻找θ的最优值的更好方法。如果你了解微积分，那么为了最小化代价函数，你可以找到代价函数的偏导数，对每个θ的值设为零，然后求解每个θ的值。如果你对微积分不熟悉，不用担心；事实证明，我们可以从这些偏导数中推导出正规方程，结果如下方程所示：
- en: '![](img/1156e0cd-2127-4b45-8251-5ab7b134d2bb.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1156e0cd-2127-4b45-8251-5ab7b134d2bb.png)'
- en: You may wonder why we need to bother with gradient descent and the added complications
    this entails, since the normal equation allows us to compute the parameters in
    one step. The reason is that the computational effort required to invert a matrix
    is not insignificant. When a feature matrix *X* becomes large (and remember *X*
    is a matrix holding all the values of features for every training sample), then
    finding the inverse of this matrix simply takes too long. Even though gradient
    descent involves many iterations, it is still faster than the normal equation
    for large datasets.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们需要费心使用梯度下降及其带来的额外复杂性，因为正规方程允许我们在一步中计算参数。原因是求逆矩阵所需的计算工作并不可忽视。当特征矩阵*X*变得很大时（请记住*X*是一个包含每个训练样本特征值的矩阵），找到这个矩阵的逆仅需太长时间。尽管梯度下降涉及多次迭代，但对于大数据集来说，仍然比正规方程快。
- en: An advantage of the normal equation is that, unlike gradient descent, it does
    not expect features to be of the same scale. Another advantage of the normal equation
    is that is not necessary to choose a learning rate.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正规方程的一个优势是，与梯度下降不同，它不要求特征具有相同的尺度。正规方程的另一个优点是不需要选择学习率。
- en: Logistic regression
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'We can use the linear regression model to perform binary classification by
    finding a decision boundary that divides two predicted classes. A common way to
    do this is by using a `sigmoid` function, defined as the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用线性回归模型通过找到分割两个预测类别的决策边界来进行二元分类。一个常见的方法是使用定义如下的`sigmoid`函数：
- en: '![](img/227a06ad-2f85-4bfa-8bca-91a5d6d65bde.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/227a06ad-2f85-4bfa-8bca-91a5d6d65bde.png)'
- en: 'The plot of the `sigmoid` function looks like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数的图像如下所示：'
- en: '![](img/ed10f260-7263-418e-86ef-4652695481e2.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed10f260-7263-418e-86ef-4652695481e2.png)'
- en: 'The `sigmoid` function can be used in the hypothesis function, to output a
    probability, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数可以用于假设函数中，以输出概率，如下所示：'
- en: '![](img/9f7db059-84ee-4fa9-b128-bec7fe1ed2da.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f7db059-84ee-4fa9-b128-bec7fe1ed2da.png)'
- en: 'Here, the output of the hypothesis function is the probability *y = 1* given
    *x* parameterized by theta. To decide when to predict *y = 0* or *y = 1*, we can
    use the following two rules:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，假设函数的输出是在参数化为θ的条件下，*y = 1* 的概率。为了决定何时预测*y = 0*或*y = 1*，我们可以使用以下两条规则：
- en: '![](img/4988133c-dfdf-4dd9-a92f-484c2182184e.png)![](img/04f301da-b0fe-45eb-8727-c3a962af3c80.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4988133c-dfdf-4dd9-a92f-484c2182184e.png)![](img/04f301da-b0fe-45eb-8727-c3a962af3c80.png)'
- en: The characteristics of the `sigmoid` function (that is, having asymptotes at
    *0* and *1*, and having a value of *0.5* at *z = 0*), has some attractive properties
    for use with logistic regression problems. Notice that the decision boundary is
    a property of the parameters of the model, not the training set. We still need
    to fit the parameters so that the cost, or the error, is minimized. To do this,
    we need to formalize what we know already.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数的特性（即在*z = 0*时有渐近线为*0*和*1*，并且在*z = 0*时的值为*0.5*），在逻辑回归问题中具有一些吸引人的特性。请注意，决策边界是模型参数的属性，而不是训练集的属性。我们仍然需要调整参数以使成本或误差最小化。为此，我们需要形式化我们已经知道的内容。'
- en: 'We have a training set with *m* samples, written as the following equation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含*m*个样本的训练集，如下所示：
- en: '![](img/2f0169de-7a8b-4ccd-b3fe-e61eacbf5a25.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f0169de-7a8b-4ccd-b3fe-e61eacbf5a25.png)'
- en: 'Each training sample consists of vector *x*, of size *n**,* where *n* is the
    number of features:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练样本由大小为*n*的特征向量*x*组成，其中*n*是特征数量：
- en: '![](img/bf321cf3-03b2-4b49-b17a-7b4ad6fb43ae.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf321cf3-03b2-4b49-b17a-7b4ad6fb43ae.png)'
- en: 'Each training sample also consists of a value *y* and, for logistic regression,
    this value is either zero or one. We also have a hypothesis function for logistic
    regression, which we can rewrite as the following equation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练样本还包括一个值*y*，对于逻辑回归来说，这个值要么是零，要么是一。我们还有一个逻辑回归的假设函数，可以重写为以下方程：
- en: '![](img/15d8625e-183d-4b07-b91f-77b48261c812.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15d8625e-183d-4b07-b91f-77b48261c812.png)'
- en: 'Using the same cost function for linear regression, with the hypotheses for
    logistic regression, we introduce a nonlinearity via the `sigmoid` function. This
    means that the cost function is no longer convex, and as a result, it may have
    a number of local minima, which can be a problem for gradient descent. It turns
    out that a function that works well for logistic regression, and results in a
    convex cost function, is the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逻辑回归的假设函数，采用与线性回归相同的成本函数，我们通过`sigmoid`函数引入了非线性。这意味着成本函数不再是凸函数，因此可能存在多个局部最小值，这对梯度下降来说可能是个问题。事实证明，对于逻辑回归而言，一个良好的函数并且导致凸成本函数的函数如下所示：
- en: '![](img/6e2dbf0d-1cbe-498f-995e-c95322f826da.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e2dbf0d-1cbe-498f-995e-c95322f826da.png)'
- en: 'We can plot these functions for the two cases:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为这两种情况绘制这些函数图。
- en: '![](img/0c45eefd-80e3-4396-936c-85e876b4bcf1.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c45eefd-80e3-4396-936c-85e876b4bcf1.png)'
- en: It can be seen from the previous diagrams that, when the label **y** equals
    **1** and the hypothesis predicts **0**, the cost approaches infinity. Also, when
    the actual value of **y** is **0**, and the hypothesis predicts **1**, similarly,
    the cost rises toward infinity. Alternatively, when the hypothesis predicts the
    correct value, either **0** or **1**, the cost falls to **0**. This is exactly
    what we want for logistic regression.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的图表中可以看出，当标签**y**等于**1**而假设值预测为**0**时，成本趋向无穷大。同样，当**y**的实际值为**0**而假设值预测为**1**时，成本也上升至无穷大。或者，当假设值预测正确的值，即**0**或**1**时，成本降至**0**。这正是逻辑回归所期望的。
- en: 'Now we need to apply gradient descent to minimize the cost. We can rewrite
    the logistic regression cost function for binary classification to a more compact
    form, summing it over multiple training samples, using the following equation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要应用梯度下降来最小化成本。我们可以将二元分类的逻辑回归成本函数重新写成更简洁的形式，通过以下方程对多个训练样本求和：
- en: '![](img/5e391513-ae15-4481-90dd-accf9a3b203c.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e391513-ae15-4481-90dd-accf9a3b203c.png)'
- en: 'Finally, we can update the parameter values with this update rule:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下更新规则更新参数值：
- en: '![](img/24af04b6-7839-47d8-ade7-04154f25c041.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24af04b6-7839-47d8-ade7-04154f25c041.png)'
- en: Superficially, this looks identical to the update rule for linear regression;
    however, the hypothesis is a function of the `sigmoid` function, so it actually
    behaves quite differently.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，这与线性回归的更新规则看起来一样；然而，假设是`sigmoid`函数的函数，因此其行为实际上有很大不同。
- en: Nonlinear models
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性模型
- en: 'We have seen that linear models, by themselves, fail to represent nonlinear
    real-world data. A possible solution is to add polynomial features to the hypotheses
    function. For example, a cubic model can be represented by the following equation:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，仅靠线性模型无法表示非线性的现实世界数据。一个可能的解决方案是向假设函数添加多项式特征。例如，一个立方模型可以用以下方程表示：
- en: '![](img/97fe1a3f-2036-4b85-a916-3585fe58a41a.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97fe1a3f-2036-4b85-a916-3585fe58a41a.png)'
- en: Here, we need to choose two derived features to add to our model. These added
    terms could simply be the square and cube of the size feature in the housing example.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要选择两个派生特征添加到我们的模型中。这些添加的项可以简单地是房屋示例中尺寸特征的平方和立方。
- en: An important consideration when adding polynomial terms is feature scaling.
    The squared and cubic terms in this model will be of quite different scales. In
    order for gradient descent to work correctly, it is necessary to scale these added
    polynomial terms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加多项式项时，一个重要的考虑因素是特征缩放。该模型中的平方和立方项的尺度会有很大的不同。为了使梯度下降正确工作，必须对这些添加的多项式项进行缩放。
- en: Choosing polynomial terms is a way to inject knowledge into a model. For example,
    simply knowing that house prices tend to flatten out relative to floor space,
    as the floor space gets large, suggests adding squared and cubic terms, giving
    us the shape that we expect the data to take. However, feature selection where,
    say in logistic regression, when we are trying to predict a complicated multidimensional
    decision boundary, it may mean thousands of polynomial terms. Under such circumstances,
    the machinery of linear regression grinds to a halt. We will see that neural networks
    offer a more automated and powerful solution to complicated nonlinear problems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 选择多项式项是向模型注入知识的一种方式。例如，仅仅知道房价在相对于房屋面积增大时趋于平稳，就建议添加平方和立方项，以便使数据呈现我们期望的形状。然而，在特征选择中，比如在逻辑回归中，当我们试图预测复杂的多维决策边界时，可能意味着成千上万个多项式项。在这种情况下，线性回归的机制将停滞不前。我们将看到，神经网络为复杂的非线性问题提供了更自动化和强大的解决方案。
- en: Artificial neural networks
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: As the name suggests, ANNs are inspired by their biological counterpart, although
    the reason is, perhaps, misunderstood. An artificial neuron, or what we will call
    a **unit**, is grossly simplified compared to a biological neuron, both in terms
    of functionality and structure. The biological inspiration comes more from the
    insight that each neuron in a brain performs an identical function regardless
    of whether it is processing sound, vision, or pondering complex mathematics problems.
    This single algorithm approach is, fundamentally, the inspiration for ANNs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名，人工神经网络受其生物对应物的启发，尽管原因可能被误解。一个人工神经元，或者我们将其称为一个**单元**，在功能和结构上与生物神经元相比都被极大简化了。生物灵感更多地来自这样的洞察力，即大脑中的每个神经元无论是处理声音、视觉还是思考复杂的数学问题，都执行相同的功能。这种单一算法方法基本上是ANN的灵感来源。
- en: An artificial neuron, a unit, performs a single simple function. It adds up
    its inputs and, dependent on an activation function, gives an output. One of the
    major benefits of ANNs is that they are highly scalable. Since they are composed
    of fundamental units, simply adding more units in the right configuration allows
    ANNs to easily scale to massive, complex data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工神经元，一个单元，执行一个简单的功能。它将其输入相加，并根据激活函数给出输出。人工神经网络的主要好处之一是它们具有高度可扩展性。由于它们由基本单元组成，只需在正确的配置中添加更多单元即可使人工神经网络轻松扩展到大规模、复杂的数据。
- en: 'The theory of ANNs has been around for quite some time, first proposed in the
    early 1940''s. However, it is not until recently that they have been able to outperform
    more traditional machine learning techniques. There are three broad reasons for
    this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）的理论已经存在了很长时间，最早是在1940年代初提出的。然而，直到最近它们才能够胜过更传统的机器学习技术。这主要有三个原因：
- en: The improvement in algorithms, notably the implementation of **backpropagation**, allowing
    an ANN to distribute the error at the output to input layers and adjust activation
    weights accordingly
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的改进，特别是实现**反向传播（backpropagation）**，使得ANN能够将输出的误差分布到输入层，并相应地调整激活权重
- en: The availability of massive datasets to train ANNs
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模数据集的可用性，用于训练人工神经网络
- en: The increase in processing power, allowing large-scale ANNs
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理能力的增加，允许大规模的人工神经网络
- en: The perceptron
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'One of the most simple ANN models is the perceptron, consisting of a single
    logistic unit. We can represent the perceptron in the following diagram:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的ANN模型之一是感知器，由单个逻辑单元组成。我们可以用以下图表现感知器：
- en: '![](img/878bbf1d-61e1-4f4d-a2e6-9e0b1944f659.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/878bbf1d-61e1-4f4d-a2e6-9e0b1944f659.png)'
- en: 'Each of the inputs are associated with a weight and these are fed into the
    logistic unit. Note that we add a bias feature, **x[0] = 1**. This logistic unit
    consists of two elements: a function to sum inputs and an activation function.
    If we use the `sigmoid` as the activation function, then we can write the following
    equation:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入都与一个权重相关联，并且这些输入被馈送到逻辑单元中。请注意，我们添加了一个偏置特征，**x[0] = 1**。这个逻辑单元包括两个元素：一个用于求和输入的函数和一个激活函数。如果我们使用`sigmoid`作为激活函数，那么我们可以写出以下方程：
- en: '![](img/5f42d9f1-a522-45ca-9d21-25b72e211ec9.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f42d9f1-a522-45ca-9d21-25b72e211ec9.png)'
- en: Note that this is exactly the hypothesis we used for logistic regression; we
    have simply swapped *θ* for *w,* to denote to the weights in the logistic unit.
    These weights are exactly equivalent to the parameters of the logistic regression
    model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这恰好是我们用于逻辑回归的假设；我们只是将*θ*换成了*w*，用来表示逻辑单元中的权重。这些权重与逻辑回归模型的参数完全相同。
- en: 'To create a neural network, we connect these logistic units into layers. The
    following diagram represents a three-layered neural network. Note that for the
    sake of clarity, we omit the bias unit:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个神经网络，我们将这些逻辑单元连接成层。以下图表示了一个三层神经网络。为了清晰起见，我们省略了偏置单元：
- en: '![](img/30bec0aa-c10b-4014-a129-03c7fe47c813.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30bec0aa-c10b-4014-a129-03c7fe47c813.png)'
- en: 'This simple ANN consists of an input layer with three units; one hidden layer,
    also with three units; and finally, a single unit in the output. We use the notation
    *ai(j)* to refer to the activation of unit *i* in layer *j* and with *W* ^((j)) denoting
    the matrix of weights that map layer *j* to layer *j+1*. Using this notation,
    we can express the activation of the three hidden units with the following equations:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的人工神经网络包括一个具有三个单元的输入层；一个也有三个单元的隐藏层；最后是一个单一的输出单元。我们使用符号 *ai(j)* 表示层 *j* 中单元
    *i* 的激活，并使用 *W* ^((j)) 表示将层 *j* 映射到层 *j+1* 的权重矩阵。利用这个符号表示法，我们可以用以下方程表示三个隐藏单元的激活：
- en: '![](img/3fd2c34b-8976-4342-922a-8695c073863d.png)![](img/16851b0a-9c75-41c0-bc77-2295c603947f.png)![](img/cb5695a9-bacc-46e7-a734-9708672c6571.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fd2c34b-8976-4342-922a-8695c073863d.png)![](img/16851b0a-9c75-41c0-bc77-2295c603947f.png)![](img/cb5695a9-bacc-46e7-a734-9708672c6571.png)'
- en: 'The activation of the output unit can then be expressed by the following equation:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出单元的激活可以用以下方程表示：
- en: '![](img/d72f5919-43ac-4479-b12c-a1f728349985.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d72f5919-43ac-4479-b12c-a1f728349985.png)'
- en: Here, *W*^(*(1)* )is a 3 x 4 matrix controlling the function mapping between
    the input, layer one, and the single hidden layer, layer two. The weight matrix
    *W^(^(2))*, of size 1 X 4, controls the mapping between the hidden layer and the
    output layer, H. More generally, a network with *s[j] *units in layer *j* and
    *s[k]* units in layer *j+1* will have a size of *s[k] *by *(s[j])+1.* For example,
    for a network that has five input units and three units in the next forward layer,
    layer two, the associated weight matrix, *W^((1))* will be of size 3 x 6.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*^(*(1)* ) 是一个 3 x 4 的矩阵，控制输入层、第一层和单个隐藏层之间的函数映射。大小为 1 x 4 的权重矩阵 *W^(^(2))*
    控制隐藏层与输出层 H 之间的映射。更一般地，一个具有 *s[j]* 层单元和 *s[k]* 层单元的网络将具有 *s[k]* x (*s[j]+1)* 的大小。例如，对于一个具有五个输入单元和三个在下一个前向层（第二层）的单元的网络，相关的权重矩阵
    *W^((1))* 将是 3 x 6 的大小。
- en: 'Having established a hypotheses function, the next step is to formulate a cost
    function to measure, and ultimately minimize, the error of the model. For classification,
    the cost function is almost identical to that used for logistic regression. The
    important difference is that with neural networks, we can add output units to
    allow multi-class classification. We can write the cost function for multiple
    outputs as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 建立了假设函数后，下一步是制定成本函数来衡量并最终最小化模型的误差。对于分类问题，成本函数几乎与逻辑回归中使用的函数相同。主要的区别在于，使用神经网络时，我们可以添加输出单元以支持多类分类。我们可以将多输出的成本函数写成如下形式：
- en: '![](img/459fdd75-08ab-4dd3-9454-25288f27e48c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/459fdd75-08ab-4dd3-9454-25288f27e48c.png)'
- en: Here, *K* is the number of output units representing the number of output classes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*K* 是表示输出类别数量的输出单元数。
- en: 'Finally, we need to minimize the cost function, which is done using the backpropagation
    algorithm. Essentially, what this does is backpropagate the error, the gradient
    of the cost function, from the output units to the input units. To do this, we
    need to evaluate partial derivatives. That is, we need to compute the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要使用反向传播算法来最小化成本函数。基本上，这个算法将误差（成本函数的梯度）从输出单元反向传播到输入单元。为了做到这一点，我们需要计算偏导数。也就是说，我们需要计算以下内容：
- en: '![](img/a05b11e0-d145-47fa-b88f-85d0d646e2bf.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a05b11e0-d145-47fa-b88f-85d0d646e2bf.png)'
- en: 'Here, *l* is the layer, *j* is the unit, and *i* is the sample. In other word,
    for each unit in each layer, and for every sample, we need to calculate the partial
    derivative, the gradient, of the cost function with respect to each parameter.
    For example, consider we have a network with four layers. Consider also that we
    are working with a single sample. We need to find the error at each layer, beginning
    at the output. The error at the output is just the error of the hypothesis:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*l* 是层，*j* 是单元，*i* 是样本。换句话说，对于每一层中的每一个单元，对于每一个样本，我们需要计算成本函数关于每个参数的偏导数，即梯度。例如，考虑一个具有四层的网络。同时考虑我们正在处理一个单样本。我们需要找到每一层的误差，从输出开始。输出层的误差就是假设的误差：
- en: '![](img/88176028-0f46-4339-a542-bca008929666.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88176028-0f46-4339-a542-bca008929666.png)'
- en: 'This is a vector of the error for each unit, *j*. The superscript *(4)* indicates
    this is the fourth layer; that is, the output layer. It turns out that, through
    some complicated math we do not need to worry about here, the error for the two
    hidden layers can be calculated with the following equations:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个单元 *j* 的误差向量。上标 *(4)* 表示这是第四层，即输出层。通过一些我们这里不需要担心的复杂数学，可以用以下方程计算两个隐藏层的误差：
- en: '![](img/7b0e678d-77b4-47b0-badc-71603e40d9f0.png)![](img/a00835ff-2cee-4bc2-b334-745674ddc273.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b0e678d-77b4-47b0-badc-71603e40d9f0.png)![](img/a00835ff-2cee-4bc2-b334-745674ddc273.png)'
- en: The `.*`operator here is element-wise vector multiplication. Notice that the
    error vector of the next forward layer is required in each of these equations.
    That is, to calculate the error in layer three, the error vector of the output
    layer is required. Similarly, to calculate the error in layer two requires the
    error vector of layer three.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 `.*` 操作符是逐元素向量乘法。请注意，每个方程中都需要下一个前向层的误差向量。也就是说，要计算第三层的误差，需要输出层的误差向量。同样地，要计算第二层的误差，需要第三层的误差向量。
- en: 'This is how backpropagation works with a single sample. To loop across an entire
    dataset, we need to accumulate the gradients for each unit and each sample. So,
    for each sample in the training set, the neural net performs forward propagation
    to compute the activation for the hidden layers and the output layer. Then, for
    the same sample, that is within the same loop, the output error can be calculated. Consequently,
    we are able calculate the error for each previous layer in turn, and the neural
    net does exactly this, accumulating each gradient in a matrix. The loop begins
    again performing the identical set of operations on the next sample, and these
    gradients are also accumulated in the error matrix. We can write an update rule
    as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是单个样本下反向传播的工作方式。要循环遍历整个数据集，我们需要累积每个单元和每个样本的梯度。因此，对于训练集中的每个样本，神经网络执行前向传播以计算隐藏层和输出层的激活。然后，在同一个样本内，也就是在同一个循环内，可以计算输出误差。因此，我们能够依次计算每个前一层的误差，并且神经网络确实这样做，将每个梯度累积在一个矩阵中。循环重新开始，在下一个样本上执行相同的操作，并将这些梯度也累积在误差矩阵中。我们可以写出如下的更新规则：
- en: '![](img/6f9c3ede-fcc7-49ec-a32d-0d0eb6500927.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f9c3ede-fcc7-49ec-a32d-0d0eb6500927.png)'
- en: 'The capital delta is the matrix that stores the accumulated gradients by adding
    the activation for layer *l,* unit *j*, and sample *i*, then multiplying it with
    the associated error of the next forward layer for this same sample, *i*. Finally,
    once we have made a pass over the entire training set—an epoch—we can calculate
    the derivative of the cost function with respect to each parameter:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 大写希腊字母 delta 是一个矩阵，它通过将层 *l*、单元 *j* 和样本 *i* 的激活累加起来，然后乘以下一个前向层对于同一样本 *i* 的相关误差，来存储累积梯度。最后，一旦我们完成整个训练集的一次遍历——一个
    epoch——我们可以计算每个参数相对于成本函数的导数：
- en: '![](img/c1980f30-b352-4d7e-8f33-a0f196067f1f.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1980f30-b352-4d7e-8f33-a0f196067f1f.png)'
- en: Once again, it is not necessary to know the formal proof for this; it's just
    to give you some intuitive understanding of the mechanics of backpropagation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，不需要了解这个的正式证明；这只是为了让你对反向传播的机制有一些直观的理解。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We have covered a lot of material in this chapter. Don''t worry if you do not
    understand some of the mathematics presented here. The aim is to give you some
    intuition into how some common machine learning algorithms work, not to have a
    complete understanding of the theory behind these algorithms. After reading this
    chapter, you should have some understanding of the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了大量的内容。如果你对这里呈现的数学内容不太理解，不要担心。我们的目标是让你对一些常见的机器学习算法如何工作有一些直观的认识，而不是完全理解这些算法背后的理论。阅读完本章后，你应该对以下内容有一些了解：
- en: General approaches to machine learning, including knowing the difference between
    supervised and unsupervised methods, online and batch learning, and rule-based,
    as opposed to model-based, learning
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的一般方法，包括了解监督和无监督方法的区别，在线学习和批量学习，以及基于规则与基于模型的学习
- en: Some unsupervised methods and their applications, such as clustering and principle
    component analysis
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些无监督方法及其应用，如聚类和主成分分析
- en: Types of classification problems, such as binary, multi-class, and multi-out
    classification
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类问题的类型，如二分类、多类分类和多输出分类
- en: Features and feature transformations
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征和特征转换
- en: The mechanics of linear regression and gradient descent
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归和梯度下降的机制
- en: An overview of neural networks and the backpropagation algorithm
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络及反向传播算法的概述
- en: In [Chapter 3](77e1b6da-e5d6-46a4-8a2c-ee1cfa686cc6.xhtml), *Computational Graphs
    and Linear Models*, we will apply some of these concepts using PyTorch. Specifically,
    we will show how to find the gradients of functions by building a simple linear
    model. You will also gain a practical understanding of backpropagation by implementing
    a simple neural network.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](77e1b6da-e5d6-46a4-8a2c-ee1cfa686cc6.xhtml)，*计算图与线性模型*，我们将运用PyTorch应用这些概念。具体来说，我们将展示如何通过构建一个简单的线性模型来找到函数的梯度。您还将通过实现一个简单的神经网络来获得反向传播的实际理解。
