["```py\nThe cat sat on the mat. \n```", "```py\n['the', 'transform', 'er', 'is', 'an', 'innovative', 'n', 'l', 'p', 'model', '!'] \n```", "```py\ntext = \"The cat slept on the couch.It was too tired to get up.\"\ntokenized text= [1996, 4937, 7771, 2006, 1996, 6411, 1012, 2009, 2001, 2205, 5458, 2000, 2131, 2039, 1012] \n```", "```py\nThe black cat sat on the couch and the brown dog slept on the rug. \n```", "```py\nblack=[[-0.01206071  0.11632373  0.06206119  0.01403395  0.09541149  0.10695464 0.02560172  0.00185677 -0.04284821  0.06146432  0.09466285  0.04642421 0.08680347  0.05684567 -0.00717266 -0.03163519  0.03292002 -0.11397766 0.01304929  0.01964396  0.01902409  0.02831945  0.05870414  0.03390711 -0.06204525  0.06173197 -0.08613958 -0.04654748  0.02728105 -0.07830904\n    …\n0.04340003 -0.13192849 -0.00945092 -0.00835463 -0.06487109  0.05862355 -0.03407936 -0.00059001 -0.01640179  0.04123065 \n-0.04756588  0.08812257 0.00200338 -0.0931043  -0.03507337  0.02153351 -0.02621627 -0.02492662 -0.05771535 -0.01164199 \n-0.03879078 -0.05506947  0.01693138 -0.04124579 -0.03779858 \n-0.01950983 -0.05398201  0.07582296  0.00038318 -0.04639162 \n-0.06819214  0.01366171  0.01411388  0.00853774  0.02183574 \n-0.03016279 -0.03184025 -0.04273562]] \n```", "```py\nbrown=[[ 1.35794589e-02 -2.18823571e-02  1.34526128e-02  6.74355254e-02\n   1.04376070e-01  1.09921647e-02 -5.46298288e-02 -1.18385479e-02\n   4.41223830e-02 -1.84863899e-02 -6.84073642e-02  3.21860164e-02\n   4.09143828e-02 -2.74433400e-02 -2.47369967e-02  7.74542615e-02\n   9.80964210e-03  2.94299088e-02  2.93895267e-02 -3.29437815e-02\n…\n  7.20389187e-02  1.57317147e-02 -3.10291946e-02 -5.51304631e-02\n  -7.03861639e-02  7.40829483e-02  1.04319192e-02 -2.01565702e-03\n   2.43322570e-02  1.92969330e-02  2.57341694e-02 -1.13280728e-01\n   8.45847875e-02  4.90090018e-03  5.33546880e-02 -2.31553353e-02\n   3.87288055e-05  3.31782512e-02 -4.00604047e-02 -1.02028981e-01\n   3.49597558e-02 -1.71501152e-02  3.55573371e-02 -1.77437533e-02\n  -5.94457164e-02  2.21221056e-02  9.73121971e-02 -4.90022525e-02]] \n```", "```py\ncosine_similarity(black, brown)= [[0.9998901]] \n```", "```py\nThe `black` cat sat on the couch and the `brown` dog slept on the rug. \n```", "```py\ndef positional_encoding(pos,pe):\nfor i in range(0, 512,2):\n         pe[0][i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n         pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\nreturn pe \n```", "```py\nplot y=sin(2/10000^(2*x/512)) \n```", "```py\nThe black cat sat on the couch and the brown dog slept on the rug. \n```", "```py\nPE(2)= \n[[ 9.09297407e-01 -4.16146845e-01  9.58144367e-01 -2.86285430e-01\n   9.87046242e-01 -1.60435960e-01  9.99164224e-01 -4.08766568e-02\n   9.97479975e-01  7.09482506e-02  9.84703004e-01  1.74241230e-01\n   9.63226616e-01  2.68690288e-01  9.35118318e-01  3.54335666e-01\n   9.02130723e-01  4.31462824e-01  8.65725577e-01  5.00518918e-01\n   8.27103794e-01  5.62049210e-01  7.87237823e-01  6.16649508e-01\n   7.46903539e-01  6.64932430e-01  7.06710517e-01  7.07502782e-01\n…\n   5.47683925e-08  1.00000000e+00  5.09659337e-08  1.00000000e+00\n   4.74274735e-08  1.00000000e+00  4.41346799e-08  1.00000000e+00\n   4.10704999e-08  1.00000000e+00  3.82190599e-08  1.00000000e+00\n   3.55655878e-08  1.00000000e+00  3.30963417e-08  1.00000000e+00\n   3.07985317e-08  1.00000000e+00  2.86602511e-08  1.00000000e+00\n   2.66704294e-08  1.00000000e+00  2.48187551e-08  1.00000000e+00\n   2.30956392e-08  1.00000000e+00  2.14921574e-08  1.00000000e+00]] \n```", "```py\nPE(10)= \n[[-5.44021130e-01 -8.39071512e-01  1.18776485e-01 -9.92920995e-01\n   6.92634165e-01 -7.21289039e-01  9.79174793e-01 -2.03019097e-01\n   9.37632740e-01  3.47627431e-01  6.40478015e-01  7.67976522e-01\n   2.09077001e-01  9.77899194e-01 -2.37917677e-01  9.71285343e-01\n  -6.12936735e-01  7.90131986e-01 -8.67519796e-01  4.97402608e-01\n  -9.87655997e-01  1.56638563e-01 -9.83699203e-01 -1.79821849e-01\n…\n  2.73841977e-07  1.00000000e+00  2.54829672e-07  1.00000000e+00\n   2.37137371e-07  1.00000000e+00  2.20673414e-07  1.00000000e+00\n   2.05352507e-07  1.00000000e+00  1.91095296e-07  1.00000000e+00\n   1.77827943e-07  1.00000000e+00  1.65481708e-07  1.00000000e+00\n   1.53992659e-07  1.00000000e+00  1.43301250e-07  1.00000000e+00\n   1.33352145e-07  1.00000000e+00  1.24093773e-07  1.00000000e+00\n   1.15478201e-07  1.00000000e+00  1.07460785e-07  1.00000000e+00]] \n```", "```py\ncosine_similarity(pos(2), pos(10))= [[0.8600013]] \n```", "```py\ncosine_similarity(black, brown)= [[0.9998901]] \n```", "```py\nfor i in range(0, 512,2):\n          pe[0][i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n          pc[0][i] = (y[0][i]*math.sqrt(d_model))+ pe[0][i]\n\n          pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\n          pc[0][i+1] = (y[0][i+1]*math.sqrt(d_model))+ pe[0][i+1] \n```", "```py\npc(black)=\n[[ 9.09297407e-01 -4.16146845e-01  9.58144367e-01 -2.86285430e-01\n   9.87046242e-01 -1.60435960e-01  9.99164224e-01 -4.08766568e-02\n   …\n  4.74274735e-08  1.00000000e+00  4.41346799e-08  1.00000000e+00\n   4.10704999e-08  1.00000000e+00  3.82190599e-08  1.00000000e+00\n   2.66704294e-08  1.00000000e+00  2.48187551e-08  1.00000000e+00\n   2.30956392e-08  1.00000000e+00  2.14921574e-08  1.00000000e+00]] \n```", "```py\ncosine_similarity(pc(black), pc(brown))= [[0.9627094]] \n```", "```py\n[[0.99987495]] word similarity\n[[0.8600013]] positional encoding vector similarity\n[[0.9627094]] final positional encoding similarity \n```", "```py\nSequence =The cat sat on the rug and it was dry-cleaned. \n```", "```py\nimport numpy as np\nfrom scipy.special import softmax \n```", "```py\nprint(\"Step 1: Input : 3 inputs, d_model=4\")\nx =np.array([[1.0, 0.0, 1.0, 0.0],   # Input 1\n             [0.0, 2.0, 0.0, 2.0],   # Input 2\n             [1.0, 1.0, 1.0, 1.0]])  # Input 3\nprint(x) \n```", "```py\nStep 1: Input : 3 inputs, d_model=4\n[[1\\. 0\\. 1\\. 0.]\n [0\\. 2\\. 0\\. 2.]\n [1\\. 1\\. 1\\. 1.]] \n```", "```py\nprint(\"Step 2: weights 3 dimensions x d_model=4\")\nprint(\"w_query\")\nw_query =np.array([[1, 0, 1],\n                   [1, 0, 0],\n                   [0, 0, 1],\n                   [0, 1, 1]])\nprint(w_query) \n```", "```py\nw_query\n[[1 0 1]\n [1 0 0]\n [0 0 1]\n [0 1 1]] \n```", "```py\nprint(\"w_key\")\nw_key =np.array([[0, 0, 1],\n                 [1, 1, 0],\n                 [0, 1, 0],\n                 [1, 1, 0]])\nprint(w_key) \n```", "```py\nw_key\n[[0 0 1]\n [1 1 0]\n [0 1 0]\n [1 1 0]] \n```", "```py\nprint(\"w_value\")\nw_value = np.array([[0, 2, 0],\n                    [0, 3, 0],\n                    [1, 0, 3],\n                    [1, 1, 0]])\nprint(w_value) \n```", "```py\nw_value\n[[0 2 0]\n [0 3 0]\n [1 0 3]\n [1 1 0]] \n```", "```py\nprint(\"Step 3: Matrix multiplication to obtain Q,K,V\")\nprint(\"Query: x * w_query\")\nQ=np.matmul(x,w_query)\nprint(Q) \n```", "```py\nStep 3: Matrix multiplication to obtain Q,K,V\nQuery: x * w_query\n[[1\\. 0\\. 2.]\n [2\\. 2\\. 2.]\n [2\\. 1\\. 3.]] \n```", "```py\nprint(\"Key: x * w_key\")\nK=np.matmul(x,w_key)\nprint(K) \n```", "```py\nKey: x * w_key\n[[0\\. 1\\. 1.]\n [4\\. 4\\. 0.]\n [2\\. 3\\. 1.]] \n```", "```py\nprint(\"Value: x * w_value\")\nV=np.matmul(x,w_value)\nprint(V) \n```", "```py\nValue: x * w_value\n[[1\\. 2\\. 3.]\n [2\\. 8\\. 0.]\n [2\\. 6\\. 3.]] \n```", "```py\nprint(\"Step 4: Scaled Attention Scores\")\nk_d=1   #square root of k_d=3 rounded down to 1 for this example\nattention_scores = (Q @ K.transpose())/k_d\nprint(attention_scores) \n```", "```py\nStep 4: Scaled Attention Scores\n[[ 2\\.  4\\.  4.]\n [ 4\\. 16\\. 12.]\n [ 4\\. 12\\. 10.]] \n```", "```py\nprint(\"Step 5: Scaled softmax attention_scores for each vector\")\nattention_scores[0]=softmax(attention_scores[0])\nattention_scores[1]=softmax(attention_scores[1])\nattention_scores[2]=softmax(attention_scores[2])\nprint(attention_scores[0])\nprint(attention_scores[1])\nprint(attention_scores[2]) \n```", "```py\nStep 5: Scaled softmax attention_scores for each vector\n[0.06337894 0.46831053 0.46831053]\n[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n[2.95387223e-04 8.80536902e-01 1.19167711e-01] \n```", "```py\nprint(\"Step 6: attention value obtained by score1/k_d * V\")\nprint(V[0])\nprint(V[1])\nprint(V[2])\nprint(\"Attention 1\")\nattention1=attention_scores[0].reshape(-1,1)\nattention1=attention_scores[0][0]*V[0]\nprint(attention1)\nprint(\"Attention 2\")\nattention2=attention_scores[0][1]*V[1]\nprint(attention2)\nprint(\"Attention 3\")\nattention3=attention_scores[0][2]*V[2]\nprint(attention3)\nStep 6: attention value obtained by score1/k_d * V\n[1\\. 2\\. 3.]\n[2\\. 8\\. 0.]\n[2\\. 6\\. 3.]\nAttention 1\n[0.06337894 0.12675788 0.19013681]\nAttention 2\n[0.93662106 3.74648425 0\\.        ]\nAttention 3\n[0.93662106 2.80986319 1.40493159] \n```", "```py\nprint(\"Step 7: summed the results to create the first line of the output matrix\")\nattention_input1=attention1+attention2+attention3\nprint(attention_input1) \n```", "```py\nStep 7: summed the results to create the first line of the output matrix\n[1.93662106 6.68310531 1.59506841]] \n```", "```py\nprint(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n#We assume we have 3 results with learned weights (they were not trained in this example)\n#We assume we are implementing the original Transformer paper.We will have 3 results of 64 dimensions each\nattention_head1=np.random.random((3, 64))\nprint(attention_head1) \n```", "```py\nStep 8: Step 1 to 7 for inputs 1 to 3\n[[0.31982626 0.99175996…(61 squeezed values)…0.16233212]\n [0.99584327 0.55528662…(61 squeezed values)…0.70160307]\n [0.14811583 0.50875291…(61 squeezed values)…0.83141355]] \n```", "```py\nprint(\"Step 9: We assume we have trained the 8 heads of the attention sublayer\")\nz0h1=np.random.random((3, 64))\nz1h2=np.random.random((3, 64))\nz2h3=np.random.random((3, 64))\nz3h4=np.random.random((3, 64))\nz4h5=np.random.random((3, 64))\nz5h6=np.random.random((3, 64))\nz6h7=np.random.random((3, 64))\nz7h8=np.random.random((3, 64))\nprint(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8) \n```", "```py\nStep 9: We assume we have trained the 8 heads of the attention sublayer\nshape of one head (3, 64) dimension of 8 heads 512 \n```", "```py\nprint(\"Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\")\noutput_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\nprint(output_attention) \n```", "```py\nStep 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n[[0.65218495 0.11961095 0.9555153  ... 0.48399266 0.80186221 0.16486792]\n [0.95510952 0.29918492 0.7010377  ... 0.20682832 0.4123836  0.90879359]\n [0.20211378 0.86541746 0.01557758 ... 0.69449636 0.02458972 0.889699  ]] \n```", "```py\nOutput=Le chat noir était assis sur le canapé et le chien marron dormait sur le tapis \n```", "```py\nInput=The black cat sat on the couch and the brown dog slept on the rug. \n```", "```py\n!pip -q install transformers \n```", "```py\n#@title Retrieve pipeline of modules and choose English to French translation\nfrom transformers import pipeline \n```", "```py\ntranslator = pipeline(\"translation_en_to_fr\")\n#One line of code!\nprint(translator(\"It is easy to translate languages with transformers\", max_length=40)) \n```", "```py\n[{'translation_text': 'Il est facile de traduire des langues à l'aide de transformateurs.'}] \n```"]